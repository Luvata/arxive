<!DOCTYPE html>
<html>
<head>
<title>2023-12-22-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.12458">When Parameter-efficient Tuning Meets General-purpose Vision-language Models. (arXiv:2312.12458v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yihang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haixin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jianlong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinlong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jinan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a></p>
<p>Instruction tuning has shown promising potential for developing
general-purpose AI capabilities by using large-scale pre-trained models and
boosts growing research to integrate multimodal information for creative
applications. However, existing works still face two main limitations: the high
training costs and heavy computing resource dependence of full model
fine-tuning, and the lack of semantic information in instructions, which
hinders multimodal alignment. Addressing these challenges, this paper proposes
a novel approach to utilize Parameter-Efficient Tuning for generAl-purpose
vision-Language models, namely PETAL. PETAL revolutionizes the training process
by requiring only 0.5% of the total parameters, achieved through a unique mode
approximation technique, which significantly reduces the training costs and
reliance on heavy computing resources. Furthermore, PETAL enhances the semantic
depth of instructions in two innovative ways: 1) by introducing adaptive
instruction mixture-of-experts(MOEs), and 2) by fortifying the score-based
linkage between parameter-efficient tuning and mutual information. Our
extensive experiments across five multimodal downstream benchmarks reveal that
PETAL not only outperforms current state-of-the-art methods in most scenarios
but also surpasses full fine-tuning models in effectiveness. Additionally, our
approach demonstrates remarkable advantages in few-shot settings, backed by
comprehensive visualization analyses. Our source code is available at:
https://github. com/melonking32/PETAL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12464">Towards Better Serialization of Tabular Data for Few-shot Classification. (arXiv:2312.12464v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaitly_S/0/1/0/all/0/1">Sukriti Jaitly</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1">Tanay Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shugani_A/0/1/0/all/0/1">Ashish Shugani</a>, <a href="http://arxiv.org/find/cs/1/au:+Grewal_R/0/1/0/all/0/1">Razik Singh Grewal</a></p>
<p>We present a study on the integration of Large Language Models (LLMs) in
tabular data classification, emphasizing an efficient framework. Building upon
existing work done in TabLLM (<a href="/abs/2210.10723">arXiv:2210.10723</a>), we introduce three novel
serialization techniques, including the standout LaTeX serialization method.
This method significantly boosts the performance of LLMs in processing
domain-specific datasets, Our method stands out for its memory efficiency and
ability to fully utilize complex data structures. Through extensive
experimentation, including various serialization approaches like feature
combination and importance, we demonstrate our work's superiority in accuracy
and efficiency over traditional models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12466">Users Approach on Providing Feedback for Smart Home Devices. (arXiv:2312.12466v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pogaku_S/0/1/0/all/0/1">Santhosh Pogaku</a></p>
<p>Smart Home technology has accomplished extraordinary interest in making
individuals' lives more straightforward and more relaxing as of late.
Technology as of late brought about delivering numerous savvy and refined
frameworks which advanced clever living innovation. In this paper, we will be
investigating the behavioural intention of user's approach on providing
feedback for smart home devices. We will be conducting an online survey for
sample of three to five students selected by simple random sampling to study
the user's motto for giving feedback on smart home devices and their
expectations. We have observed that most users are ready to share their
feedback on smart home devices actively to improvise the service and quality of
the product to fulfill the user needs and make their lives easier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12588">An Empirical study of Unsupervised Neural Machine Translation: analyzing NMT output, model&#x27;s behavior and sentences&#x27; contribution. (arXiv:2312.12588v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tourni_I/0/1/0/all/0/1">Isidora Chara Tourni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1">Derry Wijaya</a></p>
<p>Unsupervised Neural Machine Translation (UNMT) focuses on improving NMT
results under the assumption there is no human translated parallel data, yet
little work has been done so far in highlighting its advantages compared to
supervised methods and analyzing its output in aspects other than translation
accuracy. We focus on three very diverse languages, French, Gujarati, and
Kazakh, and train bilingual NMT models, to and from English, with various
levels of supervision, in high- and low- resource setups, measure quality of
the NMT output and compare the generated sequences' word order and semantic
similarity to source and reference sentences. We also use Layer-wise Relevance
Propagation to evaluate the source and target sentences' contribution to the
result, expanding the findings of previous works to the UNMT paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12624">Building a Llama2-finetuned LLM for Odia Language Utilizing Domain Knowledge Instruction Set. (arXiv:2312.12624v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kohli_G/0/1/0/all/0/1">Guneet Singh Kohli</a>, <a href="http://arxiv.org/find/cs/1/au:+Parida_S/0/1/0/all/0/1">Shantipriya Parida</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekhar_S/0/1/0/all/0/1">Sambit Sekhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Samirit Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1">Nipun B Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1">Parul Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1">Sonal Khosla</a>, <a href="http://arxiv.org/find/cs/1/au:+Patiyal_K/0/1/0/all/0/1">Kusumlata Patiyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhal_D/0/1/0/all/0/1">Debasish Dhal</a></p>
<p>Building LLMs for languages other than English is in great demand due to the
unavailability and performance of multilingual LLMs, such as understanding the
local context. The problem is critical for low-resource languages due to the
need for instruction sets. In a multilingual country like India, there is a
need for LLMs supporting Indic languages to provide generative AI and LLM-based
technologies and services to its citizens.
</p>
<p>This paper presents our approach of i) generating a large Odia instruction
set, including domain knowledge data suitable for LLM fine-tuning, and ii)
building a Llama2-finetuned model tailored for enhanced performance in the Odia
domain. The proposed work will help researchers build an instruction set and
LLM, particularly for Indic languages. We will release the model and
instruction set for the public for research and noncommercial purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12634">MotionScript: Natural Language Descriptions for Expressive 3D Human Motions. (arXiv:2312.12634v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yazdian_P/0/1/0/all/0/1">Payam Jome Yazdian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1">Eric Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Li Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1">Angelica Lim</a></p>
<p>This paper proposes MotionScript, a motion-to-text conversion algorithm and
natural language representation for human body motions. MotionScript aims to
describe movements in greater detail and with more accuracy than previous
natural language approaches. Many motion datasets describe relatively objective
and simple actions with little variation on the way they are expressed (e.g.
sitting, walking, dribbling a ball). But for expressive actions that contain a
diversity of movements in the class (e.g. being sad, dancing), or for actions
outside the domain of standard motion capture datasets (e.g. stylistic walking,
sign-language), more specific and granular natural language descriptions are
needed. Our proposed MotionScript descriptions differ from existing natural
language representations in that it provides direct descriptions in natural
language instead of simple action labels or high-level human captions. To the
best of our knowledge, this is the first attempt at translating 3D motions to
natural language descriptions without requiring training data. Our experiments
show that when MotionScript representations are used in a text-to-motion neural
task, body movements are more accurately reconstructed, and large language
models can be used to generate unseen complex motions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12655">Can Transformers Learn Sequential Function Classes In Context?. (arXiv:2312.12655v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campbell_R/0/1/0/all/0/1">Ryan Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1">Emma Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1">Evan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vir_R/0/1/0/all/0/1">Reya Vir</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsiao_E/0/1/0/all/0/1">Ethan Hsiao</a></p>
<p>In-context learning (ICL) has revolutionized the capabilities of transformer
models in NLP. In our project, we extend the understanding of the mechanisms
underpinning ICL by exploring whether transformers can learn from sequential,
non-textual function class data distributions. We introduce a novel sliding
window sequential function class and employ toy-sized transformers with a GPT-2
architecture to conduct our experiments. Our analysis indicates that these
models can indeed leverage ICL when trained on non-textual sequential function
classes. Additionally, our experiments with randomized y-label sequences
highlights that transformers retain some ICL capabilities even when the label
associations are obfuscated. We provide evidence that transformers can reason
with and understand sequentiality encoded within function classes, as reflected
by the effective learning of our proposed tasks. Our results also show that the
performance deteriorated with increasing randomness in the labels, though not
to the extent one might expect, implying a potential robustness of learned
sequentiality against label noise. Future research may want to look into how
previous explanations of transformers, such as induction heads and task
vectors, relate to sequentiality in ICL in these toy examples. Our
investigation lays the groundwork for further research into how transformers
process and perceive sequential data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12660">Is post-editing really faster than human translation?. (arXiv:2312.12660v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terribile_S/0/1/0/all/0/1">Silvia Terribile</a></p>
<p>Time efficiency is paramount for the localisation industry, which demands
ever-faster turnaround times. However, translation speed is largely
underresearched, and there is a lack of clarity about how language service
providers (LSPs) can evaluate the performance of their post-editing (PE) and
human translation (HT) services. This study constitutes the first large-scale
investigation of translation and revision speed in HT and in the PE of neural
machine translation, based on real-world data from an LSP. It uses an
exploratory data analysis approach to investigate data for 90 million words
translated by 879 linguists across 11 language pairs, over 2.5 years. The
results of this research indicate that (a) PE is usually but not always faster
than HT; (b) average speed values may be misleading; (c) translation speed is
highly variable; and (d) edit distance cannot be used as a proxy for
post-editing productivity, because it does not correlate strongly with speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12681">Imitation of Life: A Search Engine for Biologically Inspired Design. (arXiv:2312.12681v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emuna_H/0/1/0/all/0/1">Hen Emuna</a>, <a href="http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1">Nadav Borenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1">Xin Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hyeonsu Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1">Joel Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1">Aniket Kittur</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1">Dafna Shahaf</a></p>
<p>Biologically Inspired Design (BID), or Biomimicry, is a problem-solving
methodology that applies analogies from nature to solve engineering challenges.
For example, Speedo engineers designed swimsuits based on shark skin. Finding
relevant biological solutions for real-world problems poses significant
challenges, both due to the limited biological knowledge engineers and
designers typically possess and to the limited BID resources. Existing BID
datasets are hand-curated and small, and scaling them up requires costly human
annotations.
</p>
<p>In this paper, we introduce BARcode (Biological Analogy Retriever), a search
engine for automatically mining bio-inspirations from the web at scale. Using
advances in natural language understanding and data programming, BARcode
identifies potential inspirations for engineering challenges. Our experiments
demonstrate that BARcode can retrieve inspirations that are valuable to
engineers and designers tackling real-world problems, as well as recover famous
historical BID examples. We release data and code; we view BARcode as a step
towards addressing the challenges that have historically hindered the practical
application of BID to engineering innovation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12682">Mini-GPTs: Efficient Large Language Models through Contextual Pruning. (arXiv:2312.12682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valicenti_T/0/1/0/all/0/1">Tim Valicenti</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1">Justice Vidal</a>, <a href="http://arxiv.org/find/cs/1/au:+Patnaik_R/0/1/0/all/0/1">Ritik Patnaik</a></p>
<p>In AI research, the optimization of Large Language Models (LLMs) remains a
significant challenge, crucial for advancing the field's practical applications
and sustainability. Building upon the foundational work of Professor Song Han's
lab at MIT, this paper introduces a novel approach in developing Mini-GPTs via
contextual pruning. Our methodology strategically prunes the computational
architecture of traditional LLMs, like Phi-1.5, focusing on retaining core
functionalities while drastically reducing model sizes. We employ the technique
across diverse and complex datasets, including US law, Medical Q&amp;A, Skyrim
dialogue, English-Taiwanese translation, and Economics articles. The results
underscore the efficiency and effectiveness of contextual pruning, not merely
as a theoretical concept but as a practical tool in developing domain-specific,
resource-efficient LLMs. Contextual pruning is a promising method for building
domain-specific LLMs, and this research is a building block towards future
development with more hardware compute, refined fine-tuning, and quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12683">Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?. (arXiv:2312.12683v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kew_T/0/1/0/all/0/1">Tannon Kew</a>, <a href="http://arxiv.org/find/cs/1/au:+Schottmann_F/0/1/0/all/0/1">Florian Schottmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a></p>
<p>The vast majority of today's large language models are English-centric,
having been pretrained predominantly on English text. Yet, in order to meet
user expectations, models need to be able to respond appropriately in multiple
languages once deployed in downstream applications. Given limited exposure to
other languages during pretraining, cross-lingual transfer is important for
achieving decent performance in non-English settings. In this work, we
investigate just how much multilinguality is required during finetuning to
elicit strong cross-lingual generalisation across a range of tasks and target
languages. We find that, compared to English-only finetuning, multilingual
instruction tuning with as few as three languages significantly improves a
model's cross-lingual transfer abilities on generative tasks that assume
input/output language agreement, while being of less importance for highly
structured tasks. Our code and data is available at
https://github.com/ZurichNLP/multilingual-instruction-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12713">Response Enhanced Semi-Supervised Dialogue Query Generation. (arXiv:2312.12713v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jianheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Ante Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Linfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Linfeng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jinsong Su</a></p>
<p>Leveraging vast and continually updated knowledge from the Internet has been
considered an important ability for a dialogue system. Therefore, the dialogue
query generation task is proposed for generating search queries from dialogue
histories, which will be submitted to a search engine for retrieving relevant
websites on the Internet. In this regard, previous efforts were devoted to
collecting conversations with annotated queries and training a query producer
(QP) via standard supervised learning. However, these studies still face the
challenges of data scarcity and domain adaptation. To address these issues, in
this paper, we propose a semi-supervised learning framework -- SemiDQG, to
improve model performance with unlabeled conversations. Based on the
observation that the search query is typically related to the topic of dialogue
response, we train a response-augmented query producer (RA) to provide rich and
effective training signals for QP. We first apply a similarity-based query
selection strategy to select high-quality RA-generated pseudo queries, which
are used to construct pseudo instances for training QP and RA. Then, we adopt
the REINFORCE algorithm to further enhance QP, with RA-provided rewards as
fine-grained training signals. Experimental results and in-depth analysis of
three benchmarks show the effectiveness of our framework in cross-domain and
low-resource scenarios. Particularly, SemiDQG significantly surpasses ChatGPT
and competitive baselines. Our code is available at
\url{https://github.com/DeepLearnXMU/SemiDQG}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12716">BloomVQA: Assessing Hierarchical Multi-modal Comprehension. (arXiv:2312.12716v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yunye Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrestha_R/0/1/0/all/0/1">Robik Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Claypoole_J/0/1/0/all/0/1">Jared Claypoole</a>, <a href="http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1">Michael Cogswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1">Arijit Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1">Christopher Kanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1">Ajay Divakaran</a></p>
<p>We propose a novel VQA dataset, based on picture stories designed for
educating young children, that aims to facilitate comprehensive evaluation and
characterization of vision-language models on comprehension tasks. Unlike
current VQA datasets that often focus on fact-based memorization and simple
reasoning tasks without principled scientific grounding, we collect data
containing tasks reflecting different levels of comprehension and underlying
cognitive processes, as laid out in Bloom's Taxonomy, a classic framework
widely adopted in education research. The proposed BloomVQA dataset can be
mapped to a hierarchical graph-based representation of visual stories, enabling
automatic data augmentation and novel measures characterizing model consistency
across the underlying taxonomy. We demonstrate graded evaluation and
reliability analysis based on our proposed consistency metrics on
state-of-the-art vision-language models. Our results suggest that, while
current models achieve the most gain on low-level comprehension tasks, they
generally fall short on high-level tasks requiring more advanced comprehension
and cognitive skills, as 38.0% drop in VQA accuracy is observed comparing
lowest and highest level tasks. Furthermore, current models show consistency
patterns misaligned with human comprehension in various scenarios, suggesting
emergent structures of model behaviors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12736">Learning and Forgetting Unsafe Examples in Large Language Models. (arXiv:2312.12736v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiachen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Madras_D/0/1/0/all/0/1">David Madras</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengye Ren</a></p>
<p>As the number of large language models (LLMs) released to the public grows,
there is a pressing need to understand the safety implications associated with
these models learning from third-party custom finetuning data. We explore the
behavior of LLMs finetuned on noisy custom data containing unsafe content,
represented by datasets that contain biases, toxicity, and harmfulness, finding
that while aligned LLMs can readily learn this unsafe content, they also tend
to forget it more significantly than other examples when subsequently finetuned
on safer content. Drawing inspiration from the discrepancies in forgetting, we
introduce the "ForgetFilter" algorithm, which filters unsafe data based on how
strong the model's forgetting signal is for that data. We demonstrate that the
ForgetFilter algorithm ensures safety in customized finetuning without
compromising downstream task performance, unlike sequential safety finetuning.
ForgetFilter outperforms alternative strategies like replay and moral
self-correction in curbing LLMs' ability to assimilate unsafe content during
custom finetuning, e.g. 75% lower than not applying any safety measures and 62%
lower than using self-correction in toxicity score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12740">Fine-tuning Large Language Models for Adaptive Machine Translation. (arXiv:2312.12740v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1">Yasmin Moslem</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_R/0/1/0/all/0/1">Rejwanul Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Way_A/0/1/0/all/0/1">Andy Way</a></p>
<p>This paper presents the outcomes of fine-tuning Mistral 7B, a general-purpose
large language model (LLM), for adaptive machine translation (MT). The
fine-tuning process involves utilising a combination of zero-shot and one-shot
translation prompts within the medical domain. The primary objective is to
enhance real-time adaptive MT capabilities of Mistral 7B, enabling it to adapt
translations to the required domain at inference time. The results,
particularly for Spanish-to-English MT, showcase the efficacy of the fine-tuned
model, demonstrating quality improvements in both zero-shot and one-shot
translation scenarios, surpassing Mistral 7B's baseline performance. Notably,
the fine-tuned Mistral outperforms ChatGPT "gpt-3.5-turbo" in zero-shot
translation while achieving comparable one-shot translation quality. Moreover,
the zero-shot translation of the fine-tuned Mistral matches NLLB 3.3B's
performance, and its one-shot translation quality surpasses that of NLLB 3.3B.
These findings emphasise the significance of fine-tuning efficient LLMs like
Mistral 7B to yield high-quality zero-shot translations comparable to
task-oriented models like NLLB 3.3B. Additionally, the adaptive gains achieved
in one-shot translation are comparable to those of commercial LLMs such as
ChatGPT. Our experiments demonstrate that, with a relatively small dataset of
20,000 segments that incorporate a mix of zero-shot and one-shot prompts,
fine-tuning significantly enhances Mistral's in-context learning ability,
especially for real-time adaptive MT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12746">ChatFDA: Medical Records Risk Assessment. (arXiv:2312.12746v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">M Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">C Sun</a></p>
<p>In healthcare, the emphasis on patient safety and the minimization of medical
errors cannot be overstated. Despite concerted efforts, many healthcare
systems, especially in low-resource regions, still grapple with preventing
these errors effectively. This study explores a pioneering application aimed at
addressing this challenge by assisting caregivers in gauging potential risks
derived from medical notes. The application leverages data from openFDA,
delivering real-time, actionable insights regarding prescriptions. Preliminary
analyses conducted on the MIMIC-III \cite{mimic} dataset affirm a proof of
concept highlighting a reduction in medical errors and an amplification in
patient safety. This tool holds promise for drastically enhancing healthcare
outcomes in settings with limited resources. To bolster reproducibility and
foster further research, the codebase underpinning our methodology is
accessible on
https://github.com/autonlab/2023.hackAuton/tree/main/prescription_checker. This
is a submission for the 30th HackAuton CMU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12747">ALMANACS: A Simulatability Benchmark for Language Model Explainability. (arXiv:2312.12747v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mills_E/0/1/0/all/0/1">Edmund Mills</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Shiye Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1">Stuart Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Emmons_S/0/1/0/all/0/1">Scott Emmons</a></p>
<p>How do we measure the efficacy of language model explainability methods?
While many explainability methods have been developed, they are typically
evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help
fill this gap, we present ALMANACS, a language model explainability benchmark.
ALMANACS scores explainability methods on simulatability, i.e., how well the
explanations improve behavior prediction on new inputs. The ALMANACS scenarios
span twelve safety-relevant topics such as ethical reasoning and advanced AI
behaviors; they have idiosyncratic premises to invoke model-specific behavior;
and they have a train-test distributional shift to encourage faithful
explanations. By using another language model to predict behavior based on the
explanations, ALMANACS is a fully automated benchmark. We use ALMANACS to
evaluate counterfactuals, rationalizations, attention, and Integrated Gradients
explanations. Our results are sobering: when averaged across all topics, no
explanation method outperforms the explanation-free control. We conclude that
despite modest successes in prior work, developing an explanation method that
aids simulatability in ALMANACS remains an open challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12754">Spectral Prompt Tuning:Unveiling Unseen Classes for Zero-Shot Semantic Segmentation. (arXiv:2312.12754v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenhao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rongtao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shibiao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Li Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Man Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaopeng Zhang</a></p>
<p>Recently, CLIP has found practical utility in the domain of pixel-level
zero-shot segmentation tasks. The present landscape features two-stage
methodologies beset by issues such as intricate pipelines and elevated
computational costs. While current one-stage approaches alleviate these
concerns and incorporate Visual Prompt Training (VPT) to uphold CLIP's
generalization capacity, they still fall short in fully harnessing CLIP's
potential for pixel-level unseen class demarcation and precise pixel
predictions. To further stimulate CLIP's zero-shot dense prediction capability,
we propose SPT-SEG, a one-stage approach that improves CLIP's adaptability from
image to pixel. Specifically, we initially introduce Spectral Prompt Tuning
(SPT), incorporating spectral prompts into the CLIP visual encoder's shallow
layers to capture structural intricacies of images, thereby enhancing
comprehension of unseen classes. Subsequently, we introduce the Spectral Guided
Decoder (SGD), utilizing both high and low-frequency information to steer the
network's spatial focus towards more prominent classification features,
enabling precise pixel-level prediction outcomes. Through extensive experiments
on two public datasets, we demonstrate the superiority of our method over
state-of-the-art approaches, performing well across all classes and
particularly excelling in handling unseen classes. Code is available
at:https://github.com/clearxu/SPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12764">Lattice Rescoring Based on Large Ensemble of Complementary Neural Language Models. (arXiv:2312.12764v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ogawa_A/0/1/0/all/0/1">Atsunori Ogawa</a>, <a href="http://arxiv.org/find/eess/1/au:+Tawara_N/0/1/0/all/0/1">Naohiro Tawara</a>, <a href="http://arxiv.org/find/eess/1/au:+Delcroix_M/0/1/0/all/0/1">Marc Delcroix</a>, <a href="http://arxiv.org/find/eess/1/au:+Araki_S/0/1/0/all/0/1">Shoko Araki</a></p>
<p>We investigate the effectiveness of using a large ensemble of advanced neural
language models (NLMs) for lattice rescoring on automatic speech recognition
(ASR) hypotheses. Previous studies have reported the effectiveness of combining
a small number of NLMs. In contrast, in this study, we combine up to eight
NLMs, i.e., forward/backward long short-term memory/Transformer-LMs that are
trained with two different random initialization seeds. We combine these NLMs
through iterative lattice generation. Since these NLMs work complementarily
with each other, by combining them one by one at each rescoring iteration,
language scores attached to given lattice arcs can be gradually refined.
Consequently, errors of the ASR hypotheses can be gradually reduced. We also
investigate the effectiveness of carrying over contextual information (previous
rescoring results) across a lattice sequence of a long speech such as a lecture
speech. In experiments using a lecture speech corpus, by combining the eight
NLMs and using context carry-over, we obtained a 24.4% relative word error rate
reduction from the ASR 1-best baseline. For further comparison, we performed
simultaneous (i.e., non-iterative) NLM combination and 100-best rescoring using
the large ensemble of NLMs, which confirmed the advantage of lattice rescoring
with iterative NLM combination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12773">Segmenting Messy Text: Detecting Boundaries in Text Derived from Historical Newspaper Images. (arXiv:2312.12773v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1">Carol Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Crone_P/0/1/0/all/0/1">Phil Crone</a> (Ancestry.com)</p>
<p>Text segmentation, the task of dividing a document into sections, is often a
prerequisite for performing additional natural language processing tasks.
Existing text segmentation methods have typically been developed and tested
using clean, narrative-style text with segments containing distinct topics.
Here we consider a challenging text segmentation task: dividing newspaper
marriage announcement lists into units of one announcement each. In many cases
the information is not structured into sentences, and adjacent segments are not
topically distinct from each other. In addition, the text of the announcements,
which is derived from images of historical newspapers via optical character
recognition, contains many typographical errors. As a result, these
announcements are not amenable to segmentation with existing techniques. We
present a novel deep learning-based model for segmenting such text and show
that it significantly outperforms an existing state-of-the-art method on our
task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12783">Stable Distillation: Regularizing Continued Pre-training for Low-Resource Automatic Speech Recognition. (arXiv:2312.12783v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1">Ashish Seth</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1">Sreyan Ghosh</a>, <a href="http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1">S. Umesh</a>, <a href="http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a></p>
<p>Continued self-supervised (SSL) pre-training for adapting existing SSL models
to the target domain has shown to be extremely effective for low-resource
Automatic Speech Recognition (ASR). This paper proposes Stable Distillation, a
simple and novel approach for SSL-based continued pre-training that boosts ASR
performance in the target domain where both labeled and unlabeled data are
limited. Stable Distillation employs self-distillation as regularization for
continued pre-training, alleviating the over-fitting issue, a common problem
continued pre-training faces when the source and target domains differ.
Specifically, first, we perform vanilla continued pre-training on an initial
SSL pre-trained model on the target domain ASR dataset and call it the teacher.
Next, we take the same initial pre-trained model as a student to perform
continued pre-training while enforcing its hidden representations to be close
to that of the teacher (via MSE loss). This student is then used for downstream
ASR fine-tuning on the target dataset. In practice, Stable Distillation
outperforms all our baselines by 0.8 - 7 WER when evaluated in various
experimental settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12806">MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models. (arXiv:2312.12806v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Linlin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ye Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1">Gerard de Melo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a></p>
<p>The emergence of various medical large language models (LLMs) in the medical
domain has highlighted the need for unified evaluation standards, as manual
evaluation of LLMs proves to be time-consuming and labor-intensive. To address
this issue, we introduce MedBench, a comprehensive benchmark for the Chinese
medical domain, comprising 40,041 questions sourced from authentic examination
exercises and medical reports of diverse branches of medicine. In particular,
this benchmark is composed of four key components: the Chinese Medical
Licensing Examination, the Resident Standardization Training Examination, the
Doctor In-Charge Qualification Examination, and real-world clinic cases
encompassing examinations, diagnoses, and treatments. MedBench replicates the
educational progression and clinical practice experiences of doctors in
Mainland China, thereby establishing itself as a credible benchmark for
assessing the mastery of knowledge and reasoning abilities in medical language
learning models. We perform extensive experiments and conduct an in-depth
analysis from diverse perspectives, which culminate in the following findings:
(1) Chinese medical LLMs underperform on this benchmark, highlighting the need
for significant advances in clinical knowledge and diagnostic precision. (2)
Several general-domain LLMs surprisingly possess considerable medical
knowledge. These findings elucidate both the capabilities and limitations of
LLMs within the context of MedBench, with the ultimate goal of aiding the
medical research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12808">Enhancing Consistency in Multimodal Dialogue System Using LLM with Dialogue Scenario. (arXiv:2312.12808v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Onozeki_H/0/1/0/all/0/1">Hiroki Onozeki</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhiyang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Akiyama_K/0/1/0/all/0/1">Kazuma Akiyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Asahara_R/0/1/0/all/0/1">Ryutaro Asahara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1">Takumasa Kaneko</a>, <a href="http://arxiv.org/find/cs/1/au:+Inaba_M/0/1/0/all/0/1">Michimasa Inaba</a></p>
<p>This paper describes our dialogue system submitted to Dialogue Robot
Competition 2023. The system's task is to help a user at a travel agency decide
on a plan for visiting two sightseeing spots in Kyoto City that satisfy the
user. Our dialogue system is flexible and stable and responds to user
requirements by controlling dialogue flow according to dialogue scenarios. We
also improved user satisfaction by introducing motion and speech control based
on system utterances and user situations. In the preliminary round, our system
was ranked fifth in the impression evaluation and sixth in the plan evaluation
among all 12 teams.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12815">OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using Semantic Understanding in Mixed Reality. (arXiv:2312.12815v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1">Luke Yoffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Aditya Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1">Tobias H&#xf6;llerer</a></p>
<p>One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12832">Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data. (arXiv:2312.12832v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1">Peiwen Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shaoxiong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1">Boyuan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Bin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinglin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Heda Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kan Li</a></p>
<p>Large Language Models (LLMs) have performed well on various reasoning tasks,
but their inaccessibility and numerous parameters hinder wide application in
practice. One promising way is distilling the reasoning ability from LLMs to
small models by the generated chain-of-thought reasoning paths. In some cases,
however, LLMs may produce incorrect reasoning chains, especially when facing
complex mathematical problems. Previous studies only transfer knowledge from
positive samples and drop the synthesized data with wrong answers. In this
work, we illustrate the merit of negative data and propose a model
specialization framework to distill LLMs with negative samples besides positive
ones. The framework consists of three progressive steps, covering from training
to inference stages, to absorb knowledge from negative data. We conduct
extensive experiments across arithmetic reasoning tasks to demonstrate the role
of negative data in distillation from LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12850">A Stochastic Analysis of the Linguistic Provenance of English Place Names. (arXiv:2312.12850v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dalvean_M/0/1/0/all/0/1">Michael Dalvean</a></p>
<p>In English place name analysis, meanings are often derived from the
resemblance of roots in place names to topographical features, proper names
and/or habitation terms in one of the languages that have had an influence on
English place names. The problem here is that it is sometimes difficult to
determine the base language to use to interpret the roots. The purpose of this
paper is to stochastically determine the resemblance between 18799 English
place names and 84685 place names from Ireland, Scotland, Wales, Denmark,
Norway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English
place name is ranked according to the extent to which it resembles place names
from the other countries, and this provides a basis for determining the likely
language to use to interpret the place name. A number of observations can be
made using the ranking provided. In particular, it is found that `Didlington'
is the most archetypically English place name in the English sample, and `Anna'
is the least. Furthermore, it is found that the place names in the non-English
datasets are most similar to Norwegian place names and least similar to Welsh
place names.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12852">Language Resources for Dutch Large Language Modelling. (arXiv:2312.12852v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vanroy_B/0/1/0/all/0/1">Bram Vanroy</a></p>
<p>Despite the rapid expansion of types of large language models, there remains
a notable gap in models specifically designed for the Dutch language. This gap
is not only a shortage in terms of pretrained Dutch models but also in terms of
data, and benchmarks and leaderboards. This work provides a small step to
improve the situation. First, we introduce two fine-tuned variants of the Llama
2 13B model. We first fine-tuned Llama 2 using Dutch-specific web-crawled data
and subsequently refined this model further on multiple synthetic instruction
and chat datasets. These datasets as well as the model weights are made
available. In addition, we provide a leaderboard to keep track of the
performance of (Dutch) models on a number of generation tasks, and we include
results of a number of state-of-the-art models, including our own. Finally we
provide a critical conclusion on what we believe is needed to push forward
Dutch language models and the whole eco-system around the models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12853">CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models. (arXiv:2312.12853v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Dan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chaobin You</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiantao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Taihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a></p>
<p>As an indispensable ingredient of intelligence, commonsense reasoning is
crucial for large language models (LLMs) in real-world scenarios. In this
paper, we propose CORECODE, a dataset that contains abundant commonsense
knowledge manually annotated on dyadic dialogues, to evaluate the commonsense
reasoning and commonsense conflict detection capabilities of Chinese LLMs. We
categorize commonsense knowledge in everyday conversations into three
dimensions: entity, event, and social interaction. For easy and consistent
annotation, we standardize the form of commonsense knowledge annotation in
open-domain dialogues as "domain: slot = value". A total of 9 domains and 37
slots are defined to capture diverse commonsense knowledge. With these
pre-defined domains and slots, we collect 76,787 commonsense knowledge
annotations from 19,700 dialogues through crowdsourcing. To evaluate and
enhance the commonsense reasoning capability for LLMs on the curated dataset,
we establish a series of dialogue-level reasoning and detection tasks,
including commonsense knowledge filling, commonsense knowledge generation,
commonsense conflict phrase detection, domain identification, slot
identification, and event causal inference. A wide variety of existing
open-source Chinese LLMs are evaluated with these tasks on our dataset.
Experimental results demonstrate that these models are not competent to predict
CORECODE's plentiful reasoning content, and even ChatGPT could only achieve
0.275 and 0.084 accuracy on the domain identification and slot identification
tasks under the zero-shot setting. We release the data and codes of CORECODE at
https://github.com/danshi777/CORECODE to promote commonsense reasoning
evaluation and study of LLMs in the context of daily conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12881">Big Tech influence over AI research revisited: memetic analysis of attribution of ideas to affiliation. (arXiv:2312.12881v1 [physics.soc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Gizinski_S/0/1/0/all/0/1">Stanis&#x142;aw Gizi&#x144;ski</a>, <a href="http://arxiv.org/find/physics/1/au:+Kaczynska_P/0/1/0/all/0/1">Paulina Kaczy&#x144;ska</a>, <a href="http://arxiv.org/find/physics/1/au:+Ruczynski_H/0/1/0/all/0/1">Hubert Ruczy&#x144;ski</a>, <a href="http://arxiv.org/find/physics/1/au:+Wisnios_E/0/1/0/all/0/1">Emilia Wi&#x15b;nios</a>, <a href="http://arxiv.org/find/physics/1/au:+Pielinski_B/0/1/0/all/0/1">Bartosz Pieli&#x144;ski</a>, <a href="http://arxiv.org/find/physics/1/au:+Biecek_P/0/1/0/all/0/1">Przemys&#x142;aw Biecek</a>, <a href="http://arxiv.org/find/physics/1/au:+Sienkiewicz_J/0/1/0/all/0/1">Julian Sienkiewicz</a></p>
<p>There exists a growing discourse around the domination of Big Tech on the
landscape of artificial intelligence (AI) research, yet our comprehension of
this phenomenon remains cursory. This paper aims to broaden and deepen our
understanding of Big Tech's reach and power within AI research. It highlights
the dominance not merely in terms of sheer publication volume but rather in the
propagation of new ideas or \textit{memes}. Current studies often oversimplify
the concept of influence to the share of affiliations in academic papers,
typically sourced from limited databases such as arXiv or specific academic
conferences.
</p>
<p>The main goal of this paper is to unravel the specific nuances of such
influence, determining which AI ideas are predominantly driven by Big Tech
entities. By employing network and memetic analysis on AI-oriented paper
abstracts and their citation network, we are able to grasp a deeper insight
into this phenomenon. By utilizing two databases: OpenAlex and S2ORC, we are
able to perform such analysis on a much bigger scale than previous attempts.
</p>
<p>Our findings suggest, that while Big Tech-affiliated papers are
disproportionately more cited in some areas, the most cited papers are those
affiliated with both Big Tech and Academia. Focusing on the most contagious
memes, their attribution to specific affiliation groups (Big Tech, Academia,
mixed affiliation) seems to be equally distributed between those three groups.
This suggests that the notion of Big Tech domination over AI research is
oversimplified in the discourse.
</p>
<p>Ultimately, this more nuanced understanding of Big Tech's and Academia's
influence could inform a more symbiotic alliance between these stakeholders
which would better serve the dual goals of societal welfare and the scientific
integrity of AI research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12918">Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors. (arXiv:2312.12918v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi-Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Rong Jin</a></p>
<p>To combat the potential misuse of Natural Language Generation (NLG)
technology, a variety of algorithms have been developed for the detection of
AI-generated texts. Traditionally, this task is treated as a binary
classification problem. Although supervised learning has demonstrated promising
results, acquiring labeled data for detection purposes poses real-world
challenges and the risk of overfitting. In an effort to address these issues,
we delve into the realm of zero-shot machine-generated text detection. Existing
zero-shot detectors, typically designed for specific tasks or topics, often
assume uniform testing scenarios, limiting their practicality. In our research,
we explore various advanced Large Language Models (LLMs) and their specialized
variants, contributing to this field in several ways. In empirical studies, we
uncover a significant correlation between topics and detection performance.
Secondly, we delve into the influence of topic shifts on zero-shot detectors.
These investigations shed light on the adaptability and robustness of these
detection methods across diverse topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12989">Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised Learning for Biomedical Knowledge Curation: a focused study on chemical entities of biological interest. (arXiv:2312.12989v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Groves_E/0/1/0/all/0/1">Emily Groves</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Minhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulle_Y/0/1/0/all/0/1">Yusuf Abdulle</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunz_H/0/1/0/all/0/1">Holger Kunz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoelscher_Obermaier_J/0/1/0/all/0/1">Jason Hoelscher-Obermaier</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ronin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghan Wu</a></p>
<p>Automated knowledge curation for biomedical ontologies is key to ensure that
they remain comprehensive, high-quality and up-to-date. In the era of
foundational language models, this study compares and analyzes three NLP
paradigms for curation tasks: in-context learning (ICL), fine-tuning (FT), and
supervised learning (ML). Using the Chemical Entities of Biological Interest
(ChEBI) database as a model ontology, three curation tasks were devised. For
ICL, three prompting strategies were employed with GPT-4, GPT-3.5, BioGPT.
PubmedBERT was chosen for the FT paradigm. For ML, six embedding models were
utilized for training Random Forest and Long-Short Term Memory models. Five
setups were designed to assess ML and FT model performance across different
data availability scenarios.Datasets for curation tasks included: task 1
(620,386), task 2 (611,430), and task 3 (617,381), maintaining a 50:50 positive
versus negative ratio. For ICL models, GPT-4 achieved best accuracy scores of
0.916, 0.766 and 0.874 for tasks 1-3 respectively. In a direct comparison, ML
(trained on ~260,000 triples) outperformed ICL in accuracy across all tasks.
(accuracy differences: +.11, +.22 and +.17). Fine-tuned PubmedBERT performed
similarly to leading ML models in tasks 1 &amp; 2 (F1 differences: -.014 and
+.002), but worse in task 3 (-.048). Simulations revealed performance declines
in both ML and FT models with smaller and higher imbalanced training data.
where ICL (particularly GPT-4) excelled in tasks 1 &amp; 3. GPT-4 excelled in tasks
1 and 3 with less than 6,000 triples, surpassing ML/FT. ICL underperformed
ML/FT in task 2.ICL-augmented foundation models can be good assistants for
knowledge curation with correct prompting, however, not making ML and FT
paradigms obsolete. The latter two require task-specific data to beat ICL. In
such cases, ML relies on small pretrained embeddings, minimizing computational
demands.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12999">Machine Mindset: An MBTI Exploration of Large Language Models. (arXiv:2312.12999v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiaxi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_L/0/1/0/all/0/1">Liuzhenghao Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jing Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jing Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">YongHong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>We present a novel approach for integrating Myers-Briggs Type Indicator
(MBTI) personality traits into large language models (LLMs), addressing the
challenges of personality consistency in personalized AI. Our method, "Machine
Mindset," involves a two-phase fine-tuning and Direct Preference Optimization
(DPO) to embed MBTI traits into LLMs. This approach ensures that models
internalize these traits, offering a stable and consistent personality profile.
We demonstrate the effectiveness of our models across various domains, showing
alignment between model performance and their respective MBTI traits. The paper
highlights significant contributions in the development of personality datasets
and a new training methodology for personality integration in LLMs, enhancing
the potential for personalized AI applications. We also open-sourced our model
and part of the data at \url{https://github.com/PKU-YuanGroup/Machine-Mindset}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13010">AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. (arXiv:2312.13010v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1">Qingwen Bu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie M.Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1">Michael Luck</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1">Heming Cui</a></p>
<p>The advancement of natural language processing (NLP) has been significantly
boosted by the development of transformer-based large language models (LLMs).
These models have revolutionized NLP tasks, particularly in code generation,
aiding developers in creating software with enhanced efficiency. Despite their
advancements, challenges in balancing code snippet generation with effective
test case generation and execution persist. To address these issues, this paper
introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution
comprising a multi-agent framework with specialized agents: the programmer
agent, the test designer agent, and the test executor agent. During the coding
procedure, the programmer agent will focus on the code generation and
refinement based on the test executor agent's feedback. The test designer agent
will generate test cases for the generated code, and the test executor agent
will run the code with the test cases and write the feedback to the programmer.
This collaborative system ensures robust code generation, surpassing the
limitations of single-agent models and traditional methodologies. Our extensive
experiments on 9 code generation models and 12 enhancement approaches showcase
AgentCoder's superior performance over existing code generation models and
prompt engineering techniques across various benchmarks. For example,
AgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with
GPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13026">FusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous Self-Supervised Learning. (arXiv:2312.13026v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1">Ashish Seth</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1">Sreyan Ghosh</a>, <a href="http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1">S. Umesh</a>, <a href="http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a></p>
<p>Continued pre-training (CP) offers multiple advantages, like target domain
adaptation and the potential to exploit the continuous stream of unlabeled data
available online. However, continued pre-training on out-of-domain
distributions often leads to catastrophic forgetting of previously acquired
knowledge, leading to sub-optimal ASR performance. This paper presents FusDom,
a simple and novel methodology for SSL-based continued pre-training. FusDom
learns speech representations that are robust and adaptive yet not forgetful of
concepts seen in the past. Instead of solving the SSL pre-text task on the
output representations of a single model, FusDom leverages two identical
pre-trained SSL models, a teacher and a student, with a modified pre-training
head to solve the CP SSL pre-text task. This head employs a cross-attention
mechanism between the representations of both models while only the student
receives gradient updates and the teacher does not. Finally, the student is
fine-tuned for ASR. In practice, FusDom outperforms all our baselines across
settings significantly, with WER improvements in the range of 0.2 WER - 7.3 WER
in the target domain while retaining the performance in the earlier domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13040">Retrieval-augmented Multilingual Knowledge Editing. (arXiv:2312.13040v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1">Barry Haddow</a>, <a href="http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1">Alexandra Birch</a></p>
<p>Knowledge represented in Large Language Models (LLMs) is quite often
incorrect and can also become obsolete over time. Updating knowledge via
fine-tuning is computationally resource-hungry and not reliable, and so
knowledge editing (KE) has developed as an effective and economical alternative
to inject new knowledge or to fix factual errors in LLMs. Although there has
been considerable interest in this area, current KE research exclusively
focuses on the monolingual setting, typically in English. However, what happens
if the new knowledge is supplied in one language, but we would like to query
the LLM in a different language? To address the problem of multilingual
knowledge editing, we propose Retrieval-augmented Multilingual Knowledge Editor
(ReMaKE) to update new knowledge in LLMs. ReMaKE can perform model-agnostic
knowledge editing in multilingual settings. ReMaKE concatenates the new
knowledge retrieved from a multilingual knowledge base with prompts. Our
experimental results show that ReMaKE outperforms baseline knowledge editing
methods by a significant margin and is the first KE method to work in a
multilingual setting. We provide our multilingual knowledge editing dataset
(MzsRE) in 12 languages, which along with code, and additional project
information is available at https://github.com/Vicky-Wil/ReMaKE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13096">In Generative AI we Trust: Can Chatbots Effectively Verify Political Information?. (arXiv:2312.13096v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuznetsova_E/0/1/0/all/0/1">Elizaveta Kuznetsova</a>, <a href="http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1">Mykola Makhortykh</a>, <a href="http://arxiv.org/find/cs/1/au:+Vziatysheva_V/0/1/0/all/0/1">Victoria Vziatysheva</a>, <a href="http://arxiv.org/find/cs/1/au:+Stolze_M/0/1/0/all/0/1">Martha Stolze</a>, <a href="http://arxiv.org/find/cs/1/au:+Baghumyan_A/0/1/0/all/0/1">Ani Baghumyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1">Aleksandra Urman</a></p>
<p>This article presents a comparative analysis of the ability of two large
language model (LLM)-based chatbots, ChatGPT and Bing Chat, recently rebranded
to Microsoft Copilot, to detect veracity of political information. We use AI
auditing methodology to investigate how chatbots evaluate true, false, and
borderline statements on five topics: COVID-19, Russian aggression against
Ukraine, the Holocaust, climate change, and LGBTQ+ related debates. We compare
how the chatbots perform in high- and low-resource languages by using prompts
in English, Russian, and Ukrainian. Furthermore, we explore the ability of
chatbots to evaluate statements according to political communication concepts
of disinformation, misinformation, and conspiracy theory, using
definition-oriented prompts. We also systematically test how such evaluations
are influenced by source bias which we model by attributing specific claims to
various political and social actors. The results show high performance of
ChatGPT for the baseline veracity evaluation task, with 72 percent of the cases
evaluated correctly on average across languages without pre-training. Bing Chat
performed worse with a 67 percent accuracy. We observe significant disparities
in how chatbots evaluate prompts in high- and low-resource languages and how
they adapt their evaluations to political communication concepts with ChatGPT
providing more nuanced outputs than Bing Chat. Finally, we find that for some
veracity detection-related tasks, the performance of chatbots varied depending
on the topic of the statement or the source to which it is attributed. These
findings highlight the potential of LLM-based chatbots in tackling different
forms of false information in online environments, but also points to the
substantial variation in terms of how such potential is realized due to
specific factors, such as language of the prompt or the topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13103">Exploring Multimodal Large Language Models for Radiology Report Error-checking. (arXiv:2312.13103v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinge Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_E/0/1/0/all/0/1">Eva C. Keller</a>, <a href="http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1">Jamie Chow</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1">Adam P. Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Pontikos_N/0/1/0/all/0/1">Nikolas Pontikos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_P/0/1/0/all/0/1">Paul Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Michelle C. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghan Wu</a></p>
<p>This paper proposes one of the first clinical applications of multimodal
large language models (LLMs) as an assistant for radiologists to check errors
in their reports. We created an evaluation dataset from two real-world
radiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each.
A subset of original reports was modified to contain synthetic errors by
introducing various type of mistakes. The evaluation contained two difficulty
levels: SIMPLE for binary error-checking and COMPLEX for identifying error
types. LLaVA (Large Language and Visual Assistant) variant models, including
our instruction-tuned model, were used for the evaluation. Additionally, a
domain expert evaluation was conducted on a small test set. At the SIMPLE
level, the LLaVA v1.5 model outperformed other publicly available models.
Instruction tuning significantly enhanced performance by 47.4% and 25.4% on
MIMIC-CXR and IU-Xray data, respectively. The model also surpassed the domain
experts accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets
(N=21) of the test set where a clinician did not achieve the correct
conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases.
This study marks a promising step toward utilizing multi-modal LLMs to enhance
diagnostic accuracy in radiology. The ensemble model demonstrated comparable
performance to clinicians, even capturing errors overlooked by humans.
Nevertheless, future work is needed to improve the model ability to identify
the types of inconsistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13119">Prometheus: Infrastructure Security Posture Analysis with AI-generated Attack Graphs. (arXiv:2312.13119v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Katsis_C/0/1/0/all/0/1">Charalampos Katsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_F/0/1/0/all/0/1">Fan Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiahao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1">Elisa Bertino</a>, <a href="http://arxiv.org/find/cs/1/au:+Kompella_R/0/1/0/all/0/1">Ramana Rao Kompella</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1">Ashish Kundu</a></p>
<p>The rampant occurrence of cybersecurity breaches imposes substantial
limitations on the progress of network infrastructures, leading to compromised
data, financial losses, potential harm to individuals, and disruptions in
essential services. The current security landscape demands the urgent
development of a holistic security assessment solution that encompasses
vulnerability analysis and investigates the potential exploitation of these
vulnerabilities as attack paths. In this paper, we propose Prometheus, an
advanced system designed to provide a detailed analysis of the security posture
of computing infrastructures. Using user-provided information, such as device
details and software versions, Prometheus performs a comprehensive security
assessment. This assessment includes identifying associated vulnerabilities and
constructing potential attack graphs that adversaries can exploit. Furthermore,
Prometheus evaluates the exploitability of these attack paths and quantifies
the overall security posture through a scoring mechanism. The system takes a
holistic approach by analyzing security layers encompassing hardware, system,
network, and cryptography. Furthermore, Prometheus delves into the
interconnections between these layers, exploring how vulnerabilities in one
layer can be leveraged to exploit vulnerabilities in others. In this paper, we
present the end-to-end pipeline implemented in Prometheus, showcasing the
systematic approach adopted for conducting this thorough security analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13179">Contextual Code Switching for Machine Translation using Language Models. (arXiv:2312.13179v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaji_A/0/1/0/all/0/1">Arshad Kaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Manan Shah</a></p>
<p>Large language models (LLMs) have exerted a considerable impact on diverse
language-related tasks in recent years. Their demonstrated state-of-the-art
performance is achieved through methodologies such as zero-shot or few-shot
prompting. These models undergo training on extensive datasets that encompass
segments of the Internet and subsequently undergo fine-tuning tailored to
specific tasks. Notably, they exhibit proficiency in tasks such as translation,
summarization, question answering, and creative writing, even in the absence of
explicit training for those particular tasks. While they have shown substantial
improvement in the multilingual tasks their performance in the code switching,
especially for machine translation remains relatively uncharted. In this paper,
we present an extensive study on the code switching task specifically for the
machine translation task comparing multiple LLMs. Our results indicate that
despite the LLMs having promising results in the certain tasks, the models with
relatively lesser complexity outperform the multilingual large language models
in the machine translation task. We posit that the efficacy of multilingual
large language models in contextual code switching is constrained by their
training methodologies. In contrast, relatively smaller models, when trained
and fine-tuned on bespoke datasets, may yield superior results in comparison to
the majority of multilingual models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13193">HCDIR: End-to-end Hate Context Detection, and Intensity Reduction model for online comments. (arXiv:2312.13193v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1">Neeraj Kumar Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1">Koyel Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahapatra_J/0/1/0/all/0/1">Joy Mahapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1">Utpal Garain</a>, <a href="http://arxiv.org/find/cs/1/au:+Senapati_A/0/1/0/all/0/1">Apurbalal Senapati</a></p>
<p>Warning: This paper contains examples of the language that some people may
find offensive.
</p>
<p>Detecting and reducing hateful, abusive, offensive comments is a critical and
challenging task on social media. Moreover, few studies aim to mitigate the
intensity of hate speech. While studies have shown that context-level semantics
are crucial for detecting hateful comments, most of this research focuses on
English due to the ample datasets available. In contrast, low-resource
languages, like Indian languages, remain under-researched because of limited
datasets. Contrary to hate speech detection, hate intensity reduction remains
unexplored in high-resource and low-resource languages. In this paper, we
propose a novel end-to-end model, HCDIR, for Hate Context Detection, and Hate
Intensity Reduction in social media posts. First, we fine-tuned several
pre-trained language models to detect hateful comments to ascertain the
best-performing hateful comments detection model. Then, we identified the
contextual hateful words. Identification of such hateful words is justified
through the state-of-the-art explainable learning model, i.e., Integrated
Gradient (IG). Lastly, the Masked Language Modeling (MLM) model has been
employed to capture domain-specific nuances to reduce hate intensity. We masked
the 50\% hateful words of the comments identified as hateful and predicted the
alternative words for these masked terms to generate convincing sentences. An
optimal replacement for the original hate comments from the feasible sentences
is preferred. Extensive experiments have been conducted on several recent
datasets using automatic metric-based evaluation (BERTScore) and thorough human
evaluation. To enhance the faithfulness in human evaluation, we arranged a
group of three human annotators with varied expertise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13208">LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces. (arXiv:2312.13208v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingji Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1">Danilo S. Carvalho</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratt_Hartmann_I/0/1/0/all/0/1">Ian Pratt-Hartmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1">Andr&#xe9; Freitas</a></p>
<p>Deep generative neural networks, such as Variational AutoEncoders (VAEs),
offer an opportunity to better understand and control language models from the
perspective of sentence-level latent spaces. To combine the controllability of
VAE latent spaces with the state-of-the-art performance of recent large
language models (LLMs), we present in this work LlaMaVAE, which combines
expressive encoder and decoder models (sentenceT5 and LlaMA) with a VAE
architecture, aiming to provide better text generation control to LLMs. In
addition, to conditionally guide the VAE generation, we investigate a new
approach based on flow-based invertible neural networks (INNs) named Invertible
CVAE. Experimental results reveal that LlaMaVAE can outperform the previous
state-of-the-art VAE language model, Optimus, across various tasks, including
language modelling, semantic textual similarity and definition modelling.
Qualitative analysis on interpolation and traversal experiments also indicates
an increased degree of semantic clustering and geometric consistency, which
enables better generation control.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13211">DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization. (arXiv:2312.13211v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chand_R/0/1/0/all/0/1">Rahul Chand</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhu_Y/0/1/0/all/0/1">Yashoteja Prabhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">Pratyush Kumar</a></p>
<p>With the tremendous success of large transformer models in natural language
understanding, down-sizing them for cost-effective deployments has become
critical. Recent studies have explored the low-rank weight factorization
techniques which are efficient to train, and apply out-of-the-box to any
transformer architecture. Unfortunately, the low-rank assumption tends to be
over-restrictive and hinders the expressiveness of the compressed model. This
paper proposes, DSFormer, a simple alternative factorization scheme which
expresses a target weight matrix as the product of a small dense and a
semi-structured sparse matrix. The resulting approximation is more faithful to
the weight distribution in transformers and therefore achieves a stronger
efficiency-accuracy trade-off. Another concern with existing factorizers is
their dependence on a task-unaware initialization step which degrades the
accuracy of the resulting model. DSFormer addresses this issue through a novel
Straight-Through Factorizer (STF) algorithm that jointly learns all the weight
factorizations to directly maximize the final task accuracy. Extensive
experiments on multiple natural language understanding benchmarks demonstrate
that DSFormer obtains up to 40% better compression than the state-of-the-art
low-rank factorizers, leading semi-structured sparsity baselines and popular
knowledge distillation approaches. Our approach is also orthogonal to
mainstream compressors and offers up to 50% additional compression when added
to popular distilled, layer-shared and quantized transformers. We empirically
evaluate the benefits of STF over conventional optimization practices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13219">Interactive Visual Task Learning for Robots. (arXiv:2312.13219v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1">Weiwei Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1">Anant Sah</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1">Nakul Gopalan</a></p>
<p>We present a framework for robots to learn novel visual concepts and tasks
via in-situ linguistic interactions with human users. Previous approaches have
either used large pre-trained visual models to infer novel objects zero-shot,
or added novel concepts along with their attributes and representations to a
concept hierarchy. We extend the approaches that focus on learning visual
concept hierarchies by enabling them to learn novel concepts and solve unseen
robotics tasks with them. To enable a visual concept learner to solve robotics
tasks one-shot, we developed two distinct techniques. Firstly, we propose a
novel approach, Hi-Viscont(HIerarchical VISual CONcept learner for Task), which
augments information of a novel concept to its parent nodes within a concept
hierarchy. This information propagation allows all concepts in a hierarchy to
update as novel concepts are taught in a continual learning setting. Secondly,
we represent a visual task as a scene graph with language annotations, allowing
us to create novel permutations of a demonstrated task zero-shot in-situ. We
present two sets of results. Firstly, we compare Hi-Viscont with the baseline
model (FALCON) on visual question answering(VQA) in three domains. While being
comparable to the baseline model on leaf level concepts, Hi-Viscont achieves an
improvement of over 9% on non-leaf concepts on average. We compare our model's
performance against the baseline FALCON model. Our framework achieves 33%
improvements in success rate metric, and 19% improvements in the object level
accuracy compared to the baseline model. With both of these results we
demonstrate the ability of our model to learn tasks and concepts in a continual
learning setting on the robot.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.03327">Latency Adjustable Transformer Encoder for Language Understanding. (arXiv:2201.03327v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kachuee_S/0/1/0/all/0/1">Sajjad Kachuee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharifkhani_M/0/1/0/all/0/1">Mohammad Sharifkhani</a></p>
<p>Adjusting the latency, power, and accuracy of natural language understanding
models is a desirable objective of an efficient architecture. This paper
proposes an efficient Transformer architecture that adjusts the inference
computational cost adaptively with a desired inference latency speedup. In
fine-tuning phase, the proposed method detects less important hidden sequence
elements (word-vectors) and eliminates them in each encoder layer using a
proposed Attention Context Contribution (ACC) metric. After the fine-tuning
phase, with the novel offline-tuning property, the inference latency of the
model can be adjusted in a wide range of inference speedup selections without
any further training. The proposed method is applied to the BERT-base and GPT-2
models for evaluation. Extensive experiments show that most of the word-vectors
in higher Transformer layers have less contribution to the subsequent layers;
hence, they can be eliminated to improve the inference latency. Experimental
results on extensive sentiment analysis, classification, text generation tasks
and regression benchmarks like GLUE showed that the method is effective in
various datasets with minimal impact on global context. The proposed method
mathematically and experimentally improves the inference latency of BERT-base
and GPT-2 by up to 4.8 and 3.72 times with less than 0.75% accuracy drop and
passable perplexity on average. The suggested approach posits that in Large
Language Models (LLMs), although the complete network is necessary for
training, it can be truncated during the fine-tuning phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07207">Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across Modalities. (arXiv:2206.07207v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1">Hammad A. Ayyubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1">Christopher Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Chum_L/0/1/0/all/0/1">Lovish Chum</a>, <a href="http://arxiv.org/find/cs/1/au:+Lokesh_R/0/1/0/all/0/1">Rahul Lokesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yulei Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xudong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xuande Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1">Jaywon Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1">Sounak Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shih-Fu Chang</a></p>
<p>Events describe happenings in our world that are of importance. Naturally,
understanding events mentioned in multimedia content and how they are related
forms an important way of comprehending our world. Existing literature can
infer if events across textual and visual (video) domains are identical (via
grounding) and thus, on the same semantic level. However, grounding fails to
capture the intricate cross-event relations that exist due to the same events
being referred to on many semantic levels. For example, in Figure 1, the
abstract event of "war" manifests at a lower semantic level through subevents
"tanks firing" (in video) and airplane "shot" (in text), leading to a
hierarchical, multimodal relationship between the events.
</p>
<p>In this paper, we propose the task of extracting event hierarchies from
multimodal (video and text) data to capture how the same event manifests itself
in different modalities at different semantic levels. This reveals the
structure of events and is critical to understanding them. To support research
on this task, we introduce the Multimodal Hierarchical Events (MultiHiEve)
dataset. Unlike prior video-language datasets, MultiHiEve is composed of news
video-article pairs, which makes it rich in event hierarchies. We densely
annotate a part of the dataset to construct the test benchmark. We show the
limitations of state-of-the-art unimodal and multimodal baselines on this task.
Further, we address these limitations via a new weakly supervised model,
leveraging only unannotated video-article pairs from MultiHiEve. We perform a
thorough evaluation of our proposed method which demonstrates improved
performance on this task and highlight opportunities for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03087">Iterative Vision-and-Language Navigation. (arXiv:2210.03087v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1">Jacob Krantz</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Shurjo Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1">Jason Corso</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1">Peter Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Stefan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for
evaluating language-guided agents navigating in a persistent environment over
time. Existing Vision-and-Language Navigation (VLN) benchmarks erase the
agent's memory at the beginning of every episode, testing the ability to
perform cold-start navigation with no prior information. However, deployed
robots occupy the same environment for long periods of time. The IVLN paradigm
addresses this disparity by training and evaluating VLN agents that maintain
memory across tours of scenes that consist of up to 100 ordered
instruction-following Room-to-Room (R2R) episodes, each defined by an
individual language instruction and a target path. We present discrete and
continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours
each in 80 indoor scenes. We find that extending the implicit memory of
high-performing transformer VLN agents is not sufficient for IVLN, but agents
that build maps can benefit from environment persistence, motivating a renewed
focus on map-building agents in VLN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01039">SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition. (arXiv:2212.01039v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1">Yichong Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaitao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang-Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1">Edward Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a></p>
<p>Error correction in automatic speech recognition (ASR) aims to correct those
incorrect words in sentences generated by ASR models. Since recent ASR models
usually have low word error rate (WER), to avoid affecting originally correct
tokens, error correction models should only modify incorrect words, and
therefore detecting incorrect words is important for error correction. Previous
works on error correction either implicitly detect error words through
target-source attention or CTC (connectionist temporal classification) loss, or
explicitly locate specific deletion/substitution/insertion errors. However,
implicit error detection does not provide clear signal about which tokens are
incorrect and explicit error detection suffers from low detection accuracy. In
this paper, we propose SoftCorrect with a soft error detection mechanism to
avoid the limitations of both explicit and implicit error detection.
Specifically, we first detect whether a token is correct or not through a
probability produced by a dedicatedly designed language model, and then design
a constrained CTC loss that only duplicates the detected incorrect tokens to
let the decoder focus on the correction of error tokens. Compared with implicit
error detection with CTC loss, SoftCorrect provides explicit signal about which
words are incorrect and thus does not need to duplicate every token but only
incorrect tokens; compared with explicit error detection, SoftCorrect does not
detect specific deletion/substitution/insertion errors but just leaves it to
CTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that
SoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming
previous works by a large margin, while still enjoying fast speed of parallel
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05221">SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading. (arXiv:2303.05221v4 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Rabe_M/0/1/0/all/0/1">Maximilian M. Rabe</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Paape_D/0/1/0/all/0/1">Dario Paape</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mertzen_D/0/1/0/all/0/1">Daniela Mertzen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Vasishth_S/0/1/0/all/0/1">Shravan Vasishth</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Engbert_R/0/1/0/all/0/1">Ralf Engbert</a></p>
<p>Models of eye-movement control during reading, developed largely within
psychology, usually focus on visual, attentional, lexical, and motor processes
but neglect post-lexical language processing; by contrast, models of sentence
comprehension processes, developed largely within psycholinguistics, generally
focus only on post-lexical language processes. We present a model that combines
these two research threads, by integrating eye-movement control and sentence
processing. Developing such an integrated model is extremely challenging and
computationally demanding, but such an integration is an important step toward
complete mathematical models of natural language comprehension in reading. We
combine the SWIFT model of eye-movement control (Seelig et al., 2020,
doi:10.1016/j.jmp.<a href="/abs/2019.10231">2019.10231</a>3) with key components of the Lewis and Vasishth
sentence processing model (Lewis &amp; Vasishth, 2005,
doi:10.1207/s15516709cog0000_25). This integration becomes possible, for the
first time, due in part to recent advances in successful parameter
identification in dynamical models, which allows us to investigate profile
log-likelihoods for individual model parameters. We present a fully implemented
proof-of-concept model demonstrating how such an integrated model can be
achieved; our approach includes Bayesian model inference with Markov Chain
Monte Carlo (MCMC) sampling as a key computational tool. The integrated
Sentence-Processing and Eye-Movement Activation-Coupled Model (SEAM) can
successfully reproduce eye movement patterns that arise due to similarity-based
interference in reading. To our knowledge, this is the first-ever integration
of a complete process model of eye-movement control with linguistic dependency
completion processes in sentence comprehension. In future work, this proof of
concept model will need to be evaluated using a comprehensive set of benchmark
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06854">Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks. (arXiv:2303.06854v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenhan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jingdong Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1">Baharan Mirzasoleiman</a></p>
<p>Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of targeted data poisoning and backdoor attacks.
Despite this vulnerability, robust contrastive vision-language pre-training
against such attacks has remained unaddressed. In this work, we propose ROCLIP,
the first effective method for robust pre-training multimodal vision-language
models against targeted data poisoning and backdoor attacks. ROCLIP effectively
breaks the association between poisoned image-caption pairs by considering a
relatively large and varying pool of random captions, and matching every image
with the text that is most similar to it in the pool instead of its own
caption, every few epochs.It also leverages image and text augmentations to
further strengthen the defense and improve the performance of the model. Our
extensive experiments show that ROCLIP renders state-of-the-art targeted data
poisoning and backdoor attacks ineffective during pre-training CLIP models. In
particular, ROCLIP decreases the success rate for targeted data poisoning
attacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while
improving the model's linear probe performance by 10% and maintains a similar
zero shot performance compared to CLIP. By increasing the frequency of
matching, ROCLIP is able to defend strong attacks, which add up to 1% poisoned
examples to the data, and successfully maintain a low attack success rate of
12.5%, while trading off the performance on some tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15413">Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation. (arXiv:2303.15413v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Susung Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1">Donghoon Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungryong Kim</a></p>
<p>Existing score-distilling text-to-3D generation techniques, despite their
considerable promise, often encounter the view inconsistency problem. One of
the most notable issues is the Janus problem, where the most canonical view of
an object (\textit{e.g}., face or head) appears in other views. In this work,
we explore existing frameworks for score-distilling text-to-3D generation and
identify the main causes of the view inconsistency problem -- the embedded bias
of 2D diffusion models. Based on these findings, we propose two approaches to
debias the score-distillation frameworks for view-consistent text-to-3D
generation. Our first approach, called score debiasing, involves cutting off
the score estimated by 2D diffusion models and gradually increasing the
truncation value throughout the optimization process. Our second approach,
called prompt debiasing, identifies conflicting words between user prompts and
view prompts using a language model, and adjusts the discrepancy between view
prompts and the viewing direction of an object. Our experimental results show
that our methods improve the realism of the generated 3D objects by
significantly reducing artifacts and achieve a good trade-off between
faithfulness to the 2D diffusion models and 3D consistency with little
overhead. Our project page is available
at~\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01246">Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Khastgir_S/0/1/0/all/0/1">Siddartha Khastgir</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a></p>
<p>Can safety analysis make use of Large Language Models (LLMs)? A case study
explores Systems Theoretic Process Analysis (STPA) applied to Automatic
Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems
using ChatGPT. We investigate how collaboration schemes, input semantic
complexity, and prompt guidelines influence STPA results. Comparative results
show that using ChatGPT without human intervention may be inadequate due to
reliability related issues, but with careful design, it may outperform human
experts. No statistically significant differences are found when varying the
input semantic complexity or using common prompt guidelines, which suggests the
necessity for developing domain-specific prompt engineering. We also highlight
future challenges, including concerns about LLM trustworthiness and the
necessity for standardisation and regulation in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03898">The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiqiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1">Qiqiang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Mengmeng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1">Hanjie Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shaohua Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangzheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yanlong Du</a></p>
<p>In recent years, short Text Matching tasks have been widely applied in the
fields ofadvertising search and recommendation. The difficulty lies in the lack
of semantic information and word ambiguity caused by the short length of the
text. Previous works have introduced complement sentences or knowledge bases to
provide additional feature information. However, these methods have not fully
interacted between the original sentence and the complement sentence, and have
not considered the noise issue that may arise from the introduction of external
knowledge bases. Therefore, this paper proposes a short Text Matching model
that combines contrastive learning and external knowledge. The model uses a
generative model to generate corresponding complement sentences and uses the
contrastive learning method to guide the model to obtain more semantically
meaningful encoding of the original sentence. In addition, to avoid noise, we
use keywords as the main semantics of the original sentence to retrieve
corresponding knowledge words in the knowledge base, and construct a knowledge
graph. The graph encoding model is used to integrate the knowledge base
information into the model. Our designed model achieves state-of-the-art
performance on two publicly available Chinese Text Matching datasets,
demonstrating the effectiveness of our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11662">Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses. (arXiv:2305.11662v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1">Xenia Ohmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1">Elia Bruni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1">Dieuwke Hupkes</a></p>
<p>At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilinguality. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15685">RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1">Lei Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Liangchen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoskere_J/0/1/0/all/0/1">Jayakumar Hoskere</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yinxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Simon Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jindong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1">Lei Meng</a></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities in
creative tasks such as storytelling and E-mail generation. However, as LLMs are
primarily trained on final text results rather than intermediate revisions, it
might be challenging for them to perform text rewriting tasks. Most studies in
the rewriting tasks focus on a particular transformation type within the
boundaries of single sentences. In this work, we develop new strategies for
instruction tuning and reinforcement learning to better align LLMs for
cross-sentence rewriting tasks using diverse wording and structures expressed
through natural languages including 1) generating rewriting instruction data
from Wiki edits and public corpus through instruction generation and
chain-of-thought prompting; 2) collecting comparison data for reward model
training through a new ranking function. To facilitate this research, we
introduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting
types expressed through natural language instructions. Our results show
significant improvements over a variety of baselines. The public repository is
available on GitHub under Google Research
(https://github.com/google-research/google-research/tree/master/rewritelm).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16307">IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages. (arXiv:2305.16307v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gala_J/0/1/0/all/0/1">Jay Gala</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitale_P/0/1/0/all/0/1">Pranjal A. Chitale</a>, <a href="http://arxiv.org/find/cs/1/au:+AK_R/0/1/0/all/0/1">Raghavan AK</a>, <a href="http://arxiv.org/find/cs/1/au:+Gumma_V/0/1/0/all/0/1">Varun Gumma</a>, <a href="http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1">Sumanth Doddapaneni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Aswanth Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawale_J/0/1/0/all/0/1">Janki Nawale</a>, <a href="http://arxiv.org/find/cs/1/au:+Sujatha_A/0/1/0/all/0/1">Anupama Sujatha</a>, <a href="http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1">Ratish Puduppully</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1">Vivek Raghavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">Pratyush Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1">Mitesh M. Khapra</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1">Raj Dabre</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1">Anoop Kunchukuttan</a></p>
<p>India has a rich linguistic landscape with languages from 4 major language
families spoken by over a billion people. 22 of these languages are listed in
the Constitution of India (referred to as scheduled languages) are the focus of
this work. Given the linguistic diversity, high-quality and accessible Machine
Translation (MT) systems are essential in a country like India. Prior to this
work, there was (i) no parallel training data spanning all 22 languages, (ii)
no robust benchmarks covering all these languages and containing content
relevant to India, and (iii) no existing translation models which support all
the 22 scheduled languages of India. In this work, we aim to address this gap
by focusing on the missing pieces required for enabling wide, easy, and open
access to good machine translation systems for all 22 scheduled Indian
languages. We identify four key areas of improvement: curating and creating
larger training datasets, creating diverse and high-quality benchmarks,
training multilingual models, and releasing models with open access. Our first
contribution is the release of the Bharat Parallel Corpus Collection (BPCC),
the largest publicly available parallel corpora for Indic languages. BPCC
contains a total of 230M bitext pairs, of which a total of 126M were newly
added, including 644K manually translated sentence pairs created as part of
this work. Our second contribution is the release of the first n-way parallel
benchmark covering all 22 Indian languages, featuring diverse domains,
Indian-origin content, and source-original test sets. Next, we present
IndicTrans2, the first model to support all 22 languages, surpassing existing
models on multiple existing and new benchmarks created as a part of this work.
Lastly, to promote accessibility and collaboration, we release our models and
associated data with permissive licenses at
https://github.com/AI4Bharat/IndicTrans2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11698">DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weixin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hengzhi Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chulin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Mintong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenhui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chejian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zidi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_R/0/1/0/all/0/1">Ritik Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1">Rylan Schaeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1">Sang T. Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1">Simran Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1">Mantas Mazeika</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1">Dan Hendrycks</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zinan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1">Sanmi Koyejo</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12976">Evaluating the Ripple Effects of Knowledge Editing in Language Models. (arXiv:2307.12976v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1">Roi Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Biran_E/0/1/0/all/0/1">Eden Biran</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1">Ori Yoran</a>, <a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1">Amir Globerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a></p>
<p>Modern language models capture a large body of factual knowledge. However,
some facts can be incorrectly induced or become obsolete over time, resulting
in factually incorrect generations. This has led to the development of various
editing methods that allow updating facts encoded by the model. Evaluation of
these methods has primarily focused on testing whether an individual fact has
been successfully injected, and if similar predictions for other subjects have
not changed. Here we argue that such evaluation is limited, since injecting one
fact (e.g. ``Jack Depp is the son of Johnny Depp'') introduces a ``ripple
effect'' in the form of additional facts that the model needs to update
(e.g.``Jack Depp is the sibling of Lily-Rose Depp''). To address this issue, we
propose a novel set of evaluation criteria that consider the implications of an
edit on related facts. Using these criteria, we then construct RippleEdits, a
diagnostic benchmark of 5K factual edits, capturing a variety of types of
ripple effects. We evaluate prominent editing methods on RippleEdits, showing
that current methods fail to introduce consistent changes in the model's
knowledge. In addition, we find that a simple in-context editing baseline
obtains the best scores on our benchmark, suggesting a promising research
direction for model editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08742">PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shasha Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shezheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jie Yu</a></p>
<p>Model editing techniques modify a minor proportion of knowledge in Large
Language Models (LLMs) at a relatively low cost, which have demonstrated
notable success. Existing methods assume Transformer Layer (TL) hidden states
are values of key-value memories of the Feed-Forward Network (FFN). They
usually optimize the TL hidden states to memorize target knowledge and use it
to update the weights of the FFN in LLMs. However, the information flow of TL
hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,
and residual connections. Existing methods neglect the fact that the TL hidden
states contains information not specifically required for FFN. Consequently,
the performance of model editing decreases. To achieve more precise model
editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes
certain general knowledge extraction patterns. This implies that MHSA weights
do not require updating when new knowledge is introduced. Based on above
findings, we introduce PMET, which simultaneously optimizes Transformer
Component (TC, namely MHSA and FFN) hidden states, while only using the
optimized TC hidden states of FFN to precisely update FFN weights. Our
experiments demonstrate that PMET exhibits state-of-the-art performance on both
the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the
effectiveness of our enhancements, further reinforcing the finding that the
MHSA encodes certain general knowledge extraction patterns and indicating its
storage of a small amount of factual knowledge. Our code is available at
https://github.com/xpq-tech/PMET.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09156">Characterizing Information Seeking Events in Health-Related Social Discourse. (arXiv:2308.09156v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharif_O/0/1/0/all/0/1">Omar Sharif</a>, <a href="http://arxiv.org/find/cs/1/au:+Basak_M/0/1/0/all/0/1">Madhusudan Basak</a>, <a href="http://arxiv.org/find/cs/1/au:+Parvin_T/0/1/0/all/0/1">Tanzia Parvin</a>, <a href="http://arxiv.org/find/cs/1/au:+Scharfstein_A/0/1/0/all/0/1">Ava Scharfstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradham_A/0/1/0/all/0/1">Alphonso Bradham</a>, <a href="http://arxiv.org/find/cs/1/au:+Borodovsky_J/0/1/0/all/0/1">Jacob T. Borodovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Lord_S/0/1/0/all/0/1">Sarah E. Lord</a>, <a href="http://arxiv.org/find/cs/1/au:+Preum_S/0/1/0/all/0/1">Sarah M. Preum</a></p>
<p>Social media sites have become a popular platform for individuals to seek and
share health information. Despite the progress in natural language processing
for social media mining, a gap remains in analyzing health-related texts on
social discourse in the context of events. Event-driven analysis can offer
insights into different facets of healthcare at an individual and collective
level, including treatment options, misconceptions, knowledge gaps, etc. This
paper presents a paradigm to characterize health-related information-seeking in
social discourse through the lens of events. Events here are board categories
defined with domain experts that capture the trajectory of the
treatment/medication. To illustrate the value of this approach, we analyze
Reddit posts regarding medications for Opioid Use Disorder (OUD), a critical
global health concern. To the best of our knowledge, this is the first attempt
to define event categories for characterizing information-seeking in OUD social
discourse. Guided by domain experts, we develop TREAT-ISE, a novel multilabel
treatment information-seeking event dataset to analyze online discourse on an
event-based framework. This dataset contains Reddit posts on
information-seeking events related to recovery from OUD, where each post is
annotated based on the type of events. We also establish a strong performance
benchmark (77.4% F1 score) for the task by employing several machine learning
and deep learning classifiers. Finally, we thoroughly investigate the
performance and errors of ChatGPT on this task, providing valuable insights
into the LLM's capabilities and ongoing characterization efforts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13198">Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons. (arXiv:2308.13198v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1">Pengfei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yubo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a></p>
<p>Pre-trained language models (PLMs) contain vast amounts of factual knowledge,
but how the knowledge is stored in the parameters remains unclear. This paper
delves into the complex task of understanding how factual knowledge is stored
in multilingual PLMs, and introduces the Architecture-adapted Multilingual
Integrated Gradients method, which successfully localizes knowledge neurons
more precisely compared to current methods, and is more universal across
various architectures and languages. Moreover, we conduct an in-depth
exploration of knowledge neurons, leading to the following two important
discoveries: (1) The discovery of Language-Independent Knowledge Neurons, which
store factual knowledge in a form that transcends language. We design
cross-lingual knowledge editing experiments, demonstrating that the PLMs can
accomplish this task based on language-independent neurons; (2) The discovery
of Degenerate Knowledge Neurons, a novel type of neuron showing that different
knowledge neurons can store the same fact. Its property of functional overlap
endows the PLMs with a robust mastery of factual knowledge. We design
fact-checking experiments, proving that the degenerate knowledge neurons can
help the PLMs to detect wrong facts. Experiments corroborate these findings,
shedding light on the mechanisms of factual knowledge storage in multilingual
PLMs, and contribute valuable insights to the field. The code is available at
https://github.com/heng840/AMIG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01431">Benchmarking Large Language Models in Retrieval-Augmented Generation. (arXiv:2309.01431v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hongyu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xianpei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Le Sun</a></p>
<p>Retrieval-Augmented Generation (RAG) is a promising approach for mitigating
the hallucination of large language models (LLMs). However, existing research
lacks rigorous evaluation of the impact of retrieval-augmented generation on
different large language models, which make it challenging to identify the
potential bottlenecks in the capabilities of RAG for different LLMs. In this
paper, we systematically investigate the impact of Retrieval-Augmented
Generation on large language models. We analyze the performance of different
large language models in 4 fundamental abilities required for RAG, including
noise robustness, negative rejection, information integration, and
counterfactual robustness. To this end, we establish Retrieval-Augmented
Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and
Chinese. RGB divides the instances within the benchmark into 4 separate
testbeds based on the aforementioned fundamental abilities required to resolve
the case. Then we evaluate 6 representative LLMs on RGB to diagnose the
challenges of current LLMs when applying RAG. Evaluation reveals that while
LLMs exhibit a certain degree of noise robustness, they still struggle
significantly in terms of negative rejection, information integration, and
dealing with false information. The aforementioned assessment outcomes indicate
that there is still a considerable journey ahead to effectively apply RAG to
LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17255">Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaoyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hastings_J/0/1/0/all/0/1">Janna Hastings</a>, <a href="http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1">Ernesto Jim&#xe9;nez-Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1">Vanessa L&#xf3;pez</a>, <a href="http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1">Pierre Monnin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pesquita_C/0/1/0/all/0/1">Catia Pesquita</a>, <a href="http://arxiv.org/find/cs/1/au:+Skoda_P/0/1/0/all/0/1">Petr &#x160;koda</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamma_V/0/1/0/all/0/1">Valentina Tamma</a></p>
<p>The term life sciences refers to the disciplines that study living organisms
and life processes, and include chemistry, biology, medicine, and a range of
other related disciplines. Research efforts in life sciences are heavily
data-driven, as they produce and consume vast amounts of scientific data, much
of which is intrinsically relational and graph-structured.
</p>
<p>The volume of data and the complexity of scientific concepts and relations
referred to therein promote the application of advanced knowledge-driven
technologies for managing and interpreting data, with the ultimate aim to
advance scientific discovery.
</p>
<p>In this survey and position paper, we discuss recent developments and
advances in the use of graph-based technologies in life sciences and set out a
vision for how these technologies will impact these fields into the future. We
focus on three broad topics: the construction and management of Knowledge
Graphs (KGs), the use of KGs and associated technologies in the discovery of
new knowledge, and the use of KGs in artificial intelligence applications to
support explanations (explainable AI). We select a few exemplary use cases for
each topic, discuss the challenges and open research questions within these
topics, and conclude with a perspective and outlook that summarizes the
overarching challenges and their potential solutions as a guide for future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03560">Redefining Digital Health Interfaces with Large Language Models. (arXiv:2310.03560v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Imrie_F/0/1/0/all/0/1">Fergus Imrie</a>, <a href="http://arxiv.org/find/cs/1/au:+Rauba_P/0/1/0/all/0/1">Paulius Rauba</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their adoption remains comparatively limited
due, in part, to challenges surrounding usability and trust. Recently, Large
Language Models (LLMs) have emerged as general-purpose models with the ability
to process complex information and produce human-quality text, presenting a
wealth of potential applications in healthcare. Directly applying LLMs in
clinical settings is not straightforward, with LLMs susceptible to providing
inconsistent or nonsensical answers. We describe how LLM-based systems can
utilize external tools to provide a novel interface between clinicians and
digital technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLM
in clinical settings such as hallucinations. We illustrate LLM-based interfaces
with examples from cardiovascular disease and diabetes risk prediction,
highlighting the benefit compared to traditional interfaces for digital tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14747">MCC-KD: Multi-CoT Consistent Knowledge Distillation. (arXiv:2310.14747v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongzhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1">Xiaojun Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1">Ming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Ji Zhang</a></p>
<p>Large language models (LLMs) have showcased remarkable capabilities in
complex reasoning through chain of thought (CoT) prompting. Recently, there has
been a growing interest in transferring these reasoning abilities from LLMs to
smaller models. However, achieving both the diversity and consistency in
rationales presents a challenge. In this paper, we focus on enhancing these two
aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to
efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple
rationales for each question and enforce consistency among the corresponding
predictions by minimizing the bidirectional KL-divergence between the answer
distributions. We investigate the effectiveness of MCC-KD with different model
architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both
mathematical reasoning and commonsense reasoning benchmarks. The empirical
results not only confirm MCC-KD's superior performance on in-distribution
datasets but also highlight its robust generalization ability on
out-of-distribution datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15494">TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haofei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cunxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1">Wei Bi</a></p>
<p>The Transformer architecture is crucial for numerous AI models, but it still
faces challenges in long-range language modeling. Though several specific
transformer architectures have been designed to tackle issues of long-range
dependencies, existing methods like Transformer-XL are plagued by a high
percentage of ineffective memories. In this study, we present a plug-and-play
strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens
participating in attention calculation based on one simple metric. This
strategy allows us to keep tokens that are likely to have a high attention
score with the current queries and ignore the other ones. We have tested our
approach on the word-level benchmark (WikiText-103) and the character-level
benchmark (enwik8), and the results indicate an improvement without having
additional training or adding additional parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08206">Human-Centric Autonomous Systems With LLMs for User Command Reasoning. (arXiv:2311.08206v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Ci Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Marta_D/0/1/0/all/0/1">Daniel Sim&#xf5;es Marta</a>, <a href="http://arxiv.org/find/cs/1/au:+Batool_N/0/1/0/all/0/1">Nazre Batool</a>, <a href="http://arxiv.org/find/cs/1/au:+Folkesson_J/0/1/0/all/0/1">John Folkesson</a></p>
<p>The evolution of autonomous driving has made remarkable advancements in
recent years, evolving into a tangible reality. However, a human-centric
large-scale adoption hinges on meeting a variety of multifaceted requirements.
To ensure that the autonomous system meets the user's intent, it is essential
to accurately discern and interpret user commands, especially in complex or
emergency situations. To this end, we propose to leverage the reasoning
capabilities of Large Language Models (LLMs) to infer system requirements from
in-cabin users' commands. Through a series of experiments that include
different LLM models and prompt designs, we explore the few-shot multivariate
binary classification accuracy of system requirements from natural language
textual commands. We confirm the general ability of LLMs to understand and
reason about prompts but underline that their effectiveness is conditioned on
the quality of both the LLM model and the design of appropriate sequential
prompts. Code and models are public with the link
\url{https://github.com/KTH-RPL/DriveCmd_LLM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12420">How Far Have We Gone in Vulnerability Detection Using Large Language Models. (arXiv:2311.12420v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zeyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>As software becomes increasingly complex and prone to vulnerabilities,
automated vulnerability detection is critically important, yet challenging.
Given the significant successes of large language models (LLMs) in various
tasks, there is growing anticipation of their efficacy in vulnerability
detection. However, a quantitative understanding of their potential in
vulnerability detection is still missing. To bridge this gap, we introduce a
comprehensive vulnerability benchmark VulBench. This benchmark aggregates
high-quality data from a wide range of CTF (Capture-the-Flag) challenges and
real-world applications, with annotations for each vulnerable function
detailing the vulnerability type and its root cause. Through our experiments
encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models
and static analyzers, we find that several LLMs outperform traditional deep
learning approaches in vulnerability detection, revealing an untapped potential
in LLMs. This work contributes to the understanding and utilization of LLMs for
enhanced software security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03719">Assessing AI Chatbots Performance in Comprehensive Standardized Test Preparation; A Case Study with GRE. (arXiv:2312.03719v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abu_Haifa_M/0/1/0/all/0/1">Mohammad Abu-Haifa</a>, <a href="http://arxiv.org/find/cs/1/au:+Etawi_B/0/1/0/all/0/1">Bara&#x27;a Etawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkhatatbeh_H/0/1/0/all/0/1">Huthaifa Alkhatatbeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ababneh_A/0/1/0/all/0/1">Ayman Ababneh</a></p>
<p>This research paper presents a comprehensive evaluation of the performance of
three artificial 10 intelligence chatbots: Bing, ChatGPT, and GPT-4, in
addressing standardized test questions. Graduate record examination, known as
GRE, serves as a case study in this paper, encompassing both quantitative
reasoning and verbal skills. A total of 137 quantitative reasoning questions,
featuring diverse styles and 157 verbal questions categorized into varying
levels of difficulty (easy, medium, and hard) were administered to assess the
chatbots' capabilities. This paper provides a detailed examination of the
results and their implications for the utilization of artificial intelligence
in standardized test preparation by presenting the performance of each chatbot
across various skills and styles tested in the exam. Additionally, this paper
explores the proficiency of artificial intelligence in addressing image-based
questions and illustrates the uncertainty level of each chatbot. The results
reveal varying degrees of success across the chatbots, demonstrating the
influence of model sophistication and training data. GPT-4 emerged as the most
proficient, especially in complex language understanding tasks, highlighting
the evolution of artificial intelligence in language comprehension and its
ability to pass the exam with a high score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06022">Exploiting Representation Bias for Data Distillation in Abstractive Text Summarization. (arXiv:2312.06022v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1">Yash Kumar Atri</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1">Vikram Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Tanmoy Chakraborty</a></p>
<p>Abstractive text summarization is surging with the number of training samples
to cater to the needs of the deep learning models. These models tend to exploit
the training data representations to attain superior performance by improving
the quantitative element of the resultant summary. However, increasing the size
of the training set may not always be the ideal solution to maximize the
performance, and therefore, a need to revisit the quality of training samples
and the learning protocol of deep learning models is a must. In this paper, we
aim to discretize the vector space of the abstractive text summarization models
to understand the characteristics learned between the input embedding space and
the models' encoder space. We show that deep models fail to capture the
diversity of the input space. Further, the distribution of data points on the
encoder space indicates that an unchecked increase in the training samples does
not add value; rather, a tear-down of data samples is highly needed to make the
models focus on variability and faithfulness. We employ clustering techniques
to learn the diversity of a model's sample space and how data points are mapped
from the embedding space to the encoder space and vice versa. Further, we
devise a metric to filter out redundant data points to make the model more
robust and less data hungry. We benchmark our proposed method using
quantitative metrics, such as Rouge, and qualitative metrics, such as
BERTScore, FEQA and Pyramid score. We also quantify the reasons that inhibit
the models from learning the diversity from the varied input samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09085">The Earth is Flat because...: Investigating LLMs&#x27; Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rongwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Brian S. Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shujian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weiyan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhixuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Han Qiu</a></p>
<p>Large Language Models (LLMs) encapsulate vast amounts of knowledge but still
remain vulnerable to external misinformation. Existing research mainly studied
this susceptibility behavior in a single-turn setting. However, belief can
change during a multi-turn conversation, especially a persuasive one.
Therefore, in this study, we delve into LLMs' susceptibility to persuasive
conversations, particularly on factual questions that they can answer
correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which
contains factual questions paired with systematically generated persuasive
misinformation. Then, we develop a testing framework to track LLMs' belief
changes in a persuasive dialogue. Through extensive experiments, we find that
LLMs' correct beliefs on factual knowledge can be easily manipulated by various
persuasive strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11193">&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yijiong Yu</a></p>
<p>Although LLMs continue to iterate and improve, most open-source models still
have a context window of no more than 4k, limiting their ability to handle
long-context problems. Most existing open-source models for long-context chat
still lack satisfactory accuracy. To address this issue, I approach it from the
perspective of training data and theoretically prove that training the
capability to handle long contexts requires "effective" rather than "long"
data. Based on this, I propose using the "original text paraphrase" task, and
successfully extend the context window of the existing model to 32k by a
low-cost and effective method, achieving extremely high accuracy in
multi-document-QA and surpassing all existing open-source models of the same
scale. The model and training data have been open-sourced on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11276">Compositional Generalization for Multi-label Text Classification: A Data-Augmentation Approach. (arXiv:2312.11276v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1">Yuyang Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiahui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1">Donghong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1">Chong Teng</a></p>
<p>Despite significant advancements in multi-label text classification, the
ability of existing models to generalize to novel and seldom-encountered
complex concepts, which are compositions of elementary ones, remains
underexplored. This research addresses this gap. By creating unique data splits
across three benchmarks, we assess the compositional generalization ability of
existing multi-label text classification models. Our results show that these
models often fail to generalize to compositional concepts encountered
infrequently during training, leading to inferior performance on tests with
these new combinations. To address this, we introduce a data augmentation
method that leverages two innovative text generation models designed to enhance
the classification models' capacity for compositional generalization. Our
experiments show that this data augmentation approach significantly improves
the compositional generalization capabilities of classification models on our
benchmarks, with both generation models surpassing other text generation
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11517">Unlocking Musculoskeletal Disorder Risk Factors: NLP-Based Classification and Mode-Based Ranking. (arXiv:2312.11517v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1">Md Abrar Jahin</a>, <a href="http://arxiv.org/find/cs/1/au:+Talapatra_S/0/1/0/all/0/1">Subrata Talapatra</a></p>
<p>This research delves into the intricate landscape of Musculoskeletal Disorder
(MSD) risk factors, employing a novel fusion of Natural Language Processing
(NLP) techniques and mode-based ranking methodologies. The primary objective is
to advance the comprehension of MSD risk factors, their classification, and
their relative severity, facilitating more targeted preventive and management
interventions. The study utilizes eight diverse models, integrating pre-trained
transformers, cosine similarity, and various distance metrics to classify risk
factors into personal, biomechanical, workplace, psychological, and
organizational classes. Key findings reveal that the BERT model with cosine
similarity attains an overall accuracy of 28%, while the sentence transformer,
coupled with Euclidean, Bray-Curtis, and Minkowski distances, achieves a
flawless accuracy score of 100%. In tandem with the classification efforts, the
research employs a mode-based ranking approach on survey data to discern the
severity hierarchy of MSD risk factors. Intriguingly, the rankings align
precisely with the previous literature, reaffirming the consistency and
reliability of the approach. ``Working posture" emerges as the most severe risk
factor, emphasizing the critical role of proper posture in preventing MSDs. The
collective perceptions of survey participants underscore the significance of
factors like "Job insecurity," "Effort reward imbalance," and "Poor employee
facility" in contributing to MSD risks. The convergence of rankings provides
actionable insights for organizations aiming to reduce the prevalence of MSDs.
The study concludes with implications for targeted interventions,
recommendations for improving workplace conditions, and avenues for future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11562">A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook. (arXiv:2312.11562v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1">Ruihang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1">Mengzhe Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a></p>
<p>Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11681">Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows. (arXiv:2312.11681v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grunde_McLaughlin_M/0/1/0/all/0/1">Madeleine Grunde-McLaughlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1">Michelle S. Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1">Daniel S. Weld</a>, <a href="http://arxiv.org/find/cs/1/au:+Heer_J/0/1/0/all/0/1">Jeffrey Heer</a></p>
<p>LLM chains enable complex tasks by decomposing work into a sequence of
sub-tasks. Crowdsourcing workflows similarly decompose complex tasks into
smaller tasks for human crowdworkers. Chains address LLM errors analogously to
the way crowdsourcing workflows address human error. To characterize
opportunities for LLM chaining, we survey 107 papers across the crowdsourcing
and chaining literature to construct a design space for chain development. The
design space connects an LLM designer's objectives to strategies they can use
to achieve those objectives, and tactics to implement each strategy. To explore
how techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing
workflows to implement LLM chains across three case studies: creating a
taxonomy, shortening text, and writing a short story. From the design space and
our case studies, we identify which techniques transfer from crowdsourcing to
LLM chaining and raise implications for future research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11985">Climate Change from Large Language Models. (arXiv:2312.11985v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongyin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1">Prayag Tiwari</a></p>
<p>Climate change presents significant challenges to the global community, and
it is imperative to raise widespread awareness of the climate crisis and
educate users about low-carbon living. Artificial intelligence, particularly
large language models (LLMs), have emerged as powerful tools in mitigating the
climate crisis, leveraging their extensive knowledge, broad user base, and
natural language interaction capabilities. However, despite the growing body of
research on climate change, there is a lack of comprehensive assessments of
climate crisis knowledge within LLMs. This paper aims to resolve this gap by
proposing an automatic evaluation framework. We employ a hybrid approach to
data acquisition that combines data synthesis and manual collection to compile
a diverse set of questions related to the climate crisis. These questions cover
various aspects of climate change, including its causes, impacts, mitigation
strategies, and adaptation measures. We then evaluate the model knowledge
through prompt engineering based on the collected questions and generated
answers. We propose a set of comprehensive metrics to evaluate the climate
crisis knowledge, incorporating indicators from 10 different perspectives.
Experimental results show that our method is effective in evaluating the
knowledge of LLMs regarding the climate crisis. We evaluate several
state-of-the-art LLMs and find that their knowledge falls short in terms of
timeliness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12037">Founder-GPT: Self-play to evaluate the Founder-Idea fit. (arXiv:2312.12037v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1">Sichao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ihlamur_Y/0/1/0/all/0/1">Yigit Ihlamur</a></p>
<p>This research introduces an innovative evaluation method for the
"founder-idea" fit in early-stage startups, utilizing advanced large language
model techniques to assess founders' profiles against their startup ideas to
enhance decision-making. Embeddings, self-play, tree-of-thought, and
critique-based refinement techniques show early promising results that each
idea's success patterns are unique and they should be evaluated based on the
context of the founder's background.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12430">Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP. (arXiv:2312.12430v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_H/0/1/0/all/0/1">Heyi Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1">Daqian Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jize Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yuxiang Wei</a></p>
<p>We introduce Efficient Title Reranker via Broadcasting Query Encoder, a novel
title reranking technique to achieve efficient title reranking 20x-40x faster
than vanilla passage reranker. However, one of the challenges with the training
of Efficient Title Reranker is the instability. Analyzing the issue, we found
some very difficult ground truths might act as noisy labels causing accuracy to
drop as well as some extreme values in model probability output causing nan. To
address these issues, we introduce the Sigmoid Trick, a novel technique that
reduces the gradient update of both cases resulting in better retrieval
efficacy. Experiments showed the effectiveness of ETR and sigmoid trick as we
achieved four state-of-the-art positions on the kilt knowledge benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12436">A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise. (arXiv:2312.12436v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chaoyou Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yubo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengye Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Longtian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1">Gaoxiang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yunhang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengdan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Peixian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sirui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shaohui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Deqiang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Di Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xing Sun</a></p>
<p>The surge of interest towards Multi-modal Large Language Models (MLLMs),
e.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both
academia and industry. They endow Large Language Models (LLMs) with powerful
capabilities in visual understanding, enabling them to tackle diverse
multi-modal tasks. Very recently, Google released Gemini, its newest and most
capable MLLM built from the ground up for multi-modality. In light of the
superior reasoning capabilities, can Gemini challenge GPT-4V's leading position
in multi-modal learning? In this paper, we present a preliminary exploration of
Gemini Pro's visual understanding proficiency, which comprehensively covers
four domains: fundamental perception, advanced cognition, challenging vision
tasks, and various expert capacities. We compare Gemini Pro with the
state-of-the-art GPT-4V to evaluate its upper limits, along with the latest
open-sourced MLLM, Sphinx, which reveals the gap between manual efforts and
black-box systems. The qualitative samples indicate that, while GPT-4V and
Gemini showcase different answering styles and preferences, they can exhibit
comparable visual reasoning capabilities, and Sphinx still trails behind them
concerning domain generalizability. Specifically, GPT-4V tends to elaborate
detailed explanations and intermediate steps, and Gemini prefers to output a
direct and concise answer. The quantitative evaluation on the popular MME
benchmark also demonstrates the potential of Gemini to be a strong challenger
to GPT-4V. Our early investigation of Gemini also observes some common issues
of MLLMs, indicating that there still remains a considerable distance towards
artificial general intelligence. Our project for tracking the progress of MLLM
is released at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.
</p>
</p>
</div>

    </div>
    </body>
    