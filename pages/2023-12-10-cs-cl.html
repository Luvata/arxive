<!DOCTYPE html>
<html>
<head>
<title>2023-12-10-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.03705">A Process for Topic Modelling Via Word Embeddings. (arXiv:2312.03705v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ulloa_D/0/1/0/all/0/1">Diego Salda&#xf1;a Ulloa</a></p>
<p>This work combines algorithms based on word embeddings, dimensionality
reduction, and clustering. The objective is to obtain topics from a set of
unclassified texts. The algorithm to obtain the word embeddings is the BERT
model, a neural network architecture widely used in NLP tasks. Due to the high
dimensionality, a dimensionality reduction technique called UMAP is used. This
method manages to reduce the dimensions while preserving part of the local and
global information of the original data. K-Means is used as the clustering
algorithm to obtain the topics. Then, the topics are evaluated using the TF-IDF
statistics, Topic Diversity, and Topic Coherence to get the meaning of the
words on the clusters. The results of the process show good values, so the
topic modeling of this process is a viable option for classifying or clustering
texts without labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03706">An Evaluation of State-of-the-Art Large Language Models for Sarcasm Detection. (arXiv:2312.03706v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Juliann Zhou</a></p>
<p>Sarcasm, as defined by Merriam-Webster, is the use of words by someone who
means the opposite of what he is trying to say. In the field of sentimental
analysis of Natural Language Processing, the ability to correctly identify
sarcasm is necessary for understanding people's true opinions. Because the use
of sarcasm is often context-based, previous research has used language
representation models, such as Support Vector Machine (SVM) and Long Short-Term
Memory (LSTM), to identify sarcasm with contextual-based information. Recent
innovations in NLP have provided more possibilities for detecting sarcasm. In
BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding, Jacob Devlin et al. (2018) introduced a new language
representation model and demonstrated higher precision in interpreting
contextualized language. As proposed by Hazarika et al. (2018), CASCADE is a
context-driven model that produces good results for detecting sarcasm. This
study analyzes a Reddit corpus using these two state-of-the-art models and
evaluates their performance against baseline models to find the ideal approach
to sarcasm detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03707">Multi-label Text Classification using GloVe and Neural Network Models. (arXiv:2312.03707v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongren Wang</a></p>
<p>This study addresses the challenges of multi-label text classification. The
difficulties arise from imbalanced data sets, varied text lengths, and numerous
subjective feature labels. Existing solutions include traditional machine
learning and deep neural networks for predictions. However, both approaches
have their limitations. Traditional machine learning often overlooks the
associations between words, while deep neural networks, despite their better
classification performance, come with increased training complexity and time.
This paper proposes a method utilizing the bag-of-words model approach based on
the GloVe model and the CNN-BiLSTM network. The principle is to use the word
vector matrix trained by the GloVe model as the input for the text embedding
layer. Given that the GloVe model requires no further training, the neural
network model can be trained more efficiently. The method achieves an accuracy
rate of 87.26% on the test set and an F1 score of 0.8737, showcasing promising
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03708">Abstraction via exemplars? A representational case study on lexical category inference in BERT. (arXiv:2312.03708v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1">Kanishka Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1">Najoung Kim</a></p>
<p>Exemplar based accounts are often considered to be in direct opposition to
pure linguistic abstraction in explaining language learners' ability to
generalize to novel expressions. However, the recent success of neural network
language models on linguistically sensitive tasks suggests that perhaps
abstractions can arise via the encoding of exemplars. We provide empirical
evidence for this claim by adapting an existing experiment that studies how an
LM (BERT) generalizes the usage of novel tokens that belong to lexical
categories such as Noun/Verb/Adjective/Adverb from exposure to only a single
instance of their usage. We analyze the representational behavior of the novel
tokens in these experiments, and find that BERT's capacity to generalize to
unseen expressions involving the use of these novel tokens constitutes the
movement of novel token representations towards regions of known category
exemplars in two-dimensional space. Our results suggest that learners' encoding
of exemplars can indeed give rise to abstraction like behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03709">UID as a Guiding Metric for Automated Authorship Obfuscation. (arXiv:2312.03709v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abegg_N/0/1/0/all/0/1">Nicholas Abegg</a></p>
<p>Protecting the anonymity of authors has become a difficult task given the
rise of automated authorship attributors. These attributors are capable of
attributing the author of a text amongst a pool of authors with great accuracy.
In order to counter the rise of these automated attributors, there has also
been a rise of automated obfuscators. These obfuscators are capable of taking
some text, perturbing the text in some manner, and, if successful, deceive an
automated attributor in misattributing the wrong author. We devised three novel
authorship obfuscation methods that utilized a Psycho-linguistic theory known
as Uniform Information Density (UID) theory. This theory states that humans
evenly distribute information amongst speech or text so as to maximize
efficiency. Utilizing this theory in our three obfuscation methods, we
attempted to see how successfully we could deceive two separate attributors.
Obfuscating 50 human and 50 GPT-3 generated articles from the TuringBench
dataset, we observed how well each method did on deceiving the attributors.
While the quality of the obfuscation in terms of semantic preservation and
sensical changes was high, we were not able to find any evidence to indicate
UID was a viable guiding metric for obfuscation. However, due to restrictions
in time we were unable to test a large enough sample of article or tune the
parameters for our attributors to comment conclusively on UID in obfuscation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03710">Don&#x27;t Overlook the Grammatical Gender: Bias Evaluation for Hindi-English Machine Translation. (arXiv:2312.03710v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pushpdeep Singh</a></p>
<p>Neural Machine Translation (NMT) models, though state-of-the-art for
translation, often reflect social biases, particularly gender bias. Existing
evaluation benchmarks primarily focus on English as the source language of
translation. For source languages other than English, studies often employ
gender-neutral sentences for bias evaluation, whereas real-world sentences
frequently contain gender information in different forms. Therefore, it makes
more sense to evaluate for bias using such source sentences to determine if NMT
models can discern gender from the grammatical gender cues rather than relying
on biased associations. To illustrate this, we create two gender-specific
sentence sets in Hindi to automatically evaluate gender bias in various
Hindi-English (HI-EN) NMT systems. We emphasise the significance of tailoring
bias evaluation test sets to account for grammatical gender markers in the
source language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03715">Sentiment Analysis of Twitter Posts on Global Conflicts. (arXiv:2312.03715v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sasikumar_U/0/1/0/all/0/1">Ujwal Sasikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaman_A/0/1/0/all/0/1">Ank Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mawlood_Yunis_A/0/1/0/all/0/1">Abdul-Rahman Mawlood-Yunis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1">Prosenjit Chatterjee</a></p>
<p>Sentiment analysis of social media data is an emerging field with vast
applications in various domains. In this study, we developed a sentiment
analysis model to analyze social media sentiment, especially tweets, during
global conflicting scenarios. To establish our research experiment, we
identified a recent global dispute incident on Twitter and collected around
31,000 filtered Tweets for several months to analyze human sentiment worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03716">Co-guiding for Multi-intent Spoken Language Understanding. (arXiv:2312.03716v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1">Bowen Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ivor W. Tsang</a></p>
<p>Recent graph-based models for multi-intent SLU have obtained promising
results through modeling the guidance from the prediction of intents to the
decoding of slot filling. However, existing methods (1) only model the
unidirectional guidance from intent to slot, while there are bidirectional
inter-correlations between intent and slot; (2) adopt homogeneous graphs to
model the interactions between the slot semantics nodes and intent label nodes,
which limit the performance. In this paper, we propose a novel model termed
Co-guiding Net, which implements a two-stage framework achieving the mutual
guidances between the two tasks. In the first stage, the initial estimated
labels of both tasks are produced, and then they are leveraged in the second
stage to model the mutual guidances. Specifically, we propose two heterogeneous
graph attention networks working on the proposed two heterogeneous semantics
label graphs, which effectively represent the relations among the semantics
nodes and label nodes. Besides, we further propose Co-guiding-SCL Net, which
exploits the single-task and dual-task semantics contrastive relations. For the
first stage, we propose single-task supervised contrastive learning, and for
the second stage, we propose co-guiding supervised contrastive learning, which
considers the two tasks' mutual guidances in the contrastive learning
procedure. Experiment results on multi-intent SLU show that our model
outperforms existing models by a large margin, obtaining a relative improvement
of 21.3% over the previous best model on MixATIS dataset in overall accuracy.
We also evaluate our model on the zero-shot cross-lingual scenario and the
results show that our model can relatively improve the state-of-the-art model
by 33.5% on average in terms of overall accuracy for the total 9 languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03718">Large Language Models in Law: A Survey. (arXiv:2312.03718v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1">Jinqi Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1">Wensheng Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiayang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhenlian Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>The advent of artificial intelligence (AI) has significantly impacted the
traditional judicial industry. Moreover, recently, with the development of
AI-generated content (AIGC), AI and law have found applications in various
domains, including image recognition, automatic text generation, and
interactive chat. With the rapid emergence and growing popularity of large
models, it is evident that AI will drive transformation in the traditional
judicial industry. However, the application of legal large language models
(LLMs) is still in its nascent stage. Several challenges need to be addressed.
In this paper, we aim to provide a comprehensive survey of legal LLMs. We not
only conduct an extensive survey of LLMs, but also expose their applications in
the judicial system. We first provide an overview of AI technologies in the
legal field and showcase the recent research in LLMs. Then, we discuss the
practical implementation presented by legal LLMs, such as providing legal
advice to users and assisting judges during trials. In addition, we explore the
limitations of legal LLMs, including data, algorithms, and judicial practice.
Finally, we summarize practical recommendations and propose future development
directions to address these challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03719">Assessing AI Chatbots Performance in Comprehensive Standardized Test Preparation; A Case Study with GRE. (arXiv:2312.03719v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abu_Haifa_M/0/1/0/all/0/1">Mohammad Abu-Haifa</a>, <a href="http://arxiv.org/find/cs/1/au:+Etawi_B/0/1/0/all/0/1">Bara&#x27;a Etawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkhatatbeh_H/0/1/0/all/0/1">Huthaifa Alkhatatbeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ababneh_A/0/1/0/all/0/1">Ayman Ababneh</a></p>
<p>This research paper presents a comprehensive evaluation of the performance of
three artificial 10 intelligence chatbots: Bing, ChatGPT, and GPT-4, in
addressing standardized test questions. Graduate record examination, known as
GRE, serves as a case study in this paper, encompassing both quantitative
reasoning and verbal skills. A total of 137 quantitative reasoning questions,
featuring diverse styles and 157 verbal questions categorized into varying
levels of difficulty (easy, medium, and hard) were administered to assess the
chatbots' capabilities. This paper provides a detailed examination of the
results and their implications for the utilization of artificial intelligence
in standardized test preparation by presenting the performance of each chatbot
across various skills and styles tested in the exam. Additionally, this paper
explores the proficiency of artificial intelligence in addressing image-based
questions and illustrates the uncertainty level of each chatbot. The results
reveal varying degrees of success across the chatbots, demonstrating the
influence of model sophistication and training data. GPT-4 emerged as the most
proficient, especially in complex language understanding tasks, highlighting
the evolution of artificial intelligence in language comprehension and its
ability to pass the exam with a high score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03720">Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits. (arXiv:2312.03720v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Johannes Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Haag_S/0/1/0/all/0/1">Steffi Haag</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruse_L/0/1/0/all/0/1">Leona Chandra Kruse</a></p>
<p>Large language models LLMs like ChatGPT have reached the 100 Mio user barrier
in record time and might increasingly enter all areas of our life leading to a
diverse set of interactions between those Artificial Intelligence models and
humans. While many studies have discussed governance and regulations
deductively from first-order principles, few studies provide an inductive,
data-driven lens based on observing dialogues between humans and LLMs
especially when it comes to non-collaborative, competitive situations that have
the potential to pose a serious threat to people. In this work, we conduct a
user study engaging over 40 individuals across all age groups in price
negotiations with an LLM. We explore how people interact with an LLM,
investigating differences in negotiation outcomes and strategies. Furthermore,
we highlight shortcomings of LLMs with respect to their reasoning capabilities
and, in turn, susceptiveness to prompt hacking, which intends to manipulate the
LLM to make agreements that are against its instructions or beyond any
rationality. We also show that the negotiated prices humans manage to achieve
span a broad range, which points to a literacy gap in effectively interacting
with LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03721">Exploring the Robustness of Model-Graded Evaluations and Automated Interpretability. (arXiv:2312.03721v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lermen_S/0/1/0/all/0/1">Simon Lermen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvapil_O/0/1/0/all/0/1">Ond&#x159;ej Kvapil</a></p>
<p>There has been increasing interest in evaluations of language models for a
variety of risks and characteristics. Evaluations relying on natural language
understanding for grading can often be performed at scale by using other
language models. We test the robustness of these model-graded evaluations to
injections on different datasets including a new Deception Eval. These
injections resemble direct communication between the testee and the evaluator
to change their grading. We extrapolate that future, more intelligent models
might manipulate or cooperate with their evaluation model. We find significant
susceptibility to these injections in state-of-the-art commercial models on all
examined evaluations. Furthermore, similar injections can be used on automated
interpretability frameworks to produce misleading model-written explanations.
The results inspire future work and should caution against unqualified trust in
evaluations and automated interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03722">Leveraging AI-derived Data for Carbon Accounting: Information Extraction from Alternative Sources. (arXiv:2312.03722v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oladeji_O/0/1/0/all/0/1">Olamide Oladeji</a>, <a href="http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1">Seyed Shahabeddin Mousavi</a></p>
<p>Carbon accounting is a fundamental building block in our global path to
emissions reduction and decarbonization, yet many challenges exist in achieving
reliable and trusted carbon accounting measures. We motivate that carbon
accounting not only needs to be more data-driven, but also more
methodologically sound. We discuss the need for alternative, more diverse data
sources that can play a significant role on our path to trusted carbon
accounting procedures and elaborate on not only why, but how Artificial
Intelligence (AI) in general and Natural Language Processing (NLP) in
particular can unlock reasonable access to a treasure trove of alternative data
sets in light of the recent advances in the field that better enable the
utilization of unstructured data in this process. We present a case study of
the recent developments on real-world data via an NLP-powered analysis using
OpenAI's GPT API on financial and shipping data. We conclude the paper with a
discussion on how these methods and approaches can be integrated into a broader
framework for AI-enabled integrative carbon accounting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03723">ChatGPT Application In Summarizing An Evolution Of Deep Learning Techniques In Imaging: A Qualitative Study. (arXiv:2312.03723v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarraf_A/0/1/0/all/0/1">Arman Sarraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbaspour_A/0/1/0/all/0/1">Amirabbas Abbaspour</a></p>
<p>The pursuit of article or text summarization has captured the attention of
natural language processing (NLP) practitioners, presenting itself as a
formidable challenge. ChatGPT 3.5 exhibits the capacity to condense the content
of up to 3000 tokens into a single page, aiming to retain pivotal information
from a given text across diverse themes. In a conducted qualitative research
endeavor, we selected seven scientific articles and employed the publicly
available ChatGPT service to generate summaries of these articles.
Subsequently, we engaged six co-authors of the articles in a survey, presenting
five questions to evaluate the quality of the summaries compared to the
original content. The findings revealed that the summaries produced by ChatGPT
effectively encapsulated the crucial information present in the articles,
preserving the principal message of each manuscript. Nonetheless, there was a
slight diminishment in the technical depth of the summaries as opposed to the
original articles. As a result, our conclusion underscores ChatGPT's text
summarization capability as a potent tool for extracting essential insights in
a manner more aligned with reporting than purely scientific discourse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03724">DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer. (arXiv:2312.03724v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Junyuan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenhui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhangheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>Large Language Models (LLMs) have emerged as dominant tools for various
tasks, particularly when tailored for a specific target by prompt tuning.
Nevertheless, concerns surrounding data privacy present obstacles due to the
tuned prompts' dependency on sensitive private information. A practical
solution is to host a local LLM and optimize a soft prompt privately using
data. Yet, hosting a local model becomes problematic when model ownership is
protected. Alternative methods, like sending data to the model's provider for
training, intensify these privacy issues facing an untrusted provider. In this
paper, we present a novel solution called Differentially-Private Offsite Prompt
Tuning (DP-OPT) to address this challenge. Our approach involves tuning a
discrete prompt on the client side and then applying it to the desired cloud
models. We demonstrate that prompts suggested by LLMs themselves can be
transferred without compromising performance significantly. To ensure that the
prompts do not leak private information, we introduce the first private prompt
generation mechanism, by a differentially-private (DP) ensemble of in-context
learning with private demonstrations. With DP-OPT, generating
privacy-preserving prompts by Vicuna-7b can yield competitive performance
compared to non-private in-context learning on GPT3.5 or local private prompt
tuning. Codes are available at https://github.com/VITA-Group/DP-OPT .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03725">SCStory: Self-supervised and Continual Online Story Discovery. (arXiv:2312.03725v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Susik Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1">Yu Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a></p>
<p>We present a framework SCStory for online story discovery, that helps people
digest rapidly published news article streams in real-time without human
annotations. To organize news article streams into stories, existing approaches
directly encode the articles and cluster them based on representation
similarity. However, these methods yield noisy and inaccurate story discovery
results because the generic article embeddings do not effectively reflect the
story-indicative semantics in an article and cannot adapt to the rapidly
evolving news article streams. SCStory employs self-supervised and continual
learning with a novel idea of story-indicative adaptive modeling of news
article streams. With a lightweight hierarchical embedding module that first
learns sentence representations and then article representations, SCStory
identifies story-relevant information of news articles and uses them to
discover stories. The embedding module is continuously updated to adapt to
evolving news streams with a contrastive learning objective, backed up by two
unique techniques, confidence-aware memory replay and prioritized-augmentation,
employed for label absence and data scarcity problems. Thorough experiments on
real and the latest news data sets demonstrate that SCStory outperforms
existing state-of-the-art algorithms for unsupervised online story discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03726">Interpretation modeling: Social grounding of sentences by reasoning over their implicit moral judgments. (arXiv:2312.03726v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1">Liesbeth Allein</a>, <a href="http://arxiv.org/find/cs/1/au:+Trusca_M/0/1/0/all/0/1">Maria Mihaela Tru&#x15f;c&#x1ce;</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>The social and implicit nature of human communication ramifies readers'
understandings of written sentences. Single gold-standard interpretations
rarely exist, challenging conventional assumptions in natural language
processing. This work introduces the interpretation modeling (IM) task which
involves modeling several interpretations of a sentence's underlying semantics
to unearth layers of implicit meaning. To obtain these, IM is guided by
multiple annotations of social relation and common ground - in this work
approximated by reader attitudes towards the author and their understanding of
moral judgments subtly embedded in the sentence. We propose a number of
modeling strategies that rely on one-to-one and one-to-many generation methods
that take inspiration from the philosophical study of interpretation. A
first-of-its-kind IM dataset is curated to support experiments and analyses.
The modeling results, coupled with scrutiny of the dataset, underline the
challenges of IM as conflicting and complex interpretations are socially
plausible. This interplay of diverse readings is affirmed by automated and
human evaluations on the generated interpretations. Finally, toxicity analyses
in the generated interpretations demonstrate the importance of IM for refining
filters of content and assisting content moderators in safeguarding the safety
in online discourse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03727">Content-Localization based System for Analyzing Sentiment and Hate Behaviors in Low-Resource Dialectal Arabic: English to Levantine and Gulf. (arXiv:2312.03727v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alzamzami_F/0/1/0/all/0/1">Fatimah Alzamzami</a>, <a href="http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1">Abdulmotaleb El Saddik</a></p>
<p>Even though online social movements can quickly become viral on social media,
languages can be a barrier to timely monitoring and analyzing the underlying
online social behaviors (OSB). This is especially true for under-resourced
languages on social media like dialectal Arabic; the primary language used by
Arabs on social media. Therefore, it is crucial to provide solutions to
efficiently exploit resources from high-resourced languages to solve
language-dependent OSB analysis in under-resourced languages. This paper
proposes to localize content of resources in high-resourced languages into
under-resourced Arabic dialects. Content localization goes beyond content
translation that converts text from one language to another; content
localization adapts culture, language nuances and regional preferences from one
language to a specific language/dialect. Automating understanding of the
natural and familiar day-to-day expressions in different regions, is the key to
achieve a wider analysis of OSB especially for smart cities. In this paper, we
utilize content-localization based neural machine translation to develop
sentiment and hate classifiers for two low-resourced Arabic dialects: Levantine
and Gulf. Not only this but we also leverage unsupervised learning to
facilitate the analysis of sentiment and hate predictions by inferring hidden
topics from the corresponding data and providing coherent interpretations of
those topics in their native language/dialects. The experimental evaluations
and proof-of-concept COVID-19 case study on real data have validated the
effectiveness of our proposed system in precisely distinguishing sentiments and
accurately identifying hate content in both Levantine and Gulf Arabic dialects.
Our findings shed light on the importance of considering the unique nature of
dialects within the same language and ignoring the dialectal aspect would lead
to misleading analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03728">Real Customization or Just Marketing: Are Customized Versions of Chat GPT Useful?. (arXiv:2312.03728v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garrido_Merchan_E/0/1/0/all/0/1">Eduardo C. Garrido-Merch&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Arroyo_Barriguete_J/0/1/0/all/0/1">Jose L. Arroyo-Barrig&#xfc;ete</a>, <a href="http://arxiv.org/find/cs/1/au:+Borras_Pala_F/0/1/0/all/0/1">Francisco Borr&#xe1;s-Pala</a>, <a href="http://arxiv.org/find/cs/1/au:+Escobar_Torres_L/0/1/0/all/0/1">Leandro Escobar-Torres</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibarreta_C/0/1/0/all/0/1">Carlos Mart&#xed;nez de Ibarreta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortiz_Lozano_J/0/1/0/all/0/1">Jose Mar&#xed;a Ortiz-Lozano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rua_Vieites_A/0/1/0/all/0/1">Antonio Rua-Vieites</a></p>
<p>Large Language Models (LLMs), as the case of OpenAI ChatGPT-4 Turbo, are
revolutionizing several industries, including higher education. In this
context, LLMs can be personalized through a fine-tuning process to meet the
student demands on every particular subject, like statistics. Recently, OpenAI
has launched the possibility to fine-tune their model with a natural language
web interface, enabling the possibility to create customized GPT version
deliberately conditioned to meet the demands of a specific task. The objective
of this research is to assess the potential of the customized GPTs that have
recently been launched by OpenAI. After developing a Business Statistics
Virtual Professor (BSVP), tailored for students at the Universidad Pontificia
Comillas, its behavior was evaluated and compared with that of ChatGPT-4 Turbo.
The results lead to several conclusions. Firstly, a substantial modification in
the style of communication was observed. Following the instructions it was
trained with, BSVP provided responses in a more relatable and friendly tone,
even incorporating a few minor jokes. Secondly, and this is a matter of
relevance, when explicitly asked for something like, "I would like to practice
a programming exercise similar to those in R practice 4," BSVP was capable of
providing a far superior response: having access to contextual documentation,
it could fulfill the request, something beyond ChatGPT-4 Turbo's capabilities.
On the downside, the response times were generally higher. Lastly, regarding
overall performance, quality, depth, and alignment with the specific content of
the course, no statistically significant differences were observed in the
responses between BSVP and ChatGPT-4 Turbo. It appears that customized
assistants trained with prompts present advantages as virtual aids for
students, yet they do not constitute a substantial improvement over ChatGPT-4
Turbo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03729">Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?. (arXiv:2312.03729v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kevin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1">Stephen Casper</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1">Dylan Hadfield-Menell</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a></p>
<p>Neural language models (LMs) can be used to evaluate the truth of factual
statements in two ways: they can be either queried for statement probabilities,
or probed for internal representations of truthfulness. Past work has found
that these two procedures sometimes disagree, and that probes tend to be more
accurate than LM outputs. This has led some researchers to conclude that LMs
"lie" or otherwise encode non-cooperative communicative intents. Is this an
accurate description of today's LMs, or can query-probe disagreement arise in
other ways? We identify three different classes of disagreement, which we term
confabulation, deception, and heterogeneity. In many cases, the superiority of
probes is simply attributable to better calibration on uncertain answers rather
than a greater fraction of correct, high-confidence answers. In some cases,
queries and probes perform better on different subsets of inputs, and accuracy
can further be improved by ensembling the two. Code is available at
github.com/lingo-mit/lm-truthfulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03730">FakeWatch ElectionShield: A Benchmarking Framework to Detect Fake News for Credible US Elections. (arXiv:2312.03730v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1">Tahniat Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Mizanur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatrath_V/0/1/0/all/0/1">Veronica Chatrath</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamgbose_O/0/1/0/all/0/1">Oluwanifemi Bamgbose</a>, <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a></p>
<p>In today's technologically driven world, the spread of fake news,
particularly during crucial events such as elections, presents an increasing
challenge to the integrity of information. To address this challenge, we
introduce FakeWatch ElectionShield, an innovative framework carefully designed
to detect fake news. We have created a novel dataset of North American
election-related news articles through a blend of advanced language models
(LMs) and thorough human verification, for precision and relevance. We propose
a model hub of LMs for identifying fake news. Our goal is to provide the
research community with adaptable and accurate classification models in
recognizing the dynamic nature of misinformation. Extensive evaluation of fake
news classifiers on our dataset and a benchmark dataset shows our that while
state-of-the-art LMs slightly outperform the traditional ML models, classical
models are still competitive with their balance of accuracy, explainability,
and computational efficiency. This research sets the foundation for future
studies to address misinformation related to elections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03731">MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xingtong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinming Zhang</a></p>
<p>Graphs can inherently model interconnected objects on the Web, thereby
facilitating a series of Web applications, such as web analyzing and content
recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a
mainstream technique for graph representation learning. However, their efficacy
within an end-to-end supervised framework is significantly tied to the
availabilityof task-specific labels. To mitigate labeling costs and enhance
robustness in few-shot settings, pre-training on self-supervised tasks has
emerged as a promising method, while prompting has been proposed to further
narrow the objective gap between pretext and downstream tasks. Although there
has been some initial exploration of prompt-based learning on graphs, they
primarily leverage a single pretext task, resulting in a limited subset of
general knowledge that could be learned from the pre-training data. Hence, in
this paper, we propose MultiGPrompt, a novel multi-task pre-training and
prompting framework to exploit multiple pretext tasks for more comprehensive
pre-trained knowledge. First, in pre-training, we design a set of pretext
tokens to synergize multiple pretext tasks. Second, we propose a dual-prompt
mechanism consisting of composed and open prompts to leverage task-specific and
global pre-training knowledge, to guide downstream tasks in few-shot settings.
Finally, we conduct extensive experiments on six public datasets to evaluate
and analyze MultiGPrompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03732">A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA. (arXiv:2312.03732v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalajdzievski_D/0/1/0/all/0/1">Damjan Kalajdzievski</a></p>
<p>As large language models (LLMs) have become increasingly compute and memory
intensive, parameter-efficient fine-tuning (PEFT) methods are now a common
strategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),
which adds trainable low-rank "adapters" to selected layers. Each adapter
consists of a low-rank matrix product, multiplicatively scaled by a
rank-dependent factor. This scaling factor, which divides adapters by a factor
of the rank, results in slowed learning and stunted performance for LoRA with
higher-rank adapters. Consequently, the use of LoRA in practice has generally
been limited to very low ranks. In this work, we study the impact of the
scaling factor on the learning process and prove that LoRA adapters should be
divided by a factor of the square root of the rank. Modifying LoRA with the
appropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)
method, easily provides for a fine-tuning compute/performance trade-off, where
larger ranks can be used to trade off increased computational resources during
training for better fine-tuning performance, with no change in inference
computing cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03733">Methods to Estimate Large Language Model Confidence. (arXiv:2312.03733v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotelanski_M/0/1/0/all/0/1">Maia Kotelanski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallo_R/0/1/0/all/0/1">Robert Gallo</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Ashwin Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1">Thomas Savage</a></p>
<p>Large Language Models have difficulty communicating uncertainty, which is a
significant obstacle to applying LLMs to complex medical tasks. This study
evaluates methods to measure LLM confidence when suggesting a diagnosis for
challenging clinical vignettes. GPT4 was asked a series of challenging case
questions using Chain of Thought and Self Consistency prompting. Multiple
methods were investigated to assess model confidence and evaluated on their
ability to predict the models observed accuracy. The methods evaluated were
Intrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC
Agreement Frequency correlated with observed accuracy, yielding a higher Area
under the Receiver Operating Characteristic Curve compared to Intrinsic
Confidence and CoT Length analysis. SC agreement is the most useful proxy for
model confidence, especially for medical diagnosis. Model Intrinsic Confidence
and CoT Response Length exhibit a weaker ability to differentiate between
correct and incorrect answers, preventing them from being reliable and
interpretable markers for model confidence. We conclude GPT4 has a limited
ability to assess its own diagnostic accuracy. SC Agreement Frequency is the
most useful method to measure GPT4 confidence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03734">Conditional Prompt Tuning for Multimodal Fusion. (arXiv:2312.03734v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1">Ruixiang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingbo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changwen Chen</a></p>
<p>We show that the representation of one modality can effectively guide the
prompting of another modality for parameter-efficient multimodal fusion.
Specifically, we first encode one modality and use its representation as a
prior to conditionally prompt all frozen layers of the other modality. This is
achieved by disentangling the vanilla prompt vectors into three types of
specialized prompts that adaptively capture global-level and instance-level
features. To better produce the instance-wise prompt, we introduce the mixture
of prompt experts (MoPE) to dynamically route each instance to the most
suitable prompt experts for encoding. We further study a regularization term to
avoid degenerated prompt expert routing. Thanks to our design, our method can
effectively transfer the pretrained knowledge in unimodal encoders for
downstream multimodal tasks. Compared with vanilla prompting, we show that our
MoPE-based conditional prompting is more expressive, thereby scales better with
training data and the total number of prompts. We also demonstrate that our
prompt tuning is architecture-agnostic, thereby offering high modularity.
Extensive experiments over three multimodal datasets demonstrate
state-of-the-art results, matching or surpassing the performance achieved
through fine-tuning, while only necessitating 0.7% of the trainable parameters.
Code will be released: https://github.com/songrise/ConditionalPrompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03735">Advancing State of the Art in Language Modeling. (arXiv:2312.03735v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herel_D/0/1/0/all/0/1">David Herel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikolov_T/0/1/0/all/0/1">Tomas Mikolov</a></p>
<p>Generalization is arguably the most important goal of statistical language
modeling research. Publicly available benchmarks and papers published with an
open-source code have been critical to advancing the field. However, it is
often very difficult, and sometimes even impossible, to reproduce the results
fully as reported in publications. In this paper, we propose a simple framework
that should help advance the state of the art in language modeling in terms of
generalization. We propose to publish not just the code, but also probabilities
on dev and test sets with future publications so that one can easily add the
new model into an ensemble. This has crucial advantages: it is much easier to
determine whether a newly proposed model is actually complementary to the
current baseline. Therefore, instead of inventing new names for the old tricks,
the scientific community can advance faster. Finally, this approach promotes
diversity of ideas: one does not need to create an individual model that is the
new state of the art to attract attention; it will be sufficient to develop a
new model that learns patterns which other models do not. Thus, even a
suboptimal model can be found to have value. Remarkably, our approach has
yielded new state-of-the-art results across various language modeling
benchmarks up to 10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03736">De-identification of clinical free text using natural language processing: A systematic review of current approaches. (arXiv:2312.03736v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovacevic_A/0/1/0/all/0/1">Aleksandar Kova&#x10d;evi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Basaragin_B/0/1/0/all/0/1">Bojana Ba&#x161;aragin</a>, <a href="http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1">Nikola Milo&#x161;evi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1">Goran Nenadi&#x107;</a></p>
<p>Background: Electronic health records (EHRs) are a valuable resource for
data-driven medical research. However, the presence of protected health
information (PHI) makes EHRs unsuitable to be shared for research purposes.
De-identification, i.e. the process of removing PHI is a critical step in
making EHR data accessible. Natural language processing has repeatedly
demonstrated its feasibility in automating the de-identification process.
Objectives: Our study aims to provide systematic evidence on how the
de-identification of clinical free text has evolved in the last thirteen years,
and to report on the performances and limitations of the current
state-of-the-art systems. In addition, we aim to identify challenges and
potential research opportunities in this field. Methods: A systematic search in
PubMed, Web of Science and the DBLP was conducted for studies published between
January 2010 and February 2023. Titles and abstracts were examined to identify
the relevant studies. Selected studies were then analysed in-depth, and
information was collected on de-identification methodologies, data sources, and
measured performance. Results: A total of 2125 publications were identified for
the title and abstract screening. 69 studies were found to be relevant. Machine
learning (37 studies) and hybrid (26 studies) approaches are predominant, while
six studies relied only on rules. Majority of the approaches were trained and
evaluated on public corpora. The 2014 i2b2/UTHealth corpus is the most
frequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016
CEGS N-GRID (10 studies) corpora.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03737">A Generic NLI approach for Classification of Sentiment Associated with Therapies. (arXiv:2312.03737v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kanagasabai_R/0/1/0/all/0/1">Rajaraman Kanagasabai</a>, <a href="http://arxiv.org/find/cs/1/au:+Veeramani_A/0/1/0/all/0/1">Anitha Veeramani</a></p>
<p>This paper describes our system for addressing SMM4H 2023 Shared Task 2 on
"Classification of sentiment associated with therapies (aspect-oriented)". In
our work, we adopt an approach based on Natural language inference (NLI) to
formulate this task as a sentence pair classification problem, and train
transformer models to predict sentiment associated with a therapy on a given
text. Our best model achieved 75.22\% F1-score which was 11\% (4\%) more than
the mean (median) score of all teams' submissions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03738">Syntactic Fusion: Enhancing Aspect-Level Sentiment Analysis Through Multi-Tree Graph Integration. (arXiv:2312.03738v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sunny_J/0/1/0/all/0/1">Jane Sunny</a>, <a href="http://arxiv.org/find/cs/1/au:+Padraig_T/0/1/0/all/0/1">Tom Padraig</a>, <a href="http://arxiv.org/find/cs/1/au:+Terry_R/0/1/0/all/0/1">Roggie Terry</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_W/0/1/0/all/0/1">Woods Ali</a></p>
<p>Recent progress in aspect-level sentiment classification has been propelled
by the incorporation of graph neural networks (GNNs) leveraging syntactic
structures, particularly dependency trees. Nevertheless, the performance of
these models is often hampered by the innate inaccuracies of parsing
algorithms. To mitigate this challenge, we introduce SynthFusion, an innovative
graph ensemble method that amalgamates predictions from multiple parsers. This
strategy blends diverse dependency relations prior to the application of GNNs,
enhancing robustness against parsing errors while avoiding extra computational
burdens. SynthFusion circumvents the pitfalls of overparameterization and
diminishes the risk of overfitting, prevalent in models with stacked GNN
layers, by optimizing graph connectivity. Our empirical evaluations on the
SemEval14 and Twitter14 datasets affirm that SynthFusion not only outshines
models reliant on single dependency trees but also eclipses alternative
ensemble techniques, achieving this without an escalation in model complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03739">Syntax-Informed Interactive Model for Comprehensive Aspect-Based Sentiment Analysis. (arXiv:2312.03739v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galen_U/0/1/0/all/0/1">Ullman Galen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_F/0/1/0/all/0/1">Frey Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_W/0/1/0/all/0/1">Woods Ali</a></p>
<p>Aspect-based sentiment analysis (ABSA), a nuanced task in text analysis,
seeks to discern sentiment orientation linked to specific aspect terms in text.
Traditional approaches often overlook or inadequately model the explicit
syntactic structures of sentences, crucial for effective aspect term
identification and sentiment determination. Addressing this gap, we introduce
an innovative model: Syntactic Dependency Enhanced Multi-Task Interaction
Architecture (SDEMTIA) for comprehensive ABSA. Our approach innovatively
exploits syntactic knowledge (dependency relations and types) using a
specialized Syntactic Dependency Embedded Interactive Network (SDEIN). We also
incorporate a novel and efficient message-passing mechanism within a multi-task
learning framework to bolster learning efficacy. Our extensive experiments on
benchmark datasets showcase our model's superiority, significantly surpassing
existing methods. Additionally, incorporating BERT as an auxiliary feature
extractor further enhances our model's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03740">Prompting in Autoregressive Large Language Models. (arXiv:2312.03740v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhandari_P/0/1/0/all/0/1">Prabin Bhandari</a></p>
<p>Autoregressive Large Language Models have transformed the landscape of
Natural Language Processing. Pre-train and prompt paradigm has replaced the
conventional approach of pre-training and fine-tuning for many downstream NLP
tasks. This shift has been possible largely due to LLMs and innovative
prompting techniques. LLMs have shown great promise for a variety of downstream
tasks owing to their vast parameters and huge datasets that they are
pre-trained on. However, in order to fully realize their potential, their
outputs must be guided towards the desired outcomes. Prompting, in which a
specific input or instruction is provided to guide the LLMs toward the intended
output, has become a tool for achieving this goal. In this paper, we discuss
the various prompting techniques that have been applied to fully harness the
power of LLMs. We present a taxonomy of existing literature on prompting
techniques and provide a concise survey based on this taxonomy. Further, we
identify some open problems in the realm of prompting in autoregressive LLMs
which could serve as a direction for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03741">Comparing Generative Chatbots Based on Process Requirements. (arXiv:2312.03741v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lins_L/0/1/0/all/0/1">Luis Fernando Lins</a>, <a href="http://arxiv.org/find/cs/1/au:+Nascimento_N/0/1/0/all/0/1">Nathalia Nascimento</a>, <a href="http://arxiv.org/find/cs/1/au:+Alencar_P/0/1/0/all/0/1">Paulo Alencar</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_T/0/1/0/all/0/1">Toacy Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Cowan_D/0/1/0/all/0/1">Donald Cowan</a></p>
<p>Business processes are commonly represented by modelling languages, such as
Event-driven Process Chain (EPC), Yet Another Workflow Language (YAWL), and the
most popular standard notation for modelling business processes, the Business
Process Model and Notation (BPMN). Most recently, chatbots, programs that allow
users to interact with a machine using natural language, have been increasingly
used for business process execution support. A recent category of chatbots
worth mentioning is generative-based chatbots, powered by Large Language Models
(LLMs) such as OpenAI's Generative Pre-Trained Transformer (GPT) model and
Google's Pathways Language Model (PaLM), which are trained on billions of
parameters and support conversational intelligence. However, it is not clear
whether generative-based chatbots are able to understand and meet the
requirements of constructs such as those provided by BPMN for process execution
support. This paper presents a case study to compare the performance of
prominent generative models, GPT and PaLM, in the context of process execution
support. The research sheds light into the challenging problem of using
conversational approaches supported by generative chatbots as a means to
understand process-aware modelling notations and support users to execute their
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03742">Clinical Risk Prediction Using Language Models: Benefits And Considerations. (arXiv:2312.03742v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Angeela Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1">Sulabh Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Conte_J/0/1/0/all/0/1">Joseph Conte</a>, <a href="http://arxiv.org/find/cs/1/au:+Avramovic_S/0/1/0/all/0/1">Sanja Avramovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Sikdar_S/0/1/0/all/0/1">Siddhartha Sikdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1">Antonios Anastasopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sanmay Das</a></p>
<p>The utilization of Electronic Health Records (EHRs) for clinical risk
prediction is on the rise. However, strict privacy regulations limit access to
comprehensive health records, making it challenging to apply standard machine
learning algorithms in practical real-world scenarios. Previous research has
addressed this data limitation by incorporating medical ontologies and
employing transfer learning methods. In this study, we investigate the
potential of leveraging language models (LMs) as a means to incorporate
supplementary domain knowledge for improving the performance of various
EHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR data
such as clinical notes, this study focuses on using textual descriptions within
structured EHR to make predictions exclusively based on that information. We
extensively compare against previous approaches across various data types and
sizes. We find that employing LMs to represent structured EHRs, such as
diagnostic histories, leads to improved or at least comparable performance in
diverse risk prediction tasks. Furthermore, LM-based approaches offer numerous
advantages, including few-shot learning, the capability to handle previously
unseen medical concepts, and adaptability to various medical vocabularies.
Nevertheless, we underscore, through various experiments, the importance of
being cautious when employing such models, as concerns regarding the
reliability of LMs persist.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03743">Easy Data Augmentation in Sentiment Analysis of Cyberbullying. (arXiv:2312.03743v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wirawan_A/0/1/0/all/0/1">Alwan Wirawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahyono_H/0/1/0/all/0/1">Hasan Dwi Cahyono</a>, <a href="http://arxiv.org/find/cs/1/au:+Winarno/0/1/0/all/0/1">Winarno</a></p>
<p>Instagram, a social media platform, has in the vicinity of 2 billion active
users in 2023. The platform allows users to post photos and videos with one
another. However, cyberbullying remains a significant problem for about 50% of
young Indonesians. To address this issue, sentiment analysis for comment
filtering uses a Support Vector Machine (SVM) and Easy Data Augmentation (EDA).
EDA will augment the dataset, enabling robust prediction and analysis of
cyberbullying by introducing more variation. Based on the tests, SVM
combination with EDA results in a 2.52% increase in the k-Fold Cross Validation
score. Our proposed approach shows an improved accuracy of 92.5%, 2.5% higher
than that of the existing state-of-the-art method. To maintain the
reproducibility and replicability of this research, the source code can be
accessed at uns.id/eda_svm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03744">Dynamic interactive group decision making method on two-dimensional language. (arXiv:2312.03744v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yukun Zhang</a></p>
<p>The language evaluation information of the interactive group decision method
at present is based on the one-dimension language variable. At the same time,
multi-attribute group decision making method based on two-dimension linguistic
information only use single-stage and static evaluation method. In this paper,
we propose a dynamic group decision making method based on two-dimension
linguistic information, combining dynamic interactive group decision making
methods with two-dimensional language evaluation information The method first
use Two-Dimensional Uncertain Linguistic Generalized Weighted Aggregation
(DULGWA) Operators to aggregate the preference information of each decision
maker, then adopting dynamic information entropy method to obtain weights of
attributes at each stage. Finally we propose the group consistency index to
quantify the termination conditions of group interaction. One example is given
to verify the developed approach and to demonstrate its effectiveness
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03746">Evaluating Large Language Model Creativity from a Literary Perspective. (arXiv:2312.03746v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1">Murray Shanahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1">Catherine Clarke</a></p>
<p>This paper assesses the potential for large language models (LLMs) to serve
as assistive tools in the creative writing process, by means of a single,
in-depth case study. In the course of the study, we develop interactive and
multi-voice prompting strategies that interleave background descriptions (scene
setting, plot elements), instructions that guide composition, samples of text
in the target style, and critical discussion of the given samples. We
qualitatively evaluate the results from a literary critical perspective, as
well as from the standpoint of computational creativity (a sub-field of
artificial intelligence). Our findings lend support to the view that the
sophistication of the results that can be achieved with an LLM mirrors the
sophistication of the prompting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03747">Classifying patient voice in social media data using neural networks: A comparison of AI models on different data sources and therapeutic domains. (arXiv:2312.03747v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lysandrou_G/0/1/0/all/0/1">Giorgos Lysandrou</a>, <a href="http://arxiv.org/find/cs/1/au:+Owen_R/0/1/0/all/0/1">Roma English Owen</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovic_V/0/1/0/all/0/1">Vanja Popovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Brun_G/0/1/0/all/0/1">Grant Le Brun</a>, <a href="http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1">Beatrice Alex</a>, <a href="http://arxiv.org/find/cs/1/au:+Fairley_E/0/1/0/all/0/1">Elizabeth A. L. Fairley</a></p>
<p>It is essential that healthcare professionals and members of the healthcare
community can access and easily understand patient experiences in the real
world, so that care standards can be improved and driven towards personalised
drug treatment. Social media platforms and message boards are deemed suitable
sources of patient experience information, as patients have been observed to
discuss and exchange knowledge, look for and provide support online. This paper
tests the hypothesis that not all online patient experience information can be
treated and collected in the same way, as a result of the inherent differences
in the way individuals talk about their journeys, in different therapeutic
domains and or data sources.
</p>
<p>We used linguistic analysis to understand and identify similarities between
datasets, across patient language, between data sources (Reddit, SocialGist)
and therapeutic domains (cardiovascular, oncology, immunology, neurology). We
detected common vocabulary used by patients in the same therapeutic domain
across data sources, except for immunology patients, who use unique vocabulary
between the two data sources, and compared to all other datasets. We combined
linguistically similar datasets to train classifiers (CNN, transformer) to
accurately identify patient experience posts from social media, a task we refer
to as patient voice classification. The cardiovascular and neurology
transformer classifiers perform the best in their respective comparisons for
the Reddit data source, achieving F1-scores of 0.865 and 1.0 respectively. The
overall best performing classifier is the transformer classifier trained on all
data collected for this experiment, achieving F1-scores ranging between 0.863
and 0.995 across all therapeutic domain and data source specific test datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03748">Applying Large Language Models and Chain-of-Thought for Automatic Scoring. (arXiv:2312.03748v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gyeong-Geon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1">Ehsan Latif</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuansheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ninghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>This study investigates the application of large language models (LLMs),
specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT)in the automatic
scoring of student-written responses to science assessments. We focused on
overcoming the challenges of accessibility, technical complexity, and lack of
explainability that have previously limited the use of automatic assessment
tools among researchers and educators. We used a testing dataset comprising six
assessment tasks (three binomial and three trinomial) with 1,650 student
responses. We employed six prompt engineering strategies, combining zero-shot
or few-shot learning with CoT, either alone or alongside item stem and scoring
rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot
learning (acc = .60), with 12.6\% increase. CoT, when used without item stem
and scoring rubrics, did not significantly affect scoring accuracy (acc = .60).
However, CoT prompting paired with contextual item stems and rubrics proved to
be a significant contributor to scoring accuracy (13.44\% increase for
zero-shot; 3.7\% increase for few-shot). Using a novel approach PPEAS, we found
a more balanced accuracy across different proficiency categories, highlighting
the importance of domain-specific reasoning in enhancing the effectiveness of
LLMs in scoring tasks. Additionally, we also found that GPT-4 demonstrated
superior performance over GPT-3.5 in various scoring tasks, showing 8.64\%
difference. The study revealed that the single-call strategy with GPT-4,
particularly using greedy sampling, outperformed other approaches, including
ensemble voting strategies. This study demonstrates the potential of LLMs in
facilitating automatic scoring, emphasizing that CoT enhances accuracy,
particularly when used with item stem and scoring rubrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03749">Conceptual Engineering Using Large Language Models. (arXiv:2312.03749v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Allen_B/0/1/0/all/0/1">Bradley P. Allen</a></p>
<p>We describe a method, based on Jennifer Nado's definition of classification
procedures as targets of conceptual engineering, that implements such
procedures using a large language model. We then apply this method using data
from the Wikidata knowledge graph to evaluate concept definitions from two
paradigmatic conceptual engineering projects: the International Astronomical
Union's redefinition of PLANET and Haslanger's ameliorative analysis of WOMAN.
We discuss implications of this work for the theory and practice of conceptual
engineering. The code and data can be found on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03750">Analyzing the Influence of Fake News in the 2024 Elections: A Comprehensive Dataset. (arXiv:2312.03750v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Mizanur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a></p>
<p>This work introduces a dataset focused on fake news in US political speeches,
specifically examining racial slurs and biases. By scraping and annotating
40,000 news articles, using advanced NLP tools and human verification, we
provide a nuanced understanding of misinformation in political discourse. The
dataset, designed for machine learning and bias analysis, is a critical
resource for researchers, policymakers, and educators. It facilitates the
development of strategies against misinformation and enhances media literacy,
marking a significant contribution to the study of fake news and political
communication. Our dataset, focusing on the analysis of fake news in the
context of the 2024 elections, is publicly accessible for community to work on
fake news identification. Our dataset, focusing on the analysis of fake news in
the context of the 2024 elections, is publicly accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03751">Which linguistic cues make people fall for fake news? A comparison of cognitive and affective processing. (arXiv:2312.03751v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lutz_B/0/1/0/all/0/1">Bernhard Lutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_M/0/1/0/all/0/1">Marc Adam</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1">Stefan Feuerriegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Prollochs_N/0/1/0/all/0/1">Nicolas Pr&#xf6;llochs</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1">Dirk Neumann</a></p>
<p>Fake news on social media has large, negative implications for society.
However, little is known about what linguistic cues make people fall for fake
news and, hence, how to design effective countermeasures for social media. In
this study, we seek to understand which linguistic cues make people fall for
fake news. Linguistic cues (e.g., adverbs, personal pronouns, positive emotion
words, negative emotion words) are important characteristics of any text and
also affect how people process real vs. fake news. Specifically, we compare the
role of linguistic cues across both cognitive processing (related to careful
thinking) and affective processing (related to unconscious automatic
evaluations). To this end, we performed a within-subject experiment where we
collected neurophysiological measurements of 42 subjects while these read a
sample of 40 real and fake news articles. During our experiment, we measured
cognitive processing through eye fixations, and affective processing in situ
through heart rate variability. We find that users engage more in cognitive
processing for longer fake news articles, while affective processing is more
pronounced for fake news written in analytic words. To the best of our
knowledge, this is the first work studying the role of linguistic cues in fake
news processing. Altogether, our findings have important implications for
designing online platforms that encourage users to engage in careful thinking
and thus prevent them from falling for fake news.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03752">Automatic Scoring of Students&#x27; Science Writing Using Hybrid Neural Network. (arXiv:2312.03752v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1">Ehsan Latif</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>This study explores the efficacy of a multi-perspective hybrid neural network
(HNN) for scoring student responses in science education with an analytic
rubric. We compared the accuracy of the HNN model with four ML approaches
(BERT, AACR, Naive Bayes, and Logistic Regression). The results have shown that
HHN achieved 8%, 3%, 1%, and 0.12% higher accuracy than Naive Bayes, Logistic
Regression, AACR, and BERT, respectively, for five scoring aspects (p&lt;0.001).
The overall HNN's perceived accuracy (M = 96.23%, SD = 1.45%) is comparable to
the (training and inference) expensive BERT model's accuracy (M = 96.12%, SD =
1.52%). We also have observed that HNN is x2 more efficient in training and
inferencing than BERT and has comparable efficiency to the lightweight but less
accurate Naive Bayes model. Our study confirmed the accuracy and efficiency of
using HNN to score students' science writing automatically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03753">English to Arabic machine translation of mathematical documents. (arXiv:2312.03753v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eddahibi_M/0/1/0/all/0/1">Mustapha Eddahibi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mensouri_M/0/1/0/all/0/1">Mohammed Mensouri</a></p>
<p>This paper is about the development of a machine translation system tailored
specifically for LATEX mathematical documents. The system focuses on
translating English LATEX mathematical documents into Arabic LATEX, catering to
the growing demand for multilingual accessibility in scientific and
mathematical literature. With the vast proliferation of LATEX mathematical
documents the need for an efficient and accurate translation system has become
increasingly essential. This paper addresses the necessity for a robust
translation tool that enables seamless communication and comprehension of
complex mathematical content across language barriers. The proposed system
leverages a Transformer model as the core of the translation system, ensuring
enhanced accuracy and fluency in the translated Arabic LATEX documents.
Furthermore, the integration of RyDArab, an Arabic mathematical TEX extension,
along with a rule-based translator for Arabic mathematical expressions,
contributes to the precise rendering of complex mathematical symbols and
equations in the translated output. The paper discusses the architecture,
methodology, of the developed system, highlighting its efficacy in bridging the
language gap in the domain of mathematical documentation
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03755">Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models. (arXiv:2312.03755v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Engler_D/0/1/0/all/0/1">Davis Engler</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuechun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">James Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wald_D/0/1/0/all/0/1">David J. Wald</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaiswal_K/0/1/0/all/0/1">Kishor Jaiswal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Susu Xu</a></p>
<p>When a damaging earthquake occurs, immediate information about casualties is
critical for time-sensitive decision-making by emergency response and aid
agencies in the first hours and days. Systems such as Prompt Assessment of
Global Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS)
were developed to provide a forecast within about 30 minutes of any significant
earthquake globally. Traditional systems for estimating human loss in disasters
often depend on manually collected early casualty reports from global media, a
process that's labor-intensive and slow with notable time delays. Recently,
some systems have employed keyword matching and topic modeling to extract
relevant information from social media. However, these methods struggle with
the complex semantics in multilingual texts and the challenge of interpreting
ever-changing, often conflicting reports of death and injury numbers from
various unverified sources on social media platforms. In this work, we
introduce an end-to-end framework to significantly improve the timeliness and
accuracy of global earthquake-induced human loss forecasting using
multi-lingual, crowdsourced social media. Our framework integrates (1) a
hierarchical casualty extraction model built upon large language models, prompt
design, and few-shot learning to retrieve quantitative human loss claims from
social media, (2) a physical constraint-aware, dynamic-truth discovery model
that discovers the truthful human loss from massive noisy and potentially
conflicting human loss claims, and (3) a Bayesian updating loss projection
model that dynamically updates the final loss estimation using discovered
truths. We test the framework in real-time on a series of global earthquake
events in 2021 and 2022 and show that our framework streamlines casualty data
retrieval, achieving speed and accuracy comparable to manual methods by USGS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03756">LineConGraphs: Line Conversation Graphs for Effective Emotion Recognition using Graph Neural Networks. (arXiv:2312.03756v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1">Gokul S Krishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Padi_S/0/1/0/all/0/1">Sarala Padi</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenberg_C/0/1/0/all/0/1">Craig S. Greenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1">Balaraman Ravindran</a>, <a href="http://arxiv.org/find/cs/1/au:+Manoch_D/0/1/0/all/0/1">Dinesh Manoch</a>, <a href="http://arxiv.org/find/cs/1/au:+Sriram_R/0/1/0/all/0/1">Ram D.Sriram</a></p>
<p>Emotion Recognition in Conversations (ERC) is a critical aspect of affective
computing, and it has many practical applications in healthcare, education,
chatbots, and social media platforms. Earlier approaches for ERC analysis
involved modeling both speaker and long-term contextual information using graph
neural network architectures. However, it is ideal to deploy
speaker-independent models for real-world applications. Additionally, long
context windows can potentially create confusion in recognizing the emotion of
an utterance in a conversation. To overcome these limitations, we propose novel
line conversation graph convolutional network (LineConGCN) and graph attention
(LineConGAT) models for ERC analysis. These models are speaker-independent and
built using a graph construction strategy for conversations -- line
conversation graphs (LineConGraphs). The conversational context in
LineConGraphs is short-term -- limited to one previous and future utterance,
and speaker information is not part of the graph. We evaluate the performance
of our proposed models on two benchmark datasets, IEMOCAP and MELD, and show
that our LineConGAT model outperforms the state-of-the-art methods with an
F1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding
sentiment shift information into line conversation graphs further enhances the
ERC performance in the case of GCN models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03758">Stock Movement and Volatility Prediction from Tweets, Macroeconomic Factors and Historical Prices. (arXiv:2312.03758v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengkun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">YangXiao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1">Taoran Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1">Kaiqun Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Linhan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chang-Tien Lu</a></p>
<p>Predicting stock market is vital for investors and policymakers, acting as a
barometer of the economic health. We leverage social media data, a potent
source of public sentiment, in tandem with macroeconomic indicators as
government-compiled statistics, to refine stock market predictions. However,
prior research using tweet data for stock market prediction faces three
challenges. First, the quality of tweets varies widely. While many are filled
with noise and irrelevant details, only a few genuinely mirror the actual
market scenario. Second, solely focusing on the historical data of a particular
stock without considering its sector can lead to oversight. Stocks within the
same industry often exhibit correlated price behaviors. Lastly, simply
forecasting the direction of price movement without assessing its magnitude is
of limited value, as the extent of the rise or fall truly determines
profitability. In this paper, diverging from the conventional methods, we
pioneer an ECON. The framework has following advantages: First, ECON has an
adept tweets filter that efficiently extracts and decodes the vast array of
tweet data. Second, ECON discerns multi-level relationships among stocks,
sectors, and macroeconomic factors through a self-aware mechanism in semantic
space. Third, ECON offers enhanced accuracy in predicting substantial stock
price fluctuations by capitalizing on stock price movement. We showcase the
state-of-the-art performance of our proposed model using a dataset,
specifically curated by us, for predicting stock market movements and
volatility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03759">How should the advent of large language models affect the practice of science?. (arXiv:2312.03759v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Binz_M/0/1/0/all/0/1">Marcel Binz</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaniz_S/0/1/0/all/0/1">Stephan Alaniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Roskies_A/0/1/0/all/0/1">Adina Roskies</a>, <a href="http://arxiv.org/find/cs/1/au:+Aczel_B/0/1/0/all/0/1">Balazs Aczel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergstrom_C/0/1/0/all/0/1">Carl T. Bergstrom</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1">Colin Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Schad_D/0/1/0/all/0/1">Daniel Schad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulff_D/0/1/0/all/0/1">Dirk Wulff</a>, <a href="http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1">Jevin D. West</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shiffrin_R/0/1/0/all/0/1">Richard M. Shiffrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1">Samuel J. Gershman</a>, <a href="http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1">Ven Popov</a>, <a href="http://arxiv.org/find/cs/1/au:+Bender_E/0/1/0/all/0/1">Emily M. Bender</a>, <a href="http://arxiv.org/find/cs/1/au:+Marelli_M/0/1/0/all/0/1">Marco Marelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1">Matthew M. Botvinick</a>, <a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1">Zeynep Akata</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1">Eric Schulz</a></p>
<p>Large language models (LLMs) are being increasingly incorporated into
scientific workflows. However, we have yet to fully grasp the implications of
this integration. How should the advent of large language models affect the
practice of science? For this opinion piece, we have invited four diverse
groups of scientists to reflect on this query, sharing their perspectives and
engaging in debate. Schulz et al. make the argument that working with LLMs is
not fundamentally different from working with human collaborators, while Bender
et al. argue that LLMs are often misused and over-hyped, and that their
limitations warrant a focus on more specialized, easily interpretable tools.
Marelli et al. emphasize the importance of transparent attribution and
responsible use of LLMs. Finally, Botvinick and Gershman advocate that humans
should retain responsibility for determining the scientific roadmap. To
facilitate the discussion, the four perspectives are complemented with a
response from each group. By putting these different perspectives in
conversation, we aim to bring attention to important considerations within the
academic community regarding the adoption of LLMs and their impact on both
current and future scientific practices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03766">Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment. (arXiv:2312.03766v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gordon_B/0/1/0/all/0/1">Brian Gordon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1">Yonatan Bitton</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafir_Y/0/1/0/all/0/1">Yonatan Shafir</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1">Roopal Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1">Dani Lischinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1">Idan Szpektor</a></p>
<p>While existing image-text alignment models reach high quality binary
assessments, they fall short of pinpointing the exact source of misalignment.
In this paper, we present a method to provide detailed textual and visual
explanation of detected misalignments between text-image pairs. We leverage
large language models and visual grounding models to automatically construct a
training set that holds plausible misaligned captions for a given image and
corresponding textual explanations and visual indicators. We also publish a new
human curated test set comprising ground-truth textual and visual misalignment
annotations. Empirical results show that fine-tuning vision language models on
our training set enables them to articulate misalignments and visually indicate
them within images, outperforming strong baselines both on the binary alignment
classification and the explanation generation tasks. Our method code and human
curated test set are available at: https://mismatch-quest.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03769">GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science. (arXiv:2312.03769v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Varghese_A/0/1/0/all/0/1">Alan John Varghese</a>, <a href="http://arxiv.org/find/cs/1/au:+Oommen_V/0/1/0/all/0/1">Vivek Oommen</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>The new polymath Large Language Models (LLMs) can speed-up greatly scientific
reviews, possibly using more unbiased quantitative metrics, facilitating
cross-disciplinary connections, and identifying emerging trends and research
gaps by analyzing large volumes of data. However, at the present time, they
lack the required deep understanding of complex methodologies, they have
difficulty in evaluating innovative claims, and they are unable to assess
ethical issues and conflicts of interest. Herein, we consider 13 GPT-related
papers across different scientific domains, reviewed by a human reviewer and
SciSpace, a large language model, with the reviews evaluated by three distinct
types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that
50% of SciSpace's responses to objective questions align with those of a human
reviewer, with GPT-4 (informed evaluator) often rating the human reviewer
higher in accuracy, and SciSpace higher in structure, clarity, and
completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and
crowd panel) showed varying preferences between SciSpace and human responses,
with the crowd panel showing a preference for the human responses. However,
GPT-4 rated them equally in accuracy and structure but favored SciSpace for
completeness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03788">SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM. (arXiv:2312.03788v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiayi Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengcan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kaifu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangguang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1">Bin Feng</a></p>
<p>Large language models (LLMs) have shown remarkable capabilities in various
tasks. However their huge model size and the consequent demand for
computational and memory resources also pose challenges to model deployment.
Currently, 4-bit post-training quantization (PTQ) has achieved some success in
LLMs, reducing the memory footprint by approximately 75% compared to FP16
models, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,
an accurate and efficient 4-bit weight-only PTQ that requires no additional
training, which enables lossless in accuracy for LLMs for the first time. Based
on the fact that the loss of weight quantization is amplified by the activation
outliers, SmoothQuant+ smoothes the activation outliers by channel before
quantization, while adjusting the corresponding weights for mathematical
equivalence, and then performs group-wise 4-bit weight quantization for linear
layers. We have integrated SmoothQuant+ into the vLLM framework, an advanced
high-throughput inference engine specially developed for LLMs, and equipped it
with an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support
SmoothQuant+ 4-bit weight quantization. Our results show that, with
SmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100
40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0
times compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the
latency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.
This is the state-of-the-art 4-bit weight quantization for LLMs as we know.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03789">Comparative Analysis of Multilingual Text Classification &amp; Identification through Deep Learning and Embedding Visualization. (arXiv:2312.03789v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wyawhare_A/0/1/0/all/0/1">Arinjay Wyawhare</a></p>
<p>This research conducts a comparative study on multilingual text
classification methods, utilizing deep learning and embedding visualization.
The study employs LangDetect, LangId, FastText, and Sentence Transformer on a
dataset encompassing 17 languages. It explores dimensionality's impact on
clustering, revealing FastText's clearer clustering in 2D visualization due to
its extensive multilingual corpus training. Notably, the FastText multi-layer
perceptron model achieved remarkable accuracy, precision, recall, and F1 score,
outperforming the Sentence Transformer model. The study underscores the
effectiveness of these techniques in multilingual text classification,
emphasizing the importance of large multilingual corpora for training
embeddings. It lays the groundwork for future research and assists
practitioners in developing language detection and classification systems.
Additionally, it includes the comparison of multi-layer perceptron, LSTM, and
Convolution models for classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03813">Improving Activation Steering in Language Models with Mean-Centring. (arXiv:2312.03813v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jorgensen_O/0/1/0/all/0/1">Ole Jorgensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cope_D/0/1/0/all/0/1">Dylan Cope</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoots_N/0/1/0/all/0/1">Nandi Schoots</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1">Murray Shanahan</a></p>
<p>Recent work in activation steering has demonstrated the potential to better
control the outputs of Large Language Models (LLMs), but it involves finding
steering vectors. This is difficult because engineers do not typically know how
features are represented in these models. We seek to address this issue by
applying the idea of mean-centring to steering vectors. We find that taking the
average of activations associated with a target dataset, and then subtracting
the mean of all training activations, results in effective steering vectors. We
test this method on a variety of models on natural language tasks by steering
away from generating toxic text, and steering the completion of a story towards
a target genre. We also apply mean-centring to extract function vectors, more
effectively triggering the execution of a range of natural language tasks by a
significant margin (compared to previous baselines). This suggests that
mean-centring can be used to easily improve the effectiveness of activation
steering in a wide range of contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03815">LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem. (arXiv:2312.03815v1 [cs.OS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yujie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>This paper envisions a revolutionary AIOS-Agent ecosystem, where Large
Language Model (LLM) serves as the (Artificial) Intelligent Operating System
(IOS, or AIOS)--an operating system ``with soul''. Upon this foundation, a
diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are
developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift
from the traditional OS-APP ecosystem. We envision that LLM's impact will not
be limited to the AI application level, instead, it will in turn revolutionize
the design and implementation of computer system, architecture, software, and
programming language, featured by several main concepts: LLM as OS
(system-level), Agents as Applications (application-level), Natural Language as
Programming Interface (user-level), and Tools as Devices/Libraries
(hardware/middleware-level).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03818">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want. (arXiv:2312.03818v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zeyi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Ye Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yuhang Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1">Shu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yuanjun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) plays an essential role in
extracting valuable content information from images across diverse tasks. It
aligns textual and visual modalities to comprehend the entire image, including
all the details, even those irrelevant to specific tasks. However, for a finer
understanding and controlled editing of images, it becomes crucial to focus on
specific regions of interest, which can be indicated as points, masks, or boxes
by humans or perception models. To fulfill the requirements, we introduce
Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to
suggest attentive regions and fine-tuned with constructed millions of RGBA
region-text pairs. Alpha-CLIP not only preserves the visual recognition ability
of CLIP but also enables precise control over the emphasis of image contents.
It demonstrates effectiveness in various tasks, including but not limited to
open-world recognition, multimodal large language models, and conditional 2D /
3D generation. It has a strong potential to serve as a versatile tool for
image-related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03863">Efficient Large Language Models: A Survey. (arXiv:2312.03863v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1">Zhongwei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1">Samiul Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1">Zhongnan Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Quanlu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1">Mosharaf Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
important tasks such as natural language understanding, language generation,
and complex reasoning and have the potential to make a substantial impact on
our society. Such capabilities, however, come with the considerable resources
they demand, highlighting the strong need to develop effective techniques for
addressing their efficiency challenges. In this survey, we provide a systematic
and comprehensive review of efficient LLMs research. We organize the literature
in a taxonomy consisting of three main categories, covering distinct yet
interconnected efficient LLMs topics from model-centric, data-centric, and
framework-centric perspective, respectively. We have also created a GitHub
repository where we compile the papers featured in this survey at
https://github.com/AIoT-MLSys-Lab/EfficientLLMs,
https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively
maintain this repository and incorporate new research as it emerges. We hope
our survey can serve as a valuable resource to help researchers and
practitioners gain a systematic understanding of the research developments in
efficient LLMs and inspire them to contribute to this important and exciting
field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03872">The BigCode Project Governance Card. (arXiv:2312.03872v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+collaboration_BigCode/0/1/0/all/0/1">BigCode collaboration</a>: <a href="http://arxiv.org/find/cs/1/au:+Hughes_S/0/1/0/all/0/1">Sean Hughes</a>, <a href="http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1">Harm de Vries</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1">Jennifer Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrandis_C/0/1/0/all/0/1">Carlos Mu&#xf1;oz Ferrandis</a>, <a href="http://arxiv.org/find/cs/1/au:+Allal_L/0/1/0/all/0/1">Loubna Ben Allal</a>, <a href="http://arxiv.org/find/cs/1/au:+Werra_L/0/1/0/all/0/1">Leandro von Werra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jennifer Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Paquet_S/0/1/0/all/0/1">Sebastien Paquet</a>, <a href="http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1">Yacine Jernite</a></p>
<p>This document serves as an overview of the different mechanisms and areas of
governance in the BigCode project. It aims to support transparency by providing
relevant information about choices that were made during the project to the
broader public, and to serve as an example of intentional governance of an open
research project that future endeavors can leverage to shape their own
approach. The first section, Project Structure, covers the project
organization, its stated goals and values, its internal decision processes, and
its funding and resources. The second section, Data and Model Governance,
covers decisions relating to the questions of data subject consent, privacy,
and model release.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03897">Revisiting the Optimality of Word Lengths. (arXiv:2312.03897v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1">Tiago Pimentel</a>, <a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1">Clara Meister</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1">Ethan Gotlieb Wilcox</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1">Kyle Mahowald</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>Zipf (1935) posited that wordforms are optimized to minimize utterances'
communicative costs. Under the assumption that cost is given by an utterance's
length, he supported this claim by showing that words' lengths are inversely
correlated with their frequencies. Communicative cost, however, can be
operationalized in different ways. Piantadosi et al. (2011) claim that cost
should be measured as the distance between an utterance's information rate and
channel capacity, which we dub the channel capacity hypothesis (CCH) here.
Following this logic, they then proposed that a word's length should be
proportional to the expected value of its surprisal (negative log-probability
in context). In this work, we show that Piantadosi et al.'s derivation does not
minimize CCH's cost, but rather a lower bound, which we term CCH-lower. We
propose a novel derivation, suggesting an improved way to minimize CCH's cost.
Under this method, we find that a language's word lengths should instead be
proportional to the surprisal's expectation plus its variance-to-mean ratio.
Experimentally, we compare these three communicative cost functions: Zipf's,
CCH-lower , and CCH. Across 13 languages and several experimental settings, we
find that length is better predicted by frequency than either of the other
hypotheses. In fact, when surprisal's expectation, or expectation plus
variance-to-mean ratio, is estimated using better language models, it leads to
worse word length predictions. We take these results as evidence that Zipf's
longstanding hypothesis holds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03905">A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1">Kareem Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>Neuro-symbolic AI bridges the gap between purely symbolic and neural
approaches to learning. This often requires maximizing the likelihood of a
symbolic constraint w.r.t the neural network's output distribution. Such output
distributions are typically assumed to be fully-factorized. This limits the
applicability of neuro-symbolic learning to the more expressive autoregressive
distributions, e.g., transformers. Under such distributions, computing the
likelihood of even simple constraints is #P-hard. Instead of attempting to
enforce the constraint on the entire output distribution, we propose to do so
on a random, local approximation thereof. More precisely, we optimize the
likelihood of the constraint under a pseudolikelihood-based approximation
centered around a model sample. Our approximation is factorized, allowing the
reuse of solutions to sub-problems, a main tenet for efficiently computing
neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of
the likelihood, exhibiting low entropy and KL-divergence around the model
sample. We evaluate our approach on Sudoku and shortest-path prediction cast as
autoregressive generation, and observe that we greatly improve upon the base
model's ability to predict logically-consistent outputs. We also evaluate on
the task of detoxifying large language models. Using a simple constraint
disallowing a list of toxic words, we are able to steer the model's outputs
away from toxic generations, achieving SoTA detoxification compared to previous
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03912">Collaboration or Corporate Capture? Quantifying NLP&#x27;s Reliance on Industry Artifacts and Contributions. (arXiv:2312.03912v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aitken_W/0/1/0/all/0/1">Will Aitken</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1">Mohamed Abdalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudie_K/0/1/0/all/0/1">Karen Rudie</a>, <a href="http://arxiv.org/find/cs/1/au:+Stinson_C/0/1/0/all/0/1">Catherine Stinson</a></p>
<p>The advent of transformers, higher computational budgets, and big data has
engendered remarkable progress in Natural Language Processing (NLP). Impressive
performance of industry pre-trained models has garnered public attention in
recent years and made news headlines. That these are industry models is
noteworthy. Rarely, if ever, are academic institutes producing exciting new NLP
models. Using these models is critical for competing on NLP benchmarks and
correspondingly to stay relevant in NLP research. We surveyed 100 papers
published at EMNLP 2022 to determine whether this phenomenon constitutes a
reliance on industry for NLP publications.
</p>
<p>We find that there is indeed a substantial reliance. Citations of industry
artifacts and contributions across categories is at least three times greater
than industry publication rates per year. Quantifying this reliance does not
settle how we ought to interpret the results. We discuss two possible
perspectives in our discussion: 1) Is collaboration with industry still
collaboration in the absence of an alternative? Or 2) has free NLP inquiry been
captured by the motivations and research direction of private corporations?
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03987">Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration. (arXiv:2312.03987v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Meihao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaoyue Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Ju Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_C/0/1/0/all/0/1">Chengliang Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_N/0/1/0/all/0/1">Nan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guoliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaoyong Du</a></p>
<p>Entity resolution (ER) is an important data integration task with a wide
spectrum of applications. The state-of-the-art solutions on ER rely on
pre-trained language models (PLMs), which require fine-tuning on a lot of
labeled matching/non-matching entity pairs. Recently, large languages models
(LLMs), such as GPT-4, have shown the ability to perform many tasks without
tuning model parameters, which is known as in-context learning (ICL) that
facilitates effective learning from a few labeled input context demonstrations.
However, existing ICL approaches to ER typically necessitate providing a task
description and a set of demonstrations for each entity pair and thus have
limitations on the monetary cost of interfacing LLMs. To address the problem,
in this paper, we provide a comprehensive study to investigate how to develop a
cost-effective batch prompting approach to ER. We introduce a framework BATCHER
consisting of demonstration selection and question batching and explore
different design choices that support batch prompting for ER. We also devise a
covering-based demonstration selection strategy that achieves an effective
balance between matching accuracy and monetary cost. We conduct a thorough
evaluation to explore the design space and evaluate our proposed strategies.
Through extensive experiments, we find that batch prompting is very
cost-effective for ER, compared with not only PLM-based methods fine-tuned with
extensive labeled data but also LLM-based methods with manually designed
prompting. We also provide guidance for selecting appropriate design choices
for batch prompting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04021">A Study on the Calibration of In-context Learning. (arXiv:2312.04021v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanlin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi-Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1">Dhruv Madeka</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dean Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Hima Lakkaraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1">Sham Kakade</a></p>
<p>Modern auto-regressive language models are trained to minimize log loss on
broad data by predicting the next token so they are expected to get calibrated
answers when framing a problem as a next-token prediction task. We study this
for in-context learning (ICL), a widely used way to adapt frozen large language
models (LLMs) via crafting prompts, and investigate the trade-offs between
performance and calibration on a wide range of natural language understanding
and reasoning tasks. We conduct extensive experiments to show that such
trade-offs may get worse as we increase model size, incorporate more ICL
examples, and fine-tune models using instruction, dialog, or reinforcement
learning from human feedback (RLHF) on carefully curated datasets. Furthermore,
we find that common recalibration techniques that are widely effective such as
temperature scaling provide limited gains in calibration errors, suggesting
that new methods may be required for settings where models are expected to be
reliable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04032">RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training. (arXiv:2312.04032v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jaehyung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuning Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hanchao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Davis Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1">Pascale Fung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1">Fuli Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a></p>
<p>Fine-tuning pre-trained language models (LMs) has become the de facto
standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to
robustness issues, such as adversarial robustness and model calibration.
Several perspectives of robustness for LMs have been studied independently, but
lacking a unified consideration in multiple perspectives. In this paper, we
propose Robustifying LMs via Adversarial perturbation with Selective Training
(RoAST), a simple yet effective fine-tuning technique to enhance the
multi-perspective robustness of LMs in a unified way. RoAST effectively
incorporates two important sources for the model robustness, robustness on the
perturbed inputs and generalizable knowledge in pre-trained LMs. To be
specific, RoAST introduces adversarial perturbation during fine-tuning while
the model parameters are selectively updated upon their relative importance to
minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by
incorporating four representative perspectives of model robustness, we
demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning
methods on six different types of LMs, which indicates its usefulness in
practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04052">Multimodal Misinformation Detection in a South African Social Media Environment. (arXiv:2312.04052v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jager_A/0/1/0/all/0/1">Amica De Jager</a>, <a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1">Vukosi Marivate</a>, <a href="http://arxiv.org/find/cs/1/au:+Modupe_A/0/1/0/all/0/1">Abioudun Modupe</a></p>
<p>With the constant spread of misinformation on social media networks, a need
has arisen to continuously assess the veracity of digital content. This need
has inspired numerous research efforts on the development of misinformation
detection (MD) models. However, many models do not use all information
available to them and existing research contains a lack of relevant datasets to
train the models, specifically within the South African social media
environment. The aim of this paper is to investigate the transferability of
knowledge of a MD model between different contextual environments. This
research contributes a multimodal MD model capable of functioning in the South
African social media environment, as well as introduces a South African
misinformation dataset. The model makes use of multiple sources of information
for misinformation detection, namely: textual and visual elements. It uses
bidirectional encoder representations from transformers (BERT) as the textual
encoder and a residual network (ResNet) as the visual encoder. The model is
trained and evaluated on the Fakeddit dataset and a South African
misinformation dataset. Results show that using South African samples in the
training of the model increases model performance, in a South African
contextual environment, and that a multimodal model retains significantly more
knowledge than both the textual and visual unimodal models. Our study suggests
that the performance of a misinformation detection model is influenced by the
cultural nuances of its operating environment and multimodal models assist in
the transferability of knowledge between different contextual environments.
Therefore, local data should be incorporated into the training process of a
misinformation detection model in order to optimize model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04059">Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss. (arXiv:2312.04059v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhuoran Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Berry_M/0/1/0/all/0/1">Michael P. Berry</a>, <a href="http://arxiv.org/find/cs/1/au:+Chwyl_C/0/1/0/all/0/1">Christina Chwyl</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_G/0/1/0/all/0/1">Gary Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jing Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Forman_E/0/1/0/all/0/1">Evan M. Forman</a></p>
<p>Automated coaching messages for weight control can save time and costs, but
their repetitive, generic nature may limit their effectiveness compared to
human coaching. Large language model (LLM) based artificial intelligence (AI)
chatbots, like ChatGPT, could offer more personalized and novel messages to
address repetition with their data-processing abilities. While LLM AI
demonstrates promise to encourage healthier lifestyles, studies have yet to
examine the feasibility and acceptability of LLM-based BWL coaching. 87 adults
in a weight-loss trial rated ten coaching messages' helpfulness (five
human-written, five ChatGPT-generated) using a 5-point Likert scale, providing
additional open-ended feedback to justify their ratings. Participants also
identified which messages they believed were AI-generated. The evaluation
occurred in two phases: messages in Phase 1 were perceived as impersonal and
negative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated
messages were rated less helpful than human-written ones, with 66 percent
receiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI
messages matched the human-written ones regarding helpfulness, with 82% scoring
three or above. Additionally, 50% were misidentified as human-written,
suggesting AI's sophistication in mimicking human-generated content. A thematic
analysis of open-ended feedback revealed that participants appreciated AI's
empathy and personalized suggestions but found them more formulaic, less
authentic, and too data-focused. This study reveals the preliminary feasibility
and acceptability of LLM AIs, like ChatGPT, in crafting potentially effective
weight control coaching messages. Our findings also underscore areas for future
enhancement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04068">Making Translators Privacy-aware on the User&#x27;s Side. (arXiv:2312.04068v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1">Ryoma Sato</a></p>
<p>We propose PRISM to enable users of machine translation systems to preserve
the privacy of data on their own initiative. There is a growing demand to apply
machine translation systems to data that require privacy protection. While
several machine translation engines claim to prioritize privacy, the extent and
specifics of such protection are largely ambiguous. First, there is often a
lack of clarity on how and to what degree the data is protected. Even if
service providers believe they have sufficient safeguards in place,
sophisticated adversaries might still extract sensitive information. Second,
vulnerabilities may exist outside of these protective measures, such as within
communication channels, potentially leading to data leakage. As a result, users
are hesitant to utilize machine translation engines for data demanding high
levels of privacy protection, thereby missing out on their benefits. PRISM
resolves this problem. Instead of relying on the translation service to keep
data safe, PRISM provides the means to protect data on the user's side. This
approach ensures that even machine translation engines with inadequate privacy
measures can be used securely. For platforms already equipped with privacy
safeguards, PRISM acts as an additional protection layer, reinforcing their
security furthermore. PRISM adds these privacy features without significantly
compromising translation accuracy. Our experiments demonstrate the
effectiveness of PRISM using real-world translators, T5 and ChatGPT
(GPT-3.5-turbo), and the datasets with two languages. PRISM effectively
balances privacy protection with translation accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04103">Enhancing the Rationale-Input Alignment for Self-explaining Rationalization. (arXiv:2312.04103v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haozhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhiying Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">YuanKai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruixuan Li</a></p>
<p>Rationalization empowers deep learning models with self-explaining
capabilities through a cooperative game, where a generator selects a
semantically consistent subset of the input as a rationale, and a subsequent
predictor makes predictions based on the selected rationale. In this paper, we
discover that rationalization is prone to a problem named \emph{rationale
shift}, which arises from the algorithmic bias of the cooperative game.
Rationale shift refers to a situation where the semantics of the selected
rationale may deviate from the original input, but the predictor still produces
accurate predictions based on the deviation, resulting in a compromised
generator with misleading feedback.
</p>
<p>To address this issue, we first demonstrate the importance of the alignment
between the rationale and the full input through both empirical observations
and theoretical analysis. Subsequently, we introduce a novel approach called
DAR (\textbf{D}iscriminatively \textbf{A}ligned \textbf{R}ationalization),
which utilizes an auxiliary module pretrained on the full input to
discriminatively align the selected rationale and the original input. We
theoretically illustrate how DAR accomplishes the desired alignment, thereby
overcoming the rationale shift problem. The experiments on two widely used
real-world benchmarks show that the proposed method significantly improves the
explanation quality (measured by the overlap between the model-selected
explanation and the human-annotated rationale) as compared to state-of-the-art
techniques. Additionally, results on two synthetic settings further validate
the effectiveness of DAR in addressing the rationale shift problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04127">Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak. (arXiv:2312.04127v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yanrui Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sendong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Ming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bing Qin</a></p>
<p>Extensive work has been devoted to improving the safety mechanism of Large
Language Models (LLMs). However, in specific scenarios, LLMs still generate
harmful responses when faced with malicious instructions, a phenomenon referred
to as "Jailbreak Attack". In our research, we introduce a novel jailbreak
attack method (\textbf{RADIAL}), which consists of two steps: 1) Inherent
Response Tendency Analysis: we analyze the inherent affirmation and rejection
tendency of LLMs to react to real-world instructions. 2) Real-World
Instructions-Driven Jailbreak: based on our analysis, we strategically choose
several real-world instructions and embed malicious instructions into them to
amplify the LLM's potential to generate harmful responses. On three open-source
human-aligned LLMs, our method achieves excellent jailbreak attack performance
for both Chinese and English malicious instructions. Besides, we guided
detailed ablation experiments and verified the effectiveness of our core idea
"Inherent Response Tendency Analysis". Our exploration also exposes the
vulnerability of LLMs to being induced into generating more detailed harmful
responses in subsequent rounds of dialogue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04134">Using a Large Language Model to generate a Design Structure Matrix. (arXiv:2312.04134v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_E/0/1/0/all/0/1">Edwin C. Y. Koh</a></p>
<p>The Design Structure Matrix (DSM) is an established method used in dependency
modelling, especially in the design of complex engineering systems. The
generation of DSM is traditionally carried out through manual means and can
involve interviewing experts to elicit critical system elements and the
relationships between them. Such manual approaches can be time-consuming and
costly. This paper presents a workflow that uses a Large Language Model (LLM)
to support the generation of DSM and improve productivity. A prototype of the
workflow was developed in this work and applied on a diesel engine DSM
published previously. It was found that the prototype could reproduce 357 out
of 462 DSM entries published (i.e. 77.3%), suggesting that the work can aid DSM
generation. A no-code version of the prototype is made available online to
support future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04193">Language Model Knowledge Distillation for Efficient Question Answering in Spanish. (arXiv:2312.04193v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bazaga_A/0/1/0/all/0/1">Adri&#xe1;n Bazaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Micklem_G/0/1/0/all/0/1">Gos Micklem</a></p>
<p>Recent advances in the development of pre-trained Spanish language models has
led to significant progress in many Natural Language Processing (NLP) tasks,
such as question answering. However, the lack of efficient models imposes a
barrier for the adoption of such models in resource-constrained environments.
Therefore, smaller distilled models for the Spanish language could be proven to
be highly scalable and facilitate their further adoption on a variety of tasks
and scenarios. In this work, we take one step in this direction by developing
SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient
question answering in Spanish. To achieve this, we employ knowledge
distillation from a large model onto a lighter model that allows for a wider
implementation, even in areas with limited computational resources, whilst
attaining negligible performance sacrifice. Our experiments show that the dense
distilled model can still preserve the performance of its larger counterpart,
while significantly increasing inference speedup. This work serves as a
starting point for further research and investigation of model compression
efforts for Spanish language models across various NLP tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04219">Swap distance minimization in SOV languages. Cognitive and mathematical foundations. (arXiv:2312.04219v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1">Ramon Ferrer-i-Cancho</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiripad_S/0/1/0/all/0/1">Savithry Namboodiripad</a></p>
<p>Distance minimization is a general principle of language. A special case of
this principle in the domain of word order is swap distance minimization. This
principle predicts that variations from a canonical order that are reached by
fewer swaps of adjacent constituents are lest costly and thus more likely. Here
we investigate the principle in the context of the triple formed by subject
(S), object (O) and verb (V). We introduce the concept of word order rotation
as a cognitive underpinning of that prediction. When the canonical order of a
language is SOV, the principle predicts SOV &lt; SVO, OSV &lt; VSO, OVS &lt; VOS, in
order of increasing cognitive cost. We test the prediction in three flexible
order SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese
(Indo-European). Evidence of swap distance minimization is found in all three
languages, but it is weaker in Sinhalese. Swap distance minimization is
stronger than a preference for the canonical order in Korean and especially
Malayalam.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04262">PsyChat: A Client-Centric Dialogue System for Mental Health Support. (arXiv:2312.04262v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Huachuan Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Anqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>Dialogue systems are increasingly integrated into mental health support to
help clients facilitate exploration, gain insight, take action, and ultimately
heal themselves. For a dialogue system to be practical and user-friendly, it
should be client-centric, focusing on the client's behaviors. However, existing
dialogue systems publicly available for mental health support often concentrate
solely on the counselor's strategies rather than the behaviors expressed by
clients. This can lead to the implementation of unreasonable or inappropriate
counseling strategies and corresponding responses from the dialogue system. To
address this issue, we propose PsyChat, a client-centric dialogue system that
provides psychological support through online chat. The client-centric dialogue
system comprises five modules: client behavior recognition, counselor strategy
selection, input packer, response generator intentionally fine-tuned to produce
responses, and response selection. Both automatic and human evaluations
demonstrate the effectiveness and practicality of our proposed dialogue system
for real-life mental health support. Furthermore, we employ our proposed
dialogue system to simulate a real-world client-virtual-counselor interaction
scenario. The system is capable of predicting the client's behaviors, selecting
appropriate counselor strategies, and generating accurate and suitable
responses, as demonstrated in the scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04302">Prompt Highlighter: Interactive Control for Multi-Modal LLMs. (arXiv:2312.04302v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuechen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengju Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bohao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jiaya Jia</a></p>
<p>This study targets a critical aspect of multi-modal LLMs' (LLMs&amp;VLMs)
inference: explicit controllable text generation. Multi-modal LLMs empower
multi-modality understanding with the capability of semantic generation yet
bring less explainability and heavier reliance on prompt contents due to their
autoregressive generative nature. While manipulating prompt formats could
improve outputs, designing specific and precise prompts per task can be
challenging and ineffective. To tackle this issue, we introduce a novel
inference method, Prompt Highlighter, which enables users to highlight specific
prompt spans to interactively control the focus during generation. Motivated by
the classifier-free diffusion guidance, we form regular and unconditional
context pairs based on highlighted tokens, demonstrating that the
autoregressive generation in models can be guided in a classifier-free way.
Notably, we find that, during inference, guiding the models with highlighted
tokens through the attention weights leads to more desired outputs. Our
approach is compatible with current LLMs and VLMs, achieving impressive
customized generation results without training. Experiments confirm its
effectiveness in focusing on input contexts and generating reliable content.
Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and
1552.5 in MME-perception. The code is available at:
https://github.com/dvlab-research/Prompt-Highlighter/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04306">nerblackbox: A High-level Library for Named Entity Recognition in Python. (arXiv:2312.04306v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stollenwerk_F/0/1/0/all/0/1">Felix Stollenwerk</a></p>
<p>We present nerblackbox, a python library to facilitate the use of
state-of-the-art transformer-based models for named entity recognition. It
provides simple-to-use yet powerful methods to access data and models from a
wide range of sources, for fully automated model training and evaluation as
well as versatile model inference. While many technical challenges are solved
and hidden from the user by default, nerblackbox also offers fine-grained
control and a rich set of customizable features. It is thus targeted both at
application-oriented developers as well as machine learning experts and
researchers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04333">Beyond Surface: Probing LLaMA Across Scales and Layers. (arXiv:2312.04333v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1">Ning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Shining Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1">Ming Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1">Linjun Shou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>This paper presents an in-depth analysis of Large Language Models (LLMs),
focusing on LLaMA, a prominent open-source foundational model in natural
language processing. Instead of assessing LLaMA through its generative output,
we design multiple-choice tasks to probe its intrinsic understanding in
high-order tasks such as reasoning and computation. We examine the model
horizontally, comparing different sizes, and vertically, assessing different
layers. We unveil several key and uncommon findings based on the designed
probing tasks: (1) Horizontally, enlarging model sizes almost could not
automatically impart additional knowledge or computational prowess. Instead, it
can enhance reasoning abilities, especially in math problem solving, and helps
reduce hallucinations, but only beyond certain size thresholds; (2) In vertical
analysis, the lower layers of LLaMA lack substantial arithmetic and factual
knowledge, showcasing logical thinking, multilingual and recognitive abilities,
with top layers housing most computational power and real-world knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04339">Merging by Matching Models in Task Subspaces. (arXiv:2312.04339v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1">Derek Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a></p>
<p>Model merging aims to cheaply combine individual task-specific models into a
single multitask model. In this work, we view past merging methods as
leveraging different notions of a ''task subspace'' in which models are matched
before being merged. We connect the task subspace of a given model to its loss
landscape and formalize how this approach to model merging can be seen as
solving a linear system of equations. While past work has generally been
limited to linear systems that have a closed-form solution, we consider using
the conjugate gradient method to find a solution. We show that using the
conjugate gradient method can outperform closed-form solutions, enables merging
via linear systems that are otherwise intractable to solve, and flexibly allows
choosing from a wide variety of initializations and estimates for the ''task
subspace''. We ultimately demonstrate that our merging framework called
''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art
results in multitask and intermediate-task model merging. We release all of the
code and checkpoints used in our work at https://github.com/r-three/mats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04344">Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pengcheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhongying Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yanzhou Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junjun He</a></p>
<p>OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V's
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model's prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V's medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V's full
diagnostic potential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04348">When Input Integers are Given in the Unary Numeral Representation. (arXiv:2312.04348v1 [cs.CC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamakami_T/0/1/0/all/0/1">Tomoyuki Yamakami</a></p>
<p>Many NP-complete problems take integers as part of their input instances.
These input integers are generally binarized, that is, provided in the form of
the "binary" numeral representation, and the lengths of such binary forms are
used as a basis unit to measure the computational complexity of the problems.
In sharp contrast, the "unarization" (or the "unary" numeral representation) of
numbers has been known to bring a remarkably different effect onto the
computational complexity of the problems. When no computational-complexity
difference is observed between binarization and unarization of instances, on
the contrary, the problems are said to be strong NP-complete. This work
attempts to spotlight an issue of how the unarization of instances affects the
computational complexity of various combinatorial problems. We present numerous
NP-complete (or even NP-hard) problems, which turn out to be easily solvable
when input integers are represented in unary. We then discuss the computational
complexities of such problems when taking unary-form integer inputs. We hope
that a list of such problems signifies the structural differences between
strong NP-completeness and non-strong NP-completeness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04350">CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1">Felix Leeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Gresele_L/0/1/0/all/0/1">Luigi Gresele</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1">Ojasv Kamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1">Zhiheng Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1">Kevin Blin</a>, <a href="http://arxiv.org/find/cs/1/au:+Adauto_F/0/1/0/all/0/1">Fernando Gonzalez Adauto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1">Max Kleiman-Weiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>The ability to perform causal reasoning is widely considered a core feature
of intelligence. In this work, we investigate whether large language models
(LLMs) can coherently reason about causality. Much of the existing work in
natural language processing (NLP) focuses on evaluating commonsense causal
reasoning in LLMs, thus failing to assess whether a model can perform causal
inference in accordance with a set of well-defined formal rules. To address
this, we propose a new NLP task, causal inference in natural language, inspired
by the "causal inference engine" postulated by Judea Pearl et al. We compose a
large dataset, CLadder, with 10K samples: based on a collection of causal
graphs and queries (associational, interventional, and counterfactual), we
obtain symbolic questions and ground-truth answers, through an oracle causal
inference engine. These are then translated into natural language. We evaluate
multiple LLMs on our dataset, and we introduce and evaluate a bespoke
chain-of-thought prompting strategy, CausalCoT. We show that our task is highly
challenging for LLMs, and we conduct an in-depth analysis to gain deeper
insight into the causal reasoning abilities of LLMs. Our data is open-sourced
at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found
at https://github.com/causalNLP/cladder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04362">PCoQA: Persian Conversational Question Answering Dataset. (arXiv:2312.04362v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hemati_H/0/1/0/all/0/1">Hamed Hematian Hemati</a>, <a href="http://arxiv.org/find/cs/1/au:+Toghyani_A/0/1/0/all/0/1">Atousa Toghyani</a>, <a href="http://arxiv.org/find/cs/1/au:+Souri_A/0/1/0/all/0/1">Atena Souri</a>, <a href="http://arxiv.org/find/cs/1/au:+Alavian_S/0/1/0/all/0/1">Sayed Hesam Alavian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sameti_H/0/1/0/all/0/1">Hossein Sameti</a>, <a href="http://arxiv.org/find/cs/1/au:+Beigy_H/0/1/0/all/0/1">Hamid Beigy</a></p>
<p>Humans seek information regarding a specific topic through performing a
conversation containing a series of questions and answers. In the pursuit of
conversational question answering research, we introduce the PCoQA, the first
\textbf{P}ersian \textbf{Co}nversational \textbf{Q}uestion \textbf{A}nswering
dataset, a resource comprising information-seeking dialogs encompassing a total
of 9,026 contextually-driven questions. Each dialog involves a questioner, a
responder, and a document from the Wikipedia; The questioner asks several
inter-connected questions from the text and the responder provides a span of
the document as the answer for each question. PCoQA is designed to present
novel challenges compared to previous question answering datasets including
having more open-ended non-factual answers, longer answers, and fewer lexical
overlaps. This paper not only presents the comprehensive PCoQA dataset but also
reports the performance of various benchmark models. Our models include
baseline models and pre-trained models, which are leveraged to boost the
performance of the model. The dataset and benchmarks are available at our
Github page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04372">LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs. (arXiv:2312.04372v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunsheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Can Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wenqian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peiran Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Juanwu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelraouf_A/0/1/0/all/0/1">Amr Abdelraouf</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rohit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kyungtae Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1">Aniket Bera</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1">James M. Rehg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziran Wang</a></p>
<p>We present LaMPilot, a novel framework for planning in the field of
autonomous driving, rethinking the task as a code-generation process that
leverages established behavioral primitives. This approach aims to address the
challenge of interpreting and executing spontaneous user instructions such as
"overtake the car ahead," which have typically posed difficulties for existing
frameworks. We introduce the LaMPilot benchmark specifically designed to
quantitatively evaluate the efficacy of Large Language Models (LLMs) in
translating human directives into actionable driving policies. We then evaluate
a wide range of state-of-the-art code generation language models on tasks from
the LaMPilot Benchmark. The results of the experiments showed that GPT-4, with
human feedback, achieved an impressive task completion rate of 92.7% and a
minimal collision rate of 0.9%. To encourage further investigation in this
area, our code and dataset will be made available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04440">OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization. (arXiv:2312.04440v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amar_S/0/1/0/all/0/1">Shmuel Amar</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiff_L/0/1/0/all/0/1">Liat Schiff</a>, <a href="http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1">Ori Ernst</a>, <a href="http://arxiv.org/find/cs/1/au:+Shefer_A/0/1/0/all/0/1">Asi Shefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1">Ori Shapira</a>, <a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1">Ido Dagan</a></p>
<p>The performance of automatic summarization models has improved dramatically
in recent years. Yet, there is still a gap in meeting specific information
needs of users in real-world scenarios, particularly when a targeted summary is
sought, such as in the useful aspect-based summarization setting targeted in
this paper. Previous datasets and studies for this setting have predominantly
concentrated on a limited set of pre-defined aspects, focused solely on single
document inputs, or relied on synthetic data. To advance research on more
realistic scenarios, we introduce OpenAsp, a benchmark for multi-document
\textit{open} aspect-based summarization. This benchmark is created using a
novel and cost-effective annotation protocol, by which an open aspect dataset
is derived from existing generic multi-document summarization datasets. We
analyze the properties of OpenAsp showcasing its high-quality content. Further,
we show that the realistic open-aspect setting realized in OpenAsp poses a
challenge for current state-of-the-art summarization models, as well as for
large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.09517">Deep Learning for Hate Speech Detection: A Comparative Study. (arXiv:2202.09517v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1">Jitendra Singh Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1">Hezhe Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a></p>
<p>Automated hate speech detection is an important tool in combating the spread
of hate speech, particularly in social media. Numerous methods have been
developed for the task, including a recent proliferation of deep-learning based
approaches. A variety of datasets have also been developed, exemplifying
various manifestations of the hate-speech detection problem. We present here a
large-scale empirical comparison of deep and shallow hate-speech detection
methods, mediated through the three most commonly used datasets. Our goal is to
illuminate progress in the area, and identify strengths and weaknesses in the
current state-of-the-art. We particularly focus our analysis on measures of
practical performance, including detection accuracy, computational efficiency,
capability in using pre-trained models, and domain generalization. In doing so
we aim to provide guidance as to the use of hate-speech detection in practice,
quantify the state-of-the-art, and identify future research directions. Code
and dataset are available at
https://github.com/jmjmalik22/Hate-Speech-Detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.08089">Constrained Few-Shot Learning: Human-Like Low Sample Complexity Learning and Non-Episodic Text Classification. (arXiv:2208.08089v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mar_J/0/1/0/all/0/1">Jaron Mar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiamou Liu</a></p>
<p>Few-shot learning (FSL) is an emergent paradigm of learning that attempts to
learn to reason with low sample complexity to mimic the way humans learn,
generalise and extrapolate from only a few seen examples. While FSL attempts to
mimic these human characteristics, fundamentally, the task of FSL as
conventionally formulated using meta-learning with episodic-based training does
not in actuality align with how humans acquire and reason with knowledge. FSL
with episodic training, while only requires $K$ instances of each test class,
still requires a large number of labelled training instances from disjoint
classes. In this paper, we introduce the novel task of constrained few-shot
learning (CFSL), a special case of FSL where $M$, the number of instances of
each training class is constrained such that $M \leq K$ thus applying a similar
restriction during FSL training and test. We propose a method for CFSL
leveraging Cat2Vec using a novel categorical contrastive loss inspired by
cognitive theories such as fuzzy trace theory and prototype theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.14578">MAUVE Scores for Generative Models: Theory and Practice. (arXiv:2212.14578v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1">Krishna Pillutla</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1">John Thickstun</a>, <a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1">Sean Welleck</a>, <a href="http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1">Swabha Swayamdipta</a>, <a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1">Rowan Zellers</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Sewoong Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1">Zaid Harchaoui</a></p>
<p>Generative artificial intelligence has made significant strides, producing
text indistinguishable from human prose and remarkably photorealistic images.
Automatically measuring how close the generated data distribution is to the
target distribution is central to diagnosing existing models and developing
better ones. We present MAUVE, a family of comparison measures between pairs of
distributions such as those encountered in the generative modeling of text or
images. These scores are statistical summaries of divergence frontiers
capturing two types of errors in generative modeling. We explore three
approaches to statistically estimate these scores: vector quantization,
non-parametric estimation, and classifier-based estimation. We provide
statistical bounds for the vector quantization approach.
</p>
<p>Empirically, we find that the proposed scores paired with a range of
$f$-divergences and statistical estimation methods can quantify the gaps
between the distributions of human-written text and those of modern neural
language models by correlating with human judgments and identifying known
properties of the generated texts. We demonstrate in the vision domain that
MAUVE can identify known properties of generated images on par with or better
than existing metrics. In conclusion, we present practical recommendations for
using MAUVE effectively with language and image modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.09820">A Stability Analysis of Fine-Tuning a Pre-Trained Model. (arXiv:2301.09820v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zihao Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+So_A/0/1/0/all/0/1">Anthony Man-Cho So</a>, <a href="http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1">Nigel Collier</a></p>
<p>Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,
etc.) has proven to be one of the most promising paradigms in recent NLP
research. However, numerous recent works indicate that fine-tuning suffers from
the instability problem, i.e., tuning the same model under the same setting
results in significantly different performance. Many recent works have proposed
different methods to solve this problem, but there is no theoretical
understanding of why and how these methods work. In this paper, we propose a
novel theoretical stability analysis of fine-tuning that focuses on two
commonly used settings, namely, full fine-tuning and head tuning. We define the
stability under each setting and prove the corresponding stability bounds. The
theoretical bounds explain why and how several existing methods can stabilize
the fine-tuning procedure. In addition to being able to explain most of the
observed empirical discoveries, our proposed theoretical analysis framework can
also help in the design of effective and provable methods. Based on our theory,
we propose three novel strategies to stabilize the fine-tuning procedure,
namely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self
Unsupervised Re-Training (SURT). We extensively evaluate our proposed
approaches on 11 widely used real-world benchmark datasets, as well as hundreds
of synthetic classification datasets. The experiment results show that our
proposed methods significantly stabilize the fine-tuning procedure and also
corroborate our theoretical analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06458">ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yuexian Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1">David A. Clifton</a></p>
<p>Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03514">Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1">Caleb Ziems</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1">William Held</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1">Omar Shaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhehao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a></p>
<p>Large Language Models (LLMs) are capable of successfully performing many
language processing tasks zero-shot (without training data). If zero-shot LLMs
can also reliably classify and explain social phenomena like persuasiveness and
political ideology, then LLMs could augment the Computational Social Science
(CSS) pipeline in important ways. This work provides a road map for using LLMs
as CSS tools. Towards this end, we contribute a set of prompting best practices
and an extensive evaluation pipeline to measure the zero-shot performance of 13
language models on 25 representative English CSS benchmarks. On taxonomic
labeling tasks (classification), LLMs fail to outperform the best fine-tuned
models but still achieve fair levels of agreement with humans. On free-form
coding tasks (generation), LLMs produce explanations that often exceed the
quality of crowdworkers' gold references. We conclude that the performance of
today's LLMs can augment the CSS research pipeline in two ways: (1) serving as
zero-shot data annotators on human annotation teams, and (2) bootstrapping
challenging creative generation tasks (e.g., explaining the underlying
attributes of a text). In summary, LLMs are posed to meaningfully participate
in} social science analysis in partnership with humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06164">Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1">Parag Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1">Mirella Lapata</a></p>
<p>In this paper we consider the task of conversational semantic parsing over
general purpose knowledge graphs (KGs) with millions of entities, and thousands
of relation-types. We focus on models which are capable of interactively
mapping user utterances into executable logical forms (e.g., Sparql) in the
context of the conversational history. Our key idea is to represent information
about an utterance and its context via a subgraph which is created dynamically,
i.e., the number of nodes varies per utterance. Rather than treating the
subgraph as a sequence, we exploit its underlying structure and encode it with
a graph neural network which further allows us to represent a large number of
(unseen) nodes. Experimental results show that dynamic context modeling is
superior to static approaches, delivering performance improvements across the
board (i.e., for simple and complex questions). Our results further confirm
that modeling the structure of context is better at processing discourse
information, (i.e., at handling ellipsis and resolving coreference) and longer
interactions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07372">Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v3 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1">Zheng Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Toby Jia-Jun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1">Jonathan K. Kummerfeld</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a></p>
<p>Relational databases play an important role in this Big Data era. However, it
is challenging for non-experts to fully unleash the analytical power of
relational databases, since they are not familiar with database languages such
as SQL. Many techniques have been proposed to automatically generate SQL from
natural language, but they suffer from two issues: (1) they still make many
mistakes, particularly for complex queries, and (2) they do not provide a
flexible way for non-expert users to validate and refine the incorrect queries.
To address these issues, we introduce a new interaction mechanism that allows
users directly edit a step-by-step explanation of an incorrect SQL to fix SQL
errors. Experiments on the Spider benchmark show that our approach outperforms
three SOTA approaches by at least 31.6% in terms of execution accuracy. A user
study with 24 participants further shows that our approach helped users solve
significantly more SQL tasks with less time and higher confidence,
demonstrating its potential to expand access to databases, particularly for
non-experts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11426">Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1">Satyapriya Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jiaqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1">Dylan Slack</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1">Asma Ghandeharioun</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Himabindu Lakkaraju</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing complex tasks. Moreover, recent research has shown that
incorporating human-annotated rationales (e.g., Chain-of-Thought prompting)
during in-context learning can significantly enhance the performance of these
models, particularly on tasks that require reasoning capabilities. However,
incorporating such rationales poses challenges in terms of scalability as this
requires a high degree of human involvement. In this work, we present a novel
framework, Amplifying Model Performance by Leveraging In-Context Learning with
Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges
by automating the process of rationale generation. To this end, we leverage
post hoc explanation methods which output attribution scores (explanations)
capturing the influence of each of the input features on model predictions.
More specifically, we construct automated natural language rationales that
embed insights from post hoc explanations to provide corrective signals to
LLMs. Extensive experimentation with real-world datasets demonstrates that our
framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%
over a wide range of tasks, including those where prior approaches which rely
on human-annotated rationales such as Chain-of-Thought prompting fall short.
Our work makes one of the first attempts at highlighting the potential of post
hoc explanations as valuable tools for enhancing the effectiveness of LLMs.
Furthermore, we conduct additional empirical analyses and ablation studies to
demonstrate the impact of each of the components of AMPLIFY, which, in turn,
leads to critical insights for refining in-context learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00245">From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1">Peter Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1">Mandar Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_J/0/1/0/all/0/1">James Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1">Panupong Pasupat</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1">Urvashi Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kenton Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1">Kristina Toutanova</a></p>
<p>Much of the previous work towards digital agents for graphical user
interfaces (GUIs) has relied on text-based representations (derived from HTML
or other structured data sources), which are not always readily available.
These input representations have been often coupled with custom, task-specific
action spaces. This paper focuses on creating agents that interact with the
digital world using the same conceptual interface that humans commonly use --
via pixel-based screenshots and a generic action space corresponding to
keyboard and mouse actions. Building upon recent progress in pixel-based
pretraining, we show, for the first time, that it is possible for such agents
to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based
instruction following tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09778">Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rajabi_N/0/1/0/all/0/1">Navid Rajabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1">Jana Kosecka</a></p>
<p>With pre-training of vision-and-language models (VLMs) on large-scale
datasets of image-text pairs, several recent works showed that these
pre-trained models lack fine-grained understanding, such as the ability to
count and recognize verbs, attributes, or relationships. The focus of this work
is to study the ability of these models to understand spatial relations.
Previously, this has been tackled using image-text matching (e.g., Visual
Spatial Reasoning benchmark) or visual question answering (e.g., GQA or VQAv2),
both showing poor performance and a large gap compared to human performance. In
this work, we use explainability tools to understand the causes of poor
performance better and present an alternative fine-grained, compositional
approach for ranking spatial clauses. We combine the evidence from grounding
noun phrases corresponding to objects and their locations to compute the final
rank of the spatial clause. We demonstrate the approach on representative VLMs
(such as LXMERT, GPV, and MDETR) and compare and highlight their abilities to
reason about spatial relationships.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01860">Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hakim_Z/0/1/0/all/0/1">Zaber Ibn Abdul Hakim</a>, <a href="http://arxiv.org/find/cs/1/au:+Swargo_R/0/1/0/all/0/1">Rasman Mubtasim Swargo</a>, <a href="http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1">Muhammad Abdullah Adnan</a></p>
<p>In this paper, we devise a mechanism for the addition of multi-modal
information with an existing pipeline for continuous sign language recognition
and translation. In our procedure, we have incorporated optical flow
information with RGB images to enrich the features with movement-related
information. This work studies the feasibility of such modality inclusion using
a cross-modal encoder. The plugin we have used is very lightweight and doesn't
need to include a separate feature extractor for the new modality in an
end-to-end manner. We have applied the changes in both sign language
recognition and translation, improving the result in each case. We have
evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language
recognition and the RWTH-PHOENIX-2014T dataset for translation. On the
recognition task, our approach reduced the WER by 0.9, and on the translation
task, our approach increased most of the BLEU scores by ~0.6 on the test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03409">Large Language Models as Optimizers. (arXiv:2309.03409v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chengrun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuezhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yifeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hanxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a></p>
<p>Optimization is ubiquitous. While derivative-based algorithms have been
powerful tools for various problems, the absence of gradient imposes challenges
on many real-world applications. In this work, we propose Optimization by
PROmpting (OPRO), a simple and effective approach to leverage large language
models (LLMs) as optimizers, where the optimization task is described in
natural language. In each optimization step, the LLM generates new solutions
from the prompt that contains previously generated solutions with their values,
then the new solutions are evaluated and added to the prompt for the next
optimization step. We first showcase OPRO on linear regression and traveling
salesman problems, then move on to prompt optimization where the goal is to
find instructions that maximize the task accuracy. With a variety of LLMs, we
demonstrate that the best prompts optimized by OPRO outperform human-designed
prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at
https://github.com/google-deepmind/opro.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07311">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Angelica Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shwartz_Ziv_R/0/1/0/all/0/1">Ravid Shwartz-Ziv</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Leavitt_M/0/1/0/all/0/1">Matthew L. Leavitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1">Naomi Saphra</a></p>
<p>Most interpretability research in NLP focuses on understanding the behavior
and features of a fully trained model. However, certain insights into model
behavior may only be accessible by observing the trajectory of the training
process. We present a case study of syntax acquisition in masked language
models (MLMs) that demonstrates how analyzing the evolution of interpretable
artifacts throughout training deepens our understanding of emergent behavior.
In particular, we study Syntactic Attention Structure (SAS), a naturally
emerging property of MLMs wherein specific Transformer heads tend to focus on
specific syntactic relations. We identify a brief window in pretraining when
models abruptly acquire SAS, concurrent with a steep drop in loss. This
breakthrough precipitates the subsequent acquisition of linguistic
capabilities. We then examine the causal role of SAS by manipulating SAS during
training, and demonstrate that SAS is necessary for the development of
grammatical capabilities. We further find that SAS competes with other
beneficial traits during training, and that briefly suppressing SAS improves
model quality. These findings offer an interpretation of a real-world example
of both simplicity bias and breakthrough training dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01441">UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1">Hejia Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Boxun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a></p>
<p>Large Language Models (LLMs) have demonstrated impressive inferential
capabilities, with numerous research endeavors devoted to enhancing this
capacity through prompting. Despite these efforts, a unified epistemological
foundation is still conspicuously absent. Drawing inspiration from Kant's a
priori philosophy, we propose the UPAR prompting framework, designed to emulate
the structure of human cognition within LLMs. The UPAR framework is delineated
into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the
extraction of structured information from complex contexts, prior planning of
solutions, execution according to plan, and self-reflection. This structure
significantly augments the explainability and accuracy of LLM inference,
producing a human-understandable and inspectable inferential trajectory.
Furthermore, our work offers an epistemological foundation for existing
prompting techniques, allowing for a possible systematic integration of these
methods. With GPT-4, our approach elevates the accuracy from COT baseline of
22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in
the causal judgment task. Without using few-shot examples or external tools,
UPAR significantly outperforms existing prompting methods on SCIBENCH, a
challenging dataset containing collegiate-level mathematics, chemistry, and
physics scientific problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05140">Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements. (arXiv:2310.05140v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yushan Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei-Nan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a></p>
<p>Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05470">Generative Judge for Evaluating Alignment. (arXiv:2310.05470v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junlong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Weizhe Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Run-Ze Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengfei Liu</a></p>
<p>The rapid development of Large Language Models (LLMs) has substantially
expanded the range of tasks they can address. In the field of Natural Language
Processing (NLP), researchers have shifted their focus from conventional NLP
tasks (e.g., sequence tagging and parsing) towards tasks that revolve around
aligning with human needs (e.g., brainstorming and email writing). This shift
in task distribution imposes new requirements on evaluating these aligned
models regarding generality (i.e., assessing performance across diverse
scenarios), flexibility (i.e., examining under different protocols), and
interpretability (i.e., scrutinizing models with explanations). In this paper,
we propose a generative judge with 13B parameters, Auto-J, designed to address
these challenges. Our model is trained on user queries and LLM-generated
responses under massive real-world scenarios and accommodates diverse
evaluation protocols (e.g., pairwise response comparison and single-response
evaluation) with well-structured natural language critiques. To demonstrate the
efficacy of our approach, we construct a new testbed covering 58 different
scenarios. Experimentally, Auto-J outperforms a series of strong competitors,
including both open-source and closed-source models, by a large margin. We also
provide detailed analysis and case studies to further reveal the potential of
our method and make a variety of resources public at
https://github.com/GAIR-NLP/auto-j.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11616">Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilic_D/0/1/0/all/0/1">David Ili&#x107;</a></p>
<p>This study uncovers the factor of general intelligence, or g, in language
models, extending the psychometric theory traditionally applied to humans and
certain animal species. Utilizing factor analysis on two extensive datasets -
Open LLM Leaderboard with 1,232 models and General Language Understanding
Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for
a unidimensional, highly stable g factor that accounts for 85% of the variance
in model performance. The study also finds a moderate correlation of .49
between model size and g. The discovery of g in language models offers a
unified metric for model evaluation and opens new avenues for more robust,
g-based model ability assessment. These findings lay the foundation for
understanding and future research on artificial general intelligence from a
psychometric perspective and have practical implications for model evaluation
and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16427">PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. (arXiv:2310.16427v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1">Fan Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haotian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiayou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1">Nebojsa Jojic</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric P. Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiting Hu</a></p>
<p>Highly effective, task-specific prompts are often heavily engineered by
experts to integrate detailed instructions and domain insights based on a deep
understanding of both instincts of large language models (LLMs) and the
intricacies of the target task. However, automating the generation of such
expert-level prompts remains elusive. Existing prompt optimization methods tend
to overlook the depth of domain knowledge and struggle to efficiently explore
the vast space of expert-level prompts. Addressing this, we present
PromptAgent, an optimization method that autonomously crafts prompts equivalent
in quality to those handcrafted by experts. At its core, PromptAgent views
prompt optimization as a strategic planning problem and employs a principled
planning algorithm, rooted in Monte Carlo tree search, to strategically
navigate the expert-level prompt space. Inspired by human-like trial-and-error
exploration, PromptAgent induces precise expert-level insights and in-depth
instructions by reflecting on model errors and generating constructive error
feedback. Such a novel framework allows the agent to iteratively examine
intermediate prompts (states), refine them based on error feedbacks (actions),
simulate future rewards, and search for high-reward paths leading to expert
prompts. We apply PromptAgent to 12 tasks spanning three practical domains:
BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing
it significantly outperforms strong Chain-of-Thought and recent prompt
optimization baselines. Extensive analyses emphasize its capability to craft
expert-level, detailed, and domain-insightful prompts with great efficiency and
generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00502">Efficient LLM Inference on CPUs. (arXiv:2311.00502v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haihao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Hanwen Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bo Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yu Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Hengyu Meng</a></p>
<p>Large language models (LLMs) have demonstrated remarkable performance and
tremendous potential across a wide range of tasks. However, deploying these
models has been challenging due to the astronomical amount of model parameters,
which requires a demand for large memory capacity and high memory bandwidth. In
this paper, we propose an effective approach that can make the deployment of
LLMs more efficiently. We support an automatic INT4 weight-only quantization
flow and design a special LLM runtime with highly-optimized kernels to
accelerate the LLM inference on CPUs. We demonstrate the general applicability
of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase
the extreme inference efficiency on CPUs. The code is publicly available at:
https://github.com/intel/intel-extension-for-transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05876">Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications. (arXiv:2311.05876v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhangyin Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Weitao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Weijiang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haotian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qianglong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Weihua Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiaocheng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bing Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+liu_T/0/1/0/all/0/1">Ting liu</a></p>
<p>Large language models (LLMs) exhibit superior performance on various natural
language tasks, but they are susceptible to issues stemming from outdated data
and domain-specific limitations. In order to address these challenges,
researchers have pursued two primary strategies, knowledge editing and
retrieval augmentation, to enhance LLMs by incorporating external information
from different aspects. Nevertheless, there is still a notable absence of a
comprehensive survey. In this paper, we propose a review to discuss the trends
in integration of knowledge and large language models, including taxonomy of
methods, benchmarks, and applications. In addition, we conduct an in-depth
analysis of different methods and point out potential research directions in
the future. We hope this survey offers the community quick access and a
comprehensive overview of this research area, with the intention of inspiring
future research endeavors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16733">LLMs for Science: Usage for Code Generation and Data Analysis. (arXiv:2311.16733v3 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nejjar_M/0/1/0/all/0/1">Mohamed Nejjar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zacharias_L/0/1/0/all/0/1">Luca Zacharias</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiehle_F/0/1/0/all/0/1">Fabian Stiehle</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1">Ingo Weber</a></p>
<p>Large language models (LLMs) have been touted to enable increased
productivity in many areas of today's work life. Scientific research as an area
of work is no exception: the potential of LLM-based tools to assist in the
daily work of scientists has become a highly discussed topic across
disciplines. However, we are only at the very onset of this subject of study.
It is still unclear how the potential of LLMs will materialise in research
practice. With this study, we give first empirical evidence on the use of LLMs
in the research process. We have investigated a set of use cases for LLM-based
tools in scientific research, and conducted a first study to assess to which
degree current tools are helpful. In this paper we report specifically on use
cases related to software engineering, such as generating application code and
developing scripts for data analytics. While we studied seemingly simple use
cases, results across tools differ significantly. Our results highlight the
promise of LLM-based tools in general, yet we also observe various issues,
particularly regarding the integrity of the output these tools provide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00168">Navigating News Narratives: A Media Bias Analysis Dataset. (arXiv:2312.00168v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a></p>
<p>The proliferation of biased news narratives across various media platforms
has become a prominent challenge, influencing public opinion on critical topics
like politics, health, and climate change. This paper introduces the
"Navigating News Narratives: A Media Bias Analysis Dataset", a comprehensive
dataset to address the urgent need for tools to detect and analyze media bias.
This dataset encompasses a broad spectrum of biases, making it a unique and
valuable asset in the field of media studies and artificial intelligence. The
dataset is available at
https://huggingface.co/datasets/newsmediabias/news-bias-full-data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00273">Mark My Words: Analyzing and Evaluating Language Model Watermarks. (arXiv:2312.00273v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piet_J/0/1/0/all/0/1">Julien Piet</a>, <a href="http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1">Chawin Sitawarin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_V/0/1/0/all/0/1">Vivian Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1">Norman Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1">David Wagner</a></p>
<p>The capabilities of large language models have grown significantly in recent
years and so too have concerns about their misuse. In this context, the ability
to distinguish machine-generated text from human-authored content becomes
important. Prior works have proposed numerous schemes to watermark text, which
would benefit from a systematic evaluation framework. This work focuses on text
watermarking techniques - as opposed to image watermarks - and proposes
MARKMYWORDS, a comprehensive benchmark for them under different tasks as well
as practical attacks. We focus on three main metrics: quality, size (e.g. the
number of tokens needed to detect a watermark), and tamper-resistance. Current
watermarking techniques are good enough to be deployed: Kirchenbauer et al. [1]
can watermark Llama2-7B-chat with no perceivable loss in quality, the watermark
can be detected with fewer than 100 tokens, and the scheme offers good
tamper-resistance to simple attacks. We argue that watermark
indistinguishability, a criteria emphasized in some prior works, is too strong
a requirement: schemes that slightly modify logit distributions outperform
their indistinguishable counterparts with no noticeable loss in generation
quality. We publicly release our benchmark
(https://github.com/wagner-group/MarkMyWords)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02931">WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words. (arXiv:2312.02931v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lukas Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuckute_G/0/1/0/all/0/1">Greta Tuckute</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1">Klemen Kotar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_E/0/1/0/all/0/1">Eghbal Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Regev_T/0/1/0/all/0/1">Tamar Regev</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1">Ethan Wilcox</a>, <a href="http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1">Alex Warstadt</a></p>
<p>Training on multiple modalities of input can augment the capabilities of a
language model. Here, we ask whether such a training regime can improve the
quality and efficiency of these systems as well. We focus on text--audio and
introduce Whisbert, which is inspired by the text--image approach of FLAVA
(Singh et al., 2022). In accordance with Babylm guidelines (Warstadt et al.,
2023), we pretrain Whisbert on a dataset comprising only 100 million words plus
their corresponding speech from the word-aligned version of the People's Speech
dataset (Galvez et al., 2021). To assess the impact of multimodality, we
compare versions of the model that are trained on text only and on both audio
and text simultaneously. We find that while Whisbert is able to perform well on
multimodal masked modeling and surpasses the Babylm baselines in most benchmark
tasks, it struggles to optimize its complex objective and outperform its
text-only Whisbert baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03549">Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment. (arXiv:2312.03549v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Shuang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_N/0/1/0/all/0/1">Ning Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fangyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1">Ke Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jiezhong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1">Aimin Pan</a></p>
<p>Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated
remarkable accuracy in a wide range of tasks. However, training these models
can incur significant expenses, often requiring tens of thousands of GPUs for
months of continuous operation. Typically, this training is carried out in
specialized GPU clusters equipped with homogeneous high-speed Remote Direct
Memory Access (RDMA) network interface cards (NICs). The acquisition and
maintenance of such dedicated clusters is challenging. Current LLM training
frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on
optimizing training within homogeneous cluster settings. In this paper, we
introduce Holmes, a training framework for LLMs that employs thoughtfully
crafted data and model parallelism strategies over the heterogeneous NIC
environment. Our primary technical contribution lies in a novel scheduling
method that intelligently allocates distinct computational tasklets in LLM
training to specific groups of GPU devices based on the characteristics of
their connected NICs. Furthermore, our proposed framework, utilizing pipeline
parallel techniques, demonstrates scalability to multiple GPU clusters, even in
scenarios without high-speed interconnects between nodes in distinct clusters.
We conducted comprehensive experiments that involved various scenarios in the
heterogeneous NIC environment. In most cases, our framework achieves
performance levels close to those achievable with homogeneous RDMA-capable
networks (InfiniBand or RoCE), significantly exceeding training efficiency
within the pure Ethernet environment. Additionally, we verified that our
framework outperforms other mainstream LLM frameworks under heterogeneous NIC
environment in terms of training efficiency and can be seamlessly integrated
with them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03699">PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration. (arXiv:2312.03699v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heierli_J/0/1/0/all/0/1">Jasmin Heierli</a>, <a href="http://arxiv.org/find/cs/1/au:+Meisterhans_M/0/1/0/all/0/1">Max Meisterhans</a>, <a href="http://arxiv.org/find/cs/1/au:+Moser_A/0/1/0/all/0/1">Adrian Moser</a>, <a href="http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1">Andri F&#xe4;rber</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolata_M/0/1/0/all/0/1">Mateusz Dolata</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavagnin_E/0/1/0/all/0/1">Elena Gavagnin</a>, <a href="http://arxiv.org/find/cs/1/au:+Spindler_A/0/1/0/all/0/1">Alexandre de Spindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwabe_G/0/1/0/all/0/1">Gerhard Schwabe</a></p>
<p>The advent of increasingly powerful language models has raised expectations
for language-based interactions. However, controlling these models is a
challenge, emphasizing the need to be able to investigate the feasibility and
value of their application. We present PROMISE, a framework that facilitates
the development of complex language-based interactions with information
systems. Its use of state machine modeling concepts enables model-driven,
dynamic prompt orchestration across hierarchically nested states and
transitions. This improves the control of the behavior of language models and
thus enables their effective and efficient use. We show the benefits of PROMISE
in the context of application scenarios within health information systems and
demonstrate its ability to handle complex interactions.
</p>
</p>
</div>

    </div>
    </body>
    