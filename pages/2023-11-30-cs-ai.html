<!DOCTYPE html>
<html>
<head>
<title>2023-11-30-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.16109">Transfer Learning between Motor Imagery Datasets using Deep Learning -- Validation of Framework and Comparison of Datasets. (arXiv:2311.16109v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guetschel_P/0/1/0/all/0/1">Pierre Guetschel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tangermann_M/0/1/0/all/0/1">Michael Tangermann</a></p>
<p>We present a simple deep learning-based framework commonly used in computer
vision and demonstrate its effectiveness for cross-dataset transfer learning in
mental imagery decoding tasks that are common in the field of Brain-Computer
Interfaces (BCI). We investigate, on a large selection of 12 motor-imagery
datasets, which ones are well suited for transfer, both as donors and as
receivers. Challenges. Deep learning models typically require long training
times and are data-hungry, which impedes their use for BCI systems that have to
minimize the recording time for (training) examples and are subject to
constraints induced by experiments involving human subjects. A solution to both
issues is transfer learning, but it comes with its own challenge, i.e.,
substantial data distribution shifts between datasets, subjects and even
between subsequent sessions of the same subject. Approach. For every pair of
pre-training (donor) and test (receiver) dataset, we first train a model on the
donor before training merely an additional new linear classification layer
based on a few receiver trials. Performance of this transfer approach is then
tested on other trials of the receiver dataset. Significance. First, we lower
the threshold to use transfer learning between motor imagery datasets: the
overall framework is extremely simple and nevertheless obtains decent
classification scores. Second, we demonstrate that deep learning models are a
good option for motor imagery cross-dataset transfer both for the reasons
outlined in the first point and because the framework presented is viable in
online scenarios. Finally, analysing which datasets are best suited for
transfer learning can be used as a reference for future researchers to
determine which to use for pre-training or benchmarking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16111">Deep Explainability: Spin-Geometrical Neural Meta-Structures. (arXiv:2311.16111v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Karamintziou_S/0/1/0/all/0/1">Sofia Karamintziou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Meditskos_G/0/1/0/all/0/1">Georgios Meditskos</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ntioudis_D/0/1/0/all/0/1">Dimos Ntioudis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mavropoulos_T/0/1/0/all/0/1">Thanassis Mavropoulos</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Vrochidis_S/0/1/0/all/0/1">Stefanos Vrochidis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ioannis/0/1/0/all/0/1">Ioannis</a> (Yiannis) <a href="http://arxiv.org/find/q-bio/1/au:+Kompatsiaris/0/1/0/all/0/1">Kompatsiaris</a></p>
<p>We face up to the challenge of explainability in multimodal artificial
intelligence. At the nexus of neuroscience-inspired and quantum computing,
interpretable and transparent spin-geometrical meta-architectures for early
fusion of large-scale, heterogeneous, graph-structured data are envisioned,
harnessing recent evidence for relativistic quantum neural coding of
(co-)behavioral states in the self-organizing brain, under competitive,
multidimensional dynamics. The designs draw on a self-dual classical
description - via special Clifford-Lipschitz operations - of spinorial quantum
states within registers of at most 16 qubits for efficient encoding of
exponentially large neural structures. Formally 'trained', Lorentz neural
architectures with precisely one lateral layer of exclusively inhibitory
interneurons accounting for anti-modalities, as well as their co-architectures
with intra-layer connections are highlighted. In principle, the approach
accommodates the fusion of up to 16 time-invariant interconnected
(anti-)modalities and the explicit recognition of underlying multidimensional
patterns. Comprehensive insights are expected to be gained through applications
to multimodal big data, under real-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16112">Co-learning synaptic delays, weights and adaptation in spiking neural networks. (arXiv:2311.16112v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deckers_L/0/1/0/all/0/1">Lucas Deckers</a>, <a href="http://arxiv.org/find/cs/1/au:+Damme_L/0/1/0/all/0/1">Laurens Van Damme</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ing Jyh Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leekwijck_W/0/1/0/all/0/1">Werner Van Leekwijck</a>, <a href="http://arxiv.org/find/cs/1/au:+Latre_S/0/1/0/all/0/1">Steven Latr&#xe9;</a></p>
<p>Spiking neural networks (SNN) distinguish themselves from artificial neural
networks (ANN) because of their inherent temporal processing and spike-based
computations, enabling a power-efficient implementation in neuromorphic
hardware. In this paper, we demonstrate that data processing with spiking
neurons can be enhanced by co-learning the connection weights with two other
biologically inspired neuronal features: 1) a set of parameters describing
neuronal adaptation processes and 2) synaptic propagation delays. The former
allows the spiking neuron to learn how to specifically react to incoming spikes
based on its past. The trained adaptation parameters result in neuronal
heterogeneity, which is found in the brain and also leads to a greater variety
in available spike patterns. The latter enables to learn to explicitly
correlate patterns that are temporally distanced. Synaptic delays reflect the
time an action potential requires to travel from one neuron to another. We show
that each of the co-learned features separately leads to an improvement over
the baseline SNN and that the combination of both leads to state-of-the-art SNN
results on all speech recognition datasets investigated with a simple 2-hidden
layer feed-forward network. Our SNN outperforms the ANN on the neuromorpic
datasets (Spiking Heidelberg Digits and Spiking Speech Commands), even with
fewer trainable parameters. On the 35-class Google Speech Commands dataset, our
SNN also outperforms a GRU of similar size. Our work presents brain-inspired
improvements to SNN that enable them to excel over an equivalent ANN of similar
size on tasks with rich temporal dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16114">Learning Noise-Robust Joint Representation for Multimodal Emotion Recognition under Realistic Incomplete Data Scenarios. (arXiv:2311.16114v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1">Qi Fan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zuo_H/0/1/0/all/0/1">Haolin Zuo</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rui Liu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zheng Lian</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1">Guanglai Gao</a> (1) ((1) Inner Mongolia University, Hohhot, China, (2) Institute of Automation, Chinese Academy of Sciences, Beijing, China)</p>
<p>Multimodal emotion recognition (MER) in practical scenarios presents a
significant challenge due to the presence of incomplete data, such as missing
or noisy data. Traditional methods often discard missing data or replace it
with a zero vector, neglecting the availability issue of noisy data.
Consequently, these approaches are not fully applicable to realistic scenarios,
where both missing and noisy data are prevalent. To address this problem, we
propose a novel noise-robust MER model, named NMER, which effectively learns
robust multimodal joint representations from incomplete data containing noise.
Our approach incorporates two key components. First, we introduce a noise
scheduler that adjusts the type and level of noise in the training data,
emulating the characteristics of incomplete data in realistic scenarios.
Second, we employ a Variational AutoEncoder (VAE)-based NMER model to generate
robust multimodal joint representations from the noisy data, leveraging the
modality invariant feature. The experimental results on the benchmark dataset
IEMOCAP indicate the proposed NMER outperforms state-of-the-art MER systems.
The ablation results also confirm the effectiveness of the VAE structure. We
release our code at \href{https://github.com/WooyoohL/Noise-robust_MER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16115">AI and Democracy&#x27;s Digital Identity Crisis. (arXiv:2311.16115v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Shrey Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Spelliscy_C/0/1/0/all/0/1">Connor Spelliscy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vance_Law_S/0/1/0/all/0/1">Samuel Vance-Law</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_S/0/1/0/all/0/1">Scott Moore</a></p>
<p>AI-enabled tools have become sophisticated enough to allow a small number of
individuals to run disinformation campaigns of an unprecedented scale.
Privacy-preserving identity attestations can drastically reduce instances of
impersonation and make disinformation easy to identify and potentially hinder.
By understanding how identity attestations are positioned across the spectrum
of decentralization, we can gain a better understanding of the costs and
benefits of various attestations. In this paper, we discuss attestation types,
including governmental, biometric, federated, and web of trust-based, and
include examples such as e-Estonia, China's social credit system, Worldcoin,
OAuth, X (formerly Twitter), Gitcoin Passport, and EAS. We believe that the
most resilient systems create an identity that evolves and is connected to a
network of similarly evolving identities that verify one another. In this type
of system, each entity contributes its respective credibility to the
attestation process, creating a larger, more comprehensive set of attestations.
We believe these systems could be the best approach to authenticating identity
and protecting against some of the threats to democracy that AI can pose in the
hands of malicious actors. However, governments will likely attempt to mitigate
these risks by implementing centralized identity authentication systems; these
centralized systems could themselves pose risks to the democratic processes
they are built to defend. We therefore recommend that policymakers support the
development of standards-setting organizations for identity, provide legal
clarity for builders of decentralized tooling, and fund research critical to
effective identity authentication systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16119">Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition. (arXiv:2311.16119v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schulhoff_S/0/1/0/all/0/1">Sander Schulhoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1">Jeremy Pinto</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Anaum Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchard_L/0/1/0/all/0/1">Louis-Fran&#xe7;ois Bouchard</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1">Chenglei Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Anati_S/0/1/0/all/0/1">Svetlina Anati</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagliabue_V/0/1/0/all/0/1">Valen Tagliabue</a>, <a href="http://arxiv.org/find/cs/1/au:+Kost_A/0/1/0/all/0/1">Anson Liu Kost</a>, <a href="http://arxiv.org/find/cs/1/au:+Carnahan_C/0/1/0/all/0/1">Christopher Carnahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1">Jordan Boyd-Graber</a></p>
<p>Large Language Models (LLMs) are increasingly being deployed in interactive
contexts that involve direct user engagement, such as chatbots and writing
assistants. These deployments are increasingly plagued by prompt injection and
jailbreaking (collectively, prompt hacking), in which models are manipulated to
ignore their original instructions and instead follow potentially malicious
ones. Although widely acknowledged as a significant security threat, there is a
dearth of large-scale resources and quantitative studies on prompt hacking. To
address this lacuna, we launch a global prompt hacking competition, which
allows for free-form human input attacks. We elicit 600K+ adversarial prompts
against three state-of-the-art LLMs. We describe the dataset, which empirically
verifies that current LLMs can indeed be manipulated via prompt hacking. We
also present a comprehensive taxonomical ontology of the types of adversarial
prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16122">Semantic Generative Augmentations for Few-Shot Counting. (arXiv:2311.16122v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doubinsky_P/0/1/0/all/0/1">Perla Doubinsky</a> (CEDRIC - VERTIGO, CNAM), <a href="http://arxiv.org/find/cs/1/au:+Audebert_N/0/1/0/all/0/1">Nicolas Audebert</a> (CEDRIC - VERTIGO, CNAM), <a href="http://arxiv.org/find/cs/1/au:+Crucianu_M/0/1/0/all/0/1">Michel Crucianu</a> (CEDRIC - VERTIGO), <a href="http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1">Herv&#xe9; Le Borgne</a> (CEA)</p>
<p>With the availability of powerful text-to-image diffusion models, recent
works have explored the use of synthetic data to improve image classification
performances. These works show that it can effectively augment or even replace
real data. In this work, we investigate how synthetic data can benefit few-shot
class-agnostic counting. This requires to generate images that correspond to a
given input number of objects. However, text-to-image models struggle to grasp
the notion of count. We propose to rely on a double conditioning of Stable
Diffusion with both a prompt and a density map in order to augment a training
dataset for few-shot counting. Due to the small dataset size, the fine-tuned
model tends to generate images close to the training images. We propose to
enhance the diversity of synthesized images by exchanging captions between
images thus creating unseen configurations of object types and spatial layout.
Our experiments show that our diversified generation strategy significantly
improves the counting accuracy of two recent and performing few-shot counting
models on FSC147 and CARPK.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16124">DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification. (arXiv:2311.16124v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Mintong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Diffusion-based purification defenses leverage diffusion models to remove
crafted perturbations of adversarial examples and achieve state-of-the-art
robustness. Recent studies show that even advanced attacks cannot break such
defenses effectively, since the purification process induces an extremely deep
computational graph which poses the potential problem of gradient obfuscation,
high memory cost, and unbounded randomness. In this paper, we propose a unified
framework DiffAttack to perform effective and efficient attacks against
diffusion-based purification defenses, including both DDPM and score-based
approaches. In particular, we propose a deviated-reconstruction loss at
intermediate diffusion steps to induce inaccurate density gradient estimation
to tackle the problem of vanishing/exploding gradients. We also provide a
segment-wise forwarding-backwarding algorithm, which leads to memory-efficient
gradient backpropagation. We validate the attack effectiveness of DiffAttack
compared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that
DiffAttack decreases the robust accuracy of models compared with SOTA attacks
by over 20% on CIFAR-10 under $\ell_\infty$ attack $(\epsilon=8/255)$, and over
10% on ImageNet under $\ell_\infty$ attack $(\epsilon=4/255)$. We conduct a
series of ablations studies, and we find 1) DiffAttack with the
deviated-reconstruction loss added over uniformly sampled time steps is more
effective than that added over only initial/final steps, and 2) diffusion-based
purification with a moderate diffusion length is more robust under DiffAttack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16133">Effective Quantization for Diffusion Models on CPUs. (arXiv:2311.16133v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Hanwen Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haihao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yiyang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xinyu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenzhong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wenhua Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1">Kaokao Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yintong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Heng Guo</a></p>
<p>Diffusion models have gained popularity for generating images from textual
descriptions. Nonetheless, the substantial need for computational resources
continues to present a noteworthy challenge, contributing to time-consuming
processes. Quantization, a technique employed to compress deep learning models
for enhanced efficiency, presents challenges when applied to diffusion models.
These models are notably more sensitive to quantization compared to other model
types, potentially resulting in a degradation of image quality. In this paper,
we introduce a novel approach to quantize the diffusion models by leveraging
both quantization-aware training and distillation. Our results show the
quantized models can maintain the high image quality while demonstrating the
inference efficiency on CPUs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16136">ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach. (arXiv:2311.16136v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yuke Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Feng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>Over the past few years, Machine Learning-as-a-Service (MLaaS) has received a
surging demand for supporting Machine Learning-driven services to offer
revolutionized user experience across diverse application areas. MLaaS provides
inference service with low inference latency to application users based on an
ML model trained using a dataset collected from numerous individual data
owners. Recently, for the sake of data owners' privacy and to comply with the
"right to be forgotten (RTBF)" as enacted by data protection legislation, many
machine unlearning methods have been proposed to remove data owners' data from
trained models upon their unlearning requests. However, despite their promising
efficiency, almost all existing machine unlearning methods handle unlearning
requests in a manner that is independent of inference requests, which
unfortunately introduces new security and privacy vulnerabilities for machine
unlearning in MLaaS. In this paper, we propose the ERASER framework for machinE
unleaRning in MLaAS via an inferencE seRving-aware approach. ERASER proposes a
novel certified inference consistency mechanism that reduces inference latency
by selectively postponing unlearning execution incurred by unlearning requests
from data owners, while strictly adhering to the RTBF principle. ERASER offers
three groups of design choices to allow for tailor-made variants that best suit
the specific environments and preferences of different MLaaS systems. Extensive
empirical evaluations across various settings confirm ERASER's effectiveness,
e.g., it can effectively save up to 99% of inference latency and 31% of
computation overhead over the inference-oblivion baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16137">A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction. (arXiv:2311.16137v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Walker_N/0/1/0/all/0/1">Nicholas Thomas Walker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1">Stefan Ultes</a>, <a href="http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1">Pierre Lison</a></p>
<p>Knowledge graphs are often used to represent structured information in a
flexible and efficient manner, but their use in situated dialogue remains
under-explored. This paper presents a novel conversational model for
human--robot interaction that rests upon a graph-based representation of the
dialogue state. The knowledge graph representing the dialogue state is
continuously updated with new observations from the robot sensors, including
linguistic, situated and multimodal inputs, and is further enriched by other
modules, in particular for spatial understanding. The neural conversational
model employed to respond to user utterances relies on a simple but effective
graph-to-text mechanism that traverses the dialogue state graph and converts
the traversals into a natural language form. This conversion of the state graph
into text is performed using a set of parameterized functions, and the values
for those parameters are optimized based on a small set of Wizard-of-Oz
interactions. After this conversion, the text representation of the dialogue
state graph is included as part of the prompt of a large language model used to
decode the agent response. The proposed approach is empirically evaluated
through a user study with a humanoid robot that acts as conversation partner to
evaluate the impact of the graph-to-text mechanism on the response generation.
After moving a robot along a tour of an indoor environment, participants
interacted with the robot using spoken dialogue and evaluated how well the
robot was able to answer questions about what the robot observed during the
tour. User scores show a statistically significant improvement in the perceived
factuality of the robot responses when the graph-to-text approach is employed,
compared to a baseline using inputs structured as semantic triples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16138">After-Stroke Arm Paresis Detection using Kinematic Data. (arXiv:2311.16138v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1">Kenneth Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Almekhlafi_M/0/1/0/all/0/1">Mohammed Almekhlafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yanushkevich_S/0/1/0/all/0/1">Svetlana Yanushkevich</a></p>
<p>This paper presents an approach for detecting unilateral arm
paralysis/weakness using kinematic data. Our method employs temporal
convolution networks and recurrent neural networks, guided by knowledge
distillation, where we use inertial measurement units attached to the body to
capture kinematic information such as acceleration, rotation, and flexion of
body joints during an action. This information is then analyzed to recognize
body actions and patterns. Our proposed network achieves a high paretic
detection accuracy of 97.99\%, with an action classification accuracy of
77.69\%, through knowledge sharing. Furthermore, by incorporating causal
reasoning, we can gain additional insights into the patient's condition, such
as their Fugl-Meyer assessment score or impairment level based on the machine
learning result. Overall, our approach demonstrates the potential of using
kinematic data and machine learning for detecting arm paralysis/weakness. The
results suggest that our method could be a useful tool for clinicians and
healthcare professionals working with patients with this condition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16140">Adapting Segment Anything Model (SAM) through Prompt-based Learning for Enhanced Protein Identification in Cryo-EM Micrographs. (arXiv:2311.16140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1">Fei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Mingyue Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1">Biplab Poudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhas_N/0/1/0/all/0/1">Newgin Sam Ebin Sam Dhas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyawali_R/0/1/0/all/0/1">Rajan Gyawali</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhakal_A/0/1/0/all/0/1">Ashwin Dhakal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jianlin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a></p>
<p>Cryo-electron microscopy (cryo-EM) remains pivotal in structural biology, yet
the task of protein particle picking, integral for 3D protein structure
construction, is laden with manual inefficiencies. While recent AI tools such
as Topaz and crYOLO are advancing the field, they do not fully address the
challenges of cryo-EM images, including low contrast, complex shapes, and
heterogeneous conformations. This study explored prompt-based learning to adapt
the state-of-the-art image segmentation foundation model Segment Anything Model
(SAM) for cryo-EM. This focus was driven by the desire to optimize model
performance with a small number of labeled data without altering pre-trained
parameters, aiming for a balance between adaptability and foundational
knowledge retention. Through trials with three prompt-based learning
strategies, namely head prompt, prefix prompt, and encoder prompt, we observed
enhanced performance and reduced computational requirements compared to the
fine-tuning approach. This work not only highlights the potential of prompting
SAM in protein identification from cryo-EM micrographs but also suggests its
broader promise in biomedical image segmentation and object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16141">Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking Neural Networks. (arXiv:2311.16141v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Boxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1">Haihang You</a></p>
<p>Spiking Neural Networks (SNNs) have been an attractive option for deployment
on devices with limited computing resources and lower power consumption because
of the event-driven computing characteristic. As such devices have limited
computing and storage resources, pruning for SNNs has been widely focused
recently. However, the binary and non-differentiable property of spike signals
make pruning deep SNNs challenging, so existing methods require high time
overhead to make pruning decisions. In this paper, inspired by critical brain
hypothesis in neuroscience, we design a regeneration mechanism based on
criticality to efficiently obtain the critical pruned networks. Firstly, we
propose a low-cost metric for the criticality of pruning structures. Then we
re-rank the pruned structures after pruning and regenerate those with higher
criticality. We evaluate our method using VGG-16 and ResNet-19 for both
unstructured pruning and structured pruning. Our method achieves higher
performance compared to current state-of-the-art (SOTA) method with the same
time overhead. We also achieve comparable performances (even better on VGG-16)
compared to the SOTA method with 11.3x and 15.5x acceleration. Moreover, we
investigate underlying mechanism of our method and find that it efficiently
selects potential structures, learns the consistent feature representations and
reduces the overfitting during the recovery phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16143">Ransomware Detection and Classification using Machine Learning. (arXiv:2311.16143v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kunku_K/0/1/0/all/0/1">Kavitha Kunku</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaman_A/0/1/0/all/0/1">ANK Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Vicious assaults, malware, and various ransomware pose a cybersecurity
threat, causing considerable damage to computer structures, servers, and mobile
and web apps across various industries and businesses. These safety concerns
are important and must be addressed immediately. Ransomware detection and
classification are critical for guaranteeing rapid reaction and prevention.
This study uses the XGBoost classifier and Random Forest (RF) algorithms to
detect and classify ransomware attacks. This approach involves analyzing the
behaviour of ransomware and extracting relevant features that can help
distinguish between different ransomware families. The models are evaluated on
a dataset of ransomware attacks and demonstrate their effectiveness in
accurately detecting and classifying ransomware. The results show that the
XGBoost classifier, Random Forest Classifiers, can effectively detect and
classify different ransomware attacks with high accuracy, thereby providing a
valuable tool for enhancing cybersecurity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16146">Emulators in JINSP. (arXiv:2311.16146v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Miaomiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1">Lv Zhe</a></p>
<p>JINSP(Jiutian Intelligence Network Simulation Platform) describes a series of
basic emulators and their combinations, such as the simulation of the protocol
stack for dynamic users in a real environment, which is composed of user
behavior simulation, base station simulation, and terminal simulation. It is
applied in specific business scenarios, such as multi-target antenna
optimization, compression feedback, and so on. This paper provides detailed
descriptions of each emulator and its combination based on this foundation,
including the implementation process of the emulator, integration with the
platform, experimental results, and other aspects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16153">Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications. (arXiv:2311.16153v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Fengqing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhangchen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Luyao Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinyuan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Poovendran_R/0/1/0/all/0/1">Radha Poovendran</a></p>
<p>Large language models (LLMs) are increasingly deployed as the service backend
for LLM-integrated applications such as code completion and AI-powered search.
LLM-integrated applications serve as middleware to refine users' queries with
domain-specific knowledge to better inform LLMs and enhance the responses.
Despite numerous opportunities and benefits, LLM-integrated applications also
introduce new attack surfaces. Understanding, minimizing, and eliminating these
emerging attack surfaces is a new area of research. In this work, we consider a
setup where the user and LLM interact via an LLM-integrated application in the
middle. We focus on the communication rounds that begin with user's queries and
end with LLM-integrated application returning responses to the queries, powered
by LLMs at the service backend. For this query-response protocol, we identify
potential vulnerabilities that can originate from the malicious application
developer or from an outsider threat initiator that is able to control the
database access, manipulate and poison data that are high-risk for the user.
Successful exploits of the identified vulnerabilities result in the users
receiving responses tailored to the intent of a threat initiator. We assess
such threats against LLM-integrated applications empowered by OpenAI GPT-3.5
and GPT-4. Our empirical results show that the threats can effectively bypass
the restrictions and moderation policies of OpenAI, resulting in users
receiving responses that contain bias, toxic content, privacy risk, and
disinformation. To mitigate those threats, we identify and define four key
properties, namely integrity, source identification, attack detectability, and
utility preservation, that need to be satisfied by a safe LLM-integrated
application. Based on these properties, we develop a lightweight,
threat-agnostic defense that mitigates both insider and outsider threats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16158">CarbNN: A Novel Active Transfer Learning Neural Network To Build De Novo Metal Organic Frameworks (MOFs) for Carbon Capture. (arXiv:2311.16158v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Redkar_N/0/1/0/all/0/1">Neel Redkar</a></p>
<p>Over the past decade, climate change has become an increasing problem with
one of the major contributing factors being carbon dioxide (CO2) emissions;
almost 51% of total US carbon emissions are from factories. Current materials
used in CO2 capture are lacking either in efficiency, sustainability, or cost.
</p>
<p>Electrocatalysis of CO2 is a new approach where CO2 can be reduced and the
components used industrially as fuel, saving transportation costs, creating
financial incentives. Metal Organic Frameworks (MOFs) are crystals made of
organo-metals that adsorb, filter, and electrocatalyze CO2. The current
available MOFs for capture &amp; electrocatalysis are expensive to manufacture and
inefficient at capture. The goal therefore is to computationally design a MOF
that can adsorb CO2 and catalyze carbon monoxide &amp; oxygen with low cost.
</p>
<p>A novel active transfer learning neural network was developed, utilizing
transfer learning due to limited available data on 15 MOFs. Using the Cambridge
Structural Database with 10,000 MOFs, the model used incremental mutations to
fit a trained fitness hyper-heuristic function. Eventually, a Selenium MOF
(C18MgO25Se11Sn20Zn5) was converged on. Through analysis of predictions &amp;
literature, the converged MOF was shown to be more effective &amp; more
synthetically accessible than existing MOFs, showing the model had an
understanding of effective electrocatalytic structures in the material space.
This novel network can be implemented for other gas separations and catalysis
applications that have limited training accessible datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16161">Vision Encoder-Decoder Models for AI Coaching. (arXiv:2311.16161v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nayak_J/0/1/0/all/0/1">Jyothi S Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Afifah Khan Mohammed Ajmal Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Manjeshwar_C/0/1/0/all/0/1">Chirag Manjeshwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Banday_I/0/1/0/all/0/1">Imadh Ajaz Banday</a></p>
<p>This research paper introduces an innovative AI coaching approach by
integrating vision-encoder-decoder models. The feasibility of this method is
demonstrated using a Vision Transformer as the encoder and GPT-2 as the
decoder, achieving a seamless integration of visual input and textual
interaction. Departing from conventional practices of employing distinct models
for image recognition and text-based coaching, our integrated architecture
directly processes input images, enabling natural question-and-answer dialogues
with the AI coach. This unique strategy simplifies model architecture while
enhancing the overall user experience in human-AI interactions. We showcase
sample results to demonstrate the capability of the model. The results
underscore the methodology's potential as a promising paradigm for creating
efficient AI coach models in various domains involving visual inputs.
Importantly, this potential holds true regardless of the particular visual
encoder or text decoder chosen. Additionally, we conducted experiments with
different sizes of GPT-2 to assess the impact on AI coach performance,
providing valuable insights into the scalability and versatility of our
proposed methodology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16162">Leveraging Artificial Intelligence Technology for Mapping Research to Sustainable Development Goals: A Case Study. (arXiv:2311.16162v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hui Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Aryani_A/0/1/0/all/0/1">Amir Aryani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_G/0/1/0/all/0/1">Gavin Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Marcus White</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvador_Carulla_L/0/1/0/all/0/1">Luis Salvador-Carulla</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1">Shazia Sadiq</a>, <a href="http://arxiv.org/find/cs/1/au:+Sojli_E/0/1/0/all/0/1">Elvira Sojli</a>, <a href="http://arxiv.org/find/cs/1/au:+Boddy_J/0/1/0/all/0/1">Jennifer Boddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Murray_G/0/1/0/all/0/1">Greg Murray</a>, <a href="http://arxiv.org/find/cs/1/au:+Tham_W/0/1/0/all/0/1">Wing Wah Tham</a></p>
<p>The number of publications related to the Sustainable Development Goals
(SDGs) continues to grow. These publications cover a diverse spectrum of
research, from humanities and social sciences to engineering and health. Given
the imperative of funding bodies to monitor outcomes and impacts, linking
publications to relevant SDGs is critical but remains time-consuming and
difficult given the breadth and complexity of the SDGs. A publication may
relate to several goals (interconnection feature of goals), and therefore
require multidisciplinary knowledge to tag accurately. Machine learning
approaches are promising and have proven particularly valuable for tasks such
as manual data labeling and text classification. In this study, we employed
over 82,000 publications from an Australian university as a case study. We
utilized a similarity measure to map these publications onto Sustainable
Development Goals (SDGs). Additionally, we leveraged the OpenAI GPT model to
conduct the same task, facilitating a comparative analysis between the two
approaches. Experimental results show that about 82.89% of the results obtained
by the similarity measure overlap (at least one tag) with the outputs of the
GPT model. The adopted model (similarity measure) can complement GPT model for
SDG classification. Furthermore, deep learning methods, which include the
similarity measure used here, are more accessible and trusted for dealing with
sensitive data without the use of commercial AI services or the deployment of
expensive computing resources to operate large language models. Our study
demonstrates how a crafted combination of the two methods can achieve reliable
results for mapping research to the SDGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16167">MMPDE-Net and Moving Sampling Physics-informed Neural Networks Based On Moving Mesh Method. (arXiv:2311.16167v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1">Yu Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1">Qihong Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1">Yangtao Deng</a>, <a href="http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1">Qiaolin He</a></p>
<p>In this work, we propose an end-to-end adaptive sampling neural network
(MMPDE-Net) based on the moving mesh PDE method, which can adaptively generate
new coordinates of sampling points by solving the moving mesh PDE. This model
focuses on improving the efficiency of individual sampling points. Moreover, we
have developed an iterative algorithm based on MMPDE-Net, which makes the
sampling points more precise and controllable. Since MMPDE-Net is a framework
independent of the deep learning solver, we combine it with PINN to propose
MS-PINN and demonstrate its effectiveness by performing error analysis under
the assumptions given in this paper. Meanwhile, we demonstrate the performance
improvement of MS-PINN compared to PINN through numerical experiments on four
typical examples to verify the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16171">Multi-Agent Learning of Efficient Fulfilment and Routing Strategies in E-Commerce. (arXiv:2311.16171v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shelke_O/0/1/0/all/0/1">Omkar Shelke</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathakota_P/0/1/0/all/0/1">Pranavi Pathakota</a>, <a href="http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1">Anandsingh Chauhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1">Harshad Khadilkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Meisheri_H/0/1/0/all/0/1">Hardik Meisheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1">Balaraman Ravindran</a></p>
<p>This paper presents an integrated algorithmic framework for minimising
product delivery costs in e-commerce (known as the cost-to-serve or C2S). One
of the major challenges in e-commerce is the large volume of spatio-temporally
diverse orders from multiple customers, each of which has to be fulfilled from
one of several warehouses using a fleet of vehicles. This results in two levels
of decision-making: (i) selection of a fulfillment node for each order
(including the option of deferral to a future time), and then (ii) routing of
vehicles (each of which can carry multiple orders originating from the same
warehouse). We propose an approach that combines graph neural networks and
reinforcement learning to train the node selection and vehicle routing agents.
We include real-world constraints such as warehouse inventory capacity, vehicle
characteristics such as travel times, service times, carrying capacity, and
customer constraints including time windows for delivery. The complexity of
this problem arises from the fact that outcomes (rewards) are driven both by
the fulfillment node mapping as well as the routing algorithms, and are
spatio-temporally distributed. Our experiments show that this algorithmic
pipeline outperforms pure heuristic policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16172">Evolutionary Machine Learning and Games. (arXiv:2311.16172v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1">Ahmed Khalifa</a>, <a href="http://arxiv.org/find/cs/1/au:+Earle_S/0/1/0/all/0/1">Sam Earle</a>, <a href="http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1">Michael Cerny Green</a>, <a href="http://arxiv.org/find/cs/1/au:+Soros_L/0/1/0/all/0/1">Lisa Soros</a></p>
<p>Evolutionary machine learning (EML) has been applied to games in multiple
ways, and for multiple different purposes. Importantly, AI research in games is
not only about playing games; it is also about generating game content,
modeling players, and many other applications. Many of these applications pose
interesting problems for EML. We will structure this chapter on EML for games
based on whether evolution is used to augment machine learning (ML) or ML is
used to augment evolution. For completeness, we also briefly discuss the usage
of ML and evolution separately in games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16173">Conditions for Length Generalization in Learning Reasoning Skills. (arXiv:2311.16173v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Changnan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bing Liu</a></p>
<p>Reasoning is a fundamental capability of AI agents. Recently, large language
models (LLMs) have shown remarkable abilities to perform reasoning tasks.
However, numerous evaluations of the reasoning capabilities of LLMs have also
showed some limitations. An outstanding limitation is length generalization,
meaning that when trained on reasoning problems of smaller lengths or sizes,
the resulting models struggle with problems of larger sizes or lengths. This
potentially indicates some theoretical limitations of generalization in
learning reasoning skills. These evaluations and their observations motivated
us to perform a theoretical study of the length generalization problem. This
work focused on reasoning tasks that can be formulated as Markov dynamic
processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and
proves conditions that decide whether the length generalization problem can be
solved or not for a reasoning task in a particular representation. Experiments
are also conducted to verify the theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16176">Shortcut Bias Mitigation via Ensemble Diversity Using Diffusion Probabilistic Models. (arXiv:2311.16176v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1">Luca Scimeca</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Alexander Rubinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1">Damien Teney</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seong Joon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1">Armand Mihai Nicolicioiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>Spurious correlations in the data, where multiple cues are predictive of the
target labels, often lead to a phenomenon known as simplicity bias, where a
model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In
this work, we propose an ensemble diversification framework exploiting
Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show
that at particular training intervals, DPMs can generate images with novel
feature combinations, even when trained on images displaying correlated input
features. We leverage this crucial property to generate synthetic
counterfactuals to increase model diversity via ensemble disagreement. We show
that DPM-guided diversification is sufficient to remove dependence on primary
shortcut cues, without a need for additional supervised signals. We further
empirically quantify its efficacy on several diversification objectives, and
finally show improved generalization and diversification performance on par
with prior work that relies on auxiliary data collection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16179">Next-gen traffic surveillance: AI-assisted mobile traffic violation detection system. (arXiv:2311.16179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dede_D/0/1/0/all/0/1">Dila Dede</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarsil_M/0/1/0/all/0/1">Mehmet Ali Sars&#x131;l</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1">Ata Shaker</a>, <a href="http://arxiv.org/find/cs/1/au:+Altintas_O/0/1/0/all/0/1">Olgu Alt&#x131;nta&#x15f;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ergen_O/0/1/0/all/0/1">Onur Ergen</a></p>
<p>Road traffic accidents pose a significant global public health concern,
leading to injuries, fatalities, and vehicle damage. Approximately 1,3 million
people lose their lives daily due to traffic accidents [World Health
Organization, 2022]. Addressing this issue requires accurate traffic law
violation detection systems to ensure adherence to regulations. The integration
of Artificial Intelligence algorithms, leveraging machine learning and computer
vision, has facilitated the development of precise traffic rule enforcement.
This paper illustrates how computer vision and machine learning enable the
creation of robust algorithms for detecting various traffic violations. Our
model, capable of identifying six common traffic infractions, detects red light
violations, illegal use of breakdown lanes, violations of vehicle following
distance, breaches of marked crosswalk laws, illegal parking, and parking on
marked crosswalks. Utilizing online traffic footage and a self-mounted on-dash
camera, we apply the YOLOv5 algorithm's detection module to identify traffic
agents such as cars, pedestrians, and traffic signs, and the strongSORT
algorithm for continuous interframe tracking. Six discrete algorithms analyze
agents' behavior and trajectory to detect violations. Subsequently, an
Identification Module extracts vehicle ID information, such as the license
plate, to generate violation notices sent to relevant authorities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16180">Aiming to Minimize Alcohol-Impaired Road Fatalities: Utilizing Fairness-Aware and Domain Knowledge-Infused Artificial Intelligence. (arXiv:2311.16180v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkateswaran_T/0/1/0/all/0/1">Tejas Venkateswaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1">Sheikh Rabiul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Md Golam Moula Mehedi Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Mohiuddin Ahmed</a></p>
<p>Approximately 30% of all traffic fatalities in the United States are
attributed to alcohol-impaired driving. This means that, despite stringent laws
against this offense in every state, the frequency of drunk driving accidents
is alarming, resulting in approximately one person being killed every 45
minutes. The process of charging individuals with Driving Under the Influence
(DUI) is intricate and can sometimes be subjective, involving multiple stages
such as observing the vehicle in motion, interacting with the driver, and
conducting Standardized Field Sobriety Tests (SFSTs). Biases have been observed
through racial profiling, leading to some groups and geographical areas facing
fewer DUI tests, resulting in many actual DUI incidents going undetected,
ultimately leading to a higher number of fatalities. To tackle this issue, our
research introduces an Artificial Intelligence-based predictor that is both
fairness-aware and incorporates domain knowledge to analyze DUI-related
fatalities in different geographic locations. Through this model, we gain
intriguing insights into the interplay between various demographic groups,
including age, race, and income. By utilizing the provided information to
allocate policing resources in a more equitable and efficient manner, there is
potential to reduce DUI-related fatalities and have a significant impact on
road safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16185">Enhancing Sentiment Analysis Results through Outlier Detection Optimization. (arXiv:2311.16185v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuetian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_M/0/1/0/all/0/1">Mei Si</a></p>
<p>When dealing with text data containing subjective labels like speaker
emotions, inaccuracies or discrepancies among labelers are not uncommon. Such
discrepancies can significantly affect the performance of machine learning
algorithms. This study investigates the potential of identifying and addressing
outliers in text data with subjective labels, aiming to enhance classification
outcomes. We utilized the Deep SVDD algorithm, a one-class classification
method, to detect outliers in nine text-based emotion and sentiment analysis
datasets. By employing both a small-sized language model (DistilBERT base model
with 66 million parameters) and non-deep learning machine learning algorithms
(decision tree, KNN, Logistic Regression, and LDA) as the classifier, our
findings suggest that the removal of outliers can lead to enhanced results in
most cases. Additionally, as outliers in such datasets are not necessarily
unlearnable, we experienced utilizing a large language model -- DeBERTa v3
large with 131 million parameters, which can capture very complex patterns in
data. We continued to observe performance enhancements across multiple
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16191">MACE: A Multi-pattern Accommodated and Efficient Anomaly Detection Method in the Frequency Domain. (arXiv:2311.16191v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feiyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+zhang_Y/0/1/0/all/0/1">Yingying zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lunting Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1">Renhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shuiguang Deng</a></p>
<p>Anomaly detection significantly enhances the robustness of cloud systems.
While neural network-based methods have recently demonstrated strong
advantages, they encounter practical challenges in cloud environments: the
contradiction between the impracticality of maintaining a unique model for each
service and the limited ability of dealing with diverse normal patterns by a
unified model, as well as issues with handling heavy traffic in real time and
short-term anomaly detection sensitivity. Thus, we propose MACE, a
Multi-pattern Accommodated and efficient Anomaly detection method in the
frequency domain for time series anomaly detection. There are three novel
characteristics of it: (i) a pattern extraction mechanism excelling at handling
diverse normal patterns, which enables the model to identify anomalies by
examining the correlation between the data sample and its service normal
pattern, instead of solely focusing on the data sample itself; (ii) a dualistic
convolution mechanism that amplifies short-term anomalies in the time domain
and hinders the reconstruction of anomalies in the frequency domain, which
enlarges the reconstruction error disparity between anomaly and normality and
facilitates anomaly detection; (iii) leveraging the sparsity and parallelism of
frequency domain to enhance model efficiency. We theoretically and
experimentally prove that using a strategically selected subset of Fourier
bases can not only reduce computational overhead but is also profit to
distinguish anomalies, compared to using the complete spectrum. Moreover,
extensive experiments demonstrate MACE's effectiveness in handling diverse
normal patterns with a unified model and it achieves state-of-the-art
performance with high efficiency. \end{abstract}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16192">Utilizing Multiple Inputs Autoregressive Models for Bearing Remaining Useful Life Prediction. (arXiv:2311.16192v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guanhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Guoxi Sun</a></p>
<p>Accurate prediction of the Remaining Useful Life (RUL) of rolling bearings is
crucial in industrial production, yet existing models often struggle with
limited generalization capabilities due to their inability to fully process all
vibration signal patterns. We introduce a novel multi-input autoregressive
model to address this challenge in RUL prediction for bearings. Our approach
uniquely integrates vibration signals with previously predicted Health
Indicator (HI) values, employing feature fusion to output current window HI
values. Through autoregressive iterations, the model attains a global receptive
field, effectively overcoming the limitations in generalization. Furthermore,
we innovatively incorporate a segmentation method and multiple training
iterations to mitigate error accumulation in autoregressive models. Empirical
evaluation on the PMH2012 dataset demonstrates that our model, compared to
other backbone networks using similar autoregressive approaches, achieves
significantly lower Root Mean Square Error (RMSE) and Score. Notably, it
outperforms traditional autoregressive models that use label values as inputs
and non-autoregressive networks, showing superior generalization abilities with
a marked lead in RMSE and Score metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16193">Students&#x27; interest in knowledge acquisition in Artificial Intelligence. (arXiv:2311.16193v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petrescu_M/0/1/0/all/0/1">Manuela-Andreea Petrescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pop_E/0/1/0/all/0/1">Emilia-Loredana Pop</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihoc_T/0/1/0/all/0/1">Tudor-Dan Mihoc</a></p>
<p>Some students' expectations and points of view related to the Artificial
Intelligence course are explored and analyzed in this study. We anonymous
collected answers from 58 undergraduate students out of 200 enrolled in the
Computer Science specialization. The answers were analysed and interpreted
using thematic analysis to find out their interests and attractive and
unattractive aspects related to the Artificial Intelligence study topic. We
concluded that students are interested in Artificial Intelligence due to its
trendiness, applicability, their passion and interest in the subject, the
potential for future growth, and high salaries. However, the students'
expectations were mainly related to achieving medium knowledge in the
Artificial Intelligence field, and men seem to be more interested in acquiring
high-level skills than women. The most common part that wasn't enjoyed by the
students was the mathematical aspect used in Artificial Intelligence. Some of
them (a small group) were also aware of the Artificial Intelligence potential
which could be used in an unethical manner for negative purposes. Our study
also provides a short comparison to the Databases course, in which students
were not that passionate or interested in achieving medium knowledge, their
interest was related to DB usage and basic information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16195">A Foundational Framework and Methodology for Personalized Early and Timely Diagnosis. (arXiv:2311.16195v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schubert_T/0/1/0/all/0/1">Tim Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Peck_R/0/1/0/all/0/1">Richard W Peck</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimson_A/0/1/0/all/0/1">Alexander Gimson</a>, <a href="http://arxiv.org/find/cs/1/au:+Davtyan_C/0/1/0/all/0/1">Camelia Davtyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Early diagnosis of diseases holds the potential for deep transformation in
healthcare by enabling better treatment options, improving long-term survival
and quality of life, and reducing overall cost. With the advent of medical big
data, advances in diagnostic tests as well as in machine learning and
statistics, early or timely diagnosis seems within reach. Early diagnosis
research often neglects the potential for optimizing individual diagnostic
paths. To enable personalized early diagnosis, a foundational framework is
needed that delineates the diagnosis process and systematically identifies the
time-dependent value of various diagnostic tests for an individual patient
given their unique characteristics. Here, we propose the first foundational
framework for early and timely diagnosis. It builds on decision-theoretic
approaches to outline the diagnosis process and integrates machine learning and
statistical methodology for estimating the optimal personalized diagnostic
path. To describe the proposed framework as well as possibly other frameworks,
we provide essential definitions.
</p>
<p>The development of a foundational framework is necessary for several reasons:
1) formalism provides clarity for the development of decision support tools; 2)
observed information can be complemented with estimates of the future patient
trajectory; 3) the net benefit of counterfactual diagnostic paths and
associated uncertainties can be modeled for individuals 4) 'early' and 'timely'
diagnosis can be clearly defined; 5) a mechanism emerges for assessing the
value of technologies in terms of their impact on personalized early diagnosis,
resulting health outcomes and incurred costs.
</p>
<p>Finally, we hope that this foundational framework will unlock the
long-awaited potential of timely diagnosis and intervention, leading to
improved outcomes for patients and higher cost-effectiveness for healthcare
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16196">Variational Exploration Module VEM: A Cloud-Native Optimization and Validation Tool for Geospatial Modeling and AI Workflows. (arXiv:2311.16196v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuehnert_J/0/1/0/all/0/1">Julian Kuehnert</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Tadesse_H/0/1/0/all/0/1">Hiwot Tadesse</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Dearden_C/0/1/0/all/0/1">Chris Dearden</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Lickorish_R/0/1/0/all/0/1">Rosie Lickorish</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Fraccaro_P/0/1/0/all/0/1">Paolo Fraccaro</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1">Anne Jones</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Edwards_B/0/1/0/all/0/1">Blair Edwards</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Remy_S/0/1/0/all/0/1">Sekou L. Remy</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Melling_P/0/1/0/all/0/1">Peter Melling</a> (4), <a href="http://arxiv.org/find/cs/1/au:+Culmer_T/0/1/0/all/0/1">Tim Culmer</a> (4) ((1) IBM Research, Nairobi, Kenya, (2) STFC Hartree Centre, Warrington, UK, (3) IBM Research, Daresbury, UK, (4) Riskaware Ltd., Bristol, UK)</p>
<p>Geospatial observations combined with computational models have become key to
understanding the physical systems of our environment and enable the design of
best practices to reduce societal harm. Cloud-based deployments help to scale
up these modeling and AI workflows. Yet, for practitioners to make robust
conclusions, model tuning and testing is crucial, a resource intensive process
which involves the variation of model input variables. We have developed the
Variational Exploration Module which facilitates the optimization and
validation of modeling workflows deployed in the cloud by orchestrating
workflow executions and using Bayesian and machine learning-based methods to
analyze model behavior. User configurations allow the combination of diverse
sampling strategies in multi-agent environments. The flexibility and robustness
of the model-agnostic module is demonstrated using real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16197">Generation of patient specific cardiac chamber models using generative neural networks under a Bayesian framework for electroanatomical mapping. (arXiv:2311.16197v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1">Sunil Mathew</a>, <a href="http://arxiv.org/find/eess/1/au:+Sra_J/0/1/0/all/0/1">Jasbir Sra</a>, <a href="http://arxiv.org/find/eess/1/au:+Rowe_D/0/1/0/all/0/1">Daniel B. Rowe</a></p>
<p>Electroanatomical mapping is a technique used in cardiology to create a
detailed 3D map of the electrical activity in the heart. It is useful for
diagnosis, treatment planning and real time guidance in cardiac ablation
procedures to treat arrhythmias like atrial fibrillation. A probabilistic
machine learning model trained on a library of CT/MRI scans of the heart can be
used during electroanatomical mapping to generate a patient-specific 3D model
of the chamber being mapped. The use of probabilistic machine learning models
under a Bayesian framework provides a way to quantify uncertainty in results
and provide a natural framework of interpretability of the model. Here we
introduce a Bayesian approach to surface reconstruction of cardiac chamber
models from a sparse 3D point cloud data acquired during electroanatomical
mapping. We show how probabilistic graphical models trained on segmented CT/MRI
data can be used to generate cardiac chamber models from few acquired locations
thereby reducing procedure time and x-ray exposure. We show how they provide
insight into what the neural network learns from the segmented CT/MRI images
used to train the network, which provides explainability to the resulting
cardiac chamber models generated by the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16201">Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation. (arXiv:2311.16201v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+McKinzie_B/0/1/0/all/0/1">Brandon McKinzie</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zhe Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1">Vaishaal Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1">Alexander Toshev</a></p>
<p>Recent advances in image tokenizers, such as VQ-VAE, have enabled
text-to-image generation using auto-regressive methods, similar to language
modeling. However, these methods have yet to leverage pre-trained language
models, despite their adaptability to various downstream tasks. In this work,
we explore this gap by adapting a pre-trained language model for
auto-regressive text-to-image generation, and find that pre-trained language
models offer limited help. We provide a two-fold explanation by analyzing
tokens from each modality. First, we demonstrate that image tokens possess
significantly different semantics compared to text tokens, rendering
pre-trained language models no more effective in modeling them than randomly
initialized ones. Second, the text tokens in the image-text datasets are too
simple compared to normal language model pre-training data, which causes the
catastrophic degradation of language models' capability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16203">ChatTraffc: Text-to-Traffic Generation via Diffusion Model. (arXiv:2311.16203v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1">Qitan Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1">Yisheng Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Piao_X/0/1/0/all/0/1">Xinglin Piao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Baocai Yin</a></p>
<p>Traffic prediction is one of the most significant foundations in Intelligent
Transportation Systems (ITS). Traditional traffic prediction methods rely only
on historical traffic data to predict traffic trends and face two main
challenges. 1) insensitivity to unusual events. 2) poor performance in
long-term prediction. In this work, we explore how generative models combined
with text describing the traffic system can be applied for traffic generation
and name the task Text-to-Traffic Generation (TTG). The key challenge of the
TTG task is how to associate text with the spatial structure of the road
network and traffic data for generating traffic situations. To this end, we
propose ChatTraffic, the first diffusion model for text-to-traffic generation.
To guarantee the consistency between synthetic and real data, we augment a
diffusion model with the Graph Convolutional Network (GCN) to extract spatial
correlations of traffic data. In addition, we construct a large dataset
containing text-traffic pairs for the TTG task. We benchmarked our model
qualitatively and quantitatively on the released dataset. The experimental
results indicate that ChatTraffic can generate realistic traffic situations
from the text. Our code and dataset are available at
https://github.com/ChyaZhang/ChatTraffic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16204">Planning for the Efficient Updating of Mutual Fund Portfolios. (arXiv:2311.16204v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Rosa_T/0/1/0/all/0/1">Tom&#xe1;s de la Rosa</a></p>
<p>Once there is a decision of rebalancing or updating a portfolio of funds, the
process of changing the current portfolio to the target one, involves a set of
transactions that are susceptible of being optimized. This is particularly
relevant when managers have to handle the implications of different types of
instruments. In this work we present linear programming and heuristic search
approaches that produce plans for executing the update. The evaluation of our
proposals shows cost improvements over the compared based strategy. The models
can be easily extended to other realistic scenarios in which a holistic
portfolio management is required
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16206">Continual Instruction Tuning for Large Multimodal Models. (arXiv:2311.16206v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinghan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Haiyun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Ming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiao Wang</a></p>
<p>Instruction tuning is now a widely adopted approach to aligning large
multimodal models (LMMs) to follow human intent. It unifies the data format of
vision-language tasks, enabling multi-task joint training. However,
vision-language tasks are constantly being created in practice. Instead of
always re-training LMMs when new tasks arrive, continual learning offers
flexibility for models to continually and efficiently exploit the evolving
data. This work aims to explore the following two questions: 1) Do LMMs still
suffer from catastrophic forgetting in continual instruction tuning? 2) Are the
existing three classes of continual learning methods still applicable to the
continual instruction tuning of LMMs? An extensive study is conducted to
address the above questions. First, we establish the first benchmark in this
setting and reveal that catastrophic forgetting is still observed when
continually instruction-tuning LMMs. However, the multi-task joint instruction
tuning can facilitate the model's continual learning ability and mitigate
forgetting. Second, we integrate and adapt classic continual learning methods
to our context, demonstrating the efficacy of data replay and model expansion
strategies across diverse scenarios. In contrast, regularization-based methods
only perform well on models that have been jointly instruction-tuned on
multiple tasks. Third, we delve into the correlation and forgetting dynamics
between vision-language task pairs and propose task-similarity-informed
regularization and model expansion methods for continual instruction tuning of
LMMs. Experimental results show that our approach consistently boosts the
model's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16208">InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery. (arXiv:2311.16208v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Cao_H/0/1/0/all/0/1">He Cao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_Z/0/1/0/all/0/1">Zijing Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lu_X/0/1/0/all/0/1">Xingyu Lu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a></p>
<p>The rapid evolution of artificial intelligence in drug discovery encounters
challenges with generalization and extensive training, yet Large Language
Models (LLMs) offer promise in reshaping interactions with complex molecular
data. Our novel contribution, InstructMol, a multi-modal LLM, effectively
aligns molecular structures with natural language via an instruction-tuning
approach, utilizing a two-stage training strategy that adeptly combines limited
domain-specific data with molecular and textual information. InstructMol
showcases substantial performance improvements in drug discovery-related
molecular tasks, surpassing leading LLMs and significantly reducing the gap
with specialized models, thereby establishing a robust foundation for a
versatile and dependable drug discovery assistant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16254">Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation. (arXiv:2311.16254v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poppi_S/0/1/0/all/0/1">Samuele Poppi</a>, <a href="http://arxiv.org/find/cs/1/au:+Poppi_T/0/1/0/all/0/1">Tobia Poppi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cocchi_F/0/1/0/all/0/1">Federico Cocchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1">Marcella Cornia</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1">Lorenzo Baraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1">Rita Cucchiara</a></p>
<p>Vision-and-Language models such as CLIP have demonstrated remarkable
effectiveness across a wide range of tasks. However, these models are typically
trained on web-scale data, which can introduce inappropriate content and lead
to the development of unsafe and biased behavior. This, in turn, hampers their
applicability in sensitive and trustworthy contexts and could raise significant
concern in their adoption. To overcome these limitations, we introduce a
methodology to make Vision-and-Language models safer by removing their
sensitivity to not-safe-for-work concepts. We show how this can be done by
distilling from a large language model which converts between safe and unsafe
sentences and which is fine-tuned starting from just 100 manually-curated
pairs. We conduct extensive experiments on the resulting embedding space for
both retrieval and text-to-image generation, where we show that our model can
also be properly employed with pre-trained image generators. Our source code
and trained models are available at: https://github.com/aimagelab/safe-clip.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16261">RelVAE: Generative Pretraining for few-shot Visual Relationship Detection. (arXiv:2311.16261v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karapiperis_S/0/1/0/all/0/1">Sotiris Karapiperis</a>, <a href="http://arxiv.org/find/cs/1/au:+Diomataris_M/0/1/0/all/0/1">Markos Diomataris</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitsikalis_V/0/1/0/all/0/1">Vassilis Pitsikalis</a></p>
<p>Visual relations are complex, multimodal concepts that play an important role
in the way humans perceive the world. As a result of their complexity,
high-quality, diverse and large scale datasets for visual relations are still
absent. In an attempt to overcome this data barrier, we choose to focus on the
problem of few-shot Visual Relationship Detection (VRD), a setting that has
been so far neglected by the community. In this work we present the first
pretraining method for few-shot predicate classification that does not require
any annotated relations. We achieve this by introducing a generative model that
is able to capture the variation of semantic, visual and spatial information of
relations inside a latent space and later exploiting its representations in
order to achieve efficient few-shot classification. We construct few-shot
training splits and show quantitative experiments on VG200 and VRD datasets
where our model outperforms the baselines. Lastly we attempt to interpret the
decisions of the model by conducting various qualitative experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16277">A Graph Neural Network-Based QUBO-Formulated Hamiltonian-Inspired Loss Function for Combinatorial Optimization using Reinforcement Learning. (arXiv:2311.16277v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rizvee_R/0/1/0/all/0/1">Redwan Ahmed Rizvee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_R/0/1/0/all/0/1">Raheeb Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Md. Mosaddek Khan</a></p>
<p>Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to
model various NP-hard Combinatorial Optimization problems (CO) in the form of
binary variables. Ising Hamiltonian is used to model the energy function of a
system. QUBO to Ising Hamiltonian is regarded as a technique to solve various
canonical optimization problems through quantum optimization algorithms.
Recently, PI-GNN, a generic framework, has been proposed to address CO problems
over graphs based on Graph Neural Network (GNN) architecture. They introduced a
generic QUBO-formulated Hamiltonian-inspired loss function that was directly
optimized using GNN. PI-GNN is highly scalable but there lies a noticeable
decrease in the number of satisfied constraints when compared to
problem-specific algorithms and becomes more pronounced with increased graph
densities. Here, We identify a behavioral pattern related to it and devise
strategies to improve its performance. Another group of literature uses
Reinforcement learning (RL) to solve the aforementioned NP-hard problems using
problem-specific reward functions. In this work, we also focus on creating a
bridge between the RL-based solutions and the QUBO-formulated Hamiltonian. We
formulate and empirically evaluate the compatibility of the QUBO-formulated
Hamiltonian as the generic reward function in the RL-based paradigm in the form
of rewards. Furthermore, we also introduce a novel Monty Carlo Tree
Search-based strategy with GNN where we apply a guided search through manual
perturbation of node labels during training. We empirically evaluated our
methods and observed up to 44% improvement in the number of constraint
violations compared to the PI-GNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16312">Domain-Specific Deep Learning Feature Extractor for Diabetic Foot Ulcer Detection. (arXiv:2311.16312v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Basiri_R/0/1/0/all/0/1">Reza Basiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1">Milos R. Popovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Shehroz S. Khan</a></p>
<p>Diabetic Foot Ulcer (DFU) is a condition requiring constant monitoring and
evaluations for treatment. DFU patient population is on the rise and will soon
outpace the available health resources. Autonomous monitoring and evaluation of
DFU wounds is a much-needed area in health care. In this paper, we evaluate and
identify the most accurate feature extractor that is the core basis for
developing a deep-learning wound detection network. For the evaluation, we used
mAP and F1-score on the publicly available DFU2020 dataset. A combination of
UNet and EfficientNetb3 feature extractor resulted in the best evaluation among
the 14 networks compared. UNet and Efficientnetb3 can be used as the classifier
in the development of a comprehensive DFU domain-specific autonomous wound
detection pipeline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16338">Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source dataset and dataset creation methodology using instruction-following models. (arXiv:2311.16338v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grzywinski_R/0/1/0/all/0/1">Rob Grzywinski</a>, <a href="http://arxiv.org/find/cs/1/au:+DArcy_J/0/1/0/all/0/1">Joshua D&#x27;Arcy</a>, <a href="http://arxiv.org/find/cs/1/au:+Naidoff_R/0/1/0/all/0/1">Rob Naidoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1">Ashish Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Browne_A/0/1/0/all/0/1">Alex Browne</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibbons_R/0/1/0/all/0/1">Ren Gibbons</a>, <a href="http://arxiv.org/find/cs/1/au:+Bent_B/0/1/0/all/0/1">Brinnae Bent</a></p>
<p>Instruction-following language models demand robust methodologies for
information retrieval to augment instructions for question-answering
applications. A primary challenge is the resolution of coreferences in the
context of chunking strategies for long documents. The critical barrier to
experimentation of handling coreferences is a lack of open source datasets,
specifically in question-answering tasks that require coreference resolution.
In this work we present our Coreference Resolution in Question-Answering
(CRaQAn) dataset, an open-source dataset that caters to the nuanced information
retrieval requirements of coreference resolution in question-answering tasks by
providing over 250 question-answer pairs containing coreferences. To develop
this dataset, we developed a novel approach for creating high-quality datasets
using an instruction-following model (GPT-4) and a Recursive Criticism and
Improvement Loop.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16339">Reward Shaping for Improved Learning in Real-time Strategy Game Play. (arXiv:2311.16339v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kliem_J/0/1/0/all/0/1">John Kliem</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1">Prithviraj Dasgupta</a></p>
<p>We investigate the effect of reward shaping in improving the performance of
reinforcement learning in the context of the real-time strategy,
capture-the-flag game. The game is characterized by sparse rewards that are
associated with infrequently occurring events such as grabbing or capturing the
flag, or tagging the opposing player. We show that appropriately designed
reward shaping functions applied to different game events can significantly
improve the player's performance and training times of the player's learning
algorithm. We have validated our reward shaping functions within a simulated
environment for playing a marine capture-the-flag game between two players. Our
experimental results demonstrate that reward shaping can be used as an
effective means to understand the importance of different sub-tasks during
game-play towards winning the game, to encode a secondary objective functions
such as energy efficiency into a player's game-playing behavior, and, to
improve learning generalizable policies that can perform well against different
skill levels of the opponent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16353">Improving Denoising Diffusion Probabilistic Models via Exploiting Shared Representations. (arXiv:2311.16353v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pirhayatifard_D/0/1/0/all/0/1">Delaram Pirhayatifard</a>, <a href="http://arxiv.org/find/cs/1/au:+Toghani_M/0/1/0/all/0/1">Mohammad Taha Toghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1">Guha Balakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Uribe_C/0/1/0/all/0/1">C&#xe9;sar A. Uribe</a></p>
<p>In this work, we address the challenge of multi-task image generation with
limited data for denoising diffusion probabilistic models (DDPM), a class of
generative models that produce high-quality images by reversing a noisy
diffusion process. We propose a novel method, SR-DDPM, that leverages
representation-based techniques from few-shot learning to effectively learn
from fewer samples across different tasks. Our method consists of a core meta
architecture with shared parameters, i.e., task-specific layers with exclusive
parameters. By exploiting the similarity between diverse data distributions,
our method can scale to multiple tasks without compromising the image quality.
We evaluate our method on standard image datasets and show that it outperforms
both unconditional and conditional DDPM in terms of FID and SSIM metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16392">Multi-defender Security Games with Schedules. (arXiv:2311.16392v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zimeng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1">Chun Kai Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1">Fei Fang</a></p>
<p>Stackelberg Security Games are often used to model strategic interactions in
high-stakes security settings. The majority of existing models focus on
single-defender settings where a single entity assumes command of all security
assets. However, many realistic scenarios feature multiple heterogeneous
defenders with their own interests and priorities embedded in a more complex
system. Furthermore, defenders rarely choose targets to protect. Instead, they
have a multitude of defensive resources or schedules at its disposal, each with
different protective capabilities. In this paper, we study security games
featuring multiple defenders and schedules simultaneously. We show that unlike
prior work on multi-defender security games, the introduction of schedules can
cause non-existence of equilibrium even under rather restricted environments.
We prove that under the mild restriction that any subset of a schedule is also
a schedule, non-existence of equilibrium is not only avoided, but can be
computed in polynomial time in games with two defenders. Under additional
assumptions, our algorithm can be extended to games with more than two
defenders and its computation scaled up in special classes of games with
compactly represented schedules such as those used in patrolling applications.
Experimental results suggest that our methods scale gracefully with game size,
making our algorithms amongst the few that can tackle multiple heterogeneous
defenders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16424">Manifold Preserving Guided Diffusion. (arXiv:2311.16424v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yutong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Murata_N/0/1/0/all/0/1">Naoki Murata</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Chieh-Hsin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Takida_Y/0/1/0/all/0/1">Yuhta Takida</a>, <a href="http://arxiv.org/find/cs/1/au:+Uesaka_T/0/1/0/all/0/1">Toshimitsu Uesaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wei-Hsiang Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1">Yuki Mitsufuji</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a></p>
<p>Despite the recent advancements, conditional image generation still faces
challenges of cost, generalizability, and the need for task-specific training.
In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a
training-free conditional generation framework that leverages pretrained
diffusion models and off-the-shelf neural networks with minimal additional
inference cost for a broad range of tasks. Specifically, we leverage the
manifold hypothesis to refine the guided diffusion steps and introduce a
shortcut algorithm in the process. We then propose two methods for on-manifold
training-free guidance using pre-trained autoencoders and demonstrate that our
shortcut inherently preserves the manifolds when applied to latent diffusion
models. Our experiments show that MPGD is efficient and effective for solving a
variety of conditional generation applications in low-compute settings, and can
consistently offer up to 3.8x speed-ups with the same number of diffusion steps
while maintaining high sample quality compared to the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16432">Text-Driven Image Editing via Learnable Regions. (arXiv:2311.16432v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuanze Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Wen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Hsuan Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Language has emerged as a natural interface for image editing. In this paper,
we introduce a method for region-based image editing driven by textual prompts,
without the need for user-provided masks or sketches. Specifically, our
approach leverages an existing pretrained text-to-image model and introduces a
bounding box generator to find the edit regions that are aligned with the
textual prompts. We show that this simple approach enables flexible editing
that is compatible with current image generation models, and is able to handle
complex prompts featuring multiple objects, complex sentences or long
paragraphs. We conduct an extensive user study to compare our method against
state-of-the-art methods. Experiments demonstrate the competitive performance
of our method in manipulating images with high fidelity and realism that align
with the language descriptions provided. Our project webpage:
https://yuanze-lin.me/LearnableRegions_page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16450">Typhoon Intensity Prediction with Vision Transformer. (arXiv:2311.16450v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1">Pengshuai Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huichou Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingyao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruirui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a></p>
<p>Predicting typhoon intensity accurately across space and time is crucial for
issuing timely disaster warnings and facilitating emergency response. This has
vast potential for minimizing life losses and property damages as well as
reducing economic and environmental impacts. Leveraging satellite imagery for
scenario analysis is effective but also introduces additional challenges due to
the complex relations among clouds and the highly dynamic context. Existing
deep learning methods in this domain rely on convolutional neural networks
(CNNs), which suffer from limited per-layer receptive fields. This limitation
hinders their ability to capture long-range dependencies and global contextual
knowledge during inference. In response, we introduce a novel approach, namely
"Typhoon Intensity Transformer" (Tint), which leverages self-attention
mechanisms with global receptive fields per layer. Tint adopts a
sequence-to-sequence feature representation learning perspective. It begins by
cutting a given satellite image into a sequence of patches and recursively
employs self-attention operations to extract both local and global contextual
relations between all patch pairs simultaneously, thereby enhancing per-patch
feature representation learning. Extensive experiments on a publicly available
typhoon benchmark validate the efficacy of Tint in comparison with both
state-of-the-art deep learning and conventional meteorological methods. Our
code is available at https://github.com/chen-huanxin/Tint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16464">Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection. (arXiv:2311.16464v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yicheng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhuoyan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yue Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_H/0/1/0/all/0/1">Hengwei Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yatai Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a></p>
<p>Video Moment Retrieval (MR) and Highlight Detection (HD) have attracted
significant attention due to the growing demand for video analysis. Recent
approaches treat MR and HD as similar video grounding problems and address them
together with transformer-based architecture. However, we observe that the
emphasis of MR and HD differs, with one necessitating the perception of local
relationships and the other prioritizing the understanding of global contexts.
Consequently, the lack of task-specific design will inevitably lead to
limitations in associating the intrinsic specialty of two tasks. To tackle the
issue, we propose a Unified Video COMprehension framework (UVCOM) to bridge the
gap and jointly solve MR and HD effectively. By performing progressive
integration on intra and inter-modality across multi-granularity, UVCOM
achieves the comprehensive understanding in processing a video. Moreover, we
present multi-aspect contrastive learning to consolidate the local relation
modeling and global knowledge accumulation via well aligned multi-modal space.
Extensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlights
and TVSum datasets demonstrate the effectiveness and rationality of UVCOM which
outperforms the state-of-the-art methods by a remarkable margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16466">Enhancing Human Persuasion With Large Language Models. (arXiv:2311.16466v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Minkyu Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin Kim</a></p>
<p>Although large language models (LLMs) are reshaping various aspects of human
life, our current understanding of their impacts remains somewhat constrained.
Here we investigate the impact of LLMs on human communication, in the context
of consumer complaints in the financial industry. Employing an AI detection
tool on more than 780K complaints gathered by the Consumer Financial Protection
Bureau (CFPB), we find evidence of LLM usage in the writing of complaints -
shortly after the release of ChatGPT. Our analyses reveal that LLM usage is
positively correlated with the likelihood of obtaining desirable outcomes
(i.e., offer of relief from financial firms) and suggest that this positive
correlation may be partly due to the linguistic features improved by LLMs. We
test this conjecture with a preregistered experiment, which reveals results
consistent with those from observational studies: Consumer complaints written
with ChatGPT for improved linguistic qualities were more likely to receive
hypothetical relief offers than the original consumer complaints, demonstrating
the LLM's ability to enhance message persuasiveness in human communication.
Being some of the earliest empirical evidence on LLM usage for enhancing
persuasion, our results highlight the transformative potential of LLMs in human
communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16476">LANS: A Layout-Aware Neural Solver for Plane Geometry Problem. (arXiv:2311.16476v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming-Liang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhong-Zhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Fei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-Lin Liu</a></p>
<p>Geometry problem solving (GPS) is a challenging mathematical reasoning task
requiring multi-modal understanding, fusion and reasoning. Existing neural
solvers take GPS as a vision-language task but be short in the representation
of geometry diagrams which carry rich and complex layout information. In this
paper, we propose a layout-aware neural solver named LANS, integrated with two
new modules: multimodal layout-aware pre-trained language model (MLA-PLM) and
layout-aware fusion attention (LA-FA). MLA-PLM adopts structural and semantic
pre-training (SSP) to implement global relationship modeling, and point
matching pre-training (PMP) to achieve alignment between visual points and
textual points. LA-FA employs a layout-aware attention mask to realize
point-guided cross-modal fusion for further boosting layout awareness of LANS.
Extensive experiments on datasets Geometry3K and PGPS9K validate the
effectiveness of the layout-aware modules and superior problem solving
performance of our LANS solver, over existing symbolic solvers and neural
solvers. The code will make public available soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16480">MI-Gen: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images. (arXiv:2311.16480v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pingyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Honglin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chenglu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Sunyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin Yang</a></p>
<p>Whole slide images are the foundation of digital pathology for the diagnosis
and treatment of carcinomas. Writing pathology reports is laborious and
error-prone for inexperienced pathologists. To reduce the workload and improve
clinical automation, we investigate how to generate pathology reports given
whole slide images. On the data end, we curated the largest WSI-text dataset
(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text
pairs for visual-language models by recognizing and cleaning pathology reports
which narrate diagnostic slides in TCGA. On the model end, we propose the
multiple instance generative model (MI-Gen) which can produce pathology reports
for gigapixel WSIs. We benchmark our model on the largest subset of
TCGA-PathoText. Experimental results show our model can generate pathology
reports which contain multiple clinical clues. Furthermore, WSI-text prediction
can be seen as an approach of visual-language pre-training, which enables our
model to be transferred to downstream diagnostic tasks like carcinoma grading
and phenotyping. We observe that simple semantic extraction from the pathology
reports can achieve the best performance (0.838 of F1 score) on BRCA subtyping
without adding extra parameters or tricky fine-tuning. Our collected dataset
and related code will all be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16488">Efficient Multimodal Diffusion Models Using Joint Data Infilling with Partially Shared U-Net. (arXiv:2311.16488v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zizhao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1">Shaochong Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a></p>
<p>Recently, diffusion models have been used successfully to fit distributions
for cross-modal data translation and multimodal data generation. However, these
methods rely on extensive scaling, overlooking the inefficiency and
interference between modalities. We develop Partially Shared U-Net (PS-U-Net)
architecture which is an efficient multimodal diffusion model that allows text
and image inputs to pass through dedicated layers and skip-connections for
preserving modality-specific fine-grained details. Inspired by image
inpainting, we also propose a new efficient multimodal sampling method that
introduces new scenarios for conditional generation while only requiring a
simple joint distribution to be learned. Our empirical exploration of the
MS-COCO dataset demonstrates that our method generates multimodal text and
image data with higher quality compared to existing multimodal diffusion models
while having a comparable size, faster training, faster multimodal sampling,
and more flexible generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16502">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. (arXiv:2311.16502v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yuansheng Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruoqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1">Samuel Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Dongfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Weiming Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Cong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Botao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Renliang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1">Ming Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Boyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhenzhu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a></p>
<p>We introduce MMMU: a new benchmark designed to evaluate multimodal models on
massive multi-discipline tasks demanding college-level subject knowledge and
deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal
questions from college exams, quizzes, and textbooks, covering six core
disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp;
Social Science, and Tech &amp; Engineering. These questions span 30 subjects and
183 subfields, comprising 30 highly heterogeneous image types, such as charts,
diagrams, maps, tables, music sheets, and chemical structures. Unlike existing
benchmarks, MMMU focuses on advanced perception and reasoning with
domain-specific knowledge, challenging models to perform tasks akin to those
faced by experts. Our evaluation of 14 open-source LMMs and the proprietary
GPT-4V(ision) highlights the substantial challenges posed by MMMU. Even the
advanced GPT-4V only achieves a 56% accuracy, indicating significant room for
improvement. We believe MMMU will stimulate the community to build
next-generation multimodal foundation models towards expert artificial general
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16503">TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models. (arXiv:2311.16503v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yushi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1">Ruihao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianglong Liu</a></p>
<p>The Diffusion model, a prevalent framework for image generation, encounters
significant challenges in terms of broad applicability due to its extended
inference times and substantial memory requirements. Efficient Post-training
Quantization (PTQ) is pivotal for addressing these issues in traditional
models. Different from traditional models, diffusion models heavily depend on
the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$
from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a
few modules totally irrespective of the sampling data. However, existing PTQ
methods do not optimize these modules separately. They adopt inappropriate
reconstruction targets and complex calibration methods, resulting in a severe
disturbance of the temporal feature and denoising trajectory, as well as a low
compression efficiency. To solve these, we propose a Temporal Feature
Maintenance Quantization (TFMQ) framework building upon a Temporal Information
Block which is just related to the time-step $t$ and unrelated to the sampling
data. Powered by the pioneering block design, we devise temporal information
aware reconstruction (TIAR) and finite set calibration (FSC) to align the
full-precision temporal features in a limited time. Equipped with the
framework, we can maintain the most temporal information and ensure the
end-to-end generation quality. Extensive experiments on various datasets and
diffusion models prove our state-of-the-art results. Remarkably, our
quantization approach, for the first time, achieves model performance nearly on
par with the full-precision model under 4-bit weight quantization.
Additionally, our method incurs almost no extra computational cost and
accelerates quantization time by $2.0 \times$ on LSUN-Bedrooms $256 \times 256$
compared to previous works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16512">CoSeR: Bridging Image and Language for Cognitive Super-Resolution. (arXiv:2311.16512v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haoze Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianzhuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_R/0/1/0/all/0/1">Renjing Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyi Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Youliang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>Existing super-resolution (SR) models primarily focus on restoring local
texture details, often neglecting the global semantic information within the
scene. This oversight can lead to the omission of crucial semantic details or
the introduction of inaccurate textures during the recovery process. In our
work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering
SR models with the capacity to comprehend low-resolution images. We achieve
this by marrying image appearance and language understanding to generate a
cognitive embedding, which not only activates prior information from large
text-to-image diffusion models but also facilitates the generation of
high-quality reference images to optimize the SR process. To further improve
image fidelity, we propose a novel condition injection scheme called
"All-in-Attention", consolidating all conditional information into a single
module. Consequently, our method successfully restores semantically correct and
photorealistic details, demonstrating state-of-the-art performance across
multiple benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16514">Video Anomaly Detection via Spatio-Temporal Pseudo-Anomaly Generation : A Unified Approach. (arXiv:2311.16514v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1">Ayush K. Rai</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1">Tarun Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Feiyan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Drimbarean_A/0/1/0/all/0/1">Alexandru Drimbarean</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1">Kevin McGuinness</a>, <a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1">Alan F. Smeaton</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1">Noel E. O&#x27;Connor</a></p>
<p>Video Anomaly Detection (VAD) is an open-set recognition task, which is
usually formulated as a one-class classification (OCC) problem, where training
data is comprised of videos with normal instances while test data contains both
normal and anomalous instances. Recent works have investigated the creation of
pseudo-anomalies (PAs) using only the normal data and making strong assumptions
about real-world anomalies with regards to abnormality of objects and speed of
motion to inject prior information about anomalies in an autoencoder (AE) based
reconstruction model during training. This work proposes a novel method for
generating generic spatio-temporal PAs by inpainting a masked out region of an
image using a pre-trained Latent Diffusion Model and further perturbing the
optical flow using mixup to emulate spatio-temporal distortions in the data. In
addition, we present a simple unified framework to detect real-world anomalies
under the OCC setting by learning three types of anomaly indicators, namely
reconstruction quality, temporal irregularity and semantic inconsistency.
Extensive experiments on four VAD benchmark datasets namely Ped2, Avenue,
ShanghaiTech and UBnormal demonstrate that our method performs on par with
other existing state-of-the-art PAs generation and reconstruction based methods
under the OCC setting. Our analysis also examines the transferability and
generalisation of PAs across these datasets, offering valuable insights by
identifying real-world anomalies through PAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16515">Word for Person: Zero-shot Composed Person Retrieval. (arXiv:2311.16515v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Delong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haiwen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhicheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1">Fei Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Hongying Meng</a></p>
<p>Searching for specific person has great security value and social benefits,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR must depend on very
costly manual annotation dataset, while there are currently no available
resources. To mitigate this issue, we firstly introduce the Zero-shot Composed
Person Retrieval (ZS-CPR), which leverages existing domain-related data to
resolve the CPR problem without reliance on expensive annotations. Secondly, to
learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where
a lightweight Textual Inversion Network (TINet) and a text-based person
retrieval model based on fine-tuned Contrastive Language-Image Pre-training
(CLIP) network are learned without utilizing any CPR data. Thirdly, a finely
annotated Image-Text Composed Person Retrieval dataset (ITCPR) is built as the
benchmark to assess the performance of the proposed Word4Per framework.
Extensive experiments under both Rank-1 and mAP demonstrate the effectiveness
of Word4Per for the ZS-CPR task, surpassing the comparative methods by over
10%. The code and ITCPR dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16534">Graph Prompt Learning: A Comprehensive Survey and Beyond. (arXiv:2311.16534v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangguo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xixi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>Artificial General Intelligence (AGI) has revolutionized numerous fields, yet
its integration with graph data, a cornerstone in our interconnected world,
remains nascent. This paper presents a pioneering survey on the emerging domain
of graph prompts in AGI, addressing key challenges and opportunities in
harnessing graph data for AGI applications. Despite substantial advancements in
AGI across natural language processing and computer vision, the application to
graph data is relatively underexplored. This survey critically evaluates the
current landscape of AGI in handling graph data, highlighting the distinct
challenges in cross-modality, cross-domain, and cross-task applications
specific to graphs. Our work is the first to propose a unified framework for
understanding graph prompt learning, offering clarity on prompt tokens, token
structures, and insertion patterns in the graph domain. We delve into the
intrinsic properties of graph prompts, exploring their flexibility,
expressiveness, and interplay with existing graph models. A comprehensive
taxonomy categorizes over 100 works in this field, aligning them with
pre-training tasks across node-level, edge-level, and graph-level objectives.
Additionally, we present, ProG, a Python library, and an accompanying website,
to support and advance research in graph prompting. The survey culminates in a
discussion of current challenges and future directions, offering a roadmap for
research in graph prompting within AGI. Through this comprehensive analysis, we
aim to catalyze further exploration and practical applications of AGI in graph
data, underlining its potential to reshape AGI fields and beyond. ProG and the
website can be accessed by
\url{https://github.com/WxxShirley/Awesome-Graph-Prompt}, and
\url{https://github.com/sheldonresearch/ProG}, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16594">Monitor Placement for Fault Localization in Deep Neural Network Accelerators. (arXiv:2311.16594v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei-Kai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1">Benjamin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_K/0/1/0/all/0/1">Krishnendu Chakrabarty</a></p>
<p>Systolic arrays are a prominent choice for deep neural network (DNN)
accelerators because they offer parallelism and efficient data reuse. Improving
the reliability of DNN accelerators is crucial as hardware faults can degrade
the accuracy of DNN inferencing. Systolic arrays make use of a large number of
processing elements (PEs) for parallel processing, but when one PE is faulty,
the error propagates and affects the outcomes of downstream PEs. Due to the
large number of PEs, the cost associated with implementing hardware-based
runtime monitoring of every single PE is infeasible. We present a solution to
optimize the placement of hardware monitors within systolic arrays. We first
prove that $2N-1$ monitors are needed to localize a single faulty PE and we
also derive the monitor placement. We show that a second placement optimization
problem, which minimizes the set of candidate faulty PEs for a given number of
monitors, is NP-hard. Therefore, we propose a heuristic approach to balance the
reliability and hardware resource utilization in DNN accelerators when number
of monitors is limited. Experimental evaluation shows that to localize a single
faulty PE, an area overhead of only 0.33% is incurred for a $256\times 256$
systolic array.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16602">GSP-KalmanNet: Tracking Graph Signals via Neural-Aided Kalman Filtering. (arXiv:2311.16602v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Buchnik_I/0/1/0/all/0/1">Itay Buchnik</a>, <a href="http://arxiv.org/find/eess/1/au:+Sagi_G/0/1/0/all/0/1">Guy Sagi</a>, <a href="http://arxiv.org/find/eess/1/au:+Leinwand_N/0/1/0/all/0/1">Nimrod Leinwand</a>, <a href="http://arxiv.org/find/eess/1/au:+Loya_Y/0/1/0/all/0/1">Yuval Loya</a>, <a href="http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1">Nir Shlezinger</a>, <a href="http://arxiv.org/find/eess/1/au:+Routtenberg_T/0/1/0/all/0/1">Tirza Routtenberg</a></p>
<p>Dynamic systems of graph signals are encountered in various applications,
including social networks, power grids, and transportation. While such systems
can often be described as state space (SS) models, tracking graph signals via
conventional tools based on the Kalman filter (KF) and its variants is
typically challenging. This is due to the nonlinearity, high dimensionality,
irregularity of the domain, and complex modeling associated with real-world
dynamic systems of graph signals. In this work, we study the tracking of graph
signals using a hybrid model-based/data-driven approach. We develop the
GSP-KalmanNet, which tracks the hidden graphical states from the graphical
measurements by jointly leveraging graph signal processing (GSP) tools and deep
learning (DL) techniques. The derivations of the GSP-KalmanNet are based on
extending the KF to exploit the inherent graph structure via graph frequency
domain filtering, which considerably simplifies the computational complexity
entailed in processing high-dimensional signals and increases the robustness to
small topology changes. Then, we use data to learn the Kalman gain following
the recently proposed KalmanNet framework, which copes with partial and
approximated modeling, without forcing a specific model over the noise
statistics. Our empirical results demonstrate that the proposed GSP-KalmanNet
achieves enhanced accuracy and run time performance as well as improved
robustness to model misspecifications compared with both model-based and
data-driven benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16605">LasTGL: An Industrial Framework for Large-Scale Temporal Graph Learning. (arXiv:2311.16605v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jintang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_J/0/1/0/all/0/1">Jiawang Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruofan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jing Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Sheng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baokun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Changhua Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuchang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a></p>
<p>Over the past few years, graph neural networks (GNNs) have become powerful
and practical tools for learning on (static) graph-structure data. However,
many real-world applications, such as social networks and e-commerce, involve
temporal graphs where nodes and edges are dynamically evolving. Temporal graph
neural networks (TGNNs) have progressively emerged as an extension of GNNs to
address time-evolving graphs and have gradually become a trending research
topic in both academics and industry. Advancing research in such an emerging
field requires new tools to compose TGNN models and unify their different
schemes in dealing with temporal graphs. To facilitate research and application
in temporal graph learning, we introduce LasTGL, an industrial framework that
integrates unified and extensible implementations of common temporal graph
learning algorithms for various advanced tasks. The purpose of LasTGL is to
provide the essential building blocks for solving temporal graph learning
tasks, focusing on the guiding principles of user-friendliness and quick
prototyping on which PyTorch is based. In particular, LasTGL provides
comprehensive temporal graph datasets, TGNN models and utilities along with
well-documented tutorials, making it suitable for both absolute beginners and
expert deep learning practitioners alike.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16644">Finnish 5th and 6th graders&#x27; misconceptions about Artificial Intelligence. (arXiv:2311.16644v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mertala_P/0/1/0/all/0/1">Pekka Mertala</a>, <a href="http://arxiv.org/find/cs/1/au:+Fagerlund_J/0/1/0/all/0/1">Janne Fagerlund</a></p>
<p>Research on children's initial conceptions of AI is in an emerging state,
which, from a constructivist viewpoint, challenges the development of
pedagogically sound AI-literacy curricula, methods, and materials. To
contribute to resolving this need in the present paper, qualitative survey data
from 195 children were analyzed abductively to answer the following three
research questions: What kind of misconceptions do Finnish 5th and 6th graders'
have about the essence AI?; 2) How do these misconceptions relate to common
misconception types?; and 3) How profound are these misconceptions? As a
result, three misconception categories were identified: 1) Non-technological
AI, in which AI was conceptualized as peoples' cognitive processes (factual
misconception); 2) Anthropomorphic AI, in which AI was conceptualized as a
human-like entity (vernacular, non-scientific, and conceptual misconception);
and 3) AI as a machine with a pre-installed intelligence or knowledge (factual
misconception). Majority of the children evaluated their AI-knowledge low,
which implies that the misconceptions are more superficial than profound. The
findings suggest that context-specific linguistic features can contribute to
students' AI misconceptions. Implications for future research and AI literacy
education are discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16666">MultiModal-Learning for Predicting Molecular Properties: A Framework Based on Image and Graph Structures. (arXiv:2311.16666v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhuoyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_J/0/1/0/all/0/1">Jiacong Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jieyue He</a></p>
<p>The quest for accurate prediction of drug molecule properties poses a
fundamental challenge in the realm of Artificial Intelligence Drug Discovery
(AIDD). An effective representation of drug molecules emerges as a pivotal
component in this pursuit. Contemporary leading-edge research predominantly
resorts to self-supervised learning (SSL) techniques to extract meaningful
structural representations from large-scale, unlabeled molecular data,
subsequently fine-tuning these representations for an array of downstream
tasks. However, an inherent shortcoming of these studies lies in their singular
reliance on one modality of molecular information, such as molecule image or
SMILES representations, thus neglecting the potential complementarity of
various molecular modalities. In response to this limitation, we propose MolIG,
a novel MultiModaL molecular pre-training framework for predicting molecular
properties based on Image and Graph structures. MolIG model innovatively
leverages the coherence and correlation between molecule graph and molecule
image to execute self-supervised tasks, effectively amalgamating the strengths
of both molecular representation forms. This holistic approach allows for the
capture of pivotal molecular structural characteristics and high-level semantic
information. Upon completion of pre-training, Graph Neural Network (GNN)
Encoder is used for the prediction of downstream tasks. In comparison to
advanced baseline models, MolIG exhibits enhanced performance in downstream
tasks pertaining to molecular property prediction within benchmark groups such
as MoleculeNet Benchmark Group and ADMET Benchmark Group.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16671">SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation. (arXiv:2311.16671v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zarzar_J/0/1/0/all/0/1">Jesus Zarzar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a></p>
<p>We present a novel approach for digitizing real-world objects by estimating
their geometry, material properties, and environmental lighting from a set of
posed images with fixed lighting. Our method incorporates into Neural Radiance
Field (NeRF) pipelines the split sum approximation used with image-based
lighting for real-time physical-based rendering. We propose modeling the
scene's lighting with a single scene-specific MLP representing pre-integrated
image-based lighting at arbitrary resolutions. We achieve accurate modeling of
pre-integrated lighting by exploiting a novel regularizer based on efficient
Monte Carlo sampling. Additionally, we propose a new method of supervising
self-occlusion predictions by exploiting a similar regularizer based on Monte
Carlo sampling. Experimental results demonstrate the efficiency and
effectiveness of our approach in estimating scene geometry, material
properties, and lighting. Our method is capable of attaining state-of-the-art
relighting quality after only ${\sim}1$ hour of training in a single NVIDIA
A100 GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16673">Large Language Models Meet Computer Vision: A Brief Survey. (arXiv:2311.16673v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hamadi_R/0/1/0/all/0/1">Raby Hamadi</a></p>
<p>Recently, the intersection of Large Language Models (LLMs) and Computer
Vision (CV) has emerged as a pivotal area of research, driving significant
advancements in the field of Artificial Intelligence (AI). As transformers have
become the backbone of many state-of-the-art models in both Natural Language
Processing (NLP) and CV, understanding their evolution and potential
enhancements is crucial. This survey paper delves into the latest progressions
in the domain of transformers and their subsequent successors, emphasizing
their potential to revolutionize Vision Transformers (ViTs) and LLMs. This
survey also presents a comparative analysis, juxtaposing the performance
metrics of several leading paid and open-source LLMs, shedding light on their
strengths and areas of improvement as well as a literature review on how LLMs
are being used to tackle vision related tasks. Furthermore, the survey presents
a comprehensive collection of datasets employed to train LLMs, offering
insights into the diverse data available to achieve high performance in various
pre-training and downstream tasks of LLMs. The survey is concluded by
highlighting open directions in the field, suggesting potential venues for
future research and development. This survey aims to underscores the profound
intersection of LLMs on CV, leading to a new era of integrated and advanced AI
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16680">ROSO: Improving Robotic Policy Inference via Synthetic Observations. (arXiv:2311.16680v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miyashita_Y/0/1/0/all/0/1">Yusuke Miyashita</a>, <a href="http://arxiv.org/find/cs/1/au:+Gahtidis_D/0/1/0/all/0/1">Dimitris Gahtidis</a>, <a href="http://arxiv.org/find/cs/1/au:+La_C/0/1/0/all/0/1">Colin La</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabinowicz_J/0/1/0/all/0/1">Jeremy Rabinowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Leitner_J/0/1/0/all/0/1">Juxi Leitner</a></p>
<p>In this paper, we propose the use of generative artificial intelligence (AI)
to improve zero-shot performance of a pre-trained policy by altering
observations during inference. Modern robotic systems, powered by advanced
neural networks, have demonstrated remarkable capabilities on pre-trained
tasks. However, generalizing and adapting to new objects and environments is
challenging, and fine-tuning visuomotor policies is time-consuming. To overcome
these issues we propose Robotic Policy Inference via Synthetic Observations
(ROSO). ROSO uses stable diffusion to pre-process a robot's observation of
novel objects during inference time to fit within its distribution of
observations of the pre-trained policies. This novel paradigm allows us to
transfer learned knowledge from known tasks to previously unseen scenarios,
enhancing the robot's adaptability without requiring lengthy fine-tuning. Our
experiments show that incorporating generative AI into robotic inference
significantly improves successful outcomes, finishing up to 57% of tasks
otherwise unsuccessful with the pre-trained policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16681">Understanding the (Extra-)Ordinary: Validating Deep Model Decisions with Prototypical Concept-based Explanations. (arXiv:2311.16681v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1">Maximilian Dreyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Achtibat_R/0/1/0/all/0/1">Reduan Achtibat</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1">Sebastian Lapuschkin</a></p>
<p>Ensuring both transparency and safety is critical when deploying Deep Neural
Networks (DNNs) in high-risk applications, such as medicine. The field of
explainable AI (XAI) has proposed various methods to comprehend the
decision-making processes of opaque DNNs. However, only few XAI methods are
suitable of ensuring safety in practice as they heavily rely on repeated
labor-intensive and possibly biased human assessment. In this work, we present
a novel post-hoc concept-based XAI framework that conveys besides instance-wise
(local) also class-wise (global) decision-making strategies via prototypes.
What sets our approach apart is the combination of local and global strategies,
enabling a clearer understanding of the (dis-)similarities in model decisions
compared to the expected (prototypical) concept use, ultimately reducing the
dependence on human long-term assessment. Quantifying the deviation from
prototypical behavior not only allows to associate predictions with specific
model sub-strategies but also to detect outlier behavior. As such, our approach
constitutes an intuitive and explainable tool for model validation. We
demonstrate the effectiveness of our approach in identifying
out-of-distribution samples, spurious model behavior and data quality issues
across three datasets (ImageNet, CUB-200, and CIFAR-10) utilizing VGG, ResNet,
and EfficientNet architectures. Code is available on
https://github.com/maxdreyer/pcx.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16683">Hyper-Relational Knowledge Graph Neural Network for Next POI. (arXiv:2311.16683v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jixiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongkang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1">Ruotong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zipei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xuan Song</a></p>
<p>With the advancement of mobile technology, Point of Interest (POI)
recommendation systems in Location-based Social Networks (LBSN) have brought
numerous benefits to both users and companies. Many existing works employ
Knowledge Graph (KG) to alleviate the data sparsity issue in LBSN. These
approaches primarily focus on modeling the pair-wise relations in LBSN to
enrich the semantics and thereby relieve the data sparsity issue. However,
existing approaches seldom consider the hyper-relations in LBSN, such as the
mobility relation (a 3-ary relation: user-POI-time). This makes the model hard
to exploit the semantics accurately. In addition, prior works overlook the rich
structural information inherent in KG, which consists of higher-order relations
and can further alleviate the impact of data sparsity.To this end, we propose a
Hyper-Relational Knowledge Graph Neural Network (HKGNN) model. In HKGNN, a
Hyper-Relational Knowledge Graph (HKG) that models the LBSN data is constructed
to maintain and exploit the rich semantics of hyper-relations. Then we proposed
a Hypergraph Neural Network to utilize the structural information of HKG in a
cohesive way. In addition, a self-attention network is used to leverage
sequential information and make personalized recommendations. Furthermore, side
information, essential in reducing data sparsity by providing background
knowledge of POIs, is not fully utilized in current methods. In light of this,
we extended the current dataset with available side information to further
lessen the impact of data sparsity. Results of experiments on four real-world
LBSN datasets demonstrate the effectiveness of our approach compared to
existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16700">Rethinking Intermediate Layers design in Knowledge Distillation for Kidney and Liver Tumor Segmentation. (arXiv:2311.16700v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorade_V/0/1/0/all/0/1">Vandan Gorade</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Sparsh Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1">Debesh Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1">Ulas Bagci</a></p>
<p>Knowledge distillation(KD) has demonstrated remarkable success across various
domains, but its application to medical imaging tasks, such as kidney and liver
tumor segmentation, has encountered challenges. Many existing KD methods are
not specifically tailored for these tasks. Moreover, prevalent KD methods often
lack a careful consideration of what and from where to distill knowledge from
the teacher to the student. This oversight may lead to issues like the
accumulation of training bias within shallower student layers, potentially
compromising the effectiveness of KD. To address these challenges, we propose
Hierarchical Layer-selective Feedback Distillation (HLFD). HLFD strategically
distills knowledge from a combination of middle layers to earlier layers and
transfers final layer knowledge to intermediate layers at both the feature and
pixel levels. This design allows the model to learn higher-quality
representations from earlier layers, resulting in a robust and compact student
model. Extensive quantitative evaluations reveal that HLFD outperforms existing
methods by a significant margin. For example, in the kidney segmentation task,
HLFD surpasses the student model (without KD) by over 10pp, significantly
improving its focus on tumor-specific features. From a qualitative standpoint,
the student model trained using HLFD excels at suppressing irrelevant
information and can focus sharply on tumor-specific details, which opens a new
pathway for more efficient and accurate diagnostic tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16711">LEDITS++: Limitless Image Editing using Text-to-Image Models. (arXiv:2311.16711v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1">Manuel Brack</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1">Felix Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornmeier_K/0/1/0/all/0/1">Katharina Kornmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsaban_L/0/1/0/all/0/1">Linoy Tsaban</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a>, <a href="http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1">Apolin&#xe1;rio Passos</a></p>
<p>Text-to-image diffusion models have recently received increasing interest for
their astonishing ability to produce high-fidelity images from solely text
inputs. Subsequent research efforts aim to exploit and apply their capabilities
to real image editing. However, existing image-to-image methods are often
inefficient, imprecise, and of limited versatility. They either require
time-consuming fine-tuning, deviate unnecessarily strongly from the input
image, and/or lack support for multiple, simultaneous edits. To address these
issues, we introduce LEDITS++, an efficient yet versatile and precise textual
image manipulation technique. LEDITS++'s novel inversion approach requires no
tuning nor optimization and produces high-fidelity results with a few diffusion
steps. Second, our methodology supports multiple simultaneous edits and is
architecture-agnostic. Third, we use a novel implicit masking technique that
limits changes to relevant image regions. We propose the novel TEdBench++
benchmark as part of our exhaustive evaluation. Our results demonstrate the
capabilities of LEDITS++ and its improvements over previous methods. The
project page is available at https://leditsplusplus-project.static.hf.space .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16716">Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuhao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1">Lianghao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Da Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1">Kangyi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chao Huang</a></p>
<p>GNN-based recommenders have excelled in modeling intricate user-item
interactions through multi-hop message passing. However, existing methods often
overlook the dynamic nature of evolving user-item interactions, which impedes
the adaption to changing user preferences and distribution shifts in newly
arriving data. Thus, their scalability and performances in real-world dynamic
environments are limited. In this study, we propose GraphPL, a framework that
incorporates parameter-efficient and dynamic graph pre-training with prompt
learning. This novel combination empowers GNNs to effectively capture both
long-term user preferences and short-term behavior dynamics, enabling the
delivery of accurate and timely recommendations. Our GraphPL framework
addresses the challenge of evolving user preferences by seamlessly integrating
a temporal prompt mechanism and a graph-structural prompt learning mechanism
into the pre-trained GNN model. The temporal prompt mechanism encodes time
information on user-item interaction, allowing the model to naturally capture
temporal context, while the graph-structural prompt learning mechanism enables
the transfer of pre-trained knowledge to adapt to behavior dynamics without the
need for continuous incremental training. We further bring in a dynamic
evaluation setting for recommendation to mimic real-world dynamic scenarios and
bridge the offline-online gap to a better level. Our extensive experiments
including a large-scale industrial deployment showcases the lightweight plug-in
scalability of our GraphPL when integrated with various state-of-the-art
recommenders, emphasizing the advantages of GraphPL in terms of effectiveness,
robustness and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16733">LLMs for Science: Usage for Code Generation and Data Analysis. (arXiv:2311.16733v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nejjar_M/0/1/0/all/0/1">Mohamed Nejjar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zacharias_L/0/1/0/all/0/1">Luca Zacharias</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiehle_F/0/1/0/all/0/1">Fabian Stiehle</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1">Ingo Weber</a></p>
<p>Large language models (LLMs) have been touted to enable increased
productivity in many areas of today's work life. Scientific research as an area
of work is no exception: the potential of LLM-based tools to assist in the
daily work of scientists has become a highly discussed topic across
disciplines. However, we are only at the very onset of this subject of study.
It is still unclear how the potential of LLMs will materialise in research
practice. With this study, we give first empirical evidence on the use of LLMs
in the research process. We have investigated a set of use cases for LLM-based
tools in scientific research, and conducted a first study to assess to which
degree current tools are helpful. In this paper we report specifically on use
cases related to software engineering, such as generating application code and
developing scripts for data analytics. While we studied seemingly simple use
cases, results across tools differ significantly. Our results highlight the
promise of LLM-based tools in general, yet we also observe various issues,
particularly regarding the integrity of the output these tools provide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16754">Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird&#x27;s Eye View Segmentation for Connected and Autonomous Driving. (arXiv:2311.16754v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Senkang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhengru Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuguang Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1">Sam Kwong</a></p>
<p>Collaborative perception has recently gained significant attention in
autonomous driving, improving perception quality by enabling the exchange of
additional information among vehicles. However, deploying collaborative
perception systems can lead to domain shifts due to diverse environmental
conditions and data heterogeneity among connected and autonomous vehicles
(CAVs). To address these challenges, we propose a unified domain generalization
framework applicable in both training and inference stages of collaborative
perception. In the training phase, we introduce an Amplitude Augmentation
(AmpAug) method to augment low-frequency image variations, broadening the
model's ability to learn across various domains. We also employ a
meta-consistency training scheme to simulate domain shifts, optimizing the
model with a carefully designed consistency loss to encourage domain-invariant
representations. In the inference phase, we introduce an intra-system domain
alignment mechanism to reduce or potentially eliminate the domain discrepancy
among CAVs prior to inference. Comprehensive experiments substantiate the
effectiveness of our method in comparison with the existing state-of-the-art
works. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16769">Equilibrium in the Computing Continuum through Active Inference. (arXiv:2311.16769v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedlak_B/0/1/0/all/0/1">Boris Sedlak</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujol_V/0/1/0/all/0/1">Victor Casamayor Pujol</a>, <a href="http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1">Praveen Kumar Donta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1">Schahram Dustdar</a></p>
<p>Computing Continuum (CC) systems are challenged to ensure the intricate
requirements of each computational tier. Given the system's scale, the Service
Level Objectives (SLOs) which are expressed as these requirements, must be
broken down into smaller parts that can be decentralized. We present our
framework for collaborative edge intelligence enabling individual edge devices
to (1) develop a causal understanding of how to enforce their SLOs, and (2)
transfer knowledge to speed up the onboarding of heterogeneous devices. Through
collaboration, they (3) increase the scope of SLO fulfillment. We implemented
the framework and evaluated a use case in which a CC system is responsible for
ensuring Quality of Service (QoS) and Quality of Experience (QoE) during video
streaming. Our results showed that edge devices required only ten training
rounds to ensure four SLOs; furthermore, the underlying causal structures were
also rationally explainable. The addition of new types of devices can be done a
posteriori, the framework allowed them to reuse existing models, even though
the device type had been unknown. Finally, rebalancing the load within a device
cluster allowed individual edge devices to recover their SLO compliance after a
network failure from 22% to 89%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.01566">Strategyproof and Proportionally Fair Facility Location. (arXiv:2111.01566v3 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aziz_H/0/1/0/all/0/1">Haris Aziz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1">Alexander Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Barton E. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Walsh_T/0/1/0/all/0/1">Toby Walsh</a></p>
<p>We focus on a simple, one-dimensional collective decision problem (often
referred to as the facility location problem) and explore issues of
strategyproofness and proportionality-based fairness. We introduce and analyze
a hierarchy of proportionality-based fairness axioms of varying strength:
Individual Fair Share (IFS), Unanimous Fair Share (UFS), Proportionality (as in
Freeman et al, 2021), and Proportional Fairness (PF). For each axiom, we
characterize the family of mechanisms that satisfy the axiom and
strategyproofness. We show that imposing strategyproofness renders many of the
axioms to be equivalent: the family of mechanisms that satisfy proportionality,
unanimity, and strategyproofness is equivalent to the family of mechanisms that
satisfy UFS and strategyproofness, which, in turn, is equivalent to the family
of mechanisms that satisfy PF and strategyproofness. Furthermore, there is a
unique such mechanism: the Uniform Phantom mechanism, which is studied in
Freeman et al. (2021). We also characterize the outcomes of the Uniform Phantom
mechanism as the unique (pure) equilibrium outcome for any mechanism that
satisfies continuity, strict monotonicity, and UFS. Finally, we analyze the
approximation guarantees, in terms of optimal social welfare and minimum total
cost, obtained by mechanisms that are strategyproof and satisfy each
proportionality-based fairness axiom. We show that the Uniform Phantom
mechanism provides the best approximation of the optimal social welfare (and
also minimum total cost) among all mechanisms that satisfy UFS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.03002">GraphPrompt: Graph-Based Prompt Templates for Biomedical Synonym Prediction. (arXiv:2112.03002v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiayou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shizhuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhalerao_M/0/1/0/all/0/1">Megh Manoj Bhalerao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yucong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dawei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sheng Wang</a></p>
<p>In the expansion of biomedical dataset, the same category may be labeled with
different terms, thus being tedious and onerous to curate these terms.
Therefore, automatically mapping synonymous terms onto the ontologies is
desirable, which we name as biomedical synonym prediction task. Unlike
biomedical concept normalization (BCN), no clues from context can be used to
enhance synonym prediction, making it essential to extract graph features from
ontology. We introduce an expert-curated dataset OBO-syn encompassing 70
different types of concepts and 2 million curated concept-term pairs for
evaluating synonym prediction methods. We find BCN methods perform weakly on
this task for not making full use of graph information. Therefore, we propose
GraphPrompt, a prompt-based learning approach that creates prompt templates
according to the graphs. GraphPrompt obtained 37.2\% and 28.5\% improvement on
zero-shot and few-shot settings respectively, indicating the effectiveness of
these graph-based prompt templates. We envision that our method GraphPrompt and
OBO-syn dataset can be broadly applied to graph-based NLP tasks, and serve as
the basis for analyzing diverse and accumulating biomedical data. All the data
and codes are avalible at: https://github.com/HanwenXuTHU/GraphPrompt
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.05760">Big Data Analytics for Network Level Short-Term Travel Time Prediction with Hierarchical LSTM. (arXiv:2201.05760v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianya T. Zhang</a></p>
<p>The travel time data collected from widespread traffic monitoring sensors
necessitate big data analytic tools for querying, visualization, and
identifying meaningful traffic patterns. This paper utilizes a large-scale
travel time dataset from Caltrans Performance Measurement System (PeMS) system
that is an overflow for traditional data processing and modeling tools. To
overcome the challenges of the massive amount of data, the big data analytic
engines Apache Spark and Apache MXNet are applied for data wrangling and
modeling. Seasonality and autocorrelation were performed to explore and
visualize the trend of time-varying data. Inspired by the success of the
hierarchical architecture for many Artificial Intelligent (AI) tasks, we
consolidate the cell and hidden states passed from low-level to the high-level
LSTM with an attention pooling similar to how the human perception system
operates. The designed hierarchical LSTM model can consider the dependencies at
different time scales to capture the spatial-temporal correlations of
network-level travel time. Another self-attention module is then devised to
connect LSTM extracted features to the fully connected layers, predicting
travel time for all corridors instead of a single link/route. The comparison
results show that the Hierarchical LSTM with Attention (HierLSTMat) model gives
the best prediction results at 30-minute and 45-min horizons and can
successfully forecast unusual congestion. The efficiency gained from big data
analytic tools was evaluated by comparing them with popular data science and
deep learning frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02541">PCPT and ACPT: Copyright Protection and Traceability Scheme for DNN Models. (arXiv:2206.02541v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xuefeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Dahao Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1">Hangyu Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaoyi Zhou</a></p>
<p>Deep neural networks (DNNs) have achieved tremendous success in artificial
intelligence (AI) fields. However, DNN models can be easily illegally copied,
redistributed, or abused by criminals, seriously damaging the interests of
model inventors. The copyright protection of DNN models by neural network
watermarking has been studied, but the establishment of a traceability
mechanism for determining the authorized users of a leaked model is a new
problem driven by the demand for AI services. Because the existing traceability
mechanisms are used for models without watermarks, a small number of
false-positives are generated. Existing black-box active protection schemes
have loose authorization control and are vulnerable to forgery attacks.
Therefore, based on the idea of black-box neural network watermarking with the
video framing and image perceptual hash algorithm, a passive copyright
protection and traceability framework PCPT is proposed that uses an additional
class of DNN models, improving the existing traceability mechanism that yields
a small number of false-positives. Based on an authorization control strategy
and image perceptual hash algorithm, a DNN model active copyright protection
and traceability framework ACPT is proposed. This framework uses the
authorization control center constructed by the detector and verifier. This
approach realizes stricter authorization control, which establishes a strong
connection between users and model owners, improves the framework security, and
supports traceability verification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11792">Two-dimensional total absorption spectroscopy with conditional generative adversarial networks. (arXiv:2206.11792v2 [nucl-ex] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/nucl-ex/1/au:+Dembski_C/0/1/0/all/0/1">Cade Dembski</a>, <a href="http://arxiv.org/find/nucl-ex/1/au:+Kuchera_M/0/1/0/all/0/1">Michelle P. Kuchera</a>, <a href="http://arxiv.org/find/nucl-ex/1/au:+Liddick_S/0/1/0/all/0/1">Sean Liddick</a>, <a href="http://arxiv.org/find/nucl-ex/1/au:+Ramanujan_R/0/1/0/all/0/1">Raghu Ramanujan</a>, <a href="http://arxiv.org/find/nucl-ex/1/au:+Spyrou_A/0/1/0/all/0/1">Artemis Spyrou</a></p>
<p>We explore the use of machine learning techniques to remove the response of
large volume $\gamma$-ray detectors from experimental spectra. Segmented
$\gamma$-ray total absorption spectrometers (TAS) allow for the simultaneous
measurement of individual $\gamma$-ray energy (E$_\gamma$) and total excitation
energy (E$_x$). Analysis of TAS detector data is complicated by the fact that
the E$_x$ and E$_\gamma$ quantities are correlated, and therefore, techniques
that simply unfold using E$_x$ and E$_\gamma$ response functions independently
are not as accurate. In this work, we investigate the use of conditional
generative adversarial networks (cGANs) to simultaneously unfold $E_{x}$ and
$E_{\gamma}$ data in TAS detectors. Specifically, we employ a \texttt{Pix2Pix}
cGAN, a generative modeling technique based on recent advances in deep
learning, to treat \rawmatrix~ matrix unfolding as an image-to-image
translation problem. We present results for simulated and experimental matrices
of single-$\gamma$ and double-$\gamma$ decay cascades. Our model demonstrates
characterization capabilities within detector resolution limits for upwards of
93% of simulated test cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.02804">Just ClozE! A Novel Framework for Evaluating the Factual Consistency Faster in Abstractive Summarization. (arXiv:2210.02804v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Litvak_M/0/1/0/all/0/1">Marina Litvak</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanetik_N/0/1/0/all/0/1">Natalia Vanetik</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dingxin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yanquan Zhou</a></p>
<p>The issue of factual consistency in abstractive summarization has received
extensive attention in recent years, and the evaluation of factual consistency
between summary and document has become an important and urgent task. Most of
the current evaluation metrics are adopted from the question answering (QA) or
natural language inference (NLI) task. However, the application of QA-based
metrics is extremely time-consuming in practice while NLI-based metrics are
lack of interpretability. In this paper, we propose a cloze-based evaluation
framework called ClozE and show the great potential of the cloze-based metric.
It inherits strong interpretability from QA, while maintaining the speed of
NLI- level reasoning. We demonstrate that ClozE can reduce the evaluation time
by nearly 96% relative to QA-based metrics while retaining their
interpretability and performance through experiments on six human-annotated
datasets and a meta-evaluation benchmark GO FIGURE (Gabriel et al., 2021).
Finally, we discuss three important facets of ClozE in practice, which further
shows better overall performance of ClozE compared to other metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.05228">FIXED: Frustratingly Easy Domain Generalization with Mixup. (arXiv:2211.05228v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiqiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Domain generalization (DG) aims to learn a generalizable model from multiple
training domains such that it can perform well on unseen target domains. A
popular strategy is to augment training data to benefit generalization through
methods such as Mixup~\cite{zhang2018mixup}. While the vanilla Mixup can be
directly applied, theoretical and empirical investigations uncover several
shortcomings that limit its performance. Firstly, Mixup cannot effectively
identify the domain and class information that can be used for learning
invariant representations. Secondly, Mixup may introduce synthetic noisy data
points via random interpolation, which lowers its discrimination capability.
Based on the analysis, we propose a simple yet effective enhancement for
Mixup-based DG, namely domain-invariant Feature mIXup (FIX). It learns
domain-invariant representations for Mixup. To further enhance discrimination,
we leverage existing techniques to enlarge margins among classes to further
propose the domain-invariant Feature MIXup with Enhanced Discrimination (FIXED)
approach. We present theoretical insights about guarantees on its
effectiveness. Extensive experiments on seven public datasets across two
modalities including image classification (Digits-DG, PACS, Office-Home) and
time series (DSADS, PAMAP2, UCI-HAR, and USC-HAD) demonstrate that our approach
significantly outperforms nine state-of-the-art related methods, beating the
best performing baseline by 6.5\% on average in terms of test accuracy. Code is
available at:
https://github.com/jindongwang/transferlearning/tree/master/code/deep/fixed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13131">FeTrIL: Feature Translation for Exemplar-Free Class-Incremental Learning. (arXiv:2211.13131v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petit_G/0/1/0/all/0/1">Gr&#xe9;goire Petit</a>, <a href="http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1">Adrian Popescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_H/0/1/0/all/0/1">Hugo Schindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1">David Picard</a>, <a href="http://arxiv.org/find/cs/1/au:+Delezoide_B/0/1/0/all/0/1">Bertrand Delezoide</a></p>
<p>Exemplar-free class-incremental learning is very challenging due to the
negative effect of catastrophic forgetting. A balance between stability and
plasticity of the incremental process is needed in order to obtain good
accuracy for past as well as new classes. Existing exemplar-free
class-incremental methods focus either on successive fine tuning of the model,
thus favoring plasticity, or on using a feature extractor fixed after the
initial incremental state, thus favoring stability. We introduce a method which
combines a fixed feature extractor and a pseudo-features generator to improve
the stability-plasticity balance. The generator uses a simple yet effective
geometric translation of new class features to create representations of past
classes, made of pseudo-features. The translation of features only requires the
storage of the centroid representations of past classes to produce their
pseudo-features. Actual features of new classes and pseudo-features of past
classes are fed into a linear classifier which is trained incrementally to
discriminate between all classes. The incremental process is much faster with
the proposed method compared to mainstream ones which update the entire deep
model. Experiments are performed with three challenging datasets, and different
incremental settings. A comparison with ten existing methods shows that our
method outperforms the others in most cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09744">DSI++: Updating Transformer Memory with New Documents. (arXiv:2212.09744v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Sanket Vaibhav Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jai Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1">Yi Tay</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mostafa Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1">Vinh Q. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1">Jinfeng Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1">Marc Najork</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a>, <a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1">Donald Metzler</a></p>
<p>Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04023">A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. (arXiv:2302.04023v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1">Yejin Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1">Samuel Cahyawijaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Wenliang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1">Dan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1">Bryan Wilie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1">Holy Lovenia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Ziwei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tiezheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1">Willy Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1">Quyet V. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1">Pascale Fung</a></p>
<p>This paper proposes a framework for quantitatively evaluating interactive
LLMs such as ChatGPT using publicly available data sets. We carry out an
extensive technical evaluation of ChatGPT using 23 data sets covering 8
different common NLP application tasks. We evaluate the multitask, multilingual
and multi-modal aspects of ChatGPT based on these data sets and a newly
designed multimodal dataset. We find that ChatGPT outperforms LLMs with
zero-shot learning on most tasks and even outperforms fine-tuned models on some
tasks. We find that it is better at understanding non-Latin script languages
than generating them. It is able to generate multimodal content from textual
prompts, via an intermediate code generation step. Moreover, we find that
ChatGPT is 63.41% accurate on average in 10 different reasoning categories
under logical reasoning, non-textual reasoning, and commonsense reasoning,
hence making it an unreliable reasoner. It is, for example, better at deductive
than inductive reasoning. ChatGPT suffers from hallucination problems like
other LLMs and it generates more extrinsic hallucinations from its parametric
memory as it does not have access to an external knowledge base. Finally, the
interactive feature of ChatGPT enables human collaboration with the underlying
LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++
on machine translation, in a multi-turn "prompt engineering" fashion. We also
release codebase for evaluation set extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01345">PlaNet-ClothPick: Effective Fabric Flattening Based on Latent Dynamic Planning. (arXiv:2303.01345v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kadi_H/0/1/0/all/0/1">Halid Abdulrahim Kadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Terzic_K/0/1/0/all/0/1">Kasim Terzic</a></p>
<p>Why do Recurrent State Space Models such as PlaNet fail at cloth manipulation
tasks? Recent work has attributed this to the blurry prediction of the
observation, which makes it difficult to plan directly in the latent space.
This paper explores the reasons behind this by applying PlaNet in the
pick-and-place fabric-flattening domain. We find that the sharp discontinuity
of the transition function on the contour of the fabric makes it difficult to
learn an accurate latent dynamic model, causing the MPC planner to produce pick
actions slightly outside of the article. By limiting picking space on the cloth
mask and training on specially engineered trajectories, our mesh-free
PlaNet-ClothPick surpasses visual planning and policy learning methods on
principal metrics in simulation, achieving similar performance as
state-of-the-art mesh-based planning approaches. Notably, our model exhibits a
faster action inference and requires fewer transitional model parameters than
the state-of-the-art robotic systems in this domain. Other supplementary
materials are available at: https://sites.google.com/view/planet-clothpick.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01928">FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arnaiz_Rodriguez_A/0/1/0/all/0/1">Adrian Arnaiz-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliver_N/0/1/0/all/0/1">Nuria Oliver</a></p>
<p>Algorithmic fairness is of utmost societal importance, yet the current trend
in large-scale machine learning models requires training with massive datasets
that are frequently biased. In this context, pre-processing methods that focus
on modeling and correcting bias in the data emerge as valuable approaches. In
this paper, we propose FairShap, a novel instance-level data re-weighting
method for fair algorithmic decision-making through data valuation by means of
Shapley Values. FairShap is model-agnostic and easily interpretable, as it
measures the contribution of each training data point to a predefined fairness
metric. We empirically validate FairShap on several state-of-the-art datasets
of different nature, with a variety of training scenarios and models and show
how it yields fairer models with similar levels of accuracy than the baselines.
We illustrate FairShap's interpretability by means of histograms and latent
space visualizations. Moreover, we perform a utility-fairness study, and
ablation and runtime experiments to illustrate the impact of the size of the
reference dataset and FairShap's computational cost depending on the size of
the dataset and the number of features. We believe that FairShap represents a
promising direction in interpretable and model-agnostic approaches to
algorithmic fairness that yield competitive accuracy even when only biased
datasets are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09373">MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling. (arXiv:2303.09373v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelini_E/0/1/0/all/0/1">Elsa Angelini</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jia Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasmussen_J/0/1/0/all/0/1">Jerod M. Rasmussen</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_T/0/1/0/all/0/1">Thomas G. O&#x27;Connor</a>, <a href="http://arxiv.org/find/cs/1/au:+Wadhwa_P/0/1/0/all/0/1">Pathik D. Wadhwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jackowski_A/0/1/0/all/0/1">Andrea Parolin Jackowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Posner_J/0/1/0/all/0/1">Jonathan Posner</a>, <a href="http://arxiv.org/find/cs/1/au:+Laine_A/0/1/0/all/0/1">Andrew F. Laine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yun Wang</a></p>
<p>Robust segmentation is critical for deriving quantitative measures from
large-scale, multi-center, and longitudinal medical scans. Manually annotating
medical scans, however, is expensive and labor-intensive and may not always be
available in every domain. Unsupervised domain adaptation (UDA) is a
well-studied technique that alleviates this label-scarcity problem by
leveraging available labels from another domain. In this study, we introduce
Masked Autoencoding and Pseudo-Labeling Segmentation (MAPSeg), a
$\textbf{unified}$ UDA framework with great versatility and superior
performance for heterogeneous and volumetric medical image segmentation. To the
best of our knowledge, this is the first study that systematically reviews and
develops a framework to tackle four different domain shifts in medical image
segmentation. More importantly, MAPSeg is the first framework that can be
applied to $\textbf{centralized}$, $\textbf{federated}$, and
$\textbf{test-time}$ UDA while maintaining comparable performance. We compare
MAPSeg with previous state-of-the-art methods on a private infant brain MRI
dataset and a public cardiac CT-MRI dataset, and MAPSeg outperforms others by a
large margin (10.5 Dice improvement on the private MRI dataset and 5.7 on the
public CT-MRI dataset). MAPSeg poses great practical value and can be applied
to real-world problems. Our code and pretrained model will be available later.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11712">Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples). (arXiv:2303.11712v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gamba_E/0/1/0/all/0/1">Emilio Gamba</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogaerts_B/0/1/0/all/0/1">Bart Bogaerts</a>, <a href="http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1">Tias Guns</a></p>
<p>We build on a recently proposed method for stepwise explaining solutions of
Constraint Satisfaction Problems (CSP) in a human-understandable way. An
explanation here is a sequence of simple inference steps where simplicity is
quantified using a cost function. The algorithms for explanation generation
rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived
unsatisfiable formula, exploiting a one-to-one correspondence between so-called
non-redundant explanations and MUSs. However, MUS extraction algorithms do not
provide any guarantee of subset minimality or optimality with respect to a
given cost function. Therefore, we build on these formal foundations and tackle
the main points of improvement, namely how to generate explanations efficiently
that are provably optimal (with respect to the given cost metric). For that, we
developed (1) a hitting set-based algorithm for finding the optimal constrained
unsatisfiable subsets; (2) a method for re-using relevant information over
multiple algorithm calls; and (3) methods exploiting domain-specific
information to speed up the explanation sequence generation. We experimentally
validated our algorithms on a large number of CSP problems. We found that our
algorithms outperform the MUS approach in terms of explanation quality and
computational time (on average up to 56 % faster than a standard MUS approach).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.18158">Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Shafiee_S/0/1/0/all/0/1">Soroosh Shafiee</a>, <a href="http://arxiv.org/find/math/1/au:+Kilinc_Karzan_F/0/1/0/all/0/1">Fatma K&#x131;l&#x131;n&#xe7;-Karzan</a></p>
<p>Optimization problems involving minimization of a rank-one convex function
over constraints modeling restrictions on the support of the decision variables
emerge in various machine learning applications. These problems are often
modeled with indicator variables for identifying the support of the continuous
variables. In this paper we investigate compact extended formulations for such
problems through perspective reformulation techniques. In contrast to the
majority of previous work that relies on support function arguments and
disjunctive programming techniques to provide convex hull results, we propose a
constructive approach that exploits a hidden conic structure induced by
perspective functions. To this end, we first establish a convex hull result for
a general conic mixed-binary set in which each conic constraint involves a
linear function of independent continuous variables and a set of binary
variables. We then demonstrate that extended representations of sets associated
with epigraphs of rank-one convex functions over constraints modeling indicator
relations naturally admit such a conic representation. This enables us to
systematically give perspective formulations for the convex hull descriptions
of these sets with nonlinear separable or non-separable objective functions,
sign constraints on continuous variables, and combinatorial constraints on
indicator variables. We illustrate the efficacy of our results on sparse
nonnegative logistic regression problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12479">AGI: Artificial General Intelligence for Education. (arXiv:2304.12479v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1">Ehsan Latif</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1">Gengchen Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyaaba_M/0/1/0/all/0/1">Matthew Nyaaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuansheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ninghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guoyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>Artificial general intelligence (AGI) has gained global recognition as a
future technology due to the emergence of breakthrough large language models
and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional
AI models, typically designed for a limited range of tasks, demand significant
amounts of domain-specific data for training and may not always consider
intricate interpersonal dynamics in education. AGI, driven by the recent large
pre-trained models, represents a significant leap in the capability of machines
to perform tasks that require human-level intelligence, such as reasoning,
problem-solving, decision-making, and even understanding human emotions and
social interactions. This position paper reviews AGI's key concepts,
capabilities, scope, and potential within future education, including achieving
future educational goals, designing pedagogy and curriculum, and performing
assessments. It highlights that AGI can significantly improve intelligent
tutoring systems, educational assessment, and evaluation procedures. AGI
systems can adapt to individual student needs, offering tailored learning
experiences. They can also provide comprehensive feedback on student
performance and dynamically adjust teaching methods based on student progress.
The paper emphasizes that AGI's capabilities extend to understanding human
emotions and social interactions, which are critical in educational settings.
The paper discusses that ethical issues in education with AGI include data
bias, fairness, and privacy and emphasizes the need for codes of conduct to
ensure responsible AGI use in academic settings like homework, teaching, and
recruitment. We also conclude that the development of AGI necessitates
interdisciplinary collaborations between educators and AI engineers to advance
research and application efforts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08415">Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v3 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1">Francesco Conti</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulin_G/0/1/0/all/0/1">Gianna Paulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Garofalo_A/0/1/0/all/0/1">Angelo Garofalo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1">Davide Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mauro_A/0/1/0/all/0/1">Alfio Di Mauro</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutishauser_G/0/1/0/all/0/1">Georg Rutishauser</a>, <a href="http://arxiv.org/find/cs/1/au:+Ottavi_G/0/1/0/all/0/1">Gianmarco Ottavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Eggimann_M/0/1/0/all/0/1">Manuel Eggimann</a>, <a href="http://arxiv.org/find/cs/1/au:+Okuhara_H/0/1/0/all/0/1">Hayate Okuhara</a>, <a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1">Luca Benini</a></p>
<p>Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT)
System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and
nano-robotics need to run many diverse tasks within a power envelope of a few
tens of mW over a wide range of operating conditions: compute-intensive but
strongly quantized Deep Neural Network (DNN) inference, as well as signal
processing and control requiring high-precision floating-point. We present
Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in
GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16
RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a
diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions
(XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support;
2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1
(pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks
connected to an Adaptive Body Biasing (ABB) generator and a hardware control
loop, enabling on-the-fly adaptation of transistor threshold voltages.
Marsellus achieves up to 180 Gop/s or 3.32 Top/s/W on 2-bit precision
arithmetic in software, and up to 637 Gop/s or 12.4 Top/s/W on
hardware-accelerated DNN layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14561">Negative Feedback Training: A Novel Concept to Improve Robustness of NVCIM DNN Accelerators. (arXiv:2305.14561v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yifan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zheyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobo Sharon Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yiyu Shi</a></p>
<p>Compute-in-memory (CIM) accelerators built upon non-volatile memory (NVM)
devices excel in energy efficiency and latency when performing Deep Neural
Network (DNN) inference, thanks to their in-situ data processing capability.
However, the stochastic nature and intrinsic variations of NVM devices often
result in performance degradation in DNN inference. Introducing these non-ideal
device behaviors during DNN training enhances robustness, but drawbacks include
limited accuracy improvement, reduced prediction confidence, and convergence
issues. This arises from a mismatch between the deterministic training and
non-deterministic device variations, as such training, though considering
variations, relies solely on the model's final output. In this work, we draw
inspiration from the control theory and propose a novel training concept:
Negative Feedback Training (NFT) leveraging the multi-scale noisy information
captured from network. We develop two specific NFT instances, Oriented
Variational Forward (OVF) and Intermediate Representation Snapshot (IRS).
Extensive experiments show that our methods outperform existing
state-of-the-art methods with up to a 46.71% improvement in inference accuracy
while reducing epistemic uncertainty, boosting output confidence, and improving
convergence probability. Their effectiveness highlights the generality and
practicality of our NFT concept in enhancing DNN robustness against device
variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14726">In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1">Dan Iter</a>, <a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1">Reid Pryzant</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruochen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuohang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yichong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chenguang Zhu</a></p>
<p>Large language models (LLMs) can use in-context demonstrations to improve
performance on zero-shot tasks. However, selecting the best in-context examples
is challenging because model performance can vary widely depending on the
selected examples. We present a cross-entropy difference (CED) method for
selecting in-context demonstrations. Our method is based on the observation
that the effectiveness of in-context demonstrations negatively correlates with
the perplexity of the test example by a language model that was finetuned on
that demonstration. We utilize parameter efficient finetuning to train small
models on training data that are used for computing the cross-entropy
difference between a test example and every candidate in-context demonstration.
This metric is used to rank and select in-context demonstrations independently
for each test input. We evaluate our method on a mix-domain dataset that
combines 8 benchmarks, representing 4 text generation tasks, showing that CED
for in-context demonstration selection can improve performance for a variety of
LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16378">Sim-Suction: Learning a Suction Grasp Policy for Cluttered Environments Using a Synthetic Benchmark. (arXiv:2305.16378v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cappelleri_D/0/1/0/all/0/1">David J. Cappelleri</a></p>
<p>This paper presents Sim-Suction, a robust object-aware suction grasp policy
for mobile manipulation platforms with dynamic camera viewpoints, designed to
pick up unknown objects from cluttered environments. Suction grasp policies
typically employ data-driven approaches, necessitating large-scale,
accurately-annotated suction grasp datasets. However, the generation of suction
grasp datasets in cluttered environments remains underexplored, leaving
uncertainties about the relationship between the object of interest and its
surroundings. To address this, we propose a benchmark synthetic dataset,
Sim-Suction-Dataset, comprising 500 cluttered environments with 3.2 million
annotated suction grasp poses. The efficient Sim-Suction-Dataset generation
process provides novel insights by combining analytical models with dynamic
physical simulations to create fast and accurate suction grasp pose
annotations. We introduce Sim-Suction-Pointnet to generate robust 6D suction
grasp poses by learning point-wise affordances from the Sim-Suction-Dataset,
leveraging the synergy of zero-shot text-to-segmentation. Real-world
experiments for picking up all objects demonstrate that Sim-Suction-Pointnet
achieves success rates of 96.76%, 94.23%, and 92.39% on cluttered level 1
objects (prismatic shape), cluttered level 2 objects (more complex geometry),
and cluttered mixed objects, respectively. The Sim-Suction policies outperform
state-of-the-art benchmarks tested by approximately 21% in cluttered mixed
scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18228">SR-OOD: Out-of-Distribution Detection via Sample Repairing. (arXiv:2305.18228v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Rui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Andi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jinke Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruimao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a></p>
<p>Out-of-distribution (OOD) detection is a crucial task for ensuring the
reliability and robustness of machine learning models. Recent works have shown
that generative models often assign high confidence scores to OOD samples,
indicating that they fail to capture the semantic information of the data. To
tackle this problem, we take advantage of sample repairing and propose a novel
OOD detection framework, namely SR-OOD. Our framework leverages the idea that
repairing an OOD sample can reveal its semantic inconsistency with the
in-distribution data. Specifically, our framework consists of two components: a
sample repairing module and a detection module. The sample repairing module
applies erosion to an input sample and uses a generative adversarial network to
repair it. The detection module then determines whether the input sample is OOD
using a distance metric. Our framework does not require any additional data or
label information for detection, making it applicable to various scenarios. We
conduct extensive experiments on three image datasets: CIFAR-10, CelebA, and
Pokemon. The results demonstrate that our approach achieves superior
performance over the state-of-the-art generative methods in OOD detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18766">HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance. (arXiv:2305.18766v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Junzhe Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1">Peiye Zhuang</a></p>
<p>The advancements in automatic text-to-3D generation have been remarkable.
Most existing methods use pre-trained text-to-image diffusion models to
optimize 3D representations like Neural Radiance Fields (NeRFs) via
latent-space denoising score matching. Yet, these methods often result in
artifacts and inconsistencies across different views due to their suboptimal
optimization approaches and limited understanding of 3D geometry. Moreover, the
inherent constraints of NeRFs in rendering crisp geometry and stable textures
usually lead to a two-stage optimization to attain high-resolution details.
This work proposes holistic sampling and smoothing approaches to achieve
high-quality text-to-3D generation, all in a single-stage optimization. We
compute denoising scores in the text-to-image diffusion model's latent and
image spaces. Instead of randomly sampling timesteps (also referred to as noise
levels in denoising score matching), we introduce a novel timestep annealing
approach that progressively reduces the sampled timestep throughout
optimization. To generate high-quality renderings in a single-stage
optimization, we propose regularization for the variance of z-coordinates along
NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel
smoothing technique that refines importance sampling weights coarse-to-fine,
ensuring accurate and thorough sampling in high-density regions. Extensive
experiments demonstrate the superiority of our method over previous approaches,
enabling the generation of highly detailed and view-consistent 3D assets
through a single-stage training process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07745">Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vakili_S/0/1/0/all/0/1">Sattar Vakili</a>, <a href="http://arxiv.org/find/cs/1/au:+Olkhovskaya_J/0/1/0/all/0/1">Julia Olkhovskaya</a></p>
<p>Reinforcement learning (RL) has shown empirical success in various real world
settings with complex models and large state-action spaces. The existing
analytical results, however, typically focus on settings with a small number of
state-actions or simple models such as linearly modeled state-action value
functions. To derive RL policies that efficiently handle large state-action
spaces with more general value functions, some recent works have considered
nonlinear function approximation using kernel ridge regression. We propose
$\pi$-KRVI, an optimistic modification of least-squares value iteration, when
the state-action value function is represented by a reproducing kernel Hilbert
space (RKHS). We prove the first order-optimal regret guarantees under a
general setting. Our results show a significant polynomial in the number of
episodes improvement over the state of the art. In particular, with highly
non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the
existing results lead to trivial (superlinear in the number of episodes) regret
bounds. We show a sublinear regret bound that is order optimal in the case of
Mat\'ern kernels where a lower bound on regret is known.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11363">Masked Diffusion Models Are Fast Distribution Learners. (arXiv:2306.11363v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jiachen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qinglong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1">Zhongjie Ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenguang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>Diffusion model has emerged as the \emph{de-facto} model for image
generation, yet the heavy training overhead hinders its broader adoption in the
research community. We observe that diffusion models are commonly trained to
learn all fine-grained visual information from scratch. This paradigm may cause
unnecessary training costs hence requiring in-depth investigation. In this
work, we show that it suffices to train a strong diffusion model by first
pre-training the model to learn some primer distribution that loosely
characterizes the unknown real image distribution. Then the pre-trained model
can be fine-tuned for various generation tasks efficiently. In the pre-training
stage, we propose to mask a high proportion (e.g., up to 90\%) of input images
to approximately represent the primer distribution and introduce a masked
denoising score matching objective to train a model to denoise visible areas.
In subsequent fine-tuning stage, we efficiently train diffusion model without
masking. Utilizing the two-stage training framework, we achieves significant
training acceleration and a new FID score record of 6.27 on CelebA-HQ $256
\times 256$ for ViT-based diffusion models. The generalizability of a
pre-trained model further helps building models that perform better than ones
trained from scratch on different downstream datasets. For instance, a
diffusion model pre-trained on VGGFace2 attains a 46\% quality improvement when
fine-tuned on a different dataset that contains only 3000 images. Our code is
available at \url{https://github.com/jiachenlei/maskdm}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15943">No Transfers Required: Integrating Last Mile with Public Transit Using Opti-Mile. (arXiv:2306.15943v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Altaf_R/0/1/0/all/0/1">Raashid Altaf</a>, <a href="http://arxiv.org/find/cs/1/au:+Biyani_P/0/1/0/all/0/1">Pravesh Biyani</a></p>
<p>Public transit is a popular mode of transit due to its affordability, despite
the inconveniences due to the necessity of transfers required to reach most
areas. For example, in the bus and metro network of New Delhi, only 30% of
stops can be directly accessed from any starting point, thus requiring
transfers for most commutes. Additionally, last-mile services like rickshaws,
tuk-tuks or shuttles are commonly used as feeders to the nearest public transit
access points, which further adds to the complexity and inefficiency of a
journey. Ultimately, users often face a tradeoff between coverage and transfers
to reach their destination, regardless of the mode of transit or the use of
last-mile services. To address the problem of limited accessibility and
inefficiency due to transfers in public transit systems, we propose
``opti-mile," a novel trip planning approach that combines last-mile services
with public transit such that no transfers are required. Opti-mile allows users
to customise trip parameters such as maximum walking distance, and acceptable
fare range. We analyse the transit network of New Delhi, evaluating the
efficiency, feasibility and advantages of opti-mile for optimal multi-modal
trips between randomly selected source-destination pairs. We demonstrate that
opti-mile trips lead to a 10% reduction in distance travelled for 18% increase
in price compared to traditional shortest paths. We also show that opti-mile
trips provide better coverage of the city than public transit, without a
significant fare increase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00154">Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zizheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jianfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Large pretrained plain vision Transformers (ViTs) have been the workhorse for
many downstream tasks. However, existing works utilizing off-the-shelf ViTs are
inefficient in terms of training and deployment, because adopting ViTs with
individual sizes requires separate trainings and is restricted by fixed
performance-efficiency trade-offs. In this paper, we are inspired by stitchable
neural networks (SN-Net), which is a new framework that cheaply produces a
single model that covers rich subnetworks by stitching pretrained model
families, supporting diverse performance-efficiency trade-offs at runtime.
Building upon this foundation, we introduce SN-Netv2, a systematically improved
model stitching framework to facilitate downstream task adaptation.
Specifically, we first propose a two-way stitching scheme to enlarge the
stitching space. We then design a resource-constrained sampling strategy that
takes into account the underlying FLOPs distributions in the space for better
sampling. Finally, we observe that learning stitching layers as a low-rank
update plays an essential role on downstream tasks to stabilize training and
ensure a good Pareto frontier. With extensive experiments on ImageNet-1K,
ADE20K, COCO-Stuff-10K and NYUv2, SN-Netv2 demonstrates superior performance
over SN-Netv1 on downstream dense predictions and shows strong ability as a
flexible vision backbone, achieving great advantages in both training
efficiency and deployment flexibility. Code is available at
https://github.com/ziplab/SN-Netv2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12226">Geometry-Aware Adaptation for Pretrained Models. (arXiv:2307.12226v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1">Nicholas Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xintong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1">Dyah Adila</a>, <a href="http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1">Sonia Cromp</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tzu-Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jitian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a></p>
<p>Machine learning models -- including prominent zero-shot models -- are often
trained on datasets whose labels are only a small proportion of a larger label
space. Such spaces are commonly equipped with a metric that relates the labels
via distances between them. We propose a simple approach to exploit this
information to adapt the trained model to reliably predict new classes -- or,
in the case of zero-shot prediction, to improve its performance -- without any
additional training. Our technique is a drop-in replacement of the standard
prediction rule, swapping argmax with the Fr\'echet mean. We provide a
comprehensive theoretical analysis for this approach, studying (i)
learning-theoretic results trading off label space diameter, sample complexity,
and model dimension, (ii) characterizations of the full range of scenarios in
which it is possible to predict any unobserved class, and (iii) an optimal
active learning-like next class selection procedure to obtain optimal training
classes for when it is not possible to predict the entire range of unobserved
classes. Empirically, using easily-available external metrics, our proposed
approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet
and scales to hundreds of thousands of classes. When no such metric is
available, Loki can use self-derived metrics from class embeddings and obtains
a 10.5% improvement on pretrained zero-shot models such as CLIP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12689">Addressing the Impact of Localized Training Data in Graph Neural Networks. (arXiv:2307.12689v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+A_A/0/1/0/all/0/1">Akansha A</a></p>
<p>Graph Neural Networks (GNNs) have achieved notable success in learning from
graph-structured data, owing to their ability to capture intricate dependencies
and relationships between nodes. They excel in various applications, including
semi-supervised node classification, link prediction, and graph generation.
However, it is important to acknowledge that the majority of state-of-the-art
GNN models are built upon the assumption of an in-distribution setting, which
hinders their performance on real-world graphs with dynamic structures. In this
article, we aim to assess the impact of training GNNs on localized subsets of
the graph. Such restricted training data may lead to a model that performs well
in the specific region it was trained on but fails to generalize and make
accurate predictions for the entire graph. In the context of graph-based
semi-supervised learning (SSL), resource constraints often lead to scenarios
where the dataset is large, but only a portion of it can be labeled, affecting
the model's performance. This limitation affects tasks like anomaly detection
or spam detection when labeling processes are biased or influenced by human
subjectivity. To tackle the challenges posed by localized training data, we
approach the problem as an out-of-distribution (OOD) data issue by by aligning
the distributions between the training data, which represents a small portion
of labeled data, and the graph inference process that involves making
predictions for the entire graph. We propose a regularization method to
minimize distributional discrepancies between localized training data and graph
inference, improving model performance on OOD data. Extensive tests on popular
GNN models show significant performance improvement on three citation GNN
benchmark datasets. The regularization approach effectively enhances model
adaptation and generalization, overcoming challenges posed by OOD data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16769">2-Level Reinforcement Learning for Ships on Inland Waterways. (arXiv:2307.16769v2 [cs.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Waltz_M/0/1/0/all/0/1">Martin Waltz</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulig_N/0/1/0/all/0/1">Niklas Paulig</a>, <a href="http://arxiv.org/find/cs/1/au:+Okhrin_O/0/1/0/all/0/1">Ostap Okhrin</a></p>
<p>This paper proposes a realistic modularized framework for controlling
autonomous surface vehicles (ASVs) on inland waterways (IWs) based on deep
reinforcement learning (DRL). The framework comprises two levels: a high-level
local path planning (LPP) unit and a low-level path following (PF) unit, each
consisting of a DRL agent. The LPP agent is responsible for planning a path
under consideration of nearby vessels, traffic rules, and the geometry of the
waterway. We thereby transfer a recently proposed spatial-temporal recurrent
neural network architecture to continuous action spaces. The LPP agent improves
operational safety in comparison to a state-of-the-art artificial potential
field method by increasing the minimum distance to other vessels by 65% on
average. The PF agent performs low-level actuator control while accounting for
shallow water influences and the environmental forces winds, waves, and
currents. Compared with a proportional-integral-derivative (PID) controller,
the PF agent yields only 61% of the mean cross-track error while significantly
reducing control effort in terms of the required absolute rudder angle. Lastly,
both agents are jointly validated in simulation, employing the lower Elbe in
northern Germany as an example case and using real automatic identification
system (AIS) trajectories to model the behavior of other ships.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12532">FedSOL: Stabilized Orthogonal Learning in Federated Learning. (arXiv:2308.12532v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minchan Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangmook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14610">PolarRec: Radio Interferometric Data Reconstruction with Polar Coordinate Representation. (arXiv:2308.14610v2 [astro-ph.IM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Wang_R/0/1/0/all/0/1">Ruoqi Wang</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Chen_Z/0/1/0/all/0/1">Zhuoyang Chen</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zhu_J/0/1/0/all/0/1">Jiayi Zhu</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Luo_Q/0/1/0/all/0/1">Qiong Luo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Wang_F/0/1/0/all/0/1">Feng Wang</a></p>
<p>In radio astronomy, visibility data, which are measurements of wave signals
from radio telescopes, are transformed into images for observation of distant
celestial objects. However, these resultant images usually contain both real
sources and artifacts, due to signal sparsity and other factors. One way to
obtain cleaner images is to reconstruct samples into dense forms before
imaging. Unfortunately, existing reconstruction methods often miss some
components of visibility in frequency domain, so blurred object edges and
persistent artifacts remain in the images. Furthermore, the computation
overhead is high on irregular visibility samples due to the data skew. To
address these problems, we propose PolarRec, a transformer-encoder-conditioned
reconstruction pipeline with visibility samples converted into the polar
coordinate representation. This representation matches the way in which radio
telescopes observe a celestial area as the Earth rotates. As a result,
visibility samples distribute in the polar system more uniformly than in the
Cartesian space. Therefore, we propose to use radial distance in the loss
function, to help reconstruct complete visibility effectively. Also, we group
visibility samples by their polar angles and propose a group-based encoding
scheme to improve the efficiency. Our experiments demonstrate that PolarRec
markedly improves imaging results by faithfully reconstructing all frequency
components in the visibility domain while significantly reducing the
computation cost in visibility data encoding. We believe this high-quality and
high-efficiency imaging of PolarRec will better facilitate astronomers to
conduct their research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15568">Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akansha_S/0/1/0/all/0/1">Singh Akansha</a></p>
<p>Graph Neural Networks (GNNs) revolutionize machine learning for
graph-structured data, effectively capturing complex relationships. They
disseminate information through interconnected nodes, but long-range
interactions face challenges known as "over-squashing". This survey delves into
the challenge of over-squashing in Graph Neural Networks (GNNs), where
long-range information dissemination is hindered, impacting tasks reliant on
intricate long-distance interactions. It comprehensively explores the causes,
consequences, and mitigation strategies for over-squashing. Various
methodologies are reviewed, including graph rewiring, novel normalization,
spectral analysis, and curvature-based strategies, with a focus on their
trade-offs and effectiveness. The survey also discusses the interplay between
over-squashing and other GNN limitations, such as over-smoothing, and provides
a taxonomy of models designed to address these issues in node and graph-level
tasks. Benchmark datasets for performance evaluation are also detailed, making
this survey a valuable resource for researchers and practitioners in the GNN
field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01291">Generative Social Choice. (arXiv:2309.01291v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fish_S/0/1/0/all/0/1">Sara Fish</a>, <a href="http://arxiv.org/find/cs/1/au:+Golz_P/0/1/0/all/0/1">Paul G&#xf6;lz</a>, <a href="http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1">David C. Parkes</a>, <a href="http://arxiv.org/find/cs/1/au:+Procaccia_A/0/1/0/all/0/1">Ariel D. Procaccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusak_G/0/1/0/all/0/1">Gili Rusak</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapira_I/0/1/0/all/0/1">Itai Shapira</a>, <a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1">Manuel W&#xfc;thrich</a></p>
<p>Traditionally, social choice theory has only been applicable to choices among
a few predetermined alternatives but not to more complex decisions such as
collectively selecting a textual statement. We introduce generative social
choice, a framework that combines the mathematical rigor of social choice
theory with the capability of large language models to generate text and
extrapolate preferences. This framework divides the design of AI-augmented
democratic processes into two components: first, proving that the process
satisfies rigorous representation guarantees when given access to oracle
queries; second, empirically validating that these queries can be approximately
implemented using a large language model. We apply this framework to the
problem of generating a slate of statements that is representative of opinions
expressed as free-form text; specifically, we develop a democratic process with
representation guarantees and use this process to represent the opinions of
participants in a survey about chatbot personalization. We find that 93 out of
100 participants feel "mostly" or "perfectly" represented by the slate of five
statements we extracted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Aounon Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1">Chirag Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1">Suraj Srinivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aaron Jiaxun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Himabindu Lakkaraju</a></p>
<p>Large language models (LLMs) released for public use incorporate guardrails
to ensure their output is safe, often referred to as "model alignment." An
aligned language model should decline a user's request to produce harmful
content. However, such safety measures are vulnerable to adversarial attacks,
which add maliciously designed token sequences to a harmful prompt to bypass
the model's safety guards. In this work, we introduce erase-and-check, the
first framework to defend against adversarial prompts with verifiable safety
guarantees. We defend against three attack modes: i) adversarial suffix, which
appends an adversarial sequence at the end of the prompt; ii) adversarial
insertion, where the adversarial sequence is inserted anywhere in the middle of
the prompt; and iii) adversarial infusion, where adversarial tokens are
inserted at arbitrary positions in the prompt, not necessarily as a contiguous
block. Our experimental results demonstrate that this procedure can obtain
strong certified safety guarantees on harmful prompts while maintaining good
empirical performance on safe prompts. For example, against adversarial
suffixes of length 20, it certifiably detects 92% of harmful prompts and labels
94% of safe prompts correctly using the open-source language model Llama 2 as
the safety filter. We further improve the filter's performance, in terms of
accuracy and speed, by replacing Llama 2 with a DistilBERT safety classifier
fine-tuned on safe and harmful prompts. Additionally, we propose two efficient
empirical defenses: i) RandEC, a randomized version of erase-and-check that
evaluates the safety filter on a small subset of the erased subsequences, and
ii) GradEC, a gradient-based version that optimizes the erased tokens to remove
the adversarial sequence. The code for our experiments is available at
https://github.com/aounon/certified-llm-safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09919">Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents. (arXiv:2309.09919v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1">Shreyas S. Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ankit Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1">Stefanie Tellex</a></p>
<p>Recent advancements in large language models (LLMs) have enabled a new
research domain, LLM agents, for solving robotics and planning tasks by
leveraging the world knowledge and general reasoning abilities of LLMs obtained
during pretraining. However, while considerable effort has been made to teach
the robot the "dos," the "don'ts" received relatively less attention. We argue
that, for any practical usage, it is as crucial to teach the robot the
"don'ts": conveying explicit instructions about prohibited actions, assessing
the robot's comprehension of these restrictions, and, most importantly,
ensuring compliance. Moreover, verifiable safe operation is essential for
deployments that satisfy worldwide standards such as ISO 61508, which defines
standards for safely deploying robots in industrial factory environments
worldwide. Aiming at deploying the LLM agents in a collaborative environment,
we propose a queryable safety constraint module based on linear temporal logic
(LTL) that simultaneously enables natural language (NL) to temporal constraints
encoding, safety violation reasoning and explaining, and unsafe action pruning.
To demonstrate the effectiveness of our system, we conducted experiments in
VirtualHome environment and on a real robot. The experimental results show that
our system strictly adheres to the safety constraints and scales well with
complex safety constraints, highlighting its potential for practical utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10399">Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carloni_G/0/1/0/all/0/1">Gianluca Carloni</a>, <a href="http://arxiv.org/find/cs/1/au:+Colantonio_S/0/1/0/all/0/1">Sara Colantonio</a></p>
<p>We present a novel technique to discover and exploit weak causal signals
directly from images via neural networks for classification purposes. This way,
we model how the presence of a feature in one part of the image affects the
appearance of another feature in a different part of the image. Our method
consists of a convolutional neural network backbone and a causality-factors
extractor module, which computes weights to enhance each feature map according
to its causal influence in the scene. We developed different architecture
variants and empirically evaluated all of our models on two public datasets of
prostate MRI images and breast histopathology slides for cancer diagnosis. To
confirm our quantitative results, we conduct ablation studies and investigate
the explainability of our models via class activation maps. Our findings show
that our lightweight block extracts meaningful information and improves the
overall classification, together with producing more robust predictions that
focus on relevant parts of the image. That is crucial in medical imaging, where
accurate and reliable classifications are essential for effective diagnosis and
treatment planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11087">Embed-Search-Align: DNA Sequence Alignment using Transformer Models. (arXiv:2309.11087v2 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Holur_P/0/1/0/all/0/1">Pavan Holur</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Enevoldsen_K/0/1/0/all/0/1">K. C. Enevoldsen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mboning_L/0/1/0/all/0/1">Lajoyce Mboning</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Georgiou_T/0/1/0/all/0/1">Thalia Georgiou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bouchard_L/0/1/0/all/0/1">Louis-S. Bouchard</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pellegrini_M/0/1/0/all/0/1">Matteo Pellegrini</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Roychowdhury_V/0/1/0/all/0/1">Vwani Roychowdhury</a></p>
<p>DNA sequence alignment involves assigning short DNA reads to the most
probable locations on an extensive reference genome. This process is crucial
for various genomic analyses, including variant calling, transcriptomics, and
epigenomics. Conventional methods, refined over decades, tackle this challenge
in two steps: genome indexing followed by efficient search to locate likely
positions for given reads. Building on the success of Large Language Models
(LLM) in encoding text into embeddings, where the distance metric captures
semantic similarity, recent efforts have explored whether the same Transformer
architecture can produce numerical representations for DNA sequences. Such
models have shown early promise in tasks involving classification of short DNA
sequences, such as the detection of coding vs non-coding regions, as well as
the identification of enhancer and promoter sequences. Performance at sequence
classification tasks does not, however, translate to sequence alignment, where
it is necessary to conduct a genome-wide search to successfully align every
read. We address this open problem by framing it as an Embed-Search-Align task.
In this framework, a novel encoder model DNA-ESA generates representations of
reads and fragments of the reference, which are projected into a shared vector
space where the read-fragment distance is used as surrogate for alignment. In
particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised
training of DNA sequence representations, facilitating rich sequence-level
embeddings, and (2) a DNA vector store to enable search across fragments on a
global scale. DNA-ESA is &gt;97% accurate when aligning 250-length reads onto a
human reference genome of 3 gigabases (single-haploid), far exceeds the
performance of 6 recent DNA-Transformer model baselines and shows task transfer
across chromosomes and species.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11526">Likelihood-based Sensor Calibration using Affine Transformation. (arXiv:2309.11526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1">R&#xfc;diger Machhamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1">Lejla Begic Fazlic</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Eray Guven</a>, <a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1">David Junk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1">Gunes Karabulut Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1">Stefan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1">Stephan Didas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1">Klaus-Uwe Gollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1">Ralph Bergmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1">Ingo J. Timm</a>, <a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1">Guido Dartmann</a></p>
<p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. The results demonstrate the
adaptability of this solution for various applications, including software
calibration of sensors, implementation of expert-based adaptation, and paving
the way for future advancements such as distributed learning methods. One idea
here is to use the knowledge of experts for estimating an affine transformation
between different systems. We evaluate our research with simulations and also
with real measured data of a multi-sensor board with 8 identical sensors. Both
data set and evaluation script are provided for download. The results show an
improvement for both the simulation and the experiments with real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13411">Towards Attributions of Input Variables in a Coalition. (arXiv:2309.13411v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xinhao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Huiqi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1">Bo Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Quanshi Zhang</a></p>
<p>This paper aims to develop a new attribution method to explain the conflict
between individual variables' attributions and their coalition's attribution
from a fully new perspective. First, we find that the Shapley value can be
reformulated as the allocation of Harsanyi interactions encoded by the AI
model. Second, based the re-alloction of interactions, we extend the Shapley
value to the attribution of coalitions. Third we ective. We derive the
fundamental mechanism behind the conflict. This conflict come from the
interaction containing partial variables in their coalition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13607">MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance Field. (arXiv:2309.13607v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zijiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zhongwei Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Dongmei Fu</a></p>
<p>3D style transfer aims to generate stylized views of 3D scenes with specified
styles, which requires high-quality generating and keeping multi-view
consistency. Existing methods still suffer the challenges of high-quality
stylization with texture details and stylization with multimodal guidance. In
this paper, we reveal that the common training method of stylization with NeRF,
which generates stylized multi-view supervision by 2D style transfer models,
causes the same object in supervision to show various states (color tone,
details, etc.) in different views, leading NeRF to tend to smooth the texture
details, further resulting in low-quality rendering for 3D multi-style
transfer. To tackle these problems, we propose a novel Multimodal-guided 3D
Multi-style transfer of NeRF, termed MM-NeRF. First, MM-NeRF projects
multimodal guidance into a unified space to keep the multimodal styles
consistency and extracts multimodal features to guide the 3D stylization.
Second, a novel multi-head learning scheme is proposed to relieve the
difficulty of learning multi-style transfer, and a multi-view style consistent
loss is proposed to track the inconsistency of multi-view supervision data.
Finally, a novel incremental learning mechanism to generalize MM-NeRF to any
new style with small costs. Extensive experiments on several real-world
datasets show that MM-NeRF achieves high-quality 3D multi-style stylization
with multimodal guidance, and keeps multi-view consistency and style
consistency between multimodal guidance. Codes will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13620">PRIS: Practical robust invertible network for image steganography. (arXiv:2309.13620v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yitian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuhua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaodong Ma</a></p>
<p>Image steganography is a technique of hiding secret information inside
another image, so that the secret is not visible to human eyes and can be
recovered when needed. Most of the existing image steganography methods have
low hiding robustness when the container images affected by distortion. Such as
Gaussian noise and lossy compression. This paper proposed PRIS to improve the
robustness of image steganography, it based on invertible neural networks, and
put two enhance modules before and after the extraction process with a 3-step
training strategy. Moreover, rounding error is considered which is always
ignored by existing methods, but actually it is unavoidable in practical. A
gradient approximation function (GAF) is also proposed to overcome the
undifferentiable issue of rounding distortion. Experimental results show that
our PRIS outperforms the state-of-the-art robust image steganography method in
both robustness and practicability. Codes are available at
https://github.com/yanghangAI/PRIS, demonstration of our model in practical at
<a href="http://yanghang.site/hide/.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14053">Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1">Khoi Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoa Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1">Long Tran-Thanh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1">Quoc-Viet Pham</a></p>
<p>LARS and LAMB have emerged as prominent techniques in Large Batch Learning
(LBL) to ensure training stability in AI. Convergence stability is a challenge
in LBL, where the AI agent usually gets trapped in the sharp minimizer. To
address this challenge, warm-up is an efficient technique, but it lacks a
strong theoretical foundation. Specifically, the warm-up process often reduces
gradients in the early phase, inadvertently preventing the agent from escaping
the sharp minimizer early on. In light of this situation, we conduct empirical
experiments to analyze the behaviors of LARS and LAMB with and without a
warm-up strategy. Our analyses give a comprehensive insight into the behaviors
of LARS, LAMB, and the necessity of a warm-up technique in LBL, including an
explanation of their failure in many cases. Building upon these insights, we
propose a novel algorithm called Time Varying LARS (TVLARS), which facilitates
robust training in the initial phase without the need for warm-up. A
configurable sigmoid-like function is employed in TVLARS to replace the warm-up
process to enhance training stability. Moreover, TVLARS stimulates gradient
exploration in the early phase, thus allowing it to surpass the sharp minimizes
early on and gradually transition to LARS and achieving robustness of LARS in
the latter phases. Extensive experimental evaluations reveal that TVLARS
consistently outperforms LARS and LAMB in most cases, with improvements of up
to 2% in classification scenarios. Notably, in every case of self-supervised
learning, TVLARS dominates LARS and LAMB with performance improvements of up to
10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16064">Masked Autoencoders are Scalable Learners of Cellular Morphology. (arXiv:2309.16064v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kraus_O/0/1/0/all/0/1">Oren Kraus</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenyon_Dean_K/0/1/0/all/0/1">Kian Kenyon-Dean</a>, <a href="http://arxiv.org/find/cs/1/au:+Saberian_S/0/1/0/all/0/1">Saber Saberian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fallah_M/0/1/0/all/0/1">Maryam Fallah</a>, <a href="http://arxiv.org/find/cs/1/au:+McLean_P/0/1/0/all/0/1">Peter McLean</a>, <a href="http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1">Jess Leung</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1">Vasudev Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Ayla Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_J/0/1/0/all/0/1">Jia Balakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Celik_S/0/1/0/all/0/1">Safiye Celik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sypetkowski_M/0/1/0/all/0/1">Maciej Sypetkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chi Vicky Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Morse_K/0/1/0/all/0/1">Kristen Morse</a>, <a href="http://arxiv.org/find/cs/1/au:+Makes_M/0/1/0/all/0/1">Maureen Makes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mabey_B/0/1/0/all/0/1">Ben Mabey</a>, <a href="http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1">Berton Earnshaw</a></p>
<p>Inferring biological relationships from cellular phenotypes in high-content
microscopy screens provides significant opportunity and challenge in biological
research. Prior results have shown that deep vision models can capture
biological signal better than hand-crafted features. This work explores how
self-supervised deep learning approaches scale when training larger models on
larger microscopy datasets. Our results show that both CNN- and ViT-based
masked autoencoders significantly outperform weakly supervised baselines. At
the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops
sampled from 93-million microscopy images achieves relative improvements as
high as 28% over our best weakly supervised baseline at inferring known
biological relationships curated from public databases. Relevant code and
select models released with this work can be found at:
https://github.com/recursionpharma/maes_microscopy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01837">Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1">Abdul Karim Gizzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1">Mustafa Shukor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1">Ali J. Ghandour</a></p>
<p>Current AI-based methods do not provide comprehensible physical
interpretations of the utilized data, extracted features, and
predictions/inference operations. As a result, deep learning models trained
using high-resolution satellite imagery lack transparency and explainability
and can be merely seen as a black box, which limits their wide-level adoption.
Experts need help understanding the complex behavior of AI models and the
underlying decision-making process. The explainable artificial intelligence
(XAI) field is an emerging field providing means for robust, practical, and
trustworthy deployment of AI models. Several XAI techniques have been proposed
for image classification tasks, whereas the interpretation of image
segmentation remains largely unexplored. This paper offers to bridge this gap
by adapting the recent XAI classification algorithms and making them usable for
muti-class image segmentation, where we mainly focus on buildings' segmentation
from high-resolution satellite images. To benchmark and compare the performance
of the proposed approaches, we introduce a new XAI evaluation methodology and
metric based on "Entropy" to measure the model uncertainty. Conventional XAI
evaluation methods rely mainly on feeding area-of-interest regions from the
image back to the pre-trained (utility) model and then calculating the average
change in the probability of the target class. Those evaluation metrics lack
the needed robustness, and we show that using Entropy to monitor the model
uncertainty in segmenting the pixels within the target class is more suitable.
We hope this work will pave the way for additional XAI research for image
segmentation and applications in the remote sensing discipline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02071">Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shuhuai Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haozhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zefan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuchi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1">Baobao Chang</a></p>
<p>In this study, we explore the potential of Multimodal Large Language Models
(MLLMs) in improving embodied decision-making processes for agents. While Large
Language Models (LLMs) have been widely used due to their advanced reasoning
skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual
understanding and reasoning capabilities. We investigate whether
state-of-the-art MLLMs can handle embodied decision-making in an end-to-end
manner and whether collaborations between LLMs and MLLMs can enhance
decision-making. To address these questions, we introduce a new benchmark
called PCA-EVAL, which evaluates embodied decision-making from the perspectives
of Perception, Cognition, and Action. Additionally, we propose HOLMES, a
multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs
to gather multimodal information for informed decision-making. We compare
end-to-end embodied decision-making and HOLMES on our benchmark and find that
the GPT4-Vision model demonstrates strong end-to-end embodied decision-making
abilities, outperforming GPT4-HOLMES in terms of average decision accuracy
(+3%). However, this performance is exclusive to the latest GPT4-Vision model,
surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate
that powerful MLLMs like GPT4-Vision hold promise for decision-making in
embodied agents, offering new avenues for MLLM research. Code and data are open
at https://github.com/pkunlp-icler/PCA-EVAL/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuelong Li</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03211">On the Performance of Multimodal Language Models. (arXiv:2310.03211v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1">Utsav Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1">Erhan Bas</a></p>
<p>Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04438">A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting). (arXiv:2310.04438v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muktadir_G/0/1/0/all/0/1">Golam Md Muktadir</a></p>
<p>This paper presents a comprehensive exploration of the evolution of prompt
engineering and generation in the field of natural language processing (NLP).
Starting from the early language models and information retrieval systems, we
trace the key developments that have shaped prompt engineering over the years.
The introduction of attention mechanisms in 2015 revolutionized language
understanding, leading to advancements in controllability and
context-awareness. Subsequent breakthroughs in reinforcement learning
techniques further enhanced prompt engineering, addressing issues like exposure
bias and biases in generated text. We examine the significant contributions in
2018 and 2019, focusing on fine-tuning strategies, control codes, and
template-based generation. The paper also discusses the growing importance of
fairness, human-AI collaboration, and low-resource adaptation. In 2020 and
2021, contextual prompting and transfer learning gained prominence, while 2022
and 2023 witnessed the emergence of advanced techniques like unsupervised
pre-training and novel reward shaping. Throughout the paper, we reference
specific research studies that exemplify the impact of various developments on
prompt engineering. The journey of prompt engineering continues, with ethical
considerations being paramount for the responsible and inclusive future of AI
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04486">T-Rep: Representation Learning for Time Series using Time-Embeddings. (arXiv:2310.04486v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fraikin_A/0/1/0/all/0/1">Archibald Fraikin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bennetot_A/0/1/0/all/0/1">Adrien Bennetot</a>, <a href="http://arxiv.org/find/cs/1/au:+Allassonniere_S/0/1/0/all/0/1">St&#xe9;phanie Allassonni&#xe8;re</a></p>
<p>Multivariate time series present challenges to standard machine learning
techniques, as they are often unlabeled, high dimensional, noisy, and contain
missing data. To address this, we propose T-Rep, a self-supervised method to
learn time series representations at a timestep granularity. T-Rep learns
vector embeddings of time alongside its feature extractor, to extract temporal
features such as trend, periodicity, or distribution shifts from the signal.
These time-embeddings are leveraged in pretext tasks, to incorporate smooth and
fine-grained temporal dependencies in the representations, as well as reinforce
robustness to missing data. We evaluate T-Rep on downstream classification,
forecasting, and anomaly detection tasks. It is compared to existing
self-supervised algorithms for time series, which it outperforms in all three
tasks. We test T-Rep in missing data regimes, where it proves more resilient
than its counterparts. Finally, we provide latent space visualisation
experiments, highlighting the interpretability of the learned representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05898">Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a></p>
<p>Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion's efficacy.
</p>
<p>This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08559">Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement. (arXiv:2310.08559v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Linlu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liwei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Ximing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sclar_M/0/1/0/all/0/1">Melanie Sclar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1">Valentina Pyatkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1">Chandra Bhagavatula</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1">Nouha Dziri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a></p>
<p>The ability to derive underlying principles from a handful of observations
and then generalize to novel situations -- known as inductive reasoning -- is
central to human intelligence. Prior work suggests that language models (LMs)
often fall short on inductive reasoning, despite achieving impressive success
on research benchmarks. In this work, we conduct a systematic study of the
inductive reasoning capabilities of LMs through iterative hypothesis
refinement, a technique that more closely mirrors the human inductive process
than standard input-output prompting. Iterative hypothesis refinement employs a
three-step process: proposing, selecting, and refining hypotheses in the form
of textual rules. By examining the intermediate rules, we observe that LMs are
phenomenal hypothesis proposers (i.e., generating candidate rules), and when
coupled with a (task-specific) symbolic interpreter that is able to
systematically filter the proposed set of rules, this hybrid approach achieves
strong results across inductive reasoning benchmarks that require inducing
causal relations, language-like instructions, and symbolic concepts. However,
they also behave as puzzling inductive reasoners, showing notable performance
gaps between rule induction (i.e., identifying plausible rules) and rule
application (i.e., applying proposed rules to instances), suggesting that LMs
are proposing hypotheses without being able to actually apply the rules.
Through empirical and human analyses, we further reveal several discrepancies
between the inductive reasoning processes of LMs and humans, shedding light on
both the potentials and limitations of using LMs in inductive reasoning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1">Chen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pengcheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Karampatziakis_N/0/1/0/all/0/1">Nikos Karampatziakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weizhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a></p>
<p>Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves generalization in
downstream tasks. We evaluate our method on natural language understanding,
question answering, summarization, and natural language generation tasks.
Experiments show that our method is highly effective and outperforms existing
quantization methods, especially in the challenging 2-bit and 2/4-bit mixed
precision regimes. The code is available on https://github.com/yxli2123/LoftQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08992">CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hung Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hailin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1">Amrita Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1">Akash Gokul</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1">Doyen Sahoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a></p>
<p>Large Language Models (LLMs) have already become quite proficient at solving
simpler programming tasks like those in HumanEval or MBPP benchmarks. However,
solving more complex and competitive programming tasks is still quite
challenging for these models - possibly due to their tendency to generate
solutions as monolithic code blocks instead of decomposing them into logical
sub-tasks and sub-modules. On the other hand, experienced programmers
instinctively write modularized code with abstraction for solving complex
tasks, often reusing previously developed modules. To address this gap, we
propose CodeChain, a novel framework for inference that elicits modularized
code generation through a chain of self-revisions, each being guided by some
representative sub-modules generated in previous iterations. Concretely,
CodeChain first instructs the LLM to generate modularized codes through
chain-of-thought prompting. Then it applies a chain of self-revisions by
iterating the two steps: 1) extracting and clustering the generated sub-modules
and selecting the cluster representatives as the more generic and re-usable
implementations, and 2) augmenting the original chain-of-thought prompt with
these selected module-implementations and instructing the LLM to re-generate
new modularized solutions. We find that by naturally encouraging the LLM to
reuse the previously developed and verified sub-modules, CodeChain can
significantly boost both modularity as well as correctness of the generated
solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on
CodeContests. It is shown to be effective on both OpenAI LLMs as well as
open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation
studies with different methods of prompting, number of clusters, model sizes,
program qualities, etc., to provide useful insights that underpin CodeChain's
success.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09843">CoCoFormer: A controllable feature-rich polyphonic music generation method. (arXiv:2310.09843v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiuyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1">Tengfei Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingping Wang</a></p>
<p>This paper explores the modeling method of polyphonic music sequence. Due to
the great potential of Transformer models in music generation, controllable
music generation is receiving more attention. In the task of polyphonic music,
current controllable generation research focuses on controlling the generation
of chords, but lacks precise adjustment for the controllable generation of
choral music textures. This paper proposed Condition Choir Transformer
(CoCoFormer) which controls the output of the model by controlling the chord
and rhythm inputs at a fine-grained level. In this paper, the self-supervised
method improves the loss function and performs joint training through
conditional control input and unconditional input training. In order to
alleviate the lack of diversity on generated samples caused by the teacher
forcing training, this paper added an adversarial training method. CoCoFormer
enhances model performance with explicit and implicit inputs to chords and
rhythms. In this paper, the experiments proves that CoCoFormer has reached the
current better level than current models. On the premise of specifying the
polyphonic music texture, the same melody can also be generated in a variety of
ways.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11676">PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junjun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yizhen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a></p>
<p>Node-level graph anomaly detection (GAD) plays a critical role in identifying
anomalous nodes from graph-structured data in various domains such as medicine,
social networks, and e-commerce. However, challenges have arisen due to the
diversity of anomalies and the dearth of labeled data. Existing methodologies -
reconstruction-based and contrastive learning - while effective, often suffer
from efficiency issues, stemming from their complex objectives and elaborate
modules. To improve the efficiency of GAD, we introduce a simple method termed
PREprocessing and Matching (PREM for short). Our approach streamlines GAD,
reducing time and memory consumption while maintaining powerful anomaly
detection capabilities. Comprising two modules - a pre-processing module and an
ego-neighbor matching module - PREM eliminates the necessity for
message-passing propagation during training, and employs a simple contrastive
loss, leading to considerable reductions in training time and memory usage.
Moreover, through rigorous evaluations of five real-world datasets, our method
demonstrated robustness and effectiveness. Notably, when validated on the ACM
dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training
speed, and sharply reduce memory usage compared to the most efficient baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14714">BatteryML:An Open-source platform for Machine Learning on Battery Degradation. (arXiv:2310.14714v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17025">netFound: Foundation Model for Network Security. (arXiv:2310.17025v2 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guthula_S/0/1/0/all/0/1">Satyandra Guthula</a>, <a href="http://arxiv.org/find/cs/1/au:+Battula_N/0/1/0/all/0/1">Navya Battula</a>, <a href="http://arxiv.org/find/cs/1/au:+Beltiukov_R/0/1/0/all/0/1">Roman Beltiukov</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wenbo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Arpit Gupta</a></p>
<p>In ML for network security, traditional workflows rely on high-quality
labeled data and manual feature engineering, but limited datasets and human
expertise hinder feature selection, leading to models struggling to capture
crucial relationships and generalize effectively. Inspired by recent
advancements in ML application domains like GPT-4 and Vision Transformers, we
have developed netFound, a foundational model for network security. This model
undergoes pre-training using self-supervised algorithms applied to readily
available unlabeled network packet traces. netFound's design incorporates
hierarchical and multi-modal attributes of network traffic, effectively
capturing hidden networking contexts, including application logic,
communication protocols, and network conditions.
</p>
<p>With this pre-trained foundation in place, we can fine-tune netFound for a
wide array of downstream tasks, even when dealing with low-quality, limited,
and noisy labeled data. Our experiments demonstrate netFound's superiority over
existing state-of-the-art ML-based solutions across three distinct network
downstream tasks: traffic classification, network intrusion detection, and APT
detection. Furthermore, we emphasize netFound's robustness against noisy and
missing labels, as well as its ability to generalize across temporal variations
and diverse network environments. Finally, through a series of ablation
studies, we provide comprehensive insights into how our design choices enable
netFound to more effectively capture hidden networking contexts, further
solidifying its performance and utility in network security applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17639">In-Context Learning Dynamics with Random Binary Sequences. (arXiv:2310.17639v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bigelow_E/0/1/0/all/0/1">Eric J. Bigelow</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1">Ekdeep Singh Lubana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1">Robert P. Dick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1">Tomer D. Ullman</a></p>
<p>Large language models (LLMs) trained on huge corpora of text datasets
demonstrate intriguing capabilities, achieving state-of-the-art performance on
tasks they were not explicitly trained for. The precise nature of LLM
capabilities is often mysterious, and different prompts can elicit different
capabilities through in-context learning. We propose a framework that enables
us to analyze in-context learning dynamics to understand latent concepts
underlying LLMs' behavioral patterns. This provides a more nuanced
understanding than success-or-failure evaluation benchmarks, but does not
require observing internal activations as a mechanistic interpretation of
circuits would. Inspired by the cognitive science of human randomness
perception, we use random binary sequences as context and study dynamics of
in-context learning by manipulating properties of context data, such as
sequence length. In the latest GPT-3.5+ models, we find emergent abilities to
generate seemingly random numbers and learn basic formal languages, with
striking in-context learning dynamics where model outputs transition sharply
from seemingly random behaviors to deterministic repetition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18021">FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning. (arXiv:2310.18021v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaokai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_N/0/1/0/all/0/1">Na Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yiming He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">Jia Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qike Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaoxiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yanjun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chenyang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhe Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_D/0/1/0/all/0/1">Dengfeng Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fangzhen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiwen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1">Cheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhenbing Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shaorong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiangfeng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_T/0/1/0/all/0/1">Tuo Leng</a></p>
<p>This is the first paper in a series of work we have accomplished over the
past three years. In this paper, we have constructed a complete and compatible
formal plane geometry system. This will serve as a crucial bridge between
IMO-level plane geometry challenges and readable AI automated reasoning. Within
this formal framework, we have been able to seamlessly integrate modern AI
models with our formal system. AI is now capable of providing deductive
reasoning solutions to IMO-level plane geometry problems, just like handling
other natural languages, and these proofs are readable, traceable, and
verifiable. We propose the geometry formalization theory (GFT) to guide the
development of the geometry formal system. Based on the GFT, we have
established the FormalGeo, which consists of 88 geometric predicates and 196
theorems. It can represent, validate, and solve IMO-level geometry problems. we
also have crafted the FGPS (formal geometry problem solver) in Python. It
serves as both an interactive assistant for verifying problem-solving processes
and an automated problem solver. We've annotated the formalgeo7k and
formalgeo-imo datasets. The former contains 6,891 (expand to 133,818 through
data augmentation) geometry problems, while the latter includes 18 (expand to
2,627 and continuously increasing) IMO-level challenging geometry problems. All
annotated problems include detailed formal language descriptions and solutions.
Implementation of the formal system and experiments validate the correctness
and utility of the GFT. The backward depth-first search method only yields a
2.42% problem-solving failure rate, and we can incorporate deep learning
techniques to achieve lower one. The source code of FGPS and datasets are
available at https://github.com/BitSecret/FGPS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20246">Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zinan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1">Ning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1">Ming Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20323">SemanticBoost: Elevating Motion Generation with Augmented Textual Cues. (arXiv:2310.20323v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaoli Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xiaohang Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1">Chao Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Current techniques face difficulties in generating motions from intricate
semantic descriptions, primarily due to insufficient semantic annotations in
datasets and weak contextual understanding. To address these issues, we present
SemanticBoost, a novel framework that tackles both challenges simultaneously.
Our framework comprises a Semantic Enhancement module and a Context-Attuned
Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary
semantics from motion data, enriching the dataset's textual description and
ensuring precise alignment between text and motion data without depending on
large language models. On the other hand, the CAMD approach provides an
all-encompassing solution for generating high-quality, semantically consistent
motion sequences by effectively capturing context information and aligning the
generated motion with the given textual descriptions. Distinct from existing
methods, our approach can synthesize accurate orientational movements, combined
motions based on specific body part descriptions, and motions generated from
complex, extended sentences. Our experimental results demonstrate that
SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based
techniques, achieving cutting-edge performance on the Humanml3D dataset while
maintaining realistic and smooth motion generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05332">On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving. (arXiv:2311.05332v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1">Licheng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuemeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Daocheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Linran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_D/0/1/0/all/0/1">Dengke Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shaoyan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yeqi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1">Min Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shuanglu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Botian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>The pursuit of autonomous driving technology hinges on the sophisticated
integration of perception, decision-making, and control systems. Traditional
approaches, both data-driven and rule-based, have been hindered by their
inability to grasp the nuance of complex driving environments and the
intentions of other road users. This has been a significant bottleneck,
particularly in the development of common sense reasoning and nuanced scene
understanding necessary for safe and reliable autonomous driving. The advent of
Visual Language Models (VLM) represents a novel frontier in realizing fully
autonomous vehicle driving. This report provides an exhaustive evaluation of
the latest state-of-the-art VLM, GPT-4V(ision), and its application in
autonomous driving scenarios. We explore the model's abilities to understand
and reason about driving scenes, make decisions, and ultimately act in the
capacity of a driver. Our comprehensive tests span from basic scene recognition
to complex causal reasoning and real-time decision-making under varying
conditions. Our findings reveal that GPT-4V demonstrates superior performance
in scene understanding and causal reasoning compared to existing autonomous
systems. It showcases the potential to handle out-of-distribution scenarios,
recognize intentions, and make informed decisions in real driving contexts.
However, challenges remain, particularly in direction discernment, traffic
light recognition, vision grounding, and spatial reasoning tasks. These
limitations underscore the need for further research and development. Project
is now available on GitHub for interested parties to access and utilize:
\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06864">Understanding Practices around Computational News Discovery Tools in the Domain of Science Journalism. (arXiv:2311.06864v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nishal_S/0/1/0/all/0/1">Sachita Nishal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinchai_J/0/1/0/all/0/1">Jasmine Sinchai</a>, <a href="http://arxiv.org/find/cs/1/au:+Diakopoulos_N/0/1/0/all/0/1">Nicholas Diakopoulos</a></p>
<p>Science and technology journalists today face challenges in finding
newsworthy leads due to increased workloads, reduced resources, and expanding
scientific publishing ecosystems. Given this context, we explore computational
methods to aid these journalists' news discovery in terms of time-efficiency
and agency. In particular, we prototyped three computational information
subsidies into an interactive tool that we used as a probe to better understand
how such a tool may offer utility or more broadly shape the practices of
professional science journalists. Our findings highlight central considerations
around science journalists' agency, context, and responsibilities that such
tools can influence and could account for in design. Based on this, we suggest
design opportunities for greater and longer-term user agency; incorporating
contextual, personal and collaborative notions of newsworthiness; and
leveraging flexible interfaces and generative models. Overall, our findings
contribute a richer view of the sociotechnical system around computational news
discovery tools, and suggest ways to improve such tools to better support the
practices of science journalists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08379">Scheming AIs: Will AIs fake alignment during training in order to get power?. (arXiv:2311.08379v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carlsmith_J/0/1/0/all/0/1">Joe Carlsmith</a></p>
<p>This report examines whether advanced AIs that perform well in training will
be doing so in order to gain power later -- a behavior I call "scheming" (also
sometimes called "deceptive alignment"). I conclude that scheming is a
disturbingly plausible outcome of using baseline machine learning methods to
train goal-directed AIs sophisticated enough to scheme (my subjective
probability on such an outcome, given these conditions, is roughly 25%). In
particular: if performing well in training is a good strategy for gaining power
(as I think it might well be), then a very wide variety of goals would motivate
scheming -- and hence, good training performance. This makes it plausible that
training might either land on such a goal naturally and then reinforce it, or
actively push a model's motivations towards such a goal as an easy way of
improving performance. What's more, because schemers pretend to be aligned on
tests designed to reveal their motivations, it may be quite difficult to tell
whether this has occurred. However, I also think there are reasons for comfort.
In particular: scheming may not actually be such a good strategy for gaining
power; various selection pressures in training might work against schemer-like
goals (for example, relative to non-schemers, schemers need to engage in extra
instrumental reasoning, which might harm their training performance); and we
may be able to increase such pressures intentionally. The report discusses
these and a wide variety of other considerations in detail, and it suggests an
array of empirical research directions for probing the topic further.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09312">H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing. (arXiv:2311.09312v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Visani_G/0/1/0/all/0/1">Gian Marco Visani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Galvin_W/0/1/0/all/0/1">William Galvin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pun_M/0/1/0/all/0/1">Michael Neal Pun</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nourmohammad_A/0/1/0/all/0/1">Armita Nourmohammad</a></p>
<p>Accurately modeling protein 3D structure is essential for the design of
functional proteins. An important sub-task of structure modeling is protein
side-chain packing: predicting the conformation of side-chains (rotamers) given
the protein's backbone structure and amino-acid sequence. Conventional
approaches for this task rely on expensive sampling procedures over
hand-crafted energy functions and rotamer libraries. Recently, several deep
learning methods have been developed to tackle the problem in a data-driven
way, albeit with vastly different formulations (from image-to-image translation
to directly predicting atomic coordinates). Here, we frame the problem as a
joint regression over the side-chains' true degrees of freedom: the dihedral
$\chi$ angles. We carefully study possible objective functions for this task,
while accounting for the underlying symmetries of the task. We propose
Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain
packing built on top of two light-weight rotationally equivariant neural
networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is
computationally efficient and shows favorable performance against conventional
physics-based algorithms and is competitive against alternative deep learning
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09790">Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting. (arXiv:2311.09790v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilbert_R/0/1/0/all/0/1">Romain Ilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thai V. Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zonghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. Our
optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10776">Towards an Automatic AI Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kexin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kunyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yuyang Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiahui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiamin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lanqing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jiezhong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1">Qun Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guangyong Chen</a></p>
<p>Artificial intelligence (AI) for reaction condition optimization has become
an important topic in the pharmaceutical industry, given that a data-driven AI
model can assist drug discovery and accelerate reaction design. However,
existing AI models lack the chemical insights and real-time knowledge
acquisition abilities of experienced human chemists. This paper proposes a
Large Language Model (LLM) empowered AI agent to bridge this gap. We put forth
a novel three-phase paradigm and applied advanced intelligence-enhancement
methods like in-context learning and multi-LLM debate so that the AI agent can
borrow human insight and update its knowledge by searching the latest chemical
literature. Additionally, we introduce a novel Coarse-label Contrastive
Learning (CCL) based chemical fingerprint that greatly enhances the agent's
performance in optimizing the reaction condition. With the above efforts, the
proposed AI agent can autonomously generate the optimal reaction condition
recommendation without any human interaction. Further, the agent is highly
professional in terms of chemical reactions. It demonstrates close-to-human
performance and strong generalization capability in both dry-lab and wet-lab
experiments. As the first attempt in the chemical AI agent, this work goes a
step further in the field of "AI for chemistry" and opens up new possibilities
for computer-aided synthesis planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12089">Explaining Deep Learning Models for Age-related Gait Classification based on time series acceleration. (arXiv:2311.12089v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoping Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Otten_B/0/1/0/all/0/1">Bert Otten</a>, <a href="http://arxiv.org/find/cs/1/au:+Reneman_M/0/1/0/all/0/1">Michiel F Reneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamoth_C/0/1/0/all/0/1">Claudine JC Lamoth</a></p>
<p>Gait analysis holds significant importance in monitoring daily health,
particularly among older adults. Advancements in sensor technology enable the
capture of movement in real-life environments and generate big data. Machine
learning, notably deep learning (DL), shows promise to use these big data in
gait analysis. However, the inherent black-box nature of these models poses
challenges for their clinical application. This study aims to enhance
transparency in DL-based gait classification for aged-related gait patterns
using Explainable Artificial Intelligence, such as SHAP.
</p>
<p>A total of 244 subjects, comprising 129 adults and 115 older adults (age&gt;65),
were included. They performed a 3-minute walking task while accelerometers were
affixed to the lumbar segment L3. DL models, convolutional neural network (CNN)
and gated recurrent unit (GRU), were trained using 1-stride and 8-stride
accelerations, respectively, to classify adult and older adult groups. SHAP was
employed to explain the models' predictions.
</p>
<p>CNN achieved a satisfactory performance with an accuracy of 81.4% and an AUC
of 0.89, and GRU demonstrated promising results with an accuracy of 84.5% and
an AUC of 0.94. SHAP analysis revealed that both CNN and GRU assigned higher
SHAP values to the data from vertical and walking directions, particularly
emphasizing data around heel contact, spanning from the terminal swing to
loading response phases. Furthermore, SHAP values indicated that GRU did not
treat every stride equally.
</p>
<p>CNN accurately distinguished between adults and older adults based on the
characteristics of a single stride's data. GRU achieved accurate classification
by considering the relationships and subtle differences between strides. In
both models, data around heel contact emerged as most critical, suggesting
differences in acceleration and deceleration patterns during walking between
different age groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12824">Comparative Analysis of Shear Strength Prediction Models for Reinforced Concrete Slab-Column Connections. (arXiv:2311.12824v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wahab_S/0/1/0/all/0/1">Sarmed Wahab</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoudabadi_N/0/1/0/all/0/1">Nasim Shakouri Mahmoudabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Waqas_S/0/1/0/all/0/1">Sarmad Waqas</a>, <a href="http://arxiv.org/find/cs/1/au:+Herl_N/0/1/0/all/0/1">Nouman Herl</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1">Muhammad Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_K/0/1/0/all/0/1">Khurshid Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1">Afaq Ahmad</a></p>
<p>This research aims at comparative analysis of shear strength prediction at
slab-column connection, unifying machine learning, design codes and Finite
Element Analysis. Current design codes (CDCs) of ACI 318-19 (ACI), Eurocode 2
(EC2), Compressive Force Path (CFP) method, Feed Forward Neural Network (FNN)
based Artificial Neural Network (ANN), PSO-based FNN (PSOFNN), and BAT
algorithm-based BATFNN are used. The study is complemented with FEA of slab for
validating the experimental results and machine learning predictions.In the
case of hybrid models of PSOFNN and BATFNN, mean square error is used as an
objective function to obtain the optimized values of the weights, that are used
by Feed Forward Neural Network to perform predictions on the slab data. Seven
different models of PSOFNN, BATFNN, and FNN are trained on this data and the
results exhibited that PSOFNN is the best model overall. PSOFNN has the best
results for SCS=1 with highest value of R as 99.37% and lowest of MSE, and MAE
values of 0.0275%, and 1.214% respectively which are better than the best FNN
model for SCS=4 having the values of R, MSE, and MAE as 97.464%, 0.0492%, and
1.43%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13148">Building the Future of Responsible AI: A Reference Architecture for Designing Large Language Model based Agents. (arXiv:2311.13148v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrer_S/0/1/0/all/0/1">Stefan Harrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1">Jon Whittle</a></p>
<p>Large language models (LLMs) have been widely recognised as transformative
artificial generative intelligence (AGI) technologies due to their capabilities
to understand and generate content, including plans with reasoning
capabilities. Foundation model based agents derive their autonomy from the
capabilities of foundation models, which enable them to autonomously break down
a given goal into a set of manageable tasks and orchestrate task execution to
meet the goal. Despite the huge efforts put into building foundation model
based autonomous agents, the architecture design of the agents has not yet been
systematically explored. Also, while there are significant benefits of using
autonomous agents for planning and execution, there are serious considerations
regarding responsible AI related software quality attributes, such as security
and accountability. Therefore, this paper presents a pattern-oriented reference
architecture that serves as architecture design guidance and enables
responsible-AI-by-design when designing foundation model based autonomous
agents. We evaluate the completeness and utility of the proposed reference
architecture by mapping it to the architecture of two real-world agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14552">Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models. (arXiv:2311.14552v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yufei Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yousong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhiyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Ming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiao Wang</a></p>
<p>Replicating the innate human ability to detect all objects based on free-form
texts at any granularity remains a formidable challenge for Vision-Language
models. Current Large Vision Language Models (LVLMs) are predominantly
constrained to grounding a single, pre-existing object, relying solely on data
from Referring Expression Comprehension tasks. The limitation leads to a
compromise in model design, necessitating the introduction of visual expert
models or the integration of customized head structures. Beyond these
constraints, our research delves into the untapped potential of LVLMs and
uncover their inherent capability for basic object perception, allowing them to
accurately identify and locate objects of interest. Building on this insight,
we introduce a novel language-prompted localization dataset designed to fully
unleash the capabilities of LVLMs in integrating fine-grained object perception
with precise location awareness. More importantly, we present
$\textbf{Griffon}$, a purely LVLM-based baseline, which does not require the
introduction of any special tokens, expert models, or additional detection
modules. It simply maintains a consistent structure with popular LVLMs by
unifying data formats across various localization-related scenarios and is
trained end-to-end through a well-designed pipeline. Comprehensive experiments
demonstrate that $\textbf{Griffon}$ not only achieves state-of-the-art
performance on the fine-grained RefCOCO series but also approaches the
capabilities of the expert model Faster RCNN on the detection benchmark MSCOCO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15243">ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection. (arXiv:2311.15243v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yichen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zongbo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1">Bing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaoheng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qinghua Hu</a></p>
<p>Out-of-distribution (OOD) detection methods often exploit auxiliary outliers
to train model identifying OOD samples, especially discovering challenging
outliers from auxiliary outliers dataset to improve OOD detection. However,
they may still face limitations in effectively distinguishing between the most
challenging OOD samples that are much like in-distribution (ID) data, i.e.,
ID-like samples. To this end, we propose a novel OOD detection framework that
discovers ID-like outliers using CLIP from the vicinity space of the ID
samples, thus helping to identify these most challenging OOD samples. Then a
prompt learning framework is proposed that utilizes the identified ID-like
outliers to further leverage the capabilities of CLIP for OOD detection.
Benefiting from the powerful CLIP, we only need a small number of ID samples to
learn the prompts of the model without exposing other auxiliary outlier
datasets. By focusing on the most challenging ID-like OOD samples and elegantly
exploiting the capabilities of CLIP, our method achieves superior few-shot
learning performance on various real-world image datasets (e.g., in 4-shot OOD
detection on the ImageNet-1k dataset, our method reduces the average FPR95 by
12.16% and improves the average AUROC by 2.76%, compared to state-of-the-art
methods).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15951">Replay across Experiments: A Natural Extension of Off-Policy RL. (arXiv:2311.15951v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tirumala_D/0/1/0/all/0/1">Dhruva Tirumala</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1">Thomas Lampe</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jose Enrique Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Haarnoja_T/0/1/0/all/0/1">Tuomas Haarnoja</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sandy Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lever_G/0/1/0/all/0/1">Guy Lever</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1">Ben Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertweck_T/0/1/0/all/0/1">Tim Hertweck</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1">Leonard Hasenclever</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1">Nicolas Heess</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1">Markus Wulfmeier</a></p>
<p>Replaying data is a principal mechanism underlying the stability and data
efficiency of off-policy reinforcement learning (RL). We present an effective
yet simple framework to extend the use of replays across multiple experiments,
minimally adapting the RL workflow for sizeable improvements in controller
performance and research iteration times. At its core, Replay Across
Experiments (RaE) involves reusing experience from previous experiments to
improve exploration and bootstrap learning while reducing required changes to a
minimum in comparison to prior work. We empirically show benefits across a
number of RL algorithms and challenging control domains spanning both
locomotion and manipulation, including hard exploration tasks from egocentric
vision. Through comprehensive ablations, we demonstrate robustness to the
quality and amount of data available and various hyperparameter choices.
Finally, we discuss how our approach can be applied more broadly across
research life cycles and can increase resilience by reloading data across
random seeds or hyperparameter variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16103">Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models. (arXiv:2311.16103v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1">Munan Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yujia Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiaxi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dongdong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>Video-based large language models (Video-LLMs) have been recently introduced,
targeting both fundamental improvements in perception and comprehension, and a
diverse range of user inquiries. In pursuit of the ultimate goal of achieving
artificial general intelligence, a truly intelligent Video-LLM model should not
only see and understand the surroundings, but also possess human-level
commonsense, and make well-informed decisions for the users. To guide the
development of such a model, the establishment of a robust and comprehensive
evaluation system becomes crucial. To this end, this paper proposes
\textit{Video-Bench}, a new comprehensive benchmark along with a toolkit
specifically designed for evaluating Video-LLMs. The benchmark comprises 10
meticulously crafted tasks, evaluating the capabilities of Video-LLMs across
three distinct levels: Video-exclusive Understanding, Prior Knowledge-based
Question-Answering, and Comprehension and Decision-making. In addition, we
introduce an automatic toolkit tailored to process model outputs for various
tasks, facilitating the calculation of metrics and generating convenient final
scores. We evaluate 8 representative Video-LLMs using \textit{Video-Bench}. The
findings reveal that current Video-LLMs still fall considerably short of
achieving human-like comprehension and analysis of real-world videos, offering
valuable insights for future research directions. The benchmark and toolkit are
available at: \url{https://github.com/PKU-YuanGroup/Video-Bench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10328">TransONet: Automatic Segmentation of Vasculature in Computed Tomographic Angiograms Using Deep Learning. (arXiv:2311.10328v1 [eess.IV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rajeoni_A/0/1/0/all/0/1">Alireza Bagheri Rajeoni</a>, <a href="http://arxiv.org/find/eess/1/au:+Pederson_B/0/1/0/all/0/1">Breanna Pederson</a>, <a href="http://arxiv.org/find/eess/1/au:+Firooz_A/0/1/0/all/0/1">Ali Firooz</a>, <a href="http://arxiv.org/find/eess/1/au:+Abdollahi_H/0/1/0/all/0/1">Hamed Abdollahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Smith_A/0/1/0/all/0/1">Andrew K. Smith</a>, <a href="http://arxiv.org/find/eess/1/au:+Clair_D/0/1/0/all/0/1">Daniel G. Clair</a>, <a href="http://arxiv.org/find/eess/1/au:+Lessner_S/0/1/0/all/0/1">Susan M. Lessner</a>, <a href="http://arxiv.org/find/eess/1/au:+Valafar_H/0/1/0/all/0/1">Homayoun Valafar</a></p>
<p>Pathological alterations in the human vascular system underlie many chronic
diseases, such as atherosclerosis and aneurysms. However, manually analyzing
diagnostic images of the vascular system, such as computed tomographic
angiograms (CTAs) is a time-consuming and tedious process. To address this
issue, we propose a deep learning model to segment the vascular system in CTA
images of patients undergoing surgery for peripheral arterial disease (PAD).
Our study focused on accurately segmenting the vascular system (1) from the
descending thoracic aorta to the iliac bifurcation and (2) from the descending
thoracic aorta to the knees in CTA images using deep learning techniques. Our
approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset
for (1) and (2), respectively, highlighting its high accuracy and potential
clinical utility. These findings demonstrate the use of deep learning
techniques as a valuable tool for medical professionals to analyze the health
of the vascular system efficiently and accurately. Please visit the GitHub page
for this paper at https://github.com/pip-alireza/TransOnet.
</p>
</p>
</div>

    </div>
    </body>
    