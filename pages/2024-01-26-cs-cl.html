<!DOCTYPE html>
<html>
<head>
<title>2024-01-26-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.12980">Identifying Risk Patterns in Brazilian Police Reports Preceding Femicides: A Long Short Term Memory (LSTM) Based Analysis. (arXiv:2401.12980v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lima_V/0/1/0/all/0/1">Vinicius Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1">Jaque Almeida de Oliveira</a></p>
<p>Femicide refers to the killing of a female victim, often perpetrated by an
intimate partner or family member, and is also associated with gender-based
violence. Studies have shown that there is a pattern of escalating violence
leading up to these killings, highlighting the potential for prevention if the
level of danger to the victim can be assessed. Machine learning offers a
promising approach to address this challenge by predicting risk levels based on
textual descriptions of the violence. In this study, we employed the Long Short
Term Memory (LSTM) technique to identify patterns of behavior in Brazilian
police reports preceding femicides. Our first objective was to classify the
content of these reports as indicating either a lower or higher risk of the
victim being murdered, achieving an accuracy of 66%. In the second approach, we
developed a model to predict the next action a victim might experience within a
sequence of patterned events. Both approaches contribute to the understanding
and assessment of the risks associated with domestic violence, providing
authorities with valuable insights to protect women and prevent situations from
escalating.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12981">A General-purpose AI Avatar in Healthcare. (arXiv:2401.12981v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_N/0/1/0/all/0/1">Nicholas Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Alterovitz_G/0/1/0/all/0/1">Gil Alterovitz</a></p>
<p>Recent advancements in machine learning and natural language processing have
led to the rapid development of artificial intelligence (AI) as a valuable tool
in the healthcare industry. Using large language models (LLMs) as
conversational agents or chatbots has the potential to assist doctors in
diagnosing patients, detecting early symptoms of diseases, and providing health
advice to patients. This paper focuses on the role of chatbots in healthcare
and explores the use of avatars to make AI interactions more appealing to
patients. A framework of a general-purpose AI avatar application is
demonstrated by using a three-category prompt dictionary and prompt improvement
mechanism. A two-phase approach is suggested to fine-tune a general-purpose AI
language model and create different AI avatars to discuss medical issues with
users. Prompt engineering enhances the chatbot's conversational abilities and
personality traits, fostering a more human-like interaction with patients.
Ultimately, the injection of personality into the chatbot could potentially
increase patient engagement. Future directions for research include
investigating ways to improve chatbots' understanding of context and ensuring
the accuracy of their outputs through fine-tuning with specialized medical data
sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12982">Text Classification: A Review, Empirical, and Experimental Evaluation. (arXiv:2401.12982v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taha_K/0/1/0/all/0/1">Kamal Taha</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_P/0/1/0/all/0/1">Paul D. Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeun_C/0/1/0/all/0/1">Chan Yeun</a>, <a href="http://arxiv.org/find/cs/1/au:+Taha_A/0/1/0/all/0/1">Aya Taha</a></p>
<p>The explosive and widespread growth of data necessitates the use of text
classification to extract crucial information from vast amounts of data.
Consequently, there has been a surge of research in both classical and deep
learning text classification methods. Despite the numerous methods proposed in
the literature, there is still a pressing need for a comprehensive and
up-to-date survey. Existing survey papers categorize algorithms for text
classification into broad classes, which can lead to the misclassification of
unrelated algorithms and incorrect assessments of their qualities and behaviors
using the same metrics. To address these limitations, our paper introduces a
novel methodological taxonomy that classifies algorithms hierarchically into
fine-grained classes and specific techniques. The taxonomy includes methodology
categories, methodology techniques, and methodology sub-techniques. Our study
is the first survey to utilize this methodological taxonomy for classifying
algorithms for text classification. Furthermore, our study also conducts
empirical evaluation and experimental comparisons and rankings of different
algorithms that employ the same specific sub-technique, different
sub-techniques within the same technique, different techniques within the same
category, and categories
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12983">Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding. (arXiv:2401.12983v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Jie Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jixin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_P/0/1/0/all/0/1">Peng Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yujie Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Beikang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Filla_N/0/1/0/all/0/1">Nicholas Filla</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Keke Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xianqiao Wang</a></p>
<p>This study is a pioneering endeavor to investigate the capabilities of Large
Language Models (LLMs) in addressing conceptual questions within the domain of
mechanical engineering with a focus on mechanics. Our examination involves a
manually crafted exam encompassing 126 multiple-choice questions, spanning
various aspects of mechanics courses, including Fluid Mechanics, Mechanical
Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of
Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),
ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against
engineering faculties and students with or without mechanical engineering
background. The findings reveal GPT-4's superior performance over the other two
LLMs and human cohorts in answering questions across various mechanics topics,
except for Continuum Mechanics. This signals the potential future improvements
for GPT models in handling symbolic calculations and tensor analyses. The
performances of LLMs were all significantly improved with explanations prompted
prior to direct responses, underscoring the crucial role of prompt engineering.
Interestingly, GPT-3.5 demonstrates improved performance with prompts covering
a broader domain, while GPT-4 excels with prompts focusing on specific
subjects. Finally, GPT-4 exhibits notable advancements in mitigating input
bias, as evidenced by guessing preferences for humans. This study unveils the
substantial potential of LLMs as highly knowledgeable assistants in both
mechanical pedagogy and scientific research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12985">The Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias. (arXiv:2401.12985v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_K/0/1/0/all/0/1">Kausik Lakkaraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aniket Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1">Biplav Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Valtorta_M/0/1/0/all/0/1">Marco Valtorta</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dezhi Wu</a></p>
<p>Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence
(AI) systems that output polarity and emotional intensity when given a piece of
text as input. Like other AIs, SASs are also known to have unstable behavior
when subjected to changes in data which can make it problematic to trust out of
concerns like bias when AI works with humans and data has protected attributes
like gender, race, and age. Recently, an approach was introduced to assess SASs
in a blackbox setting without training data or code, and rating them for bias
using synthetic English data. We augment it by introducing two human-generated
chatbot datasets and also consider a round-trip setting of translating the data
from one language to the same through an intermediate language. We find that
these settings show SASs performance in a more realistic light. Specifically,
we find that rating SASs on the chatbot data showed more bias compared to the
synthetic data, and round-tripping using Spanish and Danish as intermediate
languages reduces the bias (up to 68% reduction) in human-generated data while,
in synthetic data, it takes a surprising turn by increasing the bias! Our
findings will help researchers and practitioners refine their SAS testing
strategies and foster trust as SASs are considered part of more
mission-critical applications for global use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12986">Crowdsourced Adaptive Surveys. (arXiv:2401.12986v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Velez_Y/0/1/0/all/0/1">Yamil Velez</a></p>
<p>Public opinion surveys are vital for informing democratic decision-making,
but responding to rapidly changing information environments and measuring
beliefs within niche communities can be challenging for traditional survey
methods. This paper introduces a crowdsourced adaptive survey methodology
(CSAS) that unites advances in natural language processing and adaptive
algorithms to generate question banks that evolve with user input. The CSAS
method converts open-ended text provided by participants into Likert-style
items and applies a multi-armed bandit algorithm to determine user-provided
questions that should be prioritized in the survey. The method's adaptive
nature allows for the exploration of new survey questions, while imposing
minimal costs in survey length. Applications in the domains of Latino
information environments and issue importance showcase CSAS's ability to
identify claims or issues that might otherwise be difficult to track using
standard approaches. I conclude by discussing the method's potential for
studying topics where participant-generated content might improve our
understanding of public opinion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12987">TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1">Taeyang Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1">Hyunkuk Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jeonghwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Min Song</a></p>
<p>Emotion Recognition in Conversation (ERC) plays a crucial role in enabling
dialogue systems to effectively respond to user requests. The emotions in a
conversation can be identified by the representations from various modalities,
such as audio, visual, and text. However, due to the weak contribution of
non-verbal modalities to recognize emotions, multimodal ERC has always been
considered a challenging task. In this paper, we propose Teacher-leading
Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal
knowledge distillation to transfer information from a language model acting as
the teacher to the non-verbal students, thereby optimizing the efficacy of the
weak modalities. We then combine multimodal features using a shifting fusion
approach in which student networks support the teacher. TelME achieves
state-of-the-art performance in MELD, a multi-speaker conversation dataset for
ERC. Finally, we demonstrate the effectiveness of our components through
additional experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12988">Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenli Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiaheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Buomsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1">Yidong Chai</a></p>
<p>This study harnesses state-of-the-art AI technology for chronic disease
management, specifically in detecting various mental disorders through
user-generated textual content. Existing studies typically rely on fully
supervised machine learning, which presents challenges such as the
labor-intensive manual process of annotating extensive training data for each
disease and the need to design specialized deep learning architectures for each
problem. To address such challenges, we propose a novel framework that
leverages advanced AI techniques, including large language models and
multi-prompt engineering. Specifically, we address two key technical challenges
in data-driven chronic disease management: (1) developing personalized prompts
to represent each user's uniqueness and (2) incorporating medical knowledge
into prompts to provide context for chronic disease detection, instruct
learning objectives, and operationalize prediction goals. We evaluate our
method using four mental disorders, which are prevalent chronic diseases
worldwide, as research cases. On the depression detection task, our method (F1
= 0.975~0.978) significantly outperforms traditional supervised learning
paradigms, including feature engineering (F1 = 0.760) and architecture
engineering (F1 = 0.756). Meanwhile, our approach demonstrates success in
few-shot learning, i.e., requiring only a minimal number of training examples
to detect chronic diseases based on user-generated textual content (i.e., only
2, 10, or 100 subjects). Moreover, our method can be generalized to other
mental disorder detection tasks, including anorexia, pathological gambling, and
self-harm (F1 = 0.919~0.978).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12989">Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports. (arXiv:2401.12989v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belisario_A/0/1/0/all/0/1">Adriano Belisario</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1">Scott Hale</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocher_L/0/1/0/all/0/1">Luc Rocher</a></p>
<p>Gun violence is a pressing and growing human rights issue that affects nearly
every dimension of the social fabric, from healthcare and education to
psychology and the economy. Reliable data on firearm events is paramount to
developing more effective public policy and emergency responses. However, the
lack of comprehensive databases and the risks of in-person surveys prevent
human rights organizations from collecting needed data in most countries. Here,
we partner with a Brazilian human rights organization to conduct a systematic
evaluation of language models to assist with monitoring real-world firearm
events from social media data. We propose a fine-tuned BERT-based model trained
on Twitter (now X) texts to distinguish gun violence reports from ordinary
Portuguese texts. Our model achieves a high AUC score of 0.97. We then
incorporate our model into a web application and test it in a live
intervention. We study and interview Brazilian analysts who continuously
fact-check social media texts to identify new gun violence events. Qualitative
assessments show that our solution helped all analysts use their time more
efficiently and expanded their search capacities. Quantitative assessments show
that the use of our model was associated with more analysts' interactions with
online users reporting gun violence. Taken together, our findings suggest that
modern Natural Language Processing techniques can help support the work of
human rights organizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12990">Topic Modelling: Going Beyond Token Outputs. (arXiv:2401.12990v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1">Lowri Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Anthi_E/0/1/0/all/0/1">Eirini Anthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arman_L/0/1/0/all/0/1">Laura Arman</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnap_P/0/1/0/all/0/1">Pete Burnap</a></p>
<p>Topic modelling is a text mining technique for identifying salient themes
from a number of documents. The output is commonly a set of topics consisting
of isolated tokens that often co-occur in such documents. Manual effort is
often associated with interpreting a topic's description from such tokens.
However, from a human's perspective, such outputs may not adequately provide
enough information to infer the meaning of the topics; thus, their
interpretability is often inaccurately understood. Although several studies
have attempted to automatically extend topic descriptions as a means of
enhancing the interpretation of topic models, they rely on external language
sources that may become unavailable, must be kept up-to-date to generate
relevant results, and present privacy issues when training on or processing
data. This paper presents a novel approach towards extending the output of
traditional topic modelling methods beyond a list of isolated tokens. This
approach removes the dependence on external sources by using the textual data
itself by extracting high-scoring keywords and mapping them to the topic
model's token outputs. To measure the interpretability of the proposed outputs
against those of the traditional topic modelling approach, independent
annotators manually scored each output based on their quality and usefulness,
as well as the efficiency of the annotation task. The proposed approach
demonstrated higher quality and usefulness, as well as higher efficiency in the
annotation task, in comparison to the outputs of a traditional topic modelling
method, demonstrating an increase in their interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12992">TranSentence: Speech-to-speech Translation via Language-agnostic Sentence-level Speech Encoding without Language-parallel Data. (arXiv:2401.12992v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seung-Bin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sang-Hoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Although there has been significant advancement in the field of
speech-to-speech translation, conventional models still require
language-parallel speech data between the source and target languages for
training. In this paper, we introduce TranSentence, a novel speech-to-speech
translation without language-parallel speech data. To achieve this, we first
adopt a language-agnostic sentence-level speech encoding that captures the
semantic information of speech, irrespective of language. We then train our
model to generate speech based on the encoded embedding obtained from a
language-agnostic sentence-level speech encoder that is pre-trained with
various languages. With this method, despite training exclusively on the target
language's monolingual data, we can generate target language speech in the
inference stage using language-agnostic speech embedding from the source
language speech. Furthermore, we extend TranSentence to multilingual
speech-to-speech translation. The experimental results demonstrate that
TranSentence is superior to other models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12993">Estimating the severity of dental and oral problems via sentiment classification over clinical reports. (arXiv:2401.12993v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahdavifar_S/0/1/0/all/0/1">Sare Mahdavifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fakhrahmad_S/0/1/0/all/0/1">Seyed Mostafa Fakhrahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Ansarifard_E/0/1/0/all/0/1">Elham Ansarifard</a></p>
<p>Analyzing authors' sentiments in texts as a technique for identifying text
polarity can be practical and useful in various fields, including medicine and
dentistry. Currently, due to factors such as patients' limited knowledge about
their condition, difficulties in accessing specialist doctors, or fear of
illness, particularly in pandemic conditions, there might be a delay between
receiving a radiology report and consulting a doctor. In some cases, this delay
can pose significant risks to the patient, making timely decision-making
crucial. Having an automatic system that can inform patients about the
deterioration of their condition by analyzing the text of radiology reports
could greatly impact timely decision-making. In this study, a dataset
comprising 1,134 cone-beam computed tomography (CBCT) photo reports was
collected from the Shiraz University of Medical Sciences. Each case was
examined, and an expert labeled a severity level for the patient's condition on
each document. After preprocessing all the text data, a deep learning model
based on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)
network architecture, known as CNN-LSTM, was developed to detect the severity
level of the patient's problem based on sentiment analysis in the radiologist's
report. The model's performance was evaluated on two datasets, each with two
and four classes, in both imbalanced and balanced scenarios. Finally, to
demonstrate the effectiveness of our model, we compared its performance with
that of other classification models. The results, along with one-way ANOVA and
Tukey's test, indicated that our proposed model (CNN-LSTM) performed the best
according to precision, recall, and f-measure criteria. This suggests that it
can be a reliable model for estimating the severity of oral and dental
diseases, thereby assisting patients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12994">Automated Scoring of Clinical Patient Notes using Advanced NLP and Pseudo Labeling. (arXiv:2401.12994v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifeng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Bin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shulin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tianbo Song</a></p>
<p>Clinical patient notes are critical for documenting patient interactions,
diagnoses, and treatment plans in medical practice. Ensuring accurate
evaluation of these notes is essential for medical education and certification.
However, manual evaluation is complex and time-consuming, often resulting in
variability and resource-intensive assessments. To tackle these challenges,
this research introduces an approach leveraging state-of-the-art Natural
Language Processing (NLP) techniques, specifically Masked Language Modeling
(MLM) pretraining, and pseudo labeling. Our methodology enhances efficiency and
effectiveness, significantly reducing training time without compromising
performance. Experimental results showcase improved model performance,
indicating a potential transformation in clinical note assessment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12995">Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed Response Generation in Dialogues. (arXiv:2401.12995v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Shivani Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Tanmoy Chakraborty</a></p>
<p>Code-mixing, the blending of multiple languages within a single conversation,
introduces a distinctive challenge, particularly in the context of response
generation. Capturing the intricacies of code-mixing proves to be a formidable
task, given the wide-ranging variations influenced by individual speaking
styles and cultural backgrounds. In this study, we explore response generation
within code-mixed conversations. We introduce a novel approach centered on
harnessing the Big Five personality traits acquired in an unsupervised manner
from the conversations to bolster the performance of response generation. These
inferred personality attributes are seamlessly woven into the fabric of the
dialogue context, using a novel fusion mechanism, PA3. It uses an effective
two-step attention formulation to fuse the dialogue and personality
information. This fusion not only enhances the contextual relevance of
generated responses but also elevates the overall performance of the model. Our
experimental results, grounded in a dataset comprising of multi-party
Hindi-English code-mix conversations, highlight the substantial advantages
offered by personality-infused models over their conventional counterparts.
This is evident in the increase observed in ROUGE and BLUE scores for the
response generation task when the identified personality is seamlessly
integrated into the dialogue context. Qualitative assessment for personality
identification and response generation aligns well with our quantitative
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12996">A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes. (arXiv:2401.12996v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Workman_T/0/1/0/all/0/1">Terri Elizabeth Workman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kupersmith_J/0/1/0/all/0/1">Joel Kupersmith</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1">Phillip Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Spevak_C/0/1/0/all/0/1">Christopher Spevak</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandbrink_F/0/1/0/all/0/1">Friedhelm Sandbrink</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Treitler_Y/0/1/0/all/0/1">Yan Cheng Qing Zeng-Treitler</a></p>
<p>Background: Electronic health records (EHRs) are a data source for opioid
research. Opioid use disorder is known to be under-coded as a diagnosis, yet
problematic opioid use can be documented in clinical notes.
</p>
<p>Objectives: Our goals were 1) to identify problematic opioid use from a full
range of clinical notes; and 2) to compare the characteristics of patients
identified as having problematic opioid use, exclusively documented in clinical
notes, to those having documented ICD opioid use disorder diagnostic codes.
</p>
<p>Materials and Methods: We developed and applied a natural language processing
(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two
Veteran Affairs service regions to identify patients with problematic opioid
use. We also used a set of ICD diagnostic codes to identify patients with
opioid use disorder from the same cohort. We compared the demographic and
clinical characteristics of patients identified only through NLP, to those of
patients identified through ICD codes.
</p>
<p>Results: NLP exclusively identified 57,331 patients; 6,997 patients had
positive ICD code identifications. Patients exclusively identified through NLP
were more likely to be women. Those identified through ICD codes were more
likely to be male, younger, have concurrent benzodiazepine prescriptions, more
comorbidities, more care encounters, and less likely to be married. Patients in
the NLP and ICD groups had substantially elevated comorbidity levels compared
to patients not documented as experiencing problematic opioid use.
</p>
<p>Conclusions: NLP is a feasible approach for identifying problematic opioid
use not otherwise recorded by ICD codes. Clinicians may be reluctant to code
for opioid use disorder. It is therefore incumbent on the healthcare team to
search for documentation of opioid concerns within clinical notes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12997">Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion. (arXiv:2401.12997v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Cunhang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yujie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jun Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1">Yonghui Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jianhua Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1">Zhao Lv</a></p>
<p>In recent years, knowledge graph completion (KGC) models based on pre-trained
language model (PLM) have shown promising results. However, the large number of
parameters and high computational cost of PLM models pose challenges for their
application in downstream tasks. This paper proposes a progressive distillation
method based on masked generation features for KGC task, aiming to
significantly reduce the complexity of pre-trained models. Specifically, we
perform pre-distillation on PLM to obtain high-quality teacher models, and
compress the PLM network to obtain multi-grade student models. However,
traditional feature distillation suffers from the limitation of having a single
representation of information in teacher models. To solve this problem, we
propose masked generation of teacher-student features, which contain richer
representation information. Furthermore, there is a significant gap in
representation ability between teacher and student. Therefore, we design a
progressive distillation method to distill student models at each grade level,
enabling efficient knowledge transfer from teachers to students. The
experimental results demonstrate that the model in the pre-distillation stage
surpasses the existing state-of-the-art methods. Furthermore, in the
progressive distillation stage, the model significantly reduces the model
parameters while maintaining a certain level of performance. Specifically, the
model parameters of the lower-grade student model are reduced by 56.7\%
compared to the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12998">Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA. (arXiv:2401.12998v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+You_M/0/1/0/all/0/1">MingKe You</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Li Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">WeiZhi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Gang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a></p>
<p>The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13060">TCE at Qur&#x27;an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur&#x27;anic QA. (arXiv:2401.13060v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elkomy_M/0/1/0/all/0/1">Mohammed Alaa Elkomy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarhan_A/0/1/0/all/0/1">Amany Sarhan</a></p>
<p>In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks
A and B. To address the challenge of low-resourced training data, we rely on
transfer learning together with a voting ensemble to improve prediction
stability across multiple runs. Additionally, we employ different architectures
and learning mechanisms for a range of Arabic pre-trained transformer-based
models for both tasks. To identify unanswerable questions, we propose using a
thresholding mechanism. Our top-performing systems greatly surpass the baseline
performance on the hidden split, achieving a MAP score of 25.05% for task A and
a partial Average Precision (pAP) of 57.11% for task B.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13085">IndiText Boost: Text Augmentation for Low Resource India Languages. (arXiv:2401.13085v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1">Onkar Litake</a>, <a href="http://arxiv.org/find/cs/1/au:+Yagnik_N/0/1/0/all/0/1">Niraj Yagnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Labhsetwar_S/0/1/0/all/0/1">Shreyas Labhsetwar</a></p>
<p>Text Augmentation is an important task for low-resource languages. It helps
deal with the problem of data scarcity. A data augmentation strategy is used to
deal with the problem of data scarcity. Through the years, much work has been
done on data augmentation for the English language. In contrast, very less work
has been done on Indian languages. This is contrary to the fact that data
augmentation is used to deal with data scarcity. In this work, we focus on
implementing techniques like Easy Data Augmentation, Back Translation,
Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for
text classification on different languages. We focus on 6 Indian languages
namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to
our knowledge, no such work exists for text augmentation on Indian languages.
We carry out binary as well as multi-class text classification to make our
results more comparable. We get surprising results as basic data augmentation
techniques surpass LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13086">Towards Trustable Language Models: Investigating Information Quality of Large Language Models. (arXiv:2401.13086v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rejeleene_R/0/1/0/all/0/1">Rick Rejeleene</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaowei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Talburt_J/0/1/0/all/0/1">John Talburt</a></p>
<p>Large language models (LLM) are generating information at a rapid pace,
requiring users to increasingly rely and trust the data. Despite remarkable
advances of LLM, Information generated by LLM is not completely trustworthy,
due to challenges in information quality. Specifically, integrity of
Information quality decreases due to unreliable, biased, tokenization during
pre-training of LLM. Moreover, due to decreased information quality issues, has
led towards hallucination, fabricated information. Unreliable information can
lead towards flawed decisions in businesses, which impacts economic activity.
In this work, we introduce novel mathematical information quality evaluation of
LLM, we furthermore analyze and highlight information quality challenges,
scaling laws to systematically scale language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13129">Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains. (arXiv:2401.13129v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yanzhen Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yu Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1">Lucian Popa</a>, <a href="http://arxiv.org/find/cs/1/au:+Shwartz_L/0/1/0/all/0/1">Larisa Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1">ChengXiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a></p>
<p>Accurately typing entity mentions from text segments is a fundamental task
for various natural language processing applications. Many previous approaches
rely on massive human-annotated data to perform entity typing. Nevertheless,
collecting such data in highly specialized science and engineering domains
(e.g., software engineering and security) can be time-consuming and costly,
without mentioning the domain gaps between training and inference data if the
model needs to be applied to confidential datasets. In this paper, we study the
task of seed-guided fine-grained entity typing in science and engineering
domains, which takes the name and a few seed entities for each entity type as
the only supervision and aims to classify new entity mentions into both seen
and unseen types (i.e., those without seed entities). To solve this problem, we
propose SEType which first enriches the weak supervision by finding more
entities for each seen type from an unlabeled corpus using the contextualized
representations of pre-trained language models. It then matches the enriched
entities to unlabeled text to get pseudo-labeled samples and trains a textual
entailment model that can make inferences for both seen and unseen types.
Extensive experiments on two datasets covering four domains demonstrate the
effectiveness of SEType in comparison with various baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13133">Analyzing COVID-19 Vaccination Sentiments in Nigerian Cyberspace: Insights from a Manually Annotated Twitter Dataset. (arXiv:2401.13133v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1">Ibrahim Said Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliyu_L/0/1/0/all/0/1">Lukman Jibril Aliyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalid_A/0/1/0/all/0/1">Abubakar Auwal Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliyu_S/0/1/0/all/0/1">Saminu Muhammad Aliyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1">Shamsuddeen Hassan Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1">Idris Abdulmumin</a>, <a href="http://arxiv.org/find/cs/1/au:+Abduljalil_B/0/1/0/all/0/1">Bala Mairiga Abduljalil</a>, <a href="http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1">Bello Shehu Bello</a>, <a href="http://arxiv.org/find/cs/1/au:+Abubakar_A/0/1/0/all/0/1">Amina Imam Abubakar</a></p>
<p>Numerous successes have been achieved in combating the COVID-19 pandemic,
initially using various precautionary measures like lockdowns, social
distancing, and the use of face masks. More recently, various vaccinations have
been developed to aid in the prevention or reduction of the severity of the
COVID-19 infection. Despite the effectiveness of the precautionary measures and
the vaccines, there are several controversies that are massively shared on
social media platforms like Twitter. In this paper, we explore the use of
state-of-the-art transformer-based language models to study people's acceptance
of vaccines in Nigeria. We developed a novel dataset by crawling multi-lingual
tweets using relevant hashtags and keywords. Our analysis and visualizations
revealed that most tweets expressed neutral sentiments about COVID-19 vaccines,
with some individuals expressing positive views, and there was no strong
preference for specific vaccine types, although Moderna received slightly more
positive sentiment. We also found out that fine-tuning a pre-trained LLM with
an appropriate dataset can yield competitive results, even if the LLM was not
initially pre-trained on the specific language of that dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13136">The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lingfeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Weiting Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sihao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yunmo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haoran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Boyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1">Philipp Koehn</a>, <a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1">Daniel Khashabi</a></p>
<p>As the influence of large language models (LLMs) spans across global
communities, their safety challenges in multilingual settings become paramount
for alignment research. This paper examines the variations in safety challenges
faced by LLMs across different languages and discusses approaches to
alleviating such concerns. By comparing how state-of-the-art LLMs respond to
the same set of malicious prompts written in higher- vs. lower-resource
languages, we observe that (1) LLMs tend to generate unsafe responses much more
often when a malicious prompt is written in a lower-resource language, and (2)
LLMs tend to generate more irrelevant responses to malicious prompts in
lower-resource languages. To understand where the discrepancy can be
attributed, we study the effect of instruction tuning with reinforcement
learning from human feedback (RLHF) or supervised finetuning (SFT) on the
HH-RLHF dataset. Surprisingly, while training with high-resource languages
improves model alignment, training in lower-resource languages yields minimal
improvement. This suggests that the bottleneck of cross-lingual alignment is
rooted in the pretraining stage. Our findings highlight the challenges in
cross-lingual LLM safety, and we hope they inform future research in this
direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13146">Locality enhanced dynamic biasing and sampling strategies for contextual ASR. (arXiv:2401.13146v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jalal_M/0/1/0/all/0/1">Md Asif Jalal</a>, <a href="http://arxiv.org/find/eess/1/au:+Parada_P/0/1/0/all/0/1">Pablo Peso Parada</a>, <a href="http://arxiv.org/find/eess/1/au:+Pavlidis_G/0/1/0/all/0/1">George Pavlidis</a>, <a href="http://arxiv.org/find/eess/1/au:+Moschopoulos_V/0/1/0/all/0/1">Vasileios Moschopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Saravanan_K/0/1/0/all/0/1">Karthikeyan Saravanan</a>, <a href="http://arxiv.org/find/eess/1/au:+Kontoulis_C/0/1/0/all/0/1">Chrysovalantis-Giorgos Kontoulis</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jisi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Drosou_A/0/1/0/all/0/1">Anastasios Drosou</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_G/0/1/0/all/0/1">Gil Ho Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Jungin Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1">Seokyeong Jung</a></p>
<p>Automatic Speech Recognition (ASR) still face challenges when recognizing
time-variant rare-phrases. Contextual biasing (CB) modules bias ASR model
towards such contextually-relevant phrases. During training, a list of biasing
phrases are selected from a large pool of phrases following a sampling
strategy. In this work we firstly analyse different sampling strategies to
provide insights into the training of CB for ASR with correlation plots between
the bias embeddings among various training stages. Secondly, we introduce a
neighbourhood attention (NA) that localizes self attention (SA) to the nearest
neighbouring frames to further refine the CB output. The results show that this
proposed approach provides on average a 25.84% relative WER improvement on
LibriSpeech sets and rare-word evaluation compared to the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13160">SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection. (arXiv:2401.13160v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1">Ke Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Heinrich Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1">Afshin Rostamizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+DeSalvo_G/0/1/0/all/0/1">Giulia DeSalvo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kagy_J/0/1/0/all/0/1">Jean-Fran&#xe7;ois Kagy</a>, <a href="http://arxiv.org/find/cs/1/au:+Karydas_L/0/1/0/all/0/1">Lazaros Karydas</a>, <a href="http://arxiv.org/find/cs/1/au:+Citovsky_G/0/1/0/all/0/1">Gui Citovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sanjiv Kumar</a></p>
<p>Pre-training large language models is known to be extremely resource
intensive and often times inefficient, under-utilizing the information
encapsulated in the training text sequences. In this paper, we present SpacTor,
a new training procedure consisting of (1) a hybrid objective combining span
corruption (SC) and token replacement detection (RTD), and (2) a two-stage
curriculum that optimizes the hybrid objective over the initial $\tau$
iterations, then transitions to standard SC loss. We show empirically that the
effectiveness of the hybrid objective is tied to the two-stage pre-training
schedule, and provide extensive analysis on why this is the case. In our
experiments with encoder-decoder architectures (T5) on a variety of NLP tasks,
SpacTor-T5 yields the same downstream performance as standard SC pre-training,
while enabling a 50% reduction in pre-training iterations and 40% reduction in
total FLOPs. Alternatively, given the same amount of computing budget, we find
that SpacTor results in significantly improved downstream benchmark
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13165">Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Sourojit Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1">Srishti Chatterjee</a></p>
<p>This chapter focuses on gender-related errors in machine translation (MT) in
the context of low-resource languages. We begin by explaining what low-resource
languages are, examining the inseparable social and computational factors that
create such linguistic hierarchies. We demonstrate through a case study of our
mother tongue Bengali, a global language spoken by almost 300 million people
but still classified as low-resource, how gender is assumed and inferred in
translations to and from the high(est)-resource English when no such
information is provided in source texts. We discuss the postcolonial and
societal impacts of such errors leading to linguistic erasure and
representational harms, and conclude by discussing potential solutions towards
uplifting languages by providing them more agency in MT conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13170">CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1">Ishani Mondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yijun Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nghiem_H/0/1/0/all/0/1">Huy Nghiem</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1">Jordan Boyd-Graber</a></p>
<p>Question answering (QA) can only make progress if we know if an answer is
correct, but for many of the most challenging and interesting QA examples,
current evaluation metrics to determine answer equivalence (AE) often do not
align with human judgments, particularly more verbose, free-form answers from
large language models (LLM). There are two challenges: a lack of data and that
models are too big: LLM-based scorers can correlate better with human judges,
but this task has only been tested on limited QA datasets, and even when
available, update of the model is limited because LLMs are large and often
expensive. We rectify both of these issues by providing clear and consistent
guidelines for evaluating AE in machine QA adopted from professional human QA
contests. We also introduce a combination of standard evaluation and a more
efficient, robust, and lightweight discriminate AE classifier-based matching
method (CFMatch, smaller than 1 MB), trained and validated to more accurately
evaluate answer correctness in accordance with adopted expert AE rules that are
more aligned with human judgments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13178">AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junlei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaohui Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingpeng Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a></p>
<p>Evaluating large language models (LLMs) as general-purpose agents is
essential for understanding their capabilities and facilitating their
integration into practical applications. However, the evaluation process
presents substantial challenges. A primary obstacle is the benchmarking of
agent performance across diverse scenarios within a unified framework,
especially in maintaining partially-observable environments and ensuring
multi-round interactions. Moreover, current evaluation frameworks mostly focus
on the final success rate, revealing few insights during the process and
failing to provide a deep understanding of the model abilities. To address
these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark
and accompanied open-source evaluation framework tailored to analytical
evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric
that captures incremental advancements as well as a comprehensive evaluation
toolkit that features easy assessment of agents for multi-faceted analysis
through interactive visualization. This not only sheds light on the
capabilities and limitations of LLM agents but also propels the
interpretability of their performance to the forefront. Ultimately, AgentBoard
serves as a significant step towards demystifying agent behaviors and
accelerating the development of stronger LLM agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13201">MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfei Zhang</a></p>
<p>Multimodal large language models (MLLM) have achieved satisfactory results in
many tasks. However, their performance in the task of person re-identification
(ReID) has not been explored to date. This paper will investigate how to adapt
them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID
image-text datasets, and then use their visual encoder as a backbone for ReID.
However, there still exist two apparent issues: (1) Designing instructions for
ReID, MLLMs may overfit specific instructions, and designing a variety of
instructions will lead to higher costs. (2) Latent image feature vectors from
LLMs are not involved in loss computation. Instructional learning, aligning
image-text features, results in indirect optimization and a learning objective
that inadequately utilizes features, limiting effectiveness in person feature
learning. To address these problems, this paper proposes MLLMReID: Multimodal
Large Language Model-based ReID. Firstly, we proposed Common Instruction, a
simple approach that leverages the essence ability of LLMs to continue writing,
avoiding complex and diverse instruction design. Secondly, we proposed
DirectReID, which effectively employs the latent image feature vectors of
images outputted by LLMs in ReID tasks. The experimental results demonstrate
the superiority of our method. We will open-source the code on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13218">ULTRA: Unleash LLMs&#x27; Potential for Event Argument Extraction through Hierarchical Modeling and Pair-wise Refinement. (arXiv:2401.13218v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinliang Frederick Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Blum_C/0/1/0/all/0/1">Carter Blum</a>, <a href="http://arxiv.org/find/cs/1/au:+Choji_T/0/1/0/all/0/1">Temma Choji</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Shalin Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Vempala_A/0/1/0/all/0/1">Alakananda Vempala</a></p>
<p>Structural extraction of events within discourse is critical since it avails
a deeper understanding of communication patterns and behavior trends. Event
argument extraction (EAE), at the core of event-centric understanding, is the
task of identifying role-specific text spans (i.e., arguments) for a given
event. Document-level EAE (DocEAE) focuses on arguments that are scattered
across an entire document. In this work, we explore the capabilities of open
source Large Language Models (LLMs), i.e., Flan-UL2, for the DocEAE task. To
this end, we propose ULTRA, a hierarchical framework that extracts event
arguments more cost-effectively -- the method needs as few as 50 annotations
and doesn't require hitting costly API endpoints. Further, it alleviates the
positional bias issue intrinsic to LLMs. ULTRA first sequentially reads text
chunks of a document to generate a candidate argument set, upon which ULTRA
learns to drop non-pertinent candidates through self-refinement. We further
introduce LEAFER to address the challenge LLMs face in locating the exact
boundary of an argument span. ULTRA outperforms strong baselines, which include
strong supervised models and ChatGPT, by 9.8% when evaluated by the exact match
(EM) metric.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13223">TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fengbin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1">Fuli Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Moxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>In this work, we address question answering (QA) over a hybrid of tabular and
textual data that are very common content on the Web (e.g. SEC filings), where
discrete reasoning capabilities are often required. Recently, large language
models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning
capabilities. We then consider harnessing the amazing power of LLMs to solve
our task. We abstract a Step-wise Pipeline for tabular and textual QA, which
consists of three key steps, including Extractor, Reasoner and Executor, and
initially design an instruction to instantiate the pipeline and validate that
GPT-4 outperforms all existing methods. However, utilizing an online LLM like
GPT-4 holds various challenges in terms of cost, latency, and data security
risk, which motivates us to specialize smaller LLMs in this task. We develop a
TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated
automatically from existing expert-annotated datasets following the Step-wise
Pipeline. The experimental results have verified that our TAT-LLM model can
outperform all baseline models, including the previous best fine-tuned models
and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.
We hope our work can serve as a pioneering example of specializing smaller
language models for specific tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13227">Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1">Baolong Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shenghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_L/0/1/0/all/0/1">Lingrui Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xueqi Chen</a></p>
<p>Exploring the application of large-scale language models to graph learning is
a novel endeavor. However, the vast amount of information inherent in large
graphs poses significant challenges to this process. This paper focuses on the
link prediction task and introduces LPNL (Link Prediction via Natural
Language), a framework based on a large language model designed for scalable
link prediction on large-scale heterogeneous graphs.We design novel prompts for
link prediction that articulate graph details in natural language. We propose a
two-stage sampling pipeline to extract crucial information from large-scale
heterogeneous graphs, and a divide-and-conquer strategy to control the input
token count within predefined limits, addressing the challenge of overwhelming
information. We fine-tune a T5 model based on our self-supervised learning
designed for for link prediction. Extensive experiments on a large public
heterogeneous graphs demonstrate that LPNL outperforms various advanced
baselines, highlighting its remarkable performance in link prediction tasks on
large-scale graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13229">From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alcoforado_A/0/1/0/all/0/1">Alexandre Alcoforado</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1">Thomas Palmeira Ferraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Okamura_L/0/1/0/all/0/1">Lucas Hideki Okamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Fama_I/0/1/0/all/0/1">Israel Campos Fama</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavado_A/0/1/0/all/0/1">Arnold Moya Lavado</a>, <a href="http://arxiv.org/find/cs/1/au:+Bueno_B/0/1/0/all/0/1">B&#xe1;rbara Dias Bueno</a>, <a href="http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1">Bruno Veloso</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1">Anna Helena Reali Costa</a></p>
<p>A major challenge in Natural Language Processing is obtaining annotated data
for supervised learning. An option is the use of crowdsourcing platforms for
data annotation. However, crowdsourcing introduces issues related to the
annotator's experience, consistency, and biases. An alternative is to use
zero-shot methods, which in turn have limitations compared to their few-shot or
fully supervised counterparts. Recent advancements driven by large language
models show potential, but struggle to adapt to specialized domains with
severely limited data. The most common approaches therefore involve the human
itself randomly annotating a set of datapoints to build initial datasets. But
randomly sampling data to be annotated is often inefficient as it ignores the
characteristics of the data and the specific needs of the model. The situation
worsens when working with imbalanced datasets, as random sampling tends to
heavily bias towards the majority classes, leading to excessive annotated data.
To address these issues, this paper contributes an automatic and informed data
selection architecture to build a small dataset for few-shot learning. Our
proposal minimizes the quantity and maximizes diversity of data selected for
human annotation, while improving model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13246">SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning. (arXiv:2401.13246v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guoxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Kexin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1">Fuying Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yiming Qian</a></p>
<p>Elucidating the reasoning process with structured explanations from question
to answer is fundamentally crucial, as it significantly enhances the
interpretability and trustworthiness of question-answering (QA) systems.
However, structured explanations demand models to perform intricate structured
reasoning, which poses great challenges. Most existing methods focus on
single-step reasoning through supervised learning, ignoring logical
dependencies between steps. Meanwhile, existing reinforcement learning
(RL)-based methods overlook the structured relationships, impeding RL's
potential in structured reasoning. In this paper, we propose SEER, a novel
method that maximizes a structure-based return to facilitate structured
reasoning and explanation. Our proposed structure-based return precisely
describes the hierarchical and branching structure inherent in structured
reasoning, effectively capturing the intricate relationships between states. We
also introduce a fine-grained reward function to meticulously delineate diverse
reasoning steps. Extensive experiments show that SEER significantly outperforms
state-of-the-art methods, achieving an absolute improvement of 6.9% over
RL-based methods on EntailmentBank, a 4.4% average improvement on STREET
benchmark, and exhibiting outstanding efficiency and cross-dataset
generalization performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13256">UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zezhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jeff Z. Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kam-Fai Wong</a></p>
<p>Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13260">MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction. (arXiv:2401.13260v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiajun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaohan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingfeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1">Tomoki Toda</a></p>
<p>The prevalent approach in speech emotion recognition (SER) involves
integrating both audio and textual information to comprehensively identify the
speaker's emotion, with the text generally obtained through automatic speech
recognition (ASR). An essential issue of this approach is that ASR errors from
the text modality can worsen the performance of SER. Previous studies have
proposed using an auxiliary ASR error detection task to adaptively assign
weights of each word in ASR hypotheses. However, this approach has limited
improvement potential because it does not address the coherence of semantic
information in the text. Additionally, the inherent heterogeneity of different
modalities leads to distribution gaps between their representations, making
their fusion challenging. Therefore, in this paper, we incorporate two
auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to
enhance the semantic coherence of ASR text, and further introduce a novel
multi-modal fusion (MF) method to learn shared representations across
modalities. We refer to our method as MF-AED-AEC. Experimental results indicate
that MF-AED-AEC significantly outperforms the baseline model by a margin of
4.1\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13275">Can AI Assistants Know What They Don&#x27;t Know?. (arXiv:2401.13275v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1">Qinyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Recently, AI assistants based on large language models (LLMs) show surprising
performance in many tasks, such as dialogue, solving math problems, writing
code, and using tools. Although LLMs possess intensive world knowledge, they
still make factual errors when facing some knowledge intensive tasks, like
open-domain question answering. These untruthful responses from the AI
assistant may cause significant risks in practical applications. We believe
that an AI assistant's refusal to answer questions it does not know is a
crucial method for reducing hallucinations and making the assistant truthful.
Therefore, in this paper, we ask the question "Can AI assistants know what they
don't know and express them through natural language?" To answer this question,
we construct a model-specific "I don't know" (Idk) dataset for an assistant,
which contains its known and unknown questions, based on existing open-domain
question answering datasets. Then we align the assistant with its corresponding
Idk dataset and observe whether it can refuse to answer its unknown questions
after alignment. Experimental results show that after alignment with Idk
datasets, the assistant can refuse to answer most its unknown questions. For
questions they attempt to answer, the accuracy is significantly higher than
before the alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13298">Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hongzhan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Ziyang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ruichao Yang</a></p>
<p>The age of social media is flooded with Internet memes, necessitating a clear
grasp and effective identification of harmful ones. This task presents a
significant challenge due to the implicit meaning embedded in memes, which is
not explicitly conveyed through the surface text and image. However, existing
harmful meme detection methods do not present readable explanations that unveil
such implicit meaning to support their detection decisions. In this paper, we
propose an explainable approach to detect harmful memes, achieved through
reasoning over conflicting rationales from both harmless and harmful positions.
Specifically, inspired by the powerful capacity of Large Language Models (LLMs)
on text generation and reasoning, we first elicit multimodal debate between
LLMs to generate the explanations derived from the contradictory arguments.
Then we propose to fine-tune a small language model as the debate judge for
harmfulness inference, to facilitate multimodal fusion between the harmfulness
rationales and the intrinsic multimodal information within memes. In this way,
our model is empowered to perform dialectical reasoning over intricate and
implicit harm-indicative patterns, utilizing multimodal explanations
originating from both harmless and harmful arguments. Extensive experiments on
three public meme datasets demonstrate that our harmful meme detection approach
achieves much better performance than state-of-the-art methods and exhibits a
superior capacity for explaining the meme harmfulness of the model predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13303">MaLA-500: Massive Language Adaptation of Large Language Models. (arXiv:2401.13303v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Peiqin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shaoxiong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1">J&#xf6;rg Tiedemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1">Andr&#xe9; F. T. Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>Large language models have advanced the state of the art in natural language
processing. However, their predominant design for English or a limited set of
languages creates a substantial gap in their effectiveness for low-resource
languages. To bridge this gap, we introduce MaLA-500, a novel large language
model designed to cover an extensive range of 534 languages. To train MaLA-500,
we employ vocabulary extension and continued pretraining on LLaMA 2 with
Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves
state-of-the-art in-context learning results. We release MaLA-500 at
https://huggingface.co/MaLA-LM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13313">InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions. (arXiv:2401.13313v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1">Ryota Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Iki_T/0/1/0/all/0/1">Taichi Iki</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1">Kyosuke Nishida</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1">Kuniko Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1">Jun Suzuki</a></p>
<p>We study the problem of completing various visual document understanding
(VDU) tasks, e.g., question answering and information extraction, on real-world
documents through human-written instructions. To this end, we propose
InstructDoc, the first large-scale collection of 30 publicly available VDU
datasets, each with diverse instructions in a unified format, which covers a
wide range of 12 tasks and includes open document types/formats. Furthermore,
to enhance the generalization performance on VDU tasks, we design a new
instruction-based document reading and understanding model, InstructDr, that
connects document images, image encoders, and large language models (LLMs)
through a trainable bridging module. Experiments demonstrate that InstructDr
can effectively adapt to new VDU datasets, tasks, and domains via given
instructions and outperforms existing multimodal LLMs and ChatGPT without
specific training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13398">Text Categorization Can Enhance Domain-Agnostic Stopword Extraction. (arXiv:2401.13398v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1">Houcemeddine Turki</a>, <a href="http://arxiv.org/find/cs/1/au:+Etori_N/0/1/0/all/0/1">Naome A. Etori</a>, <a href="http://arxiv.org/find/cs/1/au:+Taieb_M/0/1/0/all/0/1">Mohamed Ali Hadj Taieb</a>, <a href="http://arxiv.org/find/cs/1/au:+Omotayo_A/0/1/0/all/0/1">Abdul-Hakeem Omotayo</a>, <a href="http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1">Chris Chinenye Emezue</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouicha_M/0/1/0/all/0/1">Mohamed Ben Aouicha</a>, <a href="http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1">Ayodele Awokoya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawan_F/0/1/0/all/0/1">Falalu Ibrahim Lawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nixdorf_D/0/1/0/all/0/1">Doreen Nixdorf</a></p>
<p>This paper investigates the role of text categorization in streamlining
stopword extraction in natural language processing (NLP), specifically focusing
on nine African languages alongside French. By leveraging the MasakhaNEWS,
African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that
text categorization effectively identifies domain-agnostic stopwords with over
80% detection success rate for most examined languages. Nevertheless,
linguistic variances result in lower detection rates for certain languages.
Interestingly, we find that while over 40% of stopwords are common across news
categories, less than 15% are unique to a single category. Uncommon stopwords
add depth to text but their classification as stopwords depends on context.
Therefore combining statistical and linguistic approaches creates comprehensive
stopword lists, highlighting the value of our hybrid method. This research
enhances NLP for African languages and underscores the importance of text
categorization in stopword extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13444">Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dehao Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Feng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongfeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minghu Jiang</a></p>
<p>In recent times, large language models (LLMs) have showcased remarkable
capabilities. However, updating their knowledge poses challenges, potentially
leading to inaccuracies when confronted with unfamiliar queries. While
integrating knowledge graphs with LLMs has been explored, existing approaches
treat LLMs as primary decision-makers, imposing high demands on their
capabilities. This is particularly unsuitable for LLMs with lower computational
costs and relatively poorer performance. In this paper, we introduce a
Clue-Guided Path Exploration framework (CGPE) that efficiently merges a
knowledge base with an LLM, placing less stringent requirements on the model's
capabilities. Inspired by the method humans use to manually retrieve knowledge,
CGPE employs information from the question as clues to systematically explore
the required knowledge path within the knowledge base. Experiments on
open-source datasets reveal that CGPE outperforms previous methods and is
highly applicable to LLMs with fewer parameters. In some instances, even
ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.
Furthermore, the results indicate a minimal invocation frequency of CGPE on
LLMs, suggesting reduced computational overhead. For organizations and
individuals facing constraints in computational resources, our research offers
significant practical value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13463">SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chyi-Jiunn Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guan-Ting Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yung-Sung Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wei-Lun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shang-Wen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1">Abdelrahman Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1">Lin-shan Lee</a></p>
<p>Spoken Question Answering (SQA) is essential for machines to reply to user's
question by finding the answer span within a given spoken passage. SQA has been
previously achieved without ASR to avoid recognition errors and
Out-of-Vocabulary (OOV) problems. However, the real-world problem of
Open-domain SQA (openSQA), in which the machine needs to first retrieve
passages that possibly contain the answer from a spoken archive in addition,
was never considered. This paper proposes the first known end-to-end framework,
Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the
openSQA problem. SpeechDPR learns a sentence-level semantic representation by
distilling knowledge from the cascading model of unsupervised ASR (UASR) and
text dense retriever (TDR). No manually transcribed speech data is needed.
Initial experiments showed performance comparable to the cascading model of
UASR and TDR, and significantly better when UASR was poor, verifying this
approach is more robust to speech recognition errors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13478">SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yiming Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaijing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chenghao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoran Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bohao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1">Noura Al Moubayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Multi-modal information retrieval (MMIR) is a rapidly evolving field, where
significant progress, particularly in image-text pairing, has been made through
advanced representation learning and cross-modality alignment research.
However, current benchmarks for evaluating MMIR performance in image-text
pairing within the scientific domain show a notable gap, where chart and table
images described in scholarly language usually do not play a significant role.
To bridge this gap, we develop a specialised scientific MMIR (SciMMIR)
benchmark by leveraging open-access paper collections to extract data relevant
to the scientific domain. This benchmark comprises 530K meticulously curated
image-text pairs, extracted from figures and tables with detailed captions in
scientific documents. We further annotate the image-text pairs with two-level
subset-subcategory hierarchy annotations to facilitate a more comprehensive
evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations
on prominent multi-modal image-captioning and visual language models, such as
CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific
domain, including the impact of pre-training and fine-tuning settings and the
influence of the visual and textual encoders. All our data and checkpoints are
publicly available at https://github.com/Wusiwei0410/SciMMIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13481">How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ashkinaze_J/0/1/0/all/0/1">Joshua Ashkinaze</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1">Julia Mendelsohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiwei_L/0/1/0/all/0/1">Li Qiwei</a>, <a href="http://arxiv.org/find/cs/1/au:+Budak_C/0/1/0/all/0/1">Ceren Budak</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_E/0/1/0/all/0/1">Eric Gilbert</a></p>
<p>Exposure to large language model output is rapidly increasing. How will
seeing AI-generated ideas affect human ideas? We conducted an experiment (800+
participants, 40+ countries) where participants viewed creative ideas that were
from ChatGPT or prior experimental participants and then brainstormed their own
idea. We varied the number of AI-generated examples (none, low, or high
exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic
experiment design -- ideas from prior participants in an experimental condition
are used as stimuli for future participants in the same experimental condition
-- mimics the interdependent process of cultural creation: creative ideas are
built upon prior ideas. Hence, we capture the compounding effects of having
LLMs 'in the culture loop'. We find that high AI exposure (but not low AI
exposure) did not affect the creativity of individual ideas but did increase
the average amount and rate of change of collective idea diversity. AI made
ideas different, not better. There were no main effects of disclosure. We also
found that self-reported creative people were less influenced by knowing an
idea was from AI, and that participants were more likely to knowingly adopt AI
ideas when the task was difficult. Our findings suggest that introducing AI
ideas into society may increase collective diversity but not individual
creativity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13512">Can GPT-3.5 Generate and Code Discharge Summaries?. (arXiv:2401.13512v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Falis_M/0/1/0/all/0/1">Mat&#xfa;&#x161; Falis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gema_A/0/1/0/all/0/1">Aryo Pradipta Gema</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Daines_L/0/1/0/all/0/1">Luke Daines</a>, <a href="http://arxiv.org/find/cs/1/au:+Basetti_S/0/1/0/all/0/1">Siddharth Basetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Holder_M/0/1/0/all/0/1">Michael Holder</a>, <a href="http://arxiv.org/find/cs/1/au:+Penfold_R/0/1/0/all/0/1">Rose S Penfold</a>, <a href="http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1">Alexandra Birch</a>, <a href="http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1">Beatrice Alex</a></p>
<p>Objective: To investigate GPT-3.5 in generating and coding medical documents
with ICD-10 codes for data augmentation on low-resources labels.
</p>
<p>Materials and Methods: Employing GPT-3.5 we generated and coded 9,606
discharge summaries based on lists of ICD-10 code descriptions of patients with
infrequent (generation) codes within the MIMIC-IV dataset. Combined with the
baseline training set, this formed an augmented training set. Neural coding
models were trained on baseline and augmented data and evaluated on a MIMIC-IV
test set. We report micro- and macro-F1 scores on the full codeset, generation
codes, and their families. Weak Hierarchical Confusion Matrices were employed
to determine within-family and outside-of-family coding errors in the latter
codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided
self-generated data and real MIMIC-IV data. Clinical professionals evaluated
the clinical acceptability of the generated documents.
</p>
<p>Results: Augmentation slightly hinders the overall performance of the models
but improves performance for the generation candidate codes and their families,
including one unseen in the baseline training data. Augmented models display
lower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the
prompted descriptions, but performs poorly on real data. Evaluators note the
correctness of generated concepts while suffering in variety, supporting
information, and narrative.
</p>
<p>Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.
Augmentation positively affects generation code families but mainly benefits
codes with existing examples. Augmentation reduces out-of-family errors.
Discharge summaries generated by GPT-3.5 state prompted concepts correctly but
lack variety, and authenticity in narratives. They are unsuitable for clinical
practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13527">SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1">Jun Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yaqian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Benefiting from effective speech modeling, current Speech Large Language
Models (SLLMs) have demonstrated exceptional capabilities in in-context speech
generation and efficient generalization to unseen speakers. However, the
prevailing information modeling process is encumbered by certain redundancies,
leading to inefficiencies in speech generation. We propose Chain-of-Information
Generation (CoIG), a method for decoupling semantic and perceptual information
in large-scale speech generation. Building on this, we develop SpeechGPT-Gen,
an 8-billion-parameter SLLM efficient in semantic and perceptual information
modeling. It comprises an autoregressive model based on LLM for semantic
information modeling and a non-autoregressive model employing flow matching for
perceptual information modeling. Additionally, we introduce the novel approach
of infusing semantic information into the prior distribution to enhance the
efficiency of flow matching. Extensive experimental results demonstrate that
SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice
conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable
proficiency in capturing and modeling speech's semantic and perceptual
dimensions. Code and models are available at
https://github.com/0nutation/SpeechGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13565">Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zolkepli_H/0/1/0/all/0/1">Husein Zolkepli</a>, <a href="http://arxiv.org/find/cs/1/au:+Razak_A/0/1/0/all/0/1">Aisyah Razak</a>, <a href="http://arxiv.org/find/cs/1/au:+Adha_K/0/1/0/all/0/1">Kamarul Adha</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazhan_A/0/1/0/all/0/1">Ariff Nazhan</a></p>
<p>In this paper, we present significant advancements in the pretraining of
Mistral 7B, a large-scale language model, using a dataset of 32.6 GB,
equivalent to 1.1 billion tokens. We explore the impact of extending the
context length, releasing models with context lengths of 4096 and 32768 tokens,
and further refining performance with a specialized 16384 context length
instruction-tuned model, we called it Malaysian Mistral.
</p>
<p>Our experiments demonstrate the efficacy of continue pretraining and the
influence of extended context lengths on Mistral 7B's language understanding
capabilities. Additionally, we release a model specifically tuned with a 16384
context length instruction, showcasing its potential for capturing nuanced
language intricacies.
</p>
<p>Furthermore, our research contributes to the benchmarking of Malaysian
Mistral against prominent language models, including ChatGPT3.5 and Claude 2.
We present compelling results indicating Malaysian Mistral's superior
performance on Tatabahasa (Malay grammar) test set, particularly when
fine-tuned with instructions.
</p>
<p>All models released at
https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13586">Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huerta_Enochian_M/0/1/0/all/0/1">Mathew Huerta-Enochian</a></p>
<p>We present a small study analyzing how prompt token classification loss
weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on
instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1
and LLaMA 2 using multiple instruction datasets. We found that models
fine-tuned on our short-completion dataset have a negative quadratic
relationship with PLW while models fine-tuned on long-completion datasets were
unaffected by PLW.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13588">Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes. (arXiv:2401.13588v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Darren Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Cheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Bold_D/0/1/0/all/0/1">Delgersuren Bold</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouvier_M/0/1/0/all/0/1">Monique Bouvier</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiaying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1">Benjamin Shickel</a>, <a href="http://arxiv.org/find/cs/1/au:+Jabaley_C/0/1/0/all/0/1">Craig S. Jabaley</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenhui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Soojin Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Young_M/0/1/0/all/0/1">Michael J. Young</a>, <a href="http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1">Mark S. Wainwright</a>, <a href="http://arxiv.org/find/cs/1/au:+Clermont_G/0/1/0/all/0/1">Gilles Clermont</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1">Parisa Rashidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenthal_E/0/1/0/all/0/1">Eric S. Rosenthal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimisko_L/0/1/0/all/0/1">Laurie Dimisko</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1">Ran Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Joo Heung Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Carl Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a></p>
<p>The field of healthcare has increasingly turned its focus towards Large
Language Models (LLMs) due to their remarkable performance. However, their
performance in actual clinical applications has been underexplored. Traditional
evaluations based on question-answering tasks don't fully capture the nuanced
contexts. This gap highlights the need for more in-depth and practical
assessments of LLMs in real-world healthcare settings. Objective: We sought to
evaluate the performance of LLMs in the complex clinical context of adult
critical care medicine using systematic and comprehensible analytic methods,
including clinician annotation and adjudication. Methods: We investigated the
performance of three general LLMs in understanding and processing real-world
clinical notes. Concepts from 150 clinical notes were identified by MetaMap and
then labeled by 9 clinicians. Each LLM's proficiency was evaluated by
identifying the temporality and negation of these concepts using different
prompts for an in-depth analysis. Results: GPT-4 showed overall superior
performance compared to other LLMs. In contrast, both GPT-3.5 and
text-davinci-003 exhibit enhanced performance when the appropriate prompting
strategies are employed. The GPT family models have demonstrated considerable
efficiency, evidenced by their cost-effectiveness and time-saving capabilities.
Conclusion: A comprehensive qualitative performance evaluation framework for
LLMs is developed and operationalized. This framework goes beyond singular
performance aspects. With expert annotations, this methodology not only
validates LLMs' capabilities in processing complex medical data but also
establishes a benchmark for future LLM evaluations across specialized domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13594">Graph Guided Question Answer Generation for Procedural Question-Answering. (arXiv:2401.13594v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hai X. Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadji_I/0/1/0/all/0/1">Isma Hadji</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xinnuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Degutyte_Z/0/1/0/all/0/1">Ziedune Degutyte</a>, <a href="http://arxiv.org/find/cs/1/au:+Rainey_J/0/1/0/all/0/1">Jay Rainey</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazakos_E/0/1/0/all/0/1">Evangelos Kazakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1">Afsaneh Fazly</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1">Georgios Tzimiropoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1">Brais Martinez</a></p>
<p>In this paper, we focus on task-specific question answering (QA). To this
end, we introduce a method for generating exhaustive and high-quality training
data, which allows us to train compact (e.g., run on a mobile device),
task-specific QA models that are competitive against GPT variants. The key
technological enabler is a novel mechanism for automatic question-answer
generation from procedural text which can ingest large amounts of textual
instructions and produce exhaustive in-domain QA training data. While current
QA data generation methods can produce well-formed and varied data, their
non-exhaustive nature is sub-optimal for training a QA model. In contrast, we
leverage the highly structured aspect of procedural text and represent each
step and the overall flow of the procedure as graphs. We then condition on
graph nodes to automatically generate QA pairs in an exhaustive and
controllable manner. Comprehensive evaluations of our method show that: 1)
small models trained with our data achieve excellent performance on the target
QA task, even exceeding that of GPT3 and ChatGPT despite being several orders
of magnitude smaller. 2) semantic coverage is the key indicator for downstream
QA performance. Crucially, while large language models excel at syntactic
diversity, this does not necessarily result in improvements on the end QA
model. In contrast, the higher semantic coverage provided by our method is
critical for QA performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13598">Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction. (arXiv:2401.13598v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaocui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1">Rong Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1">Soujanya Poria</a></p>
<p>Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in
information systems that aims to simultaneously extract entities with semantic
relations from a document. Existing methods heavily rely on a substantial
amount of fully labeled data. However, collecting and annotating data for newly
emerging relations is time-consuming and labor-intensive. Recent advanced Large
Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text
generation capabilities, inspiring us to explore an alternative approach for
obtaining auto-labeled documents with new relations. In this paper, we propose
a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework,
which generates labeled data by retrieval and denoising knowledge from LLMs,
called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide
ChatGPT to generate labeled long-text data step by step. To improve the quality
of synthetic data, we propose a denoising strategy based on the consistency of
cross-document knowledge. Leveraging our denoised synthetic data, we proceed to
fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.
We perform experiments for both zero-shot document-level relation and triplet
extraction on two public datasets. The experimental results illustrate that our
GenRDK framework outperforms strong baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13601">MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Duzhen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yahan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiahua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1">Dan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1">Chenhui Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>In the past year, MultiModal Large Language Models (MM-LLMs) have undergone
substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or
outputs via cost-effective training strategies. The resulting models not only
preserve the inherent reasoning and decision-making capabilities of LLMs but
also empower a diverse range of MM tasks. In this paper, we provide a
comprehensive survey aimed at facilitating further research of MM-LLMs.
Specifically, we first outline general design formulations for model
architecture and training pipeline. Subsequently, we provide brief
introductions of $26$ existing MM-LLMs, each characterized by its specific
formulations. Additionally, we review the performance of MM-LLMs on mainstream
benchmarks and summarize key training recipes to enhance the potency of
MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently
maintaining a real-time tracking website for the latest developments in the
field. We hope that this survey contributes to the ongoing advancement of the
MM-LLMs domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13621">DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning. (arXiv:2401.13621v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinghao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yunhua Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Contrastive-learning-based methods have dominated sentence representation
learning. These methods regularize the representation space by pulling similar
sentence representations closer and pushing away the dissimilar ones and have
been proven effective in various NLP tasks, e.g., semantic textual similarity
(STS) tasks. However, it is challenging for these methods to learn fine-grained
semantics as they only learn from the inter-sentence perspective, i.e., their
supervision signal comes from the relationship between data samples. In this
work, we propose a novel denoising objective that inherits from another
perspective, i.e., the intra-sentence perspective. By introducing both discrete
and continuous noise, we generate noisy sentences and then train our model to
restore them to their original form. Our empirical evaluations demonstrate that
this approach delivers competitive results on both semantic textual similarity
(STS) and a wide range of transfer tasks, standing up well in comparison to
contrastive-learning-based methods. Notably, the proposed intra-sentence
denoising objective complements existing inter-sentence contrastive
methodologies and can be integrated with them to further enhance performance.
Our code is available at https://github.com/xinghaow99/DenoSent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13649">VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1">Jing Yu Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_R/0/1/0/all/0/1">Robert Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1">Lawrence Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duvvur_V/0/1/0/all/0/1">Vikram Duvvur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1">Ming Chong Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Po-Yu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shuyan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1">Daniel Fried</a></p>
<p>Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13660">MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junxiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gangavarapu_T/0/1/0/all/0/1">Tushaar Gangavarapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jing Nathan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M Rush</a></p>
<p>Token-free language models learn directly from raw bytes and remove the bias
of subword tokenization. Operating on bytes, however, results in significantly
longer sequences, and standard autoregressive Transformers scale poorly in such
settings. We experiment with MambaByte, a token-free adaptation of the Mamba
state space model, trained autoregressively on byte sequences. Our experiments
indicate the computational efficiency of MambaByte compared to other byte-level
models. We also find MambaByte to be competitive with and even outperform
state-of-the-art subword Transformers. Furthermore, owing to linear scaling in
length, MambaByte benefits from fast inference compared to Transformers. Our
findings establish the viability of MambaByte in enabling token-free language
modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.03203">A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods. (arXiv:2112.03203v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dehao Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yingzhu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhongliang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongfeng Huang</a></p>
<p>In recent years, text summarization methods have attracted much attention
again thanks to the researches on neural network models. Most of the current
text summarization methods based on neural network models are supervised
methods which need large-scale datasets. However, large-scale datasets are
difficult to obtain in practical applications. In this paper, we model the task
of extractive text summarization methods from the perspective of Information
Theory, and then describe the unsupervised extractive methods with a uniform
framework. To improve the feature distribution and to decrease the mutual
information of summarization sentences, we propose a new sentence extraction
strategy which can be applied to existing unsupervised extractive methods.
Experiments are carried out on different datasets, and results show that our
strategy is indeed effective and in line with expectations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.12312">Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies. (arXiv:2202.12312v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhengxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1">Alex Tamkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1">Isabel Papadimitriou</a></p>
<p>When we transfer a pretrained language model to a new language, there are
many axes of variation that change at once. To disentangle the impact of
different factors like syntactic similarity and vocabulary similarity, we
propose a set of controlled transfer studies: we systematically transform the
language of the GLUE benchmark, altering one axis of crosslingual variation at
a time, and then measure the resulting drops in a pretrained model's downstream
performance. We find that models can largely recover from syntactic-style
shifts, but cannot recover from vocabulary misalignment and embedding matrix
re-initialization, even with continued pretraining on 15 million tokens. %On
the other hand, transferring to a dataset with an unaligned vocabulary is
extremely hard to recover from in the low-data regime. Moreover, good-quality
tokenizers in the transfer language do not make vocabulary alignment easier.
Our experiments provide insights into the factors of cross-lingual transfer
that researchers should most focus on when designing language transfer
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13716">ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation. (arXiv:2303.13716v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhengxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1">Christopher D. Manning</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a></p>
<p>Compositional generalization benchmarks for semantic parsing seek to assess
whether models can accurately compute meanings for novel sentences, but
operationalize this in terms of logical form (LF) prediction. This raises the
concern that semantically irrelevant details of the chosen LFs could shape
model performance. We argue that this concern is realized for the COGS
benchmark. COGS poses generalization splits that appear impossible for
present-day models, which could be taken as an indictment of those models.
However, we show that the negative results trace to incidental features of COGS
LFs. Converting these LFs to semantically equivalent ones and factoring out
capabilities unrelated to semantic interpretation, we find that even baseline
models get traction. A recent variable-free translation of COGS LFs suggests
similar conclusions, but we observe this format is not semantically equivalent;
it is incapable of accurately representing some COGS meanings. These findings
inform our proposal for ReCOGS, a modified version of COGS that comes closer to
assessing the target semantic capabilities while remaining very challenging.
Overall, our results reaffirm the importance of compositional generalization
and careful benchmark task design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08809">Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. (arXiv:2305.08809v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhengxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1">Atticus Geiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a></p>
<p>Obtaining human-interpretable explanations of large, general-purpose language
models is an urgent goal for AI safety. However, it is just as important that
our interpretability methods are faithful to the causal dynamics underlying
model behavior and able to robustly generalize to unseen inputs. Distributed
Alignment Search (DAS) is a powerful gradient descent method grounded in a
theory of causal abstraction that has uncovered perfect alignments between
interpretable symbolic algorithms and small deep learning models fine-tuned for
specific tasks. In the present paper, we scale DAS significantly by replacing
the remaining brute-force search steps with learned parameters -- an approach
we call Boundless DAS. This enables us to efficiently search for interpretable
causal structure in large language models while they follow instructions. We
apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf,
solves a simple numerical reasoning problem. With Boundless DAS, we discover
that Alpaca does this by implementing a causal model with two interpretable
boolean variables. Furthermore, we find that the alignment of neural
representations with these variables is robust to changes in inputs and
instructions. These findings mark a first step toward faithfully understanding
the inner-workings of our ever-growing and most widely deployed language
models. Our tool is extensible to larger LLMs and is released publicly at
`https://github.com/stanfordnlp/pyvene`.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08845">Large Language Models are Zero-Shot Rankers for Recommender Systems. (arXiv:2305.08845v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yupeng Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zihan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Ruobing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1">Julian McAuley</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a></p>
<p>Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated
impressive general-purpose task-solving abilities, including the potential to
approach recommendation tasks. Along this line of research, this work aims to
investigate the capacity of LLMs that act as the ranking model for recommender
systems. We first formalize the recommendation problem as a conditional ranking
task, considering sequential interaction histories as conditions and the items
retrieved by other candidate generation models as candidates. To solve the
ranking task by LLMs, we carefully design the prompting template and conduct
extensive experiments on two widely-used datasets. We show that LLMs have
promising zero-shot ranking abilities but (1) struggle to perceive the order of
historical interactions, and (2) can be biased by popularity or item positions
in the prompts. We demonstrate that these issues can be alleviated using
specially designed prompting and bootstrapping strategies. Equipped with these
insights, zero-shot LLMs can even challenge conventional recommendation models
when ranking candidates are retrieved by multiple candidate generators. The
code and processed datasets are available at
https://github.com/RUCAIBox/LLMRank.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02272">OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models. (arXiv:2306.02272v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Changhun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jungyu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taesu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyungjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1">Eunhyeok Park</a></p>
<p>Large language models (LLMs) with hundreds of billions of parameters require
powerful server-grade GPUs for inference, limiting their practical deployment.
To address this challenge, we introduce the outlier-aware weight quantization
(OWQ) method, which aims to minimize LLM's footprint through low-precision
representation. OWQ prioritizes a small subset of structured weights sensitive
to quantization, storing them in high-precision, while applying highly tuned
quantization to the remaining dense weights. This sensitivity-aware
mixed-precision scheme reduces the quantization error notably, and extensive
experiments demonstrate that 3.1-bit models using OWQ perform comparably to
4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a
parameter-efficient fine-tuning for task-specific adaptation, called weak
column tuning (WCT), enabling accurate task-specific LLM adaptation with
minimal memory overhead in the optimized format. OWQ represents a notable
advancement in the flexibility, efficiency, and practicality of LLM
optimization literature. The source code is available at
https://github.com/xvyaward/owq
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03268">&quot;Medium&quot; LMs of Code in the Era of LLMs: Lessons From StackOverflow. (arXiv:2306.03268v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_M/0/1/0/all/0/1">Manisha Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1">Vincent J. Hellendoorn</a></p>
<p>Large pre-trained neural language models have brought immense progress to
both NLP and software engineering. Models in OpenAI's GPT series now dwarf
Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide
range of NLP applications. These models are trained on massive corpora of
heterogeneous data from web crawls, which enables them to learn general
language patterns and semantic relationships. However, the largest models are
both expensive to train and deploy and are often closed-source, so we lack
access to their data and design decisions. We argue that this trend towards
large, general-purpose models should be complemented with single-purpose, more
modestly sized pre-trained models. In this work, we take StackOverflow (SO) as
a domain example in which large volumes of rich aligned code and text data is
available. We adopt standard practices for pre-training large language models,
including using a very large context size (2,048 tokens), batch size (0.5M
tokens) and training set (27B tokens), coupled with a powerful toolkit
(Megatron-LM), to train two models: SOBertBase, with 109M parameters, and
SOBertLarge with 762M parameters, at a budget of just $\$187$ and $\$800$ each.
We compare the performance of our models with both the previous SOTA model
trained on SO data exclusively as well general-purpose BERT models and OpenAI's
ChatGPT on four SO-specific downstream tasks - question quality prediction,
closed question prediction, named entity recognition and obsoletion prediction
(a new task we introduce). Not only do our models consistently outperform all
baselines, the smaller model is often sufficient for strong results. Both
models are released to the public. These results demonstrate that pre-training
both extensively and properly on in-domain data can yield a powerful and
affordable alternative to leveraging closed-source general-purpose models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08877">Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. (arXiv:2306.08877v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rassin_R/0/1/0/all/0/1">Royi Rassin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1">Eran Hirsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Glickman_D/0/1/0/all/0/1">Daniel Glickman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1">Shauli Ravfogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a></p>
<p>Text-conditioned image generation models often generate incorrect
associations between entities and their visual attributes. This reflects an
impaired mapping between linguistic binding of entities and modifiers in the
prompt and visual binding of the corresponding elements in the generated image.
As one notable example, a query like "a pink sunflower and a yellow flamingo"
may incorrectly produce an image of a yellow sunflower and a pink flamingo. To
remedy this issue, we propose SynGen, an approach which first syntactically
analyses the prompt to identify entities and their modifiers, and then uses a
novel loss function that encourages the cross-attention maps to agree with the
linguistic binding reflected by the syntax. Specifically, we encourage large
overlap between attention maps of entities and their modifiers, and small
overlap with other entities and modifier words. The loss is optimized during
inference, without retraining or fine-tuning the model. Human evaluation on
three datasets, including one new and challenging set, demonstrate significant
improvements of SynGen compared with current state of the art methods. This
work highlights how making use of sentence structure during inference can
efficiently and substantially improve the faithfulness of text-to-image
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06082">VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1">Raphael Schumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wanrong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Weixi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1">Tsu-Jui Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1">Stefan Riezler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12539">CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vipul Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1">Pranav Narayanan Venkit</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1">Hugo Lauren&#xe7;on</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1">Shomir Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1">Rebecca J. Passonneau</a></p>
<p>As language models (LMs) become increasingly powerful and widely used, it is
important to quantify them for sociodemographic bias with potential for harm.
Prior measures of bias are sensitive to perturbations in the templates designed
to compare performance across social groups, due to factors such as low
diversity or limited number of templates. Also, most previous work considers
only one NLP task. We introduce Comprehensive Assessment of Language Models
(CALM) for robust measurement of two types of universally relevant
sociodemographic bias, gender and race. CALM integrates sixteen datasets for
question-answering, sentiment analysis and natural language inference. Examples
from each dataset are filtered to produce 224 templates with high diversity
(e.g., length, vocabulary). We assemble 50 highly frequent person names for
each of seven distinct demographic groups to generate 78,400 prompts covering
the three NLP tasks. Our empirical evaluation shows that CALM bias scores are
more robust and far less sensitive than previous bias measurements to
perturbations in the templates, such as synonym substitution, or to random
subset selection of templates. We apply CALM to 20 large language models, and
find that for 2 language model series, larger parameter models tend to be more
biased than smaller ones. The T0 series is the least biased model families, of
the 20 LLMs investigated here. The code is available at
https://github.com/vipulgupta1011/CALM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15812">Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1">John Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem wherein the preferences inferred from
ratings and rankings significantly disagree 60% for both human and AI
annotators. Our subsequent analysis identifies various facets of annotator
biases that explain this phenomena, such as human annotators would rate denser
responses higher while preferring accuracy during pairwise judgments. To our
surprise, we also observe that the choice of feedback protocol also has a
significant effect on the evaluation of aligned LLMs. In particular, we find
that LLMs that leverage rankings data for alignment (say model X) are preferred
over those that leverage ratings data (say model Y), with a rank-based
evaluation protocol (is X/Y's response better than reference response?) but not
with a rating-based evaluation protocol (score Rank X/Y's response on a scale
of 1-7). Our findings thus shed light on critical gaps in methods for
evaluating the real-world utility of language models and their strong
dependence on the feedback protocol used for alignment. Our code and data are
available at https://github.com/Hritikbansal/sparse_feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06657">Statistical Rejection Sampling Improves Preference Optimization. (arXiv:2309.06657v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1">Rishabh Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalman_M/0/1/0/all/0/1">Misha Khalman</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1">Mohammad Saleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jialu Liu</a></p>
<p>Improving the alignment of language models with human preferences remains an
active research challenge. Previous approaches have primarily utilized
Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as
Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence
Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have
emerged as attractive alternatives, offering improvements in stability and
scalability while maintaining competitive performance. SLiC refines its loss
function using sequence pairs sampled from a supervised fine-tuned (SFT)
policy, while DPO directly optimizes language models based on preference data,
foregoing the need for a separate reward model. However, the maximum likelihood
estimator (MLE) of the target optimal policy requires labeled preference pairs
sampled from that policy. DPO's lack of a reward model constrains its ability
to sample preference pairs from the optimal policy, and SLiC is restricted to
sampling preference pairs only from the SFT policy. To address these
limitations, we introduce a novel approach called Statistical Rejection
Sampling Optimization (RSO) that aims to source preference data from the target
optimal policy using rejection sampling, enabling a more accurate estimation of
the optimal policy. We also propose a unified framework that enhances the loss
functions used in both SLiC and DPO from a preference modeling standpoint.
Through extensive experiments across three diverse tasks, we demonstrate that
RSO consistently outperforms both SLiC and DPO on evaluations from both Large
Language Model (LLM) and human raters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07414">PromptASR for contextualized ASR with controllable style. (arXiv:2309.07414v3 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1">Xiaoyu Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1">Wei Kang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1">Zengwei Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1">Liyong Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1">Fangjun Kuang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1">Long Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1">Daniel Povey</a></p>
<p>Prompts are crucial to large language models as they provide context
information such as topic or logical relationships. Inspired by this, we
propose PromptASR, a framework that integrates prompts in end-to-end automatic
speech recognition (E2E ASR) systems to achieve contextualized ASR with
controllable style of transcriptions. Specifically, a dedicated text encoder
encodes the text prompts and the encodings are injected into the speech encoder
by cross-attending the features from two modalities. When using the ground
truth text from preceding utterances as content prompt, the proposed system
achieves 21.9% and 6.8% relative word error rate reductions on a book reading
dataset and an in-house dataset compared to a baseline ASR system. The system
can also take word-level biasing lists as prompt to improve recognition
accuracy on rare words. An additional style prompt can be given to the text
encoder and guide the ASR system to output different styles of transcriptions.
The code is available at icefall.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08347">Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiuzhou Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1">Wray Buntine</a>, <a href="http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1">Ehsan Shareghi</a></p>
<p>Semi-structured explanation depicts the implicit process of a reasoner with
an explicit representation. This explanation highlights how available
information in a specific query is utilised and supplemented with information a
reasoner produces from its internal weights towards generating an answer.
Despite the recent improvements in generative capabilities of language models,
producing structured explanations to verify a model's true reasoning
capabilities remains a challenge. This issue is particularly pronounced for
not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the
limitations of supervised fine-tuning (SFT) in tackling this challenge, and
then introduce a carefully crafted reward engineering method in reinforcement
learning (RL) to better address this problem. We investigate multiple reward
aggregation methods and provide a detailed discussion which sheds light on the
promising potential of RL for future research. Our proposed method on two
semi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE)
achieves new state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08565">How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Danni Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1">Jan Niehues</a></p>
<p>Customizing machine translation models to comply with desired attributes
(e.g., formality or grammatical gender) is a well-studied topic. However, most
current approaches rely on (semi-)supervised data with attribute annotations.
This data scarcity bottlenecks democratizing such customization possibilities
to a wider range of languages, particularly lower-resource ones. This gap is
out of sync with recent progress in pretrained massively multilingual
translation models. In response, we transfer the attribute controlling
capabilities to languages without attribute-annotated data with an NLLB-200
model as a foundation. Inspired by techniques from controllable generation, we
employ a gradient-based inference-time controller to steer the pretrained
model. The controller transfers well to zero-shot conditions, as it operates on
pretrained multilingual representations and is attribute -- rather than
language-specific. With a comprehensive comparison to finetuning-based control,
we demonstrate that, despite finetuning's clear dominance in supervised
settings, the gap to inference-time control closes when moving to zero-shot
conditions, especially with new and distant target languages. The latter also
shows stronger domain robustness. We further show that our inference-time
control complements finetuning. A human evaluation on a real low-resource
language, Bengali, confirms our findings. Our code is
https://github.com/dannigt/attribute-controller-transfer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17249">Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xingchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Proleev_L/0/1/0/all/0/1">Lev Proleev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mincu_D/0/1/0/all/0/1">Diana Mincu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1">Katherine Heller</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Subhrajit Roy</a></p>
<p>Prompting and in-context learning (ICL) have become efficient learning
paradigms for large language models (LLMs). However, LLMs suffer from prompt
brittleness and various bias factors in the prompt, including but not limited
to the formatting, the choice verbalizers, and the ICL examples. To address
this problem that results in unexpected performance degradation, calibration
methods have been developed to mitigate the effects of these biases while
recovering LLM performance. In this work, we first conduct a systematic
analysis of the existing calibration methods, where we both provide a unified
view and reveal the failure cases. Inspired by these analyses, we propose Batch
Calibration (BC), a simple yet intuitive method that controls the contextual
bias from the batched input, unifies various prior approaches, and effectively
addresses the aforementioned issues. BC is zero-shot, inference-only, and
incurs negligible additional costs. In the few-shot setup, we further extend BC
to allow it to learn the contextual bias from labeled data. We validate the
effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate
state-of-the-art performance over previous calibration baselines across more
than 10 natural language understanding and image classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01749">Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1">Brian DuSell</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1">David Chiang</a></p>
<p>Attention, specifically scaled dot-product attention, has proven effective
for natural language, but it does not have a mechanism for handling
hierarchical patterns of arbitrary nesting depth, which limits its ability to
recognize certain syntactic structures. To address this shortcoming, we propose
stack attention: an attention operator that incorporates stacks, inspired by
their theoretical connections to context-free languages (CFLs). We show that
stack attention is analogous to standard attention, but with a latent model of
syntax that requires no syntactic supervision. We propose two variants: one
related to deterministic pushdown automata (PDAs) and one based on
nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.
We show that transformers with stack attention are very effective at learning
CFLs that standard transformers struggle on, achieving strong results on a CFL
with theoretically maximal parsing difficulty. We also show that stack
attention is more effective at natural language modeling under a constrained
parameter budget, and we include results on machine translation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02374">Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbasian_M/0/1/0/all/0/1">Mahyar Abbasian</a>, <a href="http://arxiv.org/find/cs/1/au:+Azimi_I/0/1/0/all/0/1">Iman Azimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1">Amir M. Rahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Ramesh Jain</a></p>
<p>Conversational Health Agents (CHAs) are interactive systems that provide
healthcare services, such as assistance and diagnosis. Current CHAs, especially
those utilizing Large Language Models (LLMs), primarily focus on conversation
aspects. However, they offer limited agent capabilities, specifically lacking
multi-step problem-solving, personalized conversations, and multimodal data
analysis. Our aim is to overcome these limitations. We propose openCHA, an
open-source LLM-powered framework, to empower conversational agents to generate
a personalized response for users' healthcare queries. This framework enables
developers to integrate external sources including data sources, knowledge
bases, and analysis models, into their LLM-based solutions. openCHA includes an
orchestrator to plan and execute actions for gathering information from
external sources, essential for formulating responses to user inquiries. It
facilitates knowledge acquisition, problem-solving capabilities, multilingual
and multimodal conversations, and fosters interaction with various AI
platforms. We illustrate the framework's proficiency in handling complex
healthcare tasks via three demonstrations. Moreover, we release openCHA as open
source available to the community via GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08535">Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Crouse_M/0/1/0/all/0/1">Maxwell Crouse</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1">Ibrahim Abdelaziz</a>, <a href="http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1">Ramon Astudillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_K/0/1/0/all/0/1">Kinjal Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1">Soham Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumaravel_S/0/1/0/all/0/1">Sadhana Kumaravel</a>, <a href="http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1">Achille Fokoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1">Pavan Kapanipathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1">Salim Roukos</a>, <a href="http://arxiv.org/find/cs/1/au:+Lastras_L/0/1/0/all/0/1">Luis Lastras</a></p>
<p>Autonomous, goal-driven agents powered by LLMs have recently emerged as
promising tools for solving challenging problems without the need for
task-specific finetuned models that can be expensive to procure. Currently, the
design and implementation of such agents is ad hoc, as the wide variety of
tasks that LLM-based agents may be applied to naturally means there can be no
one-size-fits-all approach to agent design. In this work we aim to alleviate
the difficulty of designing and implementing new agents by proposing a
minimalistic generation framework that simplifies the process of building
agents. The framework we introduce allows the user to define desired agent
behaviors in a high-level, declarative specification that is then used to
construct a decoding monitor which guarantees the LLM will produce an output
exhibiting the desired behavior. Our declarative approach, in which the
behavior is described without concern for how it should be implemented or
enforced, enables rapid design, implementation, and experimentation with
different LLM-based agents. We demonstrate how the proposed framework can be
used to implement recent LLM-based agents (e.g., ReACT), and show how the
flexibility of our approach can be leveraged to define a new agent with more
complex behavior, the Plan-Act-Summarize-Solve (PASS) agent. Lastly, we
demonstrate that our method outperforms other agents on multiple popular
reasoning-centric question-answering benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Benjamin Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anthony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLMs), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM's. These reward models are additionally used at
inference-time to estimate LLM responses' adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09084">Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nazeer_K/0/1/0/all/0/1">Khaleelulla Khan Nazeer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schone_M/0/1/0/all/0/1">Mark Sch&#xf6;ne</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherji_R/0/1/0/all/0/1">Rishav Mukherji</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogginger_B/0/1/0/all/0/1">Bernhard Vogginger</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayr_C/0/1/0/all/0/1">Christian Mayr</a>, <a href="http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1">David Kappel</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramoney_A/0/1/0/all/0/1">Anand Subramoney</a></p>
<p>As large language models continue to scale in size rapidly, so too does the
computational power required to run them. Event-based networks on neuromorphic
devices offer a potential way to reduce energy consumption for inference
significantly. However, to date, most event-based networks that can run on
neuromorphic hardware, including spiking neural networks (SNNs), have not
achieved task performance even on par with LSTM models for language modeling.
As a result, language modeling on neuromorphic devices has seemed a distant
prospect. In this work, we demonstrate the first-ever implementation of a
language model on a neuromorphic device - specifically the SpiNNaker 2 chip -
based on a recently published event-based architecture called the EGRU.
SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale
asynchronous processing, while the EGRU is architected to leverage such
hardware efficiently while maintaining competitive task performance. This
implementation marks the first time a neuromorphic language model matches
LSTMs, setting the stage for taking task performance to the level of large
language models. We also demonstrate results on a gesture recognition task
based on inputs from a DVS camera. Overall, our results showcase the
feasibility of this neuro-inspired neural network in hardware, highlighting
significant gains versus conventional hardware in energy efficiency for the
common use case of single batch inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11803">NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs. (arXiv:2312.11803v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1">Maria Antoniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1">Aakanksha Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarado_C/0/1/0/all/0/1">Carla S. Alvarado</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lucy Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1">Irene Y. Chen</a></p>
<p>Ethical frameworks for the use of natural language processing (NLP) are
urgently needed to shape how large language models (LLMs) and similar tools are
used for healthcare applications. Healthcare faces existing challenges
including the balance of power in clinician-patient relationships, systemic
health disparities, historical injustices, and economic constraints. Drawing
directly from the voices of those most affected, and focusing on a case study
of a specific healthcare setting, we propose a set of guiding principles for
the use of NLP in maternal healthcare. We led an interactive session centered
on an LLM-based chatbot demonstration during a full-day workshop with 39
participants, and additionally surveyed 30 healthcare workers and 30 birthing
people about their values, needs, and perceptions of NLP tools in the context
of maternal health. We conducted quantitative and qualitative analyses of the
survey results and interactive discussions to consolidate our findings into a
set of guiding principles. We propose nine principles for ethical use of NLP
for maternal healthcare, grouped into three themes: (i) recognizing contextual
significance (ii) holistic measurements, and (iii) who/what is valued. For each
principle, we describe its underlying rationale and provide practical advice.
This set of principles can provide a methodological pattern for other
researchers and serve as a resource to practitioners working on maternal health
and other healthcare fields to emphasize the importance of technical nuance,
historical context, and inclusive design when developing NLP technologies for
clinical use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02981">Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1">Cheonsu Jeong</a></p>
<p>Recent releases of pre-trained Large Language Models (LLMs) have gained
considerable traction, yet research on fine-tuning and employing
domain-specific LLMs remains scarce. This study investigates approaches for
fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,
foundational models, and methods for domain-specific pre-training. Focusing on
the financial sector, it details dataset selection, preprocessing, model
choice, and considerations crucial for LLM fine-tuning in finance. Addressing
the unique characteristics of financial data, the study explores the
construction of domain-specific vocabularies and considerations for security
and regulatory compliance. In the practical application of LLM fine-tuning, the
study outlines the procedure and implementation for generating domain-specific
LLMs in finance. Various financial cases, including stock price prediction,
sentiment analysis of financial news, automated document processing, research,
information extraction, and customer service enhancement, are exemplified. The
study explores the potential of LLMs in the financial domain, identifies
limitations, and proposes directions for improvement, contributing valuable
insights for future research. Ultimately, it advances natural language
processing technology in business, suggesting proactive LLM utilization in
financial services across industries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06373">How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hongpeng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weiyan Shi</a></p>
<p>Most traditional AI safety research has approached AI models as machines and
centered on algorithm-focused attacks developed by security experts. As large
language models (LLMs) become increasingly common and competent, non-expert
users can also impose risks during daily interactions. This paper introduces a
new perspective to jailbreak LLMs as human-like communicators, to explore this
overlooked intersection between everyday language interaction and AI safety.
Specifically, we study how to persuade LLMs to jailbreak them. First, we
propose a persuasion taxonomy derived from decades of social science research.
Then, we apply the taxonomy to automatically generate interpretable persuasive
adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion
significantly increases the jailbreak performance across all risk categories:
PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b
Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused
attacks. On the defense side, we explore various mechanisms against PAP and,
found a significant gap in existing defenses, and advocate for more fundamental
mitigation for highly interactive LLMs
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06461">Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuling Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1">Chengcheng Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiaodong Gu</a></p>
<p>Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine-and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine and human-authored code.
Through a rigorous analysis of code attributes such as length, lexical
diversity, and naturalness, we expose unique pat-terns inherent to each source.
We particularly notice that the structural segmentation of code is a critical
factor in identifying its provenance. Based on our findings, we propose a novel
machine-generated code detection method called DetectCodeGPT, which improves
DetectGPT by capturing the distinct structural patterns of code. Diverging from
conventional techniques that depend on external LLMs for perturbations,
DetectCodeGPT perturbs the code corpus by strategically inserting spaces and
newlines, ensuring both efficacy and efficiency. Experiment results show that
our approach significantly outperforms state-of-the-art techniques in detecting
machine-generated code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08396">Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fangyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiliang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Justin M. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Robert Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1">Ronald M. Summers</a>, <a href="http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1">Justin F. Rousseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_P/0/1/0/all/0/1">Peiyun Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Landsman_M/0/1/0/all/0/1">Marc J Landsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Baxter_S/0/1/0/all/0/1">Sally L. Baxter</a>, <a href="http://arxiv.org/find/cs/1/au:+AlAref_S/0/1/0/all/0/1">Subhi J. Al&#x27;Aref</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1">Michael F. Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yifan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>Recent studies indicate that Generative Pre-trained Transformer 4 with Vision
(GPT-4V) outperforms human physicians in medical challenge tasks. However,
these evaluations primarily focused on the accuracy of multi-choice questions
alone. Our study extends the current scope by conducting a comprehensive
analysis of GPT-4V's rationales of image comprehension, recall of medical
knowledge, and step-by-step multimodal reasoning when solving New England
Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test
the knowledge and diagnostic capabilities of medical professionals. Evaluation
results confirmed that GPT-4V outperforms human physicians regarding
multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in
cases where physicians incorrectly answer, with over 80% accuracy. However, we
discovered that GPT-4V frequently presents flawed rationales in cases where it
makes the correct final choices (27.3%), most prominent in image comprehension
(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our
findings emphasize the necessity for further in-depth evaluations of its
rationales before integrating such models into clinical workflows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08517">Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abu_Rasheed_H/0/1/0/all/0/1">Hasan Abu-Rasheed</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulsalam_M/0/1/0/all/0/1">Mohamad Hussam Abdulsalam</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1">Christian Weber</a>, <a href="http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1">Madjid Fathi</a></p>
<p>Student commitment towards a learning recommendation is not separable from
their understanding of the reasons it was recommended to them; and their
ability to modify it based on that understanding. Among explainability
approaches, chatbots offer the potential to engage the student in a
conversation, similar to a discussion with a peer or a mentor. The capabilities
of chatbots, however, are still not sufficient to replace a human mentor,
despite the advancements of generative AI (GenAI) and large language models
(LLM). Therefore, we propose an approach to utilize chatbots as mediators of
the conversation and sources of limited and controlled generation of
explanations, to harvest the potential of LLMs while reducing their potential
risks at the same time. The proposed LLM-based chatbot supports students in
understanding learning-paths recommendations. We use a knowledge graph (KG) as
a human-curated source of information, to regulate the LLM's output through
defining its prompt's context. A group chat approach is developed to connect
students with human mentors, either on demand or in cases that exceed the
chatbot's pre-defined tasks. We evaluate the chatbot with a user study, to
provide a proof-of-concept and highlight the potential requirements and
limitations of utilizing chatbots in conversational explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10841">Using LLMs to discover emerging coded antisemitic hate-speech in extremist social media. (arXiv:2401.10841v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kikkisetti_D/0/1/0/all/0/1">Dhanush Kikkisetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Mustafa_R/0/1/0/all/0/1">Raza Ul Mustafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Melillo_W/0/1/0/all/0/1">Wendy Melillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1">Roberto Corizzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Boukouvalas_Z/0/1/0/all/0/1">Zois Boukouvalas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gill_J/0/1/0/all/0/1">Jeff Gill</a>, <a href="http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1">Nathalie Japkowicz</a></p>
<p>Online hate speech proliferation has created a difficult problem for social
media platforms. A particular challenge relates to the use of coded language by
groups interested in both creating a sense of belonging for its users and
evading detection. Coded language evolves quickly and its use varies over time.
This paper proposes a methodology for detecting emerging coded hate-laden
terminology. The methodology is tested in the context of online antisemitic
discourse. The approach considers posts scraped from social media platforms,
often used by extremist users. The posts are scraped using seed expressions
related to previously known discourse of hatred towards Jews. The method begins
by identifying the expressions most representative of each post and calculating
their frequency in the whole corpus. It filters out grammatically incoherent
expressions as well as previously encountered ones so as to focus on emergent
well-formed terminology. This is followed by an assessment of semantic
similarity to known antisemitic terminology using a fine-tuned large language
model, and subsequent filtering out of the expressions that are too distant
from known expressions of hatred. Emergent antisemitic expressions containing
terms clearly relating to Jewish topics are then removed to return only coded
expressions of hatred.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11120">Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1">David Oniani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xizhi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Visweswaran_S/0/1/0/all/0/1">Shyam Visweswaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1">Sumit Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Kooragayalu_S/0/1/0/all/0/1">Shravan Kooragayalu</a>, <a href="http://arxiv.org/find/cs/1/au:+Polanska_K/0/1/0/all/0/1">Katelyn Polanska</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanshan Wang</a></p>
<p>Background Large Language Models (LLMs), enhanced with Clinical Practice
Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS).
However, methods for incorporating CPGs into LLMs are not well studied. Methods
We develop three distinct methods for incorporating CPGs into LLMs: Binary
Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and
Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of
the proposed methods, we create a set of synthetic patient descriptions and
conduct both automatic and human evaluation of the responses generated by four
LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was
used as the baseline method. We focus on CDS for COVID-19 outpatient treatment
as the case study. Results All four LLMs exhibit improved performance when
enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP
and PAGC in automatic evaluation. All of the proposed methods demonstrated high
performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate
superior performance, as compared to plain LLMs with ZSP, in providing accurate
recommendations for COVID-19 outpatient treatment, which also highlights the
potential for broader applications beyond the case study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12143">Anisotropy Is Inherent to Self-Attention in Transformers. (arXiv:2401.12143v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Godey_N/0/1/0/all/0/1">Nathan Godey</a>, <a href="http://arxiv.org/find/cs/1/au:+Clergerie_E/0/1/0/all/0/1">&#xc9;ric de la Clergerie</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1">Beno&#xee;t Sagot</a></p>
<p>The representation degeneration problem is a phenomenon that is widely
observed among self-supervised learning methods based on Transformers. In NLP,
it takes the form of anisotropy, a singular property of hidden representations
which makes them unexpectedly close to each other in terms of angular distance
(cosine-similarity). Some recent works tend to show that anisotropy is a
consequence of optimizing the cross-entropy loss on long-tailed distributions
of tokens. We show in this paper that anisotropy can also be observed
empirically in language models with specific objectives that should not suffer
directly from the same consequences. We also show that the anisotropy problem
extends to Transformers trained on other modalities. Our observations suggest
that anisotropy is actually inherent to Transformers-based models.
</p>
</p>
</div>

    </div>
    </body>
    