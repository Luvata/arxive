<!DOCTYPE html>
<html>
<head>
<title>2023-12-15-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.07549">Make-A-Storyboard: A General Framework for Storyboard with Disentangled and Merged Control. (arXiv:2312.07549v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Sitong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Litao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lianli Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a></p>
<p>Story Visualization aims to generate images aligned with story prompts,
reflecting the coherence of storybooks through visual consistency among
characters and scenes.Whereas current approaches exclusively concentrate on
characters and neglect the visual consistency among contextually correlated
scenes, resulting in independent character images without inter-image
coherence.To tackle this issue, we propose a new presentation form for Story
Visualization called Storyboard, inspired by film-making, as illustrated in
Fig.1.Specifically, a Storyboard unfolds a story into visual representations
scene by scene. Within each scene in Storyboard, characters engage in
activities at the same location, necessitating both visually consistent scenes
and characters.For Storyboard, we design a general framework coined as
Make-A-Storyboard that applies disentangled control over the consistency of
contextual correlated characters and scenes and then merge them to form
harmonized images.Extensive experiments demonstrate 1) Effectiveness.the
effectiveness of the method in story alignment, character consistency, and
scene correlation; 2) Generalization. Our method could be seamlessly integrated
into mainstream Image Customization methods, empowering them with the
capability of story visualization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07550">Understanding (Un)Intended Memorization in Text-to-Image Generative Models. (arXiv:2312.07550v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1">Ali Naseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1">Jaechul Roh</a>, <a href="http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1">Amir Houmansadr</a></p>
<p>Multimodal machine learning, especially text-to-image models like Stable
Diffusion and DALL-E 3, has gained significance for transforming text into
detailed images.
</p>
<p>Despite their growing use and remarkable generative capabilities, there is a
pressing need for a detailed examination of these models' behavior,
particularly with respect to memorization. Historically, memorization in
machine learning has been context-dependent, with diverse definitions emerging
from classification tasks to complex models like Large Language Models (LLMs)
and Diffusion models. Yet, a definitive concept of memorization that aligns
with the intricacies of text-to-image synthesis remains elusive. This
understanding is vital as memorization poses privacy risks yet is essential for
meeting user expectations, especially when generating representations of
underrepresented entities. In this paper, we introduce a specialized definition
of memorization tailored to text-to-image models, categorizing it into three
distinct types according to user expectations. We closely examine the subtle
distinctions between intended and unintended memorization, emphasizing the
importance of balancing user privacy with the generative quality of the model
outputs. Using the Stable Diffusion model, we offer examples to validate our
memorization definitions and clarify their application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07560">AI-driven Structure Detection and Information Extraction from Historical Cadastral Maps (Early 19th Century Franciscean Cadastre in the Province of Styria) and Current High-resolution Satellite and Aerial Imagery for Remote Sensing. (arXiv:2312.07560v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goderle_W/0/1/0/all/0/1">Wolfgang G&#xf6;derle</a>, <a href="http://arxiv.org/find/cs/1/au:+Macher_C/0/1/0/all/0/1">Christian Macher</a>, <a href="http://arxiv.org/find/cs/1/au:+Mauthner_K/0/1/0/all/0/1">Katrin Mauthner</a>, <a href="http://arxiv.org/find/cs/1/au:+Pimas_O/0/1/0/all/0/1">Oliver Pimas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampetsreiter_F/0/1/0/all/0/1">Fabian Rampetsreiter</a></p>
<p>Cadastres from the 19th century are a complex as well as rich source for
historians and archaeologists, whose use presents them with great challenges.
For archaeological and historical remote sensing, we have trained several Deep
Learning models, CNNs as well as Vision Transformers, to extract large-scale
data from this knowledge representation. We present the principle results of
our work here and we present a the demonstrator of our browser-based tool that
allows researchers and public stakeholders to quickly identify spots that
featured buildings in the 19th century Franciscean Cadastre. The tool not only
supports scholars and fellow researchers in building a better understanding of
the settlement history of the region of Styria, it also helps public
administration and fellow citizens to swiftly identify areas of heightened
sensibility with regard to the cultural heritage of the region.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07561">Annotating sleep states in children from wrist-worn accelerometer data using Machine Learning. (arXiv:2312.07561v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ram_A/0/1/0/all/0/1">Ashwin Ram</a>, <a href="http://arxiv.org/find/eess/1/au:+S%2E_S/0/1/0/all/0/1">Sundar Sripada V. S.</a>, <a href="http://arxiv.org/find/eess/1/au:+Keshari_S/0/1/0/all/0/1">Shuvam Keshari</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1">Zizhe Jiang</a></p>
<p>Sleep detection and annotation are crucial for researchers to understand
sleep patterns, especially in children. With modern wrist-worn watches
comprising built-in accelerometers, sleep logs can be collected. However, the
annotation of these logs into distinct sleep events: onset and wakeup, proves
to be challenging. These annotations must be automated, precise, and scalable.
We propose to model the accelerometer data using different machine learning
(ML) techniques such as support vectors, boosting, ensemble methods, and more
complex approaches involving LSTMs and Region-based CNNs. Later, we aim to
evaluate these approaches using the Event Detection Average Precision (EDAP)
score (similar to the IOU metric) to eventually compare the predictive power
and model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07571">Investigating YOLO Models Towards Outdoor Obstacle Detection For Visually Impaired People. (arXiv:2312.07571v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Chenhao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1">Pramit Saha</a></p>
<p>The utilization of deep learning-based object detection is an effective
approach to assist visually impaired individuals in avoiding obstacles. In this
paper, we implemented seven different YOLO object detection models
\textit{viz}., YOLO-NAS (small, medium, large), YOLOv8, YOLOv7, YOLOv6, and
YOLOv5 and performed comprehensive evaluation with carefully tuned
hyperparameters, to analyze how these models performed on images containing
common daily-life objects presented on roads and sidewalks. After a systematic
investigation, YOLOv8 was found to be the best model, which reached a precision
of $80\%$ and a recall of $68.2\%$ on a well-known Obstacle Dataset which
includes images from VOC dataset, COCO dataset, and TT100K dataset along with
images collected by the researchers in the field. Despite being the latest
model and demonstrating better performance in many other applications, YOLO-NAS
was found to be suboptimal for the obstacle detection task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07580">COVID-19 Detection Using Slices Processing Techniques and a Modified Xception Classifier from Computed Tomography Images. (arXiv:2312.07580v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1">Kenan Morani</a></p>
<p>This paper extends our previous method for COVID-19 diagnosis, proposing an
enhanced solution for detecting COVID-19 from computed tomography (CT) images.
To decrease model misclassifications, two key steps of image processing were
employed. Firstly, the uppermost and lowermost slices were removed, preserving
sixty percent of each patient's slices. Secondly, all slices underwent manual
cropping to emphasize the lung areas. Subsequently, resized CT scans (224 by
224) were input into an Xception transfer learning model. Leveraging Xception's
architecture and pre-trained weights, the modified model achieved binary
classification. Promising results on the COV19-CT database showcased higher
validation accuracy and macro F1 score at both the slice and patient levels
compared to our previous solution and alternatives on the same dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07584">DFGET: Displacement-Field Assisted Graph Energy Transmitter for Gland Instance Segmentation. (arXiv:2312.07584v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jian_C/0/1/0/all/0/1">Caiqing Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yongbin Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lihui Wang</a></p>
<p>Gland instance segmentation is an essential but challenging task in the
diagnosis and treatment of adenocarcinoma. The existing models usually achieve
gland instance segmentation through multi-task learning and boundary loss
constraint. However, how to deal with the problems of gland adhesion and
inaccurate boundary in segmenting the complex samples remains a challenge. In
this work, we propose a displacement-field assisted graph energy transmitter
(DFGET) framework to solve these problems. Specifically, a novel message
passing manner based on anisotropic diffusion is developed to update the node
features, which can distinguish the isomorphic graphs and improve the
expressivity of graph nodes for complex samples. Using such graph framework,
the gland semantic segmentation map and the displacement field (DF) of the
graph nodes are estimated with two graph network branches. With the constraint
of DF, a graph cluster module based on diffusion theory is presented to improve
the intra-class feature consistency and inter-class feature discrepancy, as
well as to separate the adherent glands from the semantic segmentation maps.
Extensive comparison and ablation experiments on the GlaS dataset demonstrate
the superiority of DFGET and effectiveness of the proposed anisotropic message
passing manner and clustering method. Compared to the best comparative model,
DFGET increases the object-Dice and object-F1 score by 2.5% and 3.4%
respectively, while decreases the object-HD by 32.4%, achieving
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07586">Characteristic Guidance: Non-linear Correction for DDPM at Large Guidance Scale. (arXiv:2312.07586v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Candi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yuan Lan</a></p>
<p>Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a novel method that provides non-linear correction for
classifier-free guided DDPMs. Such correction forces the guided DDPMs to
respect the Fokker-Planck equation of their underlying diffusion process, in a
way that is first-principle, training-free, derivative-free, and compatible
with existing sampling methods. Experiments show that characteristic guidance
is robust to various applications, offers enhanced control over sample
generation, suppresses color and exposure issues even for latent space
sampling, and can handle physics problems such as the phase transitions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07621">Spatiotemporal Event Graphs for Dynamic Scene Understanding. (arXiv:2312.07621v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a></p>
<p>Dynamic scene understanding is the ability of a computer system to interpret
and make sense of the visual information present in a video of a real-world
scene. In this thesis, we present a series of frameworks for dynamic scene
understanding starting from road event detection from an autonomous driving
perspective to complex video activity detection, followed by continual learning
approaches for the life-long learning of the models. Firstly, we introduce the
ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge
the first of its kind. Due to the lack of datasets equipped with formally
specified logical requirements, we also introduce the ROad event Awareness
Dataset with logical Requirements (ROAD-R), the first publicly available
dataset for autonomous driving with requirements expressed as logical
constraints, as a tool for driving neurosymbolic research in the area. Next, we
extend event detection to holistic scene understanding by proposing two complex
activity detection methods. In the first method, we present a deformable,
spatiotemporal scene graph approach, consisting of three main building blocks:
action tube detection, a 3D deformable RoI pooling layer designed for learning
the flexible, deformable geometry of the constituent action tubes, and a scene
graph constructed by considering all parts as nodes and connecting them based
on different semantics. In a second approach evolving from the first, we
propose a hybrid graph neural network that combines attention applied to a
graph encoding of the local (short-term) dynamic scene with a temporal graph
modelling the overall long-duration activity. Finally, the last part of the
thesis is about presenting a new continual semi-supervised learning (CSSL)
paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07623">Supervised Contrastive Learning for Fine-grained Chromosome Recognition. (arXiv:2312.07623v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1">Ruijia Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1">Suncheng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chengyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1">Kui Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1">Dahong Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a></p>
<p>Chromosome recognition is an essential task in karyotyping, which plays a
vital role in birth defect diagnosis and biomedical research. However, existing
classification methods face significant challenges due to the inter-class
similarity and intra-class variation of chromosomes. To address this issue, we
propose a supervised contrastive learning strategy that is tailored to train
model-agnostic deep networks for reliable chromosome classification. This
method enables extracting fine-grained chromosomal embeddings in latent space.
These embeddings effectively expand inter-class boundaries and reduce
intra-class variations, enhancing their distinctiveness in predicting
chromosome types. On top of two large-scale chromosome datasets, we
comprehensively validate the power of our contrastive learning strategy in
boosting cutting-edge deep networks such as Transformers and ResNets. Extensive
results demonstrate that it can significantly improve models' generalization
performance, with an accuracy improvement up to +4.5%. Codes and pretrained
models will be released upon acceptance of this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07627">Multimodal Sentiment Analysis: Perceived vs Induced Sentiments. (arXiv:2312.07627v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1">Aditi Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_D/0/1/0/all/0/1">Deepika Varshney</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Saurabh Patel</a></p>
<p>Social media has created a global network where people can easily access and
exchange vast information. This information gives rise to a variety of
opinions, reflecting both positive and negative viewpoints. GIFs stand out as a
multimedia format offering a visually engaging way for users to communicate. In
this research, we propose a multimodal framework that integrates visual and
textual features to predict the GIF sentiment. It also incorporates attributes
including face emotion detection and OCR generated captions to capture the
semantic aspects of the GIF. The developed classifier achieves an accuracy of
82.7% on Twitter GIFs, which is an improvement over state-of-the-art models.
Moreover, we have based our research on the ReactionGIF dataset, analysing the
variance in sentiment perceived by the author and sentiment induced in the
reader
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07630">Pre-trained Universal Medical Image Transformer. (arXiv:2312.07630v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Lingxiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuanzhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1">Bingda Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinsheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengpeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yujiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1">Rong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a></p>
<p>Self-supervised learning has emerged as a viable method to leverage the
abundance of unlabeled medical imaging data, addressing the challenge of
labeled data scarcity in medical image analysis. In particular, masked image
modeling (MIM) with visual token reconstruction has shown promising results in
the general computer vision (CV) domain and serves as a candidate for medical
image analysis. However, the presence of heterogeneous 2D and 3D medical images
often limits the volume and diversity of training data that can be effectively
used for a single model structure. In this work, we propose a spatially
adaptive convolution (SAC) module, which adaptively adjusts convolution
parameters based on the voxel spacing of the input images. Employing this SAC
module, we build a universal visual tokenizer and a universal Vision
Transformer (ViT) capable of effectively processing a wide range of medical
images with various imaging modalities and spatial properties. Moreover, in
order to enhance the robustness of the visual tokenizer's reconstruction
objective for MIM, we suggest to generalize the discrete token output of the
visual tokenizer to a probabilistic soft token. We show that the generalized
soft token representation can be effectively integrated with the prior
distribution regularization through a constructive interpretation. As a result,
we pre-train a universal visual tokenizer followed by a universal ViT via
visual token reconstruction on 55 public medical image datasets, comprising
over 9 million 2D slices (including over 48,000 3D images). This represents the
largest, most comprehensive, and diverse dataset for pre-training 3D medical
image models to our knowledge. Experimental results on downstream medical image
classification and segmentation tasks demonstrate the superior performance of
our model and improved label efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07636">Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply. (arXiv:2312.07636v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chengting Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fengzhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hanzhi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Aili Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1">Erping Li</a></p>
<p>Traditional end-to-end (E2E) training of deep networks necessitates storing
intermediate activations for back-propagation, resulting in a large memory
footprint on GPUs and restricted model parallelization. As an alternative,
greedy local learning partitions the network into gradient-isolated modules and
trains supervisely based on local preliminary losses, thereby providing
asynchronous and parallel training methods that substantially reduce memory
cost. However, empirical experiments reveal that as the number of segmentations
of the gradient-isolated module increases, the performance of the local
learning scheme degrades substantially, severely limiting its expansibility. To
avoid this issue, we theoretically analyze the greedy local learning from the
standpoint of information theory and propose a ContSup scheme, which
incorporates context supply between isolated modules to compensate for
information loss. Experiments on benchmark datasets (i.e. CIFAR, SVHN, STL-10)
achieve SOTA results and indicate that our proposed method can significantly
improve the performance of greedy local learning with minimal memory and
computational overhead, allowing for the boost of the number of isolated
modules. Our codes are available at https://github.com/Tab-ct/ContSup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07638">Teaching Unknown Objects by Leveraging Human Gaze and Augmented Reality in Human-Robot Interaction. (arXiv:2312.07638v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weber_D/0/1/0/all/0/1">Daniel Weber</a></p>
<p>Robots are becoming increasingly popular in a wide range of environments due
to their exceptional work capacity, precision, efficiency, and scalability.
This development has been further encouraged by advances in Artificial
Intelligence, particularly Machine Learning. By employing sophisticated neural
networks, robots are given the ability to detect and interact with objects in
their vicinity. However, a significant drawback arises from the underlying
dependency on extensive datasets and the availability of substantial amounts of
training data for these object detection models. This issue becomes
particularly problematic when the specific deployment location of the robot and
the surroundings, are not known in advance. The vast and ever-expanding array
of objects makes it virtually impossible to comprehensively cover the entire
spectrum of existing objects using preexisting datasets alone. The goal of this
dissertation was to teach a robot unknown objects in the context of Human-Robot
Interaction (HRI) in order to liberate it from its data dependency, unleashing
it from predefined scenarios. In this context, the combination of eye tracking
and Augmented Reality created a powerful synergy that empowered the human
teacher to communicate with the robot and effortlessly point out objects by
means of human gaze. This holistic approach led to the development of a
multimodal HRI system that enabled the robot to identify and visually segment
the Objects of Interest in 3D space. Through the class information provided by
the human, the robot was able to learn the objects and redetect them at a later
stage. Due to the knowledge gained from this HRI based teaching, the robot's
object detection capabilities exhibited comparable performance to
state-of-the-art object detectors trained on extensive datasets, without being
restricted to predefined classes, showcasing its versatility and adaptability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07661">CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor. (arXiv:2312.07661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shuyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Runjia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiuye Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyang Li</a></p>
<p>Existing open-vocabulary image segmentation methods require a fine-tuning
step on mask annotations and/or image-text datasets. Mask labels are
labor-intensive, which limits the number of categories in segmentation
datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is
severely reduced after fine-tuning. However, without fine-tuning, VLMs trained
under weak image-text supervision tend to make suboptimal mask predictions when
there are text queries referring to non-existing concepts in the image. To
alleviate these issues, we introduce a novel recurrent framework that
progressively filters out irrelevant texts and enhances mask quality without
training efforts. The recurrent unit is a two-stage segmenter built upon a VLM
with frozen weights. Thus, our model retains the VLM's broad vocabulary space
and strengthens its segmentation capability. Experimental results show that our
method outperforms not only the training-free counterparts, but also those
fine-tuned with millions of additional data samples, and sets new
state-of-the-art records for both zero-shot semantic and referring image
segmentation tasks. Specifically, we improve the current record by 28.8, 16.0,
and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07669">GMTalker: Gaussian Mixture based Emotional talking video Portraits. (arXiv:2312.07669v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yibo Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lizhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiaoyan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a></p>
<p>Synthesizing high-fidelity and emotion-controllable talking video portraits,
with audio-lip sync, vivid expression, realistic head pose, and eye blink, is
an important and challenging task in recent years. Most of the existing methods
suffer in achieving personalized precise emotion control or continuously
interpolating between different emotions and generating diverse motion. To
address these problems, we present GMTalker, a Gaussian mixture based emotional
talking portraits generation framework. Specifically, we propose a Gaussian
Mixture based Expression Generator (GMEG) which can construct a continuous and
multi-modal latent space, achieving more flexible emotion manipulation.
Furthermore, we introduce a normalizing flow based motion generator pretrained
on the dataset with a wide-range motion to generate diverse motions. Finally,
we propose a personalized emotion-guided head generator with an Emotion Mapping
Network (EMN) which can synthesize high-fidelity and faithful emotional video
portraits. Both quantitative and qualitative experiments demonstrate our method
outperforms previous methods in image quality, photo-realism, emotion accuracy
and motion diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07705">Brain-optimized inference improves reconstructions of fMRI brain activity. (arXiv:2312.07705v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kneeland_R/0/1/0/all/0/1">Reese Kneeland</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ojeda_J/0/1/0/all/0/1">Jordyn Ojeda</a>, <a href="http://arxiv.org/find/q-bio/1/au:+St_Yves_G/0/1/0/all/0/1">Ghislain St-Yves</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Naselaris_T/0/1/0/all/0/1">Thomas Naselaris</a></p>
<p>The release of large datasets and developments in AI have led to dramatic
improvements in decoding methods that reconstruct seen images from human brain
activity. We evaluate the prospect of further improving recent decoding methods
by optimizing for consistency between reconstructions and brain activity during
inference. We sample seed reconstructions from a base decoding method, then
iteratively refine these reconstructions using a brain-optimized encoding model
that maps images to brain activity. At each iteration, we sample a small
library of images from an image distribution (a diffusion model) conditioned on
a seed reconstruction from the previous iteration. We select those that best
approximate the measured brain activity when passed through our encoding model,
and use these images for structural guidance during the generation of the small
library in the next iteration. We reduce the stochasticity of the image
distribution at each iteration, and stop when a criterion on the "width" of the
image distribution is met. We show that when this process is applied to recent
decoding methods, it outperforms the base decoding method as measured by human
raters, a variety of image feature metrics, and alignment to brain activity.
These results demonstrate that reconstruction quality can be significantly
improved by explicitly aligning decoding distributions to brain activity
distributions, even when the seed reconstruction is output from a
state-of-the-art decoding algorithm. Interestingly, the rate of refinement
varies systematically across visual cortex, with earlier visual areas generally
converging more slowly and preferring narrower image distributions, relative to
higher-level brain areas. Brain-optimized inference thus offers a succinct and
novel method for improving reconstructions and exploring the diversity of
representations across visual brain areas.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07723">Automated Behavioral Analysis Using Instance Segmentation. (arXiv:2312.07723v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Forest_J/0/1/0/all/0/1">Jeremy Forest</a>, <a href="http://arxiv.org/find/cs/1/au:+Einhorn_M/0/1/0/all/0/1">Matthew Einhorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Cleland_T/0/1/0/all/0/1">Thomas A. Cleland</a></p>
<p>Animal behavior analysis plays a crucial role in various fields, such as life
science and biomedical research. However, the scarcity of available data and
the high cost associated with obtaining a large number of labeled datasets pose
significant challenges. In this research, we propose a novel approach that
leverages instance segmentation-based transfer learning to address these
issues. By capitalizing on fine-tuning the classification head of the instance
segmentation network, we enable the tracking of multiple animals and facilitate
behavior analysis in laboratory-recorded videos. To demonstrate the
effectiveness of our method, we conducted a series of experiments, revealing
that our approach achieves exceptional performance levels, comparable to human
capabilities, across a diverse range of animal behavior analysis tasks.
Moreover, we emphasize the practicality of our solution, as it requires only a
small number of labeled images for training. To facilitate the adoption and
further development of our method, we have developed an open-source
implementation named Annolid (An annotation and instance segmentation-based
multiple animal tracking and behavior analysis package). The codebase is
publicly available on GitHub at https://github.com/cplab/annolid. This resource
serves as a valuable asset for researchers and practitioners interested in
advancing animal behavior analysis through state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07729">MedYOLO: A Medical Image Object Detection Framework. (arXiv:2312.07729v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sobek_J/0/1/0/all/0/1">Joseph Sobek</a>, <a href="http://arxiv.org/find/eess/1/au:+Inojosa_J/0/1/0/all/0/1">Jose R. Medina Inojosa</a>, <a href="http://arxiv.org/find/eess/1/au:+Inojosa_B/0/1/0/all/0/1">Betsy J. Medina Inojosa</a>, <a href="http://arxiv.org/find/eess/1/au:+Rassoulinejad_Mousavi_S/0/1/0/all/0/1">S. M. Rassoulinejad-Mousavi</a>, <a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1">Gian Marco Conte</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_Jimenez_F/0/1/0/all/0/1">Francisco Lopez-Jimenez</a>, <a href="http://arxiv.org/find/eess/1/au:+Erickson_B/0/1/0/all/0/1">Bradley J. Erickson</a></p>
<p>Artificial intelligence-enhanced identification of organs, lesions, and other
structures in medical imaging is typically done using convolutional neural
networks (CNNs) designed to make voxel-accurate segmentations of the region of
interest. However, the labels required to train these CNNs are time-consuming
to generate and require attention from subject matter experts to ensure
quality. For tasks where voxel-level precision is not required, object
detection models offer a viable alternative that can reduce annotation effort.
Despite this potential application, there are few options for general purpose
object detection frameworks available for 3-D medical imaging. We report on
MedYOLO, a 3-D object detection framework using the one-shot detection method
of the YOLO family of models and designed for use with medical imaging. We
tested this model on four different datasets: BRaTS, LIDC, an abdominal organ
Computed Tomography (CT) dataset, and an ECG-gated heart CT dataset. We found
our models achieve high performance on commonly present medium and large-sized
structures such as the heart, liver, and pancreas even without hyperparameter
tuning. However, the models struggle with very small or rarely present
structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07740">HAtt-Flow: Hierarchical Attention-Flow Mechanism for Group Activity Scene Graph Generation in Videos. (arXiv:2312.07740v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chappa_N/0/1/0/all/0/1">Naga VS Raviteja Chappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Pha Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Thi Hoang Ngan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1">Khoa Luu</a></p>
<p>Group Activity Scene Graph (GASG) generation is a challenging task in
computer vision, aiming to anticipate and describe relationships between
subjects and objects in video sequences. Traditional Video Scene Graph
Generation (VidSGG) methods focus on retrospective analysis, limiting their
predictive capabilities. To enrich the scene understanding capabilities, we
introduced a GASG dataset extending the JRDB dataset with nuanced annotations
involving \textit{Appearance, Interaction, Position, Relationship, and
Situation} attributes. This work also introduces an innovative approach,
\textbf{H}ierarchical \textbf{Att}ention-\textbf{Flow} (HAtt-Flow) Mechanism,
rooted in flow network theory to enhance GASG performance. Flow-Attention
incorporates flow conservation principles, fostering competition for sources
and allocation for sinks, effectively preventing the generation of trivial
attention. Our proposed approach offers a unique perspective on attention
mechanisms, where conventional "values" and "keys" are transformed into sources
and sinks, respectively, creating a novel framework for attention-based models.
Through extensive experiments, we demonstrate the effectiveness of our
Hatt-Flow model and the superiority of our proposed Flow-Attention mechanism.
This work represents a significant advancement in predictive video scene
understanding, providing valuable insights and techniques for applications that
require real-time relationship prediction in video data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07784">Robust MRI Reconstruction by Smoothed Unrolling (SMUG). (arXiv:2312.07784v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liang_S/0/1/0/all/0/1">Shijun Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1">Van Hoang Minh Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1">Jinghan Jia</a>, <a href="http://arxiv.org/find/eess/1/au:+Alkhouri_I/0/1/0/all/0/1">Ismail Alkhouri</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1">Saiprasad Ravishankar</a></p>
<p>As the popularity of deep learning (DL) in the field of magnetic resonance
imaging (MRI) continues to rise, recent research has indicated that DL-based
MRI reconstruction models might be excessively sensitive to minor input
disturbances, including worst-case additive perturbations. This sensitivity
often leads to unstable, aliased images. This raises the question of how to
devise DL techniques for MRI reconstruction that can be robust to train-test
variations. To address this problem, we propose a novel image reconstruction
framework, termed Smoothed Unrolling (SMUG), which advances a deep
unrolling-based MRI reconstruction model using a randomized smoothing
(RS)-based robust learning approach. RS, which improves the tolerance of a
model against input noises, has been widely used in the design of adversarial
defense approaches for image classification tasks. Yet, we find that the
conventional design that applies RS to the entire DL-based MRI model is
ineffective. In this paper, we show that SMUG and its variants address the
above issue by customizing the RS process based on the unrolling architecture
of a DL-based MRI reconstruction model. Compared to the vanilla RS approach, we
show that SMUG improves the robustness of MRI reconstruction with respect to a
diverse set of instability sources, including worst-case and random noise
perturbations to input measurements, varying measurement sampling rates, and
different numbers of unrolling steps. Furthermore, we theoretically analyze the
robustness of our method in the presence of perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07804">Uncertainty Visualization via Low-Dimensional Posterior Projections. (arXiv:2312.07804v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yair_O/0/1/0/all/0/1">Omer Yair</a>, <a href="http://arxiv.org/find/cs/1/au:+Nehme_E/0/1/0/all/0/1">Elias Nehme</a>, <a href="http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1">Tomer Michaeli</a></p>
<p>In ill-posed inverse problems, it is commonly desirable to obtain insight
into the full spectrum of plausible solutions, rather than extracting only a
single reconstruction. Information about the plausible solutions and their
likelihoods is encoded in the posterior distribution. However, for
high-dimensional data, this distribution is challenging to visualize. In this
work, we introduce a new approach for estimating and visualizing posteriors by
employing energy-based models (EBMs) over low-dimensional subspaces.
Specifically, we train a conditional EBM that receives an input measurement and
a set of directions that span some low-dimensional subspace of solutions, and
outputs the probability density function of the posterior within that space. We
demonstrate the effectiveness of our method across a diverse range of datasets
and image restoration problems, showcasing its strength in uncertainty
quantification and visualization. As we show, our method outperforms a baseline
that projects samples from a diffusion-based posterior sampler, while being
orders of magnitude faster. Furthermore, it is more accurate than a baseline
that assumes a Gaussian posterior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07806">Contextually Affinitive Neighborhood Refinery for Deep Clustering. (arXiv:2312.07806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chunlin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Ye Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingya Wang</a></p>
<p>Previous endeavors in self-supervised learning have enlightened the research
of deep clustering from an instance discrimination perspective. Built upon this
foundation, recent studies further highlight the importance of grouping
semantically similar instances. One effective method to achieve this is by
promoting the semantic structure preserved by neighborhood consistency.
However, the samples in the local neighborhood may be limited due to their
close proximity to each other, which may not provide substantial and diverse
supervision signals. Inspired by the versatile re-ranking methods in the
context of image retrieval, we propose to employ an efficient online re-ranking
process to mine more informative neighbors in a Contextually Affinitive
(ConAff) Neighborhood, and then encourage the cross-view neighborhood
consistency. To further mitigate the intrinsic neighborhood noises near cluster
boundaries, we propose a progressively relaxed boundary filtering strategy to
circumvent the issues brought by noisy neighbors. Our method can be easily
integrated into the generic self-supervised frameworks and outperforms the
state-of-the-art methods on several popular benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07814">A Foundational Multimodal Vision Language AI Assistant for Human Pathology. (arXiv:2312.07814v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Ming Y. Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bowen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1">Drew F. K. Williamson</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Richard J. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikamura_K/0/1/0/all/0/1">Kenji Ikamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerber_G/0/1/0/all/0/1">Georg Gerber</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_I/0/1/0/all/0/1">Ivy Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_L/0/1/0/all/0/1">Long Phi Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1">Tong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Parwani_A/0/1/0/all/0/1">Anil V Parwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1">Faisal Mahmood</a></p>
<p>The field of computational pathology has witnessed remarkable progress in the
development of both task-specific predictive models and task-agnostic
self-supervised vision encoders. However, despite the explosive growth of
generative artificial intelligence (AI), there has been limited study on
building general purpose, multimodal AI assistants tailored to pathology. Here
we present PathChat, a vision-language generalist AI assistant for human
pathology using an in-house developed foundational vision encoder pretrained on
100 million histology images from over 100,000 patient cases and 1.18 million
pathology image-caption pairs. The vision encoder is then combined with a
pretrained large language model and the whole system is finetuned on over
250,000 diverse disease agnostic visual language instructions. We compare
PathChat against several multimodal vision language AI assistants as well as
GPT4V, which powers the commercially available multimodal general purpose AI
assistant ChatGPT-4. When relevant clinical context is provided with the
histology image, PathChat achieved a diagnostic accuracy of 87% on
multiple-choice questions based on publicly available cases of diverse tissue
origins and disease models. Additionally, using open-ended questions and human
expert evaluation, we found that overall PathChat produced more accurate and
pathologist-preferable responses to diverse queries related to pathology. As an
interactive and general vision language AI assistant that can flexibly handle
both visual and natural language inputs, PathChat can potentially find
impactful applications in pathology education, research, and human-in-the-loop
clinical decision making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07823">Semantic-Lens: Instance-Centric Semantic Alignment for Video Super-Resolution. (arXiv:2312.07823v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Qi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Meiqin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jian Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1">Chao Yao</a></p>
<p>As a critical clue of video super-resolution (VSR), inter-frame alignment
significantly impacts overall performance. However, accurate pixel-level
alignment is a challenging task due to the intricate motion interweaving in the
video. In response to this issue, we introduce a novel paradigm for VSR named
\textbf{Semantic Lens}, predicated on semantic priors drawn from degraded
videos. Specifically, video is modeled as instances, events, and scenes via a
Semantic Extractor. Those semantics assist the Pixel Enhancer in understanding
the recovered contents and generating more realistic visual results. The
distilled global semantics embody the scene information of each frame, while
the instance-specific semantics assemble the spatial-temporal contexts related
to each instance. Furthermore, we devise a \textbf{S}emantics-\textbf{P}owered
\textbf{A}ttention \textbf{C}ross-\textbf{E}mbedding (SPACE) block to bridge
the pixel-level features with semantic knowledge, composed of a \textbf{G}lobal
\textbf{P}erspective \textbf{S}hifter (GPS) and an \textbf{I}nstance-Specific
\textbf{S}emantic \textbf{E}mbedding \textbf{E}ncoder (ISEE). Concretely, the
GPS module generates pairs of affine transformation parameters for pixel-level
feature modulation conditioned on global semantics. After that, the ISEE module
harnesses the attention mechanism to align the adjacent frames in the
instance-centric semantic space. In addition, we incorporate a simple yet
effective pre-alignment module to alleviate the difficulty of model training.
Extensive experiments demonstrate the superiority of our model over existing
state-of-the-art VSR methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07833">Stable Rivers: A Case Study in the Application of Text-to-Image Generative Models for Earth Sciences. (arXiv:2312.07833v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kupferschmidt_C/0/1/0/all/0/1">C Kupferschmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Binns_A/0/1/0/all/0/1">A.D. Binns</a>, <a href="http://arxiv.org/find/cs/1/au:+Kupferschmidt_K/0/1/0/all/0/1">K.L. Kupferschmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">G.W Taylor</a></p>
<p>Text-to-image (TTI) generative models can be used to generate photorealistic
images from a given text-string input. These models offer great potential to
mitigate challenges to the uptake of machine learning in the earth sciences.
However, the rapid increase in their use has raised questions about fairness
and biases, with most research to-date focusing on social and cultural areas
rather than domain-specific considerations. We conducted a case study for the
earth sciences, focusing on the field of fluvial geomorphology, where we
evaluated subject-area specific biases in the training data and downstream
model performance of Stable Diffusion (v1.5). In addition to perpetuating
Western biases, we found that the training data over-represented scenic
locations, such as famous rivers and waterfalls, and showed serious under- and
over-representation of many morphological and environmental terms. Despite
biased training data, we found that with careful prompting, the Stable
Diffusion model was able to generate photorealistic synthetic river images
reproducing many important environmental and morphological characteristics.
Furthermore, conditional control techniques, such as the use of condition maps
with ControlNet were effective for providing additional constraints on output
images. Despite great potential for the use of TTI models in the earth sciences
field, we advocate for caution in sensitive applications, and advocate for
domain-specific reviews of training data and image generation biases to
mitigate perpetuation of existing biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07835">Video Dynamics Prior: An Internal Learning Approach for Robust Video Enhancements. (arXiv:2312.07835v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1">Gaurav Shrivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Abhinav Shrivastava</a></p>
<p>In this paper, we present a novel robust framework for low-level vision
tasks, including denoising, object removal, frame interpolation, and
super-resolution, that does not require any external training data corpus. Our
proposed approach directly learns the weights of neural modules by optimizing
over the corrupted test sequence, leveraging the spatio-temporal coherence and
internal statistics of videos. Furthermore, we introduce a novel spatial
pyramid loss that leverages the property of spatio-temporal patch recurrence in
a video across the different scales of the video. This loss enhances robustness
to unstructured noise in both the spatial and temporal domains. This further
results in our framework being highly robust to degradation in input frames and
yields state-of-the-art results on downstream tasks such as denoising, object
removal, and frame interpolation. To validate the effectiveness of our
approach, we conduct qualitative and quantitative evaluations on standard video
datasets such as DAVIS, UCF-101, and VIMEO90K-T.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07849">Encoder-minimal and Decoder-minimal Framework for Remote Sensing Image Dehazing. (arXiv:2312.07849v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yuanbo Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a></p>
<p>Haze obscures remote sensing images, hindering valuable information
extraction. To this end, we propose RSHazeNet, an encoder-minimal and
decoder-minimal framework for efficient remote sensing image dehazing.
Specifically, regarding the process of merging features within the same level,
we develop an innovative module called intra-level transposed fusion module
(ITFM). This module employs adaptive transposed self-attention to capture
comprehensive context-aware information, facilitating the robust context-aware
feature fusion. Meanwhile, we present a cross-level multi-view interaction
module (CMIM) to enable effective interactions between features from various
levels, mitigating the loss of information due to the repeated sampling
operations. In addition, we propose a multi-view progressive extraction block
(MPEB) that partitions the features into four distinct components and employs
convolution with varying kernel sizes, groups, and dilation factors to
facilitate view-progressive feature learning. Extensive experiments demonstrate
the superiority of our proposed RSHazeNet. We release the source code and all
pre-trained models at \url{https://github.com/chdwyb/RSHazeNet}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07853">High-Order Structure Based Middle-Feature Learning for Visible-Infrared Person Re-Identification. (arXiv:2312.07853v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Liuxiang Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Si Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jin-Hao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Da-Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shunzhi Zhu</a></p>
<p>Visible-infrared person re-identification (VI-ReID) aims to retrieve images
of the same persons captured by visible (VIS) and infrared (IR) cameras.
Existing VI-ReID methods ignore high-order structure information of features
while being relatively difficult to learn a reasonable common feature space due
to the large modality discrepancy between VIS and IR images. To address the
above problems, we propose a novel high-order structure based middle-feature
learning network (HOS-Net) for effective VI-ReID. Specifically, we first
leverage a short- and long-range feature extraction (SLE) module to effectively
exploit both short-range and long-range features. Then, we propose a high-order
structure learning (HSL) module to successfully model the high-order
relationship across different local features of each person image based on a
whitened hypergraph network.This greatly alleviates model collapse and enhances
feature representations. Finally, we develop a common feature space learning
(CFL) module to learn a discriminative and reasonable common feature space
based on middle features generated by aligning features from different
modalities and ranges. In particular, a modality-range identity-center
contrastive (MRIC) loss is proposed to reduce the distances between the VIS,
IR, and middle features, smoothing the training process. Extensive experiments
on the SYSU-MM01, RegDB, and LLCM datasets show that our HOS-Net achieves
superior state-of-the-art performance. Our code is available at
\url{https://github.com/Jaulaucoeng/HOS-Net}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07854">Diffusion Models Enable Zero-Shot Pose Estimation for Lower-Limb Prosthetic Users. (arXiv:2312.07854v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianxun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Iskandar_M/0/1/0/all/0/1">Muhammad Nur Shahril Iskandar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiam_K/0/1/0/all/0/1">Keng-Hwee Chiam</a></p>
<p>The application of 2D markerless gait analysis has garnered increasing
interest and application within clinical settings. However, its effectiveness
in the realm of lower-limb amputees has remained less than optimal. In
response, this study introduces an innovative zero-shot method employing image
generation diffusion models to achieve markerless pose estimation for
lower-limb prosthetics, presenting a promising solution to gait analysis for
this specific population. Our approach demonstrates an enhancement in detecting
key points on prosthetic limbs over existing methods, and enables clinicians to
gain invaluable insights into the kinematics of lower-limb amputees across the
gait cycle. The outcomes obtained not only serve as a proof-of-concept for the
feasibility of this zero-shot approach but also underscore its potential in
advancing rehabilitation through gait analysis for this unique population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07856">DTL: Disentangled Transfer Learning for Visual Recognition. (arXiv:2312.07856v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1">Minghao Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Ke Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jianxin Wu</a></p>
<p>When pre-trained models become rapidly larger, the cost of fine-tuning on
downstream tasks steadily increases, too. To economically fine-tune these
models, parameter-efficient transfer learning (PETL) is proposed, which only
tunes a tiny subset of trainable parameters to efficiently learn quality
representations. However, current PETL methods are facing the dilemma that
during training the GPU memory footprint is not effectively reduced as
trainable parameters. PETL will likely fail, too, if the full fine-tuning
encounters the out-of-GPU-memory issue. This phenomenon happens because
trainable parameters from these methods are generally entangled with the
backbone, such that a lot of intermediate states have to be stored in GPU
memory for gradient propagation. To alleviate this problem, we introduce
Disentangled Transfer Learning (DTL), which disentangles the trainable
parameters from the backbone using a lightweight Compact Side Network (CSN). By
progressively extracting task-specific information with a few low-rank linear
mappings and appropriately adding the information back to the backbone, CSN
effectively realizes knowledge transfer in various downstream tasks. We
conducted extensive experiments to validate the effectiveness of our method.
The proposed method not only reduces a large amount of GPU memory usage and
trainable parameters, but also outperforms existing PETL methods by a
significant margin in accuracy, achieving new state-of-the-art on several
standard benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07860">Data-Dependent Higher-Order Clique Selection for Artery-Vein Segmentation by Energy Minimization. (arXiv:2312.07860v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kitamura_Y/0/1/0/all/0/1">Yoshiro Kitamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ito_W/0/1/0/all/0/1">Wataru Ito</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_H/0/1/0/all/0/1">Hiroshi Ishikawa</a></p>
<p>We propose a novel segmentation method based on energy minimization of
higher-order potentials. We introduce higher-order terms into the energy to
incorporate prior knowledge on the shape of the segments. The terms encourage
certain sets of pixels to be entirely in one segment or the other. The sets can
for instance be smooth curves in order to help delineate pulmonary vessels,
which are known to run in almost straight lines. The higher-order terms can be
converted to submodular first-order terms by adding auxiliary variables, which
can then be globally minimized using graph cuts. We also determine the weight
of these terms, or the degree of the aforementioned encouragement, in a
principled way by learning from training data with the ground truth. We
demonstrate the effectiveness of the method in a real-world application in
fully-automatic pulmonary artery-vein segmentation in CT images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07865">SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models. (arXiv:2312.07865v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhentao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tianyi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qidong Huang</a></p>
<p>Despite the success of diffusion-based customization methods on visual
content creation, increasing concerns have been raised about such techniques
from both privacy and political perspectives. To tackle this issue, several
anti-customization methods have been proposed in very recent months,
predominantly grounded in adversarial attacks. Unfortunately, most of these
methods adopt straightforward designs, such as end-to-end optimization with a
focus on adversarially maximizing the original training loss, thereby
neglecting nuanced internal properties intrinsic to the diffusion model, and
even leading to ineffective optimization in some diffusion time steps. In this
paper, we strive to bridge this gap by undertaking a comprehensive exploration
of these inherent properties, to boost the performance of current
anti-customization approaches. Two aspects of properties are investigated: 1)
We examine the relationship between time step selection and the model's
perception in the frequency domain of images and find that lower time steps can
give much more contributions to adversarial noises. This inspires us to propose
an adaptive greedy search for optimal time steps that seamlessly integrates
with existing anti-customization methods. 2) We scrutinize the roles of
features at different layers during denoising and devise a sophisticated
feature-based optimization framework for anti-customization. Experiments on
facial benchmarks demonstrate that our approach significantly increases
identity disruption, thereby enhancing user privacy and security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07871">MLNet: Mutual Learning Network with Neighborhood Invariance for Universal Domain Adaptation. (arXiv:2312.07871v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yanzuo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Meng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1">Andy J Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaohua Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1">Jian-Huang Lai</a></p>
<p>Universal domain adaptation (UniDA) is a practical but challenging problem,
in which information about the relation between the source and the target
domains is not given for knowledge transfer. Existing UniDA methods may suffer
from the problems of overlooking intra-domain variations in the target domain
and difficulty in separating between the similar known and unknown class. To
address these issues, we propose a novel \textbf{Mutual Learning Network
(MLNet)} with neighborhood invariance for UniDA. In our method,
confidence-guided invariant feature learning with self-adaptive neighbor
selection is designed to reduce the intra-domain variations for more
generalizable feature representation. By using the cross-domain mixup scheme
for better unknown-class identification, the proposed method compensates for
the misidentified known-class errors by mutual learning between the closed-set
and open-set classifiers. Extensive experiments on three publicly available
benchmarks demonstrate that our method achieves the best results compared to
the state-of-the-arts in most cases and significantly outperforms the baseline
across all the four settings in UniDA. Code is available at
https://github.com/YanzuoLu/MLNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07875">Enhance Sketch Recognition&#x27;s Explainability via Semantic Component-Level Parsing. (arXiv:2312.07875v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guangming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianci Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a></p>
<p>Free-hand sketches are appealing for humans as a universal tool to depict the
visual world. Humans can recognize varied sketches of a category easily by
identifying the concurrence and layout of the intrinsic semantic components of
the category, since humans draw free-hand sketches based a common consensus
that which types of semantic components constitute each sketch category. For
example, an airplane should at least have a fuselage and wings. Based on this
analysis, a semantic component-level memory module is constructed and embedded
in the proposed structured sketch recognition network in this paper. The memory
keys representing semantic components of each sketch category can be
self-learned and enhance the recognition network's explainability. Our proposed
networks can deal with different situations of sketch recognition, i.e., with
or without semantic components labels of strokes. Experiments on the SPG and
SketchIME datasets demonstrate the memory module's flexibility and the
recognition network's explainability. The code and data are available at
https://github.com/GuangmingZhu/SketchESC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07879">CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation. (arXiv:2312.07879v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenduo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bowen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guang Liu</a></p>
<p>Current text-to-image editing models often encounter challenges with smoothly
manipulating multiple attributes using a single instruction. Taking inspiration
from the Chain-of-Thought prompting technique utilized in language models, we
present an innovative concept known as Chain-of-Instruct Editing (CoIE), which
enhances the capabilities of these models through step-by-step editing using a
series of instructions. In particular, in the context of face manipulation, we
leverage the contextual learning abilities of a pretrained Large Language Model
(LLM), such as GPT-4, to generate a sequence of instructions from the original
input, utilizing a purpose-designed 1-shot template. To further improve the
precision of each editing step, we conduct fine-tuning on the editing models
using our self-constructed instruction-guided face editing dataset,
Instruct-CelebA. And additionally, we incorporate a super-resolution module to
mitigate the adverse effects of editability and quality degradation.
Experimental results across various challenging cases confirm the significant
boost in multi-attribute facial image manipulation using chain-of-instruct
editing. This is evident in enhanced editing success rates, measured by CLIPSim
and Coverage metrics, improved by 17.86% and 85.45% respectively, and
heightened controllability indicated by Preserve L1 and Quality metrics,
improved by 11.58% and 4.93% respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07884">Mutual-Learning Knowledge Distillation for Nighttime UAV Tracking. (arXiv:2312.07884v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yufeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_H/0/1/0/all/0/1">Haobo Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Liangliang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kunhan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guangze Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Changhong Fu</a></p>
<p>Nighttime unmanned aerial vehicle (UAV) tracking has been facilitated with
indispensable plug-and-play low-light enhancers. However, the introduction of
low-light enhancers increases the extra computational burden for the UAV,
significantly hindering the development of real-time UAV applications.
Meanwhile, these state-of-the-art (SOTA) enhancers lack tight coupling with the
advanced daytime UAV tracking approach. To solve the above issues, this work
proposes a novel mutual-learning knowledge distillation framework for nighttime
UAV tracking, i.e., MLKD. This framework is constructed to learn a compact and
fast nighttime tracker via knowledge transferring from the teacher and
knowledge sharing among various students. Specifically, an advanced teacher
based on a SOTA enhancer and a superior tracking backbone is adopted for
guiding the student based only on the tight coupling-aware tracking backbone to
directly extract nighttime object features. To address the biased learning of a
single student, diverse lightweight students with different distillation
methods are constructed to focus on various aspects of the teacher's knowledge.
Moreover, an innovative mutual-learning room is designed to elect the superior
student candidate to assist the remaining students frame-by-frame in the
training phase. Furthermore, the final best student, i.e., MLKD-Track, is
selected through the testing dataset. Extensive experiments demonstrate the
effectiveness and superiority of MLKD and MLKD-Track. The practicality of the
MLKD-Track is verified in real-world tests with different challenging
situations. The code is available at https://github.com/vision4robotics/MLKD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07899">Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Tang_Q/0/1/0/all/0/1">Qiaosi Tang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ratnayake_R/0/1/0/all/0/1">Ranjala Ratnayake</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Seabra_G/0/1/0/all/0/1">Gustavo Seabra</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jiang_Z/0/1/0/all/0/1">Zhe Jiang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fang_R/0/1/0/all/0/1">Ruogu Fang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cui_L/0/1/0/all/0/1">Lina Cui</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1">Yousong Ding</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kahveci_T/0/1/0/all/0/1">Tamer Kahveci</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1">Chenglong Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Luesch_H/0/1/0/all/0/1">Hendrik Luesch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1">Yanjun Li</a></p>
<p>Morphological profiling is a valuable tool in phenotypic drug discovery. The
advent of high-throughput automated imaging has enabled the capturing of a wide
range of morphological features of cells or organisms in response to
perturbations at the single-cell resolution. Concurrently, significant advances
in machine learning and deep learning, especially in computer vision, have led
to substantial improvements in analyzing large-scale high-content images at
high-throughput. These efforts have facilitated understanding of compound
mechanism-of-action (MOA), drug repurposing, characterization of cell
morphodynamics under perturbation, and ultimately contributing to the
development of novel therapeutics. In this review, we provide a comprehensive
overview of the recent advances in the field of morphological profiling. We
summarize the image profiling analysis workflow, survey a broad spectrum of
analysis strategies encompassing feature engineering- and deep learning-based
approaches, and introduce publicly available benchmark datasets. We place a
particular emphasis on the application of deep learning in this pipeline,
covering cell segmentation, image representation learning, and multimodal
learning. Additionally, we illuminate the application of morphological
profiling in phenotypic drug discovery and highlight potential challenges and
opportunities in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07905">Plant Disease Recognition Datasets in the Age of Deep Learning: Challenges and Opportunities. (arXiv:2312.07905v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mingle Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Ji Eun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jucheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sook Yoon</a></p>
<p>Plant disease recognition has witnessed a significant improvement with deep
learning in recent years. Although plant disease datasets are essential and
many relevant datasets are public available, two fundamental questions exist.
First, how to differentiate datasets and further choose suitable public
datasets for specific applications? Second, what kinds of characteristics of
datasets are desired to achieve promising performance in real-world
applications? To address the questions, this study explicitly propose an
informative taxonomy to describe potential plant disease datasets. We further
provide several directions for future, such as creating challenge-oriented
datasets and the ultimate objective deploying deep learning in real-world
applications with satisfactory performance. In addition, existing related
public RGB image datasets are summarized. We believe that this study will
contributing making better datasets and that this study will contribute beyond
plant disease recognition such as plant species recognition. To facilitate the
community, our project is public https://github.com/xml94/PPDRD with the
information of relevant public datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07911">Projective Parallel Single-Pixel Imaging: 3D Structured Light Scanning Under Global Illumination. (arXiv:2312.07911v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yuxi Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1">Hongzhi Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1">Huijie Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xudong Li</a></p>
<p>We present projective parallel single-pixel imaging (pPSI), a 3D photography
method that provides a robust and efficient way to analyze the light transport
behavior and enables separation of light effect due to global illumination,
thereby achieving 3D structured light scanning under global illumination. The
light transport behavior is described by the light transport coefficients
(LTC), which contain complete information for a projector camera pair, and is a
4D data set. However, the capture of LTC is generally time consuming. The 4D
LTC in pPSI are reduced to projection functions, thereby enabling a highly
efficient data capture process. We introduce the local maximum constraint,
which provides constraint for the location of candidate correspondence matching
points when projections are captured. Local slice extension (LSE) method is
introduced to accelerate the capture of projection functions. Optimization is
conducted for pPSI under several situations. The number of projection functions
required for pPSI is optimized and the influence of capture ratio in LSE on the
accuracy of the correspondence matching points is investigated. Discussions and
experiments include two typical kinds of global illuminations:
inter-reflections and subsurface scattering. The proposed method is validated
with several challenging scenarios, and outperforms the state-of-the-art
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07920">DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes. (arXiv:2312.07920v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaoyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1">Xiaojun Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongtao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Deqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>We present DrivingGaussian, an efficient and effective framework for
surrounding dynamic autonomous driving scenes. For complex scenes with moving
objects, we first sequentially and progressively model the static background of
the entire scene with incremental static 3D Gaussians. We then leverage a
composite dynamic Gaussian graph to handle multiple moving objects,
individually reconstructing each object and restoring their accurate positions
and occlusion relationships within the scene. We further use a LiDAR prior for
Gaussian Splatting to reconstruct scenes with greater details and maintain
panoramic consistency. DrivingGaussian outperforms existing methods in driving
scene reconstruction and enables photorealistic surround-view synthesis with
high-fidelity and multi-camera consistency. The source code and trained models
will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07922">Memory-Efficient Reversible Spiking Neural Networks. (arXiv:2312.07922v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a></p>
<p>Spiking neural networks (SNNs) are potential competitors to artificial neural
networks (ANNs) due to their high energy-efficiency on neuromorphic hardware.
However, SNNs are unfolded over simulation time steps during the training
process. Thus, SNNs require much more memory than ANNs, which impedes the
training of deeper SNN models. In this paper, we propose the reversible spiking
neural network to reduce the memory cost of intermediate activations and
membrane potentials during training. Firstly, we extend the reversible
architecture along temporal dimension and propose the reversible spiking block,
which can reconstruct the computational graph and recompute all intermediate
variables in forward pass with a reverse process. On this basis, we adopt the
state-of-the-art SNN models to the reversible variants, namely reversible
spiking ResNet (RevSResNet) and reversible spiking transformer (RevSFormer).
Through experiments on static and neuromorphic datasets, we demonstrate that
the memory cost per image of our reversible SNNs does not increase with the
network depth. On CIFAR10 and CIFAR100 datasets, our RevSResNet37 and
RevSFormer-4-384 achieve comparable accuracies and consume 3.79x and 3.00x
lower GPU memory per image than their counterparts with roughly identical model
complexity and parameters. We believe that this work can unleash the memory
constraints in SNN training and pave the way for training extremely large and
deep SNNs. The code is available at https://github.com/mi804/RevSNN.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07925">Polar-Doc: One-Stage Document Dewarping with Multi-Scope Constraints under Polar Representation. (arXiv:2312.07925v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiguang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiufeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>Document dewarping, aiming to eliminate geometric deformation in photographed
documents to benefit text recognition, has made great progress in recent years
but is still far from being solved. While Cartesian coordinates are typically
leveraged by state-of-the-art approaches to learn a group of deformation
control points, such representation is not efficient for dewarping model to
learn the deformation information. In this work, we explore Polar coordinates
representation for each point in document dewarping, namely Polar-Doc. In
contrast to most current works adopting a two-stage pipeline typically, Polar
representation enables a unified point regression framework for both
segmentation and dewarping network in one single stage. Such unification makes
the whole model more efficient to learn under an end-to-end optimization
pipeline, and also obtains a compact representation. Furthermore, we propose a
novel multi-scope Polar-Doc-IOU loss to constrain the relationship among
control points as a grid-based regularization under the Polar representation.
Visual comparisons and quantitative experiments on two benchmarks show that,
with much fewer parameters than the other mainstream counterparts, our
one-stage model with multi-scope constraints achieves new state-of-the-art
performance on both pixel alignment metrics and OCR metrics. Source codes will
be available at \url{*****}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07932">A Novel Framework Based on Variational Quantum Algorithms: Revolutionizing Image Classification. (arXiv:2312.07932v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_Y/0/1/0/all/0/1">Yixiong Chen</a></p>
<p>Image classification is a crucial task in machine learning. In recent years,
this field has witnessed rapid development, with a series of image
classification models being proposed and achieving state-of-the-art (SOTA)
results. Parallelly, with the advancement of quantum technologies, quantum
machine learning has attracted a lot of interest. In particular, a class of
algorithms known as variational quantum algorithms (VQAs) has been extensively
studied to improve the performance of classical machine learning. In this
paper, we propose a novel image classification framework using VQAs. The major
advantage of our framework is the elimination of the need for the global
pooling operation typically performed at the end of classical image
classification models. While global pooling can help to reduce computational
complexity, it often results in a significant loss of information. By removing
the global pooling module before the output layer, our approach allows for
effectively capturing more discriminative features and fine-grained details in
images, leading to improved classification performance. Moreover, employing
VQAs enables our framework to have fewer parameters compared to the classical
framework, even in the absence of global pooling, which makes it more
advantageous in preventing overfitting. We apply our method to different SOTA
image classification models and demonstrate the superiority of the proposed
quantum architecture over its classical counterpart through a series of
experiments on public datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07934">Toward Real World Stereo Image Super-Resolution via Hybrid Degradation Model and Discriminator for Implied Stereo Image Information. (arXiv:2312.07934v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yuanbo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1">Yuyang Xue</a>, <a href="http://arxiv.org/find/eess/1/au:+Bi_J/0/1/0/all/0/1">Jiang Bi</a>, <a href="http://arxiv.org/find/eess/1/au:+He_W/0/1/0/all/0/1">Wenlin He</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xinlin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jiajun Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Deng_W/0/1/0/all/0/1">Wei Deng</a>, <a href="http://arxiv.org/find/eess/1/au:+Nie_R/0/1/0/all/0/1">Ruofeng Nie</a>, <a href="http://arxiv.org/find/eess/1/au:+Lan_J/0/1/0/all/0/1">Junlin Lan</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1">Qinquan Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Tong_T/0/1/0/all/0/1">Tong Tong</a></p>
<p>Real-world stereo image super-resolution has a significant influence on
enhancing the performance of computer vision systems. Although existing methods
for single-image super-resolution can be applied to improve stereo images,
these methods often introduce notable modifications to the inherent disparity,
resulting in a loss in the consistency of disparity between the original and
the enhanced stereo images. To overcome this limitation, this paper proposes a
novel approach that integrates a implicit stereo information discriminator and
a hybrid degradation model. This combination ensures effective enhancement
while preserving disparity consistency. The proposed method bridges the gap
between the complex degradations in real-world stereo domain and the simpler
degradations in real-world single-image super-resolution domain. Our results
demonstrate impressive performance on synthetic and real datasets, enhancing
visual perception while maintaining disparity consistency. The complete code is
available at the following \href{https://github.com/fzuzyb/SCGLANet}{link}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07935">Comparing YOLOv8 and Mask RCNN for object segmentation in complex orchard environments. (arXiv:2312.07935v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1">Ranjan Sapkota</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_D/0/1/0/all/0/1">Dawood Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkee_M/0/1/0/all/0/1">Manoj Karkee</a></p>
<p>Instance segmentation, an important image processing operation for automation
in agriculture, is used to precisely delineate individual objects of interest
within images, which provides foundational information for various automated or
robotic tasks such as selective harvesting and precision pruning. This study
compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning
models for instance segmentation under varying orchard conditions across two
datasets. Dataset 1, collected in dormant season, includes images of dormant
apple trees, which were used to train multi-object segmentation models
delineating tree branches and trunks. Dataset 2, collected in the early growing
season, includes images of apple tree canopies with green foliage and immature
(green) apples (also called fruitlet), which were used to train single-object
segmentation models delineating only immature green apples. The results showed
that YOLOv8 performed better than Mask R-CNN, achieving good precision and
near-perfect recall across both datasets at a confidence threshold of 0.5.
Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall
of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of
0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved
a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class
scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the
inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset
1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms
and 12.8 ms achieved by Mask R-CNN's, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07937">BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. (arXiv:2312.07937v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Molin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuxuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Juze Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jingyi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingya Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lan Xu</a></p>
<p>The recently emerging text-to-motion advances have spired numerous attempts
for convenient and interactive human motion generation. Yet, existing methods
are largely limited to generating body motions only without considering the
rich two-hand motions, let alone handling various conditions like body dynamics
or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal
dataset for two-hand motion generation. Our dataset includes accurate motion
tracking for the human body and hands and provides pair-wised finger-level hand
annotations and body descriptions. We further provide a strong baseline method,
BOTH2Hands, for the novel task: generating vivid two-hand motions from both
implicit body dynamics and explicit text prompts. We first warm up two parallel
body-to-hand and text-to-hand diffusion models and then utilize the
cross-attention transformer for motion blending. Extensive experiments and
cross-validations demonstrate the effectiveness of our approach and dataset for
generating convincing two-hand motions from the hybrid body-and-textual
conditions. Our dataset and code will be disseminated to the community for
future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07943">ReFusion: Learning Image Fusion from Reconstruction with Learnable Loss via Meta-Learning. (arXiv:2312.07943v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1">Haowen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zixiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiangshe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Lilun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yukun Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Baisong Jiang</a></p>
<p>Image fusion aims to combine information from multiple source images into a
single and more informative image. A major challenge for deep learning-based
image fusion algorithms is the absence of a definitive ground truth and
distance measurement. Thus, the manually specified loss functions aiming to
steer the model learning, include hyperparameters that need to be manually
thereby limiting the model's flexibility and generalizability to unseen tasks.
To overcome the limitations of designing loss functions for specific fusion
tasks, we propose a unified meta-learning based fusion framework named
ReFusion, which learns optimal fusion loss from reconstructing source images.
ReFusion consists of a fusion module, a loss proposal module, and a
reconstruction module. Compared with the conventional methods with fixed loss
functions, ReFusion employs a parameterized loss function, which is dynamically
adapted by the loss proposal module based on the specific fusion scene and
task. To ensure that the fusion network preserves maximal information from the
source images, makes it possible to reconstruct the original images from the
fusion image, a meta-learning strategy is used to make the reconstruction loss
continually refine the parameters of the loss proposal module. Adaptive
updating is achieved by alternating between inter update, outer update, and
fusion update, where the training of the three components facilitates each
other. Extensive experiments affirm that our method can successfully adapt to
diverse fusion tasks, including infrared-visible, multi-focus, multi-exposure,
and medical image fusion problems. The code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07951">Semantic-aware Data Augmentation for Text-to-image Synthesis. (arXiv:2312.07951v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhaorui Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>Data augmentation has been recently leveraged as an effective regularizer in
various vision-language deep neural networks. However, in text-to-image
synthesis (T2Isyn), current augmentation wisdom still suffers from the semantic
mismatch between augmented paired data. Even worse, semantic collapse may occur
when generated images are less semantically constrained. In this paper, we
develop a novel Semantic-aware Data Augmentation (SADA) framework dedicated to
T2Isyn. In particular, we propose to augment texts in the semantic space via an
Implicit Textual Semantic Preserving Augmentation ($ITA$), in conjunction with
a specifically designed Image Semantic Regularization Loss ($L_r$) as Generated
Image Semantic Conservation, to cope well with semantic mismatch and collapse.
As one major contribution, we theoretically show that $ITA$ can certify better
text-image consistency while $L_r$ regularizing the semantics of generated
images would avoid semantic collapse and enhance image quality. Extensive
experiments validate that SADA enhances text-image consistency and improves
image quality significantly in T2Isyn models across various backbones.
Especially, incorporating SADA during the tuning process of Stable Diffusion
models also yields performance improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07955">Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking. (arXiv:2312.07955v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengsheng Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_D/0/1/0/all/0/1">Dizhan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huaiwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>Researchers have recently found that Self-Supervised Learning (SSL) is
vulnerable to backdoor attacks. The attacker can embed hidden SSL backdoors via
a few poisoned examples in the training dataset and maliciously manipulate the
behavior of downstream models. To defend against SSL backdoor attacks, a
feasible route is to detect and remove the poisonous samples in the training
set. However, the existing SSL backdoor defense method fails to detect the
poisonous samples precisely. In this paper, we propose to erase the SSL
backdoor by cluster activation masking and propose a novel PoisonCAM method.
After obtaining the threat model trained on the poisoned dataset, our method
can precisely detect poisonous samples based on the assumption that masking the
backdoor trigger can effectively change the activation of a downstream
clustering model. In experiments, our PoisonCAM achieves 96% accuracy for
backdoor trigger detection compared to 3% of the state-of-the-art method on
poisoned ImageNet-100. Moreover, our proposed PoisonCAM significantly improves
the performance of the trained SSL model under backdoor attacks compared to the
state-of-the-art method. Our code will be available at
https://github.com/LivXue/PoisonCAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07964">Three-Filters-to-Normal+: Revisiting Discontinuity Discrimination in Depth-to-Normal Translation. (arXiv:2312.07964v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1">Bohuan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Deming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qijun Chen</a></p>
<p>This article introduces three-filters-to-normal+ (3F2N+), an extension of our
previous work three-filters-to-normal (3F2N), with a specific focus on
incorporating discontinuity discrimination capability into surface normal
estimators (SNEs). 3F2N+ achieves this capability by utilizing a novel
discontinuity discrimination module (DDM), which combines depth curvature
minimization and correlation coefficient maximization through conditional
random fields (CRFs). To evaluate the robustness of SNEs on noisy data, we
create a large-scale synthetic surface normal (SSN) dataset containing 20
scenarios (ten indoor scenarios and ten outdoor scenarios with and without
random Gaussian noise added to depth images). Extensive experiments demonstrate
that 3F2N+ achieves greater performance than all other geometry-based surface
normal estimators, with average angular errors of 7.85$^\circ$, 8.95$^\circ$,
9.25$^\circ$, and 11.98$^\circ$ on the clean-indoor, clean-outdoor,
noisy-indoor, and noisy-outdoor datasets, respectively. We conduct three
additional experiments to demonstrate the effectiveness of incorporating our
proposed 3F2N+ into downstream robot perception tasks, including freespace
detection, 6D object pose estimation, and point cloud completion. Our source
code and datasets are publicly available at https://mias.group/3F2Nplus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07965">Pneumonia Detection on chest X-ray images Using Ensemble of Deep Convolutional Neural Networks. (arXiv:2312.07965v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mabrouk_A/0/1/0/all/0/1">Alhassan Mabrouk</a>, <a href="http://arxiv.org/find/eess/1/au:+Redondo_R/0/1/0/all/0/1">Rebeca P. D&#xed;az Redondo</a>, <a href="http://arxiv.org/find/eess/1/au:+Dahou_A/0/1/0/all/0/1">Abdelghani Dahou</a>, <a href="http://arxiv.org/find/eess/1/au:+Elaziz_M/0/1/0/all/0/1">Mohamed Abd Elaziz</a>, <a href="http://arxiv.org/find/eess/1/au:+Kayed_M/0/1/0/all/0/1">Mohammed Kayed</a></p>
<p>Pneumonia is a life-threatening lung infection resulting from several
different viral infections. Identifying and treating pneumonia on chest X-ray
images can be difficult due to its similarity to other pulmonary diseases.
Thus, the existing methods for predicting pneumonia cannot attain substantial
levels of accuracy. Therefore, this paper presents a computer-aided
classification of pneumonia, coined as Ensemble Learning (EL), to simplify the
diagnosis process on chest X-ray images. Our proposal is based on Convolutional
Neural Network (CNN) models, which are pre-trained CNN models that have been
recently employed to enhance the performance of many medical tasks instead of
training CNN models from scratch. We propose to use three well-known CNN
pre-trained (DenseNet169, MobileNetV2 and Vision Transformer) using the
ImageNet database. Then, these models are trained on the chest X-ray data set
using fine-tuning. Finally, the results are obtained by combining the extracted
features from these three models during the experimental phase. The proposed EL
approach outperforms other existing state-of-the-art methods, and it obtains an
accuracy of 93.91% and a F1-Score of 93.88% on the testing phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07969">ASLseg: Adapting SAM in the Loop for Semi-supervised Liver Tumor Segmentation. (arXiv:2312.07969v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shiyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Li Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pujin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiaoying Tang</a></p>
<p>Liver tumor segmentation is essential for computer-aided diagnosis, surgical
planning, and prognosis evaluation. However, obtaining and maintaining a
large-scale dataset with dense annotations is challenging. Semi-Supervised
Learning (SSL) is a common technique to address these challenges. Recently,
Segment Anything Model (SAM) has shown promising performance in some medical
image segmentation tasks, but it performs poorly for liver tumor segmentation.
In this paper, we propose a novel semi-supervised framework, named ASLseg,
which can effectively adapt the SAM to the SSL setting and combine both
domain-specific and general knowledge of liver tumors. Specifically, the
segmentation model trained with a specific SSL paradigm provides the generated
pseudo-labels as prompts to the fine-tuned SAM. An adaptation network is then
used to refine the SAM-predictions and generate higher-quality pseudo-labels.
Finally, the reliable pseudo-labels are selected to expand the labeled set for
iterative training. Extensive experiments on the LiTS dataset demonstrate
overwhelming performance of our ASLseg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07970">Divide and Conquer: Hybrid Pre-training for Person Search. (arXiv:2312.07970v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yanling Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Di Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanshan Zhang</a></p>
<p>Large-scale pre-training has proven to be an effective method for improving
performance across different tasks. Current person search methods use ImageNet
pre-trained models for feature extraction, yet it is not an optimal solution
due to the gap between the pre-training task and person search task (as a
downstream task). Therefore, in this paper, we focus on pre-training for person
search, which involves detecting and re-identifying individuals simultaneously.
Although labeled data for person search is scarce, datasets for two sub-tasks
person detection and re-identification are relatively abundant. To this end, we
propose a hybrid pre-training framework specifically designed for person search
using sub-task data only. It consists of a hybrid learning paradigm that
handles data with different kinds of supervisions, and an intra-task alignment
module that alleviates domain discrepancy under limited resources. To the best
of our knowledge, this is the first work that investigates how to support
full-task pre-training using sub-task data. Extensive experiments demonstrate
that our pre-trained model can achieve significant improvements across diverse
protocols, such as person search method, fine-tuning data, pre-training data
and model backbone. For example, our model improves ResNet50 based NAE by 10.3%
relative improvement w.r.t. mAP. Our code and pre-trained models are released
for plug-and-play usage to the person search community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07971">LMD: Faster Image Reconstruction with Latent Masking Diffusion. (arXiv:2312.07971v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+yu_z/0/1/0/all/0/1">zhihuan yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianjun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bowen Zhou</a></p>
<p>As a class of fruitful approaches, diffusion probabilistic models (DPMs) have
shown excellent advantages in high-resolution image reconstruction. On the
other hand, masked autoencoders (MAEs), as popular self-supervised vision
learners, have demonstrated simpler and more effective image reconstruction and
transfer capabilities on downstream tasks. However, they all require extremely
high training costs, either due to inherent high temporal-dependence (i.e.,
excessively long diffusion steps) or due to artificially low spatial-dependence
(i.e., human-formulated high mask ratio, such as 0.75). To the end, this paper
presents LMD, a faster image reconstruction framework with latent masking
diffusion. First, we propose to project and reconstruct images in latent space
through a pre-trained variational autoencoder, which is theoretically more
efficient than in the pixel-based space. Then, we combine the advantages of
MAEs and DPMs to design a progressive masking diffusion model, which gradually
increases the masking proportion by three different schedulers and reconstructs
the latent features from simple to difficult, without sequentially performing
denoising diffusion as in DPMs or using fixed high masking ratio as in MAEs, so
as to alleviate the high training time-consumption predicament. Our approach
allows for learning high-capacity models and accelerate their training (by 3x
or more) and barely reduces the original accuracy. Inference speed in
downstream tasks also significantly outperforms the previous approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07976">Challenges of YOLO Series for Object Detection in Extremely Heavy Rain: CALRA Simulator based Synthetic Evaluation Dat a set. (arXiv:2312.07976v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">T. Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1">H. Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1">Y. Lim</a></p>
<p>Recently, as many studies of autonomous vehicles have been achieved for
levels 4 and 5, there has been also increasing interest in the advancement of
perception, decision, and control technologies, which are the three major
aspects of autonomous vehicles. As for the perception technologies achieving
reliable maneuvering of autonomous vehicles, object detection by using diverse
sensors (e.g., LiDAR, radar, and camera) should be prioritized. These sensors
require to detect objects accurately and quickly in diverse weather conditions,
but they tend to have challenges to consistently detect objects in bad weather
conditions with rain, snow, or fog. Thus, in this study, based on the
experimentally obtained raindrop data from precipitation conditions, we
constructed a novel dataset that could test diverse network model in various
precipitation conditions through the CARLA simulator. Consequently, based on
our novel dataset, YOLO series, a one-stage-detector, was used to
quantitatively verify how much object detection performance could be decreased
under various precipitation conditions from normal to extreme heavy rain
situations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08004">Instance-aware Multi-Camera 3D Object Detection with Structural Priors Mining and Self-Boosting Learning. (arXiv:2312.08004v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1">Yang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1">Zequn Jie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shaoxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lechao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingjing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Camera-based bird-eye-view (BEV) perception paradigm has made significant
progress in the autonomous driving field. Under such a paradigm, accurate BEV
representation construction relies on reliable depth estimation for
multi-camera images. However, existing approaches exhaustively predict depths
for every pixel without prioritizing objects, which are precisely the entities
requiring detection in the 3D space. To this end, we propose IA-BEV, which
integrates image-plane instance awareness into the depth estimation process
within a BEV-based detector. First, a category-specific structural priors
mining approach is proposed for enhancing the efficacy of monocular depth
generation. Besides, a self-boosting learning strategy is further proposed to
encourage the model to place more emphasis on challenging objects in
computation-expensive temporal stereo matching. Together they provide advanced
depth estimation results for high-quality BEV features construction, benefiting
the ultimate 3D detection. The proposed method achieves state-of-the-art
performances on the challenging nuScenes benchmark, and extensive experimental
results demonstrate the effectiveness of our designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08007">Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring Expression Segmentation. (arXiv:2312.08007v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1">Tongtian Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yisi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Longteng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xingjian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinlong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a></p>
<p>Referring expression segmentation (RES) aims at segmenting the foreground
masks of the entities that match the descriptive natural language expression.
Previous datasets and methods for classic RES task heavily rely on the prior
assumption that one expression must refer to object-level targets. In this
paper, we take a step further to finer-grained part-level RES task. To promote
the object-level RES task towards finer-grained vision-language understanding,
we put forward a new multi-granularity referring expression segmentation (MRES)
task and construct an evaluation benchmark called RefCOCOm by manual
annotations. By employing our automatic model-assisted data engine, we build
the largest visual grounding dataset namely MRES-32M, which comprises over
32.2M high-quality masks and captions on the provided 1M images. Besides, a
simple yet strong model named UniRES is designed to accomplish the unified
object-level and part-level grounding task. Extensive experiments on our
RefCOCOm for MRES and three datasets (i.e., RefCOCO(+/g) for classic RES task
demonstrate the superiority of our method over previous state-of-the-art
methods. To foster future research into fine-grained visual grounding, our
benchmark RefCOCOm, the MRES-32M dataset and model UniRES will be publicly
available at https://github.com/Rubics-Xuan/MRES
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08009">Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix. (arXiv:2312.08009v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kewei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yizheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zhiyu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_K/0/1/0/all/0/1">Ke Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhiguo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a></p>
<p>Class-agnostic motion prediction methods aim to comprehend motion within
open-world scenarios, holding significance for autonomous driving systems.
However, training a high-performance model in a fully-supervised manner always
requires substantial amounts of manually annotated data, which can be both
expensive and time-consuming to obtain. To address this challenge, our study
explores the potential of semi-supervised learning (SSL) for class-agnostic
motion prediction. Our SSL framework adopts a consistency-based self-training
paradigm, enabling the model to learn from unlabeled data by generating pseudo
labels through test-time inference. To improve the quality of pseudo labels, we
propose a novel motion selection and re-generation module. This module
effectively selects reliable pseudo labels and re-generates unreliable ones.
Furthermore, we propose two data augmentation strategies: temporal sampling and
BEVMix. These strategies facilitate consistency regularization in SSL.
Experiments conducted on nuScenes demonstrate that our SSL method can surpass
the self-supervised approach by a large margin by utilizing only a tiny
fraction of labeled data. Furthermore, our method exhibits comparable
performance to weakly and some fully supervised methods. These results
highlight the ability of our method to strike a favorable balance between
annotation costs and performance. Code will be available at
https://github.com/kwwcv/SSMP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08010">EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1">Shahzad Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1">Sukalpa Chanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1">Yogesh S Rawat</a></p>
<p>Recent advancements in large-scale pre-training of visual-language models on
paired image-text data have demonstrated impressive generalization capabilities
for zero-shot tasks. Building on this success, efforts have been made to adapt
these image-based visual-language models, such as CLIP, for videos extending
their zero-shot capabilities to the video domain. While these adaptations have
shown promising results, they come at a significant computational cost and
struggle with effectively modeling the crucial temporal aspects inherent to the
video domain. In this study, we present EZ-CLIP, a simple and efficient
adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal
visual prompting for seamless temporal adaptation, requiring no fundamental
alterations to the core CLIP architecture while preserving its remarkable
generalization abilities. Moreover, we introduce a novel learning objective
that guides the temporal visual prompts to focus on capturing motion, thereby
enhancing its learning capabilities from video data. We conducted extensive
experiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP
for zero-shot learning and base-to-novel video action recognition, and also
demonstrating its potential for few-shot generalization.Impressively, with a
mere 5.2 million learnable parameters (as opposed to the 71.1 million in the
prior best model), EZ-CLIP can be efficiently trained on a single GPU,
outperforming existing approaches in several evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08012">uSF: Learning Neural Semantic Field with Uncertainty. (arXiv:2312.08012v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Skorokhodov_V/0/1/0/all/0/1">Vsevolod Skorokhodov</a>, <a href="http://arxiv.org/find/cs/1/au:+Drozdova_D/0/1/0/all/0/1">Darya Drozdova</a>, <a href="http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1">Dmitry Yudin</a></p>
<p>Recently, there has been an increased interest in NeRF methods which
reconstruct differentiable representation of three-dimensional scenes. One of
the main limitations of such methods is their inability to assess the
confidence of the model in its predictions. In this paper, we propose a new
neural network model for the formation of extended vector representations,
called uSF, which allows the model to predict not only color and semantic label
of each point, but also estimate the corresponding values of uncertainty. We
show that with a small number of images available for training, a model
quantifying uncertainty performs better than a model without such
functionality. Code of the uSF approach is publicly available at
https://github.com/sevashasla/usf/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08019">AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing. (arXiv:2312.08019v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1">Guoli Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bowen Zhou</a></p>
<p>With the great success of text-conditioned diffusion models in creative
text-to-image generation, various text-driven image editing approaches have
attracted the attentions of many researchers. However, previous works mainly
focus on discreteness-sensitive instructions such as adding, removing or
replacing specific objects, background elements or global styles (i.e., hard
editing), while generally ignoring subject-binding but semantically
fine-changing continuity-sensitive instructions such as actions, poses or
adjectives, and so on (i.e., soft editing), which hampers generative AI from
generating user-customized visual contents. To mitigate this predicament, we
propose a spatio-temporal guided adaptive editing algorithm AdapEdit, which
realizes adaptive image editing by introducing a soft-attention strategy to
dynamically vary the guiding degree from the editing conditions to visual
pixels from both temporal and spatial perspectives. Note our approach has a
significant advantage in preserving model priors and does not require model
training, fine-tuning, extra data, or optimization. We present our results over
a wide variety of raw images and editing instructions, demonstrating
competitive performance and showing it significantly outperforms the previous
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08020">Generalized Deepfakes Detection with Reconstructed-Blended Images and Multi-scale Feature Reconstruction Network. (arXiv:2312.08020v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chun-Shien Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">ZhiYong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1">Isao Echizen</a></p>
<p>The growing diversity of digital face manipulation techniques has led to an
urgent need for a universal and robust detection technology to mitigate the
risks posed by malicious forgeries. We present a blended-based detection
approach that has robust applicability to unseen datasets. It combines a method
for generating synthetic training samples, i.e., reconstructed blended images,
that incorporate potential deepfake generator artifacts and a detection model,
a multi-scale feature reconstruction network, for capturing the generic
boundary artifacts and noise distribution anomalies brought about by digital
face manipulations. Experiments demonstrated that this approach results in
better performance in both cross-manipulation detection and cross-dataset
detection on unseen data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08022">Mono3DVG: 3D Visual Grounding in Monocular Images. (arXiv:2312.08022v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yang Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhitong Xiong</a></p>
<p>We introduce a novel task of 3D visual grounding in monocular RGB images
using language descriptions with both appearance and geometry information.
Specifically, we build a large-scale dataset, Mono3DRefer, which contains 3D
object targets with their corresponding geometric text descriptions, generated
by ChatGPT and refined manually. To foster this task, we propose Mono3DVG-TR,
an end-to-end transformer-based network, which takes advantage of both the
appearance and geometry information in text embeddings for multi-modal learning
and 3D object localization. Depth predictor is designed to explicitly learn
geometry features. The dual text-guided adapter is proposed to refine
multiscale visual and geometry features of the referred object. Based on
depth-text-visual stacking attention, the decoder fuses object-level geometric
cues and visual appearance into a learnable query. Comprehensive benchmarks and
some insightful analyses are provided for Mono3DVG. Extensive comparisons and
ablation studies show that our method significantly outperforms all baselines.
The dataset and code will be publicly available at:
https://github.com/ZhanYang-nwpu/Mono3DVG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08029">ClusterDDPM: An EM clustering framework with Denoising Diffusion Probabilistic Models. (arXiv:2312.08029v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jie Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhong-yuan Zhang</a></p>
<p>Variational autoencoder (VAE) and generative adversarial networks (GAN) have
found widespread applications in clustering and have achieved significant
success. However, the potential of these approaches may be limited due to VAE's
mediocre generation capability or GAN's well-known instability during
adversarial training. In contrast, denoising diffusion probabilistic models
(DDPMs) represent a new and promising class of generative models that may
unlock fresh dimensions in clustering. In this study, we introduce an
innovative expectation-maximization (EM) framework for clustering using DDPMs.
In the E-step, we aim to derive a mixture of Gaussian priors for the subsequent
M-step. In the M-step, our focus lies in learning clustering-friendly latent
representations for the data by employing the conditional DDPM and matching the
distribution of latent representations to the mixture of Gaussian priors. We
present a rigorous theoretical analysis of the optimization process in the
M-step, proving that the optimizations are equivalent to maximizing the lower
bound of the Q function within the vanilla EM framework under certain
constraints. Comprehensive experiments validate the advantages of the proposed
framework, showcasing superior performance in clustering, unsupervised
conditional generation and latent representation learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08034">Individualized Deepfake Detection Exploiting Traces Due to Double Neural-Network Operations. (arXiv:2312.08034v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1">Mushfiqur Rahman</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1">Runze Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wong_C/0/1/0/all/0/1">Chau-Wai Wong</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1">Huaiyu Dai</a></p>
<p>In today's digital landscape, journalists urgently require tools to verify
the authenticity of facial images and videos depicting specific public figures
before incorporating them into news stories. Existing deepfake detectors are
not optimized for this detection task when an image is associated with a
specific and identifiable individual. This study focuses on the deepfake
detection of facial images of individual public figures. We propose to
condition the proposed detector on the identity of the identified individual
given the advantages revealed by our theory-driven simulations. While most
detectors in the literature rely on perceptible or imperceptible artifacts
present in deepfake facial images, we demonstrate that the detection
performance can be improved by exploiting the idempotency property of neural
networks. In our approach, the training process involves double neural-network
operations where we pass an authentic image through a deepfake simulating
network twice. Experimental results show that the proposed method improves the
area under the curve (AUC) from 0.92 to 0.94 and reduces its standard deviation
by 17\%. For evaluating the detection performance of individual public figures,
a facial image dataset with individuals' names is required, a criterion not met
by the current deepfake datasets. To address this, we curated a dataset
comprising 32k images featuring 45 public figures, which we intend to release
to the public after the paper is published.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08048">Compositional Inversion for Stable Diffusion Models. (arXiv:2312.08048v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xu-Lu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiao-Yong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jin-Lin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tian-Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhao-Xiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a></p>
<p>Inversion methods, such as Textual Inversion, generate personalized images by
incorporating concepts of interest provided by user images. However, existing
methods often suffer from overfitting issues, where the dominant presence of
inverted concepts leads to the absence of other desired concepts. It stems from
the fact that during inversion, the irrelevant semantics in the user images are
also encoded, forcing the inverted concepts to occupy locations far from the
core distribution in the embedding space. To address this issue, we propose a
method that guides the inversion process towards the core distribution for
compositional embeddings. Additionally, we introduce a spatial regularization
approach to balance the attention on the concepts being composed. Our method is
designed as a post-training approach and can be seamlessly integrated with
other inversion methods. Experimental results demonstrate the effectiveness of
our proposed approach in mitigating the overfitting problem and generating more
diverse and balanced compositions of concepts in the synthesized images. The
source code is available at
https://github.com/zhangxulu1996/Compositional-Inversion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08054">Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud Sequence. (arXiv:2312.08054v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhuorui Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoran Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1">Li Yi</a></p>
<p>We study a new problem of semantic complete scene forecasting (SCSF) in this
work. Given a 4D dynamic point cloud sequence, our goal is to forecast the
complete scene corresponding to the future next frame along with its semantic
labels. To tackle this challenging problem, we properly model the synergetic
relationship between future forecasting and semantic scene completion through a
novel network named SCSFNet. SCSFNet leverages a hybrid geometric
representation for high-resolution complete scene forecasting. To leverage
multi-frame observation as well as the understanding of scene dynamics to ease
the completion task, SCSFNet introduces an attention-based skip connection
scheme. To ease the need to model occlusion variations and to better focus on
the occluded part, SCSFNet utilizes auxiliary visibility grids to guide the
forecasting task. To evaluate the effectiveness of SCSFNet, we conduct
experiments on various benchmarks including two large-scale indoor benchmarks
we contributed and the outdoor SemanticKITTI benchmark. Extensive experiments
show SCSFNet outperforms baseline methods on multiple metrics by a large
margin, and also prove the synergy between future forecasting and semantic
scene completion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08056">Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision. (arXiv:2312.08056v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shengguang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenglun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1">Qi Su</a></p>
<p>Ancient artifacts are an important medium for cultural preservation and
restoration. However, many physical copies of artifacts are either damaged or
lost, leaving a blank space in archaeological and historical studies that calls
for artifact image generation techniques. Despite the significant advancements
in open-domain text-to-image synthesis, existing approaches fail to capture the
important domain knowledge presented in the textual description, resulting in
errors in recreated images such as incorrect shapes and patterns. In this
paper, we propose a novel knowledge-aware artifact image synthesis approach
that brings lost historical objects accurately into their visual forms. We use
a pretrained diffusion model as backbone and introduce three key techniques to
enhance the text-to-image generation framework: 1) we construct prompts with
explicit archaeological knowledge elicited from large language models (LLMs);
2) we incorporate additional textual guidance to correlated historical
expertise in a contrastive manner; 3) we introduce further visual-semantic
constraints on edge and perceptual features that enable our model to learn more
intricate visual details of the artifacts. Compared to existing approaches, our
proposed model produces higher-quality artifact images that align better with
the implicit details and historical knowledge contained within written
documents, thus achieving significant improvements across automatic metrics and
in human evaluation. Our code and data are available at
https://github.com/danielwusg/artifact_diffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08060">C-BEV: Contrastive Bird&#x27;s Eye View Training for Cross-View Image Retrieval and 3-DoF Pose Estimation. (arXiv:2312.08060v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fervers_F/0/1/0/all/0/1">Florian Fervers</a>, <a href="http://arxiv.org/find/cs/1/au:+Bullinger_S/0/1/0/all/0/1">Sebastian Bullinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Bodensteiner_C/0/1/0/all/0/1">Christoph Bodensteiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1">Michael Arens</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>To find the geolocation of a street-view image, cross-view geolocalization
(CVGL) methods typically perform image retrieval on a database of georeferenced
aerial images and determine the location from the visually most similar match.
Recent approaches focus mainly on settings where street-view and aerial images
are preselected to align w.r.t. translation or orientation, but struggle in
challenging real-world scenarios where varying camera poses have to be matched
to the same aerial image. We propose a novel trainable retrieval architecture
that uses bird's eye view (BEV) maps rather than vectors as embedding
representation, and explicitly addresses the many-to-one ambiguity that arises
in real-world scenarios. The BEV-based retrieval is trained using the same
contrastive setting and loss as classical retrieval.
</p>
<p>Our method C-BEV surpasses the state-of-the-art on the retrieval task on
multiple datasets by a large margin. It is particularly effective in
challenging many-to-one scenarios, e.g. increasing the top-1 recall on VIGOR's
cross-area split with unknown orientation from 31.1% to 65.0%. Although the
model is supervised only through a contrastive objective applied on image
pairings, it additionally learns to infer the 3-DoF camera pose on the matching
aerial image, and even yields a lower mean pose error than recent methods that
are explicitly trained with metric groundtruth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08071">Novel View Synthesis with View-Dependent Effects from a Single Image. (arXiv:2312.08071v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1">Juan Luis Gonzalez Bello</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Munchurl Kim</a></p>
<p>In this paper, we firstly consider view-dependent effects into single
image-based novel view synthesis (NVS) problems. For this, we propose to
exploit the camera motion priors in NVS to model view-dependent appearance or
effects (VDE) as the negative disparity in the scene. By recognizing
specularities "follow" the camera motion, we infuse VDEs into the input images
by aggregating input pixel colors along the negative depth region of the
epipolar lines. Also, we propose a `relaxed volumetric rendering' approximation
that allows computing the densities in a single pass, improving efficiency for
NVS from single images. Our method can learn single-image NVS from image
sequences only, which is a completely self-supervised learning method, for the
first time requiring neither depth nor camera pose annotations. We present
extensive experiment results and show that our proposed method can learn NVS
with VDEs, outperforming the SOTA single-view NVS methods on the RealEstate10k
and MannequinChallenge datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08078">Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Linlin Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yixuan Yuan</a></p>
<p>To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches' for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08094">3DGEN: A GAN-based approach for generating novel 3D models from image data. (arXiv:2312.08094v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schnepf_A/0/1/0/all/0/1">Antoine Schnepf</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1">Flavian Vasile</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1">Ugo Tanielian</a></p>
<p>The recent advances in text and image synthesis show a great promise for the
future of generative models in creative fields. However, a less explored area
is the one of 3D model generation, with a lot of potential applications to game
design, video production, and physical product design. In our paper, we present
3DGEN, a model that leverages the recent work on both Neural Radiance Fields
for object reconstruction and GAN-based image generation. We show that the
proposed architecture can generate plausible meshes for objects of the same
category as the training images and compare the resulting meshes with the
state-of-the-art baselines, leading to visible uplifts in generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08111">Towards Better Morphed Face Images without Ghosting Artifacts. (arXiv:2312.08111v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1">Clemens Seibold</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1">Anna Hilsmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1">Peter Eisert</a></p>
<p>Automatic generation of morphed face images often produces ghosting artifacts
due to poorly aligned structures in the input images. Manual processing can
mitigate these artifacts. However, this is not feasible for the generation of
large datasets, which are required for training and evaluating robust morphing
attack detectors. In this paper, we propose a method for automatic prevention
of ghosting artifacts based on a pixel-wise alignment during morph generation.
We evaluate our proposed method on state-of-the-art detectors and show that our
morphs are harder to detect, particularly, when combined with
style-transfer-based improvement of low-level image characteristics.
Furthermore, we show that our approach does not impair the biometric quality,
which is essential for high quality morphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08118">Neural Radiance Fields for Transparent Object Using Visual Hull. (arXiv:2312.08118v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1">Heechan Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seungkyu Lee</a></p>
<p>Unlike opaque object, novel view synthesis of transparent object is a
challenging task, because transparent object refracts light of background
causing visual distortions on the transparent object surface along the
viewpoint change. Recently introduced Neural Radiance Fields (NeRF) is a view
synthesis method. Thanks to its remarkable performance improvement, lots of
following applications based on NeRF in various topics have been developed.
However, if an object with a different refractive index is included in a scene
such as transparent object, NeRF shows limited performance because refracted
light ray at the surface of the transparent object is not appropriately
considered. To resolve the problem, we propose a NeRF-based method consisting
of the following three steps: First, we reconstruct a three-dimensional shape
of a transparent object using visual hull. Second, we simulate the refraction
of the rays inside of the transparent object according to Snell's law. Last, we
sample points through refracted rays and put them into NeRF. Experimental
evaluation results demonstrate that our method addresses the limitation of
conventional NeRF with transparent objects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08128">Clockwork Diffusion: Efficient Generation With Model-Step Distillation. (arXiv:2312.08128v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1">Amirhossein Habibian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghodrati_A/0/1/0/all/0/1">Amir Ghodrati</a>, <a href="http://arxiv.org/find/cs/1/au:+Fathima_N/0/1/0/all/0/1">Noor Fathima</a>, <a href="http://arxiv.org/find/cs/1/au:+Sautiere_G/0/1/0/all/0/1">Guillaume Sautiere</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrepalli_R/0/1/0/all/0/1">Risheek Garrepalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1">Fatih Porikli</a>, <a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1">Jens Petersen</a></p>
<p>This work aims to improve the efficiency of text-to-image diffusion models.
While diffusion models use computationally expensive UNet-based denoising
operations in every generation step, we identify that not all operations are
equally relevant for the final output quality. In particular, we observe that
UNet layers operating on high-res feature maps are relatively sensitive to
small perturbations. In contrast, low-res feature maps influence the semantic
layout of the final image and can often be perturbed with no noticeable change
in the output. Based on this observation, we propose Clockwork Diffusion, a
method that periodically reuses computation from preceding denoising steps to
approximate low-res feature maps at one or more subsequent steps. For multiple
baselines, and for both text-to-image generation and image editing, we
demonstrate that Clockwork leads to comparable or improved perceptual scores
with drastically reduced computational complexity. As an example, for Stable
Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and
CLIP change.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08136">ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields. (arXiv:2312.08136v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1">Juan Luis Gonzalez Bello</a>, <a href="http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1">Minh-Quan Viet Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Munchurl Kim</a></p>
<p>Recent advances in neural rendering have shown that, albeit slow, implicit
compact models can learn a scene's geometries and view-dependent appearances
from multiple views. To maintain such a small memory footprint but achieve
faster inference times, recent works have adopted `sampler' networks that
adaptively sample a small subset of points along each ray in the implicit
neural radiance fields. Although these methods achieve up to a 10$\times$
reduction in rendering time, they still suffer from considerable quality
degradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,
which provides an optimal trade-off between memory footprint (similar to NeRF),
speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is
equipped with a novel projection-aware sampling (PAS) network together with a
new training strategy for ray exploration and exploitation, allowing for
efficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art
metrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding
0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our
exploration and exploitation training strategy allows ProNeRF to learn the full
scenes' color and density distributions while also learning efficient ray
sampling focused on the highest-density regions. We provide extensive
experimental results that support the effectiveness of our method on the widely
adopted forward-facing and 360 datasets, LLFF and Blender, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08146">High-accuracy Vision-Based Attitude Estimation System for Air-Bearing Spacecraft Simulators. (arXiv:2312.08146v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ornati_F/0/1/0/all/0/1">Fabio Ornati</a>, <a href="http://arxiv.org/find/cs/1/au:+Domenico_G/0/1/0/all/0/1">Gianfranco Di Domenico</a>, <a href="http://arxiv.org/find/cs/1/au:+Panicucci_P/0/1/0/all/0/1">Paolo Panicucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Topputo_F/0/1/0/all/0/1">Francesco Topputo</a></p>
<p>Air-bearing platforms for simulating the rotational dynamics of satellites
require highly precise ground truth systems. Unfortunately, commercial motion
capture systems used for this scope are complex and expensive. This paper shows
a novel and versatile method to compute the attitude of rotational air-bearing
platforms using a monocular camera and sets of fiducial markers. The work
proposes a geometry-based iterative algorithm that is significantly more
accurate than other literature methods that involve the solution of the
Perspective-n-Point problem. Additionally, auto-calibration procedures to
perform a preliminary estimation of the system parameters are shown. The
developed methodology is deployed onto a Raspberry Pi 4 micro-computer and
tested with a set of LED markers. Data obtained with this setup are compared
against computer simulations of the same system to understand and validate the
attitude estimation performances. Simulation results show expected 1-sigma
accuracies in the order of $\sim$ 12 arcsec and $\sim$ 37 arcsec for about- and
cross-boresight rotations of the platform, and average latency times of 6 ms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08168">Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers. (arXiv:2312.08168v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haifeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zehan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Rongjie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Luping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xize Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1">Tao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>Recent research has evidenced the significant potentials of Large Language
Models (LLMs) in handling challenging tasks within 3D scenes. However, current
models are constrained to addressing object-centric tasks, where each
question-answer pair focuses solely on an individual object. In real-world
applications, users may pose queries involving multiple objects or expect for
answers that precisely reference various objects. We introduce the use of
object identifiers to freely reference objects during a conversation. While
this solution appears straightforward, it presents two main challenges: 1) How
to establish a reliable one-to-one correspondence between each object and its
identifier? 2) How to incorporate complex spatial relationships among dozens of
objects into the embedding space of the LLM? To address these challenges, we
propose a two-stage alignment method, which involves learning an
attribute-aware token and a relation-aware token for each object. These tokens
capture the object's attributes and spatial relationships with surrounding
objects in the 3D scene. Once the alignment is established, we can fine-tune
our model on various downstream tasks using instruction tuning. Experiments
conducted on traditional datasets like ScanQA, ScanRefer, and Nr3D/Sr3D
showcase the effectiveness of our proposed method. Additionally, we create a 3D
scene captioning dataset annotated with rich object identifiers, with the
assistant of GPT-4. This dataset aims to further explore the capability of
object identifiers in effective object referencing and precise scene
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08176">ASC: Adaptive Scale Feature Map Compression for Deep Neural Network. (arXiv:2312.08176v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1">Tian-Sheuan Chang</a></p>
<p>Deep-learning accelerators are increasingly in demand; however, their
performance is constrained by the size of the feature map, leading to high
bandwidth requirements and large buffer sizes. We propose an adaptive scale
feature map compression technique leveraging the unique properties of the
feature map. This technique adopts independent channel indexing given the weak
channel correlation and utilizes a cubical-like block shape to benefit from
strong local correlations. The method further optimizes compression using a
switchable endpoint mode and adaptive scale interpolation to handle unimodal
data distributions, both with and without outliers. This results in 4$\times$
and up to 7.69$\times$ compression rates for 16-bit data in constant and
variable bitrates, respectively. Our hardware design minimizes area cost by
adjusting interpolation scales, which facilitates hardware sharing among
interpolation points. Additionally, we introduce a threshold concept for
straightforward interpolation, preventing the need for intricate hardware. The
TSMC 28nm implementation showcases an equivalent gate count of 6135 for the
8-bit version. Furthermore, the hardware architecture scales effectively, with
only a sublinear increase in area cost. Achieving a 32$\times$ throughput
increase meets the theoretical bandwidth of DDR5-6400 at just 7.65$\times$ the
hardware cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08177">Advanced Image Segmentation Techniques for Neural Activity Detection via C-fos Immediate Early Gene Expression. (arXiv:2312.08177v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Peilin Cai</a></p>
<p>This paper investigates the application of advanced image segmentation
techniques to analyze C-fos immediate early gene expression, a crucial marker
for neural activity. Due to the complexity and high variability of neural
circuits, accurate segmentation of C-fos images is paramount for the
development of new insights into neural function. Amidst this backdrop, this
research aims to improve accuracy and minimize manual intervention in C-fos
image segmentation by leveraging the capabilities of CNNs and the Unet model.
We describe the development of a novel workflow for the segmentation process
involving Convolutional Neural Networks (CNNs) and the Unet model,
demonstrating their efficiency in various image segmentation tasks. Our
workflow incorporates pre-processing steps such as cropping, image feature
extraction, and clustering for the training dataset selection. We used an
AutoEncoder model to extract features and implement constrained clustering to
identify similarities and differences in image types. Additionally, we utilized
manual and automatic labeling approaches to enhance the performance of our
model. We demonstrated the effectiveness of our method in distinguishing areas
with significant C-fos expression from normal tissue areas. Lastly, we
implemented a modified Unet network for the detection of C-fos expressions.
This research contributes to the development of more efficient and automated
image segmentation methods, advancing the understanding of neural function in
neuroscience research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08192">PAD: Self-Supervised Pre-Training with Patchwise-Scale Adapter for Infrared Images. (arXiv:2312.08192v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1">Kun Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jinyong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1">Shiming Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chunhong Pan</a></p>
<p>Self-supervised learning (SSL) for RGB images has achieved significant
success, yet there is still limited research on SSL for infrared images,
primarily due to three prominent challenges: 1) the lack of a suitable
large-scale infrared pre-training dataset, 2) the distinctiveness of non-iconic
infrared images rendering common pre-training tasks like masked image modeling
(MIM) less effective, and 3) the scarcity of fine-grained textures making it
particularly challenging to learn general image features. To address these
issues, we construct a Multi-Scene Infrared Pre-training (MSIP) dataset
comprising 178,756 images, and introduce object-sensitive random RoI cropping,
an image preprocessing method, to tackle the challenge posed by non-iconic
images. To alleviate the impact of weak textures on feature learning, we
propose a pre-training paradigm called Pre-training with ADapter (PAD), which
uses adapters to learn domain-specific features while freezing parameters
pre-trained on ImageNet to retain the general feature extraction capability.
This new paradigm is applicable to any transformer-based SSL method.
Furthermore, to achieve more flexible coordination between pre-trained and
newly-learned features in different layers and patches, a patchwise-scale
adapter with dynamically learnable scale factors is introduced. Extensive
experiments on three downstream tasks show that PAD, with only 1.23M
pre-trainable parameters, outperforms other baseline paradigms including
continual full pre-training on MSIP. Our code and dataset are available at
https://github.com/casiatao/PAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08193">Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic Retinopathy Detection. (arXiv:2312.08193v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1">Samrat Mukherjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Bandyopadhyay_D/0/1/0/all/0/1">Dibyanayan Bandyopadhyay</a>, <a href="http://arxiv.org/find/eess/1/au:+Gain_B/0/1/0/all/0/1">Baban Gain</a>, <a href="http://arxiv.org/find/eess/1/au:+Ekbal_A/0/1/0/all/0/1">Asif Ekbal</a></p>
<p>Diabetic Retinopathy (DR) is a prevalent illness associated with Diabetes
which, if left untreated, can result in irreversible blindness. Deep Learning
based systems are gradually being introduced as automated support for clinical
diagnosis. Since healthcare has always been an extremely important domain
demanding error-free performance, any adversaries could pose a big threat to
the applicability of such systems. In this work, we use Universal Adversarial
Perturbations (UAPs) to quantify the vulnerability of Medical Deep Neural
Networks (DNNs) for detecting DR. To the best of our knowledge, this is the
very first attempt that works on attacking complete fine-grained classification
of DR images using various UAPs. Also, as a part of this work, we use UAPs to
fine-tune the trained models to defend against adversarial samples. We
experiment on several models and observe that the performance of such models
towards unseen adversarial attacks gets boosted on average by $3.41$
Cohen-kappa value and maximum by $31.92$ Cohen-kappa value. The performance
degradation on normal data upon ensembling the fine-tuned models was found to
be statistically insignificant using t-test, highlighting the benefits of
UAP-based adversarial fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08194">SVInvNet: A Densely Connected Encoder-Decoder Architecture for Seismic Velocity Inversion. (arXiv:2312.08194v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khatounabad_M/0/1/0/all/0/1">Mojtaba Najafi Khatounabad</a>, <a href="http://arxiv.org/find/cs/1/au:+Keles_H/0/1/0/all/0/1">Hacer Yalim Keles</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadioglu_S/0/1/0/all/0/1">Selma Kadioglu</a></p>
<p>This study presents a deep learning-based approach to seismic velocity
inversion problem, focusing on both noisy and noiseless training datasets of
varying sizes. Our Seismic Velocity Inversion Network (SVInvNet) introduces a
novel architecture that contains a multi-connection encoder-decoder structure
enhanced with dense blocks. This design is specifically tuned to effectively
process complex information, crucial for addressing the challenges of
non-linear seismic velocity inversion. For training and testing, we created
diverse seismic velocity models, including multi-layered, faulty, and salt dome
categories. We also investigated how different kinds of ambient noise, both
coherent and stochastic, and the size of the training dataset affect learning
outcomes. SVInvNet is trained on datasets ranging from 750 to 6,000 samples and
is tested using a large benchmark dataset of 12,000 samples. Despite its fewer
parameters compared to the baseline, SVInvNet achieves superior performance
with this dataset. The outcomes of the SVInvNet are additionally compared to
those of the Full Waveform Inversion (FWI) method. The comparative analysis
clearly reveals the effectiveness of the proposed model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08195">Concept-centric Personalization with Large-scale Diffusion Priors. (arXiv:2312.08195v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1">Pu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Feng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tianrui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1">Qing Song</a></p>
<p>Despite large-scale diffusion models being highly capable of generating
diverse open-world content, they still struggle to match the photorealism and
fidelity of concept-specific generators. In this work, we present the task of
customizing large-scale diffusion priors for specific concepts as
concept-centric personalization. Our goal is to generate high-quality
concept-centric images while maintaining the versatile controllability inherent
to open-world models, enabling applications in diverse tasks such as
concept-centric stylization and image translation. To tackle these challenges,
we identify catastrophic forgetting of guidance prediction from diffusion
priors as the fundamental issue. Consequently, we develop a guidance-decoupled
personalization framework specifically designed to address this task. We
propose Generalized Classifier-free Guidance (GCFG) as the foundational theory
for our framework. This approach extends Classifier-free Guidance (CFG) to
accommodate an arbitrary number of guidances, sourced from a variety of
conditions and models. Employing GCFG enables us to separate conditional
guidance into two distinct components: concept guidance for fidelity and
control guidance for controllability. This division makes it feasible to train
a specialized model for concept guidance, while ensuring both control and
unconditional guidance remain intact. We then present a null-text
Concept-centric Diffusion Model as a concept-specific generator to learn
concept guidance without the need for text annotations. Code will be available
at https://github.com/PRIV-Creation/Concept-centric-Personalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08212">LAMM: Label Alignment for Multi-Modal Prompt Learning. (arXiv:2312.08212v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jingsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jiacheng Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1">Suncheng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zefang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1">Ke Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Mingye Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yuzhuo Fu</a></p>
<p>With the success of pre-trained visual-language (VL) models such as CLIP in
visual representation tasks, transferring pre-trained models to downstream
tasks has become a crucial paradigm. Recently, the prompt tuning paradigm,
which draws inspiration from natural language processing (NLP), has made
significant progress in VL field. However, preceding methods mainly focus on
constructing prompt templates for text and visual inputs, neglecting the gap in
class label representations between the VL models and downstream tasks. To
address this challenge, we introduce an innovative label alignment method named
\textbf{LAMM}, which can dynamically adjust the category embeddings of
downstream datasets through end-to-end training. Moreover, to achieve a more
appropriate label distribution, we propose a hierarchical loss, encompassing
the alignment of the parameter space, feature space, and logits space. We
conduct experiments on 11 downstream vision datasets and demonstrate that our
method significantly improves the performance of existing multi-modal prompt
learning models in few-shot scenarios, exhibiting an average accuracy
improvement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.
Moreover, our methodology exhibits the preeminence in continual learning
compared to other prompt tuning methods. Importantly, our method is synergistic
with existing prompt tuning methods and can boost the performance on top of
them. Our code and dataset will be publicly available at
https://github.com/gaojingsheng/LAMM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08213">Accelerated Event-Based Feature Detection and Compression for Surveillance Video Systems. (arXiv:2312.08213v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_A/0/1/0/all/0/1">Andrew C. Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayer_Patel_K/0/1/0/all/0/1">Ketan Mayer-Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Montek Singh</a></p>
<p>The strong temporal consistency of surveillance video enables compelling
compression performance with traditional methods, but downstream vision
applications operate on decoded image frames with a high data rate. Since it is
not straightforward for applications to extract information on temporal
redundancy from the compressed video representations, we propose a novel system
which conveys temporal redundancy within a sparse decompressed representation.
We leverage a video representation framework called ADDER to transcode framed
videos to sparse, asynchronous intensity samples. We introduce mechanisms for
content adaptation, lossy compression, and asynchronous forms of classical
vision algorithms. We evaluate our system on the VIRAT surveillance video
dataset, and we show a median 43.7% speed improvement in FAST feature detection
compared to OpenCV. We run the same algorithm as OpenCV, but only process
pixels that receive new asynchronous events, rather than process every pixel in
an image frame. Our work paves the way for upcoming neuromorphic sensors and is
amenable to future applications with spiking neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08220">EventAid: Benchmarking Event-aided Image/Video Enhancement Algorithms with Real-captured Hybrid Dataset. (arXiv:2312.08220v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_P/0/1/0/all/0/1">Peiqi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yixin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_H/0/1/0/all/0/1">Hanyue Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1">Minggui Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Boxin Shi</a></p>
<p>Event cameras are emerging imaging technology that offers advantages over
conventional frame-based imaging sensors in dynamic range and sensing speed.
Complementing the rich texture and color perception of traditional image
frames, the hybrid camera system of event and frame-based cameras enables
high-performance imaging. With the assistance of event cameras, high-quality
image/video enhancement methods make it possible to break the limits of
traditional frame-based cameras, especially exposure time, resolution, dynamic
range, and frame rate limits. This paper focuses on five event-aided image and
video enhancement tasks (i.e., event-based video reconstruction, event-aided
high frame rate video reconstruction, image deblurring, image super-resolution,
and high dynamic range image reconstruction), provides an analysis of the
effects of different event properties, a real-captured and ground truth labeled
benchmark dataset, a unified benchmarking of state-of-the-art methods, and an
evaluation for two mainstream event simulators. In detail, this paper collects
a real-captured evaluation dataset EventAid for five event-aided image/video
enhancement tasks, by using "Event-RGB" multi-camera hybrid system, taking into
account scene diversity and spatiotemporal synchronization. We further perform
quantitative and visual comparisons for state-of-the-art algorithms, provide a
controlled experiment to analyze the performance limit of event-aided image
deblurring methods, and discuss open problems to inspire future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08223">Patch-wise Graph Contrastive Learning for Image Translation. (arXiv:2312.08223v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Chanyong Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1">Gihyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Recently, patch-wise contrastive learning is drawing attention for the image
translation by exploring the semantic correspondence between the input and
output images. To further explore the patch-wise topology for high-level
semantic understanding, here we exploit the graph neural network to capture the
topology-aware features. Specifically, we construct the graph based on the
patch-wise similarity from a pretrained encoder, whose adjacency matrix is
shared to enhance the consistency of patch-wise relation between the input and
the output. Then, we obtain the node feature from the graph neural network, and
enhance the correspondence between the nodes by increasing mutual information
using the contrastive loss. In order to capture the hierarchical semantic
structure, we further propose the graph pooling. Experimental results
demonstrate the state-of-art results for the image translation thanks to the
semantic encoding by the constructed graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08230">Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches. (arXiv:2312.08230v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kobsik_G/0/1/0/all/0/1">Gregor Kobsik</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1">Isaak Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1">Leif Kobbelt</a></p>
<p>Symmetry detection, especially partial and extrinsic symmetry, is essential
for various downstream tasks, like 3D geometry completion, segmentation,
compression and structure-aware shape encoding or generation. In order to
detect partial extrinsic symmetries, we propose to learn rotation, reflection,
translation and scale invariant local shape features for geodesic point cloud
patches via contrastive learning, which are robust across multiple classes and
generalize over different datasets. We show that our approach is able to
extract multiple valid solutions for this ambiguous problem. Furthermore, we
introduce a novel benchmark test for partial extrinsic symmetry detection to
evaluate our method. Lastly, we incorporate the detected symmetries together
with a region growing algorithm to demonstrate a downstream task with the goal
of computing symmetry-aware partitions of 3D shapes. To our knowledge, we are
the first to propose a self-supervised data-driven method for partial extrinsic
symmetry detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08234">Beyond the Label Itself: Latent Labels Enhance Semi-supervised Point Cloud Panoptic Segmentation. (arXiv:2312.08234v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yujun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhizhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1">Yanyun Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuan Xie</a></p>
<p>As the exorbitant expense of labeling autopilot datasets and the growing
trend of utilizing unlabeled data, semi-supervised segmentation on point clouds
becomes increasingly imperative. Intuitively, finding out more ``unspoken
words'' (i.e., latent instance information) beyond the label itself should be
helpful to improve performance. In this paper, we discover two types of latent
labels behind the displayed label embedded in LiDAR and image data. First, in
the LiDAR Branch, we propose a novel augmentation, Cylinder-Mix, which is able
to augment more yet reliable samples for training. Second, in the Image Branch,
we propose the Instance Position-scale Learning (IPSL) Module to learn and fuse
the information of instance position and scale, which is from a 2D pre-trained
detector and a type of latent label obtained from 3D to 2D projection. Finally,
the two latent labels are embedded into the multi-modal panoptic segmentation
network. The ablation of the IPSL module demonstrates its robust adaptability,
and the experiments evaluated on SemanticKITTI and nuScenes demonstrate that
our model outperforms the state-of-the-art method, LaserMix.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08240">CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation. (arXiv:2312.08240v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chisari_E/0/1/0/all/0/1">Eugenio Chisari</a>, <a href="http://arxiv.org/find/cs/1/au:+Heppert_N/0/1/0/all/0/1">Nick Heppert</a>, <a href="http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1">Tim Welschehold</a>, <a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1">Wolfram Burgard</a>, <a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1">Abhinav Valada</a></p>
<p>Reliable object grasping is a crucial capability for autonomous robots.
However, many existing grasping approaches focus on general clutter removal
without explicitly modeling objects and thus only relying on the visible local
geometry. We introduce CenterGrasp, a novel framework that combines object
awareness and holistic grasping. CenterGrasp learns a general object prior by
encoding shapes and valid grasps in a continuous latent space. It consists of
an RGB-D image encoder that leverages recent advances to detect objects and
infer their pose and latent code, and a decoder to predict shape and grasps for
each object in the scene. We perform extensive experiments on simulated as well
as real-world cluttered scenes and demonstrate strong scene reconstruction and
6-DoF grasp-pose estimation performance. Compared to the state of the art,
CenterGrasp achieves an improvement of 38.5 mm in shape reconstruction and 33
percentage points on average in grasp success. We make the code and trained
models publicly available at <a href="http://centergrasp.cs.uni-freiburg.de.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08255">OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods. (arXiv:2312.08255v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kulyabin_M/0/1/0/all/0/1">Mikhail Kulyabin</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhdanov_A/0/1/0/all/0/1">Aleksei Zhdanov</a>, <a href="http://arxiv.org/find/eess/1/au:+Nikiforova_A/0/1/0/all/0/1">Anastasia Nikiforova</a>, <a href="http://arxiv.org/find/eess/1/au:+Stepichev_A/0/1/0/all/0/1">Andrey Stepichev</a>, <a href="http://arxiv.org/find/eess/1/au:+Kuznetsova_A/0/1/0/all/0/1">Anna Kuznetsova</a>, <a href="http://arxiv.org/find/eess/1/au:+Ronkin_M/0/1/0/all/0/1">Mikhail Ronkin</a>, <a href="http://arxiv.org/find/eess/1/au:+Borisov_V/0/1/0/all/0/1">Vasilii Borisov</a>, <a href="http://arxiv.org/find/eess/1/au:+Bogachev_A/0/1/0/all/0/1">Alexander Bogachev</a>, <a href="http://arxiv.org/find/eess/1/au:+Korotkich_S/0/1/0/all/0/1">Sergey Korotkich</a>, <a href="http://arxiv.org/find/eess/1/au:+Constable_P/0/1/0/all/0/1">Paul A Constable</a>, <a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a></p>
<p>Optical coherence tomography (OCT) is a non-invasive imaging technique with
extensive clinical applications in ophthalmology. OCT enables the visualization
of the retinal layers, playing a vital role in the early detection and
monitoring of retinal diseases. OCT uses the principle of light wave
interference to create detailed images of the retinal microstructures, making
it a valuable tool for diagnosing ocular conditions. This work presents an
open-access OCT dataset (OCTDL) comprising over 1600 high-resolution OCT images
labeled according to disease group and retinal pathology. The dataset consists
of OCT records of patients with Age-related Macular Degeneration (AMD),
Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery
Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface
Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using
raster scanning protocols with dynamic scan length and image resolution. Each
retinal b-scan was acquired by centering on the fovea and interpreted and
cataloged by an experienced retinal specialist. In this work, we applied Deep
Learning classification techniques to this new open-access dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08256">A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing. (arXiv:2312.08256v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lesne_G/0/1/0/all/0/1">Gwilherm Lesn&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1">Yann Gousseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladjal_S/0/1/0/all/0/1">Sa&#xef;d Ladjal</a>, <a href="http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1">Alasdair Newson</a></p>
<p>Recent advances in the field of generative models and in particular
generative adversarial networks (GANs) have lead to substantial progress for
controlled image editing, especially compared with the pre-deep learning era.
Despite their powerful ability to apply realistic modifications to an image,
these methods often lack properties like disentanglement (the capacity to edit
attributes independently). In this paper, we propose an auto-encoder which
re-organizes the latent space of StyleGAN, so that each attribute which we wish
to edit corresponds to an axis of the new latent space, and furthermore that
the latent axes are decorrelated, encouraging disentanglement. We work in a
compressed version of the latent space, using Principal Component Analysis,
meaning that the parameter complexity of our autoencoder is reduced, leading to
short training times ($\sim$ 45 mins). Qualitative and quantitative results
demonstrate the editing capabilities of our approach, with greater
disentanglement than competing methods, while maintaining fidelity to the
original image with respect to identity. Our autoencoder architecture simple
and straightforward, facilitating implementation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08267">TABSurfer: a Hybrid Deep Learning Architecture for Subcortical Segmentation. (arXiv:2312.08267v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cao_A/0/1/0/all/0/1">Aaron Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Rao_V/0/1/0/all/0/1">Vishwanatha M. Rao</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1">Kejia Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xinru Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Laine_A/0/1/0/all/0/1">Andrew F. Laine</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jia Guo</a></p>
<p>Subcortical segmentation remains challenging despite its important
applications in quantitative structural analysis of brain MRI scans. The most
accurate method, manual segmentation, is highly labor intensive, so automated
tools like FreeSurfer have been adopted to handle this task. However, these
traditional pipelines are slow and inefficient for processing large datasets.
In this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformer
hybrid deep learning model designed for superior subcortical segmentation
compared to existing state-of-the-art tools. To evaluate, we first demonstrate
TABSurfer's consistent performance across various T1w MRI datasets with
significantly shorter processing times compared to FreeSurfer. Then, we
validate against manual segmentations, where TABSurfer outperforms FreeSurfer
based on the manual ground truth. In each test, we also establish TABSurfer's
advantage over a leading deep learning benchmark, FastSurferVINN. Together,
these studies highlight TABSurfer's utility as a powerful tool for fully
automated subcortical segmentation with high fidelity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08268">Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation. (arXiv:2312.08268v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1">Arul Selvam Periyasamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsaturyan_V/0/1/0/all/0/1">Vladimir Tsaturyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1">Sven Behnke</a></p>
<p>Object pose estimation is a long-standing problem in computer vision.
Recently, attention-based vision transformer models have achieved
state-of-the-art results in many computer vision applications. Exploiting the
permutation-invariant nature of the attention mechanism, a family of vision
transformer models formulate multi-object pose estimation as a set prediction
problem. However, existing vision transformer models for multi-object pose
estimation rely exclusively on the attention mechanism. Convolutional neural
networks, on the other hand, hard-wire various inductive biases into their
architecture. In this paper, we investigate incorporating inductive biases in
vision transformer models for multi-object pose estimation, which facilitates
learning long-range dependencies while circumventing the costly global
attention. In particular, we use multi-resolution deformable attention, where
the attention operation is performed only between a few deformed reference
points. Furthermore, we propose a query aggregation mechanism that enables
increasing the number of object queries without increasing the computational
complexity. We evaluate the proposed model on the challenging YCB-Video dataset
and report state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08288">Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data Setting. (arXiv:2312.08288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1">Piyush Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazumder_P/0/1/0/all/0/1">Pratik Mazumder</a></p>
<p>Deep learning models are known to suffer from the problem of bias, and
researchers have been exploring methods to address this issue. However, most of
these methods require prior knowledge of the bias and are not always practical.
In this paper, we focus on a more practical setting with no prior information
about the bias. Generally, in this setting, there are a large number of
bias-aligned samples that cause the model to produce biased predictions and a
few bias-conflicting samples that do not conform to the bias. If the training
data is limited, the influence of the bias-aligned samples may become even
stronger on the model predictions, and we experimentally demonstrate that
existing debiasing techniques suffer severely in such cases. In this paper, we
examine the effects of unknown bias in small dataset regimes and present a
novel approach to mitigate this issue. The proposed approach directly addresses
the issue of the extremely low occurrence of bias-conflicting samples in
limited data settings through the synthesis of hybrid samples that can be used
to reduce the effect of bias. We perform extensive experiments on several
benchmark datasets and experimentally demonstrate the effectiveness of our
proposed approach in addressing any unknown bias in the presence of limited
data. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN
debiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when
only 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a
bias-conflicting sample ratio of 0.05.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08291">VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space. (arXiv:2312.08291v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fiche_G/0/1/0/all/0/1">Gu&#xe9;nol&#xe9; Fiche</a>, <a href="http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1">Simon Leglaive</a>, <a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1">Xavier Alameda-Pineda</a>, <a href="http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1">Antonio Agudo</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1">Francesc Moreno-Noguer</a></p>
<p>Human Pose and Shape Estimation (HPSE) from RGB images can be broadly
categorized into two main groups: parametric and non-parametric approaches.
Parametric techniques leverage a low-dimensional statistical body model for
realistic results, whereas recent non-parametric methods achieve higher
precision by directly regressing the 3D coordinates of the human body. Despite
their strengths, both approaches face limitations: the parameters of
statistical body models pose challenges as regression targets, and predicting
3D coordinates introduces computational complexities and issues related to
smoothness. In this work, we take a novel approach to address the HPSE problem.
We introduce a unique method involving a low-dimensional discrete latent
representation of the human mesh, framing HPSE as a classification task.
Instead of predicting body model parameters or 3D vertex coordinates, our focus
is on forecasting the proposed discrete latent representation, which can be
decoded into a registered human mesh. This innovative paradigm offers two key
advantages: firstly, predicting a low-dimensional discrete representation
confines our predictions to the space of anthropomorphic poses and shapes;
secondly, by framing the problem as a classification task, we can harness the
discriminative power inherent in neural networks. Our proposed model, VQ-HPS, a
transformer-based architecture, forecasts the discrete latent representation of
the mesh, trained through minimizing a cross-entropy loss. Our results
demonstrate that VQ-HPS outperforms the current state-of-the-art non-parametric
approaches while yielding results as realistic as those produced by parametric
methods. This highlights the significant potential of the classification
approach for HPSE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08323">PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary Confusion. (arXiv:2312.08323v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1">Xin You</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minghui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanxiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yun Gu</a></p>
<p>Precise boundary segmentation of volumetric images is a critical task for
image-guided diagnosis and computer-assisted intervention, especially for
boundary confusion in clinical practice. However, U-shape networks cannot
effectively resolve this challenge due to the lack of boundary shape
constraints. Besides, existing methods of refining boundaries overemphasize the
slender structure, which results in the overfitting phenomenon due to networks'
limited abilities to model tiny objects. In this paper, we reconceptualize the
mechanism of boundary generation by encompassing the interaction dynamics with
adjacent regions. Moreover, we propose a unified network termed PnPNet to model
shape characteristics of the confused boundary region. Core ingredients of
PnPNet contain the pushing and pulling branches. Specifically, based on
diffusion theory, we devise the semantic difference module (SDM) from the
pushing branch to squeeze the boundary region. Explicit and implicit
differential information inside SDM significantly boost representation
abilities for inter-class boundaries. Additionally, motivated by the K-means
algorithm, the class clustering module (CCM) from the pulling branch is
introduced to stretch the intersected boundary region. Thus, pushing and
pulling branches will shrink and enlarge the boundary uncertainty respectively.
They furnish two adversarial forces to promote models to output a more precise
delineation of boundaries. We carry out experiments on three challenging public
datasets and one in-house dataset, containing three types of boundary confusion
in model predictions. Experimental results demonstrate the superiority of
PnPNet over other segmentation networks, especially on evaluation metrics of HD
and ASSD. Besides, pushing and pulling branches can serve as plug-and-play
modules to enhance classic U-shape baseline models. Codes are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08334">LD-SDM: Language-Driven Hierarchical Species Distribution Modeling. (arXiv:2312.08334v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1">Srikumar Sastry</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xin Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhakal_A/0/1/0/all/0/1">Aayush Dhakal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanal_S/0/1/0/all/0/1">Subash Khanal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1">Adeel Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1">Nathan Jacobs</a></p>
<p>We focus on the problem of species distribution modeling using global-scale
presence-only data. Most previous studies have mapped the range of a given
species using geographical and environmental features alone. To capture a
stronger implicit relationship between species, we encode the taxonomic
hierarchy of species using a large language model. This enables range mapping
for any taxonomic rank and unseen species without additional supervision.
Further, we propose a novel proximity-aware evaluation metric that enables
evaluating species distribution models using any pixel-level representation of
ground-truth species range map. The proposed metric penalizes the predictions
of a model based on its proximity to the ground truth. We describe the
effectiveness of our model by systematically evaluating on the task of species
range prediction, zero-shot prediction and geo-feature regression against the
state-of-the-art. Results show our model outperforms the strong baselines when
trained with a variety of multi-label learning losses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08338">Global Latent Neural Rendering. (arXiv:2312.08338v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanay_T/0/1/0/all/0/1">Thomas Tanay</a>, <a href="http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1">Matteo Maggioni</a></p>
<p>A recent trend among generalizable novel view synthesis methods is to learn a
rendering operator acting over single camera rays. This approach is promising
because it removes the need for explicit volumetric rendering, but it
effectively treats target images as collections of independent pixels. Here, we
propose to learn a global rendering operator acting over all camera rays
jointly. We show that the right representation to enable such rendering is the
5-dimensional plane sweep volume, consisting of the projection of the input
images on a set of planes facing the target camera. Based on this
understanding, we introduce our Convolutional Global Latent Renderer (ConvGLR),
an efficient convolutional architecture that performs the rendering operation
globally in a low-resolution latent space. Experiments on various datasets
under sparse and generalizable setups show that our approach consistently
outperforms existing methods by significant margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08343">Ehancing CT Image synthesis from multi-modal MRI data based on a multi-task neural network framework. (arXiv:2312.08343v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xin_Z/0/1/0/all/0/1">Zhuoyao Xin</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1">Christopher Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1">Dong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_C/0/1/0/all/0/1">Chunming Gu</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jia Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Hua_J/0/1/0/all/0/1">Jun Hua</a></p>
<p>Image segmentation, real-value prediction, and cross-modal translation are
critical challenges in medical imaging. In this study, we propose a versatile
multi-task neural network framework, based on an enhanced Transformer U-Net
architecture, capable of simultaneously, selectively, and adaptively addressing
these medical image tasks. Validation is performed on a public repository of
human brain MR and CT images. We decompose the traditional problem of
synthesizing CT images into distinct subtasks, which include skull
segmentation, Hounsfield unit (HU) value prediction, and image sequential
reconstruction. To enhance the framework's versatility in handling multi-modal
data, we expand the model with multiple image channels. Comparisons between
synthesized CT images derived from T1-weighted and T2-Flair images were
conducted, evaluating the model's capability to integrate multi-modal
information from both morphological and pixel value perspectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08344">FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects. (arXiv:2312.08344v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bowen Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1">Stan Birchfield</a></p>
<p>We present FoundationPose, a unified foundation model for 6D object pose
estimation and tracking, supporting both model-based and model-free setups. Our
approach can be instantly applied at test-time to a novel object without
fine-tuning, as long as its CAD model is given, or a small number of reference
images are captured. We bridge the gap between these two setups with a neural
implicit representation that allows for effective novel view synthesis, keeping
the downstream pose estimation modules invariant under the same unified
framework. Strong generalizability is achieved via large-scale synthetic
training, aided by a large language model (LLM), a novel transformer-based
architecture, and contrastive learning formulation. Extensive evaluation on
multiple public datasets involving challenging scenarios and objects indicate
our unified approach outperforms existing methods specialized for each task by
a large margin. In addition, it even achieves comparable results to
instance-level methods despite the reduced assumptions. Project page:
https://nvlabs.github.io/FoundationPose/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08364">View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data. (arXiv:2312.08364v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zeyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Raistrick_A/0/1/0/all/0/1">Alexander Raistrick</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipson_L/0/1/0/all/0/1">Lahav Lipson</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jia Deng</a></p>
<p>Procedural synthetic data generation has received increasing attention in
computer vision. Procedural signed distance functions (SDFs) are a powerful
tool for modeling large-scale detailed scenes, but existing mesh extraction
methods have artifacts or performance profiles that limit their use for
synthetic data. We propose OcMesher, a mesh extraction algorithm that
efficiently handles high-detail unbounded scenes with perfect view-consistency,
with easy export to downstream real-time engines. The main novelty of our
solution is an algorithm to construct an octree based on a given SDF and
multiple camera views. We performed extensive experiments, and show our
solution produces better synthetic data for training and evaluation of computer
vision models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08366">See, Say, and Segment: Teaching LMMs to Overcome False Premises. (arXiv:2312.08366v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tsung-Han Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Biamby_G/0/1/0/all/0/1">Giscard Biamby</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1">David Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dunlap_L/0/1/0/all/0/1">Lisa Dunlap</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Ritwik Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xudong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a></p>
<p>Current open-source Large Multimodal Models (LMMs) excel at tasks such as
open-vocabulary language grounding and segmentation but can suffer under false
premises when queries imply the existence of something that is not actually
present in the image. We observe that existing methods that fine-tune an LMM to
segment images significantly degrade their ability to reliably determine
("see") if an object is present and to interact naturally with humans ("say"),
a form of catastrophic forgetting. In this work, we propose a cascading and
joint training approach for LMMs to solve this task, avoiding catastrophic
forgetting of previous skills. Our resulting model can "see" by detecting
whether objects are present in an image, "say" by telling the user if they are
not, proposing alternative queries or correcting semantic errors in the query,
and finally "segment" by outputting the mask of the desired objects if they
exist. Additionally, we introduce a novel False Premise Correction benchmark
dataset, an extension of existing RefCOCO(+/g) referring segmentation datasets
(which we call FP-RefCOCO(+/g)). The results show that our method not only
detects false premises up to 55% better than existing approaches, but under
false premise conditions produces relative cIOU improvements of more than 31%
over baselines, and produces natural language feedback judged helpful up to 67%
of the time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08367">VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering. (arXiv:2312.08367v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Junbang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chun-Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1">Kenan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1">Yu Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Ming Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shan Yang</a></p>
<p>In this work, we propose an efficient Video-Language Alignment via
Frame-Prompting and Distilling (VLAP) network. Our VLAP model addresses both
efficient frame sampling and effective cross-modal alignment in a unified way.
In our VLAP network, we design a new learnable question-aware Frame-Prompter
together with a new cross-modal distillation (QFormer-Distiller) module.
Pre-trained large image-language models have shown promising results on
problems such as visual question answering. However, how to efficiently and
effectively sample image frames when adapting pre-trained large image-language
model to video-language alignment is still the major challenge. Compared with
prior work, our VLAP model demonstrates the capability of selecting key frames
with critical contents, thus improving the video-language alignment accuracy
while reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speed
up). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and
+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA
4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the video
question-answering benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08371">PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection. (arXiv:2312.08371v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kuan-Chih Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1">Weijie Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Hsuan Tsai</a></p>
<p>Recent temporal LiDAR-based 3D object detectors achieve promising performance
based on the two-stage proposal-based approach. They generate 3D box candidates
from the first-stage dense detector, followed by different temporal aggregation
methods. However, these approaches require per-frame objects or whole point
clouds, posing challenges related to memory bank utilization. Moreover, point
clouds and trajectory features are combined solely based on concatenation,
which may neglect effective interactions between them. In this paper, we
propose a point-trajectory transformer with long short-term memory for
efficient temporal 3D object detection. To this end, we only utilize point
clouds of current-frame objects and their historical trajectories as input to
minimize the memory bank storage requirement. Furthermore, we introduce modules
to encode trajectory features, focusing on long short-term and future-aware
perspectives, and then effectively aggregate them with point cloud features. We
conduct extensive experiments on the large-scale Waymo dataset to demonstrate
that our approach performs well against state-of-the-art methods. Code and
models will be made publicly available at https://github.com/kuanchihhuang/PTT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08372">SAM-guided Graph Cut for 3D Instance Segmentation. (arXiv:2312.08372v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Haoyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">He Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Sida Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Ruizhen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaowei Zhou</a></p>
<p>This paper addresses the challenge of 3D instance segmentation by
simultaneously leveraging 3D geometric and multi-view image information. Many
previous works have applied deep learning techniques to 3D point clouds for
instance segmentation. However, these methods often failed to generalize to
various types of scenes due to the scarcity and low-diversity of labeled 3D
point cloud data. Some recent works have attempted to lift 2D instance
segmentations to 3D within a bottom-up framework. The inconsistency in 2D
instance segmentations among views can substantially degrade the performance of
3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to
effectively exploit 2D segmentation models for 3D instance segmentation.
Specifically, we pre-segment the scene into several superpoints in 3D,
formulating the task into a graph cut problem. The superpoint graph is
constructed based on 2D segmentation models, where node features are obtained
from multi-view image features and edge weights are computed based on
multi-view segmentation results, enabling the better generalization ability. To
process the graph, we train a graph neural network using pseudo 3D labels from
2D segmentation models. Experimental results on the ScanNet, ScanNet++ and
KITTI-360 datasets demonstrate that our method achieves robust segmentation
performance and can generalize across different types of scenes. Our project
page is available at https://zju3dv.github.io/sam_graph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.13389">Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sushko_V/0/1/0/all/0/1">Vadim Sushko</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1">Juergen Gall</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1">Anna Khoreva</a></p>
<p>Given a large dataset for training, generative adversarial networks (GANs)
can achieve remarkable performance for the image synthesis task. However,
training GANs in extremely low data regimes remains a challenge, as overfitting
often occurs, leading to memorization or training divergence. In this work, we
introduce SIV-GAN, an unconditional generative model that can generate new
scene compositions from a single training image or a single video clip. We
propose a two-branch discriminator architecture, with content and layout
branches designed to judge internal content and scene layout realism separately
from each other. This discriminator design enables synthesis of visually
plausible, novel compositions of a scene, with varying content and layout,
while preserving the context of the original sample. Compared to previous
single image GANs, our model generates more diverse, higher quality images,
while not being restricted to a single image setting. We further introduce a
new challenging task of learning from a few frames of a single video. In this
training setup the training images are highly similar to each other, which
makes it difficult for prior GAN models to achieve a synthesis of both high
quality and diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.10756">Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1">Manan Oza</a>, <a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1">Sukalpa Chanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1">David Doermann</a></p>
<p>Faces generated using generative adversarial networks (GANs) have reached
unprecedented realism. These faces, also known as "Deep Fakes", appear as
realistic photographs with very little pixel-level distortions. While some work
has enabled the training of models that lead to the generation of specific
properties of the subject, generating a facial image based on a natural
language description has not been fully explored. For security and criminal
identification, the ability to provide a GAN-based system that works like a
sketch artist would be incredibly useful. In this paper, we present a novel
approach to generate facial images from semantic text descriptions. The learned
model is provided with a text description and an outline of the type of face,
which the model uses to sketch the features. Our models are trained using an
Affine Combination Module (ACM) mechanism to combine the text embedding from
BERT and the GAN latent space using a self-attention matrix. This avoids the
loss of features due to inadequate "attention", which may happen if text
embedding and latent vector are simply concatenated. Our approach is capable of
generating images that are very accurately aligned to the exhaustive textual
descriptions of faces with many fine detail features of the face and helps in
generating better images. The proposed method is also capable of making
incremental changes to a previously generated image if it is provided with
additional textual descriptions or sentences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01881">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1">Neha Kalibhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Narang_K/0/1/0/all/0/1">Kanika Narang</a>, <a href="http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1">Hamed Firooz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1">Maziar Sanjabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>Self-supervised learning (SSL) has shown impressive results in downstream
classification tasks. However, there is limited work in understanding their
failure modes and interpreting their learned representations. In this paper, we
study the representation space of state-of-the-art self-supervised models
including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins.
Without the use of class label information, we discover discriminative features
that correspond to unique physical attributes in images, present mostly in
correctly-classified representations. Using these features, we can compress the
representation space by up to 40% without significantly affecting linear
classification performance. We then propose Self-Supervised Representation
Quality Score (or Q-Score), an unsupervised score that can reliably predict if
a given sample is likely to be mis-classified during linear evaluation,
achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can
also be used as a regularization term on pre-trained encoders to remedy
low-quality representations. Fine-tuning with Q-Score regularization can boost
the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and
3.7% on ImageNet-1K compared to their baselines. Finally, using gradient
heatmaps and Salient ImageNet masks, we define a metric to quantify the
interpretability of each representation. We show that discriminative features
are strongly correlated to core attributes and, enhancing these features
through Q-score regularization makes SSL representations more interpretable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08716">GLARE: A Dataset for Traffic Sign Detection in Sun Glare. (arXiv:2209.08716v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gray_N/0/1/0/all/0/1">Nicholas Gray</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraes_M/0/1/0/all/0/1">Megan Moraes</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Alex Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_A/0/1/0/all/0/1">Allen Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_K/0/1/0/all/0/1">Kurt Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Haoyi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhishan Guo</a></p>
<p>Real-time machine learning object detection algorithms are often found within
autonomous vehicle technology and depend on quality datasets. It is essential
that these algorithms work correctly in everyday conditions as well as under
strong sun glare. Reports indicate glare is one of the two most prominent
environment-related reasons for crashes. However, existing datasets, such as
the Laboratory for Intelligent &amp; Safe Automobiles Traffic Sign (LISA) Dataset
and the German Traffic Sign Recognition Benchmark, do not reflect the existence
of sun glare at all. This paper presents the GLARE (GLARE is available at:
https://github.com/NicholasCG/GLARE_Dataset ) traffic sign dataset: a
collection of images with U.S-based traffic signs under heavy visual
interference by sunlight. GLARE contains 2,157 images of traffic signs with sun
glare, pulled from 33 videos of dashcam footage of roads in the United States.
It provides an essential enrichment to the widely used LISA Traffic Sign
dataset. Our experimental study shows that although several state-of-the-art
baseline architectures have demonstrated good performance on traffic sign
detection in conditions without sun glare in the past, they performed poorly
when tested against GLARE (e.g., average mAP0.5:0.95 of 19.4). We also notice
that current architectures have better detection when trained on images of
traffic signs in sun glare performance (e.g., average mAP0.5:0.95 of 39.6), and
perform best when trained on a mixture of conditions (e.g., average mAP0.5:0.95
of 42.3).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01789">Multiscale Structure Guided Diffusion for Image Deblurring. (arXiv:2212.01789v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengwei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Delbracio_M/0/1/0/all/0/1">Mauricio Delbracio</a>, <a href="http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1">Hossein Talebi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1">Guido Gerig</a>, <a href="http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1">Peyman Milanfar</a></p>
<p>Diffusion Probabilistic Models (DPMs) have recently been employed for image
deblurring, formulated as an image-conditioned generation process that maps
Gaussian noise to the high-quality image, conditioned on the blurry input.
Image-conditioned DPMs (icDPMs) have shown more realistic results than
regression-based methods when trained on pairwise in-domain data. However,
their robustness in restoring images is unclear when presented with
out-of-domain images as they do not impose specific degradation models or
intermediate constraints. To this end, we introduce a simple yet effective
multiscale structure guidance as an implicit bias that informs the icDPM about
the coarse structure of the sharp image at the intermediate layers. This guided
formulation leads to a significant improvement of the deblurring results,
particularly on unseen domain. The guidance is extracted from the latent space
of a regression network trained to predict the clean-sharp target at multiple
lower resolutions, thus maintaining the most salient sharp structures. With
both the blurry input and multiscale guidance, the icDPM model can better
understand the blur and recover the clean image. We evaluate a single-dataset
trained model on diverse datasets and demonstrate more robust deblurring
results with fewer artifacts on unseen data. Our method outperforms existing
baselines, achieving state-of-the-art perceptual quality while keeping
competitive distortion metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04037">ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library. (arXiv:2301.04037v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shetab_Bushehri_M/0/1/0/all/0/1">Mohammadreza Shetab-Bushehri</a>, <a href="http://arxiv.org/find/cs/1/au:+Aranda_M/0/1/0/all/0/1">Miguel Aranda</a>, <a href="http://arxiv.org/find/cs/1/au:+Mezouar_Y/0/1/0/all/0/1">Youcef Mezouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartoli_A/0/1/0/all/0/1">Adrien Bartoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozgur_E/0/1/0/all/0/1">Erol Ozgur</a></p>
<p>Tracking the 3D shape of a deforming object using only monocular 2D vision is
a challenging problem. This is because one should (i) infer the 3D shape from a
2D image, which is a severely underconstrained problem, and (ii) implement the
whole solution pipeline in real-time. The pipeline typically requires feature
detection and matching, mismatch filtering, 3D shape inference and feature
tracking algorithms. We propose ROBUSfT, a conventional pipeline based on a
template containing the object's rest shape, texturemap and deformation law.
ROBUSfT is ready-to-use, wide-baseline, capable of handling large deformations,
fast up to 30 fps, free of training, and robust against partial occlusions and
discontinuity in video frames. It outperforms the state-of-the-art methods in
challenging datasets. ROBUSfT is implemented as a publicly available C++
library and we provide a tutorial on how to use it in
https://github.com/mrshetab/ROBUSfT
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07700">Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images. (arXiv:2301.07700v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Ziyu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezapour_M/0/1/0/all/0/1">Mostafa Rezapour</a>, <a href="http://arxiv.org/find/cs/1/au:+Sajjad_U/0/1/0/all/0/1">Usama Sajjad</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurcan_M/0/1/0/all/0/1">Metin Nafi Gurcan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niazi_M/0/1/0/all/0/1">Muhammad Khalid Khan Niazi</a></p>
<p>Multiple instance learning (MIL) models have achieved remarkable success in
analyzing whole slide images (WSIs) for disease classification problems.
However, with regard to gigapixel WSI classification problems, current MIL
models are often incapable of differentiating a WSI with extremely small tumor
lesions. This minute tumor-to-normal area ratio in a MIL bag inhibits the
attention mechanism from properly weighting the areas corresponding to minor
tumor lesions. To overcome this challenge, we propose salient instance
inference MIL (SiiMIL), a weakly-supervised MIL model for WSI classification.
Our method initially learns representations of normal WSIs, and it then
compares the normal WSIs representations with all the input patches to infer
the salient instances of the input WSI. Finally, it employs attention-based MIL
to perform the slide-level classification based on the selected patches of the
WSI. Our experiments imply that SiiMIL can accurately identify tumor instances,
which could only take up less than 1% of a WSI, so that the ratio of tumor to
normal instances within a bag can increase by two to four times. It is worth
mentioning that it performs equally well for large tumor lesions. As a result,
SiiMIL achieves a significant improvement in performance over the
state-of-the-art MIL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13428">Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation. (arXiv:2301.13428v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangbin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yonggang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingjian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1">Haojie Fang</a></p>
<p>Unsupervised domain adaptation uses source data from different distributions
to solve the problem of classifying data from unlabeled target domains.
However, conventional methods require access to source data, which often raise
concerns about data privacy. In this paper, we consider a more practical but
challenging setting where the source domain data is unavailable and the target
domain data is unlabeled. Specifically, we address the domain discrepancy
problem from the perspective of contrastive learning. The key idea of our work
is to learn a domain-invariant feature by 1) performing clustering directly in
the original feature space with nearest neighbors; 2) constructing truly hard
negative pairs by extended neighbors without introducing additional
computational complexity; and 3) combining noise-contrastive estimation theory
to gain computational advantage. We conduct careful ablation studies and
extensive experiments on three common benchmarks: VisDA, Office-Home, and
Office-31. The results demonstrate the superiority of our methods compared with
other state-of-the-art works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01403">Self-Supervised Relation Alignment for Scene Graph Generation. (arXiv:2302.01403v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bicheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Renjie Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1">Leonid Sigal</a></p>
<p>The goal of scene graph generation is to predict a graph from an input image,
where nodes correspond to identified and localized objects and edges to their
corresponding interaction predicates. Existing methods are trained in a fully
supervised manner and focus on message passing mechanisms, loss functions,
and/or bias mitigation. In this work we introduce a simple-yet-effective
self-supervised relational alignment regularization designed to improve the
scene graph generation performance. The proposed alignment is general and can
be combined with any existing scene graph generation framework, where it is
trained alongside the original model's objective. The alignment is achieved
through distillation, where an auxiliary relation prediction branch, that
mirrors and shares parameters with the supervised counterpart, is designed. In
the auxiliary branch, relational input features are partially masked prior to
message passing and predicate prediction. The predictions for masked relations
are then aligned with the supervised counterparts after the message passing. We
illustrate the effectiveness of this self-supervised relational alignment in
conjunction with two scene graph generation architectures, SGTR and Neural
Motifs, and show that in both cases we achieve significantly improved
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02367">FastPillars: A Deployment-friendly Pillar-based 3D Detector. (arXiv:2302.02367v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sifan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiaobo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chengjian Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1">Zequn Jie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1">Patrick Yin Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lin Ma</a></p>
<p>The deployment of 3D detectors strikes one of the major challenges in
real-world self-driving scenarios. Existing BEV-based (i.e., Bird Eye View)
detectors favor sparse convolutions (known as SPConv) to speed up training and
inference, which puts a hard barrier for deployment, especially for on-device
applications. In this paper, to tackle the challenge of efficient 3D object
detection from an industry perspective, we devise a deployment-friendly
pillar-based 3D detector, termed FastPillars. First, we introduce a novel
lightweight Max-and-Attention Pillar Encoding (MAPE) module specially for
enhancing small 3D objects. Second, we propose a simple yet effective principle
for designing a backbone in pillar-based 3D detection. We construct FastPillars
based on these designs, achieving high performance and low latency without
SPConv. Extensive experiments on two large-scale datasets demonstrate the
effectiveness and efficiency of FastPillars for on-device 3D detection
regarding both performance and speed. Specifically, FastPillars delivers
state-of-the-art accuracy on Waymo Open Dataset with 1.8X speed up and 3.8
mAPH/L2 improvement over CenterPoint (SPConv-based). Our code is publicly
available at: https://github.com/StiphyJay/FastPillars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10915">Conformers are All You Need for Visual Speech Recognition. (arXiv:2302.10915v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1">Oscar Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1">Hank Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1">Dmitriy Serdyuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ankit Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1">Olivier Siohan</a></p>
<p>Visual speech recognition models extract visual features in a hierarchical
manner. At the lower level, there is a visual front-end with a limited temporal
receptive field that processes the raw pixels depicting the lips or faces. At
the higher level, there is an encoder that attends to the embeddings produced
by the front-end over a large temporal receptive field. Previous work has
focused on improving the visual front-end of the model to extract more useful
features for speech recognition. Surprisingly, our work shows that complex
visual front-ends are not necessary. Instead of allocating resources to a
sophisticated visual front-end, we find that a linear visual front-end paired
with a larger Conformer encoder results in lower latency, more efficient memory
usage, and improved WER performance. We achieve a new state-of-the-art of 12.8%
WER for visual speech recognition on the TED LRS3 dataset, which rivals the
performance of audio-only models from just four years ago.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09797">MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for Audio-Driven 3D Face Animation. (arXiv:2303.09797v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haozhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jia Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1">Junliang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jelo Wang</a></p>
<p>Audio-Driven Face Animation is an eagerly anticipated technique for
applications such as VR/AR, games, and movie making. With the rapid development
of 3D engines, there is an increasing demand for driving 3D faces with audio.
However, currently available 3D face animation datasets are either
scale-limited or quality-unsatisfied, which hampers further developments of
audio-driven 3D face animation. To address this challenge, we propose MMFace4D,
a large-scale multi-modal 4D (3D sequence) face dataset consisting of 431
identities, 35,904 sequences, and 3.9 million frames. MMFace4D exhibits two
compelling characteristics: 1) a remarkably diverse set of subjects and corpus,
encompassing actors spanning ages 15 to 68, and recorded sentences with
durations ranging from 0.7 to 11.4 seconds. 2) It features synchronized audio
and 3D mesh sequences with high-resolution face details. To capture the subtle
nuances of 3D facial expressions, we leverage three synchronized RGBD cameras
during the recording process. Upon MMFace4D, we construct a non-autoregressive
framework for audio-driven 3D face animation. Our framework considers the
regional and composite natures of facial animations, and surpasses contemporary
state-of-the-art approaches both qualitatively and quantitatively. The code,
model, and dataset will be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11420">ADCNet: Learning from Raw Radar Data via Distillation. (arXiv:2303.11420v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Khatri_I/0/1/0/all/0/1">Ishan Khatri</a>, <a href="http://arxiv.org/find/eess/1/au:+Happold_M/0/1/0/all/0/1">Michael Happold</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1">Chulong Chen</a></p>
<p>As autonomous vehicles and advanced driving assistance systems have entered
wider deployment, there is an increased interest in building robust perception
systems using radars. Radar-based systems are lower cost and more robust to
adverse weather conditions than their LiDAR-based counterparts; however the
point clouds produced are typically noisy and sparse by comparison. In order to
combat these challenges, recent research has focused on consuming the raw radar
data, instead of the final radar point cloud. We build on this line of work and
demonstrate that by bringing elements of the signal processing pipeline into
our network and then pre-training on the signal processing task, we are able to
achieve state of the art detection performance on the RADIal dataset. Our
method uses expensive offline signal processing algorithms to pseudo-label data
and trains a network to distill this information into a fast convolutional
backbone, which can then be finetuned for perception tasks. Extensive
experiment results corroborate the effectiveness of the proposed techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08083">VCD: Visual Causality Discovery for Cross-Modal Question Reasoning. (arXiv:2304.08083v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Ying Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jingzhou Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weixing Chen</a></p>
<p>Existing visual question reasoning methods usually fail to explicitly
discover the inherent causal mechanism and ignore jointly modeling cross-modal
event temporality and causality. In this paper, we propose a visual question
reasoning framework named Cross-Modal Question Reasoning (CMQR), to discover
temporal causal structure and mitigate visual spurious correlation by causal
intervention. To explicitly discover visual causal structure, the Visual
Causality Discovery (VCD) architecture is proposed to find question-critical
scene temporally and disentangle the visual spurious correlations by
attention-based front-door causal intervention module named Local-Global Causal
Attention Module (LGCAM). To align the fine-grained interactions between
linguistic semantics and spatial-temporal representations, we build an
Interactive Visual-Linguistic Transformer (IVLT) that builds the multi-modal
co-occurrence interactions between visual and linguistic content. Extensive
experiments on four datasets demonstrate the superiority of CMQR for
discovering visual causal structures and achieving robust question reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08485">Visual Instruction Tuning. (arXiv:2304.08485v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haotian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yong Jae Lee</a></p>
<p>Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13850">Do SSL Models Have D\&#x27;ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning. (arXiv:2304.13850v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meehan_C/0/1/0/all/0/1">Casey Meehan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bordes_F/0/1/0/all/0/1">Florian Bordes</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1">Pascal Vincent</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1">Kamalika Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chuan Guo</a></p>
<p>Self-supervised learning (SSL) algorithms can produce useful image
representations by learning to associate different parts of natural images with
one another. However, when taken to the extreme, SSL models can unintendedly
memorize specific parts in individual training samples rather than learning
semantically meaningful associations. In this work, we perform a systematic
study of the unintended memorization of image-specific information in SSL
models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show
that given the trained model and a crop of a training image containing only the
background (e.g., water, sky, grass), it is possible to infer the foreground
object with high accuracy or even visually reconstruct it. Furthermore, we show
that d\'ej\`a vu memorization is common to different SSL algorithms, is
exacerbated by certain design choices, and cannot be detected by conventional
techniques for evaluating representation quality. Our study of d\'ej\`a vu
memorization reveals previously unknown privacy risks in SSL models, as well as
suggests potential practical mitigation strategies. Code is available at
https://github.com/facebookresearch/DejaVu.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14365">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving. (arXiv:2304.14365v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1">Xiaoyu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_L/0/1/0/all/0/1">Longfei Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yucheng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huitong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hang Zhao</a></p>
<p>Robotic perception requires the modeling of both 3D geometry and semantics.
Existing methods typically focus on estimating 3D bounding boxes, neglecting
finer geometric details and struggling to handle general, out-of-vocabulary
objects. 3D occupancy prediction, which estimates the detailed occupancy states
and semantics of a scene, is an emerging task to overcome these limitations. To
support 3D occupancy prediction, we develop a label generation pipeline that
produces dense, visibility-aware labels for any given scene. This pipeline
comprises three stages: voxel densification, occlusion reasoning, and
image-guided voxel refinement. We establish two benchmarks, derived from the
Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and
Occ3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the
proposed dataset with various baseline models. Lastly, we propose a new model,
dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior
performance on the Occ3D benchmarks. The code, data, and benchmarks are
released at https://tsinghua-mars-lab.github.io/Occ3D/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02148">Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level. (arXiv:2305.02148v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sydorskyi_V/0/1/0/all/0/1">Volodymyr Sydorskyi</a>, <a href="http://arxiv.org/find/eess/1/au:+Krashenyi_I/0/1/0/all/0/1">Igor Krashenyi</a>, <a href="http://arxiv.org/find/eess/1/au:+Sakva_D/0/1/0/all/0/1">Denis Sakva</a>, <a href="http://arxiv.org/find/eess/1/au:+Zarichkovyi_O/0/1/0/all/0/1">Oleksandr Zarichkovyi</a></p>
<p>We present a new method for functional tissue unit segmentation at the
cellular level, which utilizes the latest deep learning semantic segmentation
approaches together with domain adaptation and semi-supervised learning
techniques. This approach allows for minimizing the domain gap, class
imbalance, and captures settings influence between HPA and HubMAP datasets. The
presented approach achieves comparable with state-of-the-art-result in
functional tissue unit segmentation at the cellular level. The source code is
available at https://github.com/VSydorskyy/hubmap_2022_htt_solution
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01808">Morphology Edge Attention Network and Optimal Geometric Matching Connection model for vascular segmentation. (arXiv:2306.01808v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1">Yuntao Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1">Yuxuan Qiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1">Xiaoping Yang</a></p>
<p>There are many unsolved problems in vascular image segmentation, including
vascular structural connectivity, scarce branches and missing small vessels.
Obtaining vessels that preserve their correct topological structures is
currently a crucial research issue, as it provides an overall view of one
vascular system. In order to preserve the topology and accuracy of vessel
segmentation, we proposed a novel Morphology Edge Attention Network (MEA-Net)
for the segmentation of vessel-like structures, and an Optimal Geometric
Matching Connection (OGMC) model to connect the broken vessel segments. The
MEA-Net has an edge attention module that improves the segmentation of edges
and small objects by morphology operation extracting boundary voxels on
multi-scale. The OGMC model uses the concept of curve touching from
differential geometry to filter out fragmented vessel endpoints, and then
employs minimal surfaces to determine the optimal connection order between
blood vessels. Finally, we calculate the geodesic to repair missing vessels
under a given Riemannian metric. Our method achieves superior or competitive
results compared to state-of-the-art methods on four datasets of 3D vascular
segmentation tasks, both effectively reducing vessel broken and increasing
vessel branch richness, yielding blood vessels with a more precise topological
structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07050">Revisiting Token Pruning for Object Detection and Instance Segmentation. (arXiv:2306.07050v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yifei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1">Mathias Gehrig</a>, <a href="http://arxiv.org/find/cs/1/au:+Messikommer_N/0/1/0/all/0/1">Nico Messikommer</a>, <a href="http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1">Marco Cannici</a>, <a href="http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1">Davide Scaramuzza</a></p>
<p>Vision Transformers (ViTs) have shown impressive performance in computer
vision, but their high computational cost, quadratic in the number of tokens,
limits their adoption in computation-constrained applications. However, this
large number of tokens may not be necessary, as not all tokens are equally
important. In this paper, we investigate token pruning to accelerate inference
for object detection and instance segmentation, extending prior works from
image classification. Through extensive experiments, we offer four insights for
dense tasks: (i) tokens should not be completely pruned and discarded, but
rather preserved in the feature maps for later use. (ii) reactivating
previously pruned tokens can further enhance model performance. (iii) a dynamic
pruning rate based on images is better than a fixed pruning rate. (iv) a
lightweight, 2-layer MLP can effectively prune tokens, achieving accuracy
comparable with complex gating networks with a simpler design. We assess the
effects of these design decisions on the COCO dataset and introduce an approach
that incorporates these findings, showing a reduction in performance decline
from ~1.5 mAP to ~0.3 mAP in both boxes and masks, compared to existing token
pruning methods. In relation to the dense counterpart that utilizes all tokens,
our method realizes an increase in inference speed, achieving up to 34% faster
performance for the entire network and 46% for the backbone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09337">Generative Proxemics: A Prior for 3D Social Interaction from Images. (arXiv:2306.09337v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1">Lea M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_V/0/1/0/all/0/1">Vickie Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlakos_G/0/1/0/all/0/1">Georgios Pavlakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1">Angjoo Kanazawa</a></p>
<p>Social interaction is a fundamental aspect of human behavior and
communication. The way individuals position themselves in relation to others,
also known as proxemics, conveys social cues and affects the dynamics of social
interaction. Reconstructing such interaction from images presents challenges
because of mutual occlusion and the limited availability of large training
datasets. To address this, we present a novel approach that learns a prior over
the 3D proxemics two people in close social interaction and demonstrate its use
for single-view 3D reconstruction. We start by creating 3D training data of
interacting people using image datasets with contact annotations. We then model
the proxemics using a novel denoising diffusion model called BUDDI that learns
the joint distribution over the poses of two people in close social
interaction. Sampling from our generative proxemics model produces realistic 3D
human interactions, which we validate through a perceptual study. We use BUDDI
in reconstructing two people in close proximity from a single image without any
contact annotation via an optimization approach that uses the diffusion model
as a prior. Our approach recovers accurate and plausible 3D social interactions
from noisy initial estimates, outperforming state-of-the-art methods. Our code,
data, and model are availableat our project website at: muelea.github.io/buddi.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14209">Deep image prior inpainting of ancient frescoes in the Mediterranean Alpine arc. (arXiv:2306.14209v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Merizzi_F/0/1/0/all/0/1">Fabio Merizzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Saillard_P/0/1/0/all/0/1">Perrine Saillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Acquier_O/0/1/0/all/0/1">Oceane Acquier</a>, <a href="http://arxiv.org/find/cs/1/au:+Morotti_E/0/1/0/all/0/1">Elena Morotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Piccolomini_E/0/1/0/all/0/1">Elena Loli Piccolomini</a>, <a href="http://arxiv.org/find/cs/1/au:+Calatroni_L/0/1/0/all/0/1">Luca Calatroni</a>, <a href="http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1">Rosa Maria Dess&#xec;</a></p>
<p>The unprecedented success of image reconstruction approaches based on deep
neural networks has revolutionised both the processing and the analysis
paradigms in several applied disciplines. In the field of digital humanities,
the task of digital reconstruction of ancient frescoes is particularly
challenging due to the scarce amount of available training data caused by
ageing, wear, tear and retouching over time. To overcome these difficulties, we
consider the Deep Image Prior (DIP) inpainting approach which computes
appropriate reconstructions by relying on the progressive updating of an
untrained convolutional neural network so as to match the reliable piece of
information in the image at hand while promoting regularisation elsewhere. In
comparison with state-of-the-art approaches (based on variational/PDEs and
patch-based methods), DIP-based inpainting reduces artefacts and better adapts
to contextual/non-local information, thus providing a valuable and effective
tool for art historians. As a case study, we apply such approach to reconstruct
missing image contents in a dataset of highly damaged digital images of
medieval paintings located into several chapels in the Mediterranean Alpine Arc
and provide a detailed description on how visible and invisible (e.g.,
infrared) information can be integrated for identifying and reconstructing
damaged image regions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17462">CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning. (arXiv:2306.17462v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weixing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>We present CausalVLR (Causal Visual-Linguistic Reasoning), an open-source
toolbox containing a rich set of state-of-the-art causal relation discovery and
causal inference methods for various visual-linguistic reasoning tasks, such as
VQA, image/video captioning, medical report generation, model generalization
and robustness, etc. These methods have been included in the toolbox with
PyTorch implementations under NVIDIA computing system. It not only includes
training and inference codes, but also provides model weights. We believe this
toolbox is by far the most complete visual-linguitic causal reasoning toolbox.
We wish that the toolbox and benchmark could serve the growing research
community by providing a flexible toolkit to re-implement existing methods and
develop their own new causal reasoning methods. Code and models are available
at https://github.com/HCPLab-SYSU/CausalVLR. The project is under active
development by HCP-Lab's contributors and we will keep this document updated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10705">TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1">Quang Huy Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dinh Phuc Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1">Minh Quan Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1">Duc Khai Lam</a></p>
<p>Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03977">PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning. (arXiv:2308.03977v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bordes_F/0/1/0/all/0/1">Florian Bordes</a>, <a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1">Shashank Shekhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mark Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1">Diane Bouchacourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1">Pascal Vincent</a>, <a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1">Ari S. Morcos</a></p>
<p>Synthetic image datasets offer unmatched advantages for designing and
evaluating deep neural networks: they make it possible to (i) render as many
data samples as needed, (ii) precisely control each scene and yield granular
ground truth labels (and captions), (iii) precisely control distribution shifts
between training and testing to isolate variables of interest for sound
experimentation. Despite such promise, the use of synthetic image data is still
limited -- and often played down -- mainly due to their lack of realism. Most
works therefore rely on datasets of real images, which have often been scraped
from public images on the internet, and may have issues with regards to
privacy, bias, and copyright, while offering little control over how objects
precisely appear. In this work, we present a path to democratize the use of
photorealistic synthetic data: we develop a new generation of interactive
environments for representation learning research, that offer both
controllability and realism. We use the Unreal Engine, a powerful game engine
well known in the entertainment industry, to produce PUG (Photorealistic Unreal
Graphics) environments and datasets for representation learning. In this paper,
we demonstrate the potential of PUG to enable more rigorous evaluations of
vision models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09065">Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression. (arXiv:2308.09065v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xuanlong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1">Gianni Franchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1">Emanuel Aldea</a></p>
<p>Uncertainty quantification is critical for deploying deep neural networks
(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)
is one of the most effective means to estimate the uncertainty of the main task
prediction without modifying the main task model. To be considered robust, an
AuxUE must be capable of maintaining its performance and triggering higher
uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to
provide robust aleatoric and epistemic uncertainty. However, for vision
regression tasks, current AuxUE designs are mainly adopted for aleatoric
uncertainty estimates, and AuxUE robustness has not been explored. In this
work, we propose a generalized AuxUE scheme for more robust uncertainty
quantification on regression tasks. Concretely, to achieve a more robust
aleatoric uncertainty estimation, different distribution assumptions are
considered for heteroscedastic noise, and Laplace distribution is finally
chosen to approximate the prediction error. For epistemic uncertainty, we
propose a novel solution named Discretization-Induced Dirichlet pOsterior
(DIDO), which models the Dirichlet posterior on the discretized prediction
error. Extensive experiments on age estimation, monocular depth estimation, and
super-resolution tasks show that our proposed method can provide robust
uncertainty estimates in the face of noisy inputs and that it can be scalable
to both image-level and pixel-wise tasks. Code is available at
https://github.com/ENSTA-U2IS/DIDO .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14075">FaceCoresetNet: Differentiable Coresets for Face Set Recognition. (arXiv:2308.14075v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shapira_G/0/1/0/all/0/1">Gil Shapira</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1">Yosi Keller</a></p>
<p>In set-based face recognition, we aim to compute the most discriminative
descriptor from an unbounded set of images and videos showing a single person.
A discriminative descriptor balances two policies when aggregating information
from a given set. The first is a quality-based policy: emphasizing high-quality
and down-weighting low-quality images. The second is a diversity-based policy:
emphasizing unique images in the set and down-weighting multiple occurrences of
similar images as found in video clips which can overwhelm the set
representation. This work frames face-set representation as a differentiable
coreset selection problem. Our model learns how to select a small coreset of
the input set that balances quality and diversity policies using a learned
metric parameterized by the face quality, optimized end-to-end. The selection
process is a differentiable farthest-point sampling (FPS) realized by
approximating the non-differentiable Argmax operation with differentiable
sampling from the Gumbel-Softmax distribution of distances. The small coreset
is later used as queries in a self and cross-attention architecture to enrich
the descriptor with information from the whole set. Our model is
order-invariant and linear in the input set size. We set a new SOTA to set face
verification on the IJB-B and IJB-C datasets. Our code is publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00752">Affine-Transformation-Invariant Image Classification by Differentiable Arithmetic Distribution Module. (arXiv:2309.00752v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zijie Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1">Guanfang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenqiu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1">Anup Basu</a></p>
<p>Although Convolutional Neural Networks (CNNs) have achieved promising results
in image classification, they still are vulnerable to affine transformations
including rotation, translation, flip and shuffle. The drawback motivates us to
design a module which can alleviate the impact from different affine
transformations. Thus, in this work, we introduce a more robust substitute by
incorporating distribution learning techniques, focusing particularly on
learning the spatial distribution information of pixels in images. To rectify
the issue of non-differentiability of prior distribution learning methods that
rely on traditional histograms, we adopt the Kernel Density Estimation (KDE) to
formulate differentiable histograms. On this foundation, we present a novel
Differentiable Arithmetic Distribution Module (DADM), which is designed to
extract the intrinsic probability distributions from images. The proposed
approach is able to enhance the model's robustness to affine transformations
without sacrificing its feature extraction capabilities, thus bridging the gap
between traditional CNNs and distribution-based learning. We validate the
effectiveness of the proposed approach through ablation study and comparative
experiments with LeNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03999">Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1">Neha Kalibhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1">Sam Sharpe</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodsitt_J/0/1/0/all/0/1">Jeremy Goodsitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruss_B/0/1/0/all/0/1">Bayan Bruss</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>Current state-of-the-art self-supervised approaches, are effective when
trained on individual domains but show limited generalization on unseen
domains. We observe that these models poorly generalize even when trained on a
mixture of domains, making them unsuitable to be deployed under diverse
real-world setups. We therefore propose a general-purpose, lightweight Domain
Disentanglement Module (DDM) that can be plugged into any self-supervised
encoder to effectively perform representation learning on multiple, diverse
domains with or without shared classes. During pre-training according to a
self-supervised loss, DDM enforces a disentanglement in the representation
space by splitting it into a domain-variant and a domain-invariant portion.
When domain labels are not available, DDM uses a robust clustering approach to
discover pseudo-domains. We show that pre-training with DDM can show up to 3.5%
improvement in linear probing accuracy on state-of-the-art self-supervised
models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on
multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained
with DDM show significantly improved generalization (7.4%) to unseen domains
compared to baselines. Therefore, DDM can efficiently adapt self-supervised
encoders to provide high-quality, generalizable representations for diverse
multi-domain data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08912">Delving into Multimodal Prompting for Fine-grained Visual Classification. (arXiv:2309.08912v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junyao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaoyu Du</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shengfeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zechao Li</a></p>
<p>Fine-grained visual classification (FGVC) involves categorizing fine
subdivisions within a broader category, which poses challenges due to subtle
inter-class discrepancies and large intra-class variations. However, prevailing
approaches primarily focus on uni-modal visual concepts. Recent advancements in
pre-trained vision-language models have demonstrated remarkable performance in
various high-level vision tasks, yet the applicability of such models to FGVC
tasks remains uncertain. In this paper, we aim to fully exploit the
capabilities of cross-modal description to tackle FGVC tasks and propose a
novel multimodal prompting solution, denoted as MP-FGVC, based on the
contrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises a
multimodal prompts scheme and a multimodal adaptation scheme. The former
includes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text
Prompt (DaTP), which explicitly highlights the subcategory-specific
discrepancies from the perspectives of both vision and language. The latter
aligns the vision and text prompting elements in a common semantic space,
facilitating cross-modal collaborative reasoning through a Vision-Language
Fusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a
two-stage optimization strategy for MP-FGVC to fully leverage the pre-trained
CLIP model and expedite efficient adaptation for FGVC. Extensive experiments
conducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14585">DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jun_L/0/1/0/all/0/1">Liu Jun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiantao_Z/0/1/0/all/0/1">Zhou Jiantao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiandian_Z/0/1/0/all/0/1">Zeng Jiandian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Jinyu Tian</a></p>
<p>This work investigates efficient score-based black-box adversarial attacks
with a high Attack Success Rate (ASR) and good generalizability. We design a
novel attack method based on a Disentangled Feature space, called DifAttack,
which differs significantly from the existing ones operating over the entire
feature space. Specifically, DifAttack firstly disentangles an image's latent
feature into an adversarial feature and a visual feature, where the former
dominates the adversarial capability of an image, while the latter largely
determines its visual appearance. We train an autoencoder for the
disentanglement by using pairs of clean images and their Adversarial Examples
(AEs) generated from available surrogate models via white-box attack methods.
Eventually, DifAttack iteratively optimizes the adversarial feature according
to the query feedback from the victim model until a successful AE is generated,
while keeping the visual feature unaltered. In addition, due to the avoidance
of using surrogate models' gradient information when optimizing AEs for
black-box models, our proposed DifAttack inherently possesses better attack
capability in the open-set scenario, where the training dataset of the victim
model is unknown. Extensive experimental results demonstrate that our method
achieves significant improvements in ASR and query efficiency simultaneously,
especially in the targeted attack and open-set scenarios. The code is available
at https://github.com/csjunjun/DifAttack.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03149">Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jonathan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a></p>
<p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05718">EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders. (arXiv:2310.05718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baykal_G/0/1/0/all/0/1">Gulcin Baykal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1">Melih Kandemir</a>, <a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1">Gozde Unal</a></p>
<p>Codebook collapse is a common problem in training deep generative models with
discrete representation spaces like Vector Quantized Variational Autoencoders
(VQ-VAEs). We observe that the same problem arises for the alternatively
designed discrete variational autoencoders (dVAEs) whose encoder directly
learns a distribution over the codebook embeddings to represent the data. We
hypothesize that using the softmax function to obtain a probability
distribution causes the codebook collapse by assigning overconfident
probabilities to the best matching codebook elements. In this paper, we propose
a novel way to incorporate evidential deep learning (EDL) instead of softmax to
combat the codebook collapse problem of dVAE. We evidentially monitor the
significance of attaining the probability distribution over the codebook
embeddings, in contrast to softmax usage. Our experiments using various
datasets show that our model, called EdVAE, mitigates codebook collapse while
improving the reconstruction performance, and enhances the codebook usage
compared to dVAE and VQ-VAE based models. Our code can be found at
https://github.com/ituvisionlab/EdVAE .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06470">Focus on Local Regions for Query-based Object Detection. (arXiv:2310.06470v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongbin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yamei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1">Bo Cheng</a></p>
<p>Query-based methods have garnered significant attention in object detection
since the advent of DETR, the pioneering query-based detector. However, these
methods face challenges like slow convergence and suboptimal performance.
Notably, self-attention in object detection often hampers convergence due to
its global focus. To address these issues, we propose FoLR, a transformer-like
architecture with only decoders. We improve the self-attention by isolating
connections between irrelevant objects that makes it focus on local regions but
not global regions. We also design the adaptive sampling method to extract
effective features based on queries' local regions from feature maps.
Additionally, we employ a look-back strategy for decoders to retain previous
information, followed by the Feature Mixer module to fuse features and queries.
Experimental results demonstrate FoLR's state-of-the-art performance in
query-based detectors, excelling in convergence speed and computational
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08320">Defending Our Privacy With Backdoors. (arXiv:2310.08320v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1">Dominik Hintersdorf</a>, <a href="http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1">Lukas Struppek</a>, <a href="http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1">Daniel Neider</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-"a person" instead of the person's name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new "dual-use"
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10410">Loci-Segmented: Improving Scene Segmentation Learning. (arXiv:2310.10410v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Traub_M/0/1/0/all/0/1">Manuel Traub</a>, <a href="http://arxiv.org/find/cs/1/au:+Becker_F/0/1/0/all/0/1">Frederic Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Sauter_A/0/1/0/all/0/1">Adrian Sauter</a>, <a href="http://arxiv.org/find/cs/1/au:+Otte_S/0/1/0/all/0/1">Sebastian Otte</a>, <a href="http://arxiv.org/find/cs/1/au:+Butz_M/0/1/0/all/0/1">Martin V. Butz</a></p>
<p>Slot-oriented approaches for compositional scene segmentation from images and
videos still depend on provided background information or slot assignments. We
present Loci-Segmented (Loci-s) building on the slot-based location and
identity tracking architecture Loci (Traub et al., ICLR 2023). Loci-s enables
dynamic (i) background processing by means of a foreground identifying module
and a background re-generator; (ii) top-down modified object-focused bottom-up
processing; and (iii) depth estimate generation. We also improve automatic slot
assignment via a slot-location-entity regularization mechanism and a prior
segmentation network. The results reveal superior video decomposition
performance in the MOVi datasets and in another established dataset collection
targeting scene segmentation. Loci-s outperforms the state-of-the-art with
respect to the intersection over union (IoU) score in the multi-object video
dataset MOVi-E by a large margin and even without supervised slot assignments
and without the provision of background information. We furthermore show that
Loci-s generates well-interpretable latent representations. These
representations may serve as a foundation-model-like interpretable basis for
solving downstream tasks, such as grounding language, forming compositional
rules, or solving one-shot reinforcement learning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10594">Motion2Language, unsupervised learning of synchronized semantic motion segmentation. (arXiv:2310.10594v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Radouane_K/0/1/0/all/0/1">Karim Radouane</a>, <a href="http://arxiv.org/find/cs/1/au:+Tchechmedjiev_A/0/1/0/all/0/1">Andon Tchechmedjiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Lagarde_J/0/1/0/all/0/1">Julien Lagarde</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranwez_S/0/1/0/all/0/1">Sylvie Ranwez</a></p>
<p>In this paper, we investigate building a sequence to sequence architecture
for motion to language translation and synchronization. The aim is to translate
motion capture inputs into English natural-language descriptions, such that the
descriptions are generated synchronously with the actions performed, enabling
semantic segmentation as a byproduct, but without requiring synchronized
training data. We propose a new recurrent formulation of local attention that
is suited for synchronous/live text generation, as well as an improved motion
encoder architecture better suited to smaller data and for synchronous
generation. We evaluate both contributions in individual experiments, using the
standard BLEU4 metric, as well as a simple semantic equivalence measure, on the
KIT motion language dataset. In a follow-up experiment, we assess the quality
of the synchronization of generated text in our proposed approaches through
multiple evaluation metrics. We find that both contributions to the attention
mechanism and the encoder architecture additively improve the quality of
generated text (BLEU and semantic equivalence), but also of synchronization.
Our code is available at
https://github.com/rd20karim/M2T-Segmentation/tree/main
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12004">Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of Experts And Frequency-augmented Decoder Approach. (arXiv:2310.12004v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Feng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1">Jinxi Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a></p>
<p>The recent use of diffusion prior, enhanced by pre-trained text-image models,
has markedly elevated the performance of image super-resolution (SR). To
alleviate the huge computational cost required by pixel-based diffusion SR,
latent-based methods utilize a feature encoder to transform the image and then
implement the SR image generation in a compact latent space. Nevertheless,
there are two major issues that limit the performance of latent-based
diffusion. First, the compression of latent space usually causes reconstruction
distortion. Second, huge computational cost constrains the parameter scale of
the diffusion model. To counteract these issues, we first propose a frequency
compensation module that enhances the frequency components from latent space to
pixel space. The reconstruction distortion (especially for high-frequency
information) can be significantly decreased. Then, we propose to use
Sample-Space Mixture of Experts (SS-MoE) to achieve more powerful latent-based
SR, which steadily improves the capacity of the model without a significant
increase in inference costs. These carefully crafted designs contribute to
performance improvements in largely explored 4x blind super-resolution
benchmarks and extend to large magnification factors, i.e., 8x image SR
benchmarks. The code is available at https://github.com/amandaluof/moe_sr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03426">GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values. (arXiv:2311.03426v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1">Farnoosh Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1">Walid Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1">Habib Hajimolahoseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1">Foozhan Ataiefard</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1">Mohammad Hassanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1">Saina Asani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1">Austin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1">Omar Mohamed Awad</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Massive transformer-based models face several challenges, including slow and
computationally intensive pre-training and over-parametrization. This paper
addresses these challenges by proposing a versatile method called GQKVA, which
generalizes query, key, and value grouping techniques. GQKVA is designed to
speed up transformer pre-training while reducing the model size. Our
experiments with various GQKVA variants highlight a clear trade-off between
performance and model size, allowing for customized choices based on resource
and time limitations. Our findings also indicate that the conventional
multi-head attention approach is not always the best choice, as there are
lighter and faster alternatives available. We tested our method on ViT, which
achieved an approximate 0.3% increase in accuracy while reducing the model size
by about 4% in the task of image classification. Additionally, our most
aggressive model reduction experiment resulted in a reduction of approximately
15% in model size, with only around a 1% drop in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08059">FS-Net: Full Scale Network and Adaptive Threshold for Improving Extraction of Micro-Retinal Vessel Structures. (arXiv:2311.08059v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Getahun_M/0/1/0/all/0/1">Melaku N. Getahun</a>, <a href="http://arxiv.org/find/eess/1/au:+Rogov_O/0/1/0/all/0/1">Oleg Y. Rogov</a>, <a href="http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1">Dmitry V. Dylov</a>, <a href="http://arxiv.org/find/eess/1/au:+Somov_A/0/1/0/all/0/1">Andrey Somov</a>, <a href="http://arxiv.org/find/eess/1/au:+Bouridane_A/0/1/0/all/0/1">Ahmed Bouridane</a>, <a href="http://arxiv.org/find/eess/1/au:+Hamoudi_R/0/1/0/all/0/1">Rifat Hamoudi</a></p>
<p>Retinal vascular segmentation, is a widely researched subject in biomedical
image processing, aims to relieve ophthalmologists' workload when treating and
detecting retinal disorders. However, segmenting retinal vessels has its own
set of challenges, with prior techniques failing to generate adequate results
when segmenting branches and microvascular structures. The neural network
approaches used recently are characterized by the inability to keep local and
global properties together and the failure to capture tiny end vessels make it
challenging to attain the desired result. To reduce this retinal vessel
segmentation problem, we propose a full-scale micro-vessel extraction mechanism
based on an encoder-decoder neural network architecture, sigmoid smoothing, and
an adaptive threshold method. The network consists of of residual, encoder
booster, bottleneck enhancement, squeeze, and excitation building blocks. All
of these blocks together help to improve the feature extraction and prediction
of the segmentation map. The proposed solution has been evaluated using the
DRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when
compared with previous studies. The AUC and accuracy on the DRIVE dataset are
0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are
0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916
and 0.9750, respectively. The performance achieved is one step ahead of what
has been done in previous studies, and this results in a higher chance of
having this solution in real-life diagnostic centers that seek ophthalmologists
attention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10529">Segment Anything Model with Uncertainty Rectification for Auto-Prompting Medical Image Segmentation. (arXiv:2311.10529v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shiyao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yuan Qi</a></p>
<p>The introduction of the Segment Anything Model (SAM) has marked a significant
advancement in prompt-driven image segmentation. However, SAM's application to
medical image segmentation requires manual prompting of target structures to
obtain acceptable performance, which is still labor-intensive. Despite attempts
of auto-prompting to turn SAM into a fully automatic manner, it still exhibits
subpar performance and lacks of reliability in the field of medical imaging. In
this paper, we propose UR-SAM, an uncertainty rectified SAM framework to
enhance the robustness and reliability for auto-prompting medical image
segmentation. Our method incorporates a prompt augmentation module to estimate
the distribution of predictions and generate uncertainty maps, and an
uncertainty-based rectification module to further enhance the performance of
SAM. Extensive experiments on two public 3D medical datasets covering the
segmentation of 35 organs demonstrate that without supplementary training or
fine-tuning, our method further improves the segmentation performance with up
to 10.7 % and 13.8 % in dice similarity coefficient, demonstrating efficiency
and broad capabilities for medical image segmentation without manual prompting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13435">PG-Video-LLaVA: Pixel Grounding Large Video-Language Models. (arXiv:2311.13435v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Munasinghe_S/0/1/0/all/0/1">Shehan Munasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Thushara_R/0/1/0/all/0/1">Rusiru Thushara</a>, <a href="http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1">Muhammad Maaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1">Hanoona Abdul Rasheed</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Khan</a></p>
<p>Extending image-based Large Multimodal Models (LMMs) to videos is challenging
due to the inherent complexity of video data. The recent approaches extending
image-based LMMs to videos either lack the grounding capabilities (e.g.,
VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for
better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we
propose PG-Video-LLaVA, the first LMM with pixel-level grounding capability,
integrating audio cues by transcribing them into text to enrich video-context
understanding. Our framework uses an off-the-shelf tracker and a novel
grounding module, enabling it to spatially localize objects in videos following
user instructions. We evaluate PG-Video-LLaVA using video-based generative and
question-answering benchmarks and introduce new benchmarks specifically
designed to measure prompt-based object grounding performance in videos.
Further, we propose the use of Vicuna over GPT-3.5, as utilized in
Video-ChatGPT, for video-based conversation benchmarking, ensuring
reproducibility of results which is a concern with the proprietary nature of
GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its
advantages to the video domain, delivering promising gains on video-based
conversation and grounding tasks. Project Page:
https://github.com/mbzuai-oryx/Video-LLaVA
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14521">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting. (arXiv:2311.14521v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zilong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaofeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yikai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huaping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a></p>
<p>3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor's
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02142">Object Recognition as Next Token Prediction. (arXiv:2312.02142v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_K/0/1/0/all/0/1">Kaiyu Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bor-Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1">Jonas Geiping</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hengduo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>We present an approach to pose object recognition as next token prediction.
The idea is to apply a language decoder that auto-regressively predicts the
text tokens from image embeddings to form labels. To ground this prediction
process in auto-regression, we customize a non-causal attention mask for the
decoder, incorporating two key features: modeling tokens from different labels
to be independent, and treating image tokens as a prefix. This masking
mechanism inspires an efficient method - one-shot sampling - to simultaneously
sample tokens of multiple labels in parallel and rank generated labels by their
probabilities during inference. To further enhance the efficiency, we propose a
simple strategy to construct a compact decoder by simply discarding the
intermediate blocks of a pretrained language model. This approach yields a
decoder that matches the full model's performance while being notably more
efficient. The code is available at https://github.com/kaiyuyue/nxtp
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04008">Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes. (arXiv:2312.04008v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kairui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zihao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Gengjie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Haotian Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1">Die Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jibin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhecheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fupeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1">Ziyun Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Di Lin</a></p>
<p>We advocate the idea of the natural-language-driven(NLD) simulation to
efficiently produce the object interactions between multiple objects in the
virtual road scenes, for teaching and testing the autonomous driving systems
that should take quick action to avoid collision with obstacles with
unpredictable motions. The NLD simulation allows the brief natural-language
description to control the object interactions, significantly reducing the
human efforts for creating a large amount of interaction data. To facilitate
the research of NLD simulation, we collect the Language-to-Interaction(L2I)
benchmark dataset with 120,000 natural-language descriptions of object
interactions in 6 common types of road topologies. Each description is
associated with the programming code, which the graphic render can use to
visually reconstruct the object interactions in the virtual scenes. As a
methodology contribution, we design SimCopilot to translate the interaction
descriptions to the renderable code. We use the L2I dataset to evaluate
SimCopilot's abilities to control the object motions, generate complex
interactions, and generalize interactions across road topologies. The L2I
dataset and the evaluation results motivate the relevant research of the NLD
simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04334">Towards a Perceptual Evaluation Framework for Lighting Estimation. (arXiv:2312.04334v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giroux_J/0/1/0/all/0/1">Justine Giroux</a>, <a href="http://arxiv.org/find/cs/1/au:+Dastjerdi_M/0/1/0/all/0/1">Mohammad Reza Karimi Dastjerdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1">Yannick Hold-Geoffroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_Corral_J/0/1/0/all/0/1">Javier Vazquez-Corral</a>, <a href="http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1">Jean-Fran&#xe7;ois Lalonde</a></p>
<p>Progress in lighting estimation is tracked by computing existing image
quality assessment (IQA) metrics on images from standard datasets. While this
may appear to be a reasonable approach, we demonstrate that doing so does not
correlate to human preference when the estimated lighting is used to relight a
virtual scene into a real photograph. To study this, we design a controlled
psychophysical experiment where human observers must choose their preference
amongst rendered scenes lit using a set of lighting estimation algorithms
selected from the recent literature, and use it to analyse how these algorithms
perform according to human perception. Then, we demonstrate that none of the
most popular IQA metrics from the literature, taken individually, correctly
represent human perception. Finally, we show that by learning a combination of
existing IQA metrics, we can more accurately represent human preference. This
provides a new perceptual framework to help evaluate future lighting estimation
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05799">SGNet: Structure Guided Network via Gradient-Frequency Awareness for Depth Map Super-Resolution. (arXiv:2312.05799v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengxue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhiqiang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a></p>
<p>Depth super-resolution (DSR) aims to restore high-resolution (HR) depth from
low-resolution (LR) one, where RGB image is often used to promote this task.
Recent image guided DSR approaches mainly focus on spatial domain to rebuild
depth structure. However, since the structure of LR depth is usually blurry,
only considering spatial domain is not very sufficient to acquire satisfactory
results. In this paper, we propose structure guided network (SGNet), a method
that pays more attention to gradient and frequency domains, both of which have
the inherent ability to capture high-frequency structure. Specifically, we
first introduce the gradient calibration module (GCM), which employs the
accurate gradient prior of RGB to sharpen the LR depth structure. Then we
present the Frequency Awareness Module (FAM) that recursively conducts multiple
spectrum differencing blocks (SDB), each of which propagates the precise
high-frequency components of RGB into the LR depth. Extensive experimental
results on both real and synthetic datasets demonstrate the superiority of our
SGNet, reaching the state-of-the-art. Codes and pre-trained models are
available at https://github.com/yanzq95/SGNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06968">Hallucination Augmented Contrastive Learning for Multimodal Large Language Model. (arXiv:2312.06968v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chaoya Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1">Mengfan Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wei Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1">Ming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qinghao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Ji Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shikun Zhang</a></p>
<p>Multi-modal large language models (MLLMs) have been shown to efficiently
integrate natural language with visual information to handle multi-modal tasks.
However, MLLMs still face a fundamental limitation of hallucinations, where
they tend to generate erroneous or fabricated information. In this paper, we
address hallucinations in MLLMs from a novel perspective of representation
learning. We first analyzed the representation distribution of textual and
visual tokens in MLLM, revealing two important findings: 1) there is a
significant gap between textual and visual representations, indicating
unsatisfactory cross-modal representation alignment; 2) representations of
texts that contain and do not contain hallucinations are entangled, making it
challenging to distinguish them. These two observations inspire us with a
simple yet effective method to mitigate hallucinations. Specifically, we
introduce contrastive learning into MLLMs and use text with hallucination as
hard negative examples, naturally bringing representations of non-hallucinative
text and visual samples closer while pushing way representations of
non-hallucinating and hallucinative text. We evaluate our method quantitatively
and qualitatively, showing its effectiveness in reducing hallucination
occurrences and improving performance across multiple benchmarks. On the
MMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the
baseline MiniGPT-4/LLaVA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06978">CLASS-M: Adaptive stain separation-based contrastive learning with pseudo-labeling for histopathological image classification. (arXiv:2312.06978v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bodong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Manoochehri_H/0/1/0/all/0/1">Hamid Manoochehri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1">Man Minh Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Fooladgar_F/0/1/0/all/0/1">Fahimeh Fooladgar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1">Yosep Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Knudsen_B/0/1/0/all/0/1">Beatrice S. Knudsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirohi_D/0/1/0/all/0/1">Deepika Sirohi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1">Tolga Tasdizen</a></p>
<p>Histopathological image classification is one of the critical aspects in
medical image analysis. Due to the high expense associated with the labeled
data in model training, semi-supervised learning methods have been proposed to
alleviate the need of extensively labeled datasets. In this work, we propose a
model for semi-supervised classification tasks on digital histopathological
Hematoxylin and Eosin (H&amp;E) images. We call the new model Contrastive Learning
with Adaptive Stain Separation and MixUp (CLASS-M). Our model is formed by two
main parts: contrastive learning between adaptively stain separated Hematoxylin
images and Eosin images, and pseudo-labeling using MixUp. We compare our model
with other state-of-the-art models on clear cell renal cell carcinoma (ccRCC)
datasets from our institution and The Cancer Genome Atlas Program (TCGA). We
demonstrate that our CLASS-M model has the best performance on both datasets.
The contributions of different parts in our model are also analyzed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06988">MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving. (arXiv:2312.06988v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1">Guangfeng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuzhi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wenlong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1">Pai Peng</a></p>
<p>Instance segmentation is a fundamental research in computer vision,
especially in autonomous driving. However, manual mask annotation for instance
segmentation is quite time-consuming and costly. To address this problem, some
prior works attempt to apply weakly supervised manner by exploring 2D or 3D
boxes. However, no one has ever successfully segmented 2D and 3D instances
simultaneously by only using 2D box annotations, which could further reduce the
annotation cost by an order of magnitude. Thus, we propose a novel framework
called Multimodal Weakly Supervised Instance Segmentation (MWSIS), which
incorporates various fine-grained label generation and correction modules for
both 2D and 3D modalities to improve the quality of pseudo labels, along with a
new multimodal cross-supervision approach, named Consistency Sparse Cross-modal
Supervision (CSCS), to reduce the inconsistency of multimodal predictions by
response distillation. Particularly, transferring the 3D backbone to downstream
tasks not only improves the performance of the 3D detectors, but also
outperforms fully supervised instance segmentation with only 5% fully
supervised annotations. On the Waymo dataset, the proposed framework
demonstrates significant improvements over the baseline, especially achieving
2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,
respectively. The code is available at
https://github.com/jiangxb98/mwsis-plugin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07180">Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation. (arXiv:2312.07180v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1">Ri Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ruian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xuhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shili Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Weimin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bo Yan</a></p>
<p>Existing recurrent optical flow estimation networks are computationally
expensive since they use a fixed large number of iterations to update the flow
field for each sample. An efficient network should skip iterations when the
flow improvement is limited. In this paper, we develop a Context-Aware
Iteration Policy Network for efficient optical flow estimation, which
determines the optimal number of iterations per sample. The policy network
achieves this by learning contextual information to realize whether flow
improvement is bottlenecked or minimal. On the one hand, we use iteration
embedding and historical hidden cell, which include previous iterations
information, to convey how flow has changed from previous iterations. On the
other hand, we use the incremental loss to make the policy network implicitly
perceive the magnitude of optical flow improvement in the subsequent iteration.
Furthermore, the computational complexity in our dynamic network is
controllable, allowing us to satisfy various resource preferences with a single
trained model. Our policy network can be easily integrated into
state-of-the-art optical flow networks. Extensive experiments show that our
method maintains performance while reducing FLOPs by about 40%/20% for the
Sintel/KITTI datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07424">How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation. (arXiv:2312.07424v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhongyi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guanglin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Rundong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tailin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yilong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Lina Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a></p>
<p>In machine learning, generalization against distribution shifts -- where
deployment conditions diverge from the training scenarios -- is crucial,
particularly in fields like climate modeling, biomedicine, and autonomous
driving. The emergence of foundation models, distinguished by their extensive
pretraining and task versatility, has led to an increased interest in their
adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced
publicly accessible multimodal foundation model, with extensive applications
across various domains, including anomaly detection, video understanding, image
generation, and medical diagnosis. However, its robustness against data
distributions remains largely underexplored. Addressing this gap, this study
rigorously evaluates GPT-4V's adaptability and generalization capabilities in
dynamic environments, benchmarking against prominent models like CLIP and
LLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse
datasets spanning natural, medical, and molecular domains. We further
investigate its adaptability to controlled data perturbations and examine the
efficacy of in-context learning as a tool to enhance its adaptation. Our
findings delineate GPT-4V's capability boundaries in distribution shifts,
shedding light on its strengths and limitations across various scenarios.
Importantly, this investigation contributes to our understanding of how AI
foundation models generalize to distribution shifts, offering pivotal insights
into their adaptability and robustness. Code is publicly available at
https://github.com/jameszhou-gl/gpt-4v-distribution-shift.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07472">MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception. (arXiv:2312.07472v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yiran Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1">Enshen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qichang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhenfei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1">Lu Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruimao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jing Shao</a></p>
<p>It is a long-lasting goal to design an embodied system that can solve
long-horizon open-world tasks in human-like ways. However, existing approaches
usually struggle with compound difficulties caused by the logic-aware
decomposition and context-aware execution of these tasks. To this end, we
introduce MP5, an open-ended multimodal embodied system built upon the
challenging Minecraft simulator, which can decompose feasible sub-objectives,
design sophisticated situation-aware plans, and perform embodied action
control, with frequent communication with a goal-conditioned active perception
scheme. Specifically, MP5 is developed on top of recent advances in Multimodal
Large Language Models (MLLMs), and the system is modulated into functional
modules that can be scheduled and collaborated to ultimately solve pre-defined
context- and process-dependent tasks. Extensive experiments prove that MP5 can
achieve a 22% success rate on difficult process-dependent tasks and a 91%
success rate on tasks that heavily depend on the context. Moreover, MP5
exhibits a remarkable ability to address many open-ended tasks that are
entirely novel.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16426">High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation. (arXiv:2307.16426v1 [eess.IV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1">Jiaqi Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1">Sixing Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Ying-Cong Chen</a></p>
<p>Due to limited camera capacities, digital images usually have a narrower
dynamic illumination range than real-world scene radiance. To resolve this
problem, High Dynamic Range (HDR) reconstruction is proposed to recover the
dynamic range to better represent real-world scenes. However, due to different
physical imaging parameters, the tone-mapping functions between images and real
radiance are highly diverse, which makes HDR reconstruction extremely
challenging. Existing solutions can not explicitly clarify a corresponding
relationship between the tone-mapping function and the generated HDR image, but
this relationship is vital when guiding the reconstruction of HDR images. To
address this problem, we propose a method to explicitly estimate the tone
mapping function and its corresponding HDR image in one network. Firstly, based
on the characteristics of the tone mapping function, we construct a model by a
polynomial to describe the trend of the tone curve. To fit this curve, we use a
learnable network to estimate the coefficients of the polynomial. This curve
will be automatically adjusted according to the tone space of the Low Dynamic
Range (LDR) image, and reconstruct the real HDR image. Besides, since all
current datasets do not provide the corresponding relationship between the tone
mapping function and the LDR image, we construct a new dataset with both
synthetic and real images. Extensive experiments show that our method
generalizes well under different tone-mapping functions and achieves SOTA
performance.
</p>
</p>
</div>

    </div>
    </body>
    