<!DOCTYPE html>
<html>
<head>
<title>2024-01-31-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.15088">Design &amp; Implementation of Automatic Machine Condition Monitoring and Maintenance System in Limited Resource Situations. (arXiv:2401.15088v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ripon_A/0/1/0/all/0/1">Abu Hanif Md. Ripon</a>, <a href="http://arxiv.org/find/eess/1/au:+Ullah_M/0/1/0/all/0/1">Muhammad Ahsan Ullah</a>, <a href="http://arxiv.org/find/eess/1/au:+Paul_A/0/1/0/all/0/1">Arindam Kumar Paul</a>, <a href="http://arxiv.org/find/eess/1/au:+Morshed_M/0/1/0/all/0/1">Md. Mortaza Morshed</a></p>
<p>In the era of the fourth industrial revolution, it is essential to automate
fault detection and diagnosis of machineries so that a warning system can be
developed that will help to take an appropriate action before any catastrophic
damage. Some machines health monitoring systems are used globally but they are
expensive and need trained personnel to operate and analyse. Predictive
maintenance and occupational health and safety culture are not available due to
inadequate infrastructure, lack of skilled manpower, financial crisis, and
others in developing countries. Starting from developing a cost-effective DAS
for collecting fault data in this study, the effect of limited data and
resources has been investigated while automating the process. To solve this
problem, A feature engineering and data reduction method has been developed
combining the concepts from wavelets, differential calculus, and signal
processing. Finally, for automating the whole process, all the necessary
theoretical and practical considerations to develop a predictive model have
been proposed. The DAS successfully collected the required data from the
machine that is 89% accurate compared to the professional manual monitoring
system. SVM and NN were proposed for the prediction purpose because of their
high predicting accuracy greater than 95% during training and 100% during
testing the new samples. In this study, the combination of the simple algorithm
with a rule-based system instead of a data-intensive system turned out to be
hybridization by validating with collected data. The outcome of this research
can be instantly applied to small and medium-sized industries for finding other
issues and developing accordingly. As one of the foundational studies in
automatic FDD, the findings and procedure of this study can lead others to
extend, generalize, or add other dimensions to FDD automation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15089">Accelerating Material Property Prediction using Generically Complete Isometry Invariants. (arXiv:2401.15089v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balasingham_J/0/1/0/all/0/1">Jonathan Balasingham</a>, <a href="http://arxiv.org/find/cs/1/au:+Zamaraev_V/0/1/0/all/0/1">Viktor Zamaraev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurlin_V/0/1/0/all/0/1">Vitaliy Kurlin</a></p>
<p>Material or crystal property prediction using machine learning has grown
popular in recent years as it provides a computationally efficient replacement
to classical simulation methods. A crucial first step for any of these
algorithms is the representation used for a periodic crystal. While similar
objects like molecules and proteins have a finite number of atoms and their
representation can be built based upon a finite point cloud interpretation,
periodic crystals are unbounded in size, making their representation more
challenging. In the present work, we adapt the Pointwise Distance Distribution
(PDD), a continuous and generically complete isometry invariant for periodic
point sets, as a representation for our learning algorithm. While the PDD is
effective in distinguishing periodic point sets up to isometry, there is no
consideration for the composition of the underlying material. We develop a
transformer model with a modified self-attention mechanism that can utilize the
PDD and incorporate compositional information via a spatial encoding method.
This model is tested on the crystals of the Materials Project and Jarvis-DFT
databases and shown to produce accuracy on par with state-of-the-art methods
while being several times faster in both training and prediction time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15092">A note on the capacity of the binary perceptron. (arXiv:2401.15092v1 [math.PR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Altschuler_D/0/1/0/all/0/1">Dylan J. Altschuler</a>, <a href="http://arxiv.org/find/math/1/au:+Tikhomirov_K/0/1/0/all/0/1">Konstantin Tikhomirov</a></p>
<p>Determining the capacity $\alpha_c$ of the Binary Perceptron is a
long-standing problem. Krauth and Mezard (1989) conjectured an explicit value
of $\alpha_c$, approximately equal to .833, and a rigorous lower bound matching
this prediction was recently established by Ding and Sun (2019). Regarding the
upper bound, Kim and Roche (1998) and Talagrand (1999) independently showed
that $\alpha_c$ &lt; .996, while Krauth and Mezard outlined an argument which can
be used to show that $\alpha_c$ &lt; .847. The purpose of this expository note is
to record a complete proof of the bound $\alpha_c$ &lt; .847. The proof is a
conditional first moment method combined with known results on the spherical
perceptron
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15098">Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chaofan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianrui Li</a></p>
<p>Continual reinforcement learning (CRL) empowers RL agents with the ability to
learn from a sequence of tasks, preserving previous knowledge and leveraging it
to facilitate future learning. However, existing methods often focus on
transferring low-level knowledge across similar tasks, which neglects the
hierarchical structure of human cognitive control, resulting in insufficient
knowledge transfer across diverse tasks. To enhance high-level knowledge
transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge
transfer for Continual reinforcement learning), which is structured in two
layers: 1) the high-level policy formulation which utilizes the powerful
reasoning ability of the Large Language Model (LLM) to set goals and 2) the
low-level policy learning through RL which is oriented by high-level goals.
Moreover, the knowledge base (policy library) is constructed to store policies
that can be retrieved for hierarchical knowledge transfer. Experiments
conducted in MiniGrid have demonstrated the effectiveness of Hi-Core in
handling diverse CRL tasks, outperforming popular baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15103">PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression. (arXiv:2401.15103v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lina Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1">Meilan Hao</a></p>
<p>Symbolic regression aims to derive interpretable symbolic expressions from
data in order to better understand and interpret data. %which plays an
important role in knowledge discovery and interpretable machine learning.
</p>
<p>In this study, a symbolic network called PruneSymNet is proposed for symbolic
regression. This is a novel neural network whose activation function consists
of common elementary functions and operators. The whole network is
differentiable and can be trained by gradient descent method. Each subnetwork
in the network corresponds to an expression, and our goal is to extract such
subnetworks to get the desired symbolic expression.
</p>
<p>Therefore, a greedy pruning algorithm is proposed to prune the network into a
subnetwork while ensuring the accuracy of data fitting. The proposed greedy
pruning algorithm preserves the edge with the least loss in each pruning, but
greedy algorithm often can not get the optimal solution. In order to alleviate
this problem, we combine beam search during pruning to obtain multiple
candidate expressions each time, and finally select the expression with the
smallest loss as the final result. It was tested on the public data set and
compared with the current popular algorithms. The results showed that the
proposed algorithm had better accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15105">Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery. (arXiv:2401.15105v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sui_J/0/1/0/all/0/1">Jialu Sui</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1">Yiyang Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1">Wenhan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaokang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Pun_M/0/1/0/all/0/1">Man-On Pun</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiaying Liu</a></p>
<p>The presence of cloud layers severely compromises the quality and
effectiveness of optical remote sensing (RS) images. However, existing
deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties
in accurately reconstructing the original visual authenticity and detailed
semantic content of the images. To tackle this challenge, this work proposes to
encompass enhancements at the data and methodology fronts. On the data side, an
ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial
resolution is established. This benchmark incorporates rich detailed textures
and diverse cloud coverage, serving as a robust foundation for designing and
assessing CR models. From the methodology perspective, a novel diffusion-based
framework for CR called Diffusion Enhancement (DE) is proposed to perform
progressive texture detail recovery, which mitigates the training difficulty
with improved inference accuracy. Additionally, a Weight Allocation (WA)
network is developed to dynamically adjust the weights for feature fusion,
thereby further improving performance, particularly in the context of
ultra-resolution image generation. Furthermore, a coarse-to-fine training
strategy is applied to effectively expedite training convergence while reducing
the computational complexity required to handle ultra-resolution images.
Extensive experiments on the newly established CUHK-CR and existing datasets
such as RICE confirm that the proposed DE framework outperforms existing
DL-based methods in terms of both perceptual quality and signal fidelity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15107">Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups. (arXiv:2401.15107v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wotte_Y/0/1/0/all/0/1">Yannik P. Wotte</a>, <a href="http://arxiv.org/find/math/1/au:+Califano_F/0/1/0/all/0/1">Federico Califano</a>, <a href="http://arxiv.org/find/math/1/au:+Stramigioli_S/0/1/0/all/0/1">Stefano Stramigioli</a></p>
<p>This work presents a novel approach for the optimization of dynamic systems
on finite-dimensional Lie groups. We rephrase dynamic systems as so-called
neural ordinary differential equations (neural ODEs), and formulate the
optimization problem on Lie groups. A gradient descent optimization algorithm
is presented to tackle the optimization numerically. Our algorithm is scalable,
and applicable to any finite dimensional Lie group, including matrix Lie
groups. By representing the system at the Lie algebra level, we reduce the
computational cost of the gradient computation. In an extensive example,
optimal potential energy shaping for control of a rigid body is treated. The
optimal control problem is phrased as an optimization of a neural ODE on the
Lie group SE(3), and the controller is iteratively optimized. The final
controller is validated on a state-regulation task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15108">Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1">Diwas Paudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1">Tapas K. Das</a></p>
<p>Fast-charging hubs for electric vehicles will soon become part of the newly
built infrastructure for transportation electrification across the world. These
hubs are expected to host many DC fast-charging stations and will admit EVs
only for charging. Like the gasoline refueling stations, fast-charging hubs in
a neighborhood will dynamically vary their prices to compete for the same pool
of EV owners. These hubs will interact with the electric power network by
making purchase commitments for a significant part of their power needs in the
day-ahead (DA) electricity market and meeting the difference from the real-time
(RT) market. Hubs may have supplemental battery storage systems (BSS), which
they will use for arbitrage. In this paper, we develop a two-step data-driven
dynamic pricing methodology for hubs in price competition. We first obtain the
DA commitment by solving a stochastic DA commitment model. Thereafter we obtain
the hub pricing strategies by modeling the game as a competitive Markov
decision process (CMDP) and solving it using a multi-agent deep reinforcement
learning (MADRL) approach. We develop a numerical case study for a pricing game
between two charging hubs. We solve the case study with our methodology by
using combinations of two different DRL algorithms, DQN and SAC, and two
different neural networks (NN) architectures, a feed-forward (FF) neural
network, and a multi-head attention (MHA) neural network. We construct a
measure of collusion (index) using the hub profits. A value of zero for this
index indicates no collusion (perfect competition) and a value of one indicates
full collusion (monopolistic behavior). Our results show that the collusion
index varies approximately between 0.14 and 0.45 depending on the combinations
of the algorithms and the architectures chosen by the hubs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15111">Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning. (arXiv:2401.15111v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1">Mingquan Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tianhao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1">Zhaoyi Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Holste_G/0/1/0/all/0/1">Gregory Holste</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1">Ying Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shih_G/0/1/0/all/0/1">George Shih</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1">Yifan Peng</a></p>
<p>Purpose: Limited studies exploring concrete methods or approaches to tackle
and enhance model fairness in the radiology domain. Our proposed AI model
utilizes supervised contrastive learning to minimize bias in CXR diagnosis.
</p>
<p>Materials and Methods: In this retrospective study, we evaluated our proposed
method on two datasets: the Medical Imaging and Data Resource Center (MIDRC)
dataset with 77,887 CXR images from 27,796 patients collected as of April 20,
2023 for COVID-19 diagnosis, and the NIH Chest X-ray (NIH-CXR) dataset with
112,120 CXR images from 30,805 patients collected between 1992 and 2015. In the
NIH-CXR dataset, thoracic abnormalities include atelectasis, cardiomegaly,
effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation,
edema, emphysema, fibrosis, pleural thickening, or hernia. Our proposed method
utilizes supervised contrastive learning with carefully selected positive and
negative samples to generate fair image embeddings, which are fine-tuned for
subsequent tasks to reduce bias in chest X-ray (CXR) diagnosis. We evaluated
the methods using the marginal AUC difference ($\delta$ mAUC).
</p>
<p>Results: The proposed model showed a significant decrease in bias across all
subgroups when compared to the baseline models, as evidenced by a paired T-test
(p&lt;0.0001). The $\delta$ mAUC obtained by our method were 0.0116 (95\% CI,
0.0110-0.0123), 0.2102 (95% CI, 0.2087-0.2118), and 0.1000 (95\% CI,
0.0988-0.1011) for sex, race, and age on MIDRC, and 0.0090 (95\% CI,
0.0082-0.0097) for sex and 0.0512 (95% CI, 0.0512-0.0532) for age on NIH-CXR,
respectively.
</p>
<p>Conclusion: Employing supervised contrastive learning can mitigate bias in
CXR diagnosis, addressing concerns of fairness and reliability in deep
learning-based diagnostic methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15113">Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maslov_K/0/1/0/all/0/1">Konstantin A. Maslov</a>, <a href="http://arxiv.org/find/cs/1/au:+Persello_C/0/1/0/all/0/1">Claudio Persello</a>, <a href="http://arxiv.org/find/cs/1/au:+Schellenberger_T/0/1/0/all/0/1">Thomas Schellenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1">Alfred Stein</a></p>
<p>Accurate global glacier mapping is critical for understanding climate change
impacts. It is challenged by glacier diversity, difficult-to-classify debris
and big data processing. Here we propose Glacier-VisionTransformer-U-Net
(GlaViTU), a convolutional-transformer deep learning model, and five strategies
for multitemporal global-scale glacier mapping using open satellite imagery.
Assessing the spatial, temporal and cross-sensor generalisation shows that our
best strategy achieves intersection over union &gt;0.85 on previously unobserved
images in most cases, which drops to &gt;0.75 for debris-rich areas such as
High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice.
Additionally, adding synthetic aperture radar data, namely, backscatter and
interferometric coherence, increases the accuracy in all regions where
available. The calibrated confidence for glacier extents is reported making the
predictions more reliable and interpretable. We also release a benchmark
dataset that covers 9% of glaciers worldwide. Our results support efforts
towards automated multitemporal and global glacier mapping.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15116">Efficient Online Crowdsourcing with Complex Annotations. (arXiv:2401.15116v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meir_R/0/1/0/all/0/1">Reshef Meir</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Viet-An Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_J/0/1/0/all/0/1">Jagdish Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinsberg_U/0/1/0/all/0/1">Udi Weinsberg</a></p>
<p>Crowdsourcing platforms use various truth discovery algorithms to aggregate
annotations from multiple labelers. In an online setting, however, the main
challenge is to decide whether to ask for more annotations for each item to
efficiently trade off cost (i.e., the number of annotations) for quality of the
aggregated annotations. In this paper, we propose a novel approach for general
complex annotation (such as bounding boxes and taxonomy paths), that works in
an online crowdsourcing setting. We prove that the expected average similarity
of a labeler is linear in their accuracy \emph{conditional on the reported
label}. This enables us to infer reported label accuracy in a broad range of
scenarios. We conduct extensive evaluations on real-world crowdsourcing data
from Meta and show the effectiveness of our proposed online algorithms in
improving the cost-quality trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15119">Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections. (arXiv:2401.15119v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Khairul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentine_T/0/1/0/all/0/1">Tyler Valentine</a>, <a href="http://arxiv.org/find/cs/1/au:+Sue_T/0/1/0/all/0/1">Timothy Joowon Sue</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmacharya_A/0/1/0/all/0/1">Ayush Karmacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Benham_L/0/1/0/all/0/1">Luke Neil Benham</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kingsley Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1">Judy Fox</a></p>
<p>Interpreting deep learning time series models is crucial in understanding the
model's behavior and learning patterns from raw data for real-time
decision-making. However, the complexity inherent in transformer-based time
series models poses challenges in explaining the impact of individual features
on predictions. In this study, we leverage recent local interpretation methods
to interpret state-of-the-art time series models. To use real-world datasets,
we collected three years of daily case data for 3,142 US counties. Firstly, we
compare six transformer-based models and choose the best prediction model for
COVID-19 infection. Using 13 input features from the last two weeks, we can
predict the cases for the next two weeks. Secondly, we present an innovative
way to evaluate the prediction sensitivity to 8 population age groups over
highly dynamic multivariate infection data. Thirdly, we compare our proposed
perturbation-based interpretation method with related work, including a total
of eight local interpretation methods. Finally, we apply our framework to
traffic and electricity datasets, demonstrating that our approach is generic
and can be applied to other time-series domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15121">Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">Yeachan Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1">Geonho Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonyeol Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sejun Park</a></p>
<p>The study of the expressive power of neural networks has investigated the
fundamental limits of neural networks. Most existing results assume real-valued
inputs and parameters as well as exact operations during the evaluation of
neural networks. However, neural networks are typically executed on computers
that can only represent a tiny subset of the reals and apply inexact
operations. In this work, we analyze the expressive power of neural networks
under a more realistic setup: when we use floating-point numbers and
operations. Our first set of results assumes floating-point operations where
the significand of a float is represented by finite bits but its exponent can
take any integer value. Under this setup, we show that neural networks using a
binary threshold unit or ReLU can memorize any finite input/output pairs and
can approximate any continuous function within a small error. We also show
similar results on memorization and universal approximation when floating-point
operations use finite bits for both significand and exponent; these results are
applicable to many popular floating-point formats such as those defined in the
IEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15122">A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weitao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoxinran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhethanabotla_V/0/1/0/all/0/1">Vignesh Bhethanabotla</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampal_N/0/1/0/all/0/1">Nakul Rampal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaghi_O/0/1/0/all/0/1">Omar Yaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgs_C/0/1/0/all/0/1">Christian Borgs</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hongyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chayes_J/0/1/0/all/0/1">Jennifer Chayes</a></p>
<p>In drug discovery, molecular dynamics (MD) simulation for protein-ligand
binding provides a powerful tool for predicting binding affinities, estimating
transport properties, and exploring pocket sites. There has been a long history
of improving the efficiency of MD simulations through better numerical methods
and, more recently, by augmenting them with machine learning (ML) methods. Yet,
challenges remain, such as accurate modeling of extended-timescale simulations.
To address this issue, we propose NeuralMD, the first ML surrogate that can
facilitate numerical MD and provide accurate simulations of protein-ligand
binding dynamics. We propose a principled approach that incorporates a novel
physics-informed multi-grained group symmetric framework. Specifically, we
propose (1) a BindingNet model that satisfies group symmetry using vector
frames and captures the multi-level protein-ligand interactions, and (2) an
augmented neural differential equation solver that learns the trajectory under
Newtonian mechanics. For the experiment, we design ten single-trajectory and
three multi-trajectory binding simulation tasks. We show the efficiency and
effectiveness of NeuralMD, with a 2000$\times$ speedup over standard numerical
MD simulation and outperforming all other ML approaches by up to 80\% under the
stability metric. We further qualitatively show that NeuralMD reaches more
stable binding predictions compared to other machine learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15123">Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shibo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qihang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shizhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1">Wenchao Meng</a></p>
<p>Self-supervised methods have gained prominence in time series anomaly
detection due to the scarcity of available annotations. Nevertheless, they
typically demand extensive training data to acquire a generalizable
representation map, which conflicts with scenarios of a few available samples,
thereby limiting their performance. To overcome the limitation, we propose
\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly
detection approach where the student network is trained to mimic the features
of the large language model (LLM)-based teacher network that is pretrained on
large-scale datasets. During the testing phase, anomalies are detected when the
discrepancy between the features of the teacher and student networks is large.
To circumvent the student network from learning the teacher network's feature
of anomalous samples, we devise two key strategies. 1) Prototypical signals are
incorporated into the student network to consolidate the normal feature
extraction. 2) We use synthetic anomalies to enlarge the representation gap
between the two networks. AnomalyLLM demonstrates state-of-the-art performance
on 15 datasets, improving accuracy by at least 14.5\% in the UCR dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15127">Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shafee_S/0/1/0/all/0/1">Samaneh Shafee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bessani_A/0/1/0/all/0/1">Alysson Bessani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1">Pedro M. Ferreira</a></p>
<p>Knowledge sharing about emerging threats is crucial in the rapidly advancing
field of cybersecurity and forms the foundation of Cyber Threat Intelligence.
In this context, Large Language Models are becoming increasingly significant in
the field of cybersecurity, presenting a wide range of opportunities. This
study explores the capability of chatbots such as ChatGPT, GPT4all,
Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify
cybersecurity-related text within Open Source Intelligence. We assess the
capabilities of existing chatbot models for Natural Language Processing tasks.
We consider binary classification and Named Entity Recognition as tasks. This
study analyzes well-established data collected from Twitter, derived from
previous research efforts. Regarding cybersecurity binary classification,
Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94,
and the open-source GPT4all model achieved an F1-score of 0.90. However,
concerning cybersecurity entity recognition, chatbot models have limitations
and are less effective. This study demonstrates the capability of these
chatbots only for specific tasks, such as cybersecurity binary classification,
while highlighting the need for further refinement in other tasks, such as
Named Entity Recognition tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15139">FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking. (arXiv:2401.15139v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Machkour_J/0/1/0/all/0/1">Jasin Machkour</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Palomar_D/0/1/0/all/0/1">Daniel P. Palomar</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Muma_M/0/1/0/all/0/1">Michael Muma</a></p>
<p>In high-dimensional data analysis, such as financial index tracking or
biomedical applications, it is crucial to select the few relevant variables
while maintaining control over the false discovery rate (FDR). In these
applications, strong dependencies often exist among the variables (e.g., stock
returns), which can undermine the FDR control property of existing methods like
the model-X knockoff method or the T-Rex selector. To address this issue, we
have expanded the T-Rex framework to accommodate overlapping groups of highly
correlated variables. This is achieved by integrating a nearest neighbors
penalization mechanism into the framework, which provably controls the FDR at
the user-defined target level. A real-world example of sparse index tracking
demonstrates the proposed method's ability to accurately track the S&amp;P 500
index over the past 20 years based on a small number of stocks. An open-source
implementation is provided within the R package TRexSelector on CRAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15164">AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations. (arXiv:2401.15164v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Devulapally_N/0/1/0/all/0/1">Naresh Kumar Devulapally</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1">Sidharth Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_S/0/1/0/all/0/1">Sreyasee Das Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Junsong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yu-Ping Chang</a></p>
<p>Analyzing individual emotions during group conversation is crucial in
developing intelligent agents capable of natural human-machine interaction.
While reliable emotion recognition techniques depend on different modalities
(text, audio, video), the inherent heterogeneity between these modalities and
the dynamic cross-modal interactions influenced by an individual's unique
behavioral patterns make the task of emotion recognition very challenging. This
difficulty is compounded in group settings, where the emotion and its temporal
evolution are not only influenced by the individual but also by external
contexts like audience reaction and context of the ongoing conversation. To
meet this challenge, we propose a Multimodal Attention Network that captures
cross-modal interactions at various levels of spatial abstraction by jointly
learning its interactive bunch of mode-specific Peripheral and Central
networks. The proposed MAN injects cross-modal attention via its Peripheral
key-value pairs within each layer of a mode-specific Central query network. The
resulting cross-attended mode-specific descriptors are then combined using an
Adaptive Fusion technique that enables the model to integrate the
discriminative and complementary mode-specific data patterns within an
instance-specific multimodal descriptor. Given a dialogue represented by a
sequence of utterances, the proposed AMuSE model condenses both spatial and
temporal features into two dense descriptors: speaker-level and
utterance-level. This helps not only in delivering better classification
performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy)
in large-scale public datasets but also helps the users in understanding the
reasoning behind each emotion prediction made by the model via its Multimodal
Explainability Visualization module.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15199">SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance. (arXiv:2401.15199v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kharazian_Z/0/1/0/all/0/1">Zahra Kharazian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindgren_T/0/1/0/all/0/1">Tony Lindgren</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnusson_S/0/1/0/all/0/1">Sindri Magn&#xfa;sson</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinert_O/0/1/0/all/0/1">Olof Steinert</a>, <a href="http://arxiv.org/find/cs/1/au:+Reyna_O/0/1/0/all/0/1">Oskar Andersson Reyna</a></p>
<p>This paper presents a description of a real-world, multivariate time series
dataset collected from an anonymized engine component (called Component X) of a
fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables
capturing detailed operational data, repair records, and specifications of
trucks while maintaining confidentiality by anonymization. It is well-suited
for a range of machine learning applications, such as classification,
regression, survival analysis, and anomaly detection, particularly when applied
to predictive maintenance scenarios. The large population size and variety of
features in the format of histograms and numerical counters, along with the
inclusion of temporal information, make this real-world dataset unique in the
field. The objective of releasing this dataset is to give a broad range of
researchers the possibility of working with real-world data from an
internationally well-known company and introduce a standard benchmark to the
predictive maintenance field, fostering reproducible research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15203">FedGT: Federated Node Classification with Scalable Graph Transformer. (arXiv:2401.15203v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zaixi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qingyong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Weibo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a></p>
<p>Graphs are widely used to model relational data. As graphs are getting larger
and larger in real-world scenarios, there is a trend to store and compute
subgraphs in multiple local systems. For example, recently proposed
\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs)
distributively on local subgraphs and aggregate GNN parameters with a central
server. However, existing methods have the following limitations: (1) The links
between local subgraphs are missing in subgraph federated learning. This could
severely damage the performance of GNNs that follow message-passing paradigms
to update node/edge features. (2) Most existing methods overlook the subgraph
heterogeneity issue, brought by subgraphs being from different parts of the
whole graph. To address the aforementioned challenges, we propose a scalable
\textbf{Fed}erated \textbf{G}raph \textbf{T}ransformer (\textbf{FedGT}) in the
paper. Firstly, we design a hybrid attention scheme to reduce the complexity of
the Graph Transformer to linear while ensuring a global receptive field with
theoretical bounds. Specifically, each node attends to the sampled local
neighbors and a set of curated global nodes to learn both local and global
information and be robust to missing links. The global nodes are dynamically
updated during training with an online clustering algorithm to capture the data
distribution of the corresponding local subgraph. Secondly, FedGT computes
clients' similarity based on the aligned global nodes with optimal transport.
The similarity is then used to perform weighted averaging for personalized
aggregation, which well addresses the data heterogeneity problem. Moreover,
local differential privacy is applied to further protect the privacy of
clients. Finally, extensive experimental results on 6 datasets and 2 subgraph
settings demonstrate the superiority of FedGT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15207">HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yongkang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiqun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Daling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>Full-parameter fine-tuning has become the go-to choice for adapting language
models (LMs) to downstream tasks due to its excellent performance. As LMs grow
in size, fine-tuning the full parameters of LMs requires a prohibitively large
amount of GPU memory. Existing approaches utilize zeroth-order optimizer to
conserve GPU memory, which can potentially compromise the performance of LMs as
non-zero order optimizers tend to converge more readily on most downstream
tasks. In this paper, we propose a novel optimizer-independent end-to-end
hierarchical fine-tuning strategy, HiFT, which only updates a subset of
parameters at each training step. HiFT can significantly reduce the amount of
gradients and optimizer state parameters residing in GPU memory at the same
time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT
achieves comparable performance to parameter-efficient fine-tuning and standard
full parameter fine-tuning. (2) HiFT supports various optimizers including
AdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\% GPU memory compared
with standard full-parameter fine-tuning for 7B model. (4) HiFT enables
full-parameter fine-tuning of a 7B model on single 48G A6000 with a precision
of 32 using the AdamW optimizer, without using any memory saving techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15222">Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almudaifer_A/0/1/0/all/0/1">Abdullateef I. Almudaifer</a>, <a href="http://arxiv.org/find/cs/1/au:+O%60Leary_T/0/1/0/all/0/1">Tobias O`Leary</a>, <a href="http://arxiv.org/find/cs/1/au:+Covington_W/0/1/0/all/0/1">Whitney Covington</a>, <a href="http://arxiv.org/find/cs/1/au:+Hairston_J/0/1/0/all/0/1">JaMor Hairston</a>, <a href="http://arxiv.org/find/cs/1/au:+Deitch_Z/0/1/0/all/0/1">Zachary Deitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankit Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Carroll_C/0/1/0/all/0/1">Caleb M. Carroll</a>, <a href="http://arxiv.org/find/cs/1/au:+Crisan_E/0/1/0/all/0/1">Estera Crisan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradford_W/0/1/0/all/0/1">William Bradford</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_L/0/1/0/all/0/1">Lauren Walter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellen_E/0/1/0/all/0/1">Eaton Ellen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sue S. Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_J/0/1/0/all/0/1">John D. Osborne</a></p>
<p>Background: The semantics of entities extracted from a clinical text can be
dramatically altered by modifiers, including entity negation, uncertainty,
conditionality, severity, and subject. Existing models for determining
modifiers of clinical entities involve regular expression or features weights
that are trained independently for each modifier.
</p>
<p>Methods: We develop and evaluate a multi-task transformer architecture design
where modifiers are learned and predicted jointly using the publicly available
SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that
contains modifiers shared with SemEval as well as novel modifiers specific for
OUD. We evaluate the effectiveness of our multi-task learning approach versus
previously published systems and assess the feasibility of transfer learning
for clinical entity modifiers when only a portion of clinical modifiers are
shared.
</p>
<p>Results: Our approach achieved state-of-the-art results on the ShARe corpus
from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,
1.7% on unweighted accuracy, and 10% on micro F1 scores.
</p>
<p>Conclusions: We show that learned weights from our shared model can be
effectively transferred to a new partially matched data set, validating the use
of transfer learning for clinical text modifiers
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15223">Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis. (arXiv:2401.15223v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingshi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Grujicic_D/0/1/0/all/0/1">Dusan Grujicic</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeger_S/0/1/0/all/0/1">Steven De Saeger</a>, <a href="http://arxiv.org/find/cs/1/au:+Heremans_S/0/1/0/all/0/1">Stien Heremans</a>, <a href="http://arxiv.org/find/cs/1/au:+Somers_B/0/1/0/all/0/1">Ben Somers</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew B. Blaschko</a></p>
<p>In recent years, machine learning has become crucial in remote sensing
analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy
of machine learning and satellite imagery analysis has demonstrated significant
productivity in this field, as evidenced by several studies. A notable
challenge within this area is the semantic segmentation mapping of land usage
over extensive territories, where the accessibility of accurate land-use data
and the reliability of ground truth land-use labels pose significant
difficulties. For example, providing a detailed and accurate pixel-wise labeled
dataset of the Flanders region, a first-level administrative division of
Belgium, can be particularly insightful. Yet there is a notable lack of
regulated, formalized datasets and workflows for such studies in many regions
globally. This paper introduces a comprehensive approach to addressing these
gaps. We present a densely labeled ground truth map of Flanders paired with
Sentinel-2 satellite imagery. Our methodology includes a formalized dataset
division and sampling method, utilizing the topographic map layout
'Kaartbladversnijdingen,' and a detailed semantic segmentation model training
pipeline. Preliminary benchmarking results are also provided to demonstrate the
efficacy of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15235">CascadedGaze: Efficiency in Global Context Extraction for Image Restoration. (arXiv:2401.15235v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ghasemabadi_A/0/1/0/all/0/1">Amirhosein Ghasemabadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Salameh_M/0/1/0/all/0/1">Mohammad Salameh</a>, <a href="http://arxiv.org/find/eess/1/au:+Janjua_M/0/1/0/all/0/1">Muhammad Kamran Janjua</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Chunhua Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_F/0/1/0/all/0/1">Fengyu Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_D/0/1/0/all/0/1">Di Niu</a></p>
<p>Image restoration tasks traditionally rely on convolutional neural networks.
However, given the local nature of the convolutional operator, they struggle to
capture global information. The promise of attention mechanisms in Transformers
is to circumvent this problem, but it comes at the cost of intensive
computational overhead. Many recent studies in image restoration have focused
on solving the challenge of balancing performance and computational cost via
Transformer variants. In this paper, we present CascadedGaze Network (CGNet),
an encoder-decoder architecture that employs Global Context Extractor (GCE), a
novel and efficient way to capture global information for image restoration.
The GCE module leverages small kernels across convolutional layers to learn
global dependencies, without requiring self-attention. Extensive experimental
results show that our approach outperforms a range of state-of-the-art methods
on denoising benchmark datasets including both real image denoising and
synthetic image denoising, as well as on image deblurring task, while being
more computationally efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15236">Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones. (arXiv:2401.15236v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Motetti_B/0/1/0/all/0/1">Beatrice Alessandra Motetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Crupi_L/0/1/0/all/0/1">Luca Crupi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elshaigi_M/0/1/0/all/0/1">Mustafa Omer Mohammed Elamin Elshaigi</a>, <a href="http://arxiv.org/find/cs/1/au:+Risso_M/0/1/0/all/0/1">Matteo Risso</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1">Daniele Jahier Pagliari</a>, <a href="http://arxiv.org/find/cs/1/au:+Palossi_D/0/1/0/all/0/1">Daniele Palossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1">Alessio Burrello</a></p>
<p>Sub-10cm diameter nano-drones are gaining momentum thanks to their
applicability in scenarios prevented to bigger flying drones, such as in narrow
environments and close to humans. However, their tiny form factor also brings
their major drawback: ultra-constrained memory and processors for the onboard
execution of their perception pipelines. Therefore, lightweight deep
learning-based approaches are becoming increasingly popular, stressing how
computational efficiency and energy-saving are paramount as they can make the
difference between a fully working closed-loop system and a failing one. In
this work, to maximize the exploitation of the ultra-limited resources aboard
nano-drones, we present a novel adaptive deep learning-based mechanism for the
efficient execution of a vision-based human pose estimation task. We leverage
two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different
regression performance vs. computational costs trade-offs. By combining these
CNNs with three novel adaptation strategies based on the output's temporal
consistency and on auxiliary tasks to swap the CNN being executed proactively,
we present six different systems. On a real-world dataset and the actual
nano-drone hardware, our best-performing system, compared to executing only the
bigger and most accurate SoA model, shows 28% latency reduction while keeping
the same mean absolute error (MAE), 3% MAE reduction while being iso-latency,
and the absolute peak performance, i.e., 6% better than SoA model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15238">Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vyas_T/0/1/0/all/0/1">Tirth Kiranbhai Vyas</a></p>
<p>We have described a novel approach for training tabular data using the
TabTransformer model with self-supervised learning. Traditional machine
learning models for tabular data, such as GBDT are being widely used though our
paper examines the effectiveness of the TabTransformer which is a Transformer
based model optimised specifically for tabular data. The TabTransformer
captures intricate relationships and dependencies among features in tabular
data by leveraging the self-attention mechanism of Transformers. We have used a
self-supervised learning approach in this study, where the TabTransformer
learns from unlabelled data by creating surrogate supervised tasks, eliminating
the need for the labelled data. The aim is to find the most effective
TabTransformer model representation of categorical and numerical features. To
address the challenges faced during the construction of various input settings
into the Transformers. Furthermore, a comparative analysis is also been
conducted to examine performance of the TabTransformer model against baseline
models such as MLP and supervised TabTransformer.
</p>
<p>The research has presented with a novel approach by creating various variants
of TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which
has helped to increase the effective capturing of the underlying relationship
between various features of the tabular dataset by constructing optimal inputs.
And further we have employed a self-supervised learning approach in the form of
a masking-based unsupervised setting for tabular data. The findings shed light
on the best way to represent categorical and numerical features, emphasizing
the TabTransormer performance when compared to established machine learning
models and other self-supervised learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15239">MEA-Defender: A Robust Watermark against Model Extraction Attack. (arXiv:2401.15239v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1">Peizhuo Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hualong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiachen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengzhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1">Ruigang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shenchen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingjun Zhang</a></p>
<p>Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been
trained using deep learning algorithms. To protect the Intellectual Property
(IP) of the original owners over such DNN models, backdoor-based watermarks
have been extensively studied. However, most of such watermarks fail upon model
extraction attack, which utilizes input samples to query the target model and
obtains the corresponding outputs, thus training a substitute model using such
input-output pairs. In this paper, we propose a novel watermark to protect IP
of DNN models against model extraction, named MEA-Defender. In particular, we
obtain the watermark by combining two samples from two source classes in the
input domain and design a watermark loss function that makes the output domain
of the watermark within that of the main task samples. Since both the input
domain and the output domain of our watermark are indispensable parts of those
of the main task samples, the watermark will be extracted into the stolen model
along with the main task during model extraction. We conduct extensive
experiments on four model extraction attacks, using five datasets and six
models trained based on supervised learning and self-supervised learning
algorithms. The experimental results demonstrate that MEA-Defender is highly
robust against different model extraction attacks, and various watermark
removal/detection approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15240">Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games. (arXiv:2401.15240v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haipeng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chen-Yu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Weiqiang Zheng</a></p>
<p>We study policy optimization algorithms for computing correlated equilibria
in multi-player general-sum Markov Games. Previous results achieve
$O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated
$O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated
equilibrium. In this paper, we improve both results significantly by providing
an uncoupled policy optimization algorithm that attains a near-optimal
$\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium.
Our algorithm is constructed by combining two main elements (i) smooth value
updates and (ii) the optimistic-follow-the-regularized-leader algorithm with
the log barrier regularizer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15245">GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation. (arXiv:2401.15245v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yildirim_B/0/1/0/all/0/1">Bar&#x131;&#x15f; Y&#x131;ld&#x131;r&#x131;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_M/0/1/0/all/0/1">Murat Kurt</a></p>
<p>This paper presents a plugin that adds a representation of homogeneous and
heterogeneous, optically thick, translucent materials on the Blender 3D
modeling tool. The working principle of this plugin is based on a combination
of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based
subsurface scattering method (GenSSS). The proposed plugin has been implemented
using Mitsuba renderer, which is an open source rendering software. The
proposed plugin has been validated on measured subsurface scattering data. It's
shown that the proposed plugin visualizes homogeneous and heterogeneous
subsurface scattering effects, accurately, compactly and computationally
efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15246">Training Differentially Private Ad Prediction Models with Semi-Sensitive Features. (arXiv:2401.15246v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chua_L/0/1/0/all/0/1">Lynn Chua</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qiliang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1">Badih Ghazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrison_C/0/1/0/all/0/1">Charlie Harrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1">Pritish Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Krichene_W/0/1/0/all/0/1">Walid Krichene</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Narra_K/0/1/0/all/0/1">Krishna Giri Narra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Amer Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Varadarajan_A/0/1/0/all/0/1">Avinash Varadarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chiyuan Zhang</a></p>
<p>Motivated by problems arising in digital advertising, we introduce the task
of training differentially private (DP) machine learning models with
semi-sensitive features. In this setting, a subset of the features is known to
the attacker (and thus need not be protected) while the remaining features as
well as the label are unknown to the attacker and should be protected by the DP
guarantee. This task interpolates between training the model with full DP
(where the label and all features should be protected) or with label DP (where
all the features are considered known, and only the label should be protected).
We present a new algorithm for training DP models with semi-sensitive features.
Through an empirical evaluation on real ads datasets, we demonstrate that our
algorithm surpasses in utility the baselines of (i) DP stochastic gradient
descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP
algorithm run only on the known features (while discarding the unknown ones).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15248">Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective. (arXiv:2401.15248v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1">Yue Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xiaofeng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1">Qifan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1">Belinda Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Guang Cheng</a></p>
<p>Pre-training is known to generate universal representations for downstream
tasks in large-scale deep learning such as large language models. Existing
literature, e.g., \cite{kim2020adversarial}, empirically observe that the
downstream tasks can inherit the adversarial robustness of the pre-trained
model. We provide theoretical justifications for this robustness inheritance
phenomenon. Our theoretical results reveal that feature purification plays an
important role in connecting the adversarial robustness of the pre-trained
model and the downstream tasks in two-layer neural networks. Specifically, we
show that (i) with adversarial training, each hidden node tends to pick only
one (or a few) feature; (ii) without adversarial training, the hidden nodes can
be vulnerable to attacks. This observation is valid for both supervised
pre-training and contrastive learning. With purified nodes, it turns out that
clean training is enough to achieve adversarial robustness in downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15254">Finite Sample Confidence Regions for Linear Regression Parameters Using Arbitrary Predictors. (arXiv:2401.15254v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Guille_Escuret_C/0/1/0/all/0/1">Charles Guille-Escuret</a>, <a href="http://arxiv.org/find/stat/1/au:+Ndiaye_E/0/1/0/all/0/1">Eugene Ndiaye</a></p>
<p>We explore a novel methodology for constructing confidence regions for
parameters of linear models, using predictions from any arbitrary predictor.
Our framework requires minimal assumptions on the noise and can be extended to
functions deviating from strict linearity up to some adjustable threshold,
thereby accommodating a comprehensive and pragmatically relevant set of
functions. The derived confidence regions can be cast as constraints within a
Mixed Integer Linear Programming framework, enabling optimisation of linear
objectives. This representation enables robust optimization and the extraction
of confidence intervals for specific parameter coordinates. Unlike previous
methods, the confidence region can be empty, which can be used for hypothesis
testing. Finally, we validate the empirical applicability of our method on
synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15262">Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation. (arXiv:2401.15262v1 [math.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Xie_Y/0/1/0/all/0/1">Yiling Xie</a>, <a href="http://arxiv.org/find/math/1/au:+Huo_X/0/1/0/all/0/1">Xiaoming Huo</a></p>
<p>Adversarial training has been proposed to hedge against adversarial attacks
in machine learning and statistical models. This paper focuses on adversarial
training under $\ell_\infty$-perturbation, which has recently attracted much
research attention. The asymptotic behavior of the adversarial training
estimator is investigated in the generalized linear model. The results imply
that the limiting distribution of the adversarial training estimator under
$\ell_\infty$-perturbation could put a positive probability mass at $0$ when
the true parameter is $0$, providing a theoretical guarantee of the associated
sparsity-recovery ability. Alternatively, a two-step procedure is proposed --
adaptive adversarial training, which could further improve the performance of
adversarial training under $\ell_\infty$-perturbation. Specifically, the
proposed procedure could achieve asymptotic unbiasedness and variable-selection
consistency. Numerical experiments are conducted to show the sparsity-recovery
ability of adversarial training under $\ell_\infty$-perturbation and to compare
the empirical performance between classic adversarial training and adaptive
adversarial training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15268">Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheraz_H/0/1/0/all/0/1">Haleema Sheraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kremer_S/0/1/0/all/0/1">Stefan C. Kremer</a>, <a href="http://arxiv.org/find/cs/1/au:+Skorburg_J/0/1/0/all/0/1">Joshua August Skorburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">Graham Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinnott_Armstrong_W/0/1/0/all/0/1">Walter Sinnott-Armstrong</a>, <a href="http://arxiv.org/find/cs/1/au:+Boerstler_K/0/1/0/all/0/1">Kyle Boerstler</a></p>
<p>In response to the pressing challenge of kidney allocation, characterized by
growing demands for organs, this research sets out to develop a data-driven
solution to this problem, which also incorporates stakeholder values. The
primary objective of this study is to create a method for learning both
individual and group-level preferences pertaining to kidney allocations.
Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging
two distinct datasets and evaluating across three levels - Individual, Group
and Stability - we employ machine learning classifiers assessed through several
metrics. The Individual level model predicts individual participant
preferences, the Group level model aggregates preferences across participants,
and the Stability level model, an extension of the Group level, evaluates the
stability of these preferences over time. By incorporating stakeholder
preferences into the kidney allocation process, we aspire to advance the
ethical dimensions of organ transplantation, contributing to more transparent
and equitable practices while promoting the integration of moral values into
algorithmic decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15270">SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yiqun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhili Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaowei Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_A/0/1/0/all/0/1">Aolin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuo Xu</a></p>
<p>Fairness-awareness has emerged as an essential building block for the
responsible use of artificial intelligence in real applications. In many cases,
inequity in performance is due to the change in distribution over different
regions. While techniques have been developed to improve the transferability of
fairness, a solution to the problem is not always feasible with no samples from
the new regions, which is a bottleneck for pure data-driven attempts.
Fortunately, physics-based mechanistic models have been studied for many
problems with major social impacts. We propose SimFair, a physics-guided
fairness-aware learning framework, which bridges the data limitation by
integrating physical-rule-based simulation and inverse modeling into the
training design. Using temperature prediction as an example, we demonstrate the
effectiveness of the proposed SimFair in fairness preservation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15273">Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning. (arXiv:2401.15273v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Aritra Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_J/0/1/0/all/0/1">James Anderson</a></p>
<p>Federated reinforcement learning (FRL) has emerged as a promising paradigm
for reducing the sample complexity of reinforcement learning tasks by
exploiting information from different agents. However, when each agent
interacts with a potentially different environment, little to nothing is known
theoretically about the non-asymptotic performance of FRL algorithms. The lack
of such results can be attributed to various technical challenges and their
intricate interplay: Markovian sampling, linear function approximation,
multiple local updates to save communication, heterogeneity in the reward
functions and transition kernels of the agents' MDPs, and continuous
state-action spaces. Moreover, in the on-policy setting, the behavior policies
vary with time, further complicating the analysis. In response, we introduce
FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped
with linear function approximation, to address these challenges and provide a
comprehensive finite-time error analysis. Notably, we establish that FedSARSA
converges to a policy that is near-optimal for all agents, with the extent of
near-optimality proportional to the level of heterogeneity. Furthermore, we
prove that FedSARSA leverages agent collaboration to enable linear speedups as
the number of agents increases, which holds for both fixed and adaptive
step-size configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15285">Ransomware threat mitigation through network traffic analysis and machine learning techniques. (arXiv:2401.15285v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehrban_A/0/1/0/all/0/1">Ali Mehrban</a>, <a href="http://arxiv.org/find/cs/1/au:+Geransayeh_S/0/1/0/all/0/1">Shirin Karimi Geransayeh</a></p>
<p>In recent years, there has been a noticeable increase in cyberattacks using
ransomware. Attackers use this malicious software to break into networks and
harm computer systems. This has caused significant and lasting damage to
various organizations, including government, private companies, and regular
users. These attacks often lead to the loss or exposure of sensitive
information, disruptions in normal operations, and persistent vulnerabilities.
This paper focuses on a method for recognizing and identifying ransomware in
computer networks. The approach relies on using machine learning algorithms and
analyzing the patterns of network traffic. By collecting and studying this
traffic, and then applying machine learning models, we can accurately identify
and detect ransomware. The results of implementing this method show that
machine learning algorithms can effectively pinpoint ransomware based on
network traffic, achieving high levels of precision and accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15290">Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset. (arXiv:2401.15290v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1">Hung Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Warrier_H/0/1/0/all/0/1">Harikrishna Warrier</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1">Yogesh Gupta</a></p>
<p>Electronic health record (EHR) is more and more popular, and it comes with
applying machine learning solutions to resolve various problems in the domain.
This growing research area also raises the need for EHRs accessibility. Medical
Information Mart for Intensive Care (MIMIC) dataset is a popular, public, and
free EHR dataset in a raw format that has been used in numerous studies.
However, despite of its popularity, it is lacking benchmarking work, especially
with recent state of the art works in the field of deep learning with
time-series tabular data. The aim of this work is to fill this lack by
providing a benchmark for latest version of MIMIC dataset, MIMIC-IV. We also
give a detailed literature survey about studies that has been already done for
MIIMIC-III.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15292">Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Furuhashi_T/0/1/0/all/0/1">Takanobu Furuhashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hontani_H/0/1/0/all/0/1">Hidekata Hontani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokota_T/0/1/0/all/0/1">Tatsuya Yokota</a></p>
<p>We propose a convex signal reconstruction method for block sparsity under
arbitrary linear transform with unknown block structure. The proposed method is
a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can
reconstruct signals with block sparsity under non-invertible transforms, unlike
LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse
regularization, enabling more versatile and powerful applications across
various signal processing domains. We derive an iterative algorithm for solving
proposed method and provide conditions for its convergence to the optimal
solution. Numerical experiments demonstrate the effectiveness of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15293">SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1">Foozhan Ataiefard</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1">Walid Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1">Habib Hajimolahoseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1">Saina Asani</a>, <a href="http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1">Farnoosh Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1">Mohammad Hassanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1">Omar Mohamed Awad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1">Austin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Vision transformers are known to be more computationally and data-intensive
than CNN models. These transformer models such as ViT, require all the input
image tokens to learn the relationship among them. However, many of these
tokens are not informative and may contain irrelevant information such as
unrelated background or unimportant scenery. These tokens are overlooked by the
multi-head self-attention (MHSA), resulting in many redundant and unnecessary
computations in MHSA and the feed-forward network (FFN). In this work, we
propose a method to optimize the amount of unnecessary interactions between
unimportant tokens by separating and sending them through a different low-cost
computational path. Our method does not add any parameters to the ViT model and
aims to find the best trade-off between training throughput and achieving a 0%
loss in the Top-1 accuracy of the final model. Our experimental results on
training ViT-small from scratch show that SkipViT is capable of effectively
dropping 55% of the tokens while gaining more than 13% training throughput and
maintaining classification accuracy at the level of the baseline model on
Huawei Ascend910A.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15294">Integral Operator Approaches for Scattered Data Fitting on Spheres. (arXiv:2401.15294v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Lin_S/0/1/0/all/0/1">Shao-Bo Lin</a></p>
<p>This paper focuses on scattered data fitting problems on spheres. We study
the approximation performance of a class of weighted spectral filter
algorithms, including Tikhonov regularization, Landaweber iteration, spectral
cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded
random noise. For the analysis, we develop an integral operator approach that
can be regarded as an extension of the widely used sampling inequality approach
and norming set method in the community of scattered data fitting. After
providing an equivalence between the operator differences and quadrature rules,
we succeed in deriving optimal Sobolev-type error estimates of weighted
spectral filter algorithms. Our derived error estimates do not suffer from the
saturation phenomenon for Tikhonov regularization in the literature,
native-space-barrier for existing error analysis and adapts to different
embedding spaces. We also propose a divide-and-conquer scheme to equip weighted
spectral filter algorithms to reduce their computational burden and present the
optimal approximation error bounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15295">Multi-Trigger Backdoor Attacks: More Triggers, More Threats. (arXiv:2401.15295v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yige Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xingjun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiabo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hanxun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Backdoor attacks have emerged as a primary threat to (pre-)training and
deployment of deep neural networks (DNNs). While backdoor attacks have been
extensively studied in a body of works, most of them were focused on
single-trigger attacks that poison a dataset using a single type of trigger.
Arguably, real-world backdoor attacks can be much more complex, e.g., the
existence of multiple adversaries for the same dataset if it is of high value.
In this work, we investigate the practical threat of backdoor attacks under the
setting of \textbf{multi-trigger attacks} where multiple adversaries leverage
different types of triggers to poison the same dataset. By proposing and
investigating three types of multi-trigger attacks, including parallel,
sequential, and hybrid attacks, we provide a set of important understandings of
the coexisting, overwriting, and cross-activating effects between different
triggers on the same dataset. Moreover, we show that single-trigger attacks
tend to cause overly optimistic views of the security of current defense
techniques, as all examined defense methods struggle to defend against
multi-trigger attacks. Finally, we create a multi-trigger backdoor poisoning
dataset to help future evaluation of backdoor attacks and defenses. Although
our work is purely empirical, we hope it can help steer backdoor research
toward more realistic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15299">SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1">Azmine Toushik Wasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">MD Shafikul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Akib_A/0/1/0/all/0/1">Adipto Raihan Akib</a></p>
<p>Graph Neural Networks (GNNs) have gained traction across different domains
such as transportation, bio-informatics, language processing, and computer
vision. However, there is a noticeable absence of research on applying GNNs to
supply chain networks. Supply chain networks are inherently graph-like in
structure, making them prime candidates for applying GNN methodologies. This
opens up a world of possibilities for optimizing, predicting, and solving even
the most complex supply chain problems. A major setback in this approach lies
in the absence of real-world benchmark datasets to facilitate the research and
resolution of supply chain problems using GNNs. To address the issue, we
present a real-world benchmark dataset for temporal tasks, obtained from one of
the leading FMCG companies in Bangladesh, focusing on supply chain planning for
production purposes. The dataset includes temporal data as node features to
enable sales predictions, production planning, and the identification of
factory issues. By utilizing this dataset, researchers can employ GNNs to
address numerous supply chain problems, thereby advancing the field of supply
chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15304">Adaptive Least Mean Squares Graph Neural Networks and Online Graph Signal Estimation. (arXiv:2401.15304v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Changran Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuruoglu_E/0/1/0/all/0/1">Ercan Engin Kuruoglu</a></p>
<p>The online prediction of multivariate signals, existing simultaneously in
space and time, from noisy partial observations is a fundamental task in
numerous applications. We propose an efficient Neural Network architecture for
the online estimation of time-varying graph signals named the Adaptive Least
Mean Squares Graph Neural Networks (LMS-GNN). LMS-GNN aims to capture the time
variation and bridge the cross-space-time interactions under the condition that
signals are corrupted by noise and missing values. The LMS-GNN is a combination
of adaptive graph filters and Graph Neural Networks (GNN). At each time step,
the forward propagation of LMS-GNN is similar to adaptive graph filters where
the output is based on the error between the observation and the prediction
similar to GNN. The filter coefficients are updated via backpropagation as in
GNN. Experimenting on real-world temperature data reveals that our LMS-GNN
achieves more accurate online predictions compared to graph-based methods like
adaptive graph filters and graph convolutional neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15305">A Practical Probabilistic Benchmark for AI Weather Models. (arXiv:2401.15305v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Brenowitz_N/0/1/0/all/0/1">Noah D. Brenowitz</a>, <a href="http://arxiv.org/find/physics/1/au:+Cohen_Y/0/1/0/all/0/1">Yair Cohen</a>, <a href="http://arxiv.org/find/physics/1/au:+Pathak_J/0/1/0/all/0/1">Jaideep Pathak</a>, <a href="http://arxiv.org/find/physics/1/au:+Mahesh_A/0/1/0/all/0/1">Ankur Mahesh</a>, <a href="http://arxiv.org/find/physics/1/au:+Bonev_B/0/1/0/all/0/1">Boris Bonev</a>, <a href="http://arxiv.org/find/physics/1/au:+Kurth_T/0/1/0/all/0/1">Thorsten Kurth</a>, <a href="http://arxiv.org/find/physics/1/au:+Durran_D/0/1/0/all/0/1">Dale R. Durran</a>, <a href="http://arxiv.org/find/physics/1/au:+Harrington_P/0/1/0/all/0/1">Peter Harrington</a>, <a href="http://arxiv.org/find/physics/1/au:+Pritchard_M/0/1/0/all/0/1">Michael S. Pritchard</a></p>
<p>Since the weather is chaotic, forecasts aim to predict the distribution of
future states rather than make a single prediction. Recently, multiple data
driven weather models have emerged claiming breakthroughs in skill. However,
these have mostly been benchmarked using deterministic skill scores, and little
is known about their probabilistic skill. Unfortunately, it is hard to fairly
compare AI weather models in a probabilistic sense, since variations in choice
of ensemble initialization, definition of state, and noise injection
methodology become confounding. Moreover, even obtaining ensemble forecast
baselines is a substantial engineering challenge given the data volumes
involved. We sidestep both problems by applying a decades-old idea -- lagged
ensembles -- whereby an ensemble can be constructed from a moderately-sized
library of deterministic forecasts. This allows the first parameter-free
intercomparison of leading AI weather models' probabilistic skill against an
operational baseline. The results reveal that two leading AI weather models,
i.e. GraphCast and Pangu, are tied on the probabilistic CRPS metric even though
the former outperforms the latter in deterministic scoring. We also reveal how
multiple time-step loss functions, which many data-driven weather models have
employed, are counter-productive: they improve deterministic metrics at the
cost of increased dissipation, deteriorating probabilistic skill. This is
confirmed through ablations applied to a spherical Fourier Neural Operator
(SFNO) approach to AI weather forecasting. Separate SFNO ablations modulating
effective resolution reveal it has a useful effect on ensemble dispersion
relevant to achieving good ensemble calibration. We hope these and forthcoming
insights from lagged ensembles can help guide the development of AI weather
forecasts and have thus shared the diagnostic code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15318">Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yintong Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Ying Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zeshun Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1">Tianjia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hongzhi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a></p>
<p>We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15321">Localization of Dummy Data Injection Attacks in Power Systems Considering Incomplete Topological Information: A Spatio-Temporal Graph Wavelet Convolutional Neural Network Approach. (arXiv:2401.15321v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Qu_Z/0/1/0/all/0/1">Zhaoyang Qu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1">Yunchang Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1">Siqi Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_T/0/1/0/all/0/1">Tao Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1">Min Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1">Qiming Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Bo_X/0/1/0/all/0/1">Xiaoyong Bo</a>, <a href="http://arxiv.org/find/eess/1/au:+Zang_J/0/1/0/all/0/1">Jiye Zang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1">Qi Xu</a></p>
<p>The emergence of novel the dummy data injection attack (DDIA) poses a severe
threat to the secure and stable operation of power systems. These attacks are
particularly perilous due to the minimal Euclidean spatial separation between
the injected malicious data and legitimate data, rendering their precise
detection challenging using conventional distance-based methods. Furthermore,
existing research predominantly focuses on various machine learning techniques,
often analyzing the temporal data sequences post-attack or relying solely on
Euclidean spatial characteristics. Unfortunately, this approach tends to
overlook the inherent topological correlations within the non-Euclidean spatial
attributes of power grid data, consequently leading to diminished accuracy in
attack localization. To address this issue, this study takes a comprehensive
approach. Initially, it examines the underlying principles of these new DDIAs
on power systems. Here, an intricate mathematical model of the DDIA is
designed, accounting for incomplete topological knowledge and alternating
current (AC) state estimation from an attacker's perspective. Subsequently, by
integrating a priori knowledge of grid topology and considering the temporal
correlations within measurement data and the topology-dependent attributes of
the power grid, this study introduces temporal and spatial attention matrices.
These matrices adaptively capture the spatio-temporal correlations within the
attacks. Leveraging gated stacked causal convolution and graph wavelet sparse
convolution, the study jointly extracts spatio-temporal DDIA features. Finally,
the research proposes a DDIA localization method based on spatio-temporal graph
neural networks. The accuracy and effectiveness of the DDIA model are
rigorously demonstrated through comprehensive analytical cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15330">Optimal Sparse Survival Trees. (arXiv:2401.15330v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_R/0/1/0/all/0/1">Rui Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1">Margo Seltzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a></p>
<p>Interpretability is crucial for doctors, hospitals, pharmaceutical companies
and biotechnology corporations to analyze and make decisions for high stakes
problems that involve human health. Tree-based methods have been widely adopted
for \textit{survival analysis} due to their appealing interpretablility and
their ability to capture complex relationships. However, most existing methods
to produce survival trees rely on heuristic (or greedy) algorithms, which risk
producing sub-optimal models. We present a dynamic-programming-with-bounds
approach that finds provably-optimal sparse survival tree models, frequently in
only a few seconds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15335">L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Ping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingchuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingfu Zhang</a></p>
<p>In the rapidly evolving field of machine learning, adversarial attacks
present a significant challenge to model robustness and security.
Decision-based attacks, which only require feedback on the decision of a model
rather than detailed probabilities or scores, are particularly insidious and
difficult to defend against. This work introduces L-AutoDA (Large Language
Model-based Automated Decision-based Adversarial Attacks), a novel approach
leveraging the generative capabilities of Large Language Models (LLMs) to
automate the design of these attacks. By iteratively interacting with LLMs in
an evolutionary framework, L-AutoDA automatically designs competitive attack
algorithms efficiently without much human effort. We demonstrate the efficacy
of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline
methods in both success rate and computational efficiency. Our findings
underscore the potential of language models as tools for adversarial attack
generation and highlight new avenues for the development of robust AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15337">Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data. (arXiv:2401.15337v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zenghui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xintong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruichen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jingying Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Shenda Hong</a></p>
<p>Long-term fetal heart rate (FHR) monitoring during the antepartum period,
increasingly popularized by electronic FHR monitoring, represents a growing
approach in FHR monitoring. This kind of continuous monitoring, in contrast to
the short-term one, collects an extended period of fetal heart data. This
offers a more comprehensive understanding of fetus's conditions. However, the
interpretation of long-term antenatal fetal heart monitoring is still in its
early stages, lacking corresponding clinical standards. Furthermore, the
substantial amount of data generated by continuous monitoring imposes a
significant burden on clinical work when analyzed manually. To address above
challenges, this study develops an automatic analysis system named LARA
(Long-term Antepartum Risk Analysis system) for continuous FHR monitoring,
combining deep learning and information fusion methods. LARA's core is a
well-established convolutional neural network (CNN) model. It processes
long-term FHR data as input and generates a Risk Distribution Map (RDM) and
Risk Index (RI) as the analysis results. We evaluate LARA on inner test
dataset, the performance metrics are as follows: AUC 0.872, accuracy 0.816,
specificity 0.811, sensitivity 0.806, precision 0.271, and F1 score 0.415. In
our study, we observe that long-term FHR monitoring data with higher RI is more
likely to result in adverse outcomes (p=0.0021). In conclusion, this study
introduces LARA, the first automated analysis system for long-term FHR
monitoring, initiating the further explorations into its clinical value in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15377">Validation of artificial neural networks to model the acoustic behaviour of induction motors. (arXiv:2401.15377v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jimenez_Romero_F/0/1/0/all/0/1">F.J. Jimenez-Romero</a>, <a href="http://arxiv.org/find/cs/1/au:+Guijo_Rubio_D/0/1/0/all/0/1">D. Guijo-Rubio</a>, <a href="http://arxiv.org/find/cs/1/au:+Lara_Raya_F/0/1/0/all/0/1">F.R. Lara-Raya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_Gonzalez_A/0/1/0/all/0/1">A. Ruiz-Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hervas_Martinez_C/0/1/0/all/0/1">C. Hervas-Martinez</a></p>
<p>In the last decade, the sound quality of electric induction motors is a hot
topic in the research field. Specially, due to its high number of applications,
the population is exposed to physical and psychological discomfort caused by
the noise emission. Therefore, it is necessary to minimise its psychological
impact on the population. In this way, the main goal of this work is to
evaluate the use of multitask artificial neural networks as a modelling
technique for simultaneously predicting psychoacoustic parameters of induction
motors. Several inputs are used, such as, the electrical magnitudes of the
motor power signal and the number of poles, instead of separating the noise of
the electric motor from the environmental noise. Two different kind of
artificial neural networks are proposed to evaluate the acoustic quality of
induction motors, by using the equivalent sound pressure, the loudness, the
roughness and the sharpness as outputs. Concretely, two different topologies
have been considered: simple models and more complex models. The former are
more interpretable, while the later lead to higher accuracy at the cost of
hiding the cause-effect relationship. Focusing on the simple interpretable
models, product unit neural networks achieved the best results: for MSE and for
SEP. The main benefit of this product unit model is its simplicity, since only
10 inputs variables are used, outlining the effective transfer mechanism of
multitask artificial neural networks to extract common features of multiple
tasks. Finally, a deep analysis of the acoustic quality of induction motors in
done using the best product unit neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15417">Fault Diagnosis on Induction Motor using Machine Learning and Signal Processing. (arXiv:2401.15417v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Samiullah_M/0/1/0/all/0/1">Muhammad Samiullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1">Hasan Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Zahoor_S/0/1/0/all/0/1">Shehryar Zahoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1">Anas Ali</a></p>
<p>The detection and identification of induction motor faults using machine
learning and signal processing is a valuable approach to avoiding plant
disturbances and shutdowns in the context of Industry 4.0. In this work, we
present a study on the detection and identification of induction motor faults
using machine learning and signal processing with MATLAB Simulink. We developed
a model of a three-phase induction motor in MATLAB Simulink to generate healthy
and faulty motor data. The data collected included stator currents, rotor
currents, input power, slip, rotor speed, and efficiency. We generated four
faults in the induction motor: open circuit fault, short circuit fault,
overload, and broken rotor bars. We collected a total of 150,000 data points
with a 60-40% ratio of healthy to faulty motor data. We applied Fast Fourier
Transform (FFT) to detect and identify healthy and unhealthy conditions and
added a distinctive feature in our data. The generated dataset was trained
different machine learning models. On comparing the accuracy of the models on
the test set, we concluded that the Decision Tree algorithm performed the best
with an accuracy of about 92%. Our study contributes to the literature by
providing a valuable approach to fault detection and classification with
machine learning models for industrial applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15422">A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yue Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chenlu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuan Wu</a></p>
<p>Large models, encompassing large language and diffusion models, have shown
exceptional promise in approximating human-level intelligence, garnering
significant interest from both academic and industrial spheres. However, the
training of these large models necessitates vast quantities of high-quality
data, and with continuous updates to these models, the existing reservoir of
high-quality data may soon be depleted. This challenge has catalyzed a surge in
research focused on data augmentation methods. Leveraging large models, these
data augmentation techniques have outperformed traditional approaches. This
paper offers an exhaustive review of large model-driven data augmentation
methods, adopting a comprehensive perspective. We begin by establishing a
classification of relevant studies into three main categories: image
augmentation, text augmentation, and paired data augmentation. Following this,
we delve into various data post-processing techniques pertinent to large
model-based data augmentation. Our discussion then expands to encompass the
array of applications for these data augmentation methods within natural
language processing, computer vision, and audio signal processing. We proceed
to evaluate the successes and limitations of large model-based data
augmentation across different scenarios. Concluding our review, we highlight
prospective challenges and avenues for future exploration in the field of data
augmentation. Our objective is to furnish researchers with critical insights,
ultimately contributing to the advancement of more sophisticated large models.
We consistently maintain the related open-source materials at:
https://github.com/MLGroup-JLU/LLM-data-aug-survey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15434">Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI. (arXiv:2401.15434v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jingyun Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1">Yading Yuan</a></p>
<p>Federated Learning (FL) enables collaborative model training among medical
centers without sharing private data. However, traditional FL risks on server
failures and suboptimal performance on local data due to the nature of
centralized model aggregation. To address these issues, we present Gossip
Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for
direct peer-to-peer communication. In addition, GML encourages each site to
optimize its local model through mutual learning to account for data variations
among different sites. For the task of tumor segmentation using 146 cases from
four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed
local models and achieved similar performance as FedAvg with only 25%
communication overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15444">Towards Causal Classification: A Comprehensive Study on Graph Neural Networks. (arXiv:2401.15444v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Job_S/0/1/0/all/0/1">Simi Job</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xiaohui Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Taotao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Haoran Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1">Jianming Yong</a></p>
<p>The exploration of Graph Neural Networks (GNNs) for processing
graph-structured data has expanded, particularly their potential for causal
analysis due to their universal approximation capabilities. Anticipated to
significantly enhance common graph-based tasks such as classification and
prediction, the development of a causally enhanced GNN framework is yet to be
thoroughly investigated. Addressing this shortfall, our study delves into nine
benchmark graph classification models, testing their strength and versatility
across seven datasets spanning three varied domains to discern the impact of
causality on the predictive prowess of GNNs. This research offers a detailed
assessment of these models, shedding light on their efficiency, and flexibility
in different data environments, and highlighting areas needing advancement. Our
findings are instrumental in furthering the understanding and practical
application of GNNs in diverse datacentric fields
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15447">Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing. (arXiv:2401.15447v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagalapatti_L/0/1/0/all/0/1">Lokesh Nagalapatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1">Akshay Iyer</a>, <a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1">Abir De</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1">Sunita Sarawagi</a></p>
<p>We address the Individualized continuous treatment effect (ICTE) estimation
problem where we predict the effect of any continuous-valued treatment on an
individual using observational data. The main challenge in this estimation task
is the potential confounding of treatment assignment with an individual's
covariates in the training data, whereas during inference ICTE requires
prediction on independently sampled treatments. In contrast to prior work that
relied on regularizers or unstable GAN training, we advocate the direct
approach of augmenting training individuals with independently sampled
treatments and inferred counterfactual outcomes. We infer counterfactual
outcomes using a two-pronged strategy: a Gradient Interpolation for
close-to-observed treatments, and a Gaussian Process based Kernel Smoothing
which allows us to downweigh high variance inferences. We evaluate our method
on five benchmarks and show that our method outperforms six state-of-the-art
methods on the counterfactual estimation error. We analyze the superior
performance of our method by showing that (1) our inferred counterfactual
responses are more accurate, and (2) adding them to the training data reduces
the distributional distance between the confounded training distribution and
test distribution where treatment is independent of covariates. Our proposed
method is model-agnostic and we show that it improves ICTE accuracy of several
existing models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15455">New Foggy Object Detecting Model. (arXiv:2401.15455v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banavathu_R/0/1/0/all/0/1">Rahul Banavathu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sree_M/0/1/0/all/0/1">Modem Veda Sree</a>, <a href="http://arxiv.org/find/cs/1/au:+Sri_B/0/1/0/all/0/1">Bollina Kavya Sri</a>, <a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1">Suddhasil De</a></p>
<p>Object detection in reduced visibility has become a prominent research area.
The existing techniques are not accurate enough in recognizing objects under
such circumstances. This paper introduces a new foggy object detection method
through a two-staged architecture of region identification from input images
and detecting objects in such regions. The paper confirms notable improvements
of the proposed method's accuracy and detection time over existing techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1902.08412">Adversarial Attacks on Graph Neural Networks via Meta Learning. (arXiv:1902.08412v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1">Daniel Z&#xfc;gner</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Deep learning models for graphs have advanced the state of the art on many
tasks. Despite their recent success, little is known about their robustness. We
investigate training time attacks on graph neural networks for node
classification that perturb the discrete graph structure. Our core principle is
to use meta-gradients to solve the bilevel problem underlying training-time
attacks, essentially treating the graph as a hyperparameter to optimize. Our
experiments show that small graph perturbations consistently lead to a strong
decrease in performance for graph convolutional networks, and even transfer to
unsupervised embeddings. Remarkably, the perturbations created by our algorithm
can misguide the graph neural networks such that they perform worse than a
simple baseline that ignores all relational information. Our attacks do not
assume any knowledge about or access to the target classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.10862">An Intuitive Tutorial to Gaussian Process Regression. (arXiv:2009.10862v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a></p>
<p>This tutorial aims to provide an intuitive introduction to Gaussian process
regression (GPR). GPR models have been widely used in machine learning
applications due to their representation flexibility and inherent capability to
quantify uncertainty over predictions. The tutorial starts with explaining the
basic concepts that a Gaussian process is built on, including multivariate
normal distribution, kernels, non-parametric models, and joint and conditional
probability. It then provides a concise description of GPR and an
implementation of a standard GPR algorithm. In addition, the tutorial reviews
packages for implementing state-of-the-art Gaussian process algorithms. This
tutorial is accessible to a broad audience, including those new to machine
learning, ensuring a clear understanding of GPR fundamentals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.16271">View selection in multi-view stacking: Choosing the meta-learner. (arXiv:2010.16271v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Loon_W/0/1/0/all/0/1">Wouter van Loon</a>, <a href="http://arxiv.org/find/stat/1/au:+Fokkema_M/0/1/0/all/0/1">Marjolein Fokkema</a>, <a href="http://arxiv.org/find/stat/1/au:+Szabo_B/0/1/0/all/0/1">Botond Szabo</a>, <a href="http://arxiv.org/find/stat/1/au:+Rooij_M/0/1/0/all/0/1">Mark de Rooij</a></p>
<p>Multi-view stacking is a framework for combining information from different
views (i.e. different feature sets) describing the same set of objects. In this
framework, a base-learner algorithm is trained on each view separately, and
their predictions are then combined by a meta-learner algorithm. In a previous
study, stacked penalized logistic regression, a special case of multi-view
stacking, has been shown to be useful in identifying which views are most
important for prediction. In this article we expand this research by
considering seven different algorithms to use as the meta-learner, and
evaluating their view selection and classification performance in simulations
and two applications on real gene-expression data sets. Our results suggest
that if both view selection and classification accuracy are important to the
research at hand, then the nonnegative lasso, nonnegative adaptive lasso and
nonnegative elastic net are suitable meta-learners. Exactly which among these
three is to be preferred depends on the research context. The remaining four
meta-learners, namely nonnegative ridge regression, nonnegative forward
selection, stability selection and the interpolating predictor, show little
advantages in order to be preferred over the other three.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.14331">Methods to integrate multinormals and compute classification measures. (arXiv:2012.14331v11 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1">Abhranil Das</a>, <a href="http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1">Wilson S Geisler</a></p>
<p>Univariate and multivariate normal probability distributions are widely used
when modeling decisions under uncertainty. Computing the performance of such
models requires integrating these distributions over specific domains, which
can vary widely across models. Besides some special cases, there exist no
general analytical expressions, standard numerical methods or software for
these integrals. Here we present mathematical results and open-source software
that provide (i) the probability in any domain of a normal in any dimensions
with any parameters, (ii) the probability density, cumulative distribution, and
inverse cumulative distribution of any function of a normal vector, (iii) the
classification errors among any number of normal distributions, the
Bayes-optimal discriminability index and relation to the operating
characteristic, (iv) dimension reduction and visualizations for such problems,
and (v) tests for how reliably these methods may be used on given data. We
demonstrate these tools with vision research applications of detecting
occluding objects in natural scenes, and detecting camouflage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.04565">Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations. (arXiv:2103.04565v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haimin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Min Xu</a></p>
<p>Deep neural networks have been successfully applied in various machine
learning tasks. However, studies show that neural networks are susceptible to
adversarial attacks. This exposes a potential threat to neural network-based
intelligent systems. We observe that the probability of the correct result
outputted by the neural network increases by applying small first-order
perturbations generated for non-predicted class labels to adversarial examples.
Based on this observation, we propose a method for counteracting adversarial
perturbations to improve adversarial robustness. In the proposed method, we
randomly select a number of class labels and generate small first-order
perturbations for these selected labels. The generated perturbations are added
together and then clamped onto a specified space. The obtained perturbation is
finally added to the adversarial example to counteract the adversarial
perturbation contained in the example. The proposed method is applied at
inference time and does not require retraining or finetuning the model. We
experimentally validate the proposed method on CIFAR-10 and CIFAR-100. The
results demonstrate that our method effectively improves the defense
performance of several transformation-based defense methods, especially against
strong adversarial examples generated using more iterations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.12833">Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling. (arXiv:2105.12833v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Sajiv Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1">Ayaan Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a></p>
<p>Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.00783">Computer Vision Self-supervised Learning Methods on Time Series. (arXiv:2109.00783v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Daesoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1">Erlend Aune</a></p>
<p>Self-supervised learning (SSL) has had great success in both computer vision.
Most of the current mainstream computer vision SSL frameworks are based on
Siamese network architecture. These approaches often rely on cleverly crafted
loss functions and training setups to avoid feature collapse. In this study, we
evaluate if those computer-vision SSL frameworks are also effective on a
different modality (\textit{i.e.,} time series). The effectiveness is
experimented and evaluated on the UCR and UEA archives, and we show that the
computer vision SSL frameworks can be effective even for time series. In
addition, we propose a new method that improves on the recently proposed VICReg
method. Our method improves on a \textit{covariance} term proposed in VICReg,
and in addition we augment the head of the architecture by an iterative
normalization layer that accelerates the convergence of the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.03772">Particle Transformer for Jet Tagging. (arXiv:2202.03772v3 [hep-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Qu_H/0/1/0/all/0/1">Huilin Qu</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Li_C/0/1/0/all/0/1">Congqiao Li</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Qian_S/0/1/0/all/0/1">Sitian Qian</a></p>
<p>Jet tagging is a critical yet challenging classification task in particle
physics. While deep learning has transformed jet tagging and significantly
improved performance, the lack of a large-scale public dataset impedes further
enhancement. In this work, we present JetClass, a new comprehensive dataset for
jet tagging. The JetClass dataset consists of 100 M jets, about two orders of
magnitude larger than existing public datasets. A total of 10 types of jets are
simulated, including several types unexplored for tagging so far. Based on the
large dataset, we propose a new Transformer-based architecture for jet tagging,
called Particle Transformer (ParT). By incorporating pairwise particle
interactions in the attention mechanism, ParT achieves higher tagging
performance than a plain Transformer and surpasses the previous
state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models,
once fine-tuned, also substantially enhance the performance on two widely
adopted jet tagging benchmarks. The dataset, code and models are publicly
available at https://github.com/jet-universe/particle_transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.12435">Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (arXiv:2202.12435v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Cong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a></p>
<p>The adversarial robustness of a neural network mainly relies on two factors:
model capacity and anti-perturbation ability. In this paper, we study the
anti-perturbation ability of the network from the feature maps of convolutional
layers. Our theoretical analysis discovers that larger convolutional feature
maps before average pooling can contribute to better resistance to
perturbations, but the conclusion is not true for max pooling. It brings new
inspiration to the design of robust neural networks and urges us to apply these
findings to improve existing architectures. The proposed modifications are very
simple and only require upsampling the inputs or slightly modifying the stride
configurations of downsampling operators. We verify our approaches on several
benchmark neural network architectures, including AlexNet, VGG, RestNet18, and
PreActResNet18. Non-trivial improvements in terms of both natural accuracy and
adversarial robustness can be achieved under various attack and defense
mechanisms. The code is available at \url{https://github.com/MTandHJ/rcm}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01327">Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mantripragada_K/0/1/0/all/0/1">Kiran Mantripragada</a>, <a href="http://arxiv.org/find/eess/1/au:+Qureshi_F/0/1/0/all/0/1">Faisal Z. Qureshi</a></p>
<p>We present a method for hyperspectral pixel {\it unmixing}. The proposed
method assumes that (1) {\it abundances} can be encoded as Dirichlet
distributions and (2) spectra of {\it endmembers} can be represented as
multivariate Normal distributions. The method solves the problem of abundance
estimation and endmember extraction within a variational autoencoder setting
where a Dirichlet bottleneck layer models the abundances, and the decoder
performs endmember extraction. The proposed method can also leverage transfer
learning paradigm, where the model is only trained on synthetic data containing
pixels that are linear combinations of one or more endmembers of interest. In
this case, we retrieve endmembers (spectra) from the United States Geological
Survey Spectral Library. The model thus trained can be subsequently used to
perform pixel unmixing on "real data" that contains a subset of the endmembers
used to generated the synthetic data. The model achieves state-of-the-art
results on several benchmarks: Cuprite, Urban Hydice and Samson. We also
present new synthetic dataset, OnTech-HSI-Syn-21, that can be used to study
hyperspectral pixel unmixing methods. We showcase the transfer learning
capabilities of the proposed model on Cuprite and OnTech-HSI-Syn-21 datasets.
In summary, the proposed method can be applied for pixel unmixing a variety of
domains, including agriculture, forestry, mineralogy, analysis of materials,
healthcare, etc. Additionally, the proposed method eschews the need for
labelled data for training by leveraging the transfer learning paradigm, where
the model is trained on synthetic data generated using the endmembers present
in the "real" data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.07767">Towards cost-effective and resource-aware aggregation at Edge for Federated Learning. (arXiv:2204.07767v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Ahmad Faraz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1">Sabaat Haroon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1">Haider Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yue Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Butt_A/0/1/0/all/0/1">Ali R. Butt</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Ali Anwar</a></p>
<p>Federated Learning (FL) is a machine learning approach that addresses privacy
and data transfer costs by computing data at the source. It's particularly
popular for Edge and IoT applications where the aggregator server of FL is in
resource-capped edge data centers for reducing communication costs. Existing
cloud-based aggregator solutions are resource-inefficient and expensive at the
Edge, leading to low scalability and high latency. To address these challenges,
this study compares prior and new aggregation methodologies under the changing
demands of IoT and Edge applications. This work is the first to propose an
adaptive FL aggregator at the Edge, enabling users to manage the cost and
efficiency trade-off. An extensive comparative analysis demonstrates that the
design improves scalability by up to 4X, time efficiency by 8X, and reduces
costs by more than 2X compared to extant cloud-based static methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14570">MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiahao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawei Song</a></p>
<p>Recent studies have uncovered that language model distillation is less
effective when facing a large capacity gap between the teacher and the student,
and introduced teacher assistant-based distillation to bridge the gap. As a
connection, the scale and the performance of the teacher assistant is of vital
importance to bring the knowledge from the teacher to the student. However,
existing teacher assistant-based methods require maximally many trials before
scheduling an optimal teacher assistant. To this end, we propose a minimal
distillation schedule (MiniDisc) for scheduling the optimal teacher assistant
in minimally one trial. In particular, motivated by the finding that the
performance of the student is positively correlated to the scale-performance
tradeoff of the teacher assistant, MiniDisc is designed with a
$\lambda$-tradeoff to measure the optimality of the teacher assistant without
trial distillation to the student. MiniDisc then can schedule the optimal
teacher assistant with the best $\lambda$-tradeoff in a sandwich framework.
MiniDisc is evaluated with an extensive set of experiments on GLUE.
Experimental results demonstrate the improved efficiency our MiniDisc compared
to several state-of-the-art baselines. We further apply MiniDisc to a language
model with billions of parameters and show its scalability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02286">AugLoss: A Robust Augmentation-based Fine Tuning Methodology. (arXiv:2206.02286v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Otstot_K/0/1/0/all/0/1">Kyle Otstot</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1">Andrew Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cava_J/0/1/0/all/0/1">John Kevin Cava</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1">Lalitha Sankar</a></p>
<p>Deep Learning (DL) models achieve great successes in many domains. However,
DL models increasingly face safety and robustness concerns, including noisy
labeling in the training stage and feature distribution shifts in the testing
stage. Previous works made significant progress in addressing these problems,
but the focus has largely been on developing solutions for only one problem at
a time. For example, recent work has argued for the use of tunable robust loss
functions to mitigate label noise, and data augmentation (e.g., AugMix) to
combat distribution shifts. As a step towards addressing both problems
simultaneously, we introduce AugLoss, a simple but effective methodology that
achieves robustness against both train-time noisy labeling and test-time
feature distribution shifts by unifying data augmentation and robust loss
functions. We conduct comprehensive experiments in varied settings of
real-world dataset corruption to showcase the gains achieved by AugLoss
compared to previous state-of-the-art methods. Lastly, we hope this work will
open new directions for designing more robust and reliable DL models under
real-world corruptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03183">Risk Measures and Upper Probabilities: Coherence and Stratification. (arXiv:2206.03183v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frohlich_C/0/1/0/all/0/1">Christian Fr&#xf6;hlich</a>, <a href="http://arxiv.org/find/cs/1/au:+Williamson_R/0/1/0/all/0/1">Robert C. Williamson</a></p>
<p>Machine learning typically presupposes classical probability theory which
implies that aggregation is built upon expectation. There are now multiple
reasons to motivate looking at richer alternatives to classical probability
theory as a mathematical foundation for machine learning. We systematically
examine a powerful and rich class of alternative aggregation functionals, known
variously as spectral risk measures, Choquet integrals or Lorentz norms. We
present a range of characterization results, and demonstrate what makes this
spectral family so special. In doing so we arrive at a natural stratification
of all coherent risk measures in terms of the upper probabilities that they
induce by exploiting results from the theory of rearrangement invariant Banach
spaces. We empirically demonstrate how this new approach to uncertainty helps
tackling practical machine learning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05581">Federated Offline Reinforcement Learning. (arXiv:2206.05581v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1">Doudou Zhou</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1">Yufeng Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Sonabend_W_A/0/1/0/all/0/1">Aaron Sonabend-W</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1">Junwei Lu</a>, <a href="http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1">Tianxi Cai</a></p>
<p>Evidence-based or data-driven dynamic treatment regimes are essential for
personalized medicine, which can benefit from offline reinforcement learning
(RL). Although massive healthcare data are available across medical
institutions, they are prohibited from sharing due to privacy constraints.
Besides, heterogeneity exists in different sites. As a result, federated
offline RL algorithms are necessary and promising to deal with the problems. In
this paper, we propose a multi-site Markov decision process model that allows
for both homogeneous and heterogeneous effects across sites. The proposed model
makes the analysis of the site-level features possible. We design the first
federated policy optimization algorithm for offline RL with sample complexity.
The proposed algorithm is communication-efficient, which requires only a single
round of communication interaction by exchanging summary statistics. We give a
theoretical guarantee for the proposed algorithm, where the suboptimality for
the learned policies is comparable to the rate as if data is not distributed.
Extensive simulations demonstrate the effectiveness of the proposed algorithm.
The method is applied to a sepsis dataset in multiple sites to illustrate its
use in clinical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11396">Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning. (arXiv:2206.11396v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McInroe_T/0/1/0/all/0/1">Trevor McInroe</a>, <a href="http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1">Lukas Sch&#xe4;fer</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a></p>
<p>Learning control from pixels is difficult for reinforcement learning (RL)
agents because representation learning and policy learning are intertwined.
Previous approaches remedy this issue with auxiliary representation learning
tasks, but they either do not consider the temporal aspect of the problem or
only consider single-step transitions, which may cause learning inefficiencies
if important environmental changes take many steps to manifest. We propose
Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns multiple
representations via a hierarchy of forward models that learn to communicate and
an ensemble of $n$-step critics that all operate at varying magnitudes of step
skipping. We evaluate HKSL in a suite of 30 robotic control tasks with and
without distractors and a task of our creation. We find that HKSL either
converges to higher or optimal episodic returns more quickly than several
alternative representation learning approaches. Furthermore, we find that
HKSL's representations capture task-relevant details accurately across
timescales (even in the presence of distractors) and that communication
channels between hierarchy levels organize information based on both sides of
the communication process, both of which improve sample efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.00109">A DeepParticle method for learning and generating aggregation patterns in multi-dimensional Keller-Segel chemotaxis systems. (arXiv:2209.00109v2 [physics.comp-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1">Zhongjian Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Xin_J/0/1/0/all/0/1">Jack Xin</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiwen Zhang</a></p>
<p>We study a regularized interacting particle method for computing aggregation
patterns and near singular solutions of a Keller-Segal (KS) chemotaxis system
in two and three space dimensions, then further develop DeepParticle (DP)
method to learn and generate solutions under variations of physical parameters.
The KS solutions are approximated as empirical measures of particles which
self-adapt to the high gradient part of solutions. We utilize the
expressiveness of deep neural networks (DNNs) to represent the transform of
samples from a given initial (source) distribution to a target distribution at
finite time T prior to blowup without assuming invertibility of the transforms.
In the training stage, we update the network weights by minimizing a discrete
2-Wasserstein distance between the input and target empirical measures. To
reduce computational cost, we develop an iterative divide-and-conquer algorithm
to find the optimal transition matrix in the Wasserstein distance. We present
numerical results of DP framework for successful learning and generation of KS
dynamics in the presence of laminar and chaotic flows. The physical parameter
in this work is either the small diffusivity of chemo-attractant or the
reciprocal of the flow amplitude in the advection-dominated regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.07661">On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhou Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1">Kathleen McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a></p>
<p>In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12288">Learning Ultrametric Trees for Optimal Transport Regression. (arXiv:2210.12288v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Samantha Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1">Puoya Tabaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yusu Wang</a></p>
<p>Optimal transport provides a metric which quantifies the dissimilarity
between probability measures. For measures supported in discrete metric spaces,
finding the optimal transport distance has cubic time complexity in the size of
the space. However, measures supported on trees admit a closed-form optimal
transport that can be computed in linear time. In this paper, we aim to find an
optimal tree structure for a given discrete metric space so that the
tree-Wasserstein distance approximates the optimal transport distance in the
original space. One of our key ideas is to cast the problem in ultrametric
spaces. This helps us optimize over the space of ultrametric trees -- a
mixed-discrete and continuous optimization problem -- via projected gradient
decent over the space of ultrametric matrices. During optimization, we project
the parameters to the ultrametric space via a hierarchical minimum spanning
tree algorithm, equivalent to the closest projection to ultrametrics under the
supremum norm. Experimental results on real datasets show that our approach
outperforms previous approaches (e.g. Flowtree, Quadtree) in approximating
optimal transport distances. Finally, experiments on synthetic data generated
on ground truth trees show that our algorithm can accurately uncover the
underlying trees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14164">No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1">Hanieh Naderi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinesh_C/0/1/0/all/0/1">Chinthaka Dinesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1">Ivan V. Bajic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1">Shohreh Kasaei</a></p>
<p>Adversarial attacks pose serious challenges for deep neural network
(DNN)-based analysis of various input signals. In the case of 3D point clouds,
methods have been developed to identify points that play a key role in network
decision, and these become crucial in generating existing adversarial attacks.
For example, a saliency map approach is a popular method for identifying
adversarial drop points, whose removal would significantly impact the network
decision. Generally, methods for identifying adversarial points rely on the
access to the DNN model itself to determine which points are critically
important for the model's decision. This paper aims to provide a novel
viewpoint on this problem, where adversarial points can be predicted without
access to the target DNN model, which is referred to as a ``no-box'' attack. To
this end, we define 14 point cloud features and use multiple linear regression
to examine whether these features can be used for adversarial point prediction,
and which combination of features is best suited for this purpose. Experiments
show that a suitable combination of features is able to predict adversarial
points of four different networks -- PointNet, PointNet++, DGCNN, and PointConv
-- significantly better than a random guess and comparable to white-box
attacks. Additionally, we show that no-box attack is transferable to unseen
models. The results also provide further insight into DNNs for point cloud
classification, by showing which features play key roles in their
decision-making process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06236">Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization. (arXiv:2211.06236v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kucukoglu_B/0/1/0/all/0/1">Burcu K&#xfc;&#xe7;&#xfc;ko&#x11f;lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Borkent_W/0/1/0/all/0/1">Walraaf Borkent</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckauer_B/0/1/0/all/0/1">Bodo Rueckauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1">Nasir Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Guclu_U/0/1/0/all/0/1">Umut G&#xfc;&#xe7;l&#xfc;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerven_M/0/1/0/all/0/1">Marcel van Gerven</a></p>
<p>Advances in reinforcement learning (RL) often rely on massive compute
resources and remain notoriously sample inefficient. In contrast, the human
brain is able to efficiently learn effective control strategies using limited
resources. This raises the question whether insights from neuroscience can be
used to improve current RL methods. Predictive processing is a popular
theoretical framework which maintains that the human brain is actively seeking
to minimize surprise. We show that recurrent neural networks which predict
their own sensory states can be leveraged to minimise surprise, yielding
substantial gains in cumulative reward. Specifically, we present the Predictive
Processing Proximal Policy Optimization (P4O) agent; an actor-critic
reinforcement learning agent that applies predictive processing to a recurrent
variant of the PPO algorithm by integrating a world model in its hidden state.
Even without hyperparameter tuning, P4O significantly outperforms a baseline
recurrent variant of the PPO algorithm on multiple Atari games using a single
GPU. It also outperforms other state-of-the-art agents given the same
wall-clock time and exceeds human gamer performance on multiple games including
Seaquest, which is a particularly challenging environment in the Atari domain.
Altogether, our work underscores how insights from the field of neuroscience
may support the development of more capable and efficient artificial agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07092">Offline Estimation of Controlled Markov Chains: Minimaxity and Sample Complexity. (arXiv:2211.07092v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Banerjee_I/0/1/0/all/0/1">Imon Banerjee</a>, <a href="http://arxiv.org/find/stat/1/au:+Honnappa_H/0/1/0/all/0/1">Harsha Honnappa</a>, <a href="http://arxiv.org/find/stat/1/au:+Rao_V/0/1/0/all/0/1">Vinayak Rao</a></p>
<p>In this work, we study a natural nonparametric estimator of the transition
probability matrices of a finite controlled Markov chain. We consider an
offline setting with a fixed dataset, collected using a so-called logging
policy. We develop sample complexity bounds for the estimator and establish
conditions for minimaxity. Our statistical bounds depend on the logging policy
through its mixing properties. We show that achieving a particular statistical
risk bound involves a subtle and interesting trade-off between the strength of
the mixing properties and the number of samples. We demonstrate the validity of
our results under various examples, such as ergodic Markov chains, weakly
ergodic inhomogeneous Markov chains, and controlled Markov chains with
non-stationary Markov, episodic, and greedy controls. Lastly, we use these
sample complexity bounds to establish concomitant ones for offline evaluation
of stationary Markov control policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.09949">Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tzu-Quan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tsung-Huan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chun-Yao Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kuang-Ming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tzu-hsun Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a></p>
<p>Despite the success of Transformers in self- supervised learning with
applications to various downstream tasks, the computational cost of training
and inference remains a major challenge for applying these models to a wide
spectrum of devices. Several isolated attempts have been made to compress
Transformers, but the settings and metrics are different across studies.
Trade-off at various compression rates are also largely missing in prior work,
making it difficult to compare compression techniques. In this work, we aim to
provide context for the isolated results, studying several commonly used
compression techniques, including weight pruning, head pruning, low-rank
approximation, and knowledge distillation. We report trade- off at various
compression rate, including wall-clock time, the number of parameters, and the
number of multiply-accumulate operations. Our results show that compared to
recent approaches, basic compression techniques are strong baselines. We
further present several applications of our results, revealing properties of
Transformers, such as the significance of diagonal attention heads. In
addition, our results lead to a simple combination of compression techniques
that improves trade-off over recent approaches. We hope the results would
promote more diverse comparisons among model compression techniques and promote
the use of model compression as a tool for analyzing models. Our code of
compressing speech self-supervised model is available at
https://github.com/nervjack2/Speech-SSL-Compression/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.15223">Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning. (arXiv:2211.15223v4 [math.AP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Bungert_L/0/1/0/all/0/1">Leon Bungert</a>, <a href="http://arxiv.org/find/math/1/au:+Stinson_K/0/1/0/all/0/1">Kerrek Stinson</a></p>
<p>In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski
type to a local anisotropic perimeter. The nonlocal model describes the
regularizing effect of adversarial training in binary classifications. The
energy essentially depends on the interaction between two distributions
modelling likelihoods for the associated classes. We overcome typical strict
regularity assumptions for the distributions by only assuming that they have
bounded $BV$ densities. In the natural topology coming from compactness, we
prove Gamma-convergence to a weighted perimeter with weight determined by an
anisotropic function of the two densities. Despite being local, this sharp
interface limit reflects classification stability with respect to adversarial
perturbations. We further apply our results to deduce Gamma-convergence of the
associated total variations, to study the asymptotics of adversarial training,
and to prove Gamma-convergence of graph discretizations for the nonlocal
perimeter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01642">CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis. (arXiv:2301.01642v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zheng_K/0/1/0/all/0/1">Kaizhong Zheng</a>, <a href="http://arxiv.org/find/stat/1/au:+Yu_S/0/1/0/all/0/1">Shujian Yu</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_B/0/1/0/all/0/1">Badong Chen</a></p>
<p>There is a recent trend to leverage the power of graph neural networks (GNNs)
for brain-network based psychiatric diagnosis, which,in turn, also motivates an
urgent need for psychiatrists to fully understand the decision behavior of the
used GNNs. However, most of the existing GNN explainers are either post-hoc in
which another interpretive model needs to be created to explain a well-trained
GNN, or do not consider the causal relationship between the extracted
explanation and the decision, such that the explanation itself contains
spurious correlations and suffers from weak faithfulness. In this work, we
propose a granger causality-inspired graph neural network (CI-GNN), a built-in
interpretable model that is able to identify the most influential subgraph
(i.e., functional connectivity within brain regions) that is causally related
to the decision (e.g., major depressive disorder patients or healthy controls),
without the training of an auxillary interpretive network. CI-GNN learns
disentangled subgraph-level representations {\alpha} and \b{eta} that encode,
respectively, the causal and noncausal aspects of original graph under a graph
variational autoencoder framework, regularized by a conditional mutual
information (CMI) constraint. We theoretically justify the validity of the CMI
regulation in capturing the causal relationship. We also empirically evaluate
the performance of CI-GNN against three baseline GNNs and four state-of-the-art
GNN explainers on synthetic data and three large-scale brain disease datasets.
We observe that CI-GNN achieves the best performance in a wide range of metrics
and provides more reliable and concise explanations which have clinical
evidence.The source code and implementation details of CI-GNN are freely
available at GitHub repository (https://github.com/ZKZ-Brain/CI-GNN/).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08897">ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Sahil Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1">Martin Swany</a></p>
<p>Distributed deep learning (DDL) training systems are designed for cloud and
data-center environments that assumes homogeneous compute resources, high
network bandwidth, sufficient memory and storage, as well as independent and
identically distributed (IID) data across all nodes. However, these assumptions
don't necessarily apply on the edge, especially when training neural networks
on streaming data in an online manner. Computing on the edge suffers from both
systems and statistical heterogeneity. Systems heterogeneity is attributed to
differences in compute resources and bandwidth specific to each device, while
statistical heterogeneity comes from unbalanced and skewed data on the edge.
Different streaming-rates among devices can be another source of heterogeneity
when dealing with streaming data. If the streaming rate is lower than training
batch-size, device needs to wait until enough samples have streamed in before
performing a single iteration of stochastic gradient descent (SGD). Thus,
low-volume streams act like stragglers slowing down devices with high-volume
streams in synchronous training. On the other hand, data can accumulate quickly
in the buffer if the streaming rate is too high and the devices can't train at
line-rate. In this paper, we introduce ScaDLES to efficiently train on
streaming data at the edge in an online fashion, while also addressing the
challenges of limited bandwidth and training with non-IID data. We empirically
show that ScaDLES converges up to 3.29 times faster compared to conventional
distributed SGD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12132">AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xingchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a></p>
<p>Large pretrained language models are widely used in downstream NLP tasks via
task-specific fine-tuning, but such procedures can be costly. Recently,
Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task
performance while updating much fewer parameters than full model fine-tuning
(FFT). However, it is non-trivial to make informed design choices on the PEFT
configurations, such as their architecture, the number of tunable parameters,
and even the layers in which the PEFT modules are inserted. Consequently, it is
highly likely that the current, manually designed configurations are suboptimal
in terms of their performance-efficiency trade-off. Inspired by advances in
neural architecture search, we propose AutoPEFT for automatic PEFT
configuration selection: we first design an expressive configuration search
space with multiple representative PEFT modules as building blocks. Using
multi-objective Bayesian optimisation in a low-cost setup, we then discover a
Pareto-optimal set of configurations with strong performance-cost trade-offs
across different numbers of parameters that are also highly transferable across
different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that
AutoPEFT-discovered configurations significantly outperform existing PEFT
methods and are on par or better than FFT without incurring substantial
training efficiency costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01928">Aligning Robot and Human Representations. (arXiv:2302.01928v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bobu_A/0/1/0/all/0/1">Andreea Bobu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Andi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1">Julie Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca D. Dragan</a></p>
<p>To act in the world, robots rely on a representation of salient task aspects:
for example, to carry a coffee mug, a robot may consider movement efficiency or
mug orientation in its behavior. However, if we want robots to act for and with
people, their representations must not be just functional but also reflective
of what humans care about, i.e. they must be aligned. We observe that current
learning approaches suffer from representation misalignment, where the robot's
learned representation does not capture the human's representation. We suggest
that because humans are the ultimate evaluator of robot performance, we must
explicitly focus our efforts on aligning learned representations with humans,
in addition to learning the downstream task. We advocate that current
representation learning approaches in robotics should be studied from the
perspective of how well they accomplish the objective of representation
alignment. We mathematically define the problem, identify its key desiderata,
and situate current methods within this formalism. We conclude by suggesting
future directions for exploring open challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02759">Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ren Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Sunyang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_N/0/1/0/all/0/1">Nansu Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongfang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Ming Huang</a></p>
<p>Depression is a widespread mental health issue, affecting an estimated 3.8%
of the global population. It is also one of the main contributors to disability
worldwide. Recently it is becoming popular for individuals to use social media
platforms (e.g., Reddit) to express their difficulties and health issues (e.g.,
depression) and seek support from other users in online communities. It opens
great opportunities to automatically identify social media users with
depression by parsing millions of posts for potential interventions. Deep
learning methods have begun to dominate in the field of machine learning and
natural language processing (NLP) because of their ease of use, efficient
processing, and state-of-the-art results on many NLP tasks. In this work, we
propose a hybrid deep learning model which combines a pretrained sentence BERT
(SBERT) and convolutional neural network (CNN) to detect individuals with
depression with their Reddit posts. The sentence BERT is used to learn the
meaningful representation of semantic information in each post. CNN enables the
further transformation of those embeddings and the temporal identification of
behavioral patterns of users. We trained and evaluated the model performance to
identify Reddit users with depression by utilizing the Self-reported Mental
Health Diagnoses (SMHD) data. The hybrid deep learning model achieved an
accuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art
documented result (F1 score of 0.79) by other machine learning models in the
literature. The results show the feasibility of the hybrid model to identify
individuals with depression. Although the hybrid model is validated to detect
depression with Reddit posts, it can be easily tuned and applied to other text
classification tasks and different clinical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11529">Modular Deep Learning. (arXiv:2302.11529v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1">Jonas Pfeiffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1">Sebastian Ruder</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1">Edoardo Maria Ponti</a></p>
<p>Transfer learning has recently become the dominant paradigm of machine
learning. Pre-trained models fine-tuned for downstream tasks achieve better
performance with fewer labelled examples. Nonetheless, it remains unclear how
to develop models that specialise towards multiple tasks without incurring
negative interference and that generalise systematically to non-identically
distributed tasks. Modular deep learning has emerged as a promising solution to
these challenges. In this framework, units of computation are often implemented
as autonomous parameter-efficient modules. Information is conditionally routed
to a subset of modules and subsequently aggregated. These properties enable
positive transfer and systematic generalisation by separating computation from
routing and updating modules locally. We offer a survey of modular
architectures, providing a unified view over several threads of research that
evolved independently in the scientific literature. Moreover, we explore
various additional purposes of modularity, including scaling language models,
causal inference, programme induction, and planning in reinforcement learning.
Finally, we report various concrete applications where modularity has been
successfully deployed such as cross-lingual and cross-modal knowledge transfer.
Related talks and projects to this survey, are available at
https://www.modulardeeplearning.com/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12094">Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Munoz_C/0/1/0/all/0/1">Cristian Munoz</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_K/0/1/0/all/0/1">Kleyton da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Modenesi_B/0/1/0/all/0/1">Bernardo Modenesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Koshiyama_A/0/1/0/all/0/1">Adriano Koshiyama</a></p>
<p>Rapid advancements in artificial intelligence (AI) technology have brought
about a plethora of new challenges in terms of governance and regulation. AI
systems are being integrated into various industries and sectors, creating a
demand from decision-makers to possess a comprehensive and nuanced
understanding of the capabilities and limitations of these systems. One
critical aspect of this demand is the ability to explain the results of machine
learning models, which is crucial to promoting transparency and trust in AI
systems, as well as fundamental in helping machine learning models to be
trained ethically. In this paper, we present novel metrics to quantify the
degree of which AI model predictions can be easily explainable by its features.
Our metrics summarize different aspects of explainability into scalars,
providing a more comprehensive understanding of model predictions and
facilitating communication between decision-makers and stakeholders, thereby
increasing the overall transparency and accountability of AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04488">Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mikula_M/0/1/0/all/0/1">Maciej Miku&#x142;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Antoniak_S/0/1/0/all/0/1">Szymon Antoniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1">Szymon Tworkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Albert Qiaochu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jin Peng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1">Christian Szegedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1">&#x141;ukasz Kuci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1">Piotr Mi&#x142;o&#x15b;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhuai Wu</a></p>
<p>Premise selection is a fundamental problem of automated theorem proving.
Previous works often use intricate symbolic methods, rely on domain knowledge,
and require significant engineering effort to solve this task. In this work, we
show that Magnushammer, a neural transformer-based approach, can outperform
traditional symbolic systems by a large margin. Tested on the PISA benchmark,
Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of
Sledgehammer, the most mature and popular symbolic-based solver. Furthermore,
by combining Magnushammer with a neural formal prover based on a language
model, we significantly improve the previous state-of-the-art proof rate from
$57.0\%$ to $71.0\%$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12861">Parallel Diffusion Model-based Sparse-view Cone-beam Breast CT. (arXiv:2303.12861v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1">Wenjun Xia</a>, <a href="http://arxiv.org/find/eess/1/au:+Tseng_H/0/1/0/all/0/1">Hsin Wu Tseng</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1">Chuang Niu</a>, <a href="http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1">Wenxiang Cong</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohua Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shaohua Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ning_R/0/1/0/all/0/1">Ruola Ning</a>, <a href="http://arxiv.org/find/eess/1/au:+Vedantham_S/0/1/0/all/0/1">Srinivasan Vedantham</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Ge Wang</a></p>
<p>Breast cancer is the most prevalent cancer among women worldwide, and early
detection is crucial for reducing its mortality rate and improving quality of
life. Dedicated breast computed tomography (CT) scanners offer better image
quality than mammography and tomosynthesis in general but at higher radiation
dose. To enable breast CT for cancer screening, the challenge is to minimize
the radiation dose without compromising image quality, according to the ALARA
principle (as low as reasonably achievable). Over the past years, deep learning
has shown remarkable successes in various tasks, including low-dose CT
especially few-view CT. Currently, the diffusion model presents the state of
the art for CT reconstruction. To develop the first diffusion model-based
breast CT reconstruction method, here we report innovations to address the
large memory requirement for breast cone-beam CT reconstruction and high
computational cost of the diffusion model. Specifically, in this study we
transform the cutting-edge Denoising Diffusion Probabilistic Model (DDPM) into
a parallel framework for sub-volume-based sparse-view breast CT image
reconstruction in projection and image domains. This novel approach involves
the concurrent training of two distinct DDPM models dedicated to processing
projection and image data synergistically in the dual domains. Our experimental
findings reveal that this method delivers competitive reconstruction
performance at half to one-third of the standard radiation doses. This
advancement demonstrates an exciting potential of diffusion-type models for
volumetric breast reconstruction at high-resolution with much-reduced radiation
dose and as such hopefully redefines breast cancer screening and diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04934">Model Sparsity Can Simplify Machine Unlearning. (arXiv:2304.04934v13 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinghan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiancheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1">Parikshit Ram</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuguang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gaowen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pranay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a></p>
<p>In response to recent data regulation requirements, machine unlearning (MU)
has emerged as a critical process to remove the influence of specific examples
from a given model. Although exact unlearning can be achieved through complete
model retraining using the remaining dataset, the associated computational
costs have driven the development of efficient, approximate unlearning
techniques. Moving beyond data-centric MU approaches, our study introduces a
novel model-based perspective: model sparsification via weight pruning, which
is capable of reducing the gap between exact unlearning and approximate
unlearning. We show in both theory and practice that model sparsity can boost
the multi-criteria unlearning performance of an approximate unlearner, closing
the approximation gap, while continuing to be efficient. This leads to a new MU
paradigm, termed prune first, then unlearn, which infuses a sparse model prior
into the unlearning process. Building on this insight, we also develop a
sparsity-aware unlearning method that utilizes sparsity regularization to
enhance the training process of approximate unlearning. Extensive experiments
show that our proposals consistently benefit MU in various unlearning
scenarios. A notable highlight is the 77% unlearning efficacy gain of
fine-tuning (one of the simplest unlearning methods) when using sparsity-aware
unlearning. Furthermore, we demonstrate the practical impact of our proposed MU
methods in addressing other machine learning challenges, such as defending
against backdoor attacks and enhancing transfer learning. Codes are available
at https://github.com/OPTML-Group/Unlearn-Sparse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08845">Feasible Policy Iteration. (arXiv:2304.08845v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhilong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengbo Eben Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jingliang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingjing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianyuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a></p>
<p>Safe reinforcement learning (RL) aims to find the optimal policy and its
feasible region in a constrained optimal control problem (OCP). Ensuring
feasibility and optimality simultaneously has been a major challenge. Existing
methods either attempt to solve OCPs directly with constrained optimization
algorithms, leading to unstable training processes and unsatisfactory
feasibility, or restrict policies in overly small feasible regions, resulting
in excessive conservativeness with sacrificed optimality. To address this
challenge, we propose an indirect safe RL framework called feasible policy
iteration, which guarantees that the feasible region monotonically expands and
converges to the maximum one, and the state-value function monotonically
improves and converges to the optimal one. We achieve this by designing a
policy update principle called region-wise policy improvement, which maximizes
the state-value function under the constraint of the constraint decay function
(CDF) inside the feasible region and minimizes the CDF outside the feasible
region simultaneously. This update scheme ensures that the state-value function
monotonically increases state-wise in the feasible region and the CDF
monotonically decreases state-wise in the entire state space. We prove that the
CDF converges to the solution of the risky Bellman equation while the
state-value function converges to the solution of the feasible Bellman
equation. The former represents the maximum feasible region and the latter
manifests the optimal state-value function. Experiments show that our algorithm
learns strictly safe and near-optimal policies with accurate feasible regions
on classic control tasks. It also achieves fewer constraint violations with
performance better than (or comparable to) baselines on Safety Gym.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01611">AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yicheng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kavakli_K/0/1/0/all/0/1">Koray Kavakl&#x131;</a>, <a href="http://arxiv.org/find/cs/1/au:+Urey_H/0/1/0/all/0/1">Hakan Urey</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksit_K/0/1/0/all/0/1">Kaan Ak&#x15f;it</a></p>
<p>Multi-color holograms rely on simultaneous illumination from multiple light
sources. These multi-color holograms could utilize light sources better than
conventional single-color holograms and can improve the dynamic range of
holographic displays. In this letter, we introduce AutoColor , the first
learned method for estimating the optimal light source powers required for
illuminating multi-color holograms. For this purpose, we establish the first
multi-color hologram dataset using synthetic images and their depth
information. We generate these synthetic images using a trending pipeline
combining generative, large language, and monocular depth estimation models.
Finally, we train our learned model using our dataset and experimentally
demonstrate that AutoColor significantly decreases the number of steps required
to optimize multi-color holograms from &gt; 1000 to 70 iteration steps without
compromising image quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03686">Provable Preimage Under-Approximation for Neural Networks (Full Version). (arXiv:2305.03686v4 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1">Marta Kwiatkowska</a></p>
<p>Neural network verification mainly focuses on local robustness properties,
which can be checked by bounding the image (set of outputs) of a given input
set. However, often it is important to know whether a given property holds
globally for the input domain, and if not then for what proportion of the input
the property is true. To analyze such properties requires computing preimage
abstractions of neural networks. In this work, we propose an efficient anytime
algorithm for generating symbolic under-approximations of the preimage of any
polyhedron output set for neural networks. Our algorithm combines a novel
technique for cheaply computing polytope preimage under-approximations using
linear relaxation, with a carefully-designed refinement procedure that
iteratively partitions the input region into subregions using input and ReLU
splitting in order to improve the approximation. Empirically, we validate the
efficacy of our method across a range of domains, including a high-dimensional
MNIST classification task beyond the reach of existing preimage computation
methods. Finally, as use cases, we showcase the application to quantitative
verification and robustness analysis. We present a sound and complete algorithm
for the former, which exploits our disjoint union of polytopes representation
to provide formal guarantees. For the latter, we find that our method can
provide useful quantitative information even when standard verifiers cannot
verify a robustness property.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05097">Self-Repellent Random Walks on General Graphs -- Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v3 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Doshi_V/0/1/0/all/0/1">Vishwaraj Doshi</a>, <a href="http://arxiv.org/find/math/1/au:+Hu_J/0/1/0/all/0/1">Jie Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Eun_D/0/1/0/all/0/1">Do Young Eun</a></p>
<p>We consider random walks on discrete state spaces, such as general undirected
graphs, where the random walkers are designed to approximate a target quantity
over the network topology via sampling and neighborhood exploration in the form
of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain
corresponding to a target probability distribution, we design a self-repellent
random walk (SRRW) which is less likely to transition to nodes that were highly
visited in the past, and more likely to transition to seldom visited nodes. For
a class of SRRWs parameterized by a positive real {\alpha}, we prove that the
empirical distribution of the process converges almost surely to the the target
(stationary) distribution of the underlying Markov chain kernel. We then
provide a central limit theorem and derive the exact form of the arising
asymptotic co-variance matrix, which allows us to show that the SRRW with a
stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
covariance, in the sense of Loewner ordering of co-variance matrices.
Especially for SRRW-driven MCMC algorithms, we show that the decrease in the
asymptotic sampling variance is of the order O(1/{\alpha}), eventually going
down to zero. Finally, we provide numerical simulations complimentary to our
theoretical results, also empirically testing a version of SRRW with {\alpha}
increasing in time to combine the benefits of smaller asymptotic variance due
to large {\alpha}, with empirically observed faster mixing properties of SRRW
with smaller {\alpha}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05499">Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model. (arXiv:2305.05499v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Riya_F/0/1/0/all/0/1">Farhin Farhad Riya</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoque_S/0/1/0/all/0/1">Shahinul Hoque</a>, <a href="http://arxiv.org/find/cs/1/au:+Onim_M/0/1/0/all/0/1">Md Saif Hassan Onim</a>, <a href="http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1">Edward Michaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1">Edmon Begoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jinyuan Stella Sun</a></p>
<p>The widespread adoption of Image Processing has propelled Object Recognition
(OR) models into essential roles across various applications, demonstrating the
power of AI and enabling crucial services. Among the applications, traffic sign
recognition stands out as a popular research topic, given its critical
significance in the development of autonomous vehicles. Despite their
significance, real-world challenges, such as alterations to traffic signs, can
negatively impact the performance of OR models. This study investigates the
influence of altered traffic signs on the accuracy and effectiveness of object
recognition, employing a publicly available dataset to introduce alterations in
shape, color, content, visibility, angles and background. Focusing on the
YOLOv7 (You Only Look Once) model, the study demonstrates a notable decline in
detection and classification accuracy when confronted with traffic signs in
unusual conditions including the altered traffic signs. Notably, the
alterations explored in this study are benign examples and do not involve
algorithms used for generating adversarial machine learning samples. This study
highlights the significance of enhancing the robustness of object detection
models in real-life scenarios and the need for further investigation in this
area to improve their accuracy and reliability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12201">GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training. (arXiv:2305.12201v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Sahil Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1">Martin Swany</a></p>
<p>Distributed data-parallel (DDP) training improves overall application
throughput as multiple devices train on a subset of data and aggregate updates
to produce a globally shared model. The periodic synchronization at each
iteration incurs considerable overhead, exacerbated by the increasing size and
complexity of state-of-the-art neural networks. Although many gradient
compression techniques propose to reduce communication cost, the ideal
compression factor that leads to maximum speedup or minimum data exchange
remains an open-ended problem since it varies with the quality of compression,
model size and structure, hardware, network topology and bandwidth. We propose
GraVAC, a framework to dynamically adjust compression factor throughout
training by evaluating model progress and assessing gradient information loss
associated with compression. GraVAC works in an online, black-box manner
without any prior assumptions about a model or its hyperparameters, while
achieving the same or better accuracy than dense SGD (i.e., no compression) in
the same number of iterations/epochs. As opposed to using a static compression
factor, GraVAC reduces end-to-end training time for ResNet101, VGG16 and LSTM
by 4.32x, 1.95x and 6.67x respectively. Compared to other adaptive schemes, our
framework provides 1.94x to 5.63x overall speedup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12971">Neural Cellular Automata Can Respond to Signals. (arXiv:2305.12971v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stovold_J/0/1/0/all/0/1">James Stovold</a></p>
<p>Neural Cellular Automata (NCAs) are a model of morphogenesis, capable of
growing two-dimensional artificial organisms from a single seed cell. In this
paper, we show that NCAs can be trained to respond to signals. Two types of
signal are used: internal (genomically-coded) signals, and external
(environmental) signals. Signals are presented to a single pixel for a single
timestep.
</p>
<p>Results show NCAs are able to grow into multiple distinct forms based on
internal signals, and are able to change colour based on external signals.
Overall these contribute to the development of NCAs as a model of artificial
morphogenesis, and pave the way for future developments embedding dynamic
behaviour into the NCA model.
</p>
<p>Code and target images are available through GitHub:
https://github.com/jstovold/ALIFE2023
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16304">Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zheyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weixuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1">Damien Teney</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a></p>
<p>Composed image retrieval aims to find an image that best matches a given
multi-modal user query consisting of a reference image and text pair. Existing
methods commonly pre-compute image embeddings over the entire corpus and
compare these to a reference image embedding modified by the query text at test
time. Such a pipeline is very efficient at test time since fast vector
distances can be used to evaluate candidates, but modifying the reference image
embedding guided only by a short textual description can be difficult,
especially independent of potential candidates. An alternative approach is to
allow interactions between the query and every possible candidate, i.e.,
reference-text-candidate triplets, and pick the best from the entire set.
Though this approach is more discriminative, for large-scale datasets the
computational cost is prohibitive since pre-computation of candidate embeddings
is no longer possible. We propose to combine the merits of both schemes using a
two-stage model. Our first stage adopts the conventional vector distancing
metric and performs a fast pruning among candidates. Meanwhile, our second
stage employs a dual-encoder architecture, which effectively attends to the
input triplet of reference-text-candidate and re-ranks the candidates. Both
stages utilize a vision-and-language pre-trained network, which has proven
beneficial for various downstream tasks. Our method consistently outperforms
state-of-the-art approaches on standard benchmarks for the task. Our
implementation is available at
https://github.com/Cuberick-Orion/Candidate-Reranking-CIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16573">Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasegawa_N/0/1/0/all/0/1">Naoya Hasegawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a></p>
<p>Recognition problems in long-tailed data, in which the sample size per class
is heavily skewed, have gained importance because the distribution of the
sample size per class in a dataset is generally exponential unless the sample
size is intentionally adjusted. Various methods have been devised to address
these problems. Recently, weight balancing, which combines well-known classical
regularization techniques with two-stage training, has been proposed. Despite
its simplicity, it is known for its high performance compared with existing
methods devised in various ways. However, there is a lack of understanding as
to why this method is effective for long-tailed data. In this study, we analyze
weight balancing by focusing on neural collapse and the cone effect at each
training stage and found that it can be decomposed into an increase in Fisher's
discriminant ratio of the feature extractor caused by weight decay and cross
entropy loss and implicit logit adjustment caused by weight decay and
class-balanced loss. Our analysis enables the training method to be further
simplified by reducing the number of training stages to one while increasing
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19659">Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Anant Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Shrutimoy Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Shubhajit Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Maity_B/0/1/0/all/0/1">Binita Maity</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Anirban Dasgupta</a></p>
<p>In this paper, we propose localized versions of Weisfeiler-Leman (WL)
algorithms in an effort to both increase the expressivity, as well as decrease
the computational overhead. We focus on the specific problem of subgraph
counting and give localized versions of $k-$WL for any $k$. We analyze the
power of Local $k-$WL and prove that it is more expressive than $k-$WL and at
most as expressive as $(k+1)-$WL. We give a characterization of patterns whose
count as a subgraph and induced subgraph are invariant if two graphs are Local
$k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and
recursive $k-$WL. These methods are more time and space efficient than applying
$k-$WL on the whole graph. We also propose a fragmentation technique that
guarantees the exact count of all induced subgraphs of size at most 4 using
just $1-$WL. The same idea can be extended further for larger patterns using
$k&gt;1$. We also compare the expressive power of Local $k-$WL with other GNN
hierarchies and show that given a bound on the time-complexity, our methods are
more expressive than the ones mentioned in Papp and Wattenhofer[2022a].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01875">DiffECG: A Versatile Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neifar_N/0/1/0/all/0/1">Nour Neifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Hamadou_A/0/1/0/all/0/1">Achraf Ben-Hamadou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mdhaffar_A/0/1/0/all/0/1">Afef Mdhaffar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jmaiel_M/0/1/0/all/0/1">Mohamed Jmaiel</a></p>
<p>Within cardiovascular disease detection using deep learning applied to ECG
signals, the complexities of handling physiological signals have sparked
growing interest in leveraging deep generative models for effective data
augmentation. In this paper, we introduce a novel versatile approach based on
denoising diffusion probabilistic models for ECG synthesis, addressing three
scenarios: (i) heartbeat generation, (ii) partial signal imputation, and (iii)
full heartbeat forecasting. Our approach presents the first generalized
conditional approach for ECG synthesis, and our experimental results
demonstrate its effectiveness for various ECG-related tasks. Moreover, we show
that our approach outperforms other state-of-the-art ECG generative models and
can enhance the performance of state-of-the-art classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09189">High-Resolution Convolutional Neural Networks on Homomorphically Encrypted Data via Sharding Ciphertexts. (arXiv:2306.09189v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maloney_V/0/1/0/all/0/1">Vivian Maloney</a>, <a href="http://arxiv.org/find/cs/1/au:+Obrecht_R/0/1/0/all/0/1">Richard F. Obrecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Saraph_V/0/1/0/all/0/1">Vikram Saraph</a>, <a href="http://arxiv.org/find/cs/1/au:+Rama_P/0/1/0/all/0/1">Prathibha Rama</a>, <a href="http://arxiv.org/find/cs/1/au:+Tallaksen_K/0/1/0/all/0/1">Kate Tallaksen</a></p>
<p>Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20
architecture have been privately evaluated on encrypted, low-resolution data
with the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphic
encryption scheme. We extend methods for evaluating DCNNs on images with larger
dimensions and many channels, beyond what can be stored in single ciphertexts.
Additionally, we simplify and improve the efficiency of the recently introduced
multiplexed image format, demonstrating that homomorphic evaluation can work
with standard, row-major matrix packing and results in encrypted inference time
speedups by $4.6-6.5\times$. We also show how existing DCNN models can be
regularized during the training process to further improve efficiency and
accuracy. These techniques are applied to homomorphically evaluate a DCNN with
high accuracy on the high-resolution ImageNet dataset, achieving $80.2\%$ top-1
accuracy. We also achieve an accuracy of homomorphically evaluated CNNs on the
CIFAR-10 dataset of $98.3\%$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10882">AdaStop: adaptive statistical testing for sound comparisons of Deep RL agents. (arXiv:2306.10882v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mathieu_T/0/1/0/all/0/1">Timoth&#xe9;e Mathieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1">Riccardo Della Vecchia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shilova_A/0/1/0/all/0/1">Alena Shilova</a>, <a href="http://arxiv.org/find/cs/1/au:+Centa_M/0/1/0/all/0/1">Matheus Medeiros Centa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_H/0/1/0/all/0/1">Hector Kohler</a>, <a href="http://arxiv.org/find/cs/1/au:+Maillard_O/0/1/0/all/0/1">Odalric-Ambrym Maillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1">Philippe Preux</a></p>
<p>Recently, the scientific community has questioned the statistical
reproducibility of many empirical results, especially in the field of machine
learning. To solve this reproducibility crisis, we propose a theoretically
sound methodology to compare the overall performance of multiple algorithms
with stochastic returns. We exemplify our methodology in Deep RL. Indeed, the
performance of one execution of a Deep RL algorithm is random. Therefore,
several independent executions are needed to accurately evaluate the overall
performance. When comparing several RL algorithms, a major question is how many
executions must be made and how can we ensure that the results of such a
comparison are theoretically sound. When comparing several algorithms at once,
the error of each comparison may accumulate and must be taken into account with
a multiple tests procedure to preserve low error guarantees. We introduce
AdaStop, a new statistical test based on multiple group sequential tests. When
comparing algorithms, AdaStop adapts the number of executions to stop as early
as possible while ensuring that we have enough information to distinguish
algorithms that perform better than the others in a statistical significant
way. We prove theoretically and empirically that AdaStop has a low probability
of making a (family-wise) error. Finally, we illustrate the effectiveness of
AdaStop in multiple Deep RL use-cases, including toy examples and challenging
Mujoco environments. AdaStop is the first statistical test fitted to this sort
of comparisons: AdaStop is both a significant contribution to statistics, and a
major contribution to computational studies performed in reinforcement learning
and in other domains. To summarize our contribution, we introduce AdaStop, a
formally grounded statistical tool to let anyone answer the practical question:
``Is my algorithm the new state-of-the-art?''.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11886">SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jesse Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1">Karl Pertsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiahui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1">Joseph J. Lim</a></p>
<p>Pre-training robot policies with a rich set of skills can substantially
accelerate the learning of downstream tasks. Prior works have defined
pre-training tasks via natural language instructions, but doing so requires
tedious human annotation of hundreds of thousands of instructions. Thus, we
propose SPRINT, a scalable offline policy pre-training approach which
substantially reduces the human effort needed for pre-training a diverse set of
skills. Our method uses two core ideas to automatically expand a base set of
pre-training tasks: instruction relabeling via large language models and
cross-trajectory skill chaining through offline reinforcement learning. As a
result, SPRINT pre-training equips robots with a much richer repertoire of
skills. Experimental results in a household simulator and on a real robot
kitchen manipulation task show that SPRINT leads to substantially faster
learning of new long-horizon tasks than previous pre-training approaches.
Website at https://clvrai.com/sprint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15749">To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v5 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ottati_F/0/1/0/all/0/1">Fabrizio Ottati</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qinyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brignone_G/0/1/0/all/0/1">Giovanni Brignone</a>, <a href="http://arxiv.org/find/cs/1/au:+Casu_M/0/1/0/all/0/1">Mario R. Casu</a>, <a href="http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1">Jason K. Eshraghian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavagno_L/0/1/0/all/0/1">Luciano Lavagno</a></p>
<p>As deep learning models scale, they become increasingly competitive from
domains spanning from computer vision to natural language processing; however,
this happens at the expense of efficiency since they require increasingly more
memory and computing power. The power efficiency of the biological brain
outperforms any large-scale deep learning ( DL ) model; thus, neuromorphic
computing tries to mimic the brain operations, such as spike-based information
processing, to improve the efficiency of DL models. Despite the benefits of the
brain, such as efficient information transmission, dense neuronal
interconnects, and the co-location of computation and memory, the available
biological substrate has severely constrained the evolution of biological
brains. Electronic hardware does not have the same constraints; therefore,
while modeling spiking neural networks ( SNNs) might uncover one piece of the
puzzle, the design of efficient hardware backends for SNN s needs further
investigation, potentially taking inspiration from the available work done on
the artificial neural networks ( ANNs) side. As such, when is it wise to look
at the brain while designing new hardware, and when should it be ignored? To
answer this question, we quantitatively compare the digital hardware
acceleration techniques and platforms of ANNs and SNN s. As a result, we
provide the following insights: (i) ANNs currently process static data more
efficiently, (ii) applications targeting data produced by neuromorphic sensors,
such as event-based cameras and silicon cochleas, need more investigation since
the behavior of these sensors might naturally fit the SNN paradigm, and (iii)
hybrid approaches combining SNN s and ANNs might lead to the best solutions and
should be investigated further at the hardware level, accounting for both
efficiency and loss optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00012">FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair. (arXiv:2307.00012v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fatima_S/0/1/0/all/0/1">Sakina Fatima</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemmati_H/0/1/0/all/0/1">Hadi Hemmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1">Lionel Briand</a></p>
<p>Flaky tests are problematic because they non-deterministically pass or fail
for the same software version under test, causing confusion and wasting
development effort. While machine learning models have been used to predict
flakiness and its root causes, there is much less work on providing support to
fix the problem. To address this gap, in this paper, we focus on predicting the
type of fix that is required to remove flakiness and then repair the test code
on that basis. We do this for a subset of flaky test cases where the root cause
of flakiness is in the test case itself and not in the production code. Our key
idea is to guide the repair process with additional knowledge about the test's
flakiness in the form of its predicted fix category. Thus, we first propose a
framework that automatically generates labeled datasets for 13 fix categories
and trains models to predict the fix category of a flaky test by analyzing the
test code only. Our experimental results using code models and few-shot
learning show that we can correctly predict most of the fix categories. To show
the usefulness of such fix category labels for automatically repairing
flakiness, in addition to informing testers, we augment a Large Language Model
(LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions.
The results show that our suggested fix category labels significantly enhance
the capability of GPT 3.5 Turbo, in generating fixes for flaky tests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00238">Unified Transfer Learning Models in High-Dimensional Linear Regression. (arXiv:2307.00238v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1">Shuo Shuo Liu</a></p>
<p>Transfer learning plays a key role in modern data analysis when: (1) the
target data are scarce but the source data are sufficient; (2) the
distributions of the source and target data are heterogeneous. This paper
develops an interpretable unified transfer learning model, termed as UTrans,
which can detect both transferable variables and source data. More
specifically, we establish the estimation error bounds and prove that our
bounds are lower than those with target data only. Besides, we propose a source
detection algorithm based on hypothesis testing to exclude the nontransferable
data. We evaluate and compare UTrans to the existing algorithms in multiple
experiments. It is shown that UTrans attains much lower estimation and
prediction errors than the existing methods, while preserving interpretability.
We finally apply it to the US intergenerational mobility data and compare our
proposed algorithms to the classical machine learning algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07950">Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Sahil Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1">Martin Swany</a></p>
<p>In distributed training, deep neural networks (DNNs) are launched over
multiple workers concurrently and aggregate their local updates on each step in
bulk-synchronous parallel (BSP) training. However, BSP does not linearly
scale-out due to high communication cost of aggregation. To mitigate this
overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous
Parallel (SSP) either reduce synchronization frequency or eliminate it
altogether, usually at the cost of lower final accuracy. In this paper, we
present \texttt{SelSync}, a practical, low-overhead method for DNN training
that dynamically chooses to incur or avoid communication at each step either by
calling the aggregation op or applying local updates based on their
significance. We propose various optimizations as part of \texttt{SelSync} to
improve convergence in the context of \textit{semi-synchronous} training. Our
system converges to the same or better accuracy than BSP while reducing
training time by up to 14$\times$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08962">REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1">Rithesh Murthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1">Shelby Heinecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1">Juan Carlos Niebles</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Le Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Weiran Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yihao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1">Akash Gokul</a>, <a href="http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1">Devansh Arpit</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mui_P/0/1/0/all/0/1">Phil Mui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a></p>
<p>In this paper, we propose an enhanced approach for Rapid Exploration and
eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have
inherent limitations, such as a heavy reliance on precise descriptions for
decision-making, and the lack of a systematic approach to leverage try-and-fail
procedures akin to traditional Reinforcement Learning (RL). REX introduces an
additional layer of rewards and integrates concepts similar to Upper Confidence
Bound (UCB) scores, leading to more robust and efficient AI agent performance.
This approach has the advantage of enabling the utilization of offline
behaviors from logs and allowing seamless integration with existing foundation
models while it does not require any model fine-tuning. Through comparative
analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA
Planning(RAP), REX-based methods demonstrate comparable performance and, in
certain cases, even surpass the results achieved by these existing techniques.
Notably, REX-based methods exhibit remarkable reductions in execution time,
enhancing their practical applicability across a diverse set of scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14023">Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kajitsuka_T/0/1/0/all/0/1">Tokio Kajitsuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a></p>
<p>Existing analyses of the expressive capacity of Transformer models have
required excessively deep layers for data memorization, leading to a
discrepancy with the Transformers actually used in practice. This is primarily
due to the interpretation of the softmax function as an approximation of the
hardmax function. By clarifying the connection between the softmax function and
the Boltzmann operator, we prove that a single layer of self-attention with
low-rank weight matrices possesses the capability to perfectly capture the
context of an entire input sequence. As a consequence, we show that one-layer
and single-head Transformers have a memorization capacity for finite samples,
and that Transformers consisting of one self-attention layer with two
feed-forward neural networks are universal approximators for continuous
permutation equivariant functions on a compact domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16348">Rating-based Reinforcement Learning. (arXiv:2307.16348v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+White_D/0/1/0/all/0/1">Devin White</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mingkang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novoseller_E/0/1/0/all/0/1">Ellen Novoseller</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1">Vernon J. Lawhern</a>, <a href="http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1">Nicholas Waytowich</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yongcan Cao</a></p>
<p>This paper develops a novel rating-based reinforcement learning approach that
uses human ratings to obtain human guidance in reinforcement learning.
Different from the existing preference-based and ranking-based reinforcement
learning paradigms, based on human relative preferences over sample pairs, the
proposed rating-based reinforcement learning approach is based on human
evaluation of individual trajectories without relative comparisons between
sample pairs. The rating-based reinforcement learning approach builds on a new
prediction model for human ratings and a novel multi-class loss function. We
conduct several experimental studies based on synthetic ratings and real human
ratings to evaluate the effectiveness and benefits of the new rating-based
reinforcement learning approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07491">Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1">Taesoo Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1">Taehong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1">Jaewon Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yoonsang Lee</a></p>
<p>Since the introduction of DeepMimic [Peng et al. 2018], subsequent research
has focused on expanding the repertoire of simulated motions across various
scenarios. In this study, we propose an alternative approach for this goal, a
deep reinforcement learning method based on the simulation of a
single-rigid-body character. Using the centroidal dynamics model (CDM) to
express the full-body character as a single rigid body (SRB) and training a
policy to track a reference motion, we can obtain a policy that is capable of
adapting to various unobserved environmental changes and controller transitions
without requiring any additional learning. Due to the reduced dimension of
state and action space, the learning process is sample-efficient. The final
full-body motion is kinematically generated in a physically plausible way,
based on the state of the simulated SRB character. The SRB simulation is
formulated as a quadratic programming (QP) problem, and the policy outputs an
action that allows the SRB character to follow the reference motion. We
demonstrate that our policy, efficiently trained within 30 minutes on an
ultraportable laptop, has the ability to cope with environments that have not
been experienced during learning, such as running on uneven terrain or pushing
a box, and transitions between learned policies, without any additional
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08334">Learning logic programs by discovering higher-order abstractions. (arXiv:2308.08334v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hocquette_C/0/1/0/all/0/1">C&#xe9;line Hocquette</a>, <a href="http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1">Sebastijan Duman&#x10d;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Cropper_A/0/1/0/all/0/1">Andrew Cropper</a></p>
<p>We introduce the higher-order refactoring problem, where the goal is to
compress a logic program by discovering higher-order abstractions, such as map,
filter, and fold. We implement our approach in Stevie, which formulates the
refactoring problem as a constraint optimisation problem. Our experiments on
multiple domains, including program synthesis and visual reasoning, show that
refactoring can improve the learning performance of an inductive logic
programming system, specifically improving predictive accuracies by 27% and
reducing learning times by 47%. We also show that Stevie can discover
abstractions that transfer to multiple domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11787">HypBO: Accelerating Black-Box Scientific Experiments Using Experts&#x27; Hypotheses. (arXiv:2308.11787v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cisse_A/0/1/0/all/0/1">Abdoulatif Cisse</a>, <a href="http://arxiv.org/find/cs/1/au:+Evangelopoulos_X/0/1/0/all/0/1">Xenophon Evangelopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Carruthers_S/0/1/0/all/0/1">Sam Carruthers</a>, <a href="http://arxiv.org/find/cs/1/au:+Gusev_V/0/1/0/all/0/1">Vladimir V. Gusev</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1">Andrew I. Cooper</a></p>
<p>Robotics and automation offer massive accelerations for solving intractable,
multivariate scientific problems such as materials discovery, but the available
search spaces can be dauntingly large. Bayesian optimization (BO) has emerged
as a popular sample-efficient optimization engine, thriving in tasks where no
analytic form of the target function/property is known. Here, we exploit expert
human knowledge in the form of hypotheses to direct Bayesian searches more
quickly to promising regions of chemical space. Previous methods have used
underlying distributions derived from existing experimental measurements, which
is unfeasible for new, unexplored scientific tasks. Also, such distributions
cannot capture intricate hypotheses. Our proposed method, which we call HypBO,
uses expert human hypotheses to generate improved seed samples. Unpromising
seeds are automatically discounted, while promising seeds are used to augment
the surrogate model data, thus achieving better-informed sampling. This process
continues in a global versus local search fashion, organized in a bilevel
optimization framework. We validate the performance of our method on a range of
synthetic functions and demonstrate its practical utility on a real chemical
design task where the use of expert hypotheses accelerates the search
performance significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13111">Bayesian Low-rank Adaptation for Large Language Models. (arXiv:2308.13111v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1">Adam X. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Robeyns_M/0/1/0/all/0/1">Maxime Robeyns</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1">Laurence Aitchison</a></p>
<p>Low-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient
fine-tuning of large language models (LLMs). However, fine-tuned LLMs often
become overconfident especially when fine-tuned on small datasets. Bayesian
methods, with their inherent ability to estimate uncertainty, serve as potent
tools to mitigate overconfidence and enhance calibration. In this work, we
introduce Laplace-LoRA, which applies a Bayesian approach to the LoRA
parameters. Specifically, Laplace-LoRA applies a Laplace approximation to the
posterior over the LoRA parameters, considerably improving the calibration of
fine-tuned LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13420">Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yanjie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yutong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yangyang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Ran Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1">P. N. Suganthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Swagatam Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallipeddi_R/0/1/0/all/0/1">Rammohan Mallipeddi</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_O/0/1/0/all/0/1">Oladayo Solomon Ajani. Qiang Feng</a></p>
<p>Evolutionary algorithms (EA), a class of stochastic search methods based on
the principles of natural evolution, have received widespread acclaim for their
exceptional performance in various real-world optimization problems. While
researchers worldwide have proposed a wide variety of EAs, certain limitations
remain, such as slow convergence speed and poor generalization capabilities.
Consequently, numerous scholars actively explore improvements to algorithmic
structures, operators, search patterns, etc., to enhance their optimization
performance. Reinforcement learning (RL) integrated as a component in the EA
framework has demonstrated superior performance in recent years. This paper
presents a comprehensive survey on integrating reinforcement learning into the
evolutionary algorithm, referred to as reinforcement learning-assisted
evolutionary algorithm (RL-EA). We begin with the conceptual outlines of
reinforcement learning and the evolutionary algorithm. We then provide a
taxonomy of RL-EA. Subsequently, we discuss the RL-EA integration method, the
RL-assisted strategy adopted by RL-EA, and its applications according to the
existing literature. The RL-assisted procedure is divided according to the
implemented functions including solution generation, learnable objective
function, algorithm/operator/sub-population selection, parameter adaptation,
and other strategies. Additionally, different attribute settings of RL in RL-EA
are discussed. In the applications of RL-EA section, we also demonstrate the
excellent performance of RL-EA on several benchmarks and a range of public
datasets to facilitate a quick comparative study. Finally, we analyze potential
directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06054">Breaking through the learning plateaus of in-context learning in Transformer. (arXiv:2309.06054v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jingwen Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a></p>
<p>In-context learning, i.e., learning from context examples, is an impressive
ability of Transformer. Training Transformers to possess this in-context
learning skill is computationally intensive due to the occurrence of learning
plateaus, which are periods within the training process where there is minimal
or no enhancement in the model's in-context learning capability. To study the
mechanism behind the learning plateaus, we conceptually seperate a component
within the model's internal representation that is exclusively affected by the
model's weights. We call this the "weights component", and the remainder is
identified as the "context component". By conducting meticulous and controlled
experiments on synthetic tasks, we note that the persistence of learning
plateaus correlates with compromised functionality of the weights component.
Recognizing the impaired performance of the weights component as a fundamental
behavior drives learning plateaus, we have developed three strategies to
expedite the learning of Transformers. The effectiveness of these strategies is
further confirmed in natural language processing tasks. In conclusion, our
research demonstrates the feasibility of cultivating a powerful in-context
learning ability within AI systems in an eco-friendly manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06782">Improved particle-flow event reconstruction with scalable neural networks for current and future particle detectors. (arXiv:2309.06782v4 [physics.data-an] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Pata_J/0/1/0/all/0/1">Joosep Pata</a>, <a href="http://arxiv.org/find/physics/1/au:+Wulff_E/0/1/0/all/0/1">Eric Wulff</a>, <a href="http://arxiv.org/find/physics/1/au:+Mokhtar_F/0/1/0/all/0/1">Farouk Mokhtar</a>, <a href="http://arxiv.org/find/physics/1/au:+Southwick_D/0/1/0/all/0/1">David Southwick</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_M/0/1/0/all/0/1">Mengke Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Girone_M/0/1/0/all/0/1">Maria Girone</a>, <a href="http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1">Javier Duarte</a></p>
<p>Experiments at the High-Luminosity LHC and the Future Circular Collider need
efficient algorithms to reconstruct granular events expected at such detectors
with high fidelity. We study scalable machine learning models for event
reconstruction in electron-positron collisions based on a full detector
simulation. Particle-flow reconstruction can be formulated as a supervised
learning task using tracks and calorimeter clusters. We compare a graph neural
network and kernel-based transformer and demonstrate that we can avoid
quadratic operations while achieving realistic reconstruction. We show that
hyperparameter tuning significantly improves the performance of the models. The
best graph neural network model shows improvement in the jet transverse
momentum resolution by up to 50% compared to the rule-based algorithm. Accurate
reconstruction can significantly improve future measurements at colliders. The
resulting model is portable across Nvidia, AMD and Habana hardware. Our
datasets and software are published following the findable, accessible,
interoperable, and reusable principles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11765">Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xinyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1">Richard Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1">Huseyin A. Inan</a>, <a href="http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1">Andre Manoel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1">Fatemehsadat Mireshghallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zinan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1">Sivakanth Gopi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1">Janardhan Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1">Robert Sim</a></p>
<p>We study the problem of in-context learning (ICL) with large language models
(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak
or regurgitate the private examples demonstrated in the prompt. We propose a
novel algorithm that generates synthetic few-shot demonstrations from the
private dataset with formal differential privacy (DP) guarantees, and show
empirically that it can achieve effective ICL. We conduct extensive experiments
on standard benchmarks and compare our algorithm with non-private ICL and
zero-shot solutions. Our results demonstrate that our algorithm can achieve
competitive performance with strong privacy levels. These results open up new
possibilities for ICL with privacy protection for a broad range of
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13340">Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1">Amrita Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1">Raha Moraffah</a>, <a href="http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1">Joshua Garland</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>With the advent of larger and more complex deep learning models, such as in
Natural Language Processing (NLP), model qualities like explainability and
interpretability, albeit highly desirable, are becoming harder challenges to
tackle and solve. For example, state-of-the-art models in text classification
are black-box by design. Although standard explanation methods provide some
degree of explainability, these are mostly correlation-based methods and do not
provide much insight into the model. The alternative of causal explainability
is more desirable to achieve but extremely challenging in NLP due to a variety
of reasons. Inspired by recent endeavors to utilize Large Language Models
(LLMs) as experts, in this work, we aim to leverage the instruction-following
and textual understanding capabilities of recent state-of-the-art LLMs to
facilitate causal explainability via counterfactual explanation generation for
black-box text classifiers. To do this, we propose a three-step pipeline via
which, we use an off-the-shelf LLM to: (1) identify the latent or unobserved
features in the input text, (2) identify the input features associated with the
latent features, and finally (3) use the identified input features to generate
a counterfactual explanation. We experiment with our pipeline on multiple NLP
text classification datasets, with several recent LLMs, and present interesting
and promising findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13500">Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1">Lin Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xianda Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiamou Liu</a></p>
<p>Learnersourcing offers great potential for scalable education through student
content creation. However, predicting student performance on learnersourced
questions, which is essential for personalizing the learning experience, is
challenging due to the inherent noise in student-generated data. Moreover,
while conventional graph-based methods can capture the complex network of
student and question interactions, they often fall short under cold start
conditions where limited student engagement with questions yields sparse data.
To address both challenges, we introduce an innovative strategy that synergizes
the potential of integrating Signed Graph Neural Networks (SGNNs) and Large
Language Model (LLM) embeddings. Our methodology employs a signed bipartite
graph to comprehensively model student answers, complemented by a contrastive
learning framework that enhances noise resilience. Furthermore, LLM's
contribution lies in generating foundational question embeddings, proving
especially advantageous in addressing cold start scenarios characterized by
limited graph data. Validation across five real-world datasets sourced from the
PeerWise platform underscores our approach's effectiveness. Our method
outperforms baselines, showcasing enhanced predictive accuracy and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14053">Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1">Khoi Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoa Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1">Long Tran-Thanh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1">Quoc-Viet Pham</a></p>
<p>This paper explores Large Batch Training techniques using layer-wise adaptive
scaling ratio (LARS) across diverse settings, uncovering insights. LARS
algorithms with warm-up tend to be trapped in sharp minimizers early on due to
redundant ratio scaling. Additionally, a fixed steep decline in the latter
phase restricts deep neural networks from effectively navigating early-phase
sharp minimizers. Building on these findings, we propose Time Varying LARS
(TVLARS), a novel algorithm that replaces warm-up with a configurable
sigmoid-like function for robust training in the initial phase. TVLARS promotes
gradient exploration early on, surpassing sharp optimizers and gradually
transitioning to LARS for robustness in later phases. Extensive experiments
demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases,
with up to 2\% improvement in classification scenarios. Notably, in all
self-supervised learning cases, TVLARS dominates LARS and LAMB with performance
improvements of up to 10\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15560">Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mouxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zemin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jianling Sun</a></p>
<p>Unbiased Learning to Rank (ULTR) aims to train unbiased ranking models from
biased click logs, by explicitly modeling a generation process for user
behavior and fitting click data based on examination hypothesis. Previous
research found empirically that the true latent relevance is mostly recoverable
through perfect click fitting. However, we demonstrate that this is not always
achievable, resulting in a significant reduction in ranking performance. This
research investigates the conditions under which relevance can be recovered
from click data at a foundational level. We initially characterize a ranking
model as identifiable if it can recover the true relevance up to a scaling
transformation, a criterion sufficient for the pairwise ranking objective.
Subsequently, we investigate an equivalent condition for identifiability,
articulated as a graph connectivity test problem: the recovery of relevance is
feasible if and only if the identifiability graph (IG), derived from the
underlying structure of the dataset, is connected. The presence of a
disconnected IG may lead to degenerate cases and suboptimal ranking
performance. To tackle this challenge, we introduce two methods, namely node
intervention and node merging, designed to modify the dataset and restore the
connectivity of the IG. Empirical results derived from a simulated dataset and
two real-world LTR benchmark datasets not only validate our proposed theorems
but also demonstrate the effectiveness of our methods in alleviating data bias
when the relevance model is unidentifiable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16742">Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muharram_A/0/1/0/all/0/1">Arief Purnama Muharram</a>, <a href="http://arxiv.org/find/cs/1/au:+Tahapary_D/0/1/0/all/0/1">Dicky Levenus Tahapary</a>, <a href="http://arxiv.org/find/cs/1/au:+Lestari_Y/0/1/0/all/0/1">Yeni Dwi Lestari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarayar_R/0/1/0/all/0/1">Randy Sarayar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dirjayanto_V/0/1/0/all/0/1">Valerie Josephine Dirjayanto</a></p>
<p>Diabetes, especially T2DM, continues to be a significant health problem. One
of the major concerns associated with diabetes is the development of its
complications. Diabetic nephropathy, one of the chronic complication of
diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing
diabetic nephropathy involves considering various criteria, one of which is the
presence of a pathologically significant quantity of albumin in urine, known as
albuminuria. Thus, early prediction of albuminuria in diabetic patients holds
the potential for timely preventive measures. This study aimed to develop a
supervised learning model to predict the risk of developing albuminuria in T2DM
patients. The selected supervised learning algorithms included Na\"ive Bayes,
Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost,
and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries
of diabetes complications risk factors, was used to train the algorithms. It
consisted of 10 attributes as features and 1 attribute as the target
(albuminuria). Upon conducting the experiments, the MLP demonstrated superior
performance compared to the other algorithms. It achieved accuracy and f1-score
values as high as 0.74 and 0.75, respectively, making it suitable for screening
purposes in predicting albuminuria in T2DM. Nonetheless, further studies are
warranted to enhance the model's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17194">Generalized Activation via Multivariate Projection. (arXiv:2309.17194v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiayun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yuxiao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yiwen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1">Zhuofan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1">Yilin Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a></p>
<p>Activation functions are essential to introduce nonlinearity into neural
networks, with the Rectified Linear Unit (ReLU) often favored for its
simplicity and effectiveness. Motivated by the structural similarity between a
shallow Feedforward Neural Network (FNN) and a single iteration of the
Projected Gradient Descent (PGD) algorithm, a standard approach for solving
constrained optimization problems, we consider ReLU as a projection from R onto
the nonnegative half-line R+. Building on this interpretation, we extend ReLU
by substituting it with a generalized projection operator onto a convex cone,
such as the Second-Order Cone (SOC) projection, thereby naturally extending it
to a Multivariate Projection Unit (MPU), an activation function with multiple
inputs and multiple outputs. We further provide mathematical proof establishing
that FNNs activated by SOC projections outperform those utilizing ReLU in terms
of expressive power. Experimental evaluations on widely-adopted architectures
further corroborate MPU's effectiveness against a broader range of existing
activation functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01728">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lintao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">James Y. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan-Fang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a></p>
<p>Time series forecasting holds significant importance in many real-world
dynamic systems and has been extensively studied. Unlike natural language
process (NLP) and computer vision (CV), where a single large model can tackle
multiple tasks, models for time series forecasting are often specialized,
necessitating distinct designs for different tasks and applications. While
pre-trained foundation models have made impressive strides in NLP and CV, their
development in time series domains has been constrained by data sparsity.
Recent studies have revealed that large language models (LLMs) possess robust
pattern recognition and reasoning abilities over complex sequences of tokens.
However, the challenge remains in effectively aligning the modalities of time
series data and natural language to leverage these capabilities. In this work,
we present Time-LLM, a reprogramming framework to repurpose LLMs for general
time series forecasting with the backbone language models kept intact. We begin
by reprogramming the input time series with text prototypes before feeding it
into the frozen LLM to align the two modalities. To augment the LLM's ability
to reason with time series data, we propose Prompt-as-Prefix (PaP), which
enriches the input context and directs the transformation of reprogrammed input
patches. The transformed time series patches from the LLM are finally projected
to obtain the forecasts. Our comprehensive evaluations demonstrate that
Time-LLM is a powerful time series learner that outperforms state-of-the-art,
specialized forecasting models. Moreover, Time-LLM excels in both few-shot and
zero-shot learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02446">Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1">Zheng-Xin Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Menghini_C/0/1/0/all/0/1">Cristina Menghini</a>, <a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1">Stephen H. Bach</a></p>
<p>AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03480">The ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. (arXiv:2310.03480v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dabike_G/0/1/0/all/0/1">Gerardo Roa Dabike</a>, <a href="http://arxiv.org/find/eess/1/au:+Akeroyd_M/0/1/0/all/0/1">Michael A. Akeroyd</a>, <a href="http://arxiv.org/find/eess/1/au:+Bannister_S/0/1/0/all/0/1">Scott Bannister</a>, <a href="http://arxiv.org/find/eess/1/au:+Barker_J/0/1/0/all/0/1">Jon Barker</a>, <a href="http://arxiv.org/find/eess/1/au:+Cox_T/0/1/0/all/0/1">Trevor J. Cox</a>, <a href="http://arxiv.org/find/eess/1/au:+Fazenda_B/0/1/0/all/0/1">Bruno Fazenda</a>, <a href="http://arxiv.org/find/eess/1/au:+Firth_J/0/1/0/all/0/1">Jennifer Firth</a>, <a href="http://arxiv.org/find/eess/1/au:+Graetzer_S/0/1/0/all/0/1">Simone Graetzer</a>, <a href="http://arxiv.org/find/eess/1/au:+Greasley_A/0/1/0/all/0/1">Alinka Greasley</a>, <a href="http://arxiv.org/find/eess/1/au:+Vos_R/0/1/0/all/0/1">Rebecca R. Vos</a>, <a href="http://arxiv.org/find/eess/1/au:+Whitmer_W/0/1/0/all/0/1">William M. Whitmer</a></p>
<p>This paper reports on the design and results of the 2024 ICASSP SP Cadenza
Challenge: Music Demixing/Remixing for Hearing Aids. The Cadenza project is
working to enhance the audio quality of music for those with a hearing loss.
The scenario for the challenge was listening to stereo reproduction over
loudspeakers via hearing aids. The task was to: decompose pop/rock music into
vocal, drums, bass and other (VDBO); rebalance the different tracks with
specified gains and then remixing back to stereo. End-to-end approaches were
also accepted. 17 systems were submitted by 11 teams. Causal systems performed
poorer than non-causal approaches. 9 systems beat the baseline. A common
approach was to fine-tuning pretrained demixing models. The best approach used
an ensemble of models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04676">Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots. (arXiv:2310.04676v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmidgall_S/0/1/0/all/0/1">Samuel Schmidgall</a>, <a href="http://arxiv.org/find/cs/1/au:+Krieger_A/0/1/0/all/0/1">Axel Krieger</a>, <a href="http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1">Jason Eshraghian</a></p>
<p>Recent advances in robot-assisted surgery have resulted in progressively more
precise, efficient, and minimally invasive procedures, sparking a new era of
robotic surgical intervention. This enables doctors, in collaborative
interaction with robots, to perform traditional or minimally invasive surgeries
with improved outcomes through smaller incisions. Recent efforts are working
toward making robotic surgery more autonomous which has the potential to reduce
variability of surgical outcomes and reduce complication rates. Deep
reinforcement learning methodologies offer scalable solutions for surgical
automation, but their effectiveness relies on extensive data acquisition due to
the absence of prior knowledge in successfully accomplishing tasks. Due to the
intensive nature of simulated data collection, previous works have focused on
making existing algorithms more efficient. In this work, we focus on making the
simulator more efficient, making training data much more accessible than
previously possible. We introduce Surgical Gym, an open-source high performance
platform for surgical robot learning where both the physics simulation and
reinforcement learning occur directly on the GPU. We demonstrate between
100-5000x faster training times compared with previous surgical learning
platforms. The code is available at:
https://github.com/SamuelSchmidgall/SurgicalGym.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05668">LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feiyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shuiguang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yi Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a></p>
<p>Most of current anomaly detection models assume that the normal pattern
remains same all the time. However, the normal patterns of Web services change
dramatically and frequently. The model trained on old-distribution data is
outdated after such changes. Retraining the whole model every time is
expensive. Besides, at the beginning of normal pattern changes, there is not
enough observation data from the new distribution. Retraining a large neural
network model with limited data is vulnerable to overfitting. Thus, we propose
a Light and Anti-overfitting Retraining Approach (LARA) for deep variational
auto-encoder based time series anomaly detection methods (VAEs). This work aims
to make three novel contributions: 1) the retraining process is formulated as a
convex problem and can converge at a fast rate as well as prevent overfitting;
2) designing a ruminate block, which leverages the historical data without the
need to store them; 3) mathematically proving that when fine-tuning the latent
vector and reconstructed data, the linear formations can achieve the least
adjusting errors between the ground truths and the fine-tuned ones.
</p>
<p>Moreover, we have performed many experiments to verify that retraining LARA
with even 43 time slots of data from new distribution can result in its
competitive F1 Score in comparison with the state-of-the-art anomaly detection
models trained with sufficient data. Besides, we verify its light overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06639">The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators. (arXiv:2310.06639v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marcondes_D/0/1/0/all/0/1">Diego Marcondes</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrera_J/0/1/0/all/0/1">Junior Barrera</a></p>
<p>The machine learning of lattice operators has three possible bottlenecks.
From a statistical standpoint, it is necessary to design a constrained class of
operators based on prior information with low bias, and low complexity relative
to the sample size. From a computational perspective, there should be an
efficient algorithm to minimize an empirical error over the class. From an
understanding point of view, the properties of the learned operator need to be
derived, so its behavior can be theoretically understood. The statistical
bottleneck can be overcome due to the rich literature about the representation
of lattice operators, but there is no general learning algorithm for them. In
this paper, we discuss a learning paradigm in which, by overparametrizing a
class via elements in a lattice, an algorithm for minimizing functions in a
lattice is applied to learn. We present the stochastic lattice descent
algorithm as a general algorithm to learn on constrained classes of operators
as long as a lattice overparametrization of it is fixed, and we discuss
previous works which are proves of concept. Moreover, if there are algorithms
to compute the basis of an operator from its overparametrization, then its
properties can be deduced and the understanding bottleneck is also overcome.
This learning paradigm has three properties that modern methods based on neural
networks lack: control, transparency and interpretability. Nowadays, there is
an increasing demand for methods with these characteristics, and we believe
that mathematical morphology is in a unique position to supply them. The
lattice overparametrization paradigm could be a missing piece for it to achieve
its full potential within modern machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07276">BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1">Qizhi Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jinhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kehan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kaiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lijun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yingce Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Recent advancements in biological research leverage the integration of
molecules, proteins, and natural language to enhance drug discovery. However,
current models exhibit several limitations, such as the generation of invalid
molecular SMILES, underutilization of contextual information, and equal
treatment of structured and unstructured knowledge. To address these issues, we
propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches
cross-modal integration in biology with chemical knowledge and natural language
associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular
representations and extracts knowledge from the surrounding context of
bio-entities in unstructured biological literature. Furthermore,
$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,
leading to more effective utilization of information. After fine-tuning, BioT5
shows superior performance across a wide range of tasks, demonstrating its
strong capability of capturing underlying relations and properties of
bio-entities. Our code is available at
$\href{https://github.com/QizhiPei/BioT5}{Github}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07736">Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v3 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1">Tianji Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hulsebos_M/0/1/0/all/0/1">Madelon Hulsebos</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhenjie Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1">Paul Groth</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1">H. V. Jagadish</a></p>
<p>Language models and specialized table embedding models have recently
demonstrated strong performance on many tasks over tabular data. Researchers
and practitioners are keen to leverage these models in many new application
contexts; but limited understanding of the strengths and weaknesses of these
models, and the table representations they generate, makes the process of
finding a suitable model for a given task reliant on trial and error. There is
an urgent need to gain a comprehensive understanding of these models to
minimize inefficiency and failures in downstream usage.
</p>
<p>To address this need, we propose Observatory, a formal framework to
systematically analyze embedding representations of relational tables.
Motivated both by invariants of the relational data model and by statistical
considerations regarding data distributions, we define eight primitive
properties, and corresponding measures to quantitatively characterize table
embeddings for these properties. Based on these properties, we define an
extensible framework to evaluate language and table embedding models. We
collect and synthesize a suite of datasets and use Observatory to analyze nine
such models. Our analysis provides insights into the strengths and weaknesses
of learned representations over tables. We find, for example, that some models
are sensitive to table structure such as column order, that functional
dependencies are rarely reflected in embeddings, and that specialized table
embedding models have relatively lower sample fidelity. Such insights help
researchers and practitioners better anticipate model behaviors and select
appropriate models for their downstream tasks, while guiding researchers in the
development of new models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10443">Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification. (arXiv:2310.10443v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grivas_A/0/1/0/all/0/1">Andreas Grivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1">Antonio Vergari</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1">Adam Lopez</a></p>
<p>Sigmoid output layers are widely used in multi-label classification (MLC)
tasks, in which multiple labels can be assigned to any input. In many practical
MLC tasks, the number of possible labels is in the thousands, often exceeding
the number of input features and resulting in a low-rank output layer. In
multi-class classification, it is known that such a low-rank output layer is a
bottleneck that can result in unargmaxable classes: classes which cannot be
predicted for any input. In this paper, we show that for MLC tasks, the
analogous sigmoid bottleneck results in exponentially many unargmaxable label
combinations. We explain how to detect these unargmaxable outputs and
demonstrate their presence in three widely used MLC datasets. We then show that
they can be prevented in practice by introducing a Discrete Fourier Transform
(DFT) output layer, which guarantees that all sparse label combinations with up
to $k$ active labels are argmaxable. Our DFT layer trains faster and is more
parameter efficient, matching the F1@k score of a sigmoid layer while using up
to 50% fewer trainable parameters. Our code is publicly available at
https://github.com/andreasgrv/sigmoid-bottleneck.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10483">Passive Inference Attacks on Split Learning via Adversarial Regularization. (arXiv:2310.10483v4 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaochen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xinjian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuncheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yangfan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xiaokui Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1">Beng Chin Ooi</a></p>
<p>Split Learning (SL) has emerged as a practical and efficient alternative to
traditional federated learning. While previous attempts to attack SL have often
relied on overly strong assumptions or targeted easily exploitable models, we
seek to develop more practical attacks. We introduce SDAR, a novel attack
framework against SL with an honest-but-curious server. SDAR leverages
auxiliary data and adversarial regularization to learn a decodable simulator of
the client's private model, which can effectively infer the client's private
features under the vanilla SL, and both features and labels under the U-shaped
SL. We perform extensive experiments in both configurations to validate the
effectiveness of our proposed attacks. Notably, in challenging but practical
scenarios where existing passive attacks struggle to reconstruct the client's
private data effectively, SDAR consistently achieves attack performance
comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR
achieves private feature reconstruction with less than 0.025 mean squared error
in both the vanilla and the U-shaped SL, and attains a label inference accuracy
of over 98% in the U-shaped setting, while existing attacks fail to produce
non-trivial results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10705">Empirical and Experimental Insights into Machine Learning-Based Defect Classification in Semiconductor Wafers. (arXiv:2310.10705v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taha_K/0/1/0/all/0/1">Kamal Taha</a></p>
<p>This survey paper offers a comprehensive review of methodologies utilizing
machine learning (ML) classification techniques for identifying wafer defects
in semiconductor manufacturing. Despite the growing body of research
demonstrating the effectiveness of ML in wafer defect identification, there is
a noticeable absence of comprehensive reviews on this subject. This survey
attempts to fill this void by amalgamating available literature and providing
an in-depth analysis of the advantages, limitations, and potential applications
of various ML classification algorithms in the realm of wafer defect detection.
An innovative taxonomy of methodologies that we present provides a detailed
classification of algorithms into more refined categories and techniques. This
taxonomy follows a three-tier structure, starting from broad methodology
categories and ending with specific techniques. It aids researchers in
comprehending the complex relationships between different algorithms and their
techniques. We employ a rigorous empirical and experimental evaluation to rank
these varying techniques. For the empirical evaluation, we assess techniques
based on a set of five criteria. The experimental evaluation ranks the
algorithms employing the same techniques, sub-categories, and categories. Also
the paper illuminates the future prospects of ML classification techniques for
wafer defect identification, underscoring potential advancements and
opportunities for further research in this field
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10818">Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning. (arXiv:2310.10818v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_P/0/1/0/all/0/1">Parvin Malekzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1">Ming Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1">Konstantinos N. Plataniotis</a></p>
<p>Sample efficiency is central to developing practical reinforcement learning
(RL) for complex and large-scale decision-making problems. The ability to
transfer and generalize knowledge gained from previous experiences to
downstream tasks can significantly improve sample efficiency. Recent research
indicates that successor feature (SF) RL algorithms enable knowledge
generalization between tasks with different rewards but identical transition
dynamics. It has recently been hypothesized that combining model-based (MB)
methods with SF algorithms can alleviate the limitation of fixed transition
dynamics. Furthermore, uncertainty-aware exploration is widely recognized as
another appealing approach for improving sample efficiency. Putting together
two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads
to an approach to the problem of sample efficient uncertainty-aware knowledge
transfer across tasks with different transition dynamics or/and reward
functions. In this paper, the uncertainty of the value of each action is
approximated by a Kalman filter (KF)-based multiple-model adaptive estimation.
This KF-based framework treats the parameters of a model as random variables.
To the best of our knowledge, this is the first attempt at formulating a hybrid
MB-SF algorithm capable of generalizing knowledge across large or continuous
state space tasks with various transition dynamics while requiring less
computation at decision time than MB methods. The number of samples required to
learn the tasks was compared to recent SF and MB baselines. The results show
that our algorithm generalizes its knowledge across different transition
dynamics, learns downstream tasks with significantly fewer samples than
starting from scratch, and outperforms existing approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13833">GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mufei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreacic_E/0/1/0/all/0/1">Eleonora Krea&#x10d;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Potluru_V/0/1/0/all/0/1">Vamsi K. Potluru</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a></p>
<p>Large-scale graphs with node attributes are increasingly common in various
real-world applications. Creating synthetic, attribute-rich graphs that mirror
real-world examples is crucial, especially for sharing graph data for analysis
and developing learning models when original data is restricted to be shared.
Traditional graph generation methods are limited in their capacity to handle
these complex structures. Recent advances in diffusion models have shown
potential in generating graph structures without attributes and smaller
molecular graphs. However, these models face challenges in generating large
attributed graphs due to the complex attribute-structure correlations and the
large size of these graphs. This paper introduces a novel diffusion model,
GraphMaker, specifically designed for generating large attributed graphs. We
explore various combinations of node attribute and graph structure generation
processes, finding that an asynchronous approach more effectively captures the
intricate attribute-structure correlations. We also address scalability issues
through edge mini-batching generation. To demonstrate the practicality of our
approach in graph data dissemination, we introduce a new evaluation pipeline.
The evaluation demonstrates that synthetic graphs generated by GraphMaker can
be used to develop competitive graph machine learning models for the tasks
defined over the original graphs without actually accessing these graphs, while
many leading graph generation methods fall short in this evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14526">Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunfan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Behari_N/0/1/0/all/0/1">Nikhil Behari</a>, <a href="http://arxiv.org/find/cs/1/au:+Hughes_E/0/1/0/all/0/1">Edward Hughes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Edwin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1">Dheeraj Nagaraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1">Karl Tuyls</a>, <a href="http://arxiv.org/find/cs/1/au:+Taneja_A/0/1/0/all/0/1">Aparna Taneja</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1">Milind Tambe</a></p>
<p>Restless multi-arm bandits (RMABs), a class of resource allocation problems
with broad application in areas such as healthcare, online advertising, and
anti-poaching, have recently been studied from a multi-agent reinforcement
learning perspective. Prior RMAB research suffers from several limitations,
e.g., it fails to adequately address continuous states, and requires retraining
from scratch when arms opt-in and opt-out over time, a common challenge in many
real world applications. We address these limitations by developing a neural
network-based pre-trained model (PreFeRMAB) that has general zero-shot ability
on a wide range of previously unseen RMABs, and which can be fine-tuned on
specific instances in a more sample-efficient way than retraining from scratch.
Our model also accommodates general multi-action settings and discrete or
continuous state spaces. To enable fast generalization, we learn a novel single
policy network model that utilizes feature information and employs a training
procedure in which arms opt-in and out over time. We derive a new update rule
for a crucial $\lambda$-network with theoretical convergence guarantees and
empirically demonstrate the advantages of our approach on several challenging,
real-world inspired problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19063">Feature Aggregation in Joint Sound Classification and Localization Neural Networks. (arXiv:2310.19063v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Healy_B/0/1/0/all/0/1">Brendan Healy</a>, <a href="http://arxiv.org/find/cs/1/au:+McNamee_P/0/1/0/all/0/1">Patrick McNamee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmadabadi_Z/0/1/0/all/0/1">Zahra Nili Ahmadabadi</a></p>
<p>This study addresses the application of deep learning techniques in joint
sound signal classification and localization networks. Current state-of-the-art
sound source localization deep learning networks lack feature aggregation
within their architecture. Feature aggregation enhances model performance by
enabling the consolidation of information from different feature scales,
thereby improving feature robustness and invariance. This is particularly
important in SSL networks, which must differentiate direct and indirect
acoustic signals. To address this gap, we adapt feature aggregation techniques
from computer vision neural networks to signal detection neural networks.
Additionally, we propose the Scale Encoding Network (SEN) for feature
aggregation to encode features from various scales, compressing the network for
more computationally efficient aggregation. To evaluate the efficacy of feature
aggregation in SSL networks, we integrated the following computer vision
feature aggregation sub-architectures into a SSL control architecture: Path
Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network
(BiFPN), and SEN. These sub-architectures were evaluated using two metrics for
signal classification and two metrics for direction-of-arrival regression.
PANet and BiFPN are established aggregators in computer vision models, while
the proposed SEN is a more compact aggregator. The results suggest that models
incorporating feature aggregations outperformed the control model, the Sound
Event Localization and Detection network (SELDnet), in both sound signal
classification and localization. The feature aggregation techniques enhance the
performance of sound detection neural networks, particularly in
direction-of-arrival regression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20025">GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mianchu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Rui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1">Giovanni Montana</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a></p>
<p>Offline Goal-Conditioned RL (GCRL) offers a feasible paradigm for learning
general-purpose policies from diverse and multi-task offline datasets. Despite
notable recent progress, the predominant offline GCRL methods, mainly
model-free, face constraints in handling limited data and generalizing to
unseen goals. In this work, we propose Goal-conditioned Offline Planning
(GOPlan), a novel model-based framework that contains two key phases: (1)
pretraining a prior policy capable of capturing multi-modal action distribution
within the multi-goal dataset; (2) employing the reanalysis method with
planning to generate imagined trajectories for funetuning policies.
Specifically, we base the prior policy on an advantage-weighted conditioned
generative adversarial network, which facilitates distinct mode separation,
mitigating the pitfalls of out-of-distribution (OOD) actions. For further
policy optimization, the reanalysis method generates high-quality imaginary
data by planning with learned models for both intra-trajectory and
inter-trajectory goals. With thorough experimental evaluations, we demonstrate
that GOPlan achieves state-of-the-art performance on various offline multi-goal
navigation and manipulation tasks. Moreover, our results highlight the superior
ability of GOPlan to handle small data budgets and generalize to OOD goals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01927">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1">Tobias Katsch</a></p>
<p>Linear Recurrence has proven to be a powerful tool for modeling long
sequences efficiently. In this work, we show that existing models fail to take
full advantage of its potential. Motivated by this finding, we develop
GateLoop, a foundational sequence model that generalizes linear recurrent
models such as S4, S5, LRU and RetNet, by employing data-controlled state
transitions. Utilizing this theoretical advance, GateLoop empirically
outperforms existing models for auto-regressive language modeling. Our method
comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$
parallel mode making use of highly optimized associative scan implementations.
Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing
remarkable implications for Transformer and recently proposed architectures.
Specifically, we prove that our approach can be interpreted as providing
data-controlled relative-positional information to Attention. While many
existing models solely rely on data-controlled cumulative sums for context
aggregation, our findings suggest that incorporating data-controlled complex
cumulative products may be a crucial step towards more powerful sequence
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07550">Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pleiter_B/0/1/0/all/0/1">Bart Pleiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Tajalli_B/0/1/0/all/0/1">Behrad Tajalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1">Stefanos Koffas</a>, <a href="http://arxiv.org/find/cs/1/au:+Abad_G/0/1/0/all/0/1">Gorka Abad</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Larson_M/0/1/0/all/0/1">Martha Larson</a>, <a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1">Stjepan Picek</a></p>
<p>Deep Neural Networks (DNNs) have shown great promise in various domains.
Alongside these developments, vulnerabilities associated with DNN training,
such as backdoor attacks, are a significant concern. These attacks involve the
subtle insertion of triggers during model training, allowing for manipulated
predictions.More recently, DNNs for tabular data have gained increasing
attention due to the rise of transformer models.
</p>
<p>Our research presents a comprehensive analysis of backdoor attacks on tabular
data using DNNs, particularly focusing on transformers. Given the inherent
complexities of tabular data, we explore the challenges of embedding backdoors.
Through systematic experimentation across benchmark datasets, we uncover that
transformer-based DNNs for tabular data are highly susceptible to backdoor
attacks, even with minimal feature value alterations. We also verify that our
attack can be generalized to other models, like XGBoost and DeepFM. Our results
indicate nearly perfect attack success rates (approximately 100%) by
introducing novel backdoor attack strategies to tabular data. Furthermore, we
evaluate several defenses against these attacks, identifying Spectral
Signatures as the most effective one. Our findings highlight the urgency of
addressing such vulnerabilities and provide insights into potential
countermeasures for securing DNN models against backdoors in tabular data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07957">Language Models are Better Bug Detector Through Code-Pair Classification. (arXiv:2311.07957v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alrashedy_K/0/1/0/all/0/1">Kamel Alrashedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Binjahlan_A/0/1/0/all/0/1">Ahmed Binjahlan</a></p>
<p>Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful
models for code generation and understanding. Fine-tuning these models comes
with a high computational cost and requires a large labeled dataset.
Alternatively, in-context learning techniques allow models to learn downstream
tasks with only a few examples. Recently, researchers have shown how in-context
learning performs well in bug detection and repair. In this paper, we propose
code-pair classification task in which both the buggy and non-buggy versions
are given to the model, and the model identifies the buggy ones. We evaluate
our task in real-world dataset of bug detection and two most powerful LLMs. Our
experiments indicate that an LLM can often pick the buggy from the non-buggy
version of the code, and the code-pair classification task is much easier
compared to be given a snippet and deciding if and where a bug exists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08053">ARQ for Active Learning at the Edge. (arXiv:2311.08053v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Croisfelt_V/0/1/0/all/0/1">Victor Croisfelt</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1">Shashi Raj Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1">Osvaldo Simeone</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1">Petar Popovski</a></p>
<p>Conventional retransmission (ARQ) protocols are designed with the goal of
ensuring the correct reception of all the individual transmitter's packets at
the receiver. When the transmitter is a learner communicating with a teacher,
this goal is at odds with the actual aim of the learner, which is that of
eliciting the most relevant label information from the teacher. Taking an
active learning perspective, this paper addresses the following key protocol
design questions: (i) Active batch selection: Which batch of inputs should be
sent to the teacher to acquire the most useful information and thus reduce the
number of required communication rounds? (ii) Batch encoding: Can batches of
data points be combined to reduce the communication resources required at each
communication round? Specifically, this work introduces
Communication-Constrained Bayesian Active Knowledge Distillation (CC-BAKD), a
novel protocol that integrates Bayesian active learning with compression via a
linear mix-up mechanism. Comparisons with existing active learning protocols
demonstrate the advantages of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08149">Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes. (arXiv:2311.08149v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trottet_C/0/1/0/all/0/1">C&#xe9;cile Trottet</a>, <a href="http://arxiv.org/find/cs/1/au:+Schurch_M/0/1/0/all/0/1">Manuel Sch&#xfc;rch</a>, <a href="http://arxiv.org/find/cs/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/cs/1/au:+Barua_I/0/1/0/all/0/1">Imon Barua</a>, <a href="http://arxiv.org/find/cs/1/au:+Petelytska_L/0/1/0/all/0/1">Liubov Petelytska</a>, <a href="http://arxiv.org/find/cs/1/au:+Distler_O/0/1/0/all/0/1">Oliver Distler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffmann_Vold_A/0/1/0/all/0/1">Anna-Maria Hoffmann-Vold</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1">Michael Krauthammer</a>, the <a href="http://arxiv.org/find/cs/1/au:+collaborators_E/0/1/0/all/0/1">EUSTAR collaborators</a></p>
<p>In this paper, we propose a deep generative time series approach using latent
temporal processes for modeling and holistically analyzing complex disease
trajectories. We aim to find meaningful temporal latent representations of an
underlying generative process that explain the observed disease trajectories in
an interpretable and comprehensive way. To enhance the interpretability of
these latent temporal processes, we develop a semi-supervised approach for
disentangling the latent space using established medical concepts. By combining
the generative approach with medical knowledge, we leverage the ability to
discover novel aspects of the disease while integrating medical concepts into
the model. We show that the learned temporal latent processes can be utilized
for further data analysis and clinical hypothesis testing, including finding
similar patients and clustering the disease into new sub-types. Moreover, our
method enables personalized online monitoring and prediction of multivariate
time series including uncertainty quantification. We demonstrate the
effectiveness of our approach in modeling systemic sclerosis, showcasing the
potential of our machine learning model to capture complex disease trajectories
and acquire new medical knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08724">Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Che Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>In this paper, we propose a method for knowledge graph construction in power
distribution networks. This method leverages entity features, which involve
their semantic, phonetic, and syntactic characteristics, in both the knowledge
graph of distribution network and the dispatching texts. An enhanced model
based on Convolutional Neural Network, is utilized for effectively matching
dispatch text entities with those in the knowledge graph. The effectiveness of
this model is evaluated through experiments in real-world power distribution
dispatch scenarios. The results indicate that, compared with the baselines, the
proposed model excels in linking a variety of entity types, demonstrating high
overall accuracy in power distribution knowledge graph construction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14387">Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling. (arXiv:2311.14387v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1">Zeping Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lei Wu</a></p>
<p>In this work, we investigate the margin-maximization bias exhibited by
gradient-based algorithms in classifying linearly separable data. We present an
in-depth analysis of the specific properties of the velocity field associated
with (normalized) gradients, focusing on their role in margin maximization.
Inspired by this analysis, we propose a novel algorithm called Progressive
Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at
an {\em exponential rate}. This stands in stark contrast to all existing
algorithms, which maximize the margin at a slow {\em polynomial rate}.
Specifically, we identify mild conditions on data distribution under which
existing algorithms such as gradient descent (GD) and normalized gradient
descent (NGD) {\em provably fail} in maximizing the margin efficiently. To
validate our theoretical findings, we present both synthetic and real-world
experiments. Notably, PRGD also shows promise in enhancing the generalization
performance when applied to linearly non-separable datasets and deep neural
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15480">Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Callie C. Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1">Duoduo Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guessford_J/0/1/0/all/0/1">Jesse Guessford</a></p>
<p>There has recently been a sharp increase in interest in Artificial
Intelligence-Generated Content (AIGC). Despite this, musical components such as
time signatures have not been studied sufficiently to form an algorithmic
determination approach for new compositions, especially lyrical songs. This is
likely because of the neglect of musical details, which is critical for
constructing a robust framework. Specifically, time signatures establish the
fundamental rhythmic structure for almost all aspects of a song, including the
phrases and notes. In this paper, we propose a novel approach that only uses
lyrics as input to automatically generate a fitting time signature for lyrical
songs and uncover the latent rhythmic structure utilizing explainable machine
learning models. In particular, we devise multiple methods that are associated
with discovering lyrical patterns and creating new features that simultaneously
contain lyrical, rhythmic, and statistical information. In this approach, the
best of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under
the Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In
conclusion, our research directly generates time signatures from lyrics
automatically for new scores utilizing machine learning, which is an innovative
idea that approaches an understudied component of musicology and therefore
contributes significantly to the future of Artificial Intelligence (AI) music
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16522">Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hao Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Si Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chuanfu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Che Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>To enhance the intelligence degree in operation and maintenance, a novel
method for fault detection in power grids is proposed. The proposed GNN-based
approach first identifies fault nodes through a specialized feature extraction
method coupled with a knowledge graph. By incorporating temporal data, the
method leverages the status of nodes from preceding and subsequent time periods
to help current fault detection. To validate the effectiveness of the node
features, a correlation analysis of the output features from each node was
conducted. The results from experiments show that this method can accurately
locate fault nodes in simulation scenarios with a remarkable accuracy.
Additionally, the graph neural network based feature modeling allows for a
qualitative examination of how faults spread across nodes, which provides
valuable insights for analyzing fault nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16877">Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thu Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1">Tuan L. Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1">P&#xe5;l Halvorsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1">Michael A. Riegler</a></p>
<p>Missing data is a common problem in practical settings. Various imputation
methods have been developed to deal with missing data. However, even though the
label is usually available in the training data, the common practice of
imputation usually only relies on the input and ignores the label. In this
work, we illustrate how stacking the label into the input can significantly
improve the imputation of the input. In addition, we propose a classification
strategy that initializes the predicted test label with missing values and
stacks the label with the input for imputation. This allows imputing the label
and the input at the same time. Also, the technique is capable of handling data
training with missing labels without any prior imputation and is applicable to
continuous, categorical, or mixed-type data. Experiments show promising results
in terms of accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18727">Automatic Functional Differentiation in JAX. (arXiv:2311.18727v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a></p>
<p>We extend JAX with the capability to automatically differentiate higher-order
functions (functionals and operators). By representing functions as a
generalization of arrays, we seamlessly use JAX's existing primitive system to
implement higher-order functions. We present a set of primitive operators that
serve as foundational building blocks for constructing several key types of
functionals. For every introduced primitive operator, we derive and implement
both linearization and transposition rules, aligning with JAX's internal
protocols for forward and reverse mode automatic differentiation. This
enhancement allows for functional differentiation in the same syntax
traditionally use for functions. The resulting functional gradients are
themselves functions ready to be invoked in python. We showcase this tool's
efficacy and simplicity through applications where functional derivatives are
indispensable. The source code of this work is released at
https://github.com/sail-sg/autofd .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03731">MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xingtong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinming Zhang</a></p>
<p>Graphs can inherently model interconnected objects on the Web, thereby
facilitating a series of Web applications, such as web analyzing and content
recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a
mainstream technique for graph representation learning. However, their efficacy
within an end-to-end supervised framework is significantly tied to the
availabilityof task-specific labels. To mitigate labeling costs and enhance
robustness in few-shot settings, pre-training on self-supervised tasks has
emerged as a promising method, while prompting has been proposed to further
narrow the objective gap between pretext and downstream tasks. Although there
has been some initial exploration of prompt-based learning on graphs, they
primarily leverage a single pretext task, resulting in a limited subset of
general knowledge that could be learned from the pre-training data. Hence, in
this paper, we propose MultiGPrompt, a novel multi-task pre-training and
prompting framework to exploit multiple pretext tasks for more comprehensive
pre-trained knowledge. First, in pre-training, we design a set of pretext
tokens to synergize multiple pretext tasks. Second, we propose a dual-prompt
mechanism consisting of composed and open prompts to leverage task-specific and
global pre-training knowledge, to guide downstream tasks in few-shot settings.
Finally, we conduct extensive experiments on six public datasets to evaluate
and analyze MultiGPrompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03905">A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1">Kareem Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>Neuro-symbolic AI bridges the gap between purely symbolic and neural
approaches to learning. This often requires maximizing the likelihood of a
symbolic constraint w.r.t the neural network's output distribution. Such output
distributions are typically assumed to be fully-factorized. This limits the
applicability of neuro-symbolic learning to the more expressive autoregressive
distributions, e.g., transformers. Under such distributions, computing the
likelihood of even simple constraints is #P-hard. Instead of attempting to
enforce the constraint on the entire output distribution, we propose to do so
on a random, local approximation thereof. More precisely, we optimize the
likelihood of the constraint under a pseudolikelihood-based approximation
centered around a model sample. Our approximation is factorized, allowing the
reuse of solutions to sub-problems, a main tenet for efficiently computing
neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of
the likelihood, exhibiting low entropy and KL-divergence around the model
sample. We evaluate our approach on Sudoku and shortest-path prediction cast as
autoregressive generation, and observe that we greatly improve upon the base
model's ability to predict logically-consistent outputs. We also evaluate on
the task of detoxifying large language models. Using a simple constraint
disallowing a list of toxic words, we are able to steer the model's outputs
away from toxic generations, achieving SoTA detoxification compared to previous
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04027">The sample complexity of multi-distribution learning. (arXiv:2312.04027v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a></p>
<p>Multi-distribution learning generalizes the classic PAC learning to handle
data coming from multiple distributions. Given a set of $k$ data distributions
and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis
that minimizes the maximum population loss over $k$ distributions, up to
$\epsilon$ additive error. In this paper, we settle the sample complexity of
multi-distribution learning by giving an algorithm of sample complexity
$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the
lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem
of Awasthi, Haghtalab and Zhao [AHZ23].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06695">Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leger_C/0/1/0/all/0/1">Corentin L&#xe9;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamon_G/0/1/0/all/0/1">Gautier Hamon</a>, <a href="http://arxiv.org/find/cs/1/au:+Nisioti_E/0/1/0/all/0/1">Eleni Nisioti</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinaut_X/0/1/0/all/0/1">Xavier Hinaut</a>, <a href="http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1">Cl&#xe9;ment Moulin-Frier</a></p>
<p>Animals often demonstrate a remarkable ability to adapt to their environments
during their lifetime. They do so partly due to the evolution of morphological
and neural structures. These structures capture features of environments shared
between generations to bias and speed up lifetime learning. In this work, we
propose a computational model for studying a mechanism that can enable such a
process. We adopt a computational framework based on meta reinforcement
learning as a model of the interplay between evolution and development. At the
evolutionary scale, we evolve reservoirs, a family of recurrent neural networks
that differ from conventional networks in that one optimizes not the synaptic
weights, but hyperparameters controlling macro-level properties of the
resulting network architecture. At the developmental scale, we employ these
evolved reservoirs to facilitate the learning of a behavioral policy through
Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the
environment state before providing it to an action policy. We evaluate our
approach on several 2D and 3D simulated environments. Our results show that the
evolution of reservoirs can improve the learning of diverse challenging tasks.
We study in particular three hypotheses: the use of an architecture combining
reservoirs and reinforcement learning could enable (1) solving tasks with
partial observability, (2) generating oscillatory dynamics that facilitate the
learning of locomotion tasks, and (3) facilitating the generalization of
learned behaviors to new tasks unknown during the evolution phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09486">Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zixian Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jingwei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiufeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>While recent test-time adaptations exhibit efficacy by adjusting batch
normalization to narrow domain disparities, their effectiveness diminishes with
realistic mini-batches due to inaccurate target estimation. As previous
attempts merely introduce source statistics to mitigate this issue, the
fundamental problem of inaccurate target estimation still persists, leaving the
intrinsic test-time domain shifts unresolved. This paper delves into the
problem of mini-batch degradation. By unraveling batch normalization, we
discover that the inexact target statistics largely stem from the substantially
reduced class diversity in batch. Drawing upon this insight, we introduce a
straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge
the class diversity gap between training and testing batches. Importantly, our
TEMA adaptively extends the scope of typical methods beyond the current batch
to incorporate a diverse set of class information, which in turn boosts an
accurate target estimation. Built upon this foundation, we further design a
novel layer-wise rectification strategy to consistently promote test-time
performance. Our proposed method enjoys a unique advantage as it requires
neither training nor tuning parameters, offering a truly hassle-free solution.
It significantly enhances model robustness against shifted domains and
maintains resilience in diverse real-world scenarios with various batch sizes,
achieving state-of-the-art performance on several major benchmarks. Code is
available at \url{https://github.com/kiwi12138/RealisticTTA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11456">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wei Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hanze Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Chenlu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Han Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>This paper studies the theoretical framework of the alignment process of
generative models with Reinforcement Learning from Human Feedback (RLHF). We
consider a standard mathematical formulation, the reverse-KL regularized
contextual bandit for RLHF. Despite its widespread practical application, a
rigorous theoretical analysis of this formulation remains open. We investigate
its behavior in three distinct settings -- offline, online, and hybrid -- and
propose efficient algorithms with finite-sample theoretical guarantees.
</p>
<p>Moving towards practical applications, our framework, with a robust
approximation of the information-theoretical policy improvement oracle,
naturally gives rise to several novel RLHF algorithms. This includes an
iterative version of the Direct Preference Optimization (DPO) algorithm for
online settings, and a multi-step rejection sampling strategy for offline
scenarios. Our empirical evaluations on real-world alignment experiment of
large language model demonstrate that these proposed methods significantly
surpass existing strong baselines, such as DPO and Rejection Sampling
Optimization (RSO), showcasing the connections between solid theoretical
foundations and their powerful practical implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11714">Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuansan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wijewickrema_S/0/1/0/all/0/1">Sudanthi Wijewickrema</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bester_C/0/1/0/all/0/1">Christofer Bester</a>, <a href="http://arxiv.org/find/cs/1/au:+OLeary_S/0/1/0/all/0/1">Stephen O&#x27;Leary</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1">James Bailey</a></p>
<p>Generating time series data is a promising approach to address data
deficiency problems. However, it is also challenging due to the complex
temporal properties of time series data, including local correlations as well
as global dependencies. Most existing generative models have failed to
effectively learn both the local and global properties of time series data. To
address this open problem, we propose a novel time series generative model
named 'Time-Transformer AAE', which consists of an adversarial autoencoder
(AAE) and a newly designed architecture named 'Time-Transformer' within the
decoder. The Time-Transformer first simultaneously learns local and global
features in a layer-wise parallel design, combining the abilities of Temporal
Convolutional Networks and Transformer in extracting local features and global
dependencies respectively. Second, a bidirectional cross attention is proposed
to provide complementary guidance across the two branches and achieve proper
fusion between local and global features. Experimental results demonstrate that
our model can outperform existing state-of-the-art models in 5 out of 6
datasets, specifically on those with data containing both global and local
properties. Furthermore, we highlight our model's advantage on handling this
kind of data via an artificial dataset. Finally, we show our model's ability to
address a real-world problem: data augmentation to support learning with small
datasets and imbalanced datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11730">Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ku_E/0/1/0/all/0/1">Eugene Ku</a>, <a href="http://arxiv.org/find/cs/1/au:+Arunraj_S/0/1/0/all/0/1">Swetha Arunraj</a></p>
<p>Graph Neural Networks are notorious for its memory consumption. A recent
Transformer-based GNN called Graph Transformer is shown to obtain superior
performances when long range dependencies exist. However, combining graph data
and Transformer architecture led to a combinationally worse memory issue. We
propose a novel version of "edge regularization technique" that alleviates the
need for Positional Encoding and ultimately alleviate GT's out of memory issue.
We observe that it is not clear whether having an edge regularization on top of
positional encoding is helpful. However, it seems evident that applying our
edge regularization technique indeed stably improves GT's performance compared
to GT without Positional Encoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13250">The role of data embedding in equivariant quantum convolutional neural networks. (arXiv:2312.13250v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Das_S/0/1/0/all/0/1">Sreetama Das</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Martina_S/0/1/0/all/0/1">Stefano Martina</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Caruso_F/0/1/0/all/0/1">Filippo Caruso</a></p>
<p>Geometric deep learning refers to the scenario in which the symmetries of a
dataset are used to constrain the parameter space of a neural network and thus,
improve their trainability and generalization. Recently this idea has been
incorporated into the field of quantum machine learning, which has given rise
to equivariant quantum neural networks (EQNNs). In this work, we investigate
the role of classical-to-quantum embedding on the performance of equivariant
quantum convolutional neural networks (EQCNNs) for the classification of
images. We discuss the connection between the data embedding method and the
resulting representation of a symmetry group and analyze how changing
representation affects the expressibility of an EQCNN. We numerically compare
the classification accuracy of EQCNNs with three different basis-permuted
amplitude embeddings to the one obtained from a non-equivariant quantum
convolutional neural network (QCNN). Our results show a clear dependence of
classification accuracy on the underlying embedding, especially for initial
training iterations. The improvement in classification accuracy of EQCNN over
non-equivariant QCNN may be present or absent depending on the particular
embedding and dataset used. It is expected that the results of this work can be
useful to the community for a better understanding of the importance of data
embedding choice in the context of geometric quantum machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15600">Context-aware Communication for Multi-agent Reinforcement Learning. (arXiv:2312.15600v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a></p>
<p>Effective communication protocols in multi-agent reinforcement learning
(MARL) are critical to fostering cooperation and enhancing team performance. To
leverage communication, many previous works have proposed to compress local
information into a single message and broadcast it to all reachable agents.
This simplistic messaging mechanism, however, may fail to provide adequate,
critical, and relevant information to individual agents, especially in severely
bandwidth-limited scenarios. This motivates us to develop context-aware
communication schemes for MARL, aiming to deliver personalized messages to
different agents. Our communication protocol, named CACOM, consists of two
stages. In the first stage, agents exchange coarse representations in a
broadcast fashion, providing context for the second stage. Following this,
agents utilize attention mechanisms in the second stage to selectively generate
messages personalized for the receivers. Furthermore, we employ the learned
step size quantization (LSQ) technique for message quantization to reduce the
communication overhead. To evaluate the effectiveness of CACOM, we integrate it
with both actor-critic and value-based MARL algorithms. Empirical results on
cooperative benchmark tasks demonstrate that CACOM provides evident performance
gains over baselines under communication-constrained scenarios. The code is
publicly available at https://github.com/LXXXXR/CACOM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16554">A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinyuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Gongxi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuxing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Federated learning (FL) enables multiple clients to collaboratively learn a
shared model without sharing their individual data. Concerns about utility,
privacy, and training efficiency in FL have garnered significant research
attention. Differential privacy has emerged as a prevalent technique in FL,
safeguarding the privacy of individual user data while impacting utility and
training efficiency. Within Differential Privacy Federated Learning (DPFL),
previous studies have primarily focused on the utility-privacy trade-off,
neglecting training efficiency, which is crucial for timely completion.
Moreover, differential privacy achieves privacy by introducing controlled
randomness (noise) on selected clients in each communication round. Previous
work has mainly examined the impact of noise level ($\sigma$) and communication
rounds ($T$) on the privacy-utility dynamic, overlooking other influential
factors like the sample ratio ($q$, the proportion of selected clients). This
paper systematically formulates an efficiency-constrained utility-privacy
bi-objective optimization problem in DPFL, focusing on $\sigma$, $T$, and $q$.
We provide a comprehensive theoretical analysis, yielding analytical solutions
for the Pareto front. Extensive empirical experiments verify the validity and
efficacy of our analysis, offering valuable guidance for low-cost parameter
design in DPFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16762">Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs. (arXiv:2312.16762v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wang_S/0/1/0/all/0/1">Shanshan Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Diagne_M/0/1/0/all/0/1">Mamadou Diagne</a>, <a href="http://arxiv.org/find/math/1/au:+Krstic_M/0/1/0/all/0/1">Miroslav Krsti&#x107;</a></p>
<p>Deep neural network approximation of nonlinear operators, commonly referred
to as DeepONet, has proven capable of approximating PDE backstepping designs in
which a single Goursat-form PDE governs a single feedback gain function. In
boundary control of coupled PDEs, coupled Goursat-form PDEs govern two or more
gain kernels -- a PDE structure unaddressed thus far with DeepONet. In this
note, we open the subject of approximating systems of gain kernel PDEs for
hyperbolic PDE plants by considering a simple counter-convecting $2\times 2$
coupled system in whose control a $2\times 2$ kernel PDE systems in Goursat
form arises. Applications include oil drilling, Saint-Venant model of shallow
water waves, and Aw-Rascle-Zhang model of stop-and-go instability in congested
traffic flow. In this paper we establish the continuity of the mapping from (a
total of five) plant PDE functional coefficients to the kernel PDE solutions,
prove the existence of an arbitrarily close DeepONet approximation to the
kernel PDEs, and establish that the DeepONet-approximated gains guarantee
stabilization when replacing the exact backstepping gain kernels. Taking into
account anti-collocated boundary actuation and sensing, our
$L^2$\emph{-Globally-exponentially} stabilizing (GES) approximate gain
kernel-based output feedback design implies the deep learning of both the
controller's and the observer's gains. Moreover, the encoding of the
output-feedback law into DeepONet ensures \emph{semi-global practical
exponential stability (SG-PES).} The DeepONet operator speeds up the
computation of the controller gains by multiple orders of magnitude. Its
theoretically proven stabilizing capability is demonstrated through
simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01801">A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weitao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuecang Zhang</a></p>
<p>By conceiving physical systems as 3D many-body point clouds, geometric graph
neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased
promising performance. In particular, their effective message-passing mechanics
make them adept at modeling molecules and crystalline materials. However,
current geometric GNNs only offer a mean-field approximation of the many-body
system, encapsulated within two-body message passing, thus falling short in
capturing intricate relationships within these geometric graphs. To address
this limitation, tensor networks, widely employed by computational physics to
handle manybody systems using high-order tensors, have been introduced.
Nevertheless, integrating these tensorized networks into the message-passing
framework of GNNs faces scalability and symmetry conservation (e.g.,
permutation and rotation) challenges. In response, we introduce an innovative
equivariant Matrix Product State (MPS)-based message-passing strategy, through
achieving an efficient implementation of the tensor contraction operation. Our
method effectively models complex many-body relationships, suppressing
mean-field approximations, and captures symmetries within geometric graphs.
Importantly, it seamlessly replaces the standard message-passing and
layer-aggregation modules intrinsic to geometric GNNs. We empirically validate
the superior accuracy of our approach on benchmark tasks, including predicting
classical Newton systems and quantum tensor Hamiltonian matrices. To our
knowledge, our approach represents the inaugural utilization of parameterized
geometric tensor networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03451">Optimization Over Trained Neural Networks: Taking a Relaxing Walk. (arXiv:2401.03451v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Tong_J/0/1/0/all/0/1">Jiatai Tong</a>, <a href="http://arxiv.org/find/math/1/au:+Cai_J/0/1/0/all/0/1">Junyang Cai</a>, <a href="http://arxiv.org/find/math/1/au:+Serra_T/0/1/0/all/0/1">Thiago Serra</a></p>
<p>Besides training, mathematical optimization is also used in deep learning to
model and solve formulations over trained neural networks for purposes such as
verification, compression, and optimization with learned constraints. However,
solving these formulations soon becomes difficult as the network size grows due
to the weak linear relaxation and dense constraint matrix. We have seen
improvements in recent years with cutting plane algorithms, reformulations, and
an heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we
propose a more scalable heuristic based on exploring global and local linear
relaxations of the neural network model. Our heuristic is competitive with a
state-of-the-art MILP solver and the prior heuristic while producing better
solutions with increases in input, depth, and number of neurons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04856">A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sixu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qin Li</a></p>
<p>Score-based Generative Models (SGMs) is one leading method in generative
modeling, renowned for their ability to generate high-quality samples from
complex, high-dimensional data distributions. The method enjoys empirical
success and is supported by rigorous theoretical convergence properties. In
particular, it has been shown that SGMs can generate samples from a
distribution that is close to the ground-truth if the underlying score function
is learned well, suggesting the success of SGM as a generative model. We
provide a counter-example in this paper. Through the sample complexity
argument, we provide one specific setting where the score function is learned
well. Yet, SGMs in this setting can only output samples that are Gaussian
blurring of training data points, mimicking the effects of kernel density
estimation. The finding resonates a series of recent finding that reveal that
SGMs can demonstrate strong memorization effect and fail to generate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05363">Generalizable Sleep Staging via Multi-Level Domain Alignment. (arXiv:2401.05363v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiquan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1">Sha Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1">Haiteng Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shijian Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_G/0/1/0/all/0/1">Gang Pan</a></p>
<p>Automatic sleep staging is essential for sleep assessment and disorder
diagnosis. Most existing methods depend on one specific dataset and are limited
to be generalized to other unseen datasets, for which the training data and
testing data are from the same dataset. In this paper, we introduce domain
generalization into automatic sleep staging and propose the task of
generalizable sleep staging which aims to improve the model generalization
ability to unseen datasets. Inspired by existing domain generalization methods,
we adopt the feature alignment idea and propose a framework called SleepDG to
solve it. Considering both of local salient features and sequential features
are important for sleep staging, we propose a Multi-level Feature Alignment
combining epoch-level and sequence-level feature alignment to learn
domain-invariant feature representations. Specifically, we design an
Epoch-level Feature Alignment to align the feature distribution of each single
sleep epoch among different domains, and a Sequence-level Feature Alignment to
minimize the discrepancy of sequential features among different domains.
SleepDG is validated on five public datasets, achieving the state-of-the-art
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05382">Enhanced Genetic Programming Models with Multiple Equations for Accurate Semi-Autogenous Grinding Mill Throughput Prediction. (arXiv:2401.05382v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghasemi_Z/0/1/0/all/0/1">Zahra Ghasemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nesht_M/0/1/0/all/0/1">Mehdi Nesht</a>, <a href="http://arxiv.org/find/cs/1/au:+Aldrich_C/0/1/0/all/0/1">Chris Aldrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Karageorgos_J/0/1/0/all/0/1">John Karageorgos</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanin_M/0/1/0/all/0/1">Max Zanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1">Frank Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a></p>
<p>Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding
circuit of mineral processing plants. Accurate prediction of SAG mill
throughput as a crucial performance metric is of utmost importance. The
potential of applying genetic programming (GP) for this purpose has yet to be
thoroughly investigated. This study introduces an enhanced GP approach entitled
multi-equation GP (MEGP) for more accurate prediction of SAG mill throughput.
In the new proposed method multiple equations, each accurately predicting mill
throughput for specific clusters of training data are extracted. These
equations are then employed to predict mill throughput for test data using
various approaches. To assess the effect of distance measures, four different
distance measures are employed in MEGP method. Comparative analysis reveals
that the best MEGP approach achieves an average improvement of 10.74% in
prediction accuracy compared with standard GP. In this approach, all extracted
equations are utilized and both the number of data points in each data cluster
and the distance to clusters are incorporated for calculating the final
prediction. Further investigation of distance measures indicates that among
four different metrics employed including Euclidean, Manhattan, Chebyshev, and
Cosine distance, the Euclidean distance measure yields the most accurate
results for the majority of data splits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05578">Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1">Zhenya Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingda Li</a></p>
<p>We introduce a rapid and precise analytical approach for analyzing cerebral
blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the
application of the Extreme Learning Machine (ELM). Our evaluation of ELM and
existing algorithms involves a comprehensive set of metrics. We assess these
algorithms using synthetic datasets for both semi-infinite and multi-layer
models. The results demonstrate that ELM consistently achieves higher fidelity
across various noise levels and optical parameters, showcasing robust
generalization ability and outperforming iterative fitting algorithms. Through
a comparison with a computationally efficient neural network, ELM attains
comparable accuracy with reduced training and inference times. Notably, the
absence of a back-propagation process in ELM during training results in
significantly faster training speeds compared to existing neural network
approaches. This proposed strategy holds promise for edge computing
applications with online training capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05580">Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingda Li</a></p>
<p>Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique
that measures the tissue blood flow, by using near-infrared coherent
point-source illumination to detect spectral changes. While machine learning
has demonstrated significant potential for measuring blood flow index (BFi), an
open question concerning the success of this approach pertains to its
robustness in scenarios involving deviations between datasets with varying
Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications
and various setups. This study proposes a transfer learning approach, aims to
assess the influence of SNRs on the generalization ability of learned features,
and demonstrate the robustness for transfer learning. A synthetic dataset with
varying levels of added noise is utilized to simulate different SNRs. The
proposed network takes a 1x64 autocorrelation curve as input and generates BFi
and the correlation parameter beta. The proposed model demonstrates excellent
performance across different SNRs, exhibiting enhanced fitting accuracy,
particularly for low SNR datasets when compared with other fitting methods.
This highlights its potential for clinical diagnosis and treatment across
various scenarios under different clinical setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08552">Explaining Time Series via Contrastive and Locally Sparse Perturbations. (arXiv:2401.08552v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zichuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianchun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zefan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Dongsheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Mengnan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chunlin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lunting Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a></p>
<p>Explaining multivariate time series is a compound challenge, as it requires
identifying important locations in the time series and matching complex
temporal patterns. Although previous saliency-based methods addressed the
challenges, their perturbation may not alleviate the distribution shift issue,
which is inevitable especially in heterogeneous samples. We present ContraLSP,
a locally sparse model that introduces counterfactual samples to build
uninformative perturbations but keeps distribution using contrastive learning.
Furthermore, we incorporate sample-specific sparse gates to generate more
binary-skewed and smooth masks, which easily integrate temporal trends and
select the salient features parsimoniously. Empirical studies on both synthetic
and real-world datasets show that ContraLSP outperforms state-of-the-art
models, demonstrating a substantial improvement in explanation quality for time
series data. The source code is available at
\url{https://github.com/zichuan-liu/ContraLSP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10282">BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaomin Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sakevych_M/0/1/0/all/0/1">Mykhailo Sakevych</a>, <a href="http://arxiv.org/find/eess/1/au:+Atkinson_G/0/1/0/all/0/1">Gentry Atkinson</a>, <a href="http://arxiv.org/find/eess/1/au:+Metsis_V/0/1/0/all/0/1">Vangelis Metsis</a></p>
<p>Machine learning tasks involving biomedical signals frequently grapple with
issues such as limited data availability, imbalanced datasets, labeling
complexities, and the interference of measurement noise. These challenges often
hinder the optimal training of machine learning algorithms. Addressing these
concerns, we introduce BioDiffusion, a diffusion-based probabilistic model
optimized for the synthesis of multivariate biomedical signals. BioDiffusion
demonstrates excellence in producing high-fidelity, non-stationary,
multivariate signals for a range of tasks including unconditional,
label-conditional, and signal-conditional generation. Leveraging these
synthesized signals offers a notable solution to the aforementioned challenges.
Our research encompasses both qualitative and quantitative assessments of the
synthesized data quality, underscoring its capacity to bolster accuracy in
machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed
with current leading time-series generative models, empirical evidence suggests
that BioDiffusion outperforms them in biomedical signal generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10800">Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm. (arXiv:2401.10800v2 [physics.ao-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Jacques_Dumas_V/0/1/0/all/0/1">Val&#xe9;rian Jacques-Dumas</a>, <a href="http://arxiv.org/find/physics/1/au:+Westen_R/0/1/0/all/0/1">Ren&#xe9; M. van Westen</a>, <a href="http://arxiv.org/find/physics/1/au:+Dijkstra_H/0/1/0/all/0/1">Henk A. Dijkstra</a></p>
<p>The Atlantic Meridional Overturning Circulation (AMOC) is an important
component of the global climate, known to be a tipping element, as it could
collapse under global warming. The main objective of this study is to compute
the probability that the AMOC collapses within a specified time window, using a
rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS).
However, the efficiency and accuracy of TAMS depend on the choice of the score
function. Although the definition of the optimal score function, called
``committor function" is known, it is impossible in general to compute it a
priori. Here, we combine TAMS with a Next-Generation Reservoir Computing
technique that estimates the committor function from the data generated by the
rare-event algorithm. We test this technique in a stochastic box model of the
AMOC for which two types of transition exist, the so-called F(ast)-transitions
and S(low)-transitions. Results for the F-transtions compare favorably with
those in the literature where a physically-informed score function was used. We
show that coupling a rare-event algorithm with machine learning allows for a
correct estimation of transition probabilities, transition times, and even
transition paths for a wide range of model parameters. We then extend these
results to the more difficult problem of S-transitions in the same model. In
both cases of F- and S-transitions, we also show how the Next-Generation
Reservoir Computing technique can be interpreted to retrieve an analytical
estimate of the committor function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11113">SleepNet: Attention-Enhanced Robust Sleep Prediction using Dynamic Social Networks. (arXiv:2401.11113v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalid_M/0/1/0/all/0/1">Maryam Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Klerman_E/0/1/0/all/0/1">Elizabeth B. Klerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mchill_A/0/1/0/all/0/1">Andrew W. Mchill</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillips_A/0/1/0/all/0/1">Andrew J. K. Phillips</a>, <a href="http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1">Akane Sano</a></p>
<p>Sleep behavior significantly impacts health and acts as an indicator of
physical and mental well-being. Monitoring and predicting sleep behavior with
ubiquitous sensors may therefore assist in both sleep management and tracking
of related health conditions. While sleep behavior depends on, and is reflected
in the physiology of a person, it is also impacted by external factors such as
digital media usage, social network contagion, and the surrounding weather. In
this work, we propose SleepNet, a system that exploits social contagion in
sleep behavior through graph networks and integrates it with physiological and
phone data extracted from ubiquitous mobile and wearable devices for predicting
next-day sleep labels about sleep duration. Our architecture overcomes the
limitations of large-scale graphs containing connections irrelevant to sleep
behavior by devising an attention mechanism. The extensive experimental
evaluation highlights the improvement provided by incorporating social networks
in the model. Additionally, we conduct robustness analysis to demonstrate the
system's performance in real-life conditions. The outcomes affirm the stability
of SleepNet against perturbations in input data. Further analyses emphasize the
significance of network topology in prediction performance revealing that users
with higher eigenvalue centrality are more vulnerable to data perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11648">Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1">Heejoon Koo</a></p>
<p>Predicting next visit diagnosis using Electronic Health Records (EHR) is an
essential task in healthcare, critical for devising proactive future plans for
both healthcare providers and patients. Nonetheless, many preceding studies
have not sufficiently addressed the heterogeneous and hierarchical
characteristics inherent in EHR data, inevitably leading to sub-optimal
performance. To this end, we propose NECHO, a novel medical code-centric
multimodal contrastive EHR learning framework with hierarchical regularisation.
First, we integrate multifaceted information encompassing medical codes,
demographics, and clinical notes using a tailored network design and a pair of
bimodal contrastive losses, all of which pivot around a medical codes
representation. We also regularise modality-specific encoders using a parental
level information in medical ontology to learn hierarchical structure of EHR
data. A series of experiments on MIMIC-III data demonstrates effectiveness of
our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11792">Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zuojin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">YongQiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianyu Chen</a></p>
<p>An intelligent driving system should be capable of dynamically formulating
appropriate driving strategies based on the current environment and vehicle
status, while ensuring the security and reliability of the system. However,
existing methods based on reinforcement learning and imitation learning suffer
from low safety, poor generalization, and inefficient sampling. Additionally,
they cannot accurately predict future driving trajectories, and the accurate
prediction of future driving trajectories is a precondition for making optimal
decisions. To solve these problems, in this paper, we introduce a Safe and
Generalized end-to-end Autonomous Driving System (SGADS) for complex and
various scenarios. Our SGADS incorporates variational inference with
normalizing flows, enabling the intelligent vehicle to accurately predict
future driving trajectories. Moreover, we propose the formulation of robust
safety constraints. Furthermore, we combine reinforcement learning with
demonstrations to augment search process of the agent. The experimental results
demonstrate that our SGADS can significantly improve safety performance,
exhibit strong generalization, and enhance the training efficiency of
intelligent vehicles in complex urban scenarios compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11798">Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1">Mohammad Izadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Safayani_M/0/1/0/all/0/1">Mehran Safayani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1">Abdolreza Mirzaei</a></p>
<p>Efficient real-time traffic prediction is crucial for reducing transportation
time. To predict traffic conditions, we employ a spatio-temporal graph neural
network (ST-GNN) to model our real-time traffic data as temporal graphs.
Despite its capabilities, it often encounters challenges in delivering
efficient real-time predictions for real-world traffic data. Recognizing the
significance of timely prediction due to the dynamic nature of real-time data,
we employ knowledge distillation (KD) as a solution to enhance the execution
time of ST-GNNs for traffic prediction. In this paper, We introduce a cost
function designed to train a network with fewer parameters (the student) using
distilled data from a complex network (the teacher) while maintaining its
accuracy close to that of the teacher. We use knowledge distillation,
incorporating spatial-temporal correlations from the teacher network to enable
the student to learn the complex patterns perceived by the teacher. However, a
challenge arises in determining the student network architecture rather than
considering it inadvertently. To address this challenge, we propose an
algorithm that utilizes the cost function to calculate pruning scores,
addressing small network architecture search issues, and jointly fine-tunes the
network resulting from each pruning stage using KD. Ultimately, we evaluate our
proposed ideas on two real-world datasets, PeMSD7 and PeMSD8. The results
indicate that our method can maintain the student's accuracy close to that of
the teacher, even with the retention of only $3\%$ of network parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12007">Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1">Tao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Elynn Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuzhou Chen</a></p>
<p>Graph classification is an important learning task for graph-structured data.
Graph neural networks (GNNs) have recently gained growing attention in graph
learning and have shown significant improvements in many important graph
problems. Despite their state-of-the-art performances, existing GNNs only use
local information from a very limited neighborhood around each node, suffering
from loss of multi-modal information and overheads of excessive computation. To
address these issues, we propose a novel Tensor-view Topological Graph Neural
Network (TTG-NN), a class of simple yet effective topological deep learning
built upon persistent homology, graph convolution, and tensor operations. This
new method incorporates tensor learning to simultaneously capture Tensor-view
Topological (TT), as well as Tensor-view Graph (TG) structural information on
both local and global levels. Computationally, to fully exploit graph topology
and structure, we propose two flexible TT and TG representation learning
modules that disentangle feature tensor aggregation and transformation and
learn to preserve multi-modal structure with less computation. Theoretically,
we derive high probability bounds on both the out-of-sample and in-sample mean
squared approximation errors for our proposed Tensor Transformation Layer
(TTL). Real data experiments show that the proposed TTG-NN outperforms 20
state-of-the-art methods on various graph benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12012">TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengdi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bodonhelyi_A/0/1/0/all/0/1">Anna Bodonhelyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1">Efe Bozkir</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1">Enkelejda Kasneci</a></p>
<p>Federated learning is a distributed collaborative machine learning paradigm
that has gained strong momentum in recent years. In federated learning, a
central server periodically coordinates models with clients and aggregates the
models trained locally by clients without necessitating access to local data.
Despite its potential, the implementation of federated learning continues to
encounter several challenges, predominantly the slow convergence that is
largely due to data heterogeneity. The slow convergence becomes particularly
problematic in cross-device federated learning scenarios where clients may be
strongly limited by computing power and storage space, and hence counteracting
methods that induce additional computation or memory cost on the client side
such as auxiliary objective terms and larger training iterations can be
impractical. In this paper, we propose a novel federated aggregation strategy,
TurboSVM-FL, that poses no additional computation burden on the client side and
can significantly accelerate convergence for federated classification task,
especially when clients are "lazy" and train their models solely for few epochs
for next global aggregation. TurboSVM-FL extensively utilizes support vector
machine to conduct selective aggregation and max-margin spread-out
regularization on class embeddings. We evaluate TurboSVM-FL on multiple
datasets including FEMNIST, CelebA, and Shakespeare using user-independent
validation with non-iid data distribution. Our results show that TurboSVM-FL
can significantly outperform existing popular algorithms on convergence rate
and reduce communication rounds while delivering better test metrics including
accuracy, F1 score, and MCC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12131">NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis. (arXiv:2401.12131v2 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cosler_M/0/1/0/all/0/1">Matthias Cosler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1">Christopher Hahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Omar_A/0/1/0/all/0/1">Ayham Omar</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmitt_F/0/1/0/all/0/1">Frederik Schmitt</a></p>
<p>We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for
reactive synthesis. At the core of the solver lies a seamless integration of
neural and symbolic approaches to solving the reactive synthesis problem. To
ensure soundness, the neural engine is coupled with model checkers verifying
the predictions of the underlying neural models. The open-source implementation
of NeuroSynt provides an integration framework for reactive synthesis in which
new neural and state-of-the-art symbolic approaches can be seamlessly
integrated. Extensive experiments demonstrate its efficacy in handling
challenging specifications, enhancing the state-of-the-art reactive synthesis
solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12729">Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1">Jibinraj Antony</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegiste_V/0/1/0/all/0/1">Vinit Hegiste</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazeri_A/0/1/0/all/0/1">Ali Nazeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Tavakoli_H/0/1/0/all/0/1">Hooman Tavakoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Walunj_S/0/1/0/all/0/1">Snehal Walunj</a>, <a href="http://arxiv.org/find/cs/1/au:+Plociennik_C/0/1/0/all/0/1">Christiane Plociennik</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruskowski_M/0/1/0/all/0/1">Martin Ruskowski</a></p>
<p>Object Detection (OD) has proven to be a significant computer vision method
in extracting localized class information and has multiple applications in the
industry. Although many of the state-of-the-art (SOTA) OD models perform well
on medium and large sized objects, they seem to under perform on small objects.
In most of the industrial use cases, it is difficult to collect and annotate
data for small objects, as it is time-consuming and prone to human errors.
Additionally, those datasets are likely to be unbalanced and often result in an
inefficient model convergence. To tackle this challenge, this study presents a
novel approach that injects additional data points to improve the performance
of the OD models. Using synthetic data generation, the difficulties in data
collection and annotations for small object data points can be minimized and to
create a dataset with balanced distribution. This paper discusses the effects
of a simple proportional class-balancing technique, to enable better anchor
matching of the OD models. A comparison was carried out on the performances of
the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and
synthetic datasets within an industrial use case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12923">Deep multitask neural networks for solving some stochastic optimal control problems. (arXiv:2401.12923v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Yeo_C/0/1/0/all/0/1">Christian Yeo</a></p>
<p>Most existing neural network-based approaches for solving stochastic optimal
control problems using the associated backward dynamic programming principle
rely on the ability to simulate the underlying state variables. However, in
some problems, this simulation is infeasible, leading to the discretization of
state variable space and the need to train one neural network for each data
point. This approach becomes computationally inefficient when dealing with
large state variable spaces. In this paper, we consider a class of this type of
stochastic optimal control problems and introduce an effective solution
employing multitask neural networks. To train our multitask neural network, we
introduce a novel scheme that dynamically balances the learning across tasks.
Through numerical experiments on real-world derivatives pricing problems, we
prove that our method outperforms state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13034">Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zichen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Chao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wee Sun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a></p>
<p>Acquiring an accurate world model online for model-based reinforcement
learning (MBRL) is challenging due to data nonstationarity, which typically
causes catastrophic forgetting for neural networks (NNs). From the online
learning perspective, a Follow-The-Leader (FTL) world model is desirable, which
optimally fits all previous experiences at each round. Unfortunately, NN-based
models need re-training on all accumulated data at every interaction step to
achieve FTL, which is computationally expensive for lifelong agents. In this
paper, we revisit models that can achieve FTL with incremental updates.
Specifically, our world model is a linear regression model supported by
nonlinear random features. The linear part ensures efficient FTL update while
the nonlinear random feature empowers the fitting of complex environments. To
best trade off model capacity and computation efficiency, we introduce a
locality sensitive sparse encoding, which allows us to conduct efficient sparse
updates even with very high dimensional nonlinear features. We validate the
representation power of our encoding and verify that it allows efficient online
learning under data covariate shift. We also show, in the Dyna MBRL setting,
that our world models learned online using a single pass of trajectory data
either surpass or match the performance of deep world models trained with
replay and other continual learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13098">Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ruixin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Spadon_G/0/1/0/all/0/1">Gabriel Spadon</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelot_R/0/1/0/all/0/1">Ronald Pelot</a>, <a href="http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1">Stan Matwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1">Amilcar Soares</a></p>
<p>Invasive species in water bodies pose a major threat to the environment and
biodiversity globally. Due to increased transportation and trade, non-native
species have been introduced to new environments, causing damage to ecosystems
and leading to economic losses in agriculture, forestry, and fisheries.
Therefore, there is a pressing need for risk assessment and management
techniques to mitigate the impact of these invasions. This study aims to
develop a new physics-inspired model to forecast maritime shipping traffic and
thus inform risk assessment of invasive species spread through global
transportation networks. Inspired by the gravity model for international
trades, our model considers various factors that influence the likelihood and
impact of vessel activities, such as shipping flux density, distance between
ports, trade flow, and centrality measures of transportation hubs.
Additionally, by analyzing the risk network of invasive species, we provide a
comprehensive framework for assessing the invasion threat level given a pair of
origin and destination. Accordingly, this paper introduces transformers to
gravity models to rebuild the short- and long-term dependencies that make the
risk analysis feasible. Thus, we introduce a physics-inspired framework that
achieves an 89% segmentation accuracy for existing and non-existing
trajectories and an 84.8% accuracy for the number of vessels flowing between
key port areas, representing more than 10% improvement over the traditional
deep-gravity model. Along these lines, this research contributes to a better
understanding of invasive species risk assessment. It allows policymakers,
conservationists, and stakeholders to prioritize management actions by
identifying high-risk invasion pathways. Besides, our model is versatile and
can include new data sources, making it suitable for assessing species invasion
risks in a changing global landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13231">DittoGym: Learning to Control Soft Shape-Shifting Robots. (arXiv:2401.13231v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Suning Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Boyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1">Vincent Sitzmann</a></p>
<p>Robot co-design, where the morphology of a robot is optimized jointly with a
learned policy to solve a specific task, is an emerging area of research. It
holds particular promise for soft robots, which are amenable to novel
manufacturing techniques that can realize learned morphologies and actuators.
Inspired by nature and recent novel robot designs, we propose to go a step
further and explore the novel reconfigurable robots, defined as robots that can
change their morphology within their lifetime. We formalize control of
reconfigurable soft robots as a high-dimensional reinforcement learning (RL)
problem. We unify morphology change, locomotion, and environment interaction in
the same action space, and introduce an appropriate, coarse-to-fine curriculum
that enables us to discover policies that accomplish fine-grained control of
the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark
for reconfigurable soft robots that require fine-grained morphology changes to
accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine
algorithm on DittoGym and demonstrate robots that learn to change their
morphology several times within a sequence, uniquely enabled by our RL
algorithm. More results are available at https://dittogym.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13429">Detection of Correlated Random Vectors. (arXiv:2401.13429v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elimelech_D/0/1/0/all/0/1">Dor Elimelech</a>, <a href="http://arxiv.org/find/cs/1/au:+Huleihel_W/0/1/0/all/0/1">Wasim Huleihel</a></p>
<p>In this paper, we investigate the problem of deciding whether two standard
normal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and
$\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as a
hypothesis testing problem, where under the null hypothesis, these vectors are
statistically independent, while under the alternative, $\mathsf{X}$ and a
randomly and uniformly permuted version of $\mathsf{Y}$, are correlated with
correlation $\rho$. We analyze the thresholds at which optimal testing is
information-theoretically impossible and possible, as a function of $n$ and
$\rho$. To derive our information-theoretic lower bounds, we develop a novel
technique for evaluating the second moment of the likelihood ratio using an
orthogonal polynomials expansion, which among other things, reveals a
surprising connection to integer partition functions. We also study a
multi-dimensional generalization of the above setting, where rather than two
vectors we observe two databases/matrices, and furthermore allow for partial
correlations between these two.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13802">Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khajezade_M/0/1/0/all/0/1">Mohamad Khajezade</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie JW Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1">Fatemeh Hendijani Fard</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_G/0/1/0/all/0/1">Gema Rodr&#xed;guez-P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1">Mohamed Sami Shehata</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. GPT-based models are one
of the popular ones studied for tasks such as code comment generation or test
generation. These tasks are `generative' tasks. However, there is limited
research on the usage of LLMs for `non-generative' tasks such as classification
using the prompt-based paradigm. In this preliminary exploratory study, we
investigated the applicability of LLMs for Code Clone Detection (CCD), a
non-generative task. By building a mono-lingual and cross-lingual CCD dataset
derived from CodeNet, we first investigated two different prompts using ChatGPT
to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot
setting. We then conducted an analysis to understand the strengths and
weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language
CCD attaining an F1-score of 0.877 and achieves comparable performance to fully
fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the
prompt and the difficulty level of the problems has an impact on the
performance of ChatGPT. Finally we provide insights and future directions based
on our initial analysis
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13851">Scaling NVIDIA&#x27;s Multi-speaker Multi-lingual TTS Systems with Zero-Shot TTS to Indic Languages. (arXiv:2401.13851v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Akshit Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Badlani_R/0/1/0/all/0/1">Rohan Badlani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungwon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Valle_R/0/1/0/all/0/1">Rafael Valle</a>, <a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1">Bryan Catanzaro</a></p>
<p>In this paper, we describe the TTS models developed by NVIDIA for the
MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024
Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by
training additionally on 5 minutes of target speaker data. In Track 3, we
utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as
well as external datasets. We use HiFi-GAN vocoders for all submissions.
RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on
Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS)
of 3.62.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13947">Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading. (arXiv:2401.13947v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1">Chen Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1">Andrew L. Liu</a></p>
<p>Utilizing distributed renewable and energy storage resources in local
distribution networks via peer-to-peer (P2P) energy trading has long been
touted as a solution to improve energy systems' resilience and sustainability.
Consumers and prosumers (those who have energy generation resources), however,
do not have the expertise to engage in repeated P2P trading, and the
zero-marginal costs of renewables present challenges in determining fair market
prices. To address these issues, we propose multi-agent reinforcement learning
(MARL) frameworks to help automate consumers' bidding and management of their
solar PV and energy storage resources, under a specific P2P clearing mechanism
that utilizes the so-called supply-demand ratio. In addition, we show how the
MARL frameworks can integrate physical network constraints to realize voltage
control, hence ensuring physical feasibility of the P2P energy trading and
paving way for real-world implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14211">Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1">Vasileios Tsouvalas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1">Aaqib Saeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozcelebi_T/0/1/0/all/0/1">Tanir Ozcelebi</a>, <a href="http://arxiv.org/find/cs/1/au:+Meratnia_N/0/1/0/all/0/1">Nirvana Meratnia</a></p>
<p>Federated Learning (FL) is a promising technique for the collaborative
training of deep neural networks across multiple devices while preserving data
privacy. Despite its potential benefits, FL is hindered by excessive
communication costs due to repeated server-client communication during
training. To address this challenge, model compression techniques, such as
sparsification and weight clustering are applied, which often require modifying
the underlying model aggregation schemes or involve cumbersome hyperparameter
tuning, with the latter not only adjusts the model's compression rate but also
limits model's potential for continuous improvement over growing data. In this
paper, we propose FedCompress, a novel approach that combines dynamic weight
clustering and server-side knowledge distillation to reduce communication costs
while learning highly generalizable models. Through a comprehensive evaluation
on diverse public datasets, we demonstrate the efficacy of our approach
compared to baselines in terms of communication costs and inference speed. We
will make our implementation public upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14340">Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sevilla_M/0/1/0/all/0/1">Mart&#xed;n Sevilla</a>, <a href="http://arxiv.org/find/stat/1/au:+Marques_A/0/1/0/all/0/1">Antonio Garc&#xed;a Marques</a>, <a href="http://arxiv.org/find/stat/1/au:+Segarra_S/0/1/0/all/0/1">Santiago Segarra</a></p>
<p>We propose a novel algorithm for the support estimation of partially known
Gaussian graphical models that incorporates prior information about the
underlying graph. In contrast to classical approaches that provide a point
estimate based on a maximum likelihood or a maximum a posteriori criterion
using (simple) priors on the precision matrix, we consider a prior on the graph
and rely on annealed Langevin diffusion to generate samples from the posterior
distribution. Since the Langevin sampler requires access to the score function
of the underlying graph prior, we use graph neural networks to effectively
estimate the score from a graph dataset (either available beforehand or
generated from a known distribution). Numerical experiments demonstrate the
benefits of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14403">Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Haoyu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendonca_R/0/1/0/all/0/1">Russell Mendonca</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1">Kenneth Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a></p>
<p>Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14424">Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lina Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1">Meilan Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Shu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yusong Deng</a></p>
<p>Finding a concise and interpretable mathematical formula that accurately
describes the relationship between each variable and the predicted value in the
data is a crucial task in scientific research, as well as a significant
challenge in artificial intelligence. This problem is referred to as symbolic
regression, which is an NP-hard problem. Last year, a symbolic regression
method based on Monte Carlo Tree Search (MCTS) was proposed and sota was
obtained on multiple datasets. While this algorithm has shown considerable
improvement in recovering target expressions compared to previous methods, the
lack of guidance during the MCTS process severely hampers its search
efficiency. Recently, some algorithms have added a pre-trained policy network
to guide the search of MCTS, but the pre-trained policy network generalizes
poorly. To balance efficiency and generality, we propose SR-GPT combining ideas
from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines
MCTS with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the
MCTS process, the search efficiency of MCTS is significantly improved. Next, we
utilize the MCTS results to further refine the GPT, enhancing its capabilities
and providing more accurate guidance for the MCTS process. MCTS and GPT are
coupled together and optimize each other until the target expression is
successfully determined. We conducted extensive evaluations of SR-GPT using 222
expressions sourced from over 10 different symbolic regression datasets. The
experimental results demonstrate that SR-GPT outperforms existing
state-of-the-art algorithms in accurately recovering symbolic expressions both
with and without added noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14521">Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuan-Heng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Hoshin V. Gupta</a></p>
<p>We investigate the applicability of machine learning technologies to the
development of parsimonious, interpretable, catchment-scale hydrologic models
using directed-graph architectures based on the mass-conserving perceptron
(MCP) as the fundamental computational unit. Here, we focus on architectural
complexity (depth) at a single location, rather than universal applicability
(breadth) across large samples of catchments. The goal is to discover a minimal
representation (numbers of cell-states and flow paths) that represents the
dominant processes that can explain the input-state-output behaviors of a given
catchment, with particular emphasis given to simulating the full range (high,
medium, and low) of flow dynamics. We find that a HyMod-like architecture with
three cell-states and two major flow pathways achieves such a representation at
our study location, but that the additional incorporation of an input-bypass
mechanism significantly improves the timing and shape of the hydrograph, while
the inclusion of bi-directional groundwater mass exchanges significantly
enhances the simulation of baseflow. Overall, our results demonstrate the
importance of using multiple diagnostic metrics for model evaluation, while
highlighting the need for designing training metrics that are better suited to
extracting information across the full range of flow dynamics. Further, they
set the stage for interpretable regional-scale MCP-based hydrological modeling
(using large sample data) by using neural architecture search to determine
appropriate minimal representations for catchments in different hydroclimatic
regimes.
</p>
</p>
</div>

    </div>
    </body>
    