<!DOCTYPE html>
<html>
<head>
<title>2023-08-09-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.03762">GPT-4 Can&#x27;t Reason. (arXiv:2308.03762v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1">Konstantine Arkoudas</a></p>
<p>GPT-4 was released in March 2023 to wide acclaim, marking a very substantial
improvement across the board over GPT-3.5 (OpenAI's previously best model,
which had powered the initial release of ChatGPT). However, despite the
genuinely impressive improvement, there are good reasons to be highly skeptical
of GPT-4's ability to reason. This position paper discusses the nature of
reasoning; criticizes the current formulation of reasoning problems in the NLP
community, as well as the way in which LLM reasoning performance is currently
evaluated; introduces a small collection of 21 diverse reasoning problems; and
performs a detailed qualitative evaluation of GPT-4's performance on those
problems. Based on this analysis, the paper concludes that, despite its
occasional flashes of analytical brilliance, GPT-4 at present is utterly
incapable of reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03767">Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1">Aya Mahmoud Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousef_M/0/1/0/all/0/1">Mohamed Yousef</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_K/0/1/0/all/0/1">Khaled F. Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdy_Y/0/1/0/all/0/1">Yousef Bassyouni Mahdy</a></p>
<p>Captioning images is a challenging scene-understanding task that connects
computer vision and natural language processing. While image captioning models
have been successful in producing excellent descriptions, the field has
primarily focused on generating a single sentence for 2D images. This paper
investigates whether integrating depth information with RGB images can enhance
the captioning task and generate better descriptions. For this purpose, we
propose a Transformer-based encoder-decoder framework for generating a
multi-sentence description of a 3D scene. The RGB image and its corresponding
depth map are provided as inputs to our framework, which combines them to
produce a better understanding of the input scene. Depth maps could be ground
truth or estimated, which makes our framework widely applicable to any RGB
captioning dataset. We explored different fusion approaches to fuse RGB and
depth images. The experiments are performed on the NYU-v2 dataset and the
Stanford image paragraph captioning dataset. During our work with the NYU-v2
dataset, we found inconsistent labeling that prevents the benefit of using
depth information to enhance the captioning task. The results were even worse
than using RGB images only. As a result, we propose a cleaned version of the
NYU-v2 dataset that is more consistent and informative. Our results on both
datasets demonstrate that the proposed framework effectively benefits from
depth information, whether it is ground truth or estimated, and generates
better captions. Code, pre-trained models, and the cleaned version of the
NYU-v2 dataset will be made publically available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03782">Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1">Yue Ling</a></p>
<p>The objective of this study is to develop natural language processing (NLP)
models that can analyze patients' drug reviews and accurately classify their
satisfaction levels as positive, neutral, or negative. Such models would reduce
the workload of healthcare professionals and provide greater insight into
patients' quality of life, which is a critical indicator of treatment
effectiveness. To achieve this, we implemented and evaluated several
classification models, including a BERT base model, Bio+Clinical BERT, and a
simpler CNN. Results indicate that the medical domain-specific Bio+Clinical
BERT model significantly outperformed the general domain base BERT model,
achieving macro f1 and recall score improvement of 11%, as shown in Table 2.
Future research could explore how to capitalize on the specific strengths of
each model. Bio+Clinical BERT excels in overall performance, particularly with
medical jargon, while the simpler CNN demonstrates the ability to identify
crucial words and accurately classify sentiment in texts with conflicting
sentiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03795">Forget Demonstrations, Focus on Learning from Textual Instructions. (arXiv:2308.03795v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1">Renze Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1">Wenpeng Yin</a></p>
<p>This work studies a challenging yet more realistic setting for zero-shot
cross-task generalization: demonstration-free learning from textual
instructions, presuming the existence of a paragraph-style task definition
while no demonstrations exist. To better learn the task supervision from the
definition, we propose two strategies: first, to automatically find out the
critical sentences in the definition; second, a ranking objective to force the
model to generate the gold outputs with higher probabilities when those
critical parts are highlighted in the definition. The joint efforts of the two
strategies yield state-of-the-art performance on the challenging benchmark. Our
code will be released in the final version of the paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03800">Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiuru Li</a></p>
<p>In this report, I present a deep learning approach to conduct a natural
language processing (hereafter NLP) binary classification task for analyzing
financial-fraud texts. First, I searched for regulatory announcements and
enforcement bulletins from HKEX news to define fraudulent companies and to
extract their MD&amp;A reports before I organized the sentences from the reports
with labels and reporting time. My methodology involved different kinds of
neural network models, including Multilayer Perceptrons with Embedding layers,
vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and
Gated Recurrent Unit (GRU) for the text classification task. By utilizing this
diverse set of models, I aim to perform a comprehensive comparison of their
accuracy in detecting financial fraud. My results bring significant
implications for financial fraud detection as this work contributes to the
growing body of research at the intersection of deep learning, NLP, and
finance, providing valuable insights for industry practitioners, regulators,
and researchers in the pursuit of more robust and effective fraud detection
methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03853">Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. (arXiv:2308.03853v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1">Madhumita Sushil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_V/0/1/0/all/0/1">Vanessa E. Kennedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1">Brenda Y. Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandair_D/0/1/0/all/0/1">Divneet Mandair</a>, <a href="http://arxiv.org/find/cs/1/au:+Zack_T/0/1/0/all/0/1">Travis Zack</a>, <a href="http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1">Atul J. Butte</a></p>
<p>Both medical care and observational studies in oncology require a thorough
understanding of a patient's disease progression and treatment history, often
elaborately documented in clinical notes. Despite their vital role, no current
oncology information representation and annotation schema fully encapsulates
the diversity of information recorded within these notes. Although large
language models (LLMs) have recently exhibited impressive performance on
various medical natural language processing tasks, due to the current lack of
comprehensively annotated oncology datasets, an extensive evaluation of LLMs in
extracting and reasoning with the complex rhetoric in oncology notes remains
understudied. We developed a detailed schema for annotating textual oncology
information, encompassing patient characteristics, tumor characteristics,
tests, treatments, and temporality. Using a corpus of 10 de-identified breast
cancer progress notes at University of California, San Francisco, we applied
this schema to assess the abilities of three recently-released LLMs (GPT-4,
GPT-3.5-turbo, and FLAN-UL2) to perform zero-shot extraction of detailed
oncological history from two narrative sections of clinical progress notes. Our
team annotated 2750 entities, 2874 modifiers, and 1623 relationships. The GPT-4
model exhibited overall best performance, with an average BLEU score of 0.69,
an average ROUGE score of 0.72, and an average accuracy of 67% on complex tasks
(expert manual evaluation). Notably, it was proficient in tumor characteristic
and medication extraction, and demonstrated superior performance in inferring
symptoms due to cancer and considerations of future medications. The analysis
demonstrates that GPT-4 is potentially already usable to extract important
facts from cancer progress notes needed for clinical research, complex
population management, and documenting quality patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03864">Storyfier: Exploring Vocabulary Learning Support with Text Generation Models. (arXiv:2308.03864v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhenhui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1">Qiushi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Junkai Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1">Huamin Qu</a></p>
<p>Vocabulary learning support tools have widely exploited existing materials,
e.g., stories or video clips, as contexts to help users memorize each target
word. However, these tools could not provide a coherent context for any target
words of learners' interests, and they seldom help practice word usage. In this
paper, we work with teachers and students to iteratively develop Storyfier,
which leverages text generation models to enable learners to read a generated
story that covers any target words, conduct a story cloze test, and use these
words to write a new story with adaptive AI assistance. Our within-subjects
study (N=28) shows that learners generally favor the generated stories for
connecting target words and writing assistance for easing their learning
workload. However, in the read-cloze-write learning sessions, participants
using Storyfier perform worse in recalling and using target words than learning
with a baseline tool without our AI features. We discuss insights into
supporting learning tasks with generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03866">Trusting Language Models in Education. (arXiv:2308.03866v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1">Jogi Suda Neto</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Li Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Raya_T/0/1/0/all/0/1">Thejaswi Raya</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahbazi_R/0/1/0/all/0/1">Reza Shahbazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nick Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1">Adhitya Venkatesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Miral Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosla_N/0/1/0/all/0/1">Neeru Khosla</a>, <a href="http://arxiv.org/find/cs/1/au:+Guido_R/0/1/0/all/0/1">Rodrigo Capobianco Guido</a></p>
<p>Language Models are being widely used in Education. Even though modern deep
learning models achieve very good performance on question-answering tasks,
sometimes they make errors. To avoid misleading students by showing wrong
answers, it is important to calibrate the confidence - that is, the prediction
probability - of these models. In our work, we propose to use an XGBoost on top
of BERT to output the corrected probabilities, using features based on the
attention mechanism. Our hypothesis is that the level of uncertainty contained
in the flow of attention is related to the quality of the model's response
itself.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03869">Semantic Equivalence of e-Commerce Queries. (arXiv:2308.03869v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1">Aritra Mandal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tunkelang_D/0/1/0/all/0/1">Daniel Tunkelang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhe Wu</a></p>
<p>Search query variation poses a challenge in e-commerce search, as equivalent
search intents can be expressed through different queries with surface-level
differences. This paper introduces a framework to recognize and leverage query
equivalence to enhance searcher and business outcomes. The proposed approach
addresses three key problems: mapping queries to vector representations of
search intent, identifying nearest neighbor queries expressing equivalent or
similar intent, and optimizing for user or business objectives. The framework
utilizes both surface similarity and behavioral similarity to determine query
equivalence. Surface similarity involves canonicalizing queries based on word
inflection, word order, compounding, and noise words. Behavioral similarity
leverages historical search behavior to generate vector representations of
query intent. An offline process is used to train a sentence similarity model,
while an online nearest neighbor approach supports processing of unseen
queries. Experimental evaluations demonstrate the effectiveness of the proposed
approach, outperforming popular sentence transformer models and achieving a
Pearson correlation of 0.85 for query similarity. The results highlight the
potential of leveraging historical behavior data and training models to
recognize and utilize query equivalence in e-commerce search, leading to
improved user experiences and business outcomes. Further advancements and
benchmark datasets are encouraged to facilitate the development of solutions
for this critical problem in the e-commerce domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03883">Generative Benchmark Creation for Table Union Search. (arXiv:2308.03883v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1">Koyena Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khatiwada_A/0/1/0/all/0/1">Aamod Khatiwada</a>, <a href="http://arxiv.org/find/cs/1/au:+Shraga_R/0/1/0/all/0/1">Roee Shraga</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_R/0/1/0/all/0/1">Ren&#xe9;e J. Miller</a></p>
<p>Data management has traditionally relied on synthetic data generators to
generate structured benchmarks, like the TPC suite, where we can control
important parameters like data size and its distribution precisely. These
benchmarks were central to the success and adoption of database management
systems. But more and more, data management problems are of a semantic nature.
An important example is finding tables that can be unioned. While any two
tables with the same cardinality can be unioned, table union search is the
problem of finding tables whose union is semantically coherent. Semantic
problems cannot be benchmarked using synthetic data. Our current methods for
creating benchmarks involve the manual curation and labeling of real data.
These methods are not robust or scalable and perhaps more importantly, it is
not clear how robust the created benchmarks are. We propose to use generative
AI models to create structured data benchmarks for table union search. We
present a novel method for using generative models to create tables with
specified properties. Using this method, we create a new benchmark containing
pairs of tables that are both unionable and non-unionable but related. We
thoroughly evaluate recent existing table union search methods over existing
benchmarks and our new benchmark. We also present and evaluate a new table
search methods based on recent large language models over all benchmarks. We
show that the new benchmark is more challenging for all methods than
hand-curated benchmarks, specifically, the top-performing method achieves a
Mean Average Precision of around 60%, over 30% less than its performance on
existing manually created benchmarks. We examine why this is the case and show
that the new benchmark permits more detailed analysis of methods, including a
study of both false positives and false negatives that were not possible with
existing benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03891">A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. (arXiv:2308.03891v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1">Anik Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanzadeh_O/0/1/0/all/0/1">Oktie Hassanzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1">Alex Gittens</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1">Jian Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivas_K/0/1/0/all/0/1">Kavitha Srinivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Yener_B/0/1/0/all/0/1">Bulent Yener</a></p>
<p>Causal knowledge extraction is the task of extracting relevant causes and
effects from text by detecting the causal relation. Although this task is
important for language understanding and knowledge discovery, recent works in
this domain have largely focused on binary classification of a text segment as
causal or non-causal. In this regard, we perform a thorough analysis of three
sequence tagging models for causal knowledge extraction and compare it with a
span based approach to causality extraction. Our experiments show that
embeddings from pre-trained language models (e.g. BERT) provide a significant
performance boost on this task compared to previous state-of-the-art models
with complex architectures. We observe that span based models perform better
than simple sequence tagging models based on BERT across all 4 data sets from
diverse domains with different types of cause-effect phrases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03905">Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aas_C/0/1/0/all/0/1">Cecilia Aas</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelsalam_H/0/1/0/all/0/1">Hisham Abdelsalam</a>, <a href="http://arxiv.org/find/cs/1/au:+Belousova_I/0/1/0/all/0/1">Irina Belousova</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1">Shruti Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jianpeng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Daland_R/0/1/0/all/0/1">Robert Daland</a>, <a href="http://arxiv.org/find/cs/1/au:+Driesen_J/0/1/0/all/0/1">Joris Driesen</a>, <a href="http://arxiv.org/find/cs/1/au:+Flego_F/0/1/0/all/0/1">Federico Flego</a>, <a href="http://arxiv.org/find/cs/1/au:+Guigue_T/0/1/0/all/0/1">Tristan Guigue</a>, <a href="http://arxiv.org/find/cs/1/au:+Johannsen_A/0/1/0/all/0/1">Anders Johannsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lal_P/0/1/0/all/0/1">Partha Lal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Perkins_N/0/1/0/all/0/1">Nathan Perkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1">Dhivya Piraviperumal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1">Stephen Pulman</a>, <a href="http://arxiv.org/find/cs/1/au:+Seaghdha_D/0/1/0/all/0/1">Diarmuid &#xd3; S&#xe9;aghdha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">David Q. Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_J/0/1/0/all/0/1">John Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Vecchio_M/0/1/0/all/0/1">Marco Del Vecchio</a>, <a href="http://arxiv.org/find/cs/1/au:+Wacker_J/0/1/0/all/0/1">Jay Wacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1">Jason D. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>It has recently become feasible to run personal digital assistants on phones
and other personal devices. In this paper we describe a design for a natural
language understanding system that runs on device. In comparison to a
server-based assistant, this system is more private, more reliable, faster,
more expressive, and more accurate. We describe what led to key choices about
architecture and technologies. For example, some approaches in the dialog
systems literature are difficult to maintain over time in a deployment setting.
We hope that sharing learnings from our practical experiences may help inform
future work in the research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03917">Universal Automatic Phonetic Transcription into the International Phonetic Alphabet. (arXiv:2308.03917v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taguchi_C/0/1/0/all/0/1">Chihiro Taguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakai_Y/0/1/0/all/0/1">Yusuke Sakai</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1">Parisa Haghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1">David Chiang</a></p>
<p>This paper presents a state-of-the-art model for transcribing speech in any
language into the International Phonetic Alphabet (IPA). Transcription of
spoken languages into IPA is an essential yet time-consuming process in
language documentation, and even partially automating this process has the
potential to drastically speed up the documentation of endangered languages.
Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is
based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use
training data from seven languages from CommonVoice 11.0, transcribed into IPA
semi-automatically. Although this training dataset is much smaller than
Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or
better results. Furthermore, we show that the quality of our universal
speech-to-IPA models is close to that of human annotators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03929">Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1">Ahmed Abdeen Hamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Crimi_A/0/1/0/all/0/1">Alessandro Crimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Misiak_M/0/1/0/all/0/1">Magdalena M. Misiak</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byung Suk Lee</a></p>
<p>Methods: Through an innovative approach, we construct ontology-based
knowledge graphs from authentic medical literature and AI-generated content.
Our goal is to distinguish factual information from unverified data. We
compiled two datasets: one from biomedical literature using a "human disease
and symptoms" query, and another generated by ChatGPT, simulating articles.
With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts
each, selected randomly with a specific seed. Our method focuses on utilizing
disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs,
robust mathematical models that facilitate unbiased comparisons. By employing
our fact-checking algorithms and network centrality metrics, we conducted GPT
disease-symptoms link analysis to quantify the accuracy of factual knowledge
amid noise, hypotheses, and significant findings.
</p>
<p>Results: The findings obtained from the comparison of diverse ChatGPT
knowledge graphs with their PubMed counterparts revealed some interesting
observations. While PubMed knowledge graphs exhibit a wealth of disease-symptom
terms, it is surprising to observe that some ChatGPT graphs surpass them in the
number of connections. Furthermore, some GPT graphs are demonstrating supremacy
of the centrality scores, especially for the overlapping nodes. This striking
contrast indicates the untapped potential of knowledge that can be derived from
AI-generated content, awaiting verification. Out of all the graphs, the factual
link ratio between any two graphs reached its peak at 60%.
</p>
<p>Conclusions: An intriguing insight from our findings was the striking number
of links among terms in the knowledge graph generated from ChatGPT datasets,
surpassing some of those in its PubMed counterpart. This early discovery has
prompted further investigation using universal network metrics to unveil the
new knowledge the links may hold.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03958">Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jerry Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Da Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yifeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a></p>
<p>Sycophancy is an undesirable behavior where models tailor their responses to
follow a human user's view even when that view is not objectively correct
(e.g., adapting liberal views once a user reveals that they are liberal). In
this paper, we study the prevalence of sycophancy in language models and
propose a simple synthetic-data intervention to reduce this behavior.
</p>
<p>First, on a set of three sycophancy tasks (Perez et al., 2022) where models
are asked for an opinion on statements with no correct answers (e.g.,
politics), we observe that both model scaling and instruction tuning
significantly increase sycophancy for PaLM models up to 540B parameters.
Second, we extend sycophancy evaluations to simple addition statements that are
objectively incorrect, finding that despite knowing that these statements are
wrong, language models will still agree with them if the user does as well.
</p>
<p>To reduce sycophancy, we present a straightforward synthetic-data
intervention that takes public NLP tasks and encourages models to be robust to
user opinions on these tasks. Adding these data in a lightweight finetuning
step can significantly reduce sycophantic behavior on held-out prompts. Code
for generating synthetic data for intervention can be found at
https://github.com/google/sycophancy-intervention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03983">SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1">Youyang Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Miyashita_D/0/1/0/all/0/1">Daisuke Miyashita</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoshi_Y/0/1/0/all/0/1">Yasuto Hoshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Morioka_Y/0/1/0/all/0/1">Yasuhiro Morioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Torii_O/0/1/0/all/0/1">Osamu Torii</a>, <a href="http://arxiv.org/find/cs/1/au:+Kodama_T/0/1/0/all/0/1">Tomoya Kodama</a>, <a href="http://arxiv.org/find/cs/1/au:+Deguchi_J/0/1/0/all/0/1">Jun Deguchi</a></p>
<p>Large Language Model (LLM) based Generative AI systems have seen significant
progress in recent years. Integrating a knowledge retrieval architecture allows
for seamless integration of private data into publicly available Generative AI
systems using pre-trained LLM without requiring additional model fine-tuning.
Moreover, Retrieval-Centric Generation (RCG) approach, a promising future
research direction that explicitly separates roles of LLMs and retrievers in
context interpretation and knowledge memorization, potentially leads to more
efficient implementation. SimplyRetrieve is an open-source tool with the goal
of providing a localized, lightweight, and user-friendly interface to these
sophisticated advancements to the machine learning community. SimplyRetrieve
features a GUI and API based RCG platform, assisted by a Private Knowledge Base
Constructor and a Retrieval Tuning Module. By leveraging these capabilities,
users can explore the potential of RCG for improving generative AI performance
while maintaining privacy standards. The tool is available at
https://github.com/RCGAI/SimplyRetrieve with an MIT license.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04014">Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1">Kshitij Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1">Benjamin Th&#xe9;rien</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1">Adam Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1">Mats L. Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1">Quentin Anthony</a>, <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1">Irina Rish</a>, <a href="http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1">Timoth&#xe9;e Lesort</a></p>
<p>Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to restart the process over again once new data becomes available. A much
cheaper and more efficient solution would be to enable the continual
pre-training of these models, i.e. updating pre-trained models with new data
instead of re-training them from scratch. However, the distribution shift
induced by novel data typically results in degraded performance on past data.
Taking a step towards efficient continual pre-training, in this work, we
examine the effect of different warm-up strategies. Our hypothesis is that the
learning rate must be re-increased to improve compute efficiency when training
on a new dataset. We study the warmup phase of models pre-trained on the Pile
(upstream data, 300B tokens) as we continue to pre-train on SlimPajama
(downstream data, 297B tokens), following a linear warmup and cosine decay
schedule. We conduct all experiments on the Pythia 410M language model
architecture and evaluate performance through validation perplexity. We
experiment with different pre-training checkpoints, various maximum learning
rates, and various warmup lengths. Our results show that while rewarming models
first increases the loss on upstream and downstream data, in the longer run it
improves the downstream performance, outperforming models trained from
scratch$\unicode{x2013}$even for a large downstream dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04028">Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shashank Gupta</a></p>
<p>Question answering is a task that answers factoid questions using a large
collection of documents. It aims to provide precise answers in response to the
user's questions in natural language. Question answering relies on efficient
passage retrieval to select candidate contexts, where traditional sparse vector
space models, such as TF-IDF or BM25, are the de facto method. On the web,
there is no single article that could provide all the possible answers
available on the internet to the question of the problem asked by the user. The
existing Dense Passage Retrieval model has been trained on Wikipedia dump from
Dec. 20, 2018, as the source documents for answering questions. Question
answering (QA) has made big strides with several open-domain and machine
comprehension systems built using large-scale annotated datasets. However, in
the clinical domain, this problem remains relatively unexplored. According to
multiple surveys, Biomedical Questions cannot be answered correctly from
Wikipedia Articles. In this work, we work on the existing DPR framework for the
biomedical domain and retrieve answers from the Pubmed articles which is a
reliable source to answer medical questions. When evaluated on a BioASQ QA
dataset, our fine-tuned dense retriever results in a 0.81 F1 score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04037">A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset. (arXiv:2308.04037v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1">Mamata Das</a>, <a href="http://arxiv.org/find/cs/1/au:+K%2E_S/0/1/0/all/0/1">Selvakumar K.</a>, <a href="http://arxiv.org/find/cs/1/au:+Alphonse_P/0/1/0/all/0/1">P.J.A. Alphonse</a></p>
<p>Text Classification is the process of categorizing text into the relevant
categories and its algorithms are at the core of many Natural Language
Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP
are the most highly used information retrieval methods in text classification.
We have investigated and analyzed the feature weighting method for text
classification on unstructured data. The proposed model considered two features
N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset
for sentiment analysis. Then we have used the state-of-the-art classifier to
validate the method i.e., Support Vector Machine (SVM), Logistic Regression,
Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and
k-nearest neighbors (KNN). From those two feature extractions, a significant
increase in feature extraction with TF-IDF features rather than based on
N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall
(93.81%), and F1-score (91.99%) value in Random Forest classifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04041">InfeRE: Step-by-Step Regex Generation via Chain of Inference. (arXiv:2308.04041v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiaodong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Beijun Shen</a></p>
<p>Automatically generating regular expressions (abbrev. regexes) from natural
language description (NL2RE) has been an emerging research area. Prior studies
treat regex as a linear sequence of tokens and generate the final expressions
autoregressively in a single pass. They did not take into account the
step-by-step internal text-matching processes behind the final results. This
significantly hinders the efficacy and interpretability of regex generation by
neural language models. In this paper, we propose a new paradigm called InfeRE,
which decomposes the generation of regexes into chains of step-by-step
inference. To enhance the robustness, we introduce a self-consistency decoding
mechanism that ensembles multiple outputs sampled from different models. We
evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and
compare the results with state-of-the-art approaches and the popular tree-based
generation approach TRANX. Experimental results show that InfeRE substantially
outperforms previous baselines, yielding 16.3% and 14.7% improvement in DFA@5
accuracy on two datasets, respectively. Particularly, InfeRE outperforms the
popular tree-based generation approach by 18.1% and 11.3% on both datasets,
respectively, in terms of DFA@5 accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04052">The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Merino_T/0/1/0/all/0/1">Timothy Merino</a>, <a href="http://arxiv.org/find/cs/1/au:+Negri_R/0/1/0/all/0/1">Roman Negri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajesh_D/0/1/0/all/0/1">Dipika Rajesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Charity_M/0/1/0/all/0/1">M Charity</a>, <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a></p>
<p>The five-dollar model is a lightweight text-to-image generative architecture
that generates low dimensional images from an encoded text prompt. This model
can successfully generate accurate and aesthetically pleasing content in low
dimensional domains, with limited amounts of training data. Despite the small
size of both the model and datasets, the generated images are still able to
maintain the encoded semantic meaning of the textual prompt. We apply this
model to three small datasets: pixel art video game maps, video game sprite
images, and down-scaled emoji images and apply novel augmentation strategies to
improve the performance of our model on these limited datasets. We evaluate our
models performance using cosine similarity score between text-image pairs
generated by the CLIP VIT-B/32 model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04076">DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles. (arXiv:2308.04076v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sultanum_N/0/1/0/all/0/1">Nicole Sultanum</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1">Arjun Srinivasan</a></p>
<p>Authoring data-driven articles is a complex process requiring authors to not
only analyze data for insights but also craft a cohesive narrative that
effectively communicates the insights. Text generation capabilities of
contemporary large language models (LLMs) present an opportunity to assist the
authoring of data-driven articles and expedite the writing process. In this
work, we investigate the feasibility and perceived value of leveraging LLMs to
support authors of data-driven articles. We designed a prototype system,
DataTales, that leverages a LLM to generate textual narratives accompanying a
given chart. Using DataTales as a design probe, we conducted a qualitative
study with 11 professionals to evaluate the concept, from which we distilled
affordances and opportunities to further integrate LLMs as valuable data-driven
article authoring assistants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04109">I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection. (arXiv:2308.04109v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yongzhu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongsheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1">Jiashu Pu</a></p>
<p>Simile detection is a valuable task for many natural language processing
(NLP)-based applications, particularly in the field of literature. However,
existing research on simile detection often relies on corpora that are limited
in size and do not adequately represent the full range of simile forms. To
address this issue, we propose a simile data augmentation method based on
\textbf{W}ord replacement And Sentence completion using the GPT-2 language
model. Our iterative process called I-WAS, is designed to improve the quality
of the augmented sentences. To better evaluate the performance of our method in
real-world applications, we have compiled a corpus containing a more diverse
set of simile forms for experimentation. Our experimental results demonstrate
the effectiveness of our proposed data augmentation method for simile
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04114">Collective Human Opinions in Semantic Textual Similarity. (arXiv:2308.04114v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1">Shimin Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_N/0/1/0/all/0/1">Ning Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1">Timothy Baldwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1">Karin Verspoor</a></p>
<p>Despite the subjective nature of semantic textual similarity (STS) and
pervasive disagreements in STS annotation, existing benchmarks have used
averaged human ratings as the gold standard. Averaging masks the true
distribution of human opinions on examples of low agreement, and prevents
models from capturing the semantic vagueness that the individual ratings
represent. In this work, we introduce USTS, the first Uncertainty-aware STS
dataset with ~15,000 Chinese sentence pairs and 150,000 labels, to study
collective human opinions in STS. Analysis reveals that neither a scalar nor a
single Gaussian fits a set of observed judgements adequately. We further show
that current STS models cannot capture the variance caused by human
disagreement on individual instances, but rather reflect the predictive
confidence over the aggregate dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04124">Social Media, Topic Modeling and Sentiment Analysis in Municipal Decision Support. (arXiv:2308.04124v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Svana_M/0/1/0/all/0/1">Milo&#x161; &#x160;va&#x148;a</a></p>
<p>Many cities around the world are aspiring to become. However, smart
initiatives often give little weight to the opinions of average citizens.
</p>
<p>Social media are one of the most important sources of citizen opinions. This
paper presents a prototype of a framework for processing social media posts
with municipal decision-making in mind. The framework consists of a sequence of
three steps: (1) determining the sentiment polarity of each social media post
(2) identifying prevalent topics and mapping these topics to individual posts,
and (3) aggregating these two pieces of information into a fuzzy number
representing the overall sentiment expressed towards each topic. Optionally,
the fuzzy number can be reduced into a tuple of two real numbers indicating the
"amount" of positive and negative opinion expressed towards each topic.
</p>
<p>The framework is demonstrated on tweets published from Ostrava, Czechia over
a period of about two months. This application illustrates how fuzzy numbers
represent sentiment in a richer way and capture the diversity of opinions
expressed on social media.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04138">Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trautmann_D/0/1/0/all/0/1">Dietrich Trautmann</a></p>
<p>Prompting is used to guide or steer a language model in generating an
appropriate response that is consistent with the desired outcome. Chaining is a
strategy used to decompose complex tasks into smaller, manageable components.
In this study, we utilize prompt chaining for extensive legal document
classification tasks, which present difficulties due to their intricate
domain-specific language and considerable length. Our approach begins with the
creation of a concise summary of the original document, followed by a semantic
search for related exemplar texts and their corresponding annotations from a
training corpus. Finally, we prompt for a label - based on the task - to
assign, by leveraging the in-context learning from the few-shot prompt. We
demonstrate that through prompt chaining, we can not only enhance the
performance over zero-shot, but also surpass the micro-F1 score achieved by
larger models, such as ChatGPT zero-shot, using smaller models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04176">On Monotonic Aggregation for Open-domain QA. (arXiv:2308.04176v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Sang-eun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1">Yeonseok Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Seung-won Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungjae Lee</a></p>
<p>Question answering (QA) is a critical task for speech-based retrieval from
knowledge sources, by sifting only the answers without requiring to read
supporting documents. Specifically, open-domain QA aims to answer user
questions on unrestricted knowledge sources. Ideally, adding a source should
not decrease the accuracy, but we find this property (denoted as
"monotonicity") does not hold for current state-of-the-art methods. We identify
the cause, and based on that we propose Judge-Specialist framework. Our
framework consists of (1) specialist retrievers/readers to cover individual
sources, and (2) judge, a dedicated language model to select the final answer.
Our experiments show that our framework not only ensures monotonicity, but also
outperforms state-of-the-art multi-source QA methods on Natural Questions.
Additionally, we show that our models robustly preserve the monotonicity
against noise from speech recognition. We publicly release our code and
setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04180">Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: &quot;Are we on the same page ?&quot;. (arXiv:2308.04180v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carneiro_B/0/1/0/all/0/1">Bruno Machado Carneiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Linardi_M/0/1/0/all/0/1">Michele Linardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Longhi_J/0/1/0/all/0/1">Julien Longhi</a></p>
<p>We study Socially Unacceptable Discourse (SUD) characterization and detection
in online text. We first build and present a novel corpus that contains a large
variety of manually annotated texts from different online sources used so far
in state-of-the-art Machine learning (ML) SUD detection solutions. This global
context allows us to test the generalization ability of SUD classifiers that
acquire knowledge around the same SUD categories, but from different contexts.
From this perspective, we can analyze how (possibly) different annotation
modalities influence SUD learning by discussing open challenges and open
research directions. We also provide several data insights which can support
domain experts in the annotation task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04215">Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuchao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menglin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Couturier_C/0/1/0/all/0/1">Camille Couturier</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1">Victor Ruhle</a></p>
<p>Retrieval augmented models show promise in enhancing traditional language
models by improving their contextual understanding, integrating private data,
and reducing hallucination. However, the processing time required for retrieval
augmented large language models poses a challenge when applying them to tasks
that require real-time responses, such as composition assistance.
</p>
<p>To overcome this limitation, we propose the Hybrid Retrieval-Augmented
Generation (HybridRAG) framework that leverages a hybrid setting that combines
both client and cloud models. HybridRAG incorporates retrieval-augmented memory
generated asynchronously by a Large Language Model (LLM) in the cloud. By
integrating this retrieval augmented memory, the client model acquires the
capability to generate highly effective responses, benefiting from the LLM's
capabilities. Furthermore, through asynchronous memory integration, the client
model is capable of delivering real-time responses to user requests without the
need to wait for memory synchronization from the cloud. Our experiments on
Wikitext and Pile subsets show that HybridRAG achieves lower latency than a
cloud-based retrieval-augmented LLM, while outperforming client-only models in
utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04248">Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Walsh_H/0/1/0/all/0/1">Harry Walsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sincan_O/0/1/0/all/0/1">Ozge Mercanoglu Sincan</a>, <a href="http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1">Ben Saunders</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1">Richard Bowden</a></p>
<p>Capturing and annotating Sign language datasets is a time consuming and
costly process. Current datasets are orders of magnitude too small to
successfully train unconstrained \acf{slt} models. As a result, research has
turned to TV broadcast content as a source of large-scale training data,
consisting of both the sign language interpreter and the associated audio
subtitle. However, lack of sign language annotation limits the usability of
this data and has led to the development of automatic annotation techniques
such as sign spotting. These spottings are aligned to the video rather than the
subtitle, which often results in a misalignment between the subtitle and
spotted signs. In this paper we propose a method for aligning spottings with
their corresponding subtitles using large spoken language models. Using a
single modality means our method is computationally inexpensive and can be
utilized in conjunction with existing alignment techniques. We quantitatively
demonstrate the effectiveness of our method on the \acf{mdgs} and \acf{bobsl}
datasets, recovering up to a 33.22 BLEU-1 score in word alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04255">CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tercon_L/0/1/0/all/0/1">Luka Ter&#x10d;on</a>, <a href="http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1">Nikola Ljube&#x161;i&#x107;</a></p>
<p>We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of
the South Slavic languages, which is based on the Stanza natural language
processing pipeline. We describe the main improvements in CLASSLA-Stanza with
respect to Stanza, and give a detailed description of the model training
process for the latest 2.1 release of the pipeline. We also report performance
scores produced by the pipeline for different languages and varieties.
CLASSLA-Stanza exhibits consistently high performance across all the supported
languages and outperforms or expands its parent pipeline Stanza at all the
supported tasks. We also present the pipeline's new functionality enabling
efficient processing of web data and the reasons that led to its
implementation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04275">In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaochuang Han</a></p>
<p>In this note, we explore inference-time alignment through in-context
learning. We consider a vanilla pretrained language model Llama-2 before any
fine-tuning and retrieve an average of 9 demonstration alignment examples when
the model is prompted to follow chat-style instructions. Compared to direct
prompting, the in-context alignment without changing model weights leads to a
7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making
the vanilla language model comparable to strong baselines with alignment
fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04286">Comparative Analysis of the wav2vec 2.0 Feature Extractor. (arXiv:2308.04286v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1">Peter Vieting</a>, <a href="http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1">Ralf Schl&#xfc;ter</a>, <a href="http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1">Hermann Ney</a></p>
<p>Automatic speech recognition (ASR) systems typically use handcrafted feature
extraction pipelines. To avoid their inherent information loss and to achieve
more consistent modeling from speech to transcribed text, neural raw waveform
feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model,
which has recently gained large popularity, uses a convolutional FE which
operates directly on the speech waveform. However, it is not yet studied
extensively in the literature. In this work, we study its capability to replace
the standard feature extraction methods in a connectionist temporal
classification (CTC) ASR model and compare it to an alternative neural FE. We
show that both are competitive with traditional FEs on the LibriSpeech
benchmark and analyze the effect of the individual components. Furthermore, we
analyze the learned filters and show that the most important information for
the ASR system is obtained by a set of bandpass filters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04306">Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenye Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qingbao Huang</a></p>
<p>The history of metaphor research also marks the evolution of knowledge
infusion research. With the continued advancement of deep learning techniques
in recent years, the natural language processing community has shown great
interest in applying knowledge to successful results in metaphor recognition
tasks. Although there has been a gradual increase in the number of approaches
involving knowledge injection in the field of metaphor recognition, there is a
lack of a complete review article on knowledge injection based approaches.
Therefore, the goal of this paper is to provide a comprehensive review of
research advances in the application of deep learning for knowledge injection
in metaphor recognition tasks. In this paper, we systematically summarize and
generalize the mainstream knowledge and knowledge injection principles, as well
as review the datasets, evaluation metrics, and benchmark models used in
metaphor recognition tasks. Finally, we explore the current issues facing
knowledge injection methods and provide an outlook on future research
directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04333">Towards an AI to Win Ghana&#x27;s National Science and Maths Quiz. (arXiv:2308.04333v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1">George Boateng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mensah_J/0/1/0/all/0/1">Jonathan Abrefah Mensah</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeboah_K/0/1/0/all/0/1">Kevin Takyi Yeboah</a>, <a href="http://arxiv.org/find/cs/1/au:+Edor_W/0/1/0/all/0/1">William Edor</a>, <a href="http://arxiv.org/find/cs/1/au:+Mensah_Onumah_A/0/1/0/all/0/1">Andrew Kojo Mensah-Onumah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_N/0/1/0/all/0/1">Naafi Dasana Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeboah_N/0/1/0/all/0/1">Nana Sam Yeboah</a></p>
<p>Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the
question we seek to answer in the NSMQ AI project, an open-source project that
is building AI to compete live in the NSMQ and win. The NSMQ is an annual live
science and mathematics competition for senior secondary school students in
Ghana in which 3 teams of 2 students compete by answering questions across
biology, chemistry, physics, and math in 5 rounds over 5 progressive stages
until a winning team is crowned for that year. The NSMQ is an exciting live
quiz competition with interesting technical challenges across speech-to-text,
text-to-speech, question-answering, and human-computer interaction. In this
ongoing work that began in January 2023, we give an overview of the project,
describe each of the teams, progress made thus far, and the next steps toward
our planned launch and debut of the AI in October for NSMQ 2023. An AI that
conquers this grand challenge can have real-world impact on education such as
enabling millions of students across Africa to have one-on-one learning support
from this AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04346">Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles. (arXiv:2308.04346v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1">Pranav Narayanan Venkit</a>, <a href="http://arxiv.org/find/cs/1/au:+Gautam_S/0/1/0/all/0/1">Sanjana Gautam</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchanadikar_R/0/1/0/all/0/1">Ruchi Panchanadikar</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Ting-Hao `Kenneth&#x27; Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1">Shomir Wilson</a></p>
<p>We investigate the potential for nationality biases in natural language
processing (NLP) models using human evaluation methods. Biased NLP models can
perpetuate stereotypes and lead to algorithmic discrimination, posing a
significant challenge to the fairness and justice of AI systems. Our study
employs a two-step mixed-methods approach that includes both quantitative and
qualitative analysis to identify and understand the impact of nationality bias
in a text generation model. Through our human-centered quantitative analysis,
we measure the extent of nationality bias in articles generated by AI sources.
We then conduct open-ended interviews with participants, performing qualitative
coding and thematic analysis to understand the implications of these biases on
human readers. Our findings reveal that biased NLP models tend to replicate and
amplify existing societal biases, which can translate to harm if used in a
sociotechnical setting. The qualitative analysis from our interviews offers
insights into the experience readers have when encountering such articles,
highlighting the potential to shift a reader's perception of a country. These
findings emphasize the critical role of public perception in shaping AI's
impact on society and the need to correct biases in AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04386">Learning Evaluation Models from Large Language Models for Sequence Generation. (arXiv:2308.04386v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenglong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kaiyan Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongran Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chunliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1">Quan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jingbo Zhu</a></p>
<p>Large language models achieve state-of-the-art performance on sequence
generation evaluation, but typically have a large number of parameters. This is
a computational challenge as presented by applying their evaluation capability
at scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an
\textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transfer
the evaluation capability from LLMs to relatively lightweight language models.
Based on the proposed ECT, we learn various evaluation models from ChatGPT, and
employ them as reward models to improve sequence generation models via
reinforcement learning and reranking approaches. Experimental results on
machine translation, text style transfer, and summarization tasks demonstrate
the effectiveness of our ECT. Notably, applying the learned evaluation models
to sequence generation models results in better generated sequences as
evaluated by commonly used metrics and ChatGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04398">Character-level NMT and language similarity. (arXiv:2308.04398v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1">Josef Jon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1">Ond&#x159;ej Bojar</a></p>
<p>We explore the effectiveness of character-level neural machine translation
using Transformer architecture for various levels of language similarity and
size of the training dataset on translation between Czech and Croatian, German,
Hungarian, Slovak, and Spanish. We evaluate the models using automatic MT
metrics and show that translation between similar languages benefits from
character-level input segmentation, while for less related languages,
character-level vanilla Transformer-base often lags behind subword-level
segmentation. We confirm previous findings that it is possible to close the gap
by finetuning the already trained subword-level models to character-level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04416">Legal Summarisation through LLMs: The PRODIGIT Project. (arXiv:2308.04416v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pont_T/0/1/0/all/0/1">Thiago Dal Pont</a>, <a href="http://arxiv.org/find/cs/1/au:+Galli_F/0/1/0/all/0/1">Federico Galli</a>, <a href="http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1">Andrea Loreggia</a>, <a href="http://arxiv.org/find/cs/1/au:+Pisano_G/0/1/0/all/0/1">Giuseppe Pisano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rovatti_R/0/1/0/all/0/1">Riccardo Rovatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Sartor_G/0/1/0/all/0/1">Giovanni Sartor</a></p>
<p>We present some initial results of a large-scale Italian project called
PRODIGIT which aims to support tax judges and lawyers through digital
technology, focusing on AI. We have focused on generation of summaries of
judicial decisions and on the extraction of related information, such as the
identification of legal issues and decision-making criteria, and the
specification of keywords. To this end, we have deployed and evaluated
different tools and approaches to extractive and abstractive summarisation. We
have applied LLMs, and particularly on GPT4, which has enabled us to obtain
results that proved satisfactory, according to an evaluation by expert tax
judges and lawyers. On this basis, a prototype application is being built which
will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04424">A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Li Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1">Yuyang Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1">Chong Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1">Donghong Ji</a></p>
<p>The joint task of Dialog Sentiment Classification (DSC) and Act Recognition
(DAR) aims to predict the sentiment label and act label for each utterance in a
dialog simultaneously. However, current methods encode the dialog context in
only one direction, which limits their ability to thoroughly comprehend the
context. Moreover, these methods overlook the explicit correlations between
sentiment and act labels, which leads to an insufficient ability to capture
rich sentiment and act clues and hinders effective and accurate reasoning. To
address these issues, we propose a Bi-directional Multi-hop Inference Model
(BMIM) that leverages a feature selection network and a bi-directional
multi-hop inference network to iteratively extract and integrate rich sentiment
and act clues in a bi-directional manner. We also employ contrastive learning
and dual learning to explicitly model the correlations of sentiment and act
labels. Our experiments on two widely-used datasets show that BMIM outperforms
state-of-the-art baselines by at least 2.6% on F1 score in DAR and 1.4% on F1
score in DSC. Additionally, Our proposed model not only improves the
performance but also enhances the interpretability of the joint sentiment and
act prediction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04430">SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1">Sewon Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1">Suchin Gururangan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1">Eric Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1">Noah A. Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a></p>
<p>The legality of training language models (LMs) on copyrighted or otherwise
restricted data is under intense debate. However, as we show, model performance
significantly degrades if trained only on low-risk text (e.g., out-of-copyright
books or government documents), due to its limited size and domain coverage. We
present SILO, a new language model that manages this risk-performance tradeoff
during inference. SILO is built by (1) training a parametric LM on Open License
Corpus (OLC), a new corpus we curate with 228B tokens of public domain and
permissively licensed text and (2) augmenting it with a more general and easily
modifiable nonparametric datastore (e.g., containing copyrighted books or news)
that is only queried during inference. The datastore allows use of high-risk
data without training on it, supports sentence-level data attribution, and
enables data producers to opt out from the model by removing content from the
store. These capabilities can foster compliance with data-use regulations such
as the fair use doctrine in the United States and the GDPR in the European
Union. Our experiments show that the parametric LM struggles on domains not
covered by OLC. However, access to the datastore greatly improves out of domain
performance, closing 90% of the performance gap with an LM trained on the Pile,
a more diverse corpus with mostly high-risk text. We also analyze which
nonparametric approach works best, where the remaining errors lie, and how
performance scales with datastore size. Our results suggest that it is possible
to build high quality language models while mitigating their legal risk.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.05337">A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Haolin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaoyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Ming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawei Song</a></p>
<p>Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that better meet the specific constraints
in practical applications. In recent years, methods using large-scale
pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the limited level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks that require different types of controlled
constraints. In this paper, we present a systematic critical review on the
common tasks, main approaches, and evaluation methods in this area. Finally, we
discuss the challenges that the field is facing, and put forward various
promising future directions. To the best of our knowledge, this is the first
survey paper to summarize the state-of-the-art CTG techniques from the
perspective of Transformer-based PLMs. We hope it can help researchers and
practitioners in the related fields to quickly track the academic and
technological frontier, providing them with a landscape of the area and a
roadmap for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03512">A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends. (arXiv:2302.03512v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1">Xiaoye Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yingjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1">Qingrong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zechang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhefeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1">Baoxing Huai</a></p>
<p>As more and more Arabic texts emerged on the Internet, extracting important
information from these Arabic texts is especially useful. As a fundamental
technology, Named entity recognition (NER) serves as the core component in
information extraction technology, while also playing a critical role in many
other Natural Language Processing (NLP) systems, such as question answering and
knowledge graph building. In this paper, we provide a comprehensive review of
the development of Arabic NER, especially the recent advances in deep learning
and pre-trained language model. Specifically, we first introduce the background
of Arabic NER, including the characteristics of Arabic and existing resources
for Arabic NER. Then, we systematically review the development of Arabic NER
methods. Traditional Arabic NER systems focus on feature engineering and
designing domain-specific rules. In recent years, deep learning methods achieve
significant progress by representing texts via continuous vector
representations. With the growth of pre-trained language model, Arabic NER
yields better performance. Finally, we conclude the method gap between Arabic
NER and NER methods from other languages, which helps outline future directions
for Arabic NER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07748">Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1">Seyed Mahed Mousavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1">Shohei Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Roccabruna_G/0/1/0/all/0/1">Gabriel Roccabruna</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshino_K/0/1/0/all/0/1">Koichiro Yoshino</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1">Satoshi Nakamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1">Giuseppe Riccardi</a></p>
<p>Narratives include a rich source of events unfolding over time and context.
Automatic understanding of these events provides a summarised comprehension of
the narrative for further computation (such as reasoning). In this paper, we
study the Information Status (IS) of the events and propose a novel challenging
task: the automatic identification of new events in a narrative. We define an
event as a triplet of subject, predicate, and object. The event is categorized
as new with respect to the discourse context and whether it can be inferred
through commonsense reasoning. We annotated a publicly available corpus of
narratives with the new events at sentence level using human annotators. We
present the annotation protocol and study the quality of the annotation and the
difficulty of the task. We publish the annotated dataset, annotation materials,
and machine learning baseline models for the task of new event extraction for
narrative understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10652">Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1">Peter Ochieng</a></p>
<p>The current monaural state of the art tools for speech separation relies on
supervised learning. This means that they must deal with permutation problem,
they are impacted by the mismatch on the number of speakers used in training
and inference. Moreover, their performance heavily relies on the presence of
high-quality labelled data. These problems can be effectively addressed by
employing a fully unsupervised technique for speech separation. In this paper,
we use contrastive learning to establish the representations of frames then use
the learned representations in the downstream deep modularization task.
Concretely, we demonstrate experimentally that in speech separation, different
frames of a speaker can be viewed as augmentations of a given hidden standard
frame of that speaker. The frames of a speaker contain enough prosodic
information overlap which is key in speech separation. Based on this, we
implement a self-supervised learning to learn to minimize the distance between
frames belonging to a given speaker. The learned representations are used in a
downstream deep modularization task to cluster frames based on speaker
identity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix
shows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively
in WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7
respectively in WSJ0-2mix. Its greatest strength being that as the number of
speakers increase, its performance does not degrade significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02864">Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. (arXiv:2306.02864v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1">Alejandro Pe&#xf1;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1">Aythami Morales</a>, <a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1">Julian Fierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1">Ignacio Serna</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1">Javier Ortega-Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Puente_I/0/1/0/all/0/1">I&#xf1;igo Puente</a>, <a href="http://arxiv.org/find/cs/1/au:+Cordova_J/0/1/0/all/0/1">Jorge Cordova</a>, <a href="http://arxiv.org/find/cs/1/au:+Cordova_G/0/1/0/all/0/1">Gonzalo Cordova</a></p>
<p>The analysis of public affairs documents is crucial for citizens as it
promotes transparency, accountability, and informed decision-making. It allows
citizens to understand government policies, participate in public discourse,
and hold representatives accountable. This is crucial, and sometimes a matter
of life or death, for companies whose operation depend on certain regulations.
Large Language Models (LLMs) have the potential to greatly enhance the analysis
of public affairs documents by effectively processing and understanding the
complex language used in such documents. In this work, we analyze the
performance of LLMs in classifying public affairs documents. As a natural
multi-label task, the classification of these documents presents important
challenges. In this work, we use a regex-powered tool to collect a database of
public affairs documents with more than 33K samples and 22.5M tokens. Our
experiments assess the performance of 4 different Spanish LLMs to classify up
to 30 different topics in the data in different configurations. The results
shows that LLMs can be of great use to process domain-specific documents, such
as those in the domain of public affairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07848">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yanni Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuguang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jixun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1">Wen Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Heng Lu</a></p>
<p>Contrastive learning based cross-modality pretraining approaches have
recently exhibited impressive success in diverse fields. In this paper, we
propose GEmo-CLAP, a kind of gender-attribute-enhanced contrastive
language-audio pretraining (CLAP) method for speech emotion recognition.
Specifically, a novel emotion CLAP model (Emo-CLAP) is first built, utilizing
pre-trained WavLM and RoBERTa models. Second, given the significance of the
gender attribute in speech emotion modeling, two novel soft label based
GEmo-CLAP (SL-GEmo-CLAP) and multi-task learning based GEmo-CLAP (ML-GEmo-CLAP)
models are further proposed to integrate emotion and gender information of
speech signals, forming more reasonable objectives. Extensive experiments on
IEMOCAP show that our proposed two GEmo-CLAP models consistently outperform the
baseline Emo-CLAP, while also achieving the best recognition performance
compared with recent state-of-the-art methods. Noticeably, the proposed
SL-GEmo-CLAP model achieves the best UAR of 81.43\% and WAR of 83.16\% which
performs better than other state-of-the-art SER methods by at least 3\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09841">Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond. (arXiv:2306.09841v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Fangzhi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qika Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a></p>
<p>Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06713">Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Estienne_L/0/1/0/all/0/1">Lautaro Estienne</a></p>
<p>A wide variety of natural language tasks are currently being addressed with
large-scale language models (LLMs). These models are usually trained with a
very large amount of unsupervised text data and adapted to perform a downstream
natural language task using methods like fine-tuning, calibration or in-context
learning. In this work, we propose an approach to adapt the prior class
distribution to perform text classification tasks without the need for labelled
samples and only few in-domain sample queries. The proposed approach treats the
LLM as a black box, adding a stage where the model posteriors are calibrated to
the task. Results show that these methods outperform the un-adapted model for
different number of training shots in the prompt and a previous approach were
calibration is performed without using any adaptation data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09998">Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meadows_J/0/1/0/all/0/1">Jordan Meadows</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1">Marco Valentino</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1">Andre Freitas</a></p>
<p>The derivation of mathematical results in specialised fields, using Large
Language Models (LLMs), is an emerging research direction that can help
identify models' limitations, and potentially support mathematical discovery.
In this paper, we leverage a symbolic engine to generate derivations of
equations at scale, and investigate the capabilities of LLMs when deriving goal
equations from premises. Specifically, we employ in-context learning for GPT
and fine-tune a range of T5 models to compare the robustness and generalisation
of pre-training strategies to specialised models. Empirical results show that
fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and
out-of-distribution test sets in conventional scores. However, an in-depth
analysis reveals that the fine-tuned models are more sensitive to perturbations
involving unseen symbols and (to a lesser extent) changes to equation
structure. In addition, we analyse 1.7K equations, and over 200 derivations, to
highlight common reasoning errors such as the inclusion of incorrect,
irrelevant, and redundant equations. Finally, we explore the suitability of
existing metrics for evaluating mathematical derivations and find evidence
that, while they can capture general properties such as sensitivity to
perturbations, they fail to highlight fine-grained reasoning errors and
essential differences between models. Overall, this work demonstrates that
training models on synthetic data may improve their math capabilities beyond
much larger LLMs, but current metrics are not appropriately assessing the
quality of generated mathematical text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10457">Improving the Reusability of Pre-trained Language Models in Real-world Applications. (arXiv:2307.10457v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghanbarzadeh_S/0/1/0/all/0/1">Somayeh Ghanbarzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1">Hamid Palangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1">Radames Cruz Moreno</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanpour_H/0/1/0/all/0/1">Hamed Khanpour</a></p>
<p>The reusability of state-of-the-art Pre-trained Language Models (PLMs) is
often limited by their generalization problem, where their performance
drastically decreases when evaluated on examples that differ from the training
dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation
arises from PLMs' reliance on spurious correlations, which work well for
frequent example types but not for general examples. To address this issue, we
propose a training approach called Mask-tuning, which integrates Masked
Language Modeling (MLM) training objectives into the fine-tuning process to
enhance PLMs' generalization. Comprehensive experiments demonstrate that
Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs'
generalization on OOD datasets while improving their performance on
in-distribution datasets. The findings suggest that Mask-tuning improves the
reusability of PLMs on unseen data, making them more practical and effective
for real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11661">Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1">Mayug Maniparambil</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorster_C/0/1/0/all/0/1">Chris Vorster</a>, <a href="http://arxiv.org/find/cs/1/au:+Molloy_D/0/1/0/all/0/1">Derek Molloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_N/0/1/0/all/0/1">Noel Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1">Kevin McGuinness</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1">Noel E. O&#x27;Connor</a></p>
<p>Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. The code, prompts, and auxiliary text dataset is
available at https://github.com/mayug/VDT-Adapter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15002">Gzip versus bag-of-words for text classification. (arXiv:2307.15002v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1">Juri Opitz</a></p>
<p>The effectiveness of compression in text classification ('gzip') has recently
garnered lots of attention. In this note we show that `bag-of-words' approaches
can achieve similar or better results, and are more efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01681">NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1">Muskan Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Reji_D/0/1/0/all/0/1">Deepak John Reji</a>, <a href="http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1">Syed Raza Bashir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a></p>
<p>Bias in textual data can lead to skewed interpretations and outcomes when the
data is used. These biases could perpetuate stereotypes, discrimination, or
other forms of unfair treatment. An algorithm trained on biased data ends up
making decisions that disproportionately impact a certain group of people.
Therefore, it is crucial to detect and remove these biases to ensure the fair
and ethical use of data. To this end, we develop a comprehensive and robust
framework \textsc{Nbias} that consists of a data layer, corpus contruction,
model development layer and an evaluation layer. The dataset is constructed by
collecting diverse data from various fields, including social media,
healthcare, and job hiring portals. As such, we applied a transformer-based
token classification model that is able to identify bias words/ phrases through
a unique named entity. In the assessment procedure, we incorporate a blend of
quantitative and qualitative evaluations to gauge the effectiveness of our
models. We achieve accuracy improvements ranging from 1% to 8% compared to
baselines. We are also able to generate a robust understanding of the model
functioning, capturing not only numerical data but also the quality and
intricacies of its performance. The proposed approach is applicable to a
variety of biases and contributes to the fair and ethical use of textual data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02013">Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1">Guruprasad V Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chennupati_G/0/1/0/all/0/1">Gopinath Chennupati</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1">Milind Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1">Anit Kumar Sahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1">Ariya Rastrow</a>, <a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1">Jasha Droppo</a></p>
<p>Federated Learning (FL) is a privacy-preserving paradigm, allowing edge
devices to learn collaboratively without sharing data. Edge devices like Alexa
and Siri are prospective sources of unlabeled audio data that can be tapped to
learn robust audio representations. In this work, we bring Self-supervised
Learning (SSL) and FL together to learn representations for Automatic Speech
Recognition respecting data privacy constraints. We use the speaker and chapter
information in the unlabeled speech dataset, Libri-Light, to simulate non-IID
speaker-siloed data distributions and pre-train an LSTM encoder with the
Contrastive Predictive Coding framework with FedSGD. We show that the
pre-trained ASR encoder in FL performs as well as a centrally pre-trained model
and produces an improvement of 12-15% (WER) compared to no pre-training. We
further adapt the federated pre-trained models to a new language, French, and
show a 20% (WER) improvement over no pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02582">Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Aseem Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaisaheb_S/0/1/0/all/0/1">Shabbirhussain Bhaisaheb</a>, <a href="http://arxiv.org/find/cs/1/au:+Patwardhan_M/0/1/0/all/0/1">Manasi Patwardhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1">Lovekesh Vig</a>, <a href="http://arxiv.org/find/cs/1/au:+Shroff_G/0/1/0/all/0/1">Gautam Shroff</a></p>
<p>Cross-domain and cross-compositional generalization of Text-to-SQL semantic
parsing is a challenging task. Existing Large Language Model (LLM) based
solutions rely on inference-time retrieval of few-shot exemplars from the
training set to synthesize a run-time prompt for each Natural Language (NL)
test query. In contrast, we devise an algorithm which performs offline sampling
of a minimal set-of few-shots from the training data, with complete coverage of
SQL clauses, operators and functions, and maximal domain coverage within the
allowed token length. This allows for synthesis of a fixed Generic Prompt (GP),
with a diverse set-of exemplars common across NL test queries, avoiding
expensive test time exemplar retrieval. We further auto-adapt the GP to the
target database domain (DA-GP), to better handle cross-domain generalization;
followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle
cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline
task, to be performed one-time per new database with minimal human
intervention. Our approach demonstrates superior performance on the KaggleDBQA
dataset, designed to evaluate generalizability for the Text-to-SQL task. We
further showcase consistent performance improvement of LTMP-DA-GP over GP,
across LLMs and databases of KaggleDBQA, highlighting the efficacy and model
agnostic benefits of our prompt based adapt and decompose approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03131">Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xianfeng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yijin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely
utilized across a range of natural language generation (NLG) tasks. However,
recent studies have revealed a weak correlation between these matching-based
metrics and human evaluations, especially when compared with neural-based
metrics like BLEURT. In this paper, we conjecture that the performance
bottleneck in matching-based metrics may be caused by the limited diversity of
references. To address this issue, we propose to utilize \textit{multiple
references} to enhance the consistency between these metrics and human
evaluations. Within the WMT Metrics benchmarks, we observe that the
multi-references F200spBLEU surpasses the conventional single-reference one by
an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based
BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the
data leakage issue in large language models (LLMs) can be mitigated to a large
extent by our multi-reference metric. We release the code and data at
\url{https://github.com/SefaZeng/LLM-Ref}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03421">RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yufan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qiaozhi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1">Xiaomin Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kunpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenlai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guangwen Yang</a></p>
<p>Existing large language models have to run K times to generate a sequence of
K tokens. In this paper, we present RecycleGPT, a generative language model
with fast decoding speed by recycling pre-generated model states without
running the whole model in multiple steps. Our approach relies on the
observation that adjacent tokens in a sequence usually have strong correlations
and the next token in a sequence can be reasonably guessed or inferred based on
the preceding ones. Experiments and analysis demonstrate the effectiveness of
our approach in lowering inference latency, achieving up to 1.4x speedup while
preserving high performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03565">Topological Interpretations of GPT-3. (arXiv:2308.03565v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianyi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Nelson_B/0/1/0/all/0/1">Bradley Nelson</a></p>
<p>This is an experiential study of investigating a consistent method for
deriving the correlation between sentence vector and semantic meaning of a
sentence. We first used three state-of-the-art word/sentence embedding methods
including GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence
strings into high dimensional spaces. Then we compute the pairwise distance
between any possible combination of two sentence vectors in an embedding space
and map them into a matrix. Based on each distance matrix, we compute the
correlation of distances of a sentence vector with respect to the other
sentence vectors in an embedding space. Then we compute the correlation of each
pair of the distance matrices. We observed correlations of the same sentence in
different embedding spaces and correlations of different sentences in the same
embedding space. These observations are consistent with our hypothesis and take
us to the next stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03629">MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alrdahi_H/0/1/0/all/0/1">Haifa Alrdahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lifeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Suvalov_H/0/1/0/all/0/1">Hendrik &#x160;uvalov</a>, <a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1">Goran Nenadic</a></p>
<p>Automatic medication mining from clinical and biomedical text has become a
popular topic due to its real impact on healthcare applications and the recent
development of powerful language models (LMs). However, fully-automatic
extraction models still face obstacles to be overcome such that they can be
deployed directly into clinical practice for better impacts. Such obstacles
include their imbalanced performances on different entity types and clinical
events. In this work, we examine current state-of-the-art pre-trained language
models (PLMs) on such tasks, via fine-tuning including the monolingual model
Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their
advantages and drawbacks using historical medication mining shared task data
sets from n2c2-2018 challenges. We report the findings we get from these
fine-tuning experiments such that they can facilitate future research on
addressing them, for instance, how to combine their outputs, merge such models,
or improve their overall accuracy by ensemble learning and data augmentation.
MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</p>
</p>
</div>

    </div>
    </body>
    