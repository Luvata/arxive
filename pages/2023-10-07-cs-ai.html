<!DOCTYPE html>
<html>
<head>
<title>2023-10-07-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2310.03027">Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction. (arXiv:2310.03027v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Prakash_M/0/1/0/all/0/1">M V Sai Prakash</a>, <a href="http://arxiv.org/find/physics/1/au:+N_S/0/1/0/all/0/1">Siddartha Reddy N</a>, <a href="http://arxiv.org/find/physics/1/au:+Parab_G/0/1/0/all/0/1">Ganesh Parab</a>, <a href="http://arxiv.org/find/physics/1/au:+V_V/0/1/0/all/0/1">Varun V</a>, <a href="http://arxiv.org/find/physics/1/au:+Vaddina_V/0/1/0/all/0/1">Vishal Vaddina</a>, <a href="http://arxiv.org/find/physics/1/au:+Gopalakrishnan_S/0/1/0/all/0/1">Saisubramaniam Gopalakrishnan</a></p>
<p>Molecular property prediction is a critical task in computational drug
discovery. While recent advances in Graph Neural Networks (GNNs) and
Transformers have shown to be effective and promising, they face the following
limitations: Transformer self-attention does not explicitly consider the
underlying molecule structure while GNN feature representation alone is not
sufficient to capture granular and hidden interactions and characteristics that
distinguish similar molecules. To address these limitations, we propose SYN-
FUSION, a novel approach that synergistically combines pre-trained features
from GNNs and Transformers. This approach provides a comprehensive molecular
representation, capturing both the global molecule structure and the individual
atom characteristics. Experimental results on MoleculeNet benchmarks
demonstrate superior performance, surpassing previous models in 5 out of 7
classification datasets and 4 out of 6 regression datasets. The performance of
SYN-FUSION has been compared with other Graph-Transformer models that have been
jointly trained using a combination of transformer and graph features, and it
is found that our approach is on par with those models in terms of performance.
Extensive analysis of the learned fusion model across aspects such as loss,
latent space, and weight distribution further validates the effectiveness of
SYN-FUSION. Finally, an ablation study unequivocally demonstrates that the
synergy achieved by SYN-FUSION surpasses the performance of its individual
model components and their ensemble, offering a substantial improvement in
predicting molecular properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03028">SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery. (arXiv:2310.03028v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Taub_R/0/1/0/all/0/1">Ronen Taub</a>, <a href="http://arxiv.org/find/physics/1/au:+Savir_Y/0/1/0/all/0/1">Yonatan Savir</a></p>
<p>Machine learning, and representation learning in particular, has the
potential to facilitate drug discovery by screening a large chemical space in
silico. A successful approach for representing molecules is to treat them as a
graph and utilize graph neural networks. One of the key limitations of such
methods is the necessity to represent compounds with different numbers of
atoms, which requires aggregating the atom's information. Common aggregation
operators, such as averaging, result in loss of information at the atom level.
In this work, we propose a novel aggregating approach where each atom is
weighted non-linearly using the Boltzmann distribution with a hyperparameter
analogous to temperature. We show that using this weighted aggregation improves
the ability of the gold standard message-passing neural network to predict
antibiotic activity. Moreover, by changing the temperature hyperparameter, our
approach can reveal the atoms that are important for activity prediction in a
smooth and consistent way, thus providing a novel, regulated attention
mechanism for graph neural networks. We further validate our method by showing
that it recapitulates the functional group in beta-Lactam antibiotics. The
ability of our approach to rank the atoms' importance for a desired function
can be used within any graph neural network to provide interpretability of the
results and predictions at the node level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03031">How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Urchs_S/0/1/0/all/0/1">Stefanie Urchs</a>, <a href="http://arxiv.org/find/cs/1/au:+Thurner_V/0/1/0/all/0/1">Veronika Thurner</a>, <a href="http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1">Matthias A&#xdf;enmacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1">Christian Heumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiemichen_S/0/1/0/all/0/1">Stephanie Thiemichen</a></p>
<p>With the introduction of ChatGPT, OpenAI made large language models (LLM)
accessible to users with limited IT expertise. However, users with no
background in natural language processing (NLP) might lack a proper
understanding of LLMs. Thus the awareness of their inherent limitations, and
therefore will take the systems' output at face value. In this paper, we
systematically analyse prompts and the generated responses to identify possible
problematic issues with a special focus on gender biases, which users need to
be aware of when processing the system's output. We explore how ChatGPT reacts
in English and German if prompted to answer from a female, male, or neutral
perspective. In an in-depth investigation, we examine selected prompts and
analyse to what extent responses differ if the system is prompted several times
in an identical way. On this basis, we show that ChatGPT is indeed useful for
helping non-IT users draft texts for their daily work. However, it is
absolutely crucial to thoroughly check the system's responses for biases as
well as for syntactic and grammatical mistakes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03033">Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Postovan_A/0/1/0/all/0/1">Andreea Postovan</a>, <a href="http://arxiv.org/find/cs/1/au:+Erascu_M/0/1/0/all/0/1">M&#x103;d&#x103;lina Era&#x15f;cu</a></p>
<p>Traffic signs play a critical role in road safety and traffic management for
autonomous driving systems. Accurate traffic sign classification is essential
but challenging due to real-world complexities like adversarial examples and
occlusions. To address these issues, binary neural networks offer promise in
constructing classifiers suitable for resource-constrained devices.
</p>
<p>In our previous work, we proposed high-accuracy BNN models for traffic sign
recognition, focusing on compact size for limited computation and energy
resources. To evaluate their local robustness, this paper introduces a set of
benchmark problems featuring layers that challenge state-of-the-art
verification tools. These layers include binarized convolutions, max pooling,
batch normalization, fully connected. The difficulty of the verification
problem is given by the high number of network parameters (905k - 1.7 M), of
the input dimension (2.7k-12k), and of the number of regions (43) as well by
the fact that the neural networks are not sparse.
</p>
<p>The proposed BNN models and local robustness properties can be checked at
https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition.
</p>
<p>The results of the 4th International Verification of Neural Networks
Competition (VNN-COMP'23) revealed the fact that 4, out of 7, solvers can
handle many of our benchmarks randomly selected (minimum is 6, maximum is 36,
out of 45). Surprisingly, tools output also wrong results or missing
counterexample (ranging from 1 to 4). Currently, our focus lies in exploring
the possibility of achieving a greater count of solved instances by extending
the allotted time (previously set at 8 minutes). Furthermore, we are intrigued
by the reasons behind the erroneous outcomes provided by the tools for certain
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03036">A quantum system control method based on enhanced reinforcement learning. (arXiv:2310.03036v1 [cs.ET])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bosi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jihao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yebo Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zidan_M/0/1/0/all/0/1">Mohammed Zidan</a></p>
<p>Traditional quantum system control methods often face different constraints,
and are easy to cause both leakage and stochastic control errors under the
condition of limited resources. Reinforcement learning has been proved as an
efficient way to complete the quantum system control task. To learn a
satisfactory control strategy under the condition of limited resources, a
quantum system control method based on enhanced reinforcement learning
(QSC-ERL) is proposed. The states and actions in reinforcement learning are
mapped to quantum states and control operations in quantum systems. By using
new enhanced neural networks, reinforcement learning can quickly achieve the
maximization of long-term cumulative rewards, and a quantum state can be
evolved accurately from an initial state to a target state. According to the
number of candidate unitary operations, the three-switch control is used for
simulation experiments. Compared with other methods, the QSC-ERL achieves close
to 1 fidelity learning control of quantum systems, and takes fewer episodes to
quantum state evolution under the condition of limited resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03043">A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback. (arXiv:2310.03043v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jianghong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1">Joyce C. Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1">Eugene Agichtein</a></p>
<p>Interactive search can provide a better experience by incorporating
interaction feedback from the users. This can significantly improve search
accuracy as it helps avoid irrelevant information and captures the users'
search intents. Existing state-of-the-art (SOTA) systems use reinforcement
learning (RL) models to incorporate the interactions but focus on item-level
feedback, ignoring the fine-grained information found in sentence-level
feedback. Yet such feedback requires extensive RL action space exploration and
large amounts of annotated data. This work addresses these challenges by
proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based
models, the SOTA in natural language processing, to select crucial sentences
based on users' engagement and rank the items to obtain more satisfactory
responses. We also propose two mechanisms to better explore optimal actions.
DQrank further utilizes the experience replay mechanism in DQ to store the
feedback sentences to obtain a better initial ranking performance. We validate
the effectiveness of DQrank on three search datasets. The results show that
DQRank performs at least 12% better than the previous SOTA RL approaches. We
also conduct detailed ablation studies. The ablation results demonstrate that
each model component can efficiently extract and accumulate long-term
engagement effects from the users' sentence-level feedback. This structure
offers new technologies with promised performance to construct a search system
with sentence-level interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03046">EcoAssistant: Using LLM Assistant More Affordably and Accurately. (arXiv:2310.03046v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jieyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed H. Awadallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chi Wang</a></p>
<p>Today, users ask Large language models (LLMs) as assistants to answer queries
that require external knowledge; they ask about the weather in a specific city,
about stock prices, and even about where specific locations are within their
neighborhood. These queries require the LLM to produce code that invokes
external APIs to answer the user's question, yet LLMs rarely produce correct
code on the first try, requiring iterative code refinement upon execution
results. In addition, using LLM assistants to support high query volumes can be
expensive. In this work, we contribute a framework, EcoAssistant, that enables
LLMs to answer code-driven queries more affordably and accurately. EcoAssistant
contains three components. First, it allows the LLM assistants to converse with
an automatic code executor to iteratively refine code or to produce answers
based on the execution results. Second, we use a hierarchy of LLM assistants,
which attempts to answer the query with weaker, cheaper LLMs before backing off
to stronger, expensive ones. Third, we retrieve solutions from past successful
queries as in-context demonstrations to help subsequent queries. Empirically,
we show that EcoAssistant offers distinct advantages for affordability and
accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of
GPT-4's cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03051">How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1">Aman Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Potharaju_S/0/1/0/all/0/1">Srividya Pranavi Potharaju</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aditya Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+McKee_K/0/1/0/all/0/1">Kevin R. McKee</a>, <a href="http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1">Ari Holtzman</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1">Jay Pujara</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1">Aida Nematzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1">Shyam Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1">Manaal Faruqui</a></p>
<p>"Thinking is for Doing." Humans can infer other people's mental states from
observations--an ability called Theory-of-Mind (ToM)--and subsequently act
pragmatically on those inferences. Existing question answering benchmarks such
as ToMi ask models questions to make inferences about beliefs of characters in
a story, but do not test whether models can then use these inferences to guide
their actions. We propose a new evaluation paradigm for large language models
(LLMs): Thinking for Doing (T4D), which requires models to connect inferences
about others' mental states to actions in social scenarios. Experiments on T4D
demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking
characters' beliefs in stories, but they struggle to translate this capability
into strategic action. Our analysis reveals the core challenge for LLMs lies in
identifying the implicit inferences about mental states without being
explicitly asked about as in ToMi, that lead to choosing the correct action in
T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee
and Reflect (FaR), which provides a reasoning structure that encourages LLMs to
anticipate future challenges and reason about potential actions. FaR boosts
GPT-4's performance from 50% to 71% on T4D, outperforming other prompting
methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to
diverse out-of-distribution story structures and scenarios that also require
ToM inferences to choose an action, consistently outperforming other methods
including few-shot in-context learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03052">Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sangjun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1">JinYeong Bak</a></p>
<p>Transformers have demonstrated their success in various domains and tasks.
However, Transformers struggle with long input sequences due to their limited
capacity. While one solution is to increase input length, endlessly stretching
the length is unrealistic. Furthermore, humans selectively remember and use
only relevant information from inputs, unlike Transformers which process all
raw data from start to end. We introduce Memoria, a general memory network that
applies Hebbian theory which is a major theory explaining human memory
formulation to enhance long-term dependencies in neural networks. Memoria
stores and retrieves information called engram at multiple memory levels of
working memory, short-term memory, and long-term memory, using connection
weights that change according to Hebb's rule. Through experiments with popular
Transformer-based models like BERT and GPT, we present that Memoria
significantly improves the ability to consider long-term dependencies in
various tasks. Results show that Memoria outperformed existing methodologies in
sorting and language modeling, and long text classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03055">Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems. (arXiv:2310.03055v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1">Ruturaj Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1">Utkarsh Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kale_I/0/1/0/all/0/1">Ishaan Kale</a>, <a href="http://arxiv.org/find/cs/1/au:+Shastri_A/0/1/0/all/0/1">Apoorva Shastri</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1">Anand J Kulkarni</a></p>
<p>A modified LAB algorithm is introduced in this paper. It builds upon the
original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm
that models competitive and learning behaviours within a group, establishing
hierarchical roles. The proposed algorithm incorporates the roulette wheel
approach and a reduction factor introducing inter-group competition and
iteratively narrowing down the sample space. The algorithm is validated by
solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions
are validated using standard statistical tests such as two-sided and pairwise
signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited
improved and superior robustness as well as search space exploration
capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR)
method is proposed, making the algorithm capable to solve constrained problems.
The C-SSR method enables the algorithm to identify clusters of feasible
regions, satisfying the constraints and contributing to achieve the optimal
solution. This method demonstrates its effectiveness as a potential alternative
to traditional constraint handling techniques. The results obtained using the
Modified LAB algorithm are then compared with those achieved by other recent
metaheuristic algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Eric Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1">Ray Gu</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/EvenJoker/Point-PEFT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03084">Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1">Deniz Bayazit</a>, <a href="http://arxiv.org/find/cs/1/au:+Foroutan_N/0/1/0/all/0/1">Negar Foroutan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1">Gail Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1">Antoine Bosselut</a></p>
<p>Pretrained language models (LMs) encode implicit representations of knowledge
in their parameters. However, localizing these representations and
disentangling them from each other remains an open problem. In this work, we
investigate whether pretrained language models contain various
knowledge-critical subnetworks: particular sparse computational subgraphs
responsible for encoding specific knowledge the model has memorized. We propose
a multi-objective differentiable weight masking scheme to discover these
subnetworks and show that we can use them to precisely remove specific
knowledge from models while minimizing adverse effects on the behavior of the
original language model. We demonstrate our method on multiple GPT2 variants,
uncovering highly sparse subnetworks (98%+) that are solely responsible for
specific collections of relational knowledge. When these subnetworks are
removed, the remaining network maintains most of its initial capacity (modeling
language and other memorized relational knowledge) but struggles to express the
removed knowledge, and suffers performance drops on examples needing this
removed knowledge on downstream tasks after finetuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03086">Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook. (arXiv:2310.03086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Suresh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruparan_D/0/1/0/all/0/1">Dhanyashri Guruparan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aaron_P/0/1/0/all/0/1">Pavithren Aaron</a>, <a href="http://arxiv.org/find/cs/1/au:+Telajan_P/0/1/0/all/0/1">Philemon Telajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadevan_K/0/1/0/all/0/1">Kavinesh Mahadevan</a>, <a href="http://arxiv.org/find/cs/1/au:+Davagandhi_D/0/1/0/all/0/1">Dinesh Davagandhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_O/0/1/0/all/0/1">Ong Xin Yue</a></p>
<p>Deep learning has become a powerful tool in computational biology,
revolutionising the analysis and interpretation of biological data over time.
In our article review, we delve into various aspects of deep learning in
computational biology. Specifically, we examine its history, advantages, and
challenges. Our focus is on two primary applications: DNA sequence
classification and prediction, as well as protein structure prediction from
sequence data. Additionally, we provide insights into the outlook for this
field. To fully harness the potential of deep learning in computational
biology, it is crucial to address the challenges that come with it. These
challenges include the requirement for large, labelled datasets and the
interpretability of deep learning models. The use of deep learning in the
analysis of DNA sequences has brought about a significant transformation in the
detection of genomic variants and the analysis of gene expression. This has
greatly contributed to the advancement of personalised medicine and drug
discovery. Convolutional neural networks (CNNs) have been shown to be highly
accurate in predicting genetic variations and gene expression levels. Deep
learning techniques are used for analysing epigenetic data, including DNA
methylation and histone modifications. This provides valuable insights into
metabolic conditions and gene regulation. The field of protein structure
prediction has been significantly impacted by deep learning, which has enabled
accurate determination of the three-dimensional shape of proteins and
prediction of their interactions. The future of deep learning in computational
biology looks promising. With the development of advanced deep learning models
and interpretation techniques, there is potential to overcome current
challenges and further our understanding of biological systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03094">Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1">Murong Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Liang Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Ziyu Yao</a></p>
<p>Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the "answer consistency" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03123">Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zihao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yifan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xueqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>With the blowout development of pre-trained models (PTMs), the efficient
tuning of these models for diverse downstream applications has emerged as a
pivotal research concern. Although recent investigations into prompt tuning
have provided promising avenues, three salient challenges persist: (1) memory
constraint: the continuous growth in the size of open-source PTMs renders
fine-tuning, even a fraction of their parameters, challenging for many
practitioners. (2) model privacy: existing PTMs often function as public API
services, with their parameters inaccessible for effective or tailored
fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates
high-quality datasets, which are typically localized and not shared to public.
To optimally harness each local dataset while navigating memory constraints and
preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT).
This innovative approach eschews reliance on parameter architectures and
private dataset access, instead capitalizing on a central server that aids
local users in collaboratively training a prompt generator through regular
aggregation. Local users leverage API-driven learning via a zero-order
optimizer, obviating the need for PTM deployment. Relative to extensive
fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM
storage and fine-tuning on local machines, tapping into comprehensive,
high-quality, yet private training datasets. A thorough evaluation across 40
datasets spanning CV and NLP tasks underscores the robustness of our proposed
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03131">Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biradar_G/0/1/0/all/0/1">Gagan Biradar</a>, <a href="http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1">Yacine Izza</a>, <a href="http://arxiv.org/find/cs/1/au:+Lobo_E/0/1/0/all/0/1">Elita Lobo</a>, <a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1">Vignesh Viswanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1">Yair Zick</a></p>
<p>The recent criticisms of the robustness of post hoc model approximation
explanation methods (like LIME and SHAP) have led to the rise of model-precise
abductive explanations. For each data point, abductive explanations provide a
minimal subset of features that are sufficient to generate the outcome. While
theoretically sound and rigorous, abductive explanations suffer from a major
issue -- there can be several valid abductive explanations for the same data
point. In such cases, providing a single abductive explanation can be
insufficient; on the other hand, providing all valid abductive explanations can
be incomprehensible due to their size. In this work, we solve this issue by
aggregating the many possible abductive explanations into feature importance
scores. We propose three aggregation methods: two based on power indices from
cooperative game theory and a third based on a well-known measure of causal
strength. We characterize these three methods axiomatically, showing that each
of them uniquely satisfies a set of desirable properties. We also evaluate them
on multiple datasets and show that these explanations are robust to the attacks
that fool SHAP and LIME.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03149">Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jonathan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a></p>
<p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03158">Assessment of Prediction Intervals Using Uncertainty Characteristics Curves. (arXiv:2310.03158v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1">Jiri Navratil</a>, <a href="http://arxiv.org/find/cs/1/au:+Elder_B/0/1/0/all/0/1">Benjamin Elder</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1">Matthew Arnold</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Soumya Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1">Prasanna Sattigeri</a></p>
<p>Accurate quantification of model uncertainty has long been recognized as a
fundamental requirement for trusted AI. In regression tasks, uncertainty is
typically quantified using prediction intervals calibrated to an ad-hoc
operating point, making evaluation and comparison across different studies
relatively difficult. Our work leverages: (1) the concept of operating
characteristics curves and (2) the notion of a gain over a null reference, to
derive a novel operating point agnostic assessment methodology for prediction
intervals. The paper defines the Uncertainty Characteristics Curve and
demonstrates its utility in selected scenarios. We argue that the proposed
method addresses the current need for comprehensive assessment of prediction
intervals and thus represents a valuable addition to the uncertainty
quantification toolbox.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03161">Neural architecture impact on identifying temporally extended Reinforcement Learning tasks. (arXiv:2310.03161v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+George_V/0/1/0/all/0/1">Victor Vadakechirayath George</a></p>
<p>Inspired by recent developments in attention models for image classification
and natural language processing, we present various Attention based
architectures in reinforcement learning (RL) domain, capable of performing well
on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep
Reinforcement learning techniques in various fields like robotics, gaming and
healthcare, they suffer from a major drawback that neural networks are
difficult to interpret. We try to get around this problem with the help of
Attention based models. In Attention based models, extracting and overlaying of
attention map onto images allows for direct observation of information used by
agent to select actions and easier interpretation of logic behind the chosen
actions. Our models in addition to playing well on gym-Atari environments, also
provide insights on how agent perceives its environment. In addition, motivated
by recent developments in attention based video-classification models using
Vision Transformer, we come up with an architecture based on Vision
Transformer, for image-based RL domain too. Compared to previous works in
Vision Transformer, our model is faster to train and requires fewer
computational resources. 3
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03185">Misusing Tools in Large Language Models With Visual Adversarial Examples. (arXiv:2310.03185v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xiaohan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rajesh K. Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_N/0/1/0/all/0/1">Niloofar Mireshghallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1">Taylor Berg-Kirkpatrick</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1">Earlence Fernandes</a></p>
<p>Large Language Models (LLMs) are being enhanced with the ability to use tools
and to process multiple modalities. These new capabilities bring new benefits
and also new security risks. In this work, we show that an attacker can use
visual adversarial examples to cause attacker-desired tool usage. For example,
the attacker could cause a victim LLM to delete calendar events, leak private
conversations and book hotels. Different from prior work, our attacks can
affect the confidentiality and integrity of user resources connected to the LLM
while being stealthy and generalizable to multiple input prompts. We construct
these attacks using gradient-based adversarial training and characterize
performance along multiple dimensions. We find that our adversarial images can
manipulate the LLM to invoke tools following real-world syntax almost always
(~98%) while maintaining high similarity to clean images (~0.9 SSIM).
Furthermore, using human scoring and automated metrics, we find that the
attacks do not noticeably affect the conversation (and its semantics) between
the user and the LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03186">Inferring Inference. (arXiv:2310.03186v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Raju_R/0/1/0/all/0/1">Rajkumar Vasudeva Raju</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Linderman_S/0/1/0/all/0/1">Scott Linderman</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pitkow_X/0/1/0/all/0/1">Xaq Pitkow</a></p>
<p>Patterns of microcircuitry suggest that the brain has an array of repeated
canonical computational units. Yet neural representations are distributed, so
the relevant computations may only be related indirectly to single-neuron
transformations. It thus remains an open challenge how to define canonical
distributed computations. We integrate normative and algorithmic theories of
neural computation into a mathematical framework for inferring canonical
distributed computations from large-scale neural activity patterns. At the
normative level, we hypothesize that the brain creates a structured internal
model of its environment, positing latent causes that explain its sensory
inputs, and uses those sensory inputs to infer the latent causes. At the
algorithmic level, we propose that this inference process is a nonlinear
message-passing algorithm on a graph-structured model of the world. Given a
time series of neural activity during a perceptual inference task, our
framework finds (i) the neural representation of relevant latent variables,
(ii) interactions between these variables that define the brain's internal
model of the world, and (iii) message-functions specifying the inference
algorithm. These targeted computational properties are then statistically
distinguishable due to the symmetries inherent in any canonical computation, up
to a global transformation. As a demonstration, we simulate recordings for a
model brain that implicitly implements an approximate inference algorithm on a
probabilistic graphical model. Given its external inputs and noisy neural
activity, we recover the latent variables, their neural representation and
dynamics, and canonical message-functions. We highlight features of
experimental design needed to successfully extract canonical computations from
neural data. Overall, this framework provides a new tool for discovering
interpretable structure in neural recordings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03188">Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication. (arXiv:2310.03188v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingyun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1">Huan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bang An</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lichan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1">Ed H. Chi</a></p>
<p>Many recent breakthroughs in machine learning have been enabled by the
pre-trained foundation models. By scaling up model parameters, training data,
and computation resources, foundation models have significantly advanced the
state-of-the-art in many applications. However, it is still an open question of
how to use these models to perform downstream tasks efficiently. Knowledge
distillation (KD) has been explored to tackle this challenge. KD transfers
knowledge from a large teacher model to a smaller student model. While KD has
been successful in improving student model performance, recent research has
discovered that a powerful teacher does not necessarily lead to a powerful
student, due to their huge capacity gap. In addition, the potential
distribution shifts between the pre-training data and downstream tasks can make
knowledge transfer in KD sub-optimal for improving downstream task performance.
In this paper, we extend KD with an interactive communication process to help
students of downstream tasks learn effectively from pre-trained foundation
models. Our design is inspired by the way humans learn from teachers who can
explain knowledge in a way that meets the students' needs. Specifically, we let
each model (i.e., student and teacher) train two components: (1) an encoder
encoding the model's hidden states to a message and (2) a decoder decoding any
messages to its own hidden states. With encoder and decoder, not only can the
teacher transfer rich information by encoding its hidden states, but also the
student can send messages with information of downstream tasks to the teacher.
Therefore, knowledge passing from teacher to student can be tailored to the
student's capacity and downstream tasks' distributions. We conducted
experiments on benchmark datasets to show that our communication mechanism
outperforms state-of-the-art distillation techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03195">Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions. (arXiv:2310.03195v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khadivi_M/0/1/0/all/0/1">Maziyar Khadivi</a>, <a href="http://arxiv.org/find/cs/1/au:+Charter_T/0/1/0/all/0/1">Todd Charter</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaghoubi_M/0/1/0/all/0/1">Marjan Yaghoubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jalayer_M/0/1/0/all/0/1">Masoud Jalayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahang_M/0/1/0/all/0/1">Maryam Ahang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shojaeinasab_A/0/1/0/all/0/1">Ardeshir Shojaeinasab</a>, <a href="http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1">Homayoun Najjaran</a></p>
<p>Machine scheduling aims to optimize job assignments to machines while
adhering to manufacturing rules and job specifications. This optimization leads
to reduced operational costs, improved customer demand fulfillment, and
enhanced production efficiency. However, machine scheduling remains a
challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement
Learning (DRL), a key component of artificial general intelligence, has shown
promise in various domains like gaming and robotics. Researchers have explored
applying DRL to machine scheduling problems since 1995. This paper offers a
comprehensive review and comparison of DRL-based approaches, highlighting their
methodology, applications, advantages, and limitations. It categorizes these
approaches based on computational components: conventional neural networks,
encoder-decoder architectures, graph neural networks, and metaheuristic
algorithms. Our review concludes that DRL-based methods outperform exact
solvers, heuristics, and tabular reinforcement learning algorithms in terms of
computation speed and generating near-global optimal solutions. These DRL-based
approaches have been successfully applied to static and dynamic scheduling
across diverse machine environments and job characteristics. However, DRL-based
schedulers face limitations in handling complex operational constraints,
configurable multi-objective optimization, generalization, scalability,
interpretability, and robustness. Addressing these challenges will be a crucial
focus for future research in this field. This paper serves as a valuable
resource for researchers to assess the current state of DRL-based machine
scheduling and identify research gaps. It also aids experts and practitioners
in selecting the appropriate DRL approach for production scheduling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03205">A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1">Kim Youwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1">Lee Hyun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1">Kim Sung-Bin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1">Suekyeong Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1">Janghoon Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1">Tae-Hyun Oh</a></p>
<p>We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03211">On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1">Utsav Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1">Erhan Bas</a></p>
<p>Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03218">Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Peiyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yaxuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sirui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a></p>
<p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in the field of generative modeling due to its
flexibility in the formulation and strong modeling power of the latent space.
However, the common practice of learning latent space EBMs with non-convergent
short-run MCMC for prior and posterior sampling is hindering the model from
further progress; the degenerate MCMC sampling quality in practice often leads
to degraded generation quality and instability in training, especially with
highly multi-modal and/or high-dimensional target distributions. To remedy this
sampling issue, in this paper we introduce a simple but effective
diffusion-based amortization method for long-run MCMC sampling and develop a
novel learning algorithm for the latent space EBM based on it. We provide
theoretical evidence that the learned amortization of MCMC is a valid long-run
MCMC sampler. Experiments on several image modeling benchmark datasets
demonstrate the superior performance of our method compared with strong
counterparts
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03225">Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wachi_A/0/1/0/all/0/1">Akifumi Wachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_W/0/1/0/all/0/1">Wataru Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1">Kazumune Hashimoto</a></p>
<p>Safe exploration is essential for the practical use of reinforcement learning
(RL) in many real-world scenarios. In this paper, we present a generalized safe
exploration (GSE) problem as a unified formulation of common safe exploration
problems. We then propose a solution of the GSE problem in the form of a
meta-algorithm for safe exploration, MASE, which combines an unconstrained RL
algorithm with an uncertainty quantifier to guarantee safety in the current
episode while properly penalizing unsafe explorations before actual safety
violation to discourage them in future episodes. The advantage of MASE is that
we can optimize a policy while guaranteeing with a high probability that no
safety constraint will be violated under proper assumptions. Specifically, we
present two variants of MASE with different constructions of the uncertainty
quantifier: one based on generalized linear models with theoretical guarantees
of safety and near-optimality, and another that combines a Gaussian process to
ensure safety with a deep RL algorithm to maximize the reward. Finally, we
demonstrate that our proposed algorithm achieves better performance than
state-of-the-art algorithms on grid-world and Safety Gym benchmarks without
violating any safety constraints, even during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03232">Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity. (arXiv:2310.03232v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xinyang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Burkhardt_H/0/1/0/all/0/1">Hannah A Burkhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Arean_P/0/1/0/all/0/1">Patricia A Are&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Hull_T/0/1/0/all/0/1">Thomas D Hull</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1">Trevor Cohen</a></p>
<p>Prior work has shown that analyzing the use of first-person singular pronouns
can provide insight into individuals' mental status, especially depression
symptom severity. These findings were generated by counting frequencies of
first-person singular pronouns in text data. However, counting doesn't capture
how these pronouns are used. Recent advances in neural language modeling have
leveraged methods generating contextual embeddings. In this study, we sought to
utilize the embeddings of first-person pronouns obtained from contextualized
language representation models to capture ways these pronouns are used, to
analyze mental status. De-identified text messages sent during online
psychotherapy with weekly assessment of depression severity were used for
evaluation. Results indicate the advantage of contextualized first-person
pronoun embeddings over standard classification token embeddings and
frequency-based pronoun analysis results in predicting depression symptom
severity. This suggests contextual representations of first-person pronouns can
enhance the predictive utility of language used by people with depression
symptoms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03234">Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hu_Q/0/1/0/all/0/1">Quanqi Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_D/0/1/0/all/0/1">Dixian Zhu</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a></p>
<p>This paper investigates new families of compositional optimization problems,
called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf
w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf
c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC
FCCO). There has been a growing interest in FCCO due to its wide-ranging
applications in machine learning and AI, as well as its ability to address the
shortcomings of stochastic algorithms based on empirical risk minimization.
However, current research on FCCO presumes that both the inner and outer
functions are smooth, limiting their potential to tackle a more diverse set of
problems. Our research expands on this area by examining non-smooth
weakly-convex FCCO, where the outer function is weakly convex and
non-decreasing, and the inner function is weakly-convex. We analyze a
single-loop algorithm and establish its complexity for finding an
$\epsilon$-stationary point of the Moreau envelop of the objective function.
Additionally, we also extend the algorithm to solving novel non-smooth
weakly-convex tri-level finite-sum coupled compositional optimization problems,
which feature a nested arrangement of three functions. Lastly, we explore the
applications of our algorithms in deep learning for two-way partial AUC
maximization and multi-instance two-way partial AUC maximization, using
empirical studies to showcase the effectiveness of the proposed algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03243">Sparse Deep Learning for Time Series Data: Theory and Applications. (arXiv:2310.03243v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1">Mingxuan Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1">Yan Sun</a>, <a href="http://arxiv.org/find/stat/1/au:+Liang_F/0/1/0/all/0/1">Faming Liang</a></p>
<p>Sparse deep learning has become a popular technique for improving the
performance of deep neural networks in areas such as uncertainty
quantification, variable selection, and large-scale network compression.
However, most existing research has focused on problems where the observations
are independent and identically distributed (i.i.d.), and there has been little
work on the problems where the observations are dependent, such as time series
data and sequential data in natural language processing. This paper aims to
address this gap by studying the theory for sparse deep learning with dependent
data. We show that sparse recurrent neural networks (RNNs) can be consistently
estimated, and their predictions are asymptotically normally distributed under
appropriate assumptions, enabling the prediction uncertainty to be correctly
quantified. Our numerical results show that sparse deep learning outperforms
state-of-the-art methods, such as conformal predictions, in prediction
uncertainty quantification for time series data. Furthermore, our results
indicate that the proposed method can consistently identify the autoregressive
order for time series data and outperform existing methods in large-scale model
compression. Our proposed method has important practical implications in fields
such as finance, healthcare, and energy, where both accurate point estimates
and prediction uncertainty quantification are of concern.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03272">Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiashu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanatsoulis_C/0/1/0/all/0/1">Charilaos I. Kanatsoulis</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1">Alejandro Ribeiro</a></p>
<p>Network alignment is the task of establishing one-to-one correspondences
between the nodes of different graphs and finds a plethora of applications in
high-impact domains. However, this task is known to be NP-hard in its general
form, and existing algorithms do not scale up as the size of the graphs
increases. To tackle both challenges we propose a novel generalized graph
autoencoder architecture, designed to extract powerful and robust node
embeddings, that are tailored to the alignment task. We prove that the
generated embeddings are associated with the eigenvalues and eigenvectors of
the graphs and can achieve more accurate alignment compared to classical
spectral methods. Our proposed framework also leverages transfer learning and
data augmentation to achieve efficient network alignment at a very large scale
without retraining. Extensive experiments on both network and sub-network
alignment with real-world graphs provide corroborating evidence supporting the
effectiveness and scalability of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03281">A 5&#x27; UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1">Yanyi Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yupeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaixuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_L/0/1/0/all/0/1">Le Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jason Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengdi Wang</a></p>
<p>The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a
crucial role in regulating the translation process and impacts the protein
expression level. Language models have showcased their effectiveness in
decoding the functions of protein and genome sequences. Here, we introduced a
language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is
pre-trained on endogenous 5' UTRs from multiple species and is further
augmented with supervised information including secondary structure and minimum
free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The
model outperformed the best-known benchmark by up to 42% for predicting the
Mean Ribosome Loading, and by up to 60% for predicting the Translation
Efficiency and the mRNA Expression Level. The model also applies to identifying
unannotated Internal Ribosome Entry Sites within the untranslated region and
improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we
designed a library of 211 novel 5' UTRs with high predicted values of
translation efficiency and evaluated them via a wet-lab assay. Experiment
results confirmed that our top designs achieved a 32.5% increase in protein
production level relative to well-established 5' UTR optimized for
therapeutics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03292">SoK: Access Control Policy Generation from High-level Natural Language Requirements. (arXiv:2310.03292v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jayasundara_S/0/1/0/all/0/1">Sakuna Harinda Jayasundara</a>, <a href="http://arxiv.org/find/cs/1/au:+Arachchilage_N/0/1/0/all/0/1">Nalin Asanka Gamagedara Arachchilage</a>, <a href="http://arxiv.org/find/cs/1/au:+Russello_G/0/1/0/all/0/1">Giovanni Russello</a></p>
<p>Administrator-centered access control failures can cause data breaches,
putting organizations at risk of financial loss and reputation damage. Existing
graphical policy configuration tools and automated policy generation frameworks
attempt to help administrators configure and generate access control policies
by avoiding such failures. However, graphical policy configuration tools are
prone to human errors, making them unusable. On the other hand, automated
policy generation frameworks are prone to erroneous predictions, making them
unreliable. Therefore, to find ways to improve their usability and reliability,
we conducted a Systematic Literature Review analyzing 49 publications, to
identify those tools, frameworks, and their limitations. Identifying those
limitations will help develop effective access control policy generation
solutions while avoiding access control failures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03294">LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dacheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1">Rulin Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1">Anze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric P. Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xuezhe Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a></p>
<p>Increasing the context length of large language models (LLMs) unlocks
fundamentally new capabilities, but also significantly increases the memory
footprints of training. Previous model-parallel systems such as Megatron-LM
partition and compute different attention heads in parallel, resulting in large
communication volumes, so they cannot scale beyond the number of attention
heads, thereby hindering its adoption. In this paper, we introduce a new
approach, LightSeq, for long-context LLMs training. LightSeq has many notable
advantages. First, LightSeq partitions over the sequence dimension, hence is
agnostic to model architectures and readily applicable for models with varying
numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query
attention. Second, LightSeq not only requires up to 4.7x less communication
than Megatron-LM on popular LLMs but also overlaps the communication with
computation. To further reduce the training time, LightSeq features a novel
gradient checkpointing scheme to bypass an forward computation for
memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants
with sequence lengths from 32K to 512K. Through comprehensive experiments on
single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x
end-to-end speedup, and a 2-8x longer sequence length on models with fewer
heads, compared to Megatron-LM. Codes will be available at
https://github.com/RulinShao/LightSeq.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03302">Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vora_J/0/1/0/all/0/1">Jian Vora</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1">Jure Leskovec</a></p>
<p>Scientific experimentation involves an iterative process of creating
hypotheses, designing experiments, running experiments, and analyzing the
results. Can we build AI research agents to perform these long-horizon tasks?
To take a step towards building and evaluating research agents on such
open-ended decision-making tasks, we focus on the problem of machine learning
engineering: given a task description and a dataset, build a high-performing
model. In this paper, we propose MLAgentBench, a suite of ML tasks for
benchmarking AI research agents. Agents can perform actions like
reading/writing files, executing code, and inspecting outputs. With these
actions, agents could run experiments, analyze the results, and modify the code
of entire machine learning pipelines, such as data processing, architecture,
training processes, etc. The benchmark then automatically evaluates the agent's
performance objectively over various metrics related to performance and
efficiency. We also design an LLM-based research agent to automatically perform
experimentation loops in such an environment. Empirically, we find that a
GPT-4-based research agent can feasibly build compelling ML models over many
tasks in MLAgentBench, displaying highly interpretable plans and actions.
However, the success rates vary considerably; they span from almost 90\% on
well-established older datasets to as low as 10\% on recent Kaggle Challenges
-- unavailable during the LLM model's pretraining -- and even 0\% on newer
research challenges like BabyLM. Finally, we identify several key challenges
for LLM-based research agents such as long-term planning and hallucination. Our
code is released at https://github.com/snap-stanford/MLAgentBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03309">Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shaotian Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chen Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jieping Ye</a></p>
<p>Exploiting large language models (LLMs) to tackle deductive reasoning has
garnered growing attention. It still remains highly challenging to achieve
satisfactory results in complex deductive problems, characterized by plenty of
premises (i.e., facts or rules) entailing intricate relationships among
entities and requiring multi-hop reasoning. One intuitive solution is to
decompose the original task into smaller sub-tasks, and then chain the multiple
casual reasoning steps together in a forward (e.g., Selection-Inference) or
backward (e.g., LAMBADA) direction. However, these techniques inevitably
necessitate a large number of overall stages, leading to computationally
expensive operations and a higher possibility of making misleading steps. In
addition to stage-by-stage decomposition, we draw inspiration from another
aspect of human problem-solving. Humans tend to distill the most relevant
information and organize their thoughts systematically (e.g., creating mind
maps), which assists them in answering questions or drawing conclusions
precisely and quickly. In light of this, we propose a novel reasoning approach
named Concise and Organized Perception (COP). COP carefully analyzes the given
statements to efficiently identify the most pertinent information while
eliminating redundancy. It then prompts the LLMs in a more organized form that
adapts to the model's inference process. By perceiving concise and organized
proofs, the deductive reasoning abilities of LLMs can be better elicited, and
the risk of acquiring errors caused by excessive reasoning stages is mitigated.
Furthermore, our approach can be combined with the aforementioned ones to
further boost their performance. Extensive experimental results on three
popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD)
show that COP significantly outperforms previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03314">Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction. (arXiv:2310.03314v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1">Aadi Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tohme_T/0/1/0/all/0/1">Tony Tohme</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaotong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Youcef_Toumi_K/0/1/0/all/0/1">Kamal Youcef-Toumi</a></p>
<p>Human motion prediction is an essential step for efficient and safe
human-robot collaboration. Current methods either purely rely on representing
the human joints in some form of neural network-based architecture or use
regression models offline to fit hyper-parameters in the hope of capturing a
model encompassing human motion. While these methods provide good initial
results, they are missing out on leveraging well-studied human body kinematic
models as well as body and scene constraints which can help boost the efficacy
of these prediction frameworks while also explicitly avoiding implausible human
joint configurations. We propose a novel human motion prediction framework that
incorporates human joint constraints and scene constraints in a Gaussian
Process Regression (GPR) model to predict human motion over a set time horizon.
This formulation is combined with an online context-aware constraints model to
leverage task-dependent motions. It is tested on a human arm kinematic model
and implemented on a human-robot collaborative setup with a UR5 robot arm to
demonstrate the real-time capability of our approach. Simulations were also
performed on datasets like HA4M and ANDY. The simulation and experimental
results demonstrate considerable improvements in a Gaussian Process framework
when these constraints are explicitly considered.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03325">Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yilue Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Peiyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lifeng Fan</a></p>
<p>Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories and unseen object categories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03352">Tractable Bounding of Counterfactual Queries by Knowledge Compilation. (arXiv:2310.03352v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huber_D/0/1/0/all/0/1">David Huber</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yizuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonucci_A/0/1/0/all/0/1">Alessandro Antonucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Darwiche_A/0/1/0/all/0/1">Adnan Darwiche</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaffalon_M/0/1/0/all/0/1">Marco Zaffalon</a></p>
<p>We discuss the problem of bounding partially identifiable queries, such as
counterfactuals, in Pearlian structural causal models. A recently proposed
iterated EM scheme yields an inner approximation of those bounds by sampling
the initialisation parameters. Such a method requires multiple (Bayesian
network) queries over models sharing the same structural equations and
topology, but different exogenous probabilities. This setup makes a compilation
of the underlying model to an arithmetic circuit advantageous, thus inducing a
sizeable inferential speed-up. We show how a single symbolic knowledge
compilation allows us to obtain the circuit structure with symbolic parameters
to be replaced by their actual values when computing the different queries. We
also discuss parallelisation techniques to further speed up the bound
computation. Experiments against standard Bayesian network inference show clear
computational advantages with up to an order of magnitude of speed-up.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03353">Deep Geometric Learning with Monotonicity Constraints for Alzheimer&#x27;s Disease Progression. (arXiv:2310.03353v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Seungwoo Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Wonsik Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1">Junghyo Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Suk_H/0/1/0/all/0/1">Heung-Il Suk</a></p>
<p>Alzheimer's disease (AD) is a devastating neurodegenerative condition that
precedes progressive and irreversible dementia; thus, predicting its
progression over time is vital for clinical diagnosis and treatment. Numerous
studies have implemented structural magnetic resonance imaging (MRI) to model
AD progression, focusing on three integral aspects: (i) temporal variability,
(ii) incomplete observations, and (iii) temporal geometric characteristics.
However, deep learning-based approaches regarding data variability and sparsity
have yet to consider inherent geometrical properties sufficiently. The ordinary
differential equation-based geometric modeling method (ODE-RGRU) has recently
emerged as a promising strategy for modeling time-series data by intertwining a
recurrent neural network and an ODE in Riemannian space. Despite its
achievements, ODE-RGRU encounters limitations when extrapolating positive
definite symmetric metrics from incomplete samples, leading to feature reverse
occurrences that are particularly problematic, especially within the clinical
facet. Therefore, this study proposes a novel geometric learning approach that
models longitudinal MRI biomarkers and cognitive scores by combining three
modules: topological space shift, ODE-RGRU, and trajectory estimation. We have
also developed a training algorithm that integrates manifold mapping with
monotonicity constraints to reflect measurement transition irreversibility. We
verify our proposed method's efficacy by predicting clinical labels and
cognitive scores over time in regular and irregular settings. Furthermore, we
thoroughly analyze our proposed framework through an ablation study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03354">Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zelai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yancheng Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a></p>
<p>Self-play (SP) is a popular multi-agent reinforcement learning (MARL)
framework for solving competitive games, where each agent optimizes policy by
treating others as part of the environment. Despite the empirical successes,
the theoretical properties of SP-based methods are limited to two-player
zero-sum games. However, for mixed cooperative-competitive games where agents
on the same team need to cooperate with each other, we can show a simple
counter-example where SP-based methods cannot converge to a global Nash
equilibrium (NE) with high probability. Alternatively, Policy-Space Response
Oracles (PSRO) is an iterative framework for learning NE, where the best
responses w.r.t. previous policies are learned in each iteration. PSRO can be
directly extended to mixed cooperative-competitive settings by jointly learning
team best responses with all convergence properties unchanged. However, PSRO
requires repeatedly training joint policies from scratch till convergence,
which makes it hard to scale to complex games. In this work, we develop a novel
algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both
frameworks. FXP simultaneously trains an SP-based main policy and a counter
population of best response policies. The main policy is trained by fictitious
self-play and cross-play against the counter population, while the counter
policies are trained as the best responses to the main policy's past versions.
We validate our method in matrix games and show that FXP converges to global
NEs while SP methods fail. We also conduct experiments in a gridworld domain,
where FXP achieves higher Elo ratings and lower exploitabilities than
baselines, and a more challenging football game, where FXP defeats SOTA models
with over 94% win rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03358">Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1">Nuoyan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a></p>
<p>Deep neural networks are vulnerable to adversarial noise. Adversarial
training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two characteristics of robust
representation: (1) $\bf{exclusion}$: the feature of natural examples keeps
away from that of other classes; (2) $\bf{alignment}$: the feature of natural
and corresponding adversarial examples is close to each other. These motivate
us to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance. Code is
available at &lt;https://github.com/changzhang777/ANCRA&gt;.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03365">Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jafari_H/0/1/0/all/0/1">Hossein Jafari</a>, <a href="http://arxiv.org/find/eess/1/au:+Faez_K/0/1/0/all/0/1">Karim Faez</a>, <a href="http://arxiv.org/find/eess/1/au:+Amindavar_H/0/1/0/all/0/1">Hamidreza Amindavar</a></p>
<p>Lung cancer is highly lethal, emphasizing the critical need for early
detection. However, identifying lung nodules poses significant challenges for
radiologists, who rely heavily on their expertise and experience for accurate
diagnosis. To address this issue, computer-aided diagnosis systems based on
machine learning techniques have emerged to assist doctors in identifying lung
nodules from computed tomography (CT) scans. Unfortunately, existing networks
in this domain often suffer from computational complexity, leading to high
rates of false negatives and false positives, limiting their effectiveness. To
address these challenges, we present an innovative model that harnesses the
strengths of both convolutional neural networks and vision transformers.
Inspired by object detection in videos, we treat each 3D CT image as a video,
individual slices as frames, and lung nodules as objects, enabling a
time-series application. The primary objective of our work is to overcome
hardware limitations during model training, allowing for efficient processing
of 2D data while utilizing inter-slice information for accurate identification
based on 3D image context. We validated the proposed network by applying a
10-fold cross-validation technique to the publicly available Lung Nodule
Analysis 2016 dataset. Our proposed architecture achieves an average
sensitivity criterion of 97.84% and a competition performance metrics (CPM) of
96.0% with few parameters. Comparative analysis with state-of-the-art
advancements in lung nodule identification demonstrates the significant
accuracy achieved by our proposed model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03374">Design Optimizer for Planar Soft-Growing Robot Manipulators. (arXiv:2310.03374v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stroppa_F/0/1/0/all/0/1">Fabio Stroppa</a></p>
<p>Soft-growing robots are innovative devices that feature plant-inspired growth
to navigate environments. Thanks to their embodied intelligence of adapting to
their surroundings and the latest innovation in actuation and manufacturing, it
is possible to employ them for specific manipulation tasks. The applications of
these devices include exploration of delicate/dangerous environments,
manipulation of items, or assistance in domestic environments.
</p>
<p>This work presents a novel approach for design optimization of soft-growing
robots, which will be used prior to manufacturing to suggest engineers -- or
robot designer enthusiasts -- the optimal dimension of the robot to be built
for solving a specific task. I modeled the design process as a multi-objective
optimization problem, in which I optimize the kinematic chain of a soft
manipulator to reach targets and avoid unnecessary overuse of material and
resources. The method exploits the advantages of population-based optimization
algorithms, in particular evolutionary algorithms, to transform the problem
from multi-objective into a single-objective thanks to an efficient
mathematical formulation, the novel rank-partitioning algorithm, and obstacle
avoidance integrated within the optimizer operators.
</p>
<p>I tested the proposed method on different tasks to access its optimality,
which showed significant performance in solving the problem. Finally,
comparative experiments showed that the proposed method works better than the
one existing in the literature in terms of precision, resource consumption, and
run time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03376">Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rula_A/0/1/0/all/0/1">Anisa Rula</a>, <a href="http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1">Jennifer D&#x27;Souza</a></p>
<p>Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03392">Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review. (arXiv:2310.03392v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bach_T/0/1/0/all/0/1">Tita A. Bach</a>, <a href="http://arxiv.org/find/cs/1/au:+Kristiansen_J/0/1/0/all/0/1">Jenny K. Kristiansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Babic_A/0/1/0/all/0/1">Aleksandar Babic</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1">Alon Jacovi</a></p>
<p>Ensuring quality human-AI interaction (HAII) in safety-critical industries is
essential. Failure to do so can lead to catastrophic and deadly consequences.
Despite this urgency, what little research there is on HAII is fragmented and
inconsistent. We present here a survey of that literature and recommendations
for research best practices that will improve the field. We divided our
investigation into the following research areas: (1) terms used to describe
HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII,
and (4) how HAII is measured. Additionally, we described the capabilities and
maturity of the AI-enabled systems used in safety-critical industries discussed
in these articles. We found that no single term is used across the literature
to describe HAII and some terms have multiple meanings. According to our
literature, five factors influence HAII: user characteristics and background
(e.g., user personality, perceptions), AI interface and features (e.g.,
interactive UI design), AI output (e.g., accuracy, actionable recommendations),
explainability and interpretability (e.g., level of detail, user
understanding), and usage of AI (e.g., heterogeneity of environments and user
needs). HAII is most commonly measured with user-related subjective metrics
(e.g., user perception, trust, and attitudes), and AI-assisted decision-making
is the most common primary role of AI-enabled systems. Based on this review, we
conclude that there are substantial research gaps in HAII. Researchers and
developers need to codify HAII terminology, involve users throughout the AI
lifecycle (especially during development), and tailor HAII in safety-critical
industries to the users and environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03399">GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1">Taraneh Younesian</a>, <a href="http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1">Thiviyan Thanapalasingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krieken_E/0/1/0/all/0/1">Emile van Krieken</a>, <a href="http://arxiv.org/find/cs/1/au:+Daza_D/0/1/0/all/0/1">Daniel Daza</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloem_P/0/1/0/all/0/1">Peter Bloem</a></p>
<p>Graph neural networks (GNNs) learn the representation of nodes in a graph by
aggregating the neighborhood information in various ways. As these networks
grow in depth, their receptive field grows exponentially due to the increase in
neighborhood sizes, resulting in high memory costs. Graph sampling solves
memory issues in GNNs by sampling a small ratio of the nodes in the graph. This
way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed
sampling heuristics, which may not generalize to different structures or tasks.
We introduce GRAPES, an adaptive graph sampling method that learns to identify
sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet
to learn node sampling probabilities given the classification objectives. We
evaluate GRAPES across several small- and large-scale graph benchmarks and
demonstrate its effectiveness in accuracy and scalability. In contrast to
existing sampling methods, GRAPES maintains high accuracy even with small
sample sizes and, therefore, can scale to very large graphs. Our code is
publicly available at https://github.com/dfdazac/grapes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03419">Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Ling Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1">Moksh Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Madan_K/0/1/0/all/0/1">Kanika Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>Generative Flow Networks (GFlowNets) are amortized samplers that learn
stochastic policies to sequentially generate compositional objects from a given
unnormalized reward distribution. They can generate diverse sets of high-reward
objects, which is an important consideration in scientific discovery tasks.
However, as they are typically trained from a given extrinsic reward function,
it remains an important open challenge about how to leverage the power of
pre-training and train GFlowNets in an unsupervised fashion for efficient
adaptation to downstream tasks. Inspired by recent successes of unsupervised
pre-training in various domains, we introduce a novel approach for reward-free
pre-training of GFlowNets. By framing the training as a self-supervised
problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to
explore the candidate space. Specifically, OC-GFN learns to reach any targeted
outcomes, akin to goal-conditioned policies in reinforcement learning. We show
that the pre-trained OC-GFN model can allow for a direct extraction of a policy
capable of sampling from any new reward functions in downstream tasks.
Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an
intractable marginalization over possible outcomes. We propose a novel way to
approximate this marginalization by learning an amortized predictor enabling
efficient fine-tuning. Extensive experimental results validate the efficacy of
our approach, demonstrating the effectiveness of pre-training the OC-GFN, and
its ability to swiftly adapt to downstream tasks and discover modes more
efficiently. This work may serve as a foundation for further exploration of
pre-training strategies in the context of GFlowNets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03457">A Quantitatively Interpretable Model for Alzheimer&#x27;s Disease Prediction Using Deep Counterfactuals. (arXiv:2310.03457v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1">Kwanseok Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1">Da-Woon Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulyadi_A/0/1/0/all/0/1">Ahmad Wisnu Mulyadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Wonsik Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_E/0/1/0/all/0/1">Eunsong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kun Ho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Suk_H/0/1/0/all/0/1">Heung-Il Suk</a></p>
<p>Deep learning (DL) for predicting Alzheimer's disease (AD) has provided
timely intervention in disease progression yet still demands attentive
interpretability to explain how their DL models make definitive decisions.
Recently, counterfactual reasoning has gained increasing attention in medical
research because of its ability to provide a refined visual explanatory map.
However, such visual explanatory maps based on visual inspection alone are
insufficient unless we intuitively demonstrate their medical or neuroscientific
validity via quantitative features. In this study, we synthesize the
counterfactual-labeled structural MRIs using our proposed framework and
transform it into a gray matter density map to measure its volumetric changes
over the parcellated region of interest (ROI). We also devised a lightweight
linear classifier to boost the effectiveness of constructed ROIs, promoted
quantitative interpretation, and achieved comparable predictive performance to
DL methods. Throughout this, our framework produces an ``AD-relatedness index''
for each ROI and offers an intuitive understanding of brain status for an
individual patient and across patient groups with respect to AD progression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03477">Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation. (arXiv:2310.03477v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Remy_F/0/1/0/all/0/1">Fran&#xe7;ois Remy</a>, <a href="http://arxiv.org/find/cs/1/au:+Delobelle_P/0/1/0/all/0/1">Pieter Delobelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Berendt_B/0/1/0/all/0/1">Bettina Berendt</a>, <a href="http://arxiv.org/find/cs/1/au:+Demuynck_K/0/1/0/all/0/1">Kris Demuynck</a>, <a href="http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1">Thomas Demeester</a></p>
<p>Training monolingual language models for low and mid-resource languages is
made challenging by limited and often inadequate pretraining data. In this
study, we propose a novel model conversion strategy to address this issue,
adapting high-resources monolingual language models to a new target language.
By generalizing over a word translation dictionary encompassing both the source
and target languages, we map tokens from the target tokenizer to semantically
similar tokens from the source language tokenizer. This one-to-many token
mapping improves tremendously the initialization of the embedding table for the
target language. We conduct experiments to convert high-resource models to mid-
and low-resource languages, namely Dutch and Frisian. These converted models
achieve a new state-of-the-art performance on these languages across all sorts
of downstream tasks. By reducing significantly the amount of data and time
required for training state-of-the-art models, our novel model conversion
strategy has the potential to benefit many languages worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03494">How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcin_S/0/1/0/all/0/1">Samuel Garcin</a>, <a href="http://arxiv.org/find/cs/1/au:+Doran_J/0/1/0/all/0/1">James Doran</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shangmin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_C/0/1/0/all/0/1">Christopher G. Lucas</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a></p>
<p>A key limitation preventing the wider adoption of autonomous agents trained
via deep reinforcement learning (RL) is their limited ability to generalise to
new environments, even when these share similar characteristics with
environments encountered during training. In this work, we investigate how a
non-uniform sampling strategy of individual environment instances, or levels,
affects the zero-shot generalisation (ZSG) ability of RL agents, considering
two failure modes: overfitting and over-generalisation. As a first step, we
measure the mutual information (MI) between the agent's internal representation
and the set of training levels, which we find to be well-correlated to instance
overfitting. In contrast to uniform sampling, adaptive sampling strategies
prioritising levels based on their value loss are more effective at maintaining
lower MI, which provides a novel theoretical justification for this class of
techniques. We then turn our attention to unsupervised environment design (UED)
methods, which adaptively generate new training levels and minimise MI more
effectively than methods sampling from a fixed set. However, we find UED
methods significantly shift the training distribution, resulting in
over-generalisation and worse ZSG performance over the distribution of
interest. To prevent both instance overfitting and over-generalisation, we
introduce self-supervised environment design (SSED). SSED generates levels
using a variational autoencoder, effectively reducing MI while minimising the
shift with the distribution of interest, and leads to statistically significant
improvements in ZSG over fixed-set level sampling strategies and UED methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03518">Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations. (arXiv:2310.03518v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiachi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1">Guanting Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xiaoshuai Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zechen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shanglin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinzheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Keqing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1">Bo Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiran Xu</a></p>
<p>In real dialogue scenarios, as there are unknown input noises in the
utterances, existing supervised slot filling models often perform poorly in
practical applications. Even though there are some studies on noise-robust
models, these works are only evaluated on rule-based synthetic datasets, which
is limiting, making it difficult to promote the research of noise-robust
methods. In this paper, we introduce a noise robustness evaluation dataset
named Noise-SF for slot filling task. The proposed dataset contains five types
of human-annotated noise, and all those noises are exactly existed in real
extensive robust-training methods of slot filling into the proposed framework.
By conducting exhaustive empirical evaluation experiments on Noise-SF, we find
that baseline models have poor performance in robustness evaluation, and the
proposed framework can effectively improve the robustness of models. Based on
the empirical experimental results, we make some forward-looking suggestions to
fuel the research in this direction. Our dataset Noise-SF will be released at
https://github.com/dongguanting/Noise-SF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03579">Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems. (arXiv:2310.03579v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Trang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1">Alexander Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Madan_K/0/1/0/all/0/1">Kanika Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dianbo Liu</a></p>
<p>Understanding causal relationships within Gene Regulatory Networks (GRNs) is
essential for unraveling the gene interactions in cellular processes. However,
causal discovery in GRNs is a challenging problem for multiple reasons
including the existence of cyclic feedback loops and uncertainty that yields
diverse possible causal structures. Previous works in this area either ignore
cyclic dynamics (assume acyclic structure) or struggle with scalability. We
introduce Swift-DynGFN as a novel framework that enhances causal structure
learning in GRNs while addressing scalability concerns. Specifically,
Swift-DynGFN exploits gene-wise independence to boost parallelization and to
lower computational cost. Experiments on real single-cell RNA velocity and
synthetic GRN datasets showcase the advancement in learning causal structure in
GRNs and scalability in larger systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03581">Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End. (arXiv:2310.03581v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Frey_J/0/1/0/all/0/1">Jonas Frey</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudin_N/0/1/0/all/0/1">Nikita Rudin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattamala_M/0/1/0/all/0/1">Matias Mattamala</a>, <a href="http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1">Cesar Cadena</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1">Marco Hutter</a></p>
<p>Autonomous robots must navigate reliably in unknown environments even under
compromised exteroceptive perception, or perception failures. Such failures
often occur when harsh environments lead to degraded sensing, or when the
perception algorithm misinterprets the scene due to limited generalization. In
this paper, we model perception failures as invisible obstacles and pits, and
train a reinforcement learning (RL) based local navigation policy to guide our
legged robot. Unlike previous works relying on heuristics and anomaly detection
to update navigational information, we train our navigation policy to
reconstruct the environment information in the latent space from corrupted
perception and react to perception failures end-to-end. To this end, we
incorporate both proprioception and exteroception into our policy inputs,
thereby enabling the policy to sense collisions on different body parts and
pits, prompting corresponding reactions. We validate our approach in simulation
and on the real quadruped robot ANYmal running in real-time (&lt;10 ms CPU
inference). In a quantitative comparison with existing heuristic-based locally
reactive planners, our policy increases the success rate over 30% when facing
perception failures. Project Page: https://bit.ly/45NBTuh.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03605">FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Collyer_J/0/1/0/all/0/1">Josh Collyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1">Tim Watson</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillips_I/0/1/0/all/0/1">Iain Phillips</a></p>
<p>Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03613">Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xidong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jianhui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhengmian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aidong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a></p>
<p>The minimax problems arise throughout machine learning applications, ranging
from adversarial training and policy evaluation in reinforcement learning to
AUROC maximization. To address the large-scale data challenges across multiple
clients with communication-efficient distributed training, federated learning
(FL) is gaining popularity. Many optimization algorithms for minimax problems
have been developed in the centralized setting (\emph{i.e.} single-machine).
Nonetheless, the algorithm for minimax problems under FL is still
underexplored. In this paper, we study a class of federated nonconvex minimax
optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and
reduce existing complexity results for the most common minimax problems. For
nonconvex-concave problems, we propose FedSGDA+ and reduce the communication
complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and
nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known
sample complexity of $O(\kappa^{3} N^{-1}\varepsilon^{-3})$ and the best-known
communication complexity of $O(\kappa^{2}\varepsilon^{-2})$. FedSGDA-M is the
first algorithm to match the best sample complexity $O(\varepsilon^{-3})$
achieved by the single-machine method under the nonconvex-strongly-concave
setting. Extensive experimental results on fair classification and AUROC
maximization show the efficiency of our algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03620">PeaTMOSS: Mining Pre-Trained Models in Open-Source Software. (arXiv:2310.03620v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wenxin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_J/0/1/0/all/0/1">Jason Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasmin_J/0/1/0/all/0/1">Jerin Yasmin</a>, <a href="http://arxiv.org/find/cs/1/au:+Synovic_N/0/1/0/all/0/1">Nicholas Synovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Sashti_R/0/1/0/all/0/1">Rajeev Sashti</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sophie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1">George K. Thiruvathukal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1">James C. Davis</a></p>
<p>Developing and training deep learning models is expensive, so software
engineers have begun to reuse pre-trained deep learning models (PTMs) and
fine-tune them for downstream tasks. Despite the wide-spread use of PTMs, we
know little about the corresponding software engineering behaviors and
challenges.
</p>
<p>To enable the study of software engineering with PTMs, we present the
PeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS has
three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software
repositories that use PTMs, and (3) a mapping between PTMs and the projects
that use them. We challenge PeaTMOSS miners to discover software engineering
practices around PTMs. A demo and link to the full dataset are available at:
https://github.com/PurdueDualityLab/PeaTMOSS-Demos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03635">CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jiayuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuelin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>Building machines that can reason about physical events and their causal
relationships is crucial for flexible interaction with the physical world.
However, most existing physical and causal reasoning benchmarks are exclusively
based on synthetically generated events and synthetic natural language
descriptions of causal relationships. This design brings up two issues. First,
there is a lack of diversity in both event types and natural language
descriptions; second, causal relationships based on manually-defined heuristics
are different from human judgments. To address both shortcomings, we present
the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of
physical events with human labels. We employ two techniques to improve data
collection efficiency: first, a novel iterative event cloze task to elicit a
new representation of events in videos, which we term Causal Event Graphs
(CEGs); second, a data augmentation technique based on neural language
generative models. We convert the collected CEGs into questions and answers to
be consistent with prior work. Finally, we study a collection of baseline
approaches for CLEVRER-Humans question-answering, highlighting the great
challenges set forth by our benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03659">Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures. (arXiv:2310.03659v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Handler_T/0/1/0/all/0/1">Thorsten H&#xe4;ndler</a></p>
<p>Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03666">MapperGPT: Large Language Models for Linking and Mapping Entities. (arXiv:2310.03666v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Matentzoglu_N/0/1/0/all/0/1">Nicolas Matentzoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1">J. Harry Caufield</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_H/0/1/0/all/0/1">Harshad B. Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Reese_J/0/1/0/all/0/1">Justin T. Reese</a>, <a href="http://arxiv.org/find/cs/1/au:+Moxon_S/0/1/0/all/0/1">Sierra Moxon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyeongsik Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_N/0/1/0/all/0/1">Nomi L. Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1">Melissa A Haendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mungall_C/0/1/0/all/0/1">Christopher J. Mungall</a></p>
<p>Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
</p>
<p>Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
</p>
<p>We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03684">SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1">Alexander Robey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1">Eric Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1">Hamed Hassani</a>, <a href="http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1">George J. Pappas</a></p>
<p>Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03687">Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries. (arXiv:2310.03687v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ikram_Z/0/1/0/all/0/1">Zarif Ikram</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Ling Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dianbo Liu</a></p>
<p>Due to limited resources and fast economic growth, designing optimal
transportation road networks with traffic simulation and validation in a
cost-effective manner is vital for developing countries, where extensive manual
testing is expensive and often infeasible. Current rule-based road design
generators lack diversity, a key feature for design robustness. Generative Flow
Networks (GFlowNets) learn stochastic policies to sample from an unnormalized
reward distribution, thus generating high-quality solutions while preserving
their diversity. In this work, we formulate the problem of linking incident
roads to the circular junction of a roundabout by a Markov decision process,
and we leverage GFlowNets as the Junction-Art road generator. We compare our
method with related methods and our empirical results show that our method
achieves better diversity while preserving a high validity score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03693">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiangyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tinghao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1">Peter Henderson</a></p>
<p>Optimizing large language models (LLMs) for downstream use cases often
involves the customization of pre-trained LLMs through further fine-tuning.
Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5
Turbo on custom datasets also encourage this practice. But, what are the safety
costs associated with such custom fine-tuning? We note that while existing
safety alignment infrastructures can restrict harmful behaviors of LLMs at
inference time, they do not cover safety risks when fine-tuning privileges are
extended to end-users. Our red teaming studies find that the safety alignment
of LLMs can be compromised by fine-tuning with only a few adversarially
designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety
guardrails by fine-tuning it on only 10 such examples at a cost of less than
$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful
instructions. Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used datasets can
also inadvertently degrade the safety alignment of LLMs, though to a lesser
extent. These findings suggest that fine-tuning aligned LLMs introduces new
safety risks that current safety infrastructures fall short of addressing --
even if a model's initial safety alignment is impeccable, it is not necessarily
to be maintained after custom fine-tuning. We outline and critically analyze
potential mitigations and advocate for further research efforts toward
reinforcing safety protocols for the custom fine-tuning of aligned LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03708">Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanhui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jing Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiangyu Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Language models (LMs), despite aligning well with an average labeler through
reinforcement learning from human feedback (RLHF), may not universally suit
diverse human preferences. Recent approaches therefore opt for customization by
collecting multi-dimensional feedback and creating distinct rewards for each
dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored
to different preferences using multi-objective RL (MORL) with different reward
weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for
MORLHF with diverse and usually conflicting objectives. In this paper, we
present Multi-Objective Direct Preference Optimization (MODPO), an RL-free
algorithm that extends Direct Preference Optimization (DPO) for multiple
alignment objectives. Essentially, MODPO trains different LMs to represent
different collective reward models that combine all objectives with specific
weightings. With a simple cross-entropy loss, the LMs optimized against the
MODPO objective are analytically the exact solutions of the original MORLHF
objective. Empirical results in safety alignment and long-form question
answering confirm that MODPO matches or outperforms existing methods,
efficiently producing a Pareto-optimal set of LMs that cater to diverse
preferences with 3 times less computational resources compared with MORLHF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03710">Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Crispino_N/0/1/0/all/0/1">Nicholas Crispino</a>, <a href="http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1">Kyle Montgomery</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1">Fankun Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenguang Wang</a></p>
<p>We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03714">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1">Omar Khattab</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhvi_A/0/1/0/all/0/1">Arnav Singhvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_P/0/1/0/all/0/1">Paridhi Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1">Keshav Santhanam</a>, <a href="http://arxiv.org/find/cs/1/au:+Vardhamanan_S/0/1/0/all/0/1">Sri Vardhamanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Haq_S/0/1/0/all/0/1">Saiful Haq</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Ashutosh Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1">Thomas T. Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Moazam_H/0/1/0/all/0/1">Hanna Moazam</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_H/0/1/0/all/0/1">Heather Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei Zaharia</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a></p>
<p>The ML community is rapidly exploring techniques for prompting language
models (LMs) and for stacking them into pipelines that solve complex tasks.
Unfortunately, existing LM pipelines are typically implemented using hard-coded
"prompt templates", i.e. lengthy strings discovered via trial and error. Toward
a more systematic approach for developing and optimizing LM pipelines, we
introduce DSPy, a programming model that abstracts LM pipelines as text
transformation graphs, i.e. imperative computational graphs where LMs are
invoked through declarative modules. DSPy modules are parameterized, meaning
they can learn (by creating and collecting demonstrations) how to apply
compositions of prompting, finetuning, augmentation, and reasoning techniques.
We design a compiler that will optimize any DSPy pipeline to maximize a given
metric. We conduct two case studies, showing that succinct DSPy programs can
express and optimize sophisticated LM pipelines that reason about math word
problems, tackle multi-hop retrieval, answer complex questions, and control
agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and
llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot
prompting (generally by over 25% and 65%, respectively) and pipelines with
expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top
of that, DSPy programs compiled to open and relatively small LMs like
770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely
on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at
https://github.com/stanfordnlp/dspy
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03715">Artificial Intelligence Index Report 2023. (arXiv:2310.03715v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maslej_N/0/1/0/all/0/1">Nestor Maslej</a>, <a href="http://arxiv.org/find/cs/1/au:+Fattorini_L/0/1/0/all/0/1">Loredana Fattorini</a>, <a href="http://arxiv.org/find/cs/1/au:+Brynjolfsson_E/0/1/0/all/0/1">Erik Brynjolfsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Etchemendy_J/0/1/0/all/0/1">John Etchemendy</a>, <a href="http://arxiv.org/find/cs/1/au:+Ligett_K/0/1/0/all/0/1">Katrina Ligett</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1">Terah Lyons</a>, <a href="http://arxiv.org/find/cs/1/au:+Manyika_J/0/1/0/all/0/1">James Manyika</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1">Helen Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1">Juan Carlos Niebles</a>, <a href="http://arxiv.org/find/cs/1/au:+Parli_V/0/1/0/all/0/1">Vanessa Parli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1">Yoav Shoham</a>, <a href="http://arxiv.org/find/cs/1/au:+Wald_R/0/1/0/all/0/1">Russell Wald</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">Jack Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrault_R/0/1/0/all/0/1">Raymond Perrault</a></p>
<p>Welcome to the sixth edition of the AI Index Report. This year, the report
introduces more original data than any previous edition, including a new
chapter on AI public opinion, a more thorough technical performance chapter,
original analysis about large language and multimodal models, detailed trends
in global AI legislation records, a study of the environmental impact of AI
systems, and more. The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Our mission is to provide
unbiased, rigorously vetted, broadly sourced data in order for policymakers,
researchers, executives, journalists, and the general public to develop a more
thorough and nuanced understanding of the complex field of AI. The report aims
to be the world's most credible and authoritative source for data and insights
about AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03718">Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yihang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cen_Z/0/1/0/all/0/1">Zhepeng Cen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiacheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tingnan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a></p>
<p>Safe reinforcement learning (RL) focuses on training reward-maximizing agents
subject to pre-defined safety constraints. Yet, learning versatile safe
policies that can adapt to varying safety constraint requirements during
deployment without retraining remains a largely unexplored and challenging
area. In this work, we formulate the versatile safe RL problem and consider two
primary requirements: training efficiency and zero-shot adaptation capability.
To address them, we introduce the Conditioned Constrained Policy Optimization
(CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation
(VVE) for approximating value functions under unseen threshold conditions, and
(2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint
thresholds during policy optimization. Our extensive experiments demonstrate
that CCPO outperforms the baselines in terms of safety and task performance
while preserving zero-shot adaptation capabilities to different constraint
thresholds data-efficiently. This makes our approach suitable for real-world
dynamic applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03731">MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Houxing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Aojun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zimu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Sichun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weikang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Linqi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1">Mingjie Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>The recently released GPT-4 Code Interpreter has demonstrated remarkable
proficiency in solving challenging math problems, primarily attributed to its
ability to seamlessly reason with natural language, generate code, execute
code, and continue reasoning based on the execution output. In this paper, we
present a method to fine-tune open-source language models, enabling them to use
code for modeling and deriving math equations and, consequently, enhancing
their mathematical reasoning abilities. We propose a method of generating novel
and high-quality datasets with math problems and their code-based solutions,
referred to as MathCodeInstruct. Each solution interleaves natural language,
code, and execution results. We also introduce a customized supervised
fine-tuning and inference approach. This approach yields the MathCoder models,
a family of models capable of generating code-based solutions for solving
challenging math problems. Impressively, the MathCoder models achieve
state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K
(83.9%) datasets, substantially outperforming other open-source alternatives.
Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K
and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The
dataset and models will be released at https://github.com/mathllm/MathCoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.00696">Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms. (arXiv:2102.00696v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tekin_S/0/1/0/all/0/1">Selim Furkan Tekin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazla_A/0/1/0/all/0/1">Arda Fazla</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozat_S/0/1/0/all/0/1">Suleyman Serdar Kozat</a></p>
<p>Numerical weather forecasting using high-resolution physical models often
requires extensive computational resources on supercomputers, which diminishes
their wide usage in most real-life applications. As a remedy, applying deep
learning methods has revealed innovative solutions within this field. To this
end, we introduce a novel deep learning architecture for forecasting
high-resolution spatio-temporal weather data. Our approach extends the
conventional encoder-decoder structure by integrating Convolutional Long-short
Term Memory and Convolutional Neural Networks. In addition, we incorporate
attention and context matcher mechanisms into the model architecture. Our
Weather Model achieves significant performance improvements compared to
baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our
experimental evaluation involves high-scale, real-world benchmark numerical
weather datasets, namely the ERA5 hourly dataset on pressure levels and
WeatherBench. Our results demonstrate substantial improvements in identifying
spatial and temporal correlations with attention matrices focusing on distinct
parts of the input series to model atmospheric circulations. We also compare
our model with high-resolution physical models using the benchmark metrics and
show that our Weather Model is accurate and easy to interpret.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.07454">Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Renanse_A/0/1/0/all/0/1">Animesh Renanse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Alok Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1">Rohitash Chandra</a></p>
<p>It is well known that canonical recurrent neural networks (RNNs) face
limitations in learning long-term dependencies which have been addressed by
memory structures in long short-term memory (LSTM) networks. Neural Turing
machines (NTMs) are novel RNNs that implement the notion of programmable
computers with neural network controllers that can learn simple algorithmic
tasks. Matrix neural networks feature matrix representation which inherently
preserves the spatial structure of data when compared to canonical neural
networks that use vector-based representation. One may then argue that neural
networks with matrix representations may have the potential to provide better
memory capacity. In this paper, we define and study a probabilistic notion of
memory capacity based on Fisher information for matrix-based RNNs. We find
bounds on memory capacity for such networks under various hypotheses and
compare them with their vector counterparts. In particular, we show that the
memory capacity of such networks is bounded by $N^2$ for $N\times N$ state
matrix which generalizes the one known for vector networks. We also show and
analyze the increase in memory capacity for such networks which is introduced
when one exhibits an external state memory, such as NTMs. Consequently, we
construct NTMs with RNN controllers with matrix-based representation of
external memory, leading us to introduce Matrix NTMs. We demonstrate the
performance of this class of memory networks under certain algorithmic learning
tasks such as copying and recall and compare it with Matrix RNNs. We find an
improvement in the performance of Matrix NTMs by the addition of external
memory, in comparison to Matrix RNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.05641">SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network. (arXiv:2108.05641v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinpeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xudong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Senzhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1">Kaimin Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jiaqi Ji</a></p>
<p>The Session-Based Recommendation System aims to predict the user's next click
based on their previous session sequence. The current studies generally learn
user preferences according to the transitions of items in the user's session
sequence. However, other effective information in the session sequence, such as
user profiles, are largely ignored which may lead to the model unable to learn
the user's specific preferences. In this paper, we propose SR-HetGNN, a novel
session recommendation method that uses a heterogeneous graph neural network
(HetGNN) to learn session embeddings and capture the specific preferences of
anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs
containing various types of nodes according to the session sequence, which can
capture the dependencies among items, users, and sessions. Second, HetGNN
captures the complex transitions between items and learns the item embeddings
containing user information. Finally, local and global session embeddings are
combined with the attentional network to obtain the final session embedding,
considering the influence of users' long and short-term preferences. SR-HetGNN
is shown to be superior to the existing state-of-the-art session-based
recommendation methods through extensive experiments over two real large
datasets Diginetica and Tmall.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.14883">Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shenggui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1">Zhengda Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiarui Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haichen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yang You</a></p>
<p>The success of Transformer models has pushed the deep learning model scale to
billions of parameters. Due to the limited memory resource of a single GPU,
However, the best practice for choosing the optimal parallel strategy is still
lacking, since it requires domain expertise in both deep learning and parallel
computing.
</p>
<p>The Colossal-AI system addressed the above challenge by introducing a unified
interface to scale your sequential code of model training to distributed
environments. It supports parallel training methods such as data, pipeline,
tensor, and sequence parallelism, as well as heterogeneous training methods
integrated with zero redundancy optimizer. Compared to the baseline system,
Colossal-AI can achieve up to 2.76 times training speedup on large-scale
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08417">Characterization of causal ancestral graphs for time series with latent confounders. (arXiv:2112.08417v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gerhardus_A/0/1/0/all/0/1">Andreas Gerhardus</a></p>
<p>In this paper, we introduce a novel class of graphical models for
representing time lag specific causal relationships and independencies of
multivariate time series with unobserved confounders. We completely
characterize these graphs and show that they constitute proper subsets of the
currently employed model classes. As we show, from the novel graphs one can
thus draw stronger causal inferences -- without additional assumptions. We
further introduce a graphical representation of Markov equivalence classes of
the novel graphs. This graphical representation contains more causal knowledge
than what current state-of-the-art causal discovery algorithms learn.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.09573">Diversity in deep generative models and generative AI. (arXiv:2202.09573v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turinici_G/0/1/0/all/0/1">Gabriel Turinici</a></p>
<p>The decoder-based machine learning generative algorithms such as Generative
Adversarial Networks (GAN), Variational Auto-Encoders (VAE), Transformers show
impressive results when constructing objects similar to those in a training
ensemble. However, the generation of new objects builds mainly on the
understanding of the hidden structure of the training dataset followed by a
sampling from a multi-dimensional normal variable. In particular each sample is
independent from the others and can repeatedly propose same type of objects. To
cure this drawback we introduce a kernel-based measure quantization method that
can produce new objects from a given target measure by approximating it as a
whole and even staying away from elements already drawn from that distribution.
This ensures a better diversity of the produced objects. The method is tested
on classic machine learning benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.05250">Spatial-temporal associations representation and application for process monitoring using graph convolution neural network. (arXiv:2205.05250v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Hao Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaojun Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chunhua Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhiwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_W/0/1/0/all/0/1">Weihua Gui</a></p>
<p>Thank you very much for the attention and concern of colleagues and scholars
in this work. With the comments and guidance of experts, editors, and
reviewers, this work has been accepted for publishing in the journal "Process
Safety and Environmental Protection". The theme of this paper relies on the
Spatial-temporal associations of numerous variables in the same industrial
processes, which refers to numerous variables obtained in dynamic industrial
processes with Spatial-temporal correlation characteristics, i.e., these
variables are not only highly correlated in time but also interrelated in
space. To handle this problem, three key issues need to be well addressed:
variable characteristics modeling and representation, graph network
construction (temporal information), and graph characteristics perception. The
first issue is implemented by assuming the data follows one improved Gaussian
distribution, while the graph network can be defined by the monitoring
variables and their edges which are calculated by their characteristics in
time. Finally, these networks corresponding to process states at different
times are fed into a graph convolutional neural network to implement graph
classification to achieve process monitoring. A benchmark experiment (Tennessee
Eastman chemical process) and one application study (cobalt purification from
zinc solution) are employed to demonstrate the feasibility and applicability of
this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12266">Decoding speech perception from non-invasive brain recordings. (arXiv:2208.12266v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Defossez_A/0/1/0/all/0/1">Alexandre D&#xe9;fossez</a>, <a href="http://arxiv.org/find/eess/1/au:+Caucheteux_C/0/1/0/all/0/1">Charlotte Caucheteux</a>, <a href="http://arxiv.org/find/eess/1/au:+Rapin_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Rapin</a>, <a href="http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1">Ori Kabeli</a>, <a href="http://arxiv.org/find/eess/1/au:+King_J/0/1/0/all/0/1">Jean-R&#xe9;mi King</a></p>
<p>Decoding speech from brain activity is a long-awaited goal in both healthcare
and neuroscience. Invasive devices have recently led to major milestones in
that regard: deep learning algorithms trained on intracranial recordings now
start to decode elementary linguistic features (e.g. letters, words,
spectrograms). However, extending this approach to natural speech and
non-invasive brain recordings remains a major challenge. Here, we introduce a
model trained with contrastive-learning to decode self-supervised
representations of perceived speech from the non-invasive recordings of a large
cohort of healthy individuals. To evaluate this approach, we curate and
integrate four public datasets, encompassing 175 volunteers recorded with
magneto- or electro-encephalography (M/EEG), while they listened to short
stories and isolated sentences. The results show that our model can identify,
from 3 seconds of MEG signals, the corresponding speech segment with up to 41%
accuracy out of more than 1,000 distinct possibilities on average across
participants, and more than 80% in the very best participants - a performance
that allows the decoding of words and phrases absent from the training set. The
comparison of our model to a variety of baselines highlights the importance of
(i) a contrastive objective, (ii) pretrained representations of speech and
(iii) a common convolutional architecture simultaneously trained across
multiple participants. Finally, the analysis of the decoder's predictions
suggests that they primarily depend on lexical and contextual semantic
representations. Overall, this effective decoding of perceived speech from
non-invasive recordings delineates a promising path to decode language from
brain activity, without putting patients at risk for brain surgery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.12148">Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection. (arXiv:2209.12148v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1">Neelu Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1">Nicolae-Catalin Ristea</a>, <a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1">Radu Tudor Ionescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1">Kamal Nasrollahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1">Thomas B. Moeslund</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a></p>
<p>Anomaly detection has recently gained increasing attention in the field of
computer vision, likely due to its broad set of applications ranging from
product fault detection on industrial production lines and impending event
detection in video surveillance to finding lesions in medical scans. Regardless
of the domain, anomaly detection is typically framed as a one-class
classification task, where the learning is conducted on normal examples only.
An entire family of successful anomaly detection methods is based on learning
to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and
exerting the magnitude of the reconstruction error as an indicator for the
abnormality level. Unlike other reconstruction-based methods, we present a
novel self-supervised masked convolutional transformer block (SSMCTB) that
comprises the reconstruction-based functionality at a core architectural level.
The proposed self-supervised block is extremely flexible, enabling information
masking at any layer of a neural network and being compatible with a wide range
of neural architectures. In this work, we extend our previous self-supervised
predictive convolutional attentive block (SSPCAB) with a 3D masked
convolutional layer, a transformer for channel-wise attention, as well as a
novel self-supervised objective based on Huber loss. Furthermore, we show that
our block is applicable to a wider variety of tasks, adding anomaly detection
in medical images and thermal videos to the previously considered tasks based
on RGB images and surveillance videos. We exhibit the generality and
flexibility of SSMCTB by integrating it into multiple state-of-the-art neural
models for anomaly detection, bringing forth empirical results that confirm
considerable performance improvements on five benchmarks. We release our code
and data as open source at: https://github.com/ristea/ssmctb.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.17505">Space-Fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v4 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Casadei_R/0/1/0/all/0/1">Roberto Casadei</a>, <a href="http://arxiv.org/find/cs/1/au:+Mariani_S/0/1/0/all/0/1">Stefano Mariani</a>, <a href="http://arxiv.org/find/cs/1/au:+Pianini_D/0/1/0/all/0/1">Danilo Pianini</a>, <a href="http://arxiv.org/find/cs/1/au:+Viroli_M/0/1/0/all/0/1">Mirko Viroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambonelli_F/0/1/0/all/0/1">Franco Zambonelli</a></p>
<p>A recurrent task in coordinated systems is managing (estimating, predicting,
or controlling) signals that vary in space, such as distributed sensed data or
computation outcomes. Especially in large-scale settings, the problem can be
addressed through decentralised and situated computing systems: nodes can
locally sense, process, and act upon signals, and coordinate with neighbours to
implement collective strategies. Accordingly, in this work we devise
distributed coordination strategies for the estimation of a spatial phenomenon
through collaborative adaptive sampling. Our design is based on the idea of
dynamically partitioning space into regions that compete and grow/shrink to
provide accurate aggregate sampling. Such regions hence define a sort of
virtualised space that is "fluid", since its structure adapts in response to
pressure forces exerted by the underlying phenomenon. We provide an adaptive
sampling algorithm in the field-based coordination framework, and prove it is
self-stabilising and locally optimal. Finally, we verify by simulation that the
proposed algorithm effectively carries out a spatially adaptive sampling while
maintaining a tuneable trade-off between accuracy and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07091">BiViT: Extremely Compressed Binary Vision Transformer. (arXiv:2211.07091v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yefei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Z/0/1/0/all/0/1">Zhenyu Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Luoming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Model binarization can significantly compress model size, reduce energy
consumption, and accelerate inference through efficient bit-wise operations.
Although binarizing convolutional neural networks have been extensively
studied, there is little work on exploring binarization of vision Transformers
which underpin most recent breakthroughs in visual recognition. To this end, we
propose to solve two fundamental challenges to push the horizon of Binary
Vision Transformers (BiViT). First, the traditional binary method does not take
the long-tailed distribution of softmax attention into consideration, bringing
large binarization errors in the attention module. To solve this, we propose
Softmax-aware Binarization, which dynamically adapts to the data distribution
and reduces the error caused by binarization. Second, to better preserve the
information of the pretrained model and restore accuracy, we propose a
Cross-layer Binarization scheme that decouples the binarization of
self-attention and multi-layer perceptrons (MLPs), and Parameterized Weight
Scales which introduce learnable scaling factors for weight binarization.
Overall, our method performs favorably against state-of-the-arts by 19.8% on
the TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6%
Top-1 accuracy over Swin-S model. Additionally, on COCO object detection, our
method achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask R-CNN
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13118">Decision Diagram-Based Branch-and-Bound with Caching for Dominance and Suboptimality Detection. (arXiv:2211.13118v3 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coppe_V/0/1/0/all/0/1">Vianney Copp&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gillard_X/0/1/0/all/0/1">Xavier Gillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaus_P/0/1/0/all/0/1">Pierre Schaus</a></p>
<p>The branch-and-bound algorithm based on decision diagrams introduced by
Bergman et al. in 2016 is a framework for solving discrete optimization
problems with a dynamic programming formulation. It works by compiling a series
of bounded-width decision diagrams that can provide lower and upper bounds for
any given subproblem. Eventually, every part of the search space will be either
explored or pruned by the algorithm, thus proving optimality. This paper
presents new ingredients to speed up the search by exploiting the structure of
dynamic programming models. The key idea is to prevent the repeated expansion
of nodes corresponding to the same dynamic programming states by querying
expansion thresholds cached throughout the search. These thresholds are based
on dominance relations between partial solutions previously found and on the
pruning inequalities of the filtering techniques introduced by Gillard et al.
in 2021. Computational experiments show that the pruning brought by this
caching mechanism allows significantly reducing the number of nodes expanded by
the algorithm. This results in more benchmark instances of difficult
optimization problems being solved in less time while using narrower decision
diagrams.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02648">Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moayeri_M/0/1/0/all/0/1">Mazda Moayeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>We present a simple but effective method to measure and mitigate model biases
caused by reliance on spurious cues. Instead of requiring costly changes to
one's data or model training, our method better utilizes the data one already
has by sorting them. Specifically, we rank images within their classes based on
spuriosity (the degree to which common spurious cues are present), proxied via
deep neural features of an interpretable network. With spuriosity rankings, it
is easy to identify minority subpopulations (i.e. low spuriosity images) and
assess model bias as the gap in accuracy between high and low spuriosity
images. One can even efficiently remove a model's bias at little cost to
accuracy by finetuning its classification head on low spuriosity images,
resulting in fairer treatment of samples regardless of spuriosity. We
demonstrate our method on ImageNet, annotating $5000$ class-feature
dependencies ($630$ of which we find to be spurious) and generating a dataset
of $325k$ soft segmentations for these features along the way. Having computed
spuriosity rankings via the identified spurious neural features, we assess
biases for $89$ diverse models and find that class-wise biases are highly
correlated across models. Our results suggest that model bias due to spurious
feature reliance is influenced far more by what the model is trained on than
how it is trained.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04554">Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis. (arXiv:2301.04554v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1">Benedetta Tondi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1">Mauro Barni</a></p>
<p>We propose a Universal Defence against backdoor attacks based on Clustering
and Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a
Deep Neural Network model is subject to a backdoor attack by inspecting the
training dataset. CCA-UD first clusters the samples of the training set by
means of density-based clustering. Then, it applies a novel strategy to detect
the presence of poisoned clusters. The proposed strategy is based on a general
misclassification behaviour observed when the features of a representative
example of the analysed cluster are added to benign samples. The capability of
inducing a misclassification error is a general characteristic of poisoned
samples, hence the proposed defence is attack-agnostic. This marks a
significant difference with respect to existing defences, that, either can
defend against only some types of backdoor attacks, or are effective only when
some conditions on the poisoning ratio or the kind of triggering signal used by
the attacker are satisfied.
</p>
<p>Experiments carried out on several classification tasks and network
architectures, considering different types of backdoor attacks (with either
clean or corrupted labels), and triggering signals, including both global and
local triggering signals, as well as sample-specific and source-specific
triggers, reveal that the proposed method is very effective to defend against
backdoor attacks in all the cases, always outperforming the state of the art
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06421">AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents. (arXiv:2301.06421v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pei-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tielman_M/0/1/0/all/0/1">Myrthe L. Tielman</a>, <a href="http://arxiv.org/find/cs/1/au:+Heylen_D/0/1/0/all/0/1">Dirk K.J. Heylen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1">Catholijn M. Jonker</a>, <a href="http://arxiv.org/find/cs/1/au:+Riemsdijk_M/0/1/0/all/0/1">M. Birna van Riemsdijk</a></p>
<p>AI alignment is about ensuring AI systems only pursue goals and activities
that are beneficial to humans. Most of the current approach to AI alignment is
to learn what humans value from their behavioural data. This paper proposes a
different way of looking at the notion of alignment, namely by introducing AI
Alignment Dialogues: dialogues with which users and agents try to achieve and
maintain alignment via interaction. We argue that alignment dialogues have a
number of advantages in comparison to data-driven approaches, especially for
behaviour support agents, which aim to support users in achieving their desired
future behaviours rather than their current behaviours. The advantages of
alignment dialogues include allowing the users to directly convey higher-level
concepts to the agent, and making the agent more transparent and trustworthy.
In this paper we outline the concept and high-level structure of alignment
dialogues. Moreover, we conducted a qualitative focus group user study from
which we developed a model that describes how alignment dialogues affect users,
and created design suggestions for AI alignment dialogues. Through this we
establish foundations for AI alignment dialogues and shed light on what
requires further development and research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04054">Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1">Michael Hagmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Meier_P/0/1/0/all/0/1">Philipp Meier</a>, <a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1">Stefan Riezler</a></p>
<p>Reliability of machine learning evaluation -- the consistency of observed
evaluation scores across replicated model training runs -- is affected by
several sources of nondeterminism which can be regarded as measurement noise.
Current tendencies to remove noise in order to enforce reproducibility of
research results neglect inherent nondeterminism at the implementation level
and disregard crucial interaction effects between algorithmic noise factors and
data properties. This limits the scope of conclusions that can be drawn from
such experiments. Instead of removing noise, we propose to incorporate several
sources of variance, including their interaction with data properties, into an
analysis of significance and reliability of machine learning evaluation, with
the aim to draw inferences beyond particular instances of trained models. We
show how to use linear mixed effects models (LMEMs) to analyze performance
evaluation scores, and to conduct statistical inference with a generalized
likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources
of noise like meta-parameter variations into statistical significance testing,
and to assess performance differences conditional on data properties.
Furthermore, a variance component analysis (VCA) enables the analysis of the
contribution of noise sources to overall variance and the computation of a
reliability coefficient by the ratio of substantial to total variance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10650">Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v4 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Slusarz_N/0/1/0/all/0/1">Natalia &#x15a;lusarz</a>, <a href="http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1">Ekaterina Komendantskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1">Matthew L. Daggitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Stewart_R/0/1/0/all/0/1">Robert Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Stark_K/0/1/0/all/0/1">Kathrin Stark</a></p>
<p>Differentiable logics (DL) have recently been proposed as a method of
training neural networks to satisfy logical specifications. A DL consists of a
syntax in which specifications are stated and an interpretation function that
translates expressions in the syntax into loss functions. These loss functions
can then be used during training with standard gradient descent algorithms. The
variety of existing DLs and the differing levels of formality with which they
are treated makes a systematic comparative study of their properties and
implementations difficult. This paper remedies this problem by suggesting a
meta-language for defining DLs that we call the Logic of Differentiable Logics,
or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and
for the first time introduces the formalism for reasoning about vectors and
learners. Semantically, it introduces a general interpretation function that
can be instantiated to define loss functions arising from different existing
DLs. We use LDL to establish several theoretical properties of existing DLs,
and to conduct their empirical study in neural network verification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05128">Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Maxwell Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1">Nathanael Sch&#xe4;rli</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a></p>
<p>Large language models (LLMs) have achieved impressive performance on code
generation. However, for complex programming tasks, generating the correct
solution in one go becomes challenging, thus some prior works have designed
program repair approaches to improve code generation performance. In this work,
we propose Self-Debugging, which teaches a large language model to debug its
predicted program via few-shot demonstrations. In particular, we demonstrate
that Self-Debugging can teach the large language model to perform rubber duck
debugging; i.e., without any human feedback on the code correctness or error
messages, the model is able to identify its mistakes by investigating the
execution results and explaining the generated code in natural language.
Self-Debugging achieves the state-of-the-art performance on several code
generation benchmarks, including the Spider dataset for text-to-SQL generation,
TransCoder for C++-to-Python translation, and MBPP for text-to-Python
generation. On the Spider benchmark where there are no unit tests to verify the
correctness of predictions, Self-Debugging with code explanation consistently
improves the baseline by 2-3%, and improves the prediction accuracy on problems
of the hardest level by 9%. On TransCoder and MBPP where unit tests are
available, Self-Debugging improves the baseline accuracy by up to 12%.
Meanwhile, by leveraging feedback messages and reusing failed predictions,
Self-Debugging notably improves sample efficiency, and can match or outperform
baseline models that generate more than 10x candidate programs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06715">Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Crabbe_J/0/1/0/all/0/1">Jonathan Crabb&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Interpretability methods are valuable only if their explanations faithfully
describe the explained model. In this work, we consider neural networks whose
predictions are invariant under a specific symmetry group. This includes
popular architectures, ranging from convolutional to graph neural networks. Any
explanation that faithfully explains this type of model needs to be in
agreement with this invariance property. We formalize this intuition through
the notion of explanation invariance and equivariance by leveraging the
formalism from geometric deep learning. Through this rigorous formalism, we
derive (1) two metrics to measure the robustness of any interpretability method
with respect to the model symmetry group; (2) theoretical robustness guarantees
for some popular interpretability methods and (3) a systematic approach to
increase the invariance of any interpretability method with respect to a
symmetry group. By empirically measuring our metrics for explanations of models
associated with various modalities and symmetry groups, we derive a set of 5
guidelines to allow users and developers of interpretability methods to produce
robust explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08247">MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data. (arXiv:2304.08247v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tianyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1">Lisa C. Adams</a>, <a href="http://arxiv.org/find/cs/1/au:+Papaioannou_J/0/1/0/all/0/1">Jens-Michalis Papaioannou</a>, <a href="http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1">Paul Grundmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Oberhauser_T/0/1/0/all/0/1">Tom Oberhauser</a>, <a href="http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1">Alexander L&#xf6;ser</a>, <a href="http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1">Daniel Truhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1">Keno K. Bressem</a></p>
<p>As large language models (LLMs) like OpenAI's GPT series continue to make
strides, we witness the emergence of artificial intelligence applications in an
ever-expanding range of fields. In medicine, these LLMs hold considerable
promise for improving medical workflows, diagnostics, patient care, and
education. Yet, there is an urgent need for open-source models that can be
deployed on-premises to safeguard patient privacy. In our work, we present an
innovative dataset consisting of over 160,000 entries, specifically crafted to
fine-tune LLMs for effective medical applications. We investigate the impact of
fine-tuning these datasets on publicly accessible pre-trained LLMs, and
subsequently, we juxtapose the performance of pre-trained-only models against
the fine-tuned models concerning the examinations that future medical doctors
must pass to achieve certification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14993">ChatGPT in the Classroom: An Analysis of Its Strengths and Weaknesses for Solving Undergraduate Computer Science Questions. (arXiv:2304.14993v3 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1">Ishika Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Budhiraja_R/0/1/0/all/0/1">Ritvik Budhiraja</a>, <a href="http://arxiv.org/find/cs/1/au:+Dev_H/0/1/0/all/0/1">Harshal Dev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadia_J/0/1/0/all/0/1">Jahnvi Kadia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ataullah_M/0/1/0/all/0/1">M. Osama Ataullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Sayan Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1">Dhruv Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Akolekar_H/0/1/0/all/0/1">Harshal D. Akolekar</a></p>
<p>ChatGPT is an AI language model developed by OpenAI that can understand and
generate human-like text. It can be used for a variety of use cases such as
language generation, question answering, text summarization, chatbot
development, language translation, sentiment analysis, content creation,
personalization, text completion, and storytelling. While ChatGPT has garnered
significant positive attention, it has also generated a sense of apprehension
and uncertainty in academic circles. There is concern that students may
leverage ChatGPT to complete take-home assignments and exams and obtain
favorable grades without genuinely acquiring knowledge. This paper adopts a
quantitative approach to demonstrate ChatGPT's high degree of unreliability in
answering a diverse range of questions pertaining to topics in undergraduate
computer science. Our analysis shows that students may risk self-sabotage by
blindly depending on ChatGPT to complete assignments and exams. We build upon
this analysis to provide constructive recommendations to both students and
instructors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12081">MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement. (arXiv:2305.12081v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chufan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Tabular data prediction has been employed in medical applications such as
patient health risk prediction. However, existing methods usually revolve
around the algorithm design while overlooking the significance of data
engineering. Medical tabular datasets frequently exhibit significant
heterogeneity across different sources, with limited sample sizes per source.
As such, previous predictors are often trained on manually curated small
datasets that struggle to generalize across different tabular datasets during
inference. This paper proposes to scale medical tabular data predictors
(MediTab) to various tabular inputs with varying features. The method uses a
data engine that leverages large language models (LLMs) to consolidate tabular
samples to overcome the barrier across tables with distinct schema. It also
aligns out-domain data with the target task using a "learn, annotate, and
refinement" pipeline. The expanded training data then enables the pre-trained
MediTab to infer for arbitrary tabular input in the domain without fine-tuning,
resulting in significant improvements over supervised baselines: it reaches an
average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3
trial outcome prediction datasets, respectively. In addition, MediTab exhibits
impressive zero-shot performances: it outperforms supervised XGBoost models by
8.9% and 17.2% on average in two prediction tasks, respectively. The code is
available at https://github.com/RyanWangZf/MediTab.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12766">Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>Large language models (LLMs) have initiated a paradigm shift in transfer
learning. In contrast to the classic pretraining-then-finetuning procedure, in
order to use LLMs for downstream prediction tasks, one only needs to provide a
few demonstrations, known as in-context examples, without adding more or
updating existing model parameters. This in-context learning (ICL) capability
of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs
acquire such capabilities. In this paper, we investigate the reason why a
transformer-based language model can accomplish in-context learning after
pre-training on a general language corpus by proposing one hypothesis that LLMs
can simulate kernel regression with internal representations when faced with
in-context examples. More concretely, we first prove that Bayesian inference on
in-context prompts can be asymptotically understood as kernel regression $\hat
y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context
demonstrations grows. Then, we empirically investigate the in-context behaviors
of language models. We find that during ICL, the attention and hidden features
in LLMs match the behaviors of a kernel regression. Finally, our theory
provides insights into multiple phenomena observed in the ICL field: why
retrieving demonstrative samples similar to test samples can help, why ICL
performance is sensitive to the output formats, and why ICL accuracy benefits
from selecting in-distribution and representative samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13673">Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1">Zeyuan Allen-Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a></p>
<p>We design controlled experiments to study HOW generative language models,
like GPT, learn context-free grammars (CFGs) -- diverse language systems with a
tree-like structure capturing many aspects of natural languages, programs, and
logics. CFGs are as hard as pushdown automata, and can be ambiguous so that
verifying if a string satisfies the rules requires dynamic programming. We
construct synthetic data and demonstrate that even for difficult (long and
ambiguous) CFGs, pre-trained transformers can learn to generate sentences with
near-perfect accuracy and impressive diversity.
</p>
<p>More importantly, we delve into the physical principles behind how
transformers learns CFGs. We discover that the hidden states within the
transformer implicitly and precisely encode the CFG structure (such as putting
tree node information exactly on the subtree boundary), and learn to form
"boundary to boundary" attentions resembling dynamic programming. We also cover
some extension of CFGs as well as the robustness aspect of transformers against
grammar mistakes. Overall, our research provides a comprehensive and empirical
understanding of how transformers learn CFGs, and reveals the physical
mechanisms utilized by transformers to capture the structure and rules of
languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14979">Assessment of the Reliablity of a Model&#x27;s Decision by Generalizing Attribution to the Wavelet Domain. (arXiv:2305.14979v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1">Gabriel Kasmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1">Laurent Dubus</a>, <a href="http://arxiv.org/find/cs/1/au:+Drenan_Y/0/1/0/all/0/1">Yves-Marie Saint Drenan</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1">Philippe Blanc</a></p>
<p>Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process's reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where {\it and} on what
scales the model focuses, thus enabling us to assess whether a decision is
reliable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15086">Unpaired Image-to-Image Translation via Neural Schr\&quot;odinger Bridge. (arXiv:2305.15086v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Beomsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1">Gihyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kwanyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. Although
diffusion models have achieved remarkable progress in recent years, they have
limitations in the unpaired image-to-image translation tasks due to the
Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to
translate between two arbitrary distributions, have risen as an attractive
solution to this problem. However, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which
expresses SB problem as a sequence of adversarial learning problems. This
allows us to incorporate advanced discriminators and regularization to learn a
SB between unpaired data. We demonstrate that UNSB is scalable and successfully
solves various unpaired image-to-image translation tasks. Code:
\url{https://github.com/cyclomon/UNSB}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01879">Revisiting the Role of Language Priors in Vision-Language Models. (arXiv:2306.01879v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiqiu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a></p>
<p>Vision-language models (VLMs) are impactful in part because they can be
applied to a variety of visual understanding tasks in a zero-shot fashion,
without any fine-tuning. We study $\textit{generative VLMs}$ that are trained
for next-word generation given an image. We explore their zero-shot performance
on the illustrative task of image-text retrieval across 8 popular
vision-language benchmarks. Our first observation is that they can be
repurposed for discriminative tasks (such as image-text retrieval) by simply
computing the match score of generating a particular text string given an
image. We call this probabilistic score the $\textit{Visual Generative
Pre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces
near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on
others. We analyze this behavior through a probabilistic lens, pointing out
that some benchmarks inadvertently capture unnatural language distributions by
creating adversarial but unlikely text captions. In fact, we demonstrate that
even a "blind" language model that ignores any image evidence can sometimes
outperform all prior art, reminiscent of similar challenges faced by the
visual-question answering (VQA) community many years ago. We derive a
probabilistic post-processing scheme that controls for the amount of linguistic
bias in generative VLMs at test time without having to retrain or fine-tune the
model. We show that the VisualGPTScore, when appropriately debiased, is a
strong zero-shot baseline for vision-language understanding, oftentimes
producing state-of-the-art accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03116">Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shikun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1">Xiaobo Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jiankang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shiming Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a></p>
<p>Learning from crowds describes that the annotations of training data are
obtained with crowd-sourcing services. Multiple annotators each complete their
own small part of the annotations, where labeling mistakes that depend on
annotators occur frequently. Modeling the label-noise generation process by the
noise transition matrix is a power tool to tackle the label noise. In
real-world crowd-sourcing scenarios, noise transition matrices are both
annotator- and instance-dependent. However, due to the high complexity of
annotator- and instance-dependent transition matrices (AIDTM), annotation
sparsity, which means each annotator only labels a little part of instances,
makes modeling AIDTM very challenging. Prior works simplify the problem by
assuming the transition matrix is instance-independent or using simple
parametric ways, which lose modeling generality. Motivated by this, we target a
more realistic problem, estimating general AIDTM in practice. Without losing
modeling generality, we parameterize AIDTM with deep neural networks. To
alleviate the modeling challenge, we suppose every annotator shares its noise
pattern with similar annotators, and estimate AIDTM via knowledge transfer. We
hence first model the mixture of noise patterns by all annotators, and then
transfer this modeling to individual annotators. Furthermore, considering that
the transfer from the mixture of noise patterns to individuals may cause two
annotators with highly different noise generations to perturb each other, we
employ the knowledge transfer between identified neighboring annotators to
calibrate the modeling. Theoretical analyses are derived to demonstrate that
both the knowledge transfer from global to individuals and the knowledge
transfer between neighboring individuals can help model general AIDTM.
Experiments confirm the superiority of the proposed approach on synthetic and
real-world crowd-sourcing data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04018">PyTrial: Machine Learning Software and Benchmark for Clinical Trial Applications. (arXiv:2306.04018v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1">Tianfan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Clinical trials are conducted to test the effectiveness and safety of
potential drugs in humans for regulatory approval. Machine learning (ML) has
recently emerged as a new tool to assist in clinical trials. Despite this
progress, there have been few efforts to document and benchmark ML4Trial
algorithms available to the ML research community. Additionally, the
accessibility to clinical trial-related datasets is limited, and there is a
lack of well-defined clinical tasks to facilitate the development of new
algorithms.
</p>
<p>To fill this gap, we have developed PyTrial that provides benchmarks and
open-source implementations of a series of ML algorithms for clinical trial
design and operations. In this paper, we thoroughly investigate 34 ML
algorithms for clinical trials across 6 different tasks, including patient
outcome prediction, trial site selection, trial outcome prediction,
patient-trial matching, trial similarity search, and synthetic data generation.
We have also collected and prepared 23 ML-ready datasets as well as their
working examples in Jupyter Notebooks for quick implementation and testing.
</p>
<p>PyTrial defines each task through a simple four-step process: data loading,
model specification, model training, and model evaluation, all achievable with
just a few lines of code. Furthermore, our modular API architecture empowers
practitioners to expand the framework to incorporate new algorithms and tasks
effortlessly. The code is available at https://github.com/RyanWangZf/PyTrial.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08586">FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts. (arXiv:2306.08586v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1">Chen Dun</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1">Mirian Hipolito Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed Hassan Awadallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1">Robert Sim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1">Anastasios Kyrillidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1">Dimitrios Dimitriadis</a></p>
<p>One of the goals in Federated Learning (FL) is to create personalized models
that can adapt to the context of each participating client, while utilizing
knowledge from a shared global model. Yet, often, personalization requires a
fine-tuning step using clients' labeled data in order to achieve good
performance. This may not be feasible in scenarios where incoming clients are
fresh and/or have privacy concerns. It, then, remains open how one can achieve
just-in-time personalization in these scenarios. We propose FedJETs, a novel
solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our
method leverages the diversity of the clients to train specialized experts on
different subsets of classes, and a gating function to route the input to the
most relevant expert(s). Our gating function harnesses the knowledge of a
pretrained model common expert to enhance its routing decisions on-the-fly. As
a highlight, our approach can improve accuracy up to 18\% in state of the art
FL settings, while maintaining competitive zero-shot performance. In practice,
our method can handle non-homogeneous data distributions, scale more
efficiently, and improve the state-of-the-art performance on common FL
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09376">Modularizing while Training: A New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1">Binhang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hailong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Ruobing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiang Gao</a></p>
<p>Deep neural network (DNN) models have become increasingly crucial components
in intelligent software systems. However, training a DNN model is typically
expensive in terms of both time and money. To address this issue, researchers
have recently focused on reusing existing DNN models - borrowing the idea of
code reuse in software engineering. However, reusing an entire model could
cause extra overhead or inherits the weakness from the undesired
functionalities. Hence, existing work proposes to decompose an already trained
model into modules, i.e., modularizing-after-training, and enable module reuse.
Since trained models are not built for modularization,
modularizing-after-training incurs huge overhead and model accuracy loss. In
this paper, we propose a novel approach that incorporates modularization into
the model training process, i.e., modularizing-while-training (MwT). We train a
model to be structurally modular through two loss functions that optimize
intra-module cohesion and inter-module coupling. We have implemented the
proposed approach for modularizing Convolutional Neural Network (CNN) models in
this work. The evaluation results on representative models demonstrate that MwT
outperforms the state-of-the-art approach. Specifically, the accuracy loss
caused by MwT is only 1.13 percentage points, which is 1.76 percentage points
less than that of the baseline. The kernel retention rate of the modules
generated by MwT is only 14.58%, with a reduction of 74.31% over the
state-of-the-art approach. Furthermore, the total time cost required for
training and modularizing is only 108 minutes, half of the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04333">Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Boya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weijian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihua Zhang</a></p>
<p>Adversarial attacks have the potential to mislead deep neural network
classifiers by introducing slight perturbations. Developing algorithms that can
mitigate the effects of these attacks is crucial for ensuring the safe use of
artificial intelligence. Recent studies have suggested that score-based
diffusion models are effective in adversarial defenses. However, existing
diffusion-based defenses rely on the sequential simulation of the reversed
stochastic differential equations of diffusion models, which are
computationally inefficient and yield suboptimal results. In this paper, we
introduce a novel adversarial defense scheme named ScoreOpt, which optimizes
adversarial samples at test-time, towards original clean data in the direction
guided by score-based priors. We conduct comprehensive experiments on multiple
datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results
demonstrate that our approach outperforms existing adversarial defenses in
terms of both robustness performance and inference speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06092">Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1">Stefano Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1">Boris Hanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1">Domenico Marinucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1">Ivan Nourdin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1">Giovanni Peccati</a></p>
<p>We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0$, with the exponent
depending on the metric used to measure discrepancy. Our bounds are strictly
stronger in terms of their dependence on network width than any previously
available in the literature; in the one-dimensional case, we also prove that
they are optimal, i.e., we establish matching lower bounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06125">Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmalstieg_F/0/1/0/all/0/1">Fabian Schmalstieg</a>, <a href="http://arxiv.org/find/cs/1/au:+Honerkamp_D/0/1/0/all/0/1">Daniel Honerkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1">Tim Welschehold</a>, <a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1">Abhinav Valada</a></p>
<p>Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real world that demonstrate that, with
accurate perception, the decision making of HIMOS effectively transfers to new
environments in a zero-shot manner. It shows robustness to unseen subpolicies,
failures in their execution, and different robot kinematics. These capabilities
open the door to a wide range of downstream tasks across embodied AI and
real-world use cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00143">Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bassan_S/0/1/0/all/0/1">Shahaf Bassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Amir_G/0/1/0/all/0/1">Guy Amir</a>, <a href="http://arxiv.org/find/cs/1/au:+Corsi_D/0/1/0/all/0/1">Davide Corsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Refaeli_I/0/1/0/all/0/1">Idan Refaeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1">Guy Katz</a></p>
<p>Deep neural networks (DNNs) are increasingly being used as controllers in
reactive systems. However, DNNs are highly opaque, which renders it difficult
to explain and justify their actions. To mitigate this issue, there has been a
surge of interest in explainable AI (XAI) techniques, capable of pinpointing
the input features that caused the DNN to act as it did. Existing XAI
techniques typically face two limitations: (i) they are heuristic, and do not
provide formal guarantees that the explanations are correct; and (ii) they
often apply to ``one-shot'' systems, where the DNN is invoked independently of
past invocations, as opposed to reactive systems. Here, we begin bridging this
gap, and propose a formal DNN-verification-based XAI technique for reasoning
about multi-step, reactive systems. We suggest methods for efficiently
calculating succinct explanations, by exploiting the system's transition
constraints in order to curtail the search space explored by the underlying
verifier. We evaluate our approach on two popular benchmarks from the domain of
automated navigation; and observe that our methods allow the efficient
computation of minimal and minimum explanations, significantly outperforming
the state of the art. We also demonstrate that our methods produce formal
explanations that are more reliable than competing, non-verification-based XAI
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00436">SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miao_N/0/1/0/all/0/1">Ning Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1">Yee Whye Teh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a></p>
<p>The recent progress in large language models (LLMs), especially the invention
of chain-of-thought prompting, has made it possible to automatically answer
questions by stepwise reasoning. However, when faced with more complicated
problems that require non-linear thinking, even the strongest LLMs make
mistakes. To address this, we explore whether LLMs are able to recognize errors
in their own step-by-step reasoning, without resorting to external resources.
To this end, we propose SelfCheck, a general-purpose zero-shot verification
schema for recognizing such errors. We then use the results of these checks to
improve question-answering performance by conducting weighted voting on
multiple solutions to the question. We test SelfCheck on three datasets (GSM8K,
MathQA, and MATH) and find that it successfully recognizes errors and, in turn,
increases final answer accuracies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04586">Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v9 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1">Mark Stefik</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1">Robert Price</a></p>
<p>The mainstream AIs approaches are the generative and deep learning approaches
with large language models (LLMs) and the manually constructed symbolic
approach. Both approaches have led to valuable AI systems and impressive feats.
However, manually constructed AIs are brittle even in circumscribed domains.
Generative AIs make strange mistakes and do not notice them. In both approaches
the AIs cannot be instructed easily, fail to use common sense, and lack
curiosity. They have abstract knowledge but lack social alignment.
Developmental AIs have more potential. They start with innate competences,
interact with their environment, and learn from their interactions. They
interact and learn from people and establish perceptual, cognitive, and common
grounding. Developmental AIs have demonstrated capabilities including
multimodal perception, object recognition, and manipulation. Powerful
computational models for hierarchical planning, abstraction discovery,
curiosity, and language acquisition exist but need to be adapted to a
developmental learning based approach. The promise is that developmental AIs
will acquire self-developed and socially developed competences. They would
address the shortcomings of current mainstream AI approaches, and ultimately
lead to sophisticated forms of learning involving critical reading, provenance
evaluation, and hypothesis testing. However, developmental AI projects have not
yet fully reached the Speaking Gap corresponding to toddler development at
about two years of age, before their speech is fluent. The AIs do not bridge
the Reading Gap, to skillfully and skeptically learn from written and online
information resources. This position paper lays out the prospects, gaps, and
challenges for extending the practice of developmental AIs to create resilient,
intelligent, and human-compatible AIs that learn what they need to know.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12439">BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tinghao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiangyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Ping He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a></p>
<p>We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00079">On the Implicit Bias of Adam. (arXiv:2309.00079v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cattaneo_M/0/1/0/all/0/1">Matias D. Cattaneo</a>, <a href="http://arxiv.org/find/cs/1/au:+Klusowski_J/0/1/0/all/0/1">Jason M. Klusowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Shigida_B/0/1/0/all/0/1">Boris Shigida</a></p>
<p>In previous literature, backward error analysis was used to find ordinary
differential equations (ODEs) approximating the gradient descent trajectory. It
was found that finite step sizes implicitly regularize solutions because terms
appearing in the ODEs penalize the two-norm of the loss gradients. We prove
that the existence of similar implicit regularization in RMSProp and Adam
depends on their hyperparameters and the training stage, but with a different
"norm" involved: the corresponding ODE terms either penalize the (perturbed)
one-norm of the loss gradients or, on the contrary, hinder its decrease (the
latter case being typical). We also conduct numerical experiments and discuss
how the proven facts can influence generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01807">Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katdare_P/0/1/0/all/0/1">Pulkit Katdare</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1">Katherine Driggs-Campbell</a></p>
<p>Reinforcement Learning (RL) methods are typically sample-inefficient, making
it challenging to train and deploy RL-policies in real world robots. Even a
robust policy trained in simulation requires a real-world deployment to assess
their performance. This paper proposes a new approach to evaluate the
real-world performance of agent policies prior to deploying them in the real
world. Our approach incorporates a simulator along with real-world offline data
to evaluate the performance of any policy using the framework of Marginalized
Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large
density ratios that deviate from a reasonable range and (2) indirect
supervision, where the ratio needs to be inferred indirectly, thus exacerbating
estimation error. Our approach addresses these challenges by introducing the
target policy's occupancy in the simulator as an intermediate variable and
learning the density ratio as the product of two terms that can be learned
separately. The first term is learned with direct supervision and the second
term has a small magnitude, thus making it computationally efficient. We
analyze the sample complexity as well as error propagation of our two
step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim
environments such as Cartpole, Reacher, and Half-Cheetah. Our results show that
our method generalizes well across a variety of Sim2Sim gap, target policies
and offline data collection policies. We also demonstrate the performance of
our algorithm on a Sim2Real task of validating the performance of a 7 DoF
robotic arm using offline data along with the Gazebo simulator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07056">Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Jaouni_T/0/1/0/all/0/1">Tareq Jaouni</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Arlt_S/0/1/0/all/0/1">S&#xf6;ren Arlt</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ruiz_Gonzalez_C/0/1/0/all/0/1">Carlos Ruiz-Gonzalez</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Karimi_E/0/1/0/all/0/1">Ebrahim Karimi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gu_X/0/1/0/all/0/1">Xuemei Gu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Krenn_M/0/1/0/all/0/1">Mario Krenn</a></p>
<p>Despite their promise to facilitate new scientific discoveries, the
opaqueness of neural networks presents a challenge in interpreting the logic
behind their findings. Here, we use a eXplainable-AI (XAI) technique called
$inception$ or $deep$ $dreaming$, which has been invented in machine learning
for computer vision. We use this technique to explore what neural networks
learn about quantum optics experiments. Our story begins by training deep
neural networks on the properties of quantum systems. Once trained, we "invert"
the neural network -- effectively asking how it imagines a quantum system with
a specific property, and how it would continuously modify the quantum system to
change a property. We find that the network can shift the initial distribution
of properties of the quantum system, and we can conceptualize the learned
strategies of the neural network. Interestingly, we find that, in the first
layers, the neural network identifies simple properties, while in the deeper
ones, it can identify complex quantum structures and even quantum entanglement.
This is in reminiscence of long-understood properties known in computer vision,
which we now identify in a complex natural science task. Our approach could be
useful in a more interpretable way to develop new advanced AI-based scientific
discovery techniques in quantum physics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07936">Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Monteiro_R/0/1/0/all/0/1">Rafael Monteiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Sau_K/0/1/0/all/0/1">Kartik Sau</a></p>
<p>In this paper, we introduce a new heuristics for global optimization in
scenarios where extensive evaluations of the cost function are expensive,
inaccessible, or even prohibitive. The method, which we call
Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic
Optimization, and Reinforcement Learning techniques, relying on historical
information from previously sampled points to make judicious choices of
parameter values where the cost function should be evaluated at. Unlike
optimization by Replica Exchange Monte Carlo methods, the number of evaluations
of the cost function required in this approach is comparable to that used by
Simulated Annealing, quality that is especially important in contexts like
high-throughput computing or high-performance computing tasks, where
evaluations are either computationally expensive or take a long time to be
performed. The method also differs from standard Surrogate Optimization
techniques, for it does not construct a surrogate model that aims at
approximating or reconstructing the objective function. We illustrate our
method by applying it to low dimensional optimization problems (dimensions 1,
2, 4, and 8) that mimick known difficulties of minimization on rugged energy
landscapes often seen in Condensed Matter Physics, where cost functions are
rugged and plagued with local minima. When compared to classical Simulated
Annealing, the LSS shows an effective acceleration of the optimization process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08621">Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF. (arXiv:2309.08621v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aird_A/0/1/0/all/0/1">Amanda Aird</a>, <a href="http://arxiv.org/find/cs/1/au:+All_C/0/1/0/all/0/1">Cassidy All</a>, <a href="http://arxiv.org/find/cs/1/au:+Farastu_P/0/1/0/all/0/1">Paresha Farastu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stefancova_E/0/1/0/all/0/1">Elena Stefancova</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Joshua Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattei_N/0/1/0/all/0/1">Nicholas Mattei</a>, <a href="http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1">Robin Burke</a></p>
<p>Fairness problems in recommender systems often have a complexity in practice
that is not adequately captured in simplified research formulations. A social
choice formulation of the fairness problem, operating within a multi-agent
architecture of fairness concerns, offers a flexible and multi-aspect
alternative to fairness-aware recommendation approaches. Leveraging social
choice allows for increased generality and the possibility of tapping into
well-studied social choice algorithms for resolving the tension between
multiple, competing fairness concerns. This paper explores a range of options
for choice mechanisms in multi-aspect fairness applications using both real and
synthetic data and shows that different classes of choice and allocation
mechanisms yield different but consistent fairness / accuracy tradeoffs. We
also show that a multi-agent formulation offers flexibility in adapting to user
population dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08916">BGGAN: Generative AI Enables Representing Brain Structure-Function Connections for Alzheimer&#x27;s Disease. (arXiv:2309.08916v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuqiang Wang</a></p>
<p>The relationship between brain structure and function is critical for
revealing the pathogenesis of brain disease, including Alzheimer's disease
(AD). However, it is a great challenge to map brain structure-function
connections due to various reasons. In this work, a bidirectional graph
generative adversarial networks (BGGAN) is proposed to represent brain
structure-function connections. Specifically, by designing a module
incorporating inner graph convolution network (InnerGCN), the generators of
BGGAN can employ features of direct and indirect brain regions to learn the
mapping function between structural domain and functional domain. Besides, a
new module named Balancer is designed to counterpoise the optimization between
generators and discriminators. By introducing the Balancer into BGGAN, both the
structural generator and functional generator can not only alleviate the issue
of mode collapse but also learn complementarity of structural and functional
features. Experimental results using ADNI datasets show that the both the
generated structure connections and generated function connections can improve
the identification accuracy of AD. More importantly, based the proposed model,
it is found that the relationship between brain structure and function is not a
complete one-to-one correspondence. Brain structure is the basis of brain
function. The strong structural connections are almost accompanied by strong
functional connections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09476">Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Johor Jara Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1">Seth Cooper</a>, <a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1">Matthew Guzdial</a></p>
<p>Automated game design (AGD), the study of automatically generating game
rules, has a long history in technical games research. AGD approaches generally
rely on approximations of human play, either objective functions or AI agents.
Despite this, the majority of these approximators are static, meaning they do
not reflect human player's ability to learn and improve in a game. In this
paper, we investigate the application of Reinforcement Learning (RL) as an
approximator for human play for rule generation. We recreate the classic AGD
environment Mechanic Maker in Unity as a new, open-source rule generation
framework. Our results demonstrate that RL produces distinct sets of rules from
an A* agent baseline, which may be more usable by humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10514">Partially Specified Causal Simulations. (arXiv:2309.10514v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zamanian_A/0/1/0/all/0/1">A. Zamanian</a>, <a href="http://arxiv.org/find/stat/1/au:+Mareis_L/0/1/0/all/0/1">L. Mareis</a>, <a href="http://arxiv.org/find/stat/1/au:+Ahmidi_N/0/1/0/all/0/1">N. Ahmidi</a></p>
<p>Simulation studies play a key role in the validation of causal inference
methods. The simulation results are reliable only if the study is designed
according to the promised operational conditions of the method-in-test. Still,
many causal inference literature tend to design over-restricted or misspecified
studies. In this paper, we elaborate on the problem of improper simulation
design for causal methods and compile a list of desiderata for an effective
simulation framework. We then introduce partially randomized causal simulation
(PARCS), a simulation framework that meets those desiderata. PARCS synthesizes
data based on graphical causal models and a wide range of adjustable
parameters. There is a legible mapping from usual causal assumptions to the
parameters, thus, users can identify and specify the subset of related
parameters and randomize the remaining ones to generate a range of complying
data-generating processes for their causal method. The result is a more
comprehensive and inclusive empirical investigation for causal claims. Using
PARCS, we reproduce and extend the simulation studies of two well-known causal
discovery and missing data analysis papers to emphasize the necessity of a
proper simulation design. Our results show that those papers would have
improved and extended the findings, had they used PARCS for simulation. The
framework is implemented as a Python package, too. By discussing the
comprehensiveness and transparency of PARCS, we encourage causal inference
researchers to utilize it as a standard tool for future works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11981">Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1">Patricio Vera</a>, <a href="http://arxiv.org/find/cs/1/au:+Moya_P/0/1/0/all/0/1">Pedro Moya</a>, <a href="http://arxiv.org/find/cs/1/au:+Barraza_L/0/1/0/all/0/1">Lisa Barraza</a></p>
<p>In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12871">AnglE-optimized Text Embeddings. (arXiv:2309.12871v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a></p>
<p>High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14073">Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach. (arXiv:2309.14073v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Saremi_M/0/1/0/all/0/1">Mehrzad Saremi</a></p>
<p>We propose a graphical structure for structural equation models that is
stable under marginalization under linearity and Gaussianity assumptions. We
show that computing the maximum likelihood estimation of this model is
equivalent to training a neural network. We implement a GPU-based algorithm
that computes the maximum likelihood estimation of these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14331">LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hongwu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ran_R/0/1/0/all/0/1">Ran Ran</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yukui Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiahui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaoyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Thorat_K/0/1/0/all/0/1">Kiran Thorat</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1">Tong Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenghong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaolin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Caiwen Ding</a></p>
<p>The growth of Graph Convolution Network (GCN) model sizes has revolutionized
numerous applications, surpassing human performance in areas such as personal
healthcare and financial systems. The deployment of GCNs in the cloud raises
privacy concerns due to potential adversarial attacks on client data. To
address security concerns, Privacy-Preserving Machine Learning (PPML) using
Homomorphic Encryption (HE) secures sensitive client data. However, it
introduces substantial computational overhead in practical applications. To
tackle those challenges, we present LinGCN, a framework designed to reduce
multiplication depth and optimize the performance of HE based GCN inference.
LinGCN is structured around three key elements: (1) A differentiable structural
linearization algorithm, complemented by a parameterized discrete indicator
function, co-trained with model weights to meet the optimization goal. This
strategy promotes fine-grained node-level non-linear location selection,
resulting in a model with minimized multiplication depth. (2) A compact
node-wise polynomial replacement policy with a second-order trainable
activation function, steered towards superior convergence by a two-level
distillation approach from an all-ReLU based teacher model. (3) an enhanced HE
solution that enables finer-grained operator fusion for node-wise activation
functions, further reducing multiplication level consumption in HE-based
inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that
LinGCN excels in latency, accuracy, and scalability for homomorphically
encrypted inference, outperforming solutions such as CryptoGCN. Remarkably,
LinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preserving
an inference accuracy of 75% and notably reducing multiplication depth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17147">Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ashwin_J/0/1/0/all/0/1">Julian Ashwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1">Aditya Chhabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1">Vijayendra Rao</a></p>
<p>Large Language Models (LLMs) are quickly becoming ubiquitous, but the
implications for social science research are not yet well understood. This
paper asks whether LLMs can help us analyse large-N qualitative data from
open-ended interviews, with an application to transcripts of interviews with
Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of
caution is needed in using LLMs to annotate text as there is a risk of
introducing biases that can lead to misleading inferences. We here mean bias in
the technical sense, that the errors that LLMs make in annotating interview
transcripts are not random with respect to the characteristics of the interview
subjects. Training simpler supervised models on high-quality human annotations
with flexible coding leads to less measurement error and bias than LLM
annotations. Therefore, given that some high quality annotations are necessary
in order to asses whether an LLM introduces bias, we argue that it is probably
preferable to train a bespoke model on these annotations than it is to use an
LLM for annotation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17167">DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1">Neil Zhenqiang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Large language models (LLMs) have achieved remarkable performance in various
evaluation benchmarks. However, concerns about their performance are raised on
potential data contamination in their considerable volume of training corpus.
Moreover, the static nature and fixed complexity of current benchmarks may
inadequately gauge the advancing capabilities of LLMs. In this paper, we
introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic
evaluation of LLMs. Based on our proposed dynamic evaluation framework, we
build graph-informed DyVal by leveraging the structural advantage of directed
acyclic graphs to dynamically generate evaluation samples with controllable
complexities. DyVal generates challenging evaluation sets on reasoning tasks
including mathematics, logical reasoning, and algorithm problems. We evaluate
various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments
demonstrate that LLMs perform worse in DyVal-generated evaluation samples with
different complexities, emphasizing the significance of dynamic evaluation. We
also analyze the failure cases and results of different prompting methods.
Moreover, DyVal-generated samples are not only evaluation sets, but also
helpful data for fine-tuning to improve the performance of LLMs on existing
benchmarks. We hope that DyVal can shed light on the future evaluation research
of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17260">PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suomela_L/0/1/0/all/0/1">Lauri Suomela</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalliola_J/0/1/0/all/0/1">Jussi Kalliola</a>, <a href="http://arxiv.org/find/cs/1/au:+Edelman_H/0/1/0/all/0/1">Harry Edelman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamarainen_J/0/1/0/all/0/1">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a></p>
<p>Recent results suggest that splitting topological navigation into
robot-independent and robot-specific components improves navigation performance
by enabling the robot-independent part to be trained with data collected by
different robot types. However, the navigation methods are still limited by the
scarcity of suitable training data and suffer from poor computational scaling.
In this work, we present PlaceNav, subdividing the robot-independent part into
navigation-specific and generic computer vision components. We utilize visual
place recognition for the subgoal selection of the topological navigation
pipeline. This makes subgoal selection more efficient and enables leveraging
large-scale datasets from non-robotics sources, increasing training data
availability. Bayesian filtering, enabled by place recognition, further
improves navigation performance by increasing the temporal consistency of
subgoals. Our experimental results verify the design and the new model obtains
a 76% higher success rate in indoor and 23% higher in outdoor navigation tasks
with higher computational efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17329">Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kangxian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiancheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Donglai Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1">Ziqiao Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1">Pascal Fua</a></p>
<p>Pulmonary diseases rank prominently among the principal causes of death
worldwide. Curing them will require, among other things, a better understanding
of the many complex 3D tree-shaped structures within the pulmonary system, such
as airways, arteries, and veins. In theory, they can be modeled using
high-resolution image stacks. Unfortunately, standard CNN approaches operating
on dense voxel grids are prohibitively expensive. To remedy this, we introduce
a point-based approach that preserves graph connectivity of tree skeleton and
incorporates an implicit surface representation. It delivers SOTA accuracy at a
low computational cost and the resulting models have usable surfaces. Due to
the scarcity of publicly accessible data, we have also curated an extensive
dataset to evaluate our approach and will make it public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00035">LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1">Laurence Aitchison</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1">Maja Rudolph</a></p>
<p>Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test
data or out-of-distribution samples. One approach commonly used in vision for
alleviating this issue is a deep ensemble, which constructs an ensemble by
training the same model multiple times using different random initializations.
However, there is a huge challenge to ensembling LLMs: the most effective LLMs
are very, very large. Keeping a single LLM in memory is already challenging
enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many
settings. To address these issues, we propose an ensemble approach using
Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.
Critically, these low-rank adapters represent a very small number of
parameters, orders of magnitude less than the underlying pre-trained model.
Thus, it is possible to construct large ensembles of LoRA adapters with almost
the same computational overhead as using the original model. We find that LoRA
ensembles, applied on its own or on top of pre-existing regularization
techniques, gives consistent improvements in predictive accuracy and
uncertainty quantification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01425">Borges and AI. (arXiv:2310.01425v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1">L&#xe9;on Bottou</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01791">Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barenboim_M/0/1/0/all/0/1">Moran Barenboim</a>, <a href="http://arxiv.org/find/cs/1/au:+Indelman_V/0/1/0/all/0/1">Vadim Indelman</a></p>
<p>Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02298">Prompting Audios Using Acoustic Properties For Emotion Representation. (arXiv:2310.02298v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1">Hira Dhamyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Elizalde_B/0/1/0/all/0/1">Benjamin Elizalde</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1">Soham Deshmukh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huaming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1">Bhiksha Raj</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Rita Singh</a></p>
<p>Emotions lie on a continuum, but current models treat emotions as a finite
valued discrete variable. This representation does not capture the diversity in
the expression of emotion. To better represent emotions we propose the use of
natural language descriptions (or prompts). In this work, we address the
challenge of automatically generating these prompts and training a model to
better learn emotion representations from audio and prompt pairs. We use
acoustic properties that are correlated to emotion like pitch, intensity,
speech rate, and articulation rate to automatically generate prompts i.e.
'acoustic prompts'. We use a contrastive learning objective to map speech to
their respective acoustic prompts. We evaluate our model on Emotion Audio
Retrieval and Speech Emotion Recognition. Our results show that the acoustic
prompts significantly improve the model's performance in EAR, in various
Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on
the Ravdess dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02357">On the definition of toxicity in NLP. (arXiv:2310.02357v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berezin_S/0/1/0/all/0/1">Sergey Berezin</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahbakhsh_R/0/1/0/all/0/1">Reza Farahbakhsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Crespi_N/0/1/0/all/0/1">Noel Crespi</a></p>
<p>The fundamental problem in toxicity detection task lies in the fact that the
toxicity is ill-defined. This causes us to rely on subjective and vague data in
models' training, which results in non-robust and non-accurate results: garbage
in - garbage out.
</p>
<p>This work suggests a new, stress-level-based definition of toxicity designed
to be objective and context-aware. On par with it, we also describe possible
ways of applying this new definition to dataset creation and model training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02601">MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lanqing Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1">Dit-Yan Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02861">Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiangyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sibo Wang</a></p>
<p>Graph-level anomaly detection has gained significant attention as it finds
many applications in various domains, such as cancer diagnosis and enzyme
prediction. However, existing methods fail to capture the underlying properties
of graph anomalies, resulting in unexplainable framework design and
unsatisfying performance. In this paper, we take a step back and re-investigate
the spectral differences between anomalous and normal graphs. Our main
observation shows a significant disparity in the accumulated spectral energy
between these two classes. Moreover, we prove that the accumulated spectral
energy of the graph signal can be represented by its Rayleigh Quotient,
indicating that the Rayleigh Quotient is a driving factor behind the anomalous
properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph
Neural Network (RQGNN), the first spectral GNN for graph-level anomaly
detection, providing a new perspective on exploring the inherent spectral
features of anomalous graphs. Specifically, we introduce a novel framework that
consists of two components: the Rayleigh Quotient learning component (RQL) and
Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the
Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space
of graphs. Extensive experiments on 10 real-world datasets show that RQGNN
outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC,
demonstrating the effectiveness of our framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15724">REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v3 [cs.RO] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zeyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bahety_A/0/1/0/all/0/1">Arpit Bahety</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shuran Song</a></p>
<p>The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLMs for robot failure explanation, we introduce REFLECT,
a framework which queries LLM for failure reasoning based on a hierarchical
summary of robot past experiences generated from multisensory observations. The
failure explanation can further guide a language-based planner to correct the
failure and complete the task. To systematically evaluate the framework, we
create the RoboFail dataset with a variety of tasks and failure scenarios. We
demonstrate that the LLM-based framework is able to generate informative
failure explanations that assist successful correction planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10025">An Empirical Study on Fertility Proposals Using Multi-Grained Topic Analysis Methods. (arXiv:2307.10025v2 [cs.HC] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yulin Zhou</a></p>
<p>Fertility issues are closely related to population security, in 60 years
China's population for the first time in a negative growth trend, the change of
fertility policy is of great concern to the community. 2023 "two sessions"
proposal "suggests that the country in the form of legislation, the birth of
the registration of the cancellation of the marriage restriction" This topic
was once a hot topic on the Internet, and "unbundling" the relationship between
birth registration and marriage has become the focus of social debate. In this
paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment
analysis to conduct multi-granularity semantic analysis of microblog comments.
It is found that the discussion on the proposal of "removing marriage
restrictions from birth registration" involves the individual, society and the
state at three dimensions, and is detailed into social issues such as personal
behaviour, social ethics and law, and national policy, with people's sentiment
inclined to be negative in most of the topics. Based on this, eight proposals
were made to provide a reference for governmental decision making and to form a
reference method for researching public opinion on political issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01423">An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akram_A/0/1/0/all/0/1">Arslan Akram</a></p>
<p>Since ChatGPT has emerged as a major AIGC model, providing high-quality
responses across a wide range of applications (including software development
and maintenance), it has attracted much interest from many individuals. ChatGPT
has great promise, but there are serious problems that might arise from its
misuse, especially in the realms of education and public safety. Several AIGC
detectors are available, and they have all been tested on genuine text.
However, more study is needed to see how effective they are for multi-domain
ChatGPT material. This study aims to fill this need by creating a multi-domain
dataset for testing the state-of-the-art APIs and tools for detecting
artificially generated information used by universities and other research
institutions. A large dataset consisting of articles, abstracts, stories, news,
and product reviews was created for this study. The second step is to use the
newly created dataset to put six tools through their paces. Six different
artificial intelligence (AI) text identification systems, including "GPTkit,"
"GPTZero," "Originality," "Sapling," "Writer," and "Zylalab," have accuracy
rates between 55.29 and 97.0%. Although all the tools fared well in the
evaluations, originality was particularly effective across the board.
</p>
</p>
</div>

    </div>
    </body>
    