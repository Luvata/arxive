<!DOCTYPE html>
<html>
<head>
<title>2023-11-20-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.09223">Non-line-of-sight imaging in the presence of scattering media using phasor fields. (arXiv:2311.09223v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Luesia_P/0/1/0/all/0/1">Pablo Luesia</a>, <a href="http://arxiv.org/find/eess/1/au:+Crespo_M/0/1/0/all/0/1">Miguel Crespo</a>, <a href="http://arxiv.org/find/eess/1/au:+Jarabo_A/0/1/0/all/0/1">Adrian Jarabo</a>, <a href="http://arxiv.org/find/eess/1/au:+Redo_Sanchez_A/0/1/0/all/0/1">Albert Redo-Sanchez</a></p>
<p>Non-line-of-sight (NLOS) imaging aims to reconstruct partially or completely
occluded scenes. Recent approaches have demonstrated high-quality
reconstructions of complex scenes with arbitrary reflectance, occlusions, and
significant multi-path effects. However, previous works focused on surface
scattering only, which reduces the generality in more challenging scenarios
such as scenes submerged in scattering media. In this work, we investigate
current state-of-the-art NLOS imaging methods based on phasor fields to
reconstruct scenes submerged in scattering media. We empirically analyze the
capability of phasor fields in reconstructing complex synthetic scenes
submerged in thick scattering media. We also apply the method to real scenes,
showing that it performs similarly to recent diffuse optical tomography
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09225">Autonomous Driving using Spiking Neural Networks on Dynamic Vision Sensor Data: A Case Study of Traffic Light Change Detection. (arXiv:2311.09225v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuelei Chen</a></p>
<p>Autonomous driving is a challenging task that has gained broad attention from
both academia and industry. Current solutions using convolutional neural
networks require large amounts of computational resources, leading to high
power consumption. Spiking neural networks (SNNs) provide an alternative
computation model to process information and make decisions. This biologically
plausible model has the advantage of low latency and energy efficiency. Recent
work using SNNs for autonomous driving mostly focused on simple tasks like lane
keeping in simplified simulation environments. This project studies SNNs on
photo-realistic driving scenes in the CARLA simulator, which is an important
step toward using SNNs on real vehicles. The efficacy and generalizability of
the method will be investigated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09240">Devil in the Landscapes: Inferring Epidemic Exposure Risks from Street View Imagery. (arXiv:2311.09240v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhenyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1">Yanxin Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a></p>
<p>Built environment supports all the daily activities and shapes our health.
Leveraging informative street view imagery, previous research has established
the profound correlation between the built environment and chronic,
non-communicable diseases; however, predicting the exposure risk of infectious
diseases remains largely unexplored. The person-to-person contacts and
interactions contribute to the complexity of infectious disease, which is
inherently different from non-communicable diseases. Besides, the complex
relationships between street view imagery and epidemic exposure also hinder
accurate predictions. To address these problems, we construct a regional
mobility graph informed by the gravity model, based on which we propose a
transmission-aware graph convolutional network (GCN) to capture disease
transmission patterns arising from human mobility. Experiments show that the
proposed model significantly outperforms baseline models by 8.54% in weighted
F1, shedding light on a low-cost, scalable approach to assess epidemic exposure
risks from street view imagery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09241">Chain of Images for Intuitively Reasoning. (arXiv:2311.09241v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanxu Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haotong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a></p>
<p>The human brain is naturally equipped to comprehend and interpret visual
information rapidly. When confronted with complex problems or concepts, we use
flowcharts, sketches, and diagrams to aid our thought process. Leveraging this
inherent ability can significantly enhance logical reasoning. However, current
Large Language Models (LLMs) do not utilize such visual intuition to help their
thinking. Even the most advanced version language models (e.g., GPT-4V and
LLaVA) merely align images into textual space, which means their reasoning
processes remain purely verbal. To mitigate such limitations, we present a
Chain of Images (CoI) approach, which can convert complex language reasoning
problems to simple pattern recognition by generating a series of images as
intermediate representations. Furthermore, we have developed a CoI evaluation
dataset encompassing 15 distinct domains where images can intuitively aid
problem-solving. Based on this dataset, we aim to construct a benchmark to
assess the capability of future multimodal large-scale models to leverage
images for reasoning. In supporting our CoI reasoning, we introduce a symbolic
multimodal large language model (SyMLLM) that generates images strictly based
on language instructions and accepts both text and image as input. Experiments
on Geometry, Chess and Common Sense tasks sourced from the CoI evaluation
dataset show that CoI improves performance significantly over the pure-language
Chain of Thoughts (CoT) baselines. The code is available at
https://github.com/GraphPKU/CoI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09253">The Perception-Robustness Tradeoff in Deterministic Image Restoration. (arXiv:2311.09253v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ohayon_G/0/1/0/all/0/1">Guy Ohayon</a>, <a href="http://arxiv.org/find/eess/1/au:+Michaeli_T/0/1/0/all/0/1">Tomer Michaeli</a>, <a href="http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1">Michael Elad</a></p>
<p>We study the behavior of deterministic methods for solving inverse problems
in imaging. These methods are commonly designed to achieve two goals: (1)
attaining high perceptual quality, and (2) generating reconstructions that are
consistent with the measurements. We provide a rigorous proof that the better a
predictor satisfies these two requirements, the larger its Lipschitz constant
must be, regardless of the nature of the degradation involved. In particular,
to approach perfect perceptual quality and perfect consistency, the Lipschitz
constant of the model must grow to infinity. This implies that such methods are
necessarily more susceptible to adversarial attacks. We demonstrate our theory
on single image super-resolution algorithms, addressing both noisy and
noiseless settings. We also show how this undesired behavior can be leveraged
to explore the posterior distribution, thereby allowing the deterministic model
to imitate stochastic methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09256">Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset. (arXiv:2311.09256v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyo_J/0/1/0/all/0/1">Jacob Tyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1">Youngseog Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Olarinre_M/0/1/0/all/0/1">Motolani Olarinre</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a></p>
<p>This paper introduces the off-road motorcycle Racer number Dataset (RnD), a
new challenging dataset for optical character recognition (OCR) research. RnD
contains 2,411 images from professional motorsports photographers that depict
motorcycle racers in off-road competitions. The images exhibit a wide variety
of factors that make OCR difficult, including mud occlusions, motion blur,
non-standard fonts, glare, complex backgrounds, etc. The dataset has 5,578
manually annotated bounding boxes around visible motorcycle numbers, along with
transcribed digits and letters. Our experiments benchmark leading OCR
algorithms and reveal an end-to-end F1 score of only 0.527 on RnD, even after
fine-tuning. Analysis of performance on different occlusion types shows mud as
the primary challenge, degrading accuracy substantially compared to normal
conditions. But the models struggle with other factors including glare, blur,
shadows, and dust. Analysis exposes substantial room for improvement and
highlights failure cases of existing models. RnD represents a valuable new
benchmark to drive innovation in real-world OCR capabilities. The authors hope
the community will build upon this dataset and baseline experiments to make
progress on the open problem of robustly recognizing text in unconstrained
natural environments. The dataset is available at
https://github.com/JacobTyo/SwinTextSpotter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09257">UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zhisheng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1">Tingbo Hou</a></p>
<p>Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
\blfootnote{*Work done as a student researcher of Google, $\dagger$ indicates
equal contribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09265">FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier. (arXiv:2311.09265v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1">Zhongjie Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1">Weining Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Mingyi Jin</a></p>
<p>With the emergence of diffusion models and rapid development in image
processing, it has become effortless to generate fancy images in tasks such as
style transfer and image editing. However, these impressive image processing
approaches face consistency issues in video processing. In this paper, we
propose a powerful model-free toolkit called FastBlend to address the
consistency problem for video processing. Based on a patch matching algorithm,
we design two inference modes, including blending and interpolation. In the
blending mode, FastBlend eliminates video flicker by blending the frames within
a sliding window. Moreover, we optimize both computational efficiency and video
quality according to different application scenarios. In the interpolation
mode, given one or more keyframes rendered by diffusion models, FastBlend can
render the whole video. Since FastBlend does not modify the generation process
of diffusion models, it exhibits excellent compatibility. Extensive experiments
have demonstrated the effectiveness of FastBlend. In the blending mode,
FastBlend outperforms existing methods for video deflickering and video
synthesis. In the interpolation mode, FastBlend surpasses video interpolation
and model-based video processing approaches. The source codes have been
released on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09269">NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios. (arXiv:2311.09269v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1">En-Te Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1">Wei-Jie Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Ding-Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Long Zeng</a></p>
<p>Existing Object Pose Estimation (OPE) methods for stacked scenarios are not
robust to changes in object scale. This paper proposes a new 6DoF OPE network
(NormNet) for different scale objects in stacked scenarios. Specifically, each
object's scale is first learned with point-wise regression. Then, all objects
in the stacked scenario are normalized into the same scale through semantic
segmentation and affine transformation. Finally, they are fed into a shared
pose estimator to recover their 6D poses. In addition, we introduce a new
Sim-to-Real transfer pipeline, combining style transfer and domain
randomization. This improves the NormNet's performance on real data even if we
only train it on synthetic data. Extensive experiments demonstrate that the
proposed method achieves state-of-the-art performance on public benchmarks and
the MultiScale dataset we constructed. The real-world experiments show that our
method can robustly estimate the 6D pose of objects at different scales.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09276">Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset. (arXiv:2311.09276v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramasubramanian_M/0/1/0/all/0/1">Muthukumaran Ramasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurung_I/0/1/0/all/0/1">Iksha Gurung</a>, <a href="http://arxiv.org/find/cs/1/au:+Gahlot_S/0/1/0/all/0/1">Shubhankar Gahlot</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1">Ronny H&#xe4;nsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Molthan_A/0/1/0/all/0/1">Andrew L. Molthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Maskey_M/0/1/0/all/0/1">Manil Maskey</a></p>
<p>Accurate detection of inundated water extents during flooding events is
crucial in emergency response decisions and aids in recovery efforts. Satellite
Remote Sensing data provides a global framework for detecting flooding extents.
Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has
proven to be useful in detecting water bodies due to low backscatter of water
features in both co-polarized and cross-polarized SAR imagery. However,
increased backscatter can be observed in certain flooded regions such as
presence of infrastructure and trees - rendering simple methods such as pixel
intensity thresholding and time-series differencing inadequate. Machine
Learning techniques has been leveraged to precisely capture flood extents in
flooded areas with bumps in backscatter but needs high amounts of labelled data
to work desirably. Hence, we created a labeled known water body extent and
flooded area extents during known flooding events covering about 36,000 sq.
kilometers of regions within mainland U.S and Bangladesh. Further, We also
leveraged citizen science by open-sourcing the dataset and hosting an open
competition based on the dataset to rapidly prototype flood extent detection
using community generated models. In this paper we present the information
about the dataset, the data processing pipeline, a baseline model and the
details about the competition, along with discussion on winning approaches. We
believe the dataset adds to already existing datasets based on Sentinel-1C SAR
data and leads to more robust modeling of flood extents. We also hope the
results from the competition pushes the research in flood extent detection
further.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09346">Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change. (arXiv:2311.09346v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yan Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shengyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Armeni_I/0/1/0/all/0/1">Iro Armeni</a></p>
<p>Building 3D geometric maps of man-made spaces is a well-established and
active field that is fundamental to computer vision and robotics. However,
considering the evolving nature of built environments, it is essential to
question the capabilities of current mapping efforts in handling temporal
changes. In addition, spatiotemporal mapping holds significant potential for
achieving sustainability and circularity goals. Existing mapping approaches
focus on small changes, such as object relocation or self-driving car
operation; in all cases where the main structure of the scene remains fixed.
Consequently, these approaches fail to address more radical changes in the
structure of the built environment, such as geometry and topology. To this end,
we introduce the Nothing Stands Still (NSS) benchmark, which focuses on the
spatiotemporal registration of 3D scenes undergoing large spatial and temporal
change, ultimately creating one coherent spatiotemporal map. Specifically, the
benchmark involves registering two or more partial 3D point clouds (fragments)
from the same scene but captured from different spatiotemporal views. In
addition to the standard pairwise registration, we assess the multi-way
registration of multiple fragments that belong to any temporal stage. As part
of NSS, we introduce a dataset of 3D point clouds recurrently captured in
large-scale building indoor environments that are under construction or
renovation. The NSS benchmark presents three scenarios of increasing
difficulty, to quantify the generalization ability of point cloud registration
methods over space (within one building and across buildings) and time. We
conduct extensive evaluations of state-of-the-art methods on NSS. The results
demonstrate the necessity for novel methods specifically designed to handle
large spatiotemporal changes. The homepage of our benchmark is at
<a href="http://nothing-stands-still.com.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09355">Privacy Threats in Stable Diffusion Models. (arXiv:2311.09355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cilloni_T/0/1/0/all/0/1">Thomas Cilloni</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_C/0/1/0/all/0/1">Charles Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_C/0/1/0/all/0/1">Charles Walter</a></p>
<p>This paper introduces a novel approach to membership inference attacks (MIA)
targeting stable diffusion computer vision models, specifically focusing on the
highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract
sensitive information about a model's training data, posing significant privacy
concerns. Despite its advancements in image synthesis, our research reveals
privacy vulnerabilities in the stable diffusion models' outputs. Exploiting
this information, we devise a black-box MIA that only needs to query the victim
model repeatedly. Our methodology involves observing the output of a stable
diffusion model at different generative epochs and training a classification
model to distinguish when a series of intermediates originated from a training
sample or not. We propose numerous ways to measure the membership features and
discuss what works best. The attack's efficacy is assessed using the ROC AUC
method, demonstrating a 60\% success rate in inferring membership information.
This paper contributes to the growing body of research on privacy and security
in machine learning, highlighting the need for robust defenses against MIAs.
Our findings prompt a reevaluation of the privacy implications of stable
diffusion models, urging practitioners and developers to implement enhanced
security measures to safeguard against such attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09361">RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior. (arXiv:2311.09361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1">James A. D. Gardner</a>, <a href="http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1">Bernhard Egger</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1">William A. P. Smith</a></p>
<p>Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. This results in limitations
for the inverse setting in terms of the expressivity of the illumination
conditions, especially when taking specular reflections into account. We
propose a conditional neural field representation based on a variational
auto-decoder and a transformer decoder. We extend Vector Neurons to build
equivariance directly into our architecture, and leveraging insights from depth
estimation through a scale-invariant loss function, we enable the accurate
representation of High Dynamic Range (HDR) images. The result is a compact,
rotation-equivariant HDR neural illumination model capable of capturing
complex, high-frequency features in natural environment maps. Training our
model on a curated dataset of 1.6K HDR environment maps of natural scenes, we
compare it against traditional representations, demonstrate its applicability
for an inverse rendering task and show environment map completion from partial
observations. We share our PyTorch implementation, dataset and trained models
at https://github.com/JADGardner/ns_reni
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09401">MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains. (arXiv:2311.09401v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Helen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a></p>
<p>Medical imaging data is often siloed within hospitals, limiting the amount of
data available for specialized model development. With limited in-domain data,
one might hope to leverage larger datasets from related domains. In this paper,
we analyze the benefit of transferring self-supervised contrastive
representations from moment contrast (MoCo) pretraining on out-of-distribution
data to settings with limited data. We consider two X-ray datasets which image
different parts of the body, and compare transferring from each other to
transferring from ImageNet. We find that depending on quantity of labeled and
unlabeled data, contrastive pretraining on larger out-of-distribution datasets
can perform nearly as well or better than MoCo pretraining in-domain, and
pretraining on related domains leads to higher performance than if one were to
use the ImageNet pretrained weights. Finally, we provide a preliminary way of
quantifying similarity between datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09402">Synthetically Enhanced: Unveiling Synthetic Data&#x27;s Potential in Medical Imaging Research. (arXiv:2311.09402v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khosravi_B/0/1/0/all/0/1">Bardia Khosravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Frank Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dapamede_T/0/1/0/all/0/1">Theo Dapamede</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouzrokh_P/0/1/0/all/0/1">Pouria Rouzrokh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gamble_C/0/1/0/all/0/1">Cooper U. Gamble</a>, <a href="http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1">Hari M. Trivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyles_C/0/1/0/all/0/1">Cody C. Wyles</a>, <a href="http://arxiv.org/find/cs/1/au:+Sellergren_A/0/1/0/all/0/1">Andrew B. Sellergren</a>, <a href="http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1">Saptarshi Purkayastha</a>, <a href="http://arxiv.org/find/cs/1/au:+Erickson_B/0/1/0/all/0/1">Bradley J. Erickson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1">Judy W. Gichoya</a></p>
<p>Chest X-rays (CXR) are the most common medical imaging study and are used to
diagnose multiple medical conditions. This study examines the impact of
synthetic data supplementation, using diffusion models, on the performance of
deep learning (DL) classifiers for CXR analysis. We employed three datasets:
CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising
diffusion probabilistic models (DDPMs) to generate synthetic frontal
radiographs. Our approach ensured that synthetic images mirrored the
demographic and pathological traits of the original data. Evaluating the
classifiers' performance on internal and external datasets revealed that
synthetic data supplementation enhances model accuracy, particularly in
detecting less prevalent pathologies. Furthermore, models trained on synthetic
data alone approached the performance of those trained on real data. This
suggests that synthetic data can potentially compensate for real data shortages
in training robust DL models. However, despite promising outcomes, the
superiority of real data persists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09424">Predicting Spine Geometry and Scoliosis from DXA Scans. (arXiv:2311.09424v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1">Amir Jamaludin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1">Timor Kadir</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1">Emma Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a></p>
<p>Our objective in this paper is to estimate spine curvature in DXA scans. To
this end we first train a neural network to predict the middle spine curve in
the scan, and then use an integral-based method to determine the curvature
along the spine curve. We use the curvature to compare to the standard angle
scoliosis measure obtained using the DXA Scoliosis Method (DSM). The
performance improves over the prior work of Jamaludin et al. 2018. We show that
the maximum curvature can be used as a scoring function for ordering the
severity of spinal deformation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09499">Center Focusing Network for Real-Time LiDAR Panoptic Segmentation. (arXiv:2311.09499v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoyan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yongli Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Baocai Yin</a></p>
<p>LiDAR panoptic segmentation facilitates an autonomous vehicle to
comprehensively understand the surrounding objects and scenes and is required
to run in real time. The recent proposal-free methods accelerate the algorithm,
but their effectiveness and efficiency are still limited owing to the
difficulty of modeling non-existent instance centers and the costly
center-based clustering modules. To achieve accurate and real-time LiDAR
panoptic segmentation, a novel center focusing network (CFNet) is introduced.
Specifically, the center focusing feature encoding (CFFE) is proposed to
explicitly understand the relationships between the original LiDAR points and
virtual instance centers by shifting the LiDAR points and filling in the center
points. Moreover, to leverage the redundantly detected centers, a fast center
deduplication module (CDM) is proposed to select only one center for each
instance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation
benchmarks demonstrate that our CFNet outperforms all existing methods by a
large margin and is 1.6 times faster than the most efficient method. The code
is available at https://github.com/GangZhang842/CFNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09500">Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation. (arXiv:2311.09500v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yangzheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1">Michael Greenspan</a></p>
<p>This paper addresses the simulation-to-real domain gap in 6DoF PE, and
proposes a novel self-supervised keypoint radial voting-based 6DoF PE
framework, effectively narrowing this gap using a learnable kernel in RKHS. We
formulate this domain gap as a distance in high-dimensional feature space,
distinct from previous iterative matching methods. We propose an adapter
network, which evolves the network parameters from the source domain, which has
been massively trained on synthetic data with synthetic poses, to the target
domain, which is trained on real data. Importantly, the real data training only
uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real
groundtruth data annotations. RKHSPose achieves state-of-the-art performance on
three commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion
LINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully
supervised methods on all six applicable BOP core datasets, achieving within
-10.8% to -0.3% of the top fully supervised results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09520">MDFL: Multi-domain Diffusion-driven Feature Learning. (arXiv:2311.09520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Daixun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a></p>
<p>High-dimensional images, known for their rich semantic information, are
widely applied in remote sensing and other fields. The spatial information in
these images reflects the object's texture features, while the spectral
information reveals the potential spectral representations across different
bands. Currently, the understanding of high-dimensional images remains limited
to a single-domain perspective with performance degradation. Motivated by the
masking texture effect observed in the human visual system, we present a
multi-domain diffusion-driven feature learning network (MDFL) , a scheme to
redefine the effective information domain that the model really focuses on.
This method employs diffusion-based posterior sampling to explicitly consider
joint information interactions between the high-dimensional manifold structures
in the spectral, spatial, and frequency domains, thereby eliminating the
influence of masking texture effects in visual models. Additionally, we
introduce a feature reuse mechanism to gather deep and raw features of
high-dimensional data. We demonstrate that MDFL significantly improves the
feature extraction performance of high-dimensional data, thereby providing a
powerful aid for revealing the intrinsic patterns and structures of such data.
The experimental results on three multi-modal remote sensing datasets show that
MDFL reaches an average overall accuracy of 98.25%, outperforming various
state-of-the-art baseline schemes. The code will be released, contributing to
the computer vision community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09540">FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion. (arXiv:2311.09540v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">DaiXun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Leyuan Fang</a></p>
<p>Multi-satellite, multi-modality in-orbit fusion is a challenging task as it
explores the fusion representation of complex high-dimensional data under
limited computational resources. Deep neural networks can reveal the underlying
distribution of multi-modal remote sensing data, but the in-orbit fusion of
multimodal data is more difficult because of the limitations of different
sensor imaging characteristics, especially when the multimodal data follows
non-independent identically distribution (Non-IID) distributions. To address
this problem while maintaining classification performance, this paper proposes
a manifold-driven multi-modality fusion framework, FedFusion, which randomly
samples local data on each client to jointly estimate the prominent manifold
structure of shallow features of each client and explicitly compresses the
feature matrices into a low-rank subspace through cascading and additive
approaches, which is used as the feature input of the subsequent classifier.
Considering the physical space limitations of the satellite constellation, we
developed a multimodal federated learning module designed specifically for
manifold data in a deep latent space. This module achieves iterative updating
of the sub-network parameters of each client through global weighted averaging,
constructing a framework that can represent compact representations of each
client. The proposed framework surpasses existing methods in terms of
performance on three multimodal datasets, achieving a classification average
accuracy of 94.35$\%$ while compressing communication costs by a factor of 4.
Furthermore, extensive numerical evaluations of real-world satellite images
were conducted on the orbiting edge computing architecture based on Jetson TX2
industrial modules, which demonstrated that FedFusion significantly reduced
training time by 48.4 minutes (15.18%) while optimizing accuracy.}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09543">Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery. (arXiv:2311.09543v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Ming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jian_W/0/1/0/all/0/1">Weihua Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a></p>
<p>Though significant progress in human pose and shape recovery from monocular
RGB images has been made in recent years, obtaining 3D human motion with high
accuracy and temporal consistency from videos remains challenging. Existing
video-based methods tend to reconstruct human motion from global image
features, which lack detailed representation capability and limit the
reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining
Network (TAR), to synchronously explore temporal-aware global and local image
features for accurate pose and shape recovery. First, a global transformer
encoder is introduced to obtain temporal global features from static feature
sequences. Second, a bidirectional ConvGRU network takes the sequence of
high-resolution feature maps as input, and outputs temporal local feature maps
that maintain high resolution and capture the local motion of the human body.
Finally, a recurrent refinement module iteratively updates estimated SMPL
parameters by leveraging both global and local temporal information to achieve
accurate and smooth results. Extensive experiments demonstrate that our TAR
obtains more accurate results than previous state-of-the-art methods on popular
benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09571">3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation. (arXiv:2311.09571v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Decatur_D/0/1/0/all/0/1">Dale Decatur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_I/0/1/0/all/0/1">Itai Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1">Kfir Aberman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1">Rana Hanocka</a></p>
<p>In this work we develop 3D Paintbrush, a technique for automatically
texturing local semantic regions on meshes via text descriptions. Our method is
designed to operate directly on meshes, producing texture maps which seamlessly
integrate into standard graphics pipelines. We opt to simultaneously produce a
localization map (to specify the edit region) and a texture map which conforms
to it. This synergistic approach improves the quality of both the localization
and the stylization. To enhance the details and resolution of the textured
area, we leverage multiple stages of a cascaded diffusion model to supervise
our local editing technique with generative priors learned from images at
different resolutions. Our technique, referred to as Cascaded Score
Distillation (CSD), simultaneously distills scores at multiple resolutions in a
cascaded fashion, enabling control over both the granularity and global
understanding of the supervision. We demonstrate the effectiveness of 3D
Paintbrush to locally texture a variety of shapes within different semantic
regions. Project page: https://threedle.github.io/3d-paintbrush
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09574">LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1">Vivek Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoli Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1">Vrishab Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1">Brent Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1">Oscar Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1">Rebecca Rojansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1">Andrew Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1">Fabiola Valvert</a>, <a href="http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1">Edward Briercheck</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1">David Weinstock</a>, <a href="http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1">Yasodha Natkunam</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1">Sebastian Fernandez-Pol</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1">Pranav Rajpurkar</a></p>
<p>The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09590">MARformer: An Efficient Metal Artifact Reduction Transformer for Dental CBCT Images. (arXiv:2311.09590v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1">Yuxuan Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jun Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1">Dinggang Shen</a></p>
<p>Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and
surgery. However, the metal teeth implants could bring annoying metal artifacts
during the CBCT imaging process, interfering diagnosis and downstream
processing such as tooth segmentation. In this paper, we develop an efficient
Transformer to perform metal artifacts reduction (MAR) from dental CBCT images.
The proposed MAR Transformer (MARformer) reduces computation complexity in the
multihead self-attention by a new Dimension-Reduced Self-Attention (DRSA)
module, based on that the CBCT images have globally similar structure. A
Patch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive
local image information for fine-grained restoration. Experimental results on
CBCT images with synthetic and real-world metal artifacts show that our
MARformer is efficient and outperforms previous MAR methods and two restoration
Transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09599">Gradual Source Domain Expansion for Unsupervised Domain Adaptation. (arXiv:2311.09599v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Westfechtel_T/0/1/0/all/0/1">Thomas Westfechtel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1">Hao-Wei Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dexuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1">Tatsuya Harada</a></p>
<p>Unsupervised domain adaptation (UDA) tries to overcome the need for a large
labeled dataset by transferring knowledge from a source dataset, with lots of
labeled data, to a target dataset, that has no labeled data. Since there are no
labels in the target domain, early misalignment might propagate into the later
stages and lead to an error build-up. In order to overcome this problem, we
propose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA
task several times from scratch, each time reinitializing the network weights,
but each time expands the source dataset with target data. In particular, the
highest-scoring target data of the previous run are employed as pseudo-source
samples with their respective pseudo-label. Using this strategy, the
pseudo-source samples induce knowledge extracted from the previous run directly
from the start of the new training. This helps align the two domains better,
especially in the early training epochs. In this study, we first introduce a
strong baseline network and apply our GSDE strategy to it. We conduct
experiments and ablation studies on three benchmarks (Office-31, OfficeHome,
and DomainNet) and outperform state-of-the-art methods. We further show that
the proposed GSDE strategy can improve the accuracy of a variety of different
state-of-the-art UDA approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09607">Multi-Task Learning Approach for Unified Biometric Estimation from Fetal Ultrasound Anomaly Scans. (arXiv:2311.09607v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Qazi_M/0/1/0/all/0/1">Mohammad Areeb Qazi</a>, <a href="http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1">Mohammed Talha Alam</a>, <a href="http://arxiv.org/find/eess/1/au:+Almakky_I/0/1/0/all/0/1">Ibrahim Almakky</a>, <a href="http://arxiv.org/find/eess/1/au:+Diehl_W/0/1/0/all/0/1">Werner Gerhard Diehl</a>, <a href="http://arxiv.org/find/eess/1/au:+Bricker_L/0/1/0/all/0/1">Leanne Bricker</a>, <a href="http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1">Mohammad Yaqub</a></p>
<p>Precise estimation of fetal biometry parameters from ultrasound images is
vital for evaluating fetal growth, monitoring health, and identifying potential
complications reliably. However, the automated computerized segmentation of the
fetal head, abdomen, and femur from ultrasound images, along with the
subsequent measurement of fetal biometrics, remains challenging. In this work,
we propose a multi-task learning approach to classify the region into head,
abdomen and femur as well as estimate the associated parameters. We were able
to achieve a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44
mm on abdomen circumference and 1.10 mm on femur length with a classification
accuracy of 99.91\% on a dataset of fetal Ultrasound images. To achieve this,
we leverage a weighted joint classification and segmentation loss function to
train a U-Net architecture with an added classification head. The code can be
accessed through
\href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\texttt{Github}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09612">Efficient End-to-End Visual Document Understanding with Rationale Distillation. (arXiv:2311.09612v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Alekh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1">Mandar Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a>, <a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1">Kristina Toutanova</a></p>
<p>Understanding visually situated language requires recognizing text and visual
elements, and interpreting complex layouts. State-of-the-art methods commonly
use specialized pre-processing tools, such as optical character recognition
(OCR) systems, that map document image inputs to extracted information in the
space of textual tokens, and sometimes also employ large language models (LLMs)
to reason in text token space. However, the gains from external tools and LLMs
come at the cost of increased computational and engineering complexity. In this
paper, we ask whether small pretrained image-to-text models can learn selective
text or layout recognition and reasoning as an intermediate inference step in
an end-to-end model for pixel-level visual language understanding. We
incorporate the outputs of such OCR tools, LLMs, and larger multimodal models
as intermediate ``rationales'' on training data, and train a small student
model to predict both rationales and answers for input questions based on those
training examples. A student model based on Pix2Struct (282M parameters)
achieves consistent improvements on three visual document understanding
benchmarks representing infographics, scanned documents, and figures, with
improvements of more than 4\% absolute over a comparable Pix2Struct model that
predicts answers directly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09614">Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images. (arXiv:2311.09614v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahamed_S/0/1/0/all/0/1">Shadab Ahamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yixi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gowdy_C/0/1/0/all/0/1">Claire Gowdy</a>, <a href="http://arxiv.org/find/cs/1/au:+O_J/0/1/0/all/0/1">Joo H. O</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloise_I/0/1/0/all/0/1">Ingrid Bloise</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1">Don Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Martineau_P/0/1/0/all/0/1">Patrick Martineau</a>, <a href="http://arxiv.org/find/cs/1/au:+Benard_F/0/1/0/all/0/1">Fran&#xe7;ois B&#xe9;nard</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousefirizi_F/0/1/0/all/0/1">Fereshteh Yousefirizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1">Rahul Dodhia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavista_J/0/1/0/all/0/1">Juan M. Lavista</a>, <a href="http://arxiv.org/find/cs/1/au:+Weeks_W/0/1/0/all/0/1">William B. Weeks</a>, <a href="http://arxiv.org/find/cs/1/au:+Uribe_C/0/1/0/all/0/1">Carlos F. Uribe</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1">Arman Rahmim</a></p>
<p>This study performs comprehensive evaluation of four neural network
architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion
segmentation from PET/CT images. These networks were trained, validated, and
tested on a diverse, multi-institutional dataset of 611 cases. Internal testing
(88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed
SegResNet as the top performer with a median Dice similarity coefficient (DSC)
of 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a
median false negative volume (FNV) of 0 ml. On the unseen external test set
(145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best
median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.
We assessed reproducibility of six lesion measures, calculated their prediction
errors, and examined DSC performance in relation to these lesion measures,
offering insights into segmentation accuracy and clinical relevance.
Additionally, we introduced three lesion detection criteria, addressing the
clinical need for identifying lesions, counting them, and segmenting based on
metabolic characteristics. We also performed expert intra-observer variability
analysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to
assist in the development of more resilient segmentation algorithms. Finally,
we performed inter-observer agreement assessment underscoring the importance of
a standardized ground truth segmentation protocol involving multiple expert
annotators. Code is available at:
https://github.com/microsoft/lymphoma-segmentation-dnn
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09623">Apoptosis classification using attention based spatio temporal graph convolution neural network. (arXiv:2311.09623v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1">Akash Awasthi</a></p>
<p>Accurate classification of apoptosis plays an important role in cell biology
research. There are many state-of-the-art approaches which use deep CNNs to
perform the apoptosis classification but these approaches do not account for
the cell interaction. Our paper proposes the Attention Graph spatio-temporal
graph convolutional network to classify the cell death based on the target
cells in the video. This method considers the interaction of multiple target
cells at each time stamp. We model the whole video sequence as a set of graphs
and classify the target cell in the video as dead or alive. Our method
encounters both spatial and temporal relationships.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09625">DECDM: Document Enhancement using Cycle-Consistent Diffusion Models. (arXiv:2311.09625v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rimchala_J/0/1/0/all/0/1">Joy Rimchala</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouatadid_L/0/1/0/all/0/1">Lalla Mouatadid</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1">Kamalika Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sricharan Kumar</a></p>
<p>The performance of optical character recognition (OCR) heavily relies on
document image quality, which is crucial for automatic document processing and
document intelligence. However, most existing document enhancement methods
require supervised data pairs, which raises concerns about data separation and
privacy protection, and makes it challenging to adapt these methods to new
domain pairs. To address these issues, we propose DECDM, an end-to-end
document-level image translation method inspired by recent advances in
diffusion models. Our method overcomes the limitations of paired training by
independently training the source (noisy input) and target (clean output)
models, making it possible to apply domain-specific diffusion models to other
pairs. DECDM trains on one dataset at a time, eliminating the need to scan both
datasets concurrently, and effectively preserving data privacy from the source
or target domain. We also introduce simple data augmentation strategies to
improve character-glyph conservation during translation. We compare DECDM with
state-of-the-art methods on multiple synthetic data and benchmark datasets,
such as document denoising and {\color{black}shadow} removal, and demonstrate
the superiority of performance quantitatively and qualitatively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09639">On the Quantification of Image Reconstruction Uncertainty without Training Data. (arXiv:2311.09639v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1">Sirui Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_V/0/1/0/all/0/1">Victor Fung</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a></p>
<p>Computational imaging plays a pivotal role in determining hidden information
from sparse measurements. A robust inverse solver is crucial to fully
characterize the uncertainty induced by these measurements, as it allows for
the estimation of the complete posterior of unrecoverable targets. This, in
turn, facilitates a probabilistic interpretation of observational data for
decision-making. In this study, we propose a deep variational framework that
leverages a deep generative model to learn an approximate posterior
distribution to effectively quantify image reconstruction uncertainty without
the need for training data. We parameterize the target posterior using a
flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve
accurate uncertainty estimation. To bolster stability, we introduce a robust
flow-based model with bi-directional regularization and enhance expressivity
through gradient boosting. Additionally, we incorporate a space-filling design
to achieve substantial variance reduction on both latent prior space and target
posterior space. We validate our method on several benchmark tasks and two
real-world applications, namely fastMRI and black hole image reconstruction.
Our results indicate that our method provides reliable and high-quality image
reconstruction with robust uncertainty estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09642">Weakly Supervised Anomaly Detection for Chest X-Ray Image. (arXiv:2311.09642v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ni_H/0/1/0/all/0/1">Haoqi Ni</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Ximiao Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Min Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lang_N/0/1/0/all/0/1">Ning Lang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1">Xiuzhuang Zhou</a></p>
<p>Chest X-Ray (CXR) examination is a common method for assessing thoracic
diseases in clinical applications. While recent advances in deep learning have
enhanced the significance of visual analysis for CXR anomaly detection, current
methods often miss key cues in anomaly images crucial for identifying disease
regions, as they predominantly rely on unsupervised training with normal
images. This letter focuses on a more practical setup in which few-shot anomaly
images with only image-level labels are available during training. For this
purpose, we propose WSCXR, a weakly supervised anomaly detection framework for
CXR. WSCXR firstly constructs sets of normal and anomaly image features
respectively. It then refines the anomaly image features by eliminating normal
region features through anomaly feature mining, thus fully leveraging the
scarce yet crucial features of diseased areas. Additionally, WSCXR employs a
linear mixing strategy to augment the anomaly features, facilitating the
training of anomaly detector with few-shot anomaly images. Experiments on two
CXR datasets demonstrate the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09646">Reconstructing Continuous Light Field From Single Coded Image. (arXiv:2311.09646v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_Y/0/1/0/all/0/1">Yuya Ishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1">Keita Takahashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsutake_C/0/1/0/all/0/1">Chihiro Tsutake</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujii_T/0/1/0/all/0/1">Toshiaki Fujii</a></p>
<p>We propose a method for reconstructing a continuous light field of a target
scene from a single observed image. Our method takes the best of two worlds:
joint aperture-exposure coding for compressive light-field acquisition, and a
neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding
implemented in a camera enables effective embedding of 3-D scene information
into an observed image, but in previous works, it was used only for
reconstructing discretized light-field views. NeRF-based neural rendering
enables high quality view synthesis of a 3-D scene from continuous viewpoints,
but when only a single image is given as the input, it struggles to achieve
satisfactory quality. Our method integrates these two techniques into an
efficient and end-to-end trainable pipeline. Trained on a wide variety of
scenes, our method can reconstruct continuous light fields accurately and
efficiently without any test time optimization. To our knowledge, this is the
first work to bridge two worlds: camera design for efficiently acquiring 3-D
information and neural rendering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09652">Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes. (arXiv:2311.09652v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dashpute_A/0/1/0/all/0/1">Aniket Dashpute</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiazhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1">James Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1">Oliver Cossairt</a>, <a href="http://arxiv.org/find/cs/1/au:+Veeraraghavan_A/0/1/0/all/0/1">Ashok Veeraraghavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Willomitzer_F/0/1/0/all/0/1">Florian Willomitzer</a></p>
<p>Event-based structured light systems have recently been introduced as an
exciting alternative to conventional frame-based triangulation systems for the
3D measurements of diffuse surfaces. Important benefits include the fast
capture speed and the high dynamic range provided by the event camera - albeit
at the cost of lower data quality. So far, both low-accuracy event-based as
well as high-accuracy frame-based 3D imaging systems are tailored to a specific
surface type, such as diffuse or specular, and can not be used for a broader
class of object surfaces ("mixed reflectance scenes"). In this paper, we
present a novel event-based structured light system that enables fast 3D
imaging of mixed reflectance scenes with high accuracy. On the captured events,
we use epipolar constraints that intrinsically enable decomposing the measured
reflections into diffuse, two-bounce specular, and other multi-bounce
reflections. The diffuse objects in the scene are reconstructed using
triangulation. Eventually, the reconstructed diffuse scene parts are used as a
"display" to evaluate the specular scene parts via deflectometry. This novel
procedure allows us to use the entire scene as a virtual screen, using only a
scanning laser and an event camera. The resulting system achieves fast and
motion-robust (14Hz) reconstructions of mixed reflectance scenes with &lt; 500
$\mu$m accuracy. Moreover, we introduce a "superfast" capture mode (250Hz) for
the 3D measurement of diffuse scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09653">Improved TokenPose with Sparsity. (arXiv:2311.09653v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Anning Li</a></p>
<p>Over the past few years, the vision transformer and its various forms have
gained significance in human pose estimation. By treating image patches as
tokens, transformers can capture global relationships wisely, estimate the
keypoint tokens by leveraging the visual tokens, and recognize the posture of
the human body. Nevertheless, global attention is computationally demanding,
which poses a challenge for scaling up transformer-based methods to
high-resolution features. In this paper, we introduce sparsity in both keypoint
token attention and visual token attention to improve human pose estimation.
Experimental results on the MPII dataset demonstrate that our model has a
higher level of accuracy and proved the feasibility of the method, achieving
new state-of-the-art results. The idea can also provide references for other
transformer-based models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09655">Multi-View Spectrogram Transformer for Respiratory Sound Classification. (arXiv:2311.09655v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wentao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuchen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jianfeng Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1">Ruibin Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xudong Jiang</a></p>
<p>Deep neural networks have been applied to audio spectrograms for respiratory
sound classification. Existing models often treat the spectrogram as a
synthetic image while overlooking its physical characteristics. In this paper,
a Multi-View Spectrogram Transformer (MVST) is proposed to embed different
views of time-frequency characteristics into the vision transformer.
Specifically, the proposed MVST splits the mel-spectrogram into different sized
patches, representing the multi-view acoustic elements of a respiratory sound.
These patches and positional embeddings are then fed into transformer encoders
to extract the attentional information among patches through a self-attention
mechanism. Finally, a gated fusion scheme is designed to automatically weigh
the multi-view features to highlight the best one in a specific scenario.
Experimental results on the ICBHI dataset demonstrate that the proposed MVST
significantly outperforms state-of-the-art methods for classifying respiratory
sounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09671">Robust Contrastive Learning With Theory Guarantee. (arXiv:2311.09671v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1">Ngoc N. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1">Lam Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1">Hoang Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bui_A/0/1/0/all/0/1">Anh Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Tung Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Toan Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1">Dinh Phung</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Trung Le</a></p>
<p>Contrastive learning (CL) is a self-supervised training paradigm that allows
us to extract meaningful features without any label information. A typical CL
framework is divided into two phases, where it first tries to learn the
features from unlabelled data, and then uses those features to train a linear
classifier with the labeled data. While a fair amount of existing theoretical
works have analyzed how the unsupervised loss in the first phase can support
the supervised loss in the second phase, none has examined the connection
between the unsupervised loss and the robust supervised loss, which can shed
light on how to construct an effective unsupervised loss for the first phase of
CL. To fill this gap, our work develops rigorous theories to dissect and
identify which components in the unsupervised loss can help improve the robust
supervised loss and conduct proper experiments to verify our findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09680">Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
in reliable methods. Despite the abundance of literature on trustworthy LMs in
language, a systematic survey specifically delving into the trustworthiness of
LMs in vision remains absent. In order to mitigate this gap, we summarize four
relevant concerns that obstruct the trustworthy usage in vision of LMs in this
survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers'
understanding of the field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09726">MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations. (arXiv:2311.09726v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenglai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinwang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changdong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianju Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a></p>
<p>Fully supervised change detection methods have achieved significant
advancements in performance, yet they depend severely on acquiring costly
pixel-level labels. Considering that the patch-level annotations also contain
abundant information corresponding to both changed and unchanged objects in
bi-temporal images, an intuitive solution is to segment the changes with
patch-level annotations. How to capture the semantic variations associated with
the changed and unchanged regions from the patch-level annotations to obtain
promising change results is the critical challenge for the weakly supervised
change detection task. In this paper, we propose a memory-supported transformer
(MS-Former), a novel framework consisting of a bi-directional attention block
(BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised
change detection with patch-level annotations. More specifically, the BAM
captures contexts associated with the changed and unchanged regions from the
temporal difference features to construct informative prototypes stored in the
memory bank. On the other hand, the BAM extracts useful information from the
prototypes as supplementary contexts to enhance the temporal difference
features, thereby better distinguishing changed and unchanged regions. After
that, the PSS guides the network learning valuable knowledge from the
patch-level annotations, thus further elevating the performance. Experimental
results on three benchmark datasets demonstrate the effectiveness of our
proposed method in the change detection task. The demo code for our work will
be publicly available at \url{https://github.com/guanyuezhen/MS-Former}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09737">Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation. (arXiv:2311.09737v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bingnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhitong Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuming He</a></p>
<p>Cross-modal MRI segmentation is of great value for computer-aided medical
diagnosis, enabling flexible data acquisition and model generalization.
However, most existing methods have difficulty in handling local variations in
domain shift and typically require a significant amount of data for training,
which hinders their usage in practice. To address these problems, we propose a
novel adaptive domain generalization framework, which integrates a
learning-free cross-domain representation based on image gradient maps and a
class prior-informed test-time adaptation strategy for mitigating local domain
shift. We validate our approach on two multi-modal MRI datasets with six
cross-modal segmentation tasks. Across all the task settings, our method
consistently outperforms competing approaches and shows a stable performance
even with limited training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09744">Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages. (arXiv:2311.09744v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1">Leopold M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemmer_P/0/1/0/all/0/1">Patrick Hemmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Queisner_M/0/1/0/all/0/1">Moritz Queisner</a>, <a href="http://arxiv.org/find/cs/1/au:+Sauer_I/0/1/0/all/0/1">Igor Sauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Allmendinger_S/0/1/0/all/0/1">Simeon Allmendinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakubik_J/0/1/0/all/0/1">Johannes Jakubik</a>, <a href="http://arxiv.org/find/cs/1/au:+Vossing_M/0/1/0/all/0/1">Michael V&#xf6;ssing</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a></p>
<p>A significant challenge in image-guided surgery is the accurate measurement
task of relevant structures such as vessel segments, resection margins, or
bowel lengths. While this task is an essential component of many surgeries, it
involves substantial human effort and is prone to inaccuracies. In this paper,
we develop a novel human-AI-based method for laparoscopic measurements
utilizing stereo vision that has been guided by practicing surgeons. Based on a
holistic qualitative requirements analysis, this work proposes a comprehensive
measurement method, which comprises state-of-the-art machine learning
architectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed
in various realistic experimental evaluation environments. Our results outline
the potential of our method achieving high accuracies in distance measurements
with errors below 1 mm. Furthermore, on-surface measurements demonstrate
robustness when applied in challenging environments with textureless regions.
Overall, by addressing the inherent challenges of image-guided surgery, we lay
the foundation for a more robust and accurate solution for intra- and
postoperative measurements, enabling more precise, safe, and efficient surgical
procedures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09753">DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics. (arXiv:2311.09753v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1">Aniket Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1">Maiterya Suin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Anshul Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1">Ketul Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1">Rama Chellappa</a></p>
<p>Diffusion models have advanced generative AI significantly in terms of
editing and creating naturalistic images. However, efficiently improving
generated image quality is still of paramount interest. In this context, we
propose a generic "naturalness" preserving loss function, viz., kurtosis
concentration (KC) loss, which can be readily applied to any standard diffusion
model pipeline to elevate the image quality. Our motivation stems from the
projected kurtosis concentration property of natural images, which states that
natural images have nearly constant kurtosis values across different band-pass
versions of the image. To retain the "naturalness" of the generated images, we
enforce reducing the gap between the highest and lowest kurtosis values across
the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note
that our approach does not require any additional guidance like classifier or
classifier-free guidance to improve the image quality. We validate the proposed
approach for three diverse tasks, viz., (1) personalized few-shot finetuning
using text guidance, (2) unconditional image generation, and (3) image
super-resolution. Integrating the proposed KC loss has improved the perceptual
quality across all these tasks in terms of both FID, MUSIQ score, and user
evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09757">UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution. (arXiv:2311.09757v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Le Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Li Yan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1">Tie Yong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1">Shi Hui Ying</a></p>
<p>Partially supervised segmentation is a label-saving method based on datasets
with fractional classes labeled and intersectant. However, it is still far from
landing on real-world medical applications due to privacy concerns and data
heterogeneity. As a remedy without privacy leakage, federated partially
supervised segmentation (FPSS) is formulated in this work. The main challenges
for FPSS are class heterogeneity and client drift. We propose a Unified
Federated Partially-labeled Segmentation (UFPS) framework to segment pixels
within all classes for partially-annotated datasets by training a totipotential
global model without class collision. Our framework includes Unified Label
Learning and sparsed Unified Sharpness Aware Minimization for unification of
class and feature space, respectively. We find that vanilla combinations for
traditional methods in partially supervised segmentation and federated learning
are mainly hampered by class collision through empirical study. Our
comprehensive experiments on real medical datasets demonstrate better
deconflicting and generalization ability of UFPS compared with modified
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09759">Scene Text Image Super-resolution based on Text-conditional Diffusion Models. (arXiv:2311.09759v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Noguchi_C/0/1/0/all/0/1">Chihiro Noguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fukuda_S/0/1/0/all/0/1">Shun Fukuda</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1">Masao Yamanaka</a></p>
<p>Scene Text Image Super-resolution (STISR) has recently achieved great success
as a preprocessing method for scene text recognition. STISR aims to transform
blurred and noisy low-resolution (LR) text images in real-world settings into
clear high-resolution (HR) text images suitable for scene text recognition. In
this study, we leverage text-conditional diffusion models (DMs), known for
their impressive text-to-image synthesis capabilities, for STISR tasks. Our
experimental results revealed that text-conditional DMs notably surpass
existing STISR methods. Especially when texts from LR text images are given as
input, the text-conditional DMs are able to produce superior quality
super-resolution text images. Utilizing this capability, we propose a novel
framework for synthesizing LR-HR paired text image datasets. This framework
consists of three specialized text-conditional DMs, each dedicated to text
image synthesis, super-resolution, and image degradation. These three modules
are vital for synthesizing distinct LR and HR paired images, which are more
suitable for training STISR methods. Our experiments confirmed that these
synthesized image pairs significantly enhance the performance of STISR methods
in the TextZoom evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09768">Utilizing dataset affinity prediction in object detection to assess training data. (arXiv:2311.09768v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1">Stefan Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1">Jens Bayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1">Ronny Hug</a>, <a href="http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1">Wolfgang H&#xfc;bner</a>, <a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1">Michael Arens</a></p>
<p>Data pooling offers various advantages, such as increasing the sample size,
improving generalization, reducing sampling bias, and addressing data sparsity
and quality, but it is not straightforward and may even be counterproductive.
Assessing the effectiveness of pooling datasets in a principled manner is
challenging due to the difficulty in estimating the overall information content
of individual datasets. Towards this end, we propose incorporating a data
source prediction module into standard object detection pipelines. The module
runs with minimal overhead during inference time, providing additional
information about the data source assigned to individual detections. We show
the benefits of the so-called dataset affinity score by automatically selecting
samples from a heterogeneous pool of vehicle datasets. The results show that
object detectors can be trained on a significantly sparser set of training
samples without losing detection accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09778">Certified Control for Train Sign Classification. (arXiv:2311.09778v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rossbach_J/0/1/0/all/0/1">Jan Ro&#xdf;bach</a> (Heinrich-Heine-Universit&#xe4;t D&#xfc;sseldorf), <a href="http://arxiv.org/find/cs/1/au:+Leuschel_M/0/1/0/all/0/1">Michael Leuschel</a> (Heinrich-Heine-Universit&#xe4;t D&#xfc;sseldorf)</p>
<p>There is considerable industrial interest in integrating AI techniques into
railway systems, notably for fully autonomous train systems. The KI-LOK
research project is involved in developing new methods for certifying such
AI-based systems. Here we explore the utility of a certified control
architecture for a runtime monitor that prevents false positive detection of
traffic signs in an AI-based perception system. The monitor uses classical
computer vision algorithms to check if the signs -- detected by an AI object
detection model -- fit predefined specifications. We provide such
specifications for some critical signs and integrate a Python prototype of the
monitor with a popular object detection model to measure relevant performance
metrics on generated data. Our initial results are promising, achieving
considerable precision gains with only minor recall reduction; however, further
investigation into generalization possibilities will be necessary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09806">EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices. (arXiv:2311.09806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jingnan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yichao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1">Bowen Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiangjing Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a></p>
<p>Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use surface-based
representations or differentiable volume rendering to generate high-fidelity
scenes. However, these methods require excessive runtime for rendering, making
them impractical for daily applications. To address these challenges, we
present $\textbf{EvaSurf}$, an $\textbf{E}$fficient
$\textbf{V}$iew-$\textbf{A}$ware Implicit Textured $\textbf{Surf}$ace
Reconstruction method on Mobile Devices. In our method, we first employ an
efficient surface-based model with a multi-view supervision module to ensure
accurate mesh creation. To enable high-fidelity rendering, we learn an implicit
texture embedded with a set of Gaussian lobes to capture view-dependent
information. Furthermore, With the explicit geometry and the implicit texture,
we can employ a lightweight neural shader to reduce the expense of computation
and further support real-time rendering on common mobile devices. Extensive
experiments demonstrate that our method can reconstruct high-quality appearance
and accurate mesh on both synthetic and real-world datasets. Moreover, our
method can be trained in just 1-2 hours using a single GPU and run on mobile
devices at over 40FPS (Frames Per Second), with a final package required for
rendering taking up only 40-50 MB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09817">Neural-Logic Human-Object Interaction Detection. (arXiv:2311.09817v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liulei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jianan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenguan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>The interaction decoder utilized in prevalent Transformer-based HOI detectors
typically accepts pre-composed human-object pairs as inputs. Though achieving
remarkable performance, such paradigm lacks feasibility and cannot explore
novel combinations over entities during decoding. We present L OGIC HOI, a new
HOI detector that leverages neural-logic reasoning and Transformer to infer
feasible interactions between entities. Specifically, we modify the
self-attention mechanism in vanilla Transformer, enabling it to reason over the
&lt;human, action, object&gt; triplet and constitute novel interactions. Meanwhile,
such reasoning process is guided by two crucial properties for understanding
HOI: affordances (the potential actions an object can facilitate) and proxemics
(the spatial relations between humans and objects). We formulate these two
properties in first-order logic and ground them into continuous space to
constrain the learning process of our approach, leading to improved performance
and zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and
HICO-DET under both normal and zero-shot setups, achieving significant
improvements over existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09819">PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments. (arXiv:2311.09819v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongbin Liu</a></p>
<p>In surgical procedures, correct instrument counting is essential. Instance
segmentation is a location method that locates not only an object's bounding
box but also each pixel's specific details. However, obtaining mask-level
annotations is labor-intensive in instance segmentation. To address this issue,
we propose a novel yet effective weakly-supervised surgical instrument instance
segmentation approach, named Point-based Weakly-supervised Instance
Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with
point-to-box and point-to-mask branches to model the relationships between
feature points and bounding boxes, as well as feature points and segmentation
masks on FPN, accomplishing instrument detection and segmentation jointly in a
single model. Since mask level annotations are hard to available in the real
world, for point-to-mask training, we introduce an unsupervised projection
loss, utilizing the projected relation between predicted masks and bboxes as
supervision signal. On the other hand, we annotate a few pixels as the key
pixel for each instrument. Based on this, we further propose a key pixel
association loss and a key pixel distribution loss, driving the point-to-mask
branch to generate more accurate segmentation predictions. To comprehensively
evaluate this task, we unveil a novel surgical instrument dataset with manual
annotations, setting up a benchmark for further research. Our comprehensive
research trial validated the superior performance of our PWISeg. The results
show that the accuracy of surgical instrument segmentation is improved,
surpassing most methods of instance segmentation via weakly supervised bounding
boxes. This improvement is consistently observed in our proposed dataset and
when applied to the public HOSPI-Tools dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09822">MAM-E: Mammographic synthetic image generation with diffusion models. (arXiv:2311.09822v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Montoya_del_Angel_R/0/1/0/all/0/1">Ricardo Montoya-del-Angel</a>, <a href="http://arxiv.org/find/eess/1/au:+Sam_Millan_K/0/1/0/all/0/1">Karla Sam-Millan</a>, <a href="http://arxiv.org/find/eess/1/au:+Vilanova_J/0/1/0/all/0/1">Joan C Vilanova</a>, <a href="http://arxiv.org/find/eess/1/au:+Marti_R/0/1/0/all/0/1">Robert Mart&#xed;</a></p>
<p>Generative models are used as an alternative data augmentation technique to
alleviate the data scarcity problem faced in the medical imaging field.
Diffusion models have gathered special attention due to their innovative
generation approach, the high quality of the generated images and their
relatively less complex training process compared with Generative Adversarial
Networks. Still, the implementation of such models in the medical domain
remains at early stages. In this work, we propose exploring the use of
diffusion models for the generation of high quality full-field digital
mammograms using state-of-the-art conditional diffusion pipelines.
Additionally, we propose using stable diffusion models for the inpainting of
synthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of
generative models for high quality mammography synthesis controlled by a text
prompt and capable of generating synthetic lesions on specific regions of the
breast. Finally, we provide quantitative and qualitative assessment of the
generated images and easy-to-use graphical user interfaces for mammography
synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09846">GroupMixer: Patch-based Group Convolutional Neural Network for Breast Cancer Detection from Histopathological Images. (arXiv:2311.09846v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Modarres_A/0/1/0/all/0/1">Ardavan Modarres</a>, <a href="http://arxiv.org/find/eess/1/au:+Esfahani_E/0/1/0/all/0/1">Erfan Ebrahim Esfahani</a>, <a href="http://arxiv.org/find/eess/1/au:+Bahrami_M/0/1/0/all/0/1">Mahsa Bahrami</a></p>
<p>Diagnosis of breast cancer malignancy at the early stages is a crucial step
for controlling its side effects. Histopathological analysis provides a unique
opportunity for malignant breast cancer detection. However, such a task would
be tedious and time-consuming for the histopathologists. Deep Neural Networks
enable us to learn informative features directly from raw histopathological
images without manual feature extraction. Although Convolutional Neural
Networks (CNNs) have been the dominant architectures in the computer vision
realm, Transformer-based architectures have shown promising results in
different computer vision tasks. Although harnessing the capability of
Transformer-based architectures for medical image analysis seems interesting,
these architectures are large, have a significant number of trainable
parameters, and require large datasets to be trained on, which are usually rare
in the medical domain. It has been claimed and empirically proved that at least
part of the superior performance of Transformer-based architectures in Computer
Vision domain originates from patch embedding operation. In this paper, we
borrowed the previously introduced idea of integrating a fully Convolutional
Neural Network architecture with Patch Embedding operation and presented an
efficient CNN architecture for breast cancer malignancy detection from
histopathological images. Despite the number of parameters that is
significantly smaller than other methods, the accuracy performance metrics
achieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x
magnifications respectively. We took a step forward and modified the
architecture using Group Convolution and Channel Shuffling ideas and reduced
the number of trainable parameters even more with a negligible decline in
performance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the
mentioned magnifications respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09847">Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model. (arXiv:2311.09847v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schafer_R/0/1/0/all/0/1">Raphael Sch&#xe4;fer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicke_T/0/1/0/all/0/1">Till Nicke</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofener_H/0/1/0/all/0/1">Henning H&#xf6;fener</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_A/0/1/0/all/0/1">Annkristin Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1">Dorit Merhof</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuerhake_F/0/1/0/all/0/1">Friedrich Feuerhake</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_V/0/1/0/all/0/1">Volkmar Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1">Johannes Lotz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiessling_F/0/1/0/all/0/1">Fabian Kiessling</a></p>
<p>Foundational models, pretrained on a large scale, have demonstrated
substantial success across non-medical domains. However, training these models
typically requires large, comprehensive datasets, which contrasts with the
smaller and more heterogeneous datasets common in biomedical imaging. Here, we
propose a multi-task learning strategy that decouples the number of training
tasks from memory requirements. We trained a Universal bioMedical PreTrained
model (UMedPT) on a multi-task database including tomographic, microscopic, and
X-ray images, with various labelling strategies such as classification,
segmentation, and object detection. The UMedPT foundational model outperformed
ImageNet pretraining and the previous state-of-the-art models. For tasks
related to the pretraining database, it maintained its performance with only 1%
of the original training data and without fine-tuning. For out-of-domain tasks
it required not more than 50% of the original training data. In an external
independent validation imaging features extracted using UMedPT proved to be a
new standard for cross-center transferability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09849">Rusty Detection Using Image Processing For Maintenance Of Stations. (arXiv:2311.09849v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tung_D/0/1/0/all/0/1">Dao Duy Tung</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1">Ho Xuan Hung</a></p>
<p>This study addresses the challenge of accurately seg-menting rusted areas on
painted construction surfaces. A method leveraging digital image processing is
explored to calculate the percentage of rust present on painted coatings. The
proposed segmentation approach is based on the HSV color model. To equalize
luminosity and mitigate the influence of illumination, a fundamental model of
single-scale Retinex is applied specifically to the saturation component.
</p>
<p>Subsequently, the image undergoes further processing, involv-ing manual color
filtering. This step is crucial for refining the identification of rusted
regions. To enhance precision and filter out noise, the pixel areas selected
through color filtering are subjected to the DBScan algorithm. This multi-step
process aims to achieve a robust segmentation of rusted areas on painted
construction surfaces, providing a valuable contribution to the field of
corrosion detection and analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09902">Selection of Distinct Morphologies to Divide &amp; Conquer Gigapixel Pathology Images. (arXiv:2311.09902v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shafique_A/0/1/0/all/0/1">Abubakr Shafique</a>, <a href="http://arxiv.org/find/cs/1/au:+Alfasly_S/0/1/0/all/0/1">Saghir Alfasly</a>, <a href="http://arxiv.org/find/cs/1/au:+Alsaafin_A/0/1/0/all/0/1">Areej Alsaafin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nejat_P/0/1/0/all/0/1">Peyman Nejat</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1">Jibran A. Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1">H.R.Tizhoosh</a></p>
<p>Whole slide images (WSIs) are massive digital pathology files illustrating
intricate tissue structures. Selecting a small, representative subset of
patches from each WSI is essential yet challenging. Therefore, following the
"Divide &amp; Conquer" approach becomes essential to facilitate WSI analysis
including the classification and the WSI matching in computational pathology.
To this end, we propose a novel method termed "Selection of Distinct
Morphologies" (SDM) to choose a subset of WSI patches. The aim is to encompass
all inherent morphological variations within a given WSI while simultaneously
minimizing the number of selected patches to represent these variations,
ensuring a compact yet comprehensive set of patches. This systematically
curated patch set forms what we term a "montage". We assess the
representativeness of the SDM montage across various public and private
histopathology datasets. This is conducted by using the leave-one-out WSI
search and matching evaluation method, comparing it with the state-of-the-art
Yottixel's mosaic. SDM demonstrates remarkable efficacy across all datasets
during its evaluation. Furthermore, SDM eliminates the necessity for empirical
parameterization, a crucial aspect of Yottixel's mosaic, by inherently
optimizing the selection process to capture the distinct morphological features
within the WSI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09919">DSR-Diff: Depth Map Super-Resolution with Diffusion Model. (arXiv:2311.09919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1">Bin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qingmin Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenming Yang</a></p>
<p>Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09939">RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection. (arXiv:2311.09939v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1">Stefanos-Iordanis Papadopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1">Christos Koutlis</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1">Symeon Papadopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrantonakis_P/0/1/0/all/0/1">Panagiotis C. Petrantonakis</a></p>
<p>Online misinformation is often multimodal in nature, i.e., it is caused by
misleading associations between texts and accompanying images. To support the
fact-checking process, researchers have been recently developing automatic
multimodal methods that gather and analyze external information, evidence,
related to the image-text pairs under examination. However, prior works assumed
all collected evidence to be relevant. In this study, we introduce a "Relevant
Evidence Detection" (RED) module to discern whether each piece of evidence is
relevant, to support or refute the claim. Specifically, we develop the
"Relevant Evidence Detection Directed Transformer" (RED-DOT) and explore
multiple architectural variants (e.g., single or dual-stage) and mechanisms
(e.g., "guided attention"). Extensive ablation and comparative experiments
demonstrate that RED-DOT achieves significant improvements over the
state-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our
evidence re-ranking and element-wise modality fusion led to RED-DOT achieving
competitive and even improved performance on NewsCLIPings+, without the need
for numerous evidence or multiple backbone encoders. Finally, our qualitative
analysis demonstrates that the proposed "guided attention" module has the
potential to enhance the architecture's interpretability. We release our code
at: https://github.com/stevejpapad/relevant-evidence-detection
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09942">Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection. (arXiv:2311.09942v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bechar_A/0/1/0/all/0/1">Amine Bechar</a>, <a href="http://arxiv.org/find/eess/1/au:+Elmir_Y/0/1/0/all/0/1">Youssef Elmir</a>, <a href="http://arxiv.org/find/eess/1/au:+Medjoudj_R/0/1/0/all/0/1">Rafik Medjoudj</a>, <a href="http://arxiv.org/find/eess/1/au:+Himeur_Y/0/1/0/all/0/1">Yassine Himeur</a>, <a href="http://arxiv.org/find/eess/1/au:+Amira_A/0/1/0/all/0/1">Abbes Amira</a></p>
<p>This paper discusses the role of Transfer Learning (TL) and transformers in
cancer detection based on image analysis. With the enormous evolution of cancer
patients, the identification of cancer cells in a patient's body has emerged as
a trend in the field of Artificial Intelligence (AI). This process involves
analyzing medical images, such as Computed Tomography (CT) scans and Magnetic
Resonance Imaging (MRIs), to identify abnormal growths that may help in cancer
detection. Many techniques and methods have been realized to improve the
quality and performance of cancer classification and detection, such as TL,
which allows the transfer of knowledge from one task to another with the same
task or domain. TL englobes many methods, particularly those used in image
analysis, such as transformers and Convolutional Neural Network (CNN) models
trained on the ImageNet dataset. This paper analyzes and criticizes each method
of TL based on image analysis and compares the results of each method, showing
that transformers have achieved the best results with an accuracy of 97.41% for
colon cancer detection and 94.71% for Histopathological Lung cancer. Future
directions for cancer detection based on image analysis are also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09952">Score-based generative models learn manifold-like structures with constrained mixing. (arXiv:2311.09952v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wenliang_L/0/1/0/all/0/1">Li Kevin Wenliang</a>, <a href="http://arxiv.org/find/stat/1/au:+Moran_B/0/1/0/all/0/1">Ben Moran</a></p>
<p>How do score-based generative models (SBMs) learn the data distribution
supported on a low-dimensional manifold? We investigate the score model of a
trained SBM through its linear approximations and subspaces spanned by local
feature vectors. During diffusion as the noise decreases, the local
dimensionality increases and becomes more varied between different sample
sequences. Importantly, we find that the learned vector field mixes samples by
a non-conservative field within the manifold, although it denoises with normal
projections as if there is an energy function in off-manifold directions. At
each noise level, the subspace spanned by the local features overlap with an
effective density function. These observations suggest that SBMs can flexibly
mix samples with the learned score field while carefully maintaining a
manifold-like structure of the data distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09958">VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model. (arXiv:2311.09958v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Klein_G/0/1/0/all/0/1">Geoff Klein</a>, <a href="http://arxiv.org/find/eess/1/au:+Hardisty_M/0/1/0/all/0/1">Michael Hardisty</a>, <a href="http://arxiv.org/find/eess/1/au:+Whyne_C/0/1/0/all/0/1">Cari Whyne</a>, <a href="http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1">Anne L. Martel</a></p>
<p>Vertebral detection and segmentation are critical steps for treatment
planning in spine surgery and radiation therapy. Accurate identification and
segmentation are complicated in imaging that does not include the full spine,
in cases with variations in anatomy (T13 and/or L6 vertebrae), and in the
presence of fracture or hardware. This paper proposes VertDetect, a fully
automated end-to-end 3D vertebral instance segmentation Convolutional Neural
Network (CNN) model to predict vertebral level labels and segmentations for all
vertebrae present in a CT scan. The utilization of a shared CNN backbone
provides the detection and segmentation branches of the network with feature
maps containing both spinal and vertebral level information. A Graph
Convolutional Network (GCN) layer is used to improve vertebral labelling by
using the known structure of the spine. This model achieved a Dice Similarity
Coefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI,
0.835-0.909) in the VerSe 2019 and 0.868 (95\% CI, 0.834-0.890) and 0.869 (95\%
CI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively.
This model achieved state-of-the-art performance for an end-to-end
architecture, whose design facilitates the extraction of features that can be
subsequently used for downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09965">SurgPLAN: Surgical Phase Localization Network for Phase Recognition. (arXiv:2311.09965v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xingjian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">You Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zongmin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongbin Liu</a></p>
<p>Surgical phase recognition is crucial to providing surgery understanding in
smart operating rooms. Despite great progress in automatic surgical phase
recognition, most existing methods are still restricted by two problems. First,
these methods cannot capture discriminative visual features for each frame and
motion information with simple 2D networks. Second, the frame-by-frame
recognition paradigm degrades the performance due to unstable predictions
within each phase, termed as phase shaking. To address these two challenges, we
propose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a
more accurate and stable surgical phase recognition with the principle of
temporal detection. Specifically, we first devise a Pyramid SlowFast (PSF)
architecture to serve as the visual backbone to capture multi-scale spatial and
temporal features by two branches with different frame sampling rates.
Moreover, we propose a Temporal Phase Localization (TPL) module to generate the
phase prediction based on temporal region proposals, which ensures accurate and
consistent predictions within each surgical phase. Extensive experiments
confirm the significant advantages of our SurgPLAN over frame-by-frame
approaches in terms of both accuracy and stability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09974">From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning. (arXiv:2311.09974v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiansong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peizhong Liu</a></p>
<p>In recent years, self-supervised contrastive learning has emerged as a
distinguished paradigm in the artificial intelligence landscape. It facilitates
unsupervised feature learning through contrastive delineations at the instance
level. However, crafting an effective self-supervised paradigm remains a
pivotal challenge within this field. This paper delves into two crucial factors
impacting self-supervised contrastive learning-bach size and pretext tasks, and
from a data processing standpoint, proposes an adaptive technique of batch
fusion. The proposed method, via dimensionality reduction and reconstruction of
batch data, enables formerly isolated individual data to partake in intra-batch
communication through the Embedding Layer. Moreover, it adaptively amplifies
the self-supervised feature encoding capability as the training progresses. We
conducted a linear classification test of this method based on the classic
contrastive learning framework on ImageNet-1k. The empirical findings
illustrate that our approach achieves state-of-the-art performance under
equitable comparisons. Benefiting from its "plug-and-play" characteristics, we
further explored other contrastive learning methods. On the ImageNet-100,
compared to the original performance, the top1 has seen a maximum increase of
1.25%. We suggest that the proposed method may contribute to the advancement of
data-driven self-supervised learning research, bringing a fresh perspective to
this community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09998">DeepEMD: A Transformer-based Fast Estimation of the Earth Mover&#x27;s Distance. (arXiv:2311.09998v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Atul Kumar Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1">Francois Fleuret</a></p>
<p>The Earth Mover's Distance (EMD) is the measure of choice between point
clouds. However the computational cost to compute it makes it prohibitive as a
training loss, and the standard approach is to use a surrogate such as the
Chamfer distance. We propose an attention-based model to compute an accurate
approximation of the EMD that can be used as a training loss for generative
models. To get the necessary accurate estimation of the gradients we train our
model to explicitly compute the matching between point clouds instead of EMD
itself. We cast this new objective as the estimation of an attention matrix
that approximates the ground truth matching matrix. Experiments show that this
model provides an accurate estimate of the EMD and its gradient with a wall
clock speed-up of more than two orders of magnitude with respect to the exact
Hungarian matching algorithm and one order of magnitude with respect to the
standard approximate Sinkhorn algorithm, allowing in particular to train a
point cloud VAE with the EMD itself. Extensive evaluation show the remarkable
behaviour of this model when operating out-of-distribution, a key requirement
for a distance surrogate. Finally, the model generalizes very well to point
clouds during inference several times larger than during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09999">TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection. (arXiv:2311.09999v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fucka_M/0/1/0/all/0/1">Matic Fu&#x10d;ka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1">Vitjan Zavrtanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Skocaj_D/0/1/0/all/0/1">Danijel Sko&#x10d;aj</a></p>
<p>Surface anomaly detection is a vital component in manufacturing inspection.
Reconstructive anomaly detection methods restore the normal appearance of an
object, ideally modifying only the anomalous regions. Due to the limitations of
commonly used reconstruction architectures, the produced reconstructions are
often poor and either still contain anomalies or lack details in anomaly-free
regions. Recent reconstructive methods adopt diffusion models, however with the
standard diffusion process the problems are not adequately addressed. We
propose a novel transparency-based diffusion process, where the transparency of
anomalous regions is progressively increased, restoring their normal appearance
accurately and maintaining the appearance of anomaly-free regions without loss
of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative
anomaly detection method that implements the proposed diffusion process,
enabling accurate downstream anomaly detection. TransFusion achieves
state-of-the-art performance on both the VisA and the MVTec AD datasets, with
an image-level AUROC of 98.5% and 99.2%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10115">An Efficient Smoothing and Thresholding Image Segmentation Framework with Weighted Anisotropic-Isotropic Total Variation. (arXiv:2202.10115v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1">Kevin Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1">Yifei Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1">Fredrick Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1">Jack Xin</a></p>
<p>In this paper, we design an efficient, multi-stage image segmentation
framework that incorporates a weighted difference of anisotropic and isotropic
total variation (AITV). The segmentation framework generally consists of two
stages: smoothing and thresholding, thus referred to as SaT. In the first
stage, a smoothed image is obtained by an AITV-regularized Mumford-Shah (MS)
model, which can be solved efficiently by the alternating direction method of
multipliers (ADMM) with a closed-form solution of a proximal operator of the
$\ell_1 -\alpha \ell_2$ regularizer. Convergence of the ADMM algorithm is
analyzed. In the second stage, we threshold the smoothed image by $K$-means
clustering to obtain the final segmentation result. Numerical experiments
demonstrate that the proposed segmentation framework is versatile for both
grayscale and color images, efficient in producing high-quality segmentation
results within a few seconds, and robust to input images that are corrupted
with noise, blur, or both. We compare the AITV method with its original convex
TV and nonconvex TV$^p (0&lt;p&lt;1)$ counterparts, showcasing the qualitative and
quantitative advantages of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.09048">Global Contrast Masked Autoencoders Are Powerful Pathological Representation Learners. (arXiv:2205.09048v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Quan_H/0/1/0/all/0/1">Hao Quan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xingyu Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1">Weixing Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1">Qun Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Zou_M/0/1/0/all/0/1">Mingchen Zou</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1">Ruijie Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1">Tingting Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Qi_R/0/1/0/all/0/1">Ruiqun Qi</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1">Xinghua Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Cui_X/0/1/0/all/0/1">Xiaoyu Cui</a></p>
<p>Based on digital pathology slice scanning technology, artificial intelligence
algorithms represented by deep learning have achieved remarkable results in the
field of computational pathology. Compared to other medical images, pathology
images are more difficult to annotate, and thus, there is an extreme lack of
available datasets for conducting supervised learning to train robust deep
learning models. In this paper, we propose a self-supervised learning (SSL)
model, the global contrast-masked autoencoder (GCMAE), which can train the
encoder to have the ability to represent local-global features of pathological
images, also significantly improve the performance of transfer learning across
data sets. In this study, the ability of the GCMAE to learn migratable
representations was demonstrated through extensive experiments using a total of
three different disease-specific hematoxylin and eosin (HE)-stained pathology
datasets: Camelyon16, NCTCRC and BreakHis. In addition, this study designed an
effective automated pathology diagnosis process based on the GCMAE for clinical
applications. The source code of this paper is publicly available at
https://github.com/StarUniversus/gcmae.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01125">Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep Reconstruction without Reference. (arXiv:2210.01125v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xiaodong Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1">Longhui Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Chang_D/0/1/0/all/0/1">Dingyue Chang</a>, <a href="http://arxiv.org/find/eess/1/au:+He_P/0/1/0/all/0/1">Peng He</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_P/0/1/0/all/0/1">Peng Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1">Hengyong Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1">Weiwen Wu</a></p>
<p>Spectral computed tomography based on a photon-counting detector (PCD)
attracts more and more attentions since it has the capability to provide more
accurate identification and quantitative analysis for biomedical materials. The
limited number of photons within narrow energy bins leads to imaging results of
low signal-noise ratio. The existing supervised deep reconstruction networks
for CT reconstruction are difficult to address these challenges because it is
usually impossible to acquire noise-free clinical images with clear structures
as references. In this paper, we propose an iterative deep reconstruction
network to synergize unsupervised method and data priors into a unified
framework, named as Spectral2Spectral. Our Spectral2Spectral employs an
unsupervised deep training strategy to obtain high-quality images from noisy
data in an end-to-end fashion. The structural similarity prior within
image-spectral domain is refined as a regularization term to further constrain
the network training. The weights of neural network are automatically updated
to capture image features and structures within the iterative process. Three
large-scale preclinical datasets experiments demonstrate that the
Spectral2spectral reconstructs better image quality than other the
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05673">Performance Deterioration of Deep Learning Models after Clinical Deployment: A Case Study with Auto-segmentation for Definitive Prostate Cancer Radiotherapy. (arXiv:2210.05673v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Biling Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Dohopolski_M/0/1/0/all/0/1">Michael Dohopolski</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_T/0/1/0/all/0/1">Ti Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1">Junjie Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Hannan_R/0/1/0/all/0/1">Raquibul Hannan</a>, <a href="http://arxiv.org/find/eess/1/au:+Desai_N/0/1/0/all/0/1">Neil Desai</a>, <a href="http://arxiv.org/find/eess/1/au:+Garant_A/0/1/0/all/0/1">Aurelie Garant</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1">Daniel Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1">Dan Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1">Mu-Han Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Timmerman_R/0/1/0/all/0/1">Robert Timmerman</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xinlei Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1">Steve Jiang</a></p>
<p>We evaluated the temporal performance of a deep learning (DL) based
artificial intelligence (AI) model for auto segmentation in prostate
radiotherapy, seeking to correlate its efficacy with changes in clinical
landscapes. Our study involved 1328 prostate cancer patients who underwent
definitive radiotherapy from January 2006 to August 2022 at the University of
Texas Southwestern Medical Center. We trained a UNet based segmentation model
on data from 2006 to 2011 and tested it on data from 2012 to 2022 to simulate
real world clinical deployment. We measured the model performance using the
Dice similarity coefficient (DSC), visualized the trends in contour quality
using exponentially weighted moving average (EMA) curves. Additionally, we
performed Wilcoxon Rank Sum Test to analyze the differences in DSC
distributions across distinct periods, and multiple linear regression to
investigate the impact of various clinical factors. The model exhibited peak
performance in the initial phase (from 2012 to 2014) for segmenting the
prostate, rectum, and bladder. However, we observed a notable decline in
performance for the prostate and rectum after 2015, while bladder contour
quality remained stable. Key factors that impacted the prostate contour quality
included physician contouring styles, the use of various hydrogel spacer, CT
scan slice thickness, MRI-guided contouring, and using intravenous (IV)
contrast. Rectum contour quality was influenced by factors such as slice
thickness, physician contouring styles, and the use of various hydrogel
spacers. The bladder contour quality was primarily affected by using IV
contrast. This study highlights the challenges in maintaining AI model
performance consistency in a dynamic clinical setting. It underscores the need
for continuous monitoring and updating of AI models to ensure their ongoing
effectiveness and relevance in patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10365">Data Consistent Deep Rigid MRI Motion Correction. (arXiv:2301.10365v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Singh_N/0/1/0/all/0/1">Nalini M. Singh</a>, <a href="http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1">Neel Dey</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1">Malte Hoffmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1">Bruce Fischl</a>, <a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1">Elfar Adalsteinsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Frost_R/0/1/0/all/0/1">Robert Frost</a>, <a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1">Adrian V. Dalca</a>, <a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1">Polina Golland</a></p>
<p>Motion artifacts are a pervasive problem in MRI, leading to misdiagnosis or
mischaracterization in population-level imaging studies. Current retrospective
rigid intra-slice motion correction techniques jointly optimize estimates of
the image and the motion parameters. In this paper, we use a deep network to
reduce the joint image-motion parameter search to a search over rigid motion
parameters alone. Our network produces a reconstruction as a function of two
inputs: corrupted k-space data and motion parameters. We train the network
using simulated, motion-corrupted k-space data generated with known motion
parameters. At test-time, we estimate unknown motion parameters by minimizing a
data consistency loss between the motion parameters, the network-based image
reconstruction given those parameters, and the acquired measurements.
Intra-slice motion correction experiments on simulated and realistic 2D fast
spin echo brain MRI achieve high reconstruction fidelity while providing the
benefits of explicit data consistency optimization. Our code is publicly
available at https://www.github.com/nalinimsingh/neuroMoCo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00903">No One Left Behind: Real-World Federated Class-Incremental Learning. (arXiv:2302.00903v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiahua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongliu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1">Yang Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Gan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yulun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>Federated learning (FL) is a hot collaborative training framework via
aggregating model parameters of decentralized local clients. However, most FL
methods unreasonably assume data categories of FL framework are known and fixed
in advance. Moreover, some new local clients that collect novel categories
unseen by other clients may be introduced to FL training irregularly. These
issues render global model to undergo catastrophic forgetting on old
categories, when local clients receive new categories consecutively under
limited memory of storing old categories. To tackle the above issues, we
propose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local
clients are left behind as they learn new classes continually, by addressing
local and global catastrophic forgetting. Specifically, considering tackling
class imbalance of local client to surmount local forgetting, we develop a
category-balanced gradient-adaptive compensation loss and a category
gradient-induced semantic distillation loss. They can balance heterogeneous
forgetting speeds of hard-to-forget and easy-to-forget old categories, while
ensure consistent class-relations within different tasks. Moreover, a proxy
server is designed to tackle global forgetting caused by Non-IID class
imbalance between different clients. It augments perturbed prototype images of
new categories collected from local clients via self-supervised prototype
augmentation, thus improving robustness to choose the best old global model for
local-side semantic distillation loss. Experiments on representative datasets
verify superior performance of our model against comparison methods. The code
is available at https://github.com/JiahuaDong/LGA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05425">Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments. (arXiv:2302.05425v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kara_E/0/1/0/all/0/1">Erdi Kara</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">George Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1">Joseph J. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrandez_Quinto_G/0/1/0/all/0/1">Gonzalo Ferrandez-Quinto</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhoden_L/0/1/0/all/0/1">Leviticus J. Rhoden</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Maximilian Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1">J. Nathan Kutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1">Aminur Rahman</a></p>
<p>We present a deep-learning based tracking objects of interest in walking
droplet and granular intruder experiments. In a typical walking droplet
experiment, a liquid droplet, known as \textit{walker}, propels itself
laterally on the free surface of a vibrating bath of the same liquid. This
motion is the result of the interaction between the droplets and the surface
waves generated by the droplet itself after each successive bounce. A walker
can exhibit a highly irregular trajectory over the course of its motion,
including rapid acceleration and complex interactions with the other walkers
present in the same bath. In analogy with the hydrodynamic experiments, the
granular matter experiments consist of a vibrating bath of very small solid
particles and a larger solid \textit{intruder}. Like the fluid droplets, the
intruder interacts with and travels the domain due to the waves of the bath but
tends to move much slower and much less smoothly than the droplets. When
multiple intruders are introduced, they also exhibit complex interactions with
each other. We leverage the state-of-art object detection model YOLO and the
Hungarian Algorithm to accurately extract the trajectory of a walker or
intruder in real-time. Our proposed methodology is capable of tracking
individual walker(s) or intruder(s) in digital images acquired from a broad
spectrum of experimental settings and does not suffer from any identity-switch
issues. Thus, the deep learning approach developed in this work could be used
to automatize the efficient, fast and accurate extraction of observables of
interests in walking droplet and granular flow experiments. Such extraction
capabilities are critically enabling for downstream tasks such as building
data-driven dynamical models for the coarse-grained dynamics and interactions
of the objects of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07661">Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds. (arXiv:2302.07661v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cortinhal_T/0/1/0/all/0/1">Tiago Cortinhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1">Eren Erdal Aksoy</a></p>
<p>This work presents a new depth- and semantics-aware conditional generative
model, named TITAN-Next, for cross-domain image-to-image translation in a
multi-modal setup between LiDAR and camera sensors. The proposed model
leverages scene semantics as a mid-level representation and is able to
translate raw LiDAR point clouds to RGB-D camera images by solely relying on
semantic scene segments. We claim that this is the first framework of its kind
and it has practical applications in autonomous vehicles such as providing a
fail-safe mechanism and augmenting available data in the target image domain.
The proposed model is evaluated on the large-scale and challenging
Semantic-KITTI dataset, and experimental findings show that it considerably
outperforms the original TITAN-Net and other strong baselines by 23.7$\%$
margin in terms of IoU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02575">MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition. (arXiv:2303.02575v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1">Ruiqi Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a></p>
<p>We present a novel approach for action recognition in UAV videos. Our
formulation is designed to handle occlusion and viewpoint changes caused by the
movement of a UAV. We use the concept of mutual information to compute and
align the regions corresponding to human action or motion in the temporal
domain. This enables our recognition model to learn from the key features
associated with the motion. We also propose a novel frame sampling method that
uses joint mutual information to acquire the most informative frame sequence in
UAV videos. We have integrated our approach with X3D and evaluated the
performance on multiple datasets. In practice, we achieve 18.9% improvement in
Top-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,
2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%
improvement on NEC Drones(Choi et al., 2020).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06817">Transformation-Invariant Network for Few-Shot Object Detection in Remote Sensing Images. (arXiv:2303.06817v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nanqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Celik_T/0/1/0/all/0/1">Turgay Celik</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zongxin Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng-Chao Li</a></p>
<p>Object detection in remote sensing images relies on a large amount of labeled
data for training. However, the increasing number of new categories and class
imbalance make exhaustive annotation impractical. Few-shot object detection
(FSOD) addresses this issue by leveraging meta-learning on seen base classes
and fine-tuning on novel classes with limited labeled samples. Nonetheless, the
substantial scale and orientation variations of objects in remote sensing
images pose significant challenges to existing few-shot object detection
methods. To overcome these challenges, we propose integrating a feature pyramid
network and utilizing prototype features to enhance query features, thereby
improving existing FSOD methods. We refer to this modified FSOD approach as a
Strong Baseline, which has demonstrated significant performance improvements
compared to the original baselines. Furthermore, we tackle the issue of spatial
misalignment caused by orientation variations between the query and support
images by introducing a Transformation-Invariant Network (TINet). TINet ensures
geometric invariance and explicitly aligns the features of the query and
support branches, resulting in additional performance gains while maintaining
the same inference speed as the Strong Baseline. Extensive experiments on three
widely used remote sensing object detection datasets, i.e., NWPU VHR-10.v2,
DIOR, and HRRSD demonstrated the effectiveness of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12470">Vision-based Estimation of Fatigue and Engagement in Cognitive Training Sessions. (arXiv:2304.12470v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanchen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Turnbull_A/0/1/0/all/0/1">Adam Turnbull</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yunlong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heffner_K/0/1/0/all/0/1">Kathi Heffner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Feng Vankee Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1">Ehsan Adeli</a></p>
<p>Computerized cognitive training (CCT) is a scalable, well-tolerated
intervention that has promise for slowing cognitive decline. Outcomes from CCT
are limited by a lack of effective engagement, which is decreased by factors
such as mental fatigue, particularly in older adults at risk for dementia.
There is a need for scalable, automated measures that can monitor mental
fatigue during CCT. Here, we develop and validate a novel Recurrent Video
Transformer (RVT) method for monitoring real-time mental fatigue in older
adults with mild cognitive impairment from video-recorded facial gestures
during CCT. The RVT model achieved the highest balanced accuracy(78%) and
precision (0.82) compared to the prior state-of-the-art models for binary and
multi-class classification of mental fatigue and was additionally validated via
significant association (p=0.023) with CCT reaction time. By leveraging dynamic
temporal information, the RVT model demonstrates the potential to accurately
measure real-time mental fatigue, laying the foundation for future personalized
CCT that increase effective engagement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05400">Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Siedel_G/0/1/0/all/0/1">Georg Siedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1">Weijia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vock_S/0/1/0/all/0/1">Silvia Vock</a>, <a href="http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1">Andrey Morozov</a></p>
<p>Robustness is a fundamental property of machine learning classifiers to
achieve safety and reliability. In the fields of adversarial robustness and
formal robustness verification of image classification models, robustness is
commonly defined as the stability to all input variations within an Lp-norm
distance. However, robustness to random corruptions is usually improved and
evaluated using variations observed in the real-world, while mathematically
defined Lp-norm corruptions are rarely considered. This study investigates the
use of random Lp-norm corruptions to augment the training and test data of
image classifiers. We adapt an approach from the field of adversarial
robustness to assess the model robustness to imperceptible random corruptions.
We empirically and theoretically investigate whether robustness is transferable
across different Lp-norms and derive conclusions on which Lp-norm corruptions a
model should be trained and evaluated on. We find that training data
augmentation with L0-norm corruptions improves corruption robustness while
maintaining accuracy compared to standard training and when applied on top of
selected state-of-the-art data augmentation techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15270">Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation. (arXiv:2305.15270v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Spitale_M/0/1/0/all/0/1">Micol Spitale</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1">Hatice Gunes</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Siyang Song</a></p>
<p>Generating facial reactions in a human-human dyadic interaction is complex
and highly dependent on the context since more than one facial reactions can be
appropriate for the speaker's behaviour. This has challenged existing machine
learning (ML) methods, whose training strategies enforce models to reproduce a
specific (not multiple) facial reaction from each input speaker behaviour. This
paper proposes the first multiple appropriate facial reaction generation
framework that re-formulates the one-to-many mapping facial reaction generation
problem as a one-to-one mapping problem. This means that we approach this
problem by considering the generation of a distribution of the listener's
appropriate facial reactions instead of multiple different appropriate facial
reactions, i.e., 'many' appropriate facial reaction labels are summarised as
'one' distribution label during training. Our model consists of a perceptual
processor, a cognitive processor, and a motor processor. The motor processor is
implemented with a novel Reversible Multi-dimensional Edge Graph Neural Network
(REGNN). This allows us to obtain a distribution of appropriate real facial
reactions during the training process, enabling the cognitive processor to be
trained to predict the appropriate facial reaction distribution. At the
inference stage, the REGNN decodes an appropriate facial reaction by using this
distribution as input. Experimental results demonstrate that our approach
outperforms existing models in generating more appropriate, realistic, and
synchronized facial reactions. The improved performance is largely attributed
to the proposed appropriate facial reaction distribution learning strategy and
the use of a REGNN. The code is available at
https://github.com/TongXu-05/REGNN-Multiple-Appropriate-Facial-Reaction-Generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17718">FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions. (arXiv:2305.17718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rotstein_N/0/1/0/all/0/1">Noam Rotstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Bensaid_D/0/1/0/all/0/1">David Bensaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Brody_S/0/1/0/all/0/1">Shaked Brody</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1">Roy Ganz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1">Ron Kimmel</a></p>
<p>The advent of vision-language pre-training techniques enhanced substantial
progress in the development of models for image captioning. However, these
models frequently produce generic captions and may omit semantically important
image details. This limitation can be traced back to the image-text datasets;
while their captions typically offer a general description of image content,
they frequently omit salient details. Considering the magnitude of these
datasets, manual reannotation is impractical, emphasizing the need for an
automated approach. To address this challenge, we leverage existing captions
and explore augmenting them with visual details using "frozen" vision experts
including an object detector, an attribute recognizer, and an Optical Character
Recognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such
vision experts with the original captions using a large language model (LLM),
yielding comprehensive image descriptions. We automatically curate a training
set of 12M image-enriched caption pairs. These pairs undergo extensive
evaluation through both quantitative and qualitative analyses. Subsequently,
this data is utilized to train a captioning generation BLIP-based model. This
model outperforms current state-of-the-art approaches, producing more precise
and detailed descriptions, demonstrating the effectiveness of the proposed
data-centric approach. We release this large-scale dataset of enriched
image-caption pairs for the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18651">UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (arXiv:2305.18651v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1">Zhen Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zidi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Backdoor (Trojan) attack is a common threat to deep neural networks, where
samples from one or more source classes embedded with a backdoor trigger will
be misclassified to adversarial target classes. Existing methods for detecting
whether a classifier is backdoor attacked are mostly designed for attacks with
a single adversarial target (e.g., all-to-one attack). To the best of our
knowledge, without supervision, no existing methods can effectively address the
more general X2X attack with an arbitrary number of source classes, each paired
with an arbitrary target class. In this paper, we propose UMD, the first
Unsupervised Model Detection method that effectively detects X2X backdoor
attacks via a joint inference of the adversarial (source, target) class pairs.
In particular, we first define a novel transferability statistic to measure and
select a subset of putative backdoor class pairs based on a proposed clustering
approach. Then, these selected class pairs are jointly assessed based on an
aggregation of their reverse-engineered trigger size for detection inference,
using a robust and unsupervised anomaly detector we proposed. We conduct
comprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show
that our unsupervised UMD outperforms SOTA detectors (even with supervision) by
17%, 4%, and 8%, respectively, in terms of the detection accuracy against
diverse X2X attacks. We also show the strong detection performance of UMD
against several strong adaptive attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19730">Data Representations&#x27; Study of Latent Image Manifolds. (arXiv:2305.19730v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaufman_I/0/1/0/all/0/1">Ilya Kaufman</a>, <a href="http://arxiv.org/find/cs/1/au:+Azencot_O/0/1/0/all/0/1">Omri Azencot</a></p>
<p>Deep neural networks have been demonstrated to achieve phenomenal success in
many domains, and yet their inner mechanisms are not well understood. In this
paper, we investigate the curvature of image manifolds, i.e., the manifold
deviation from being flat in its principal directions. We find that
state-of-the-art trained convolutional neural networks for image classification
have a characteristic curvature profile along layers: an initial steep
increase, followed by a long phase of a plateau, and followed by another
increase. In contrast, this behavior does not appear in untrained networks in
which the curvature flattens. We also show that the curvature gap between the
last two layers has a strong correlation with the generalization capability of
the network. Moreover, we find that the intrinsic dimension of latent codes is
not necessarily indicative of curvature. Finally, we observe that common
regularization methods such as mixup yield flatter representations when
compared to other methods. Our experiments show consistent results over a
variety of deep learning architectures and multiple data sets. Our code is
publicly available at https://github.com/azencot-group/CRLM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02582">Enhancing Point Annotations with Superpixel and Confidence Learning Guided for Improving Semi-Supervised OCT Fluid Segmentation. (arXiv:2306.02582v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1">Tengjin Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1">Kai Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhiming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunxiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gewen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaqi Wang</a></p>
<p>Automatic segmentation of fluid in Optical Coherence Tomography (OCT) images
is beneficial for ophthalmologists to make an accurate diagnosis. Although
semi-supervised OCT fluid segmentation networks enhance their performance by
introducing additional unlabeled data, the performance enhancement is limited.
To address this, we propose Superpixel and Confident Learning Guide Point
Annotations Network (SCLGPA-Net) based on the teacher-student architecture,
which can learn OCT fluid segmentation from limited fully-annotated data and
abundant point-annotated data. Specifically, we use points to annotate fluid
regions in unlabeled OCT images and the Superpixel-Guided Pseudo-Label
Generation (SGPLG) module generates pseudo-labels and pixel-level label trust
maps from the point annotations. The label trust maps provide an indication of
the reliability of the pseudo-labels. Furthermore, we propose the Confident
Learning Guided Label Refinement (CLGLR) module identifies error information in
the pseudo-labels and leads to further refinement. Experiments on the RETOUCH
dataset show that we are able to reduce the need for fully-annotated data by
94.22\%, closing the gap with the best fully supervised baselines to a mean IoU
of only 2\%. Furthermore, We constructed a private 2D OCT fluid segmentation
dataset for evaluation. Compared with other methods, comprehensive experimental
results demonstrate that the proposed method can achieve excellent performance
in OCT fluid segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12983">Towards More Realistic Membership Inference Attacks on Large Diffusion Models. (arXiv:2306.12983v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dubinski_J/0/1/0/all/0/1">Jan Dubi&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kowalczuk_A/0/1/0/all/0/1">Antoni Kowalczuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawlak_S/0/1/0/all/0/1">Stanis&#x142;aw Pawlak</a>, <a href="http://arxiv.org/find/cs/1/au:+Rokita_P/0/1/0/all/0/1">Przemys&#x142;aw Rokita</a>, <a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Morawiecki_P/0/1/0/all/0/1">Pawe&#x142; Morawiecki</a></p>
<p>Generative diffusion models, including Stable Diffusion and Midjourney, can
generate visually appealing, diverse, and high-resolution images for various
applications. These models are trained on billions of internet-sourced images,
raising significant concerns about the potential unauthorized use of
copyright-protected images. In this paper, we examine whether it is possible to
determine if a specific image was used in the training set, a problem known in
the cybersecurity community and referred to as a membership inference attack.
Our focus is on Stable Diffusion, and we address the challenge of designing a
fair evaluation framework to answer this membership question. We propose a
methodology to establish a fair evaluation setup and apply it to Stable
Diffusion, enabling potential extensions to other generative models. Utilizing
this evaluation setup, we execute membership attacks (both known and newly
introduced). Our research reveals that previously proposed evaluation setups do
not provide a full understanding of the effectiveness of membership inference
attacks. We conclude that the membership inference attack remains a significant
challenge for large diffusion models (often deployed as black-box systems),
indicating that related privacy and copyright issues will persist in the
foreseeable future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14752">MedLSAM: Localize and Segment Anything Model for 3D CT Images. (arXiv:2306.14752v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1">Wenhui Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoting Zhang</a></p>
<p>The Segment Anything Model (SAM) has recently emerged as a groundbreaking
model in the field of image segmentation. Nevertheless, both the original SAM
and its medical adaptations necessitate slice-by-slice annotations, which
directly increase the annotation workload with the size of the dataset. We
propose MedLSAM to address this issue, ensuring a constant annotation workload
irrespective of dataset size and thereby simplifying the annotation process.
Our model introduces a 3D localization foundation model capable of localizing
any target anatomical part within the body. To achieve this, we develop a
Localize Anything Model for 3D Medical Images (MedLAM), utilizing two
self-supervision tasks: unified anatomical mapping (UAM) and multi-scale
similarity (MSS) across a comprehensive dataset of 14,012 CT scans. We then
establish a methodology for accurate segmentation by integrating MedLAM with
SAM. By annotating several extreme points across three directions on a few
templates, our model can autonomously identify the target anatomical region on
all data scheduled for annotation. This allows our framework to generate a 2D
bbox for every slice of the image, which is then leveraged by SAM to carry out
segmentation. We carried out comprehensive experiments on two 3D datasets
encompassing 38 distinct organs. Our findings are twofold: 1) MedLAM is capable
of directly localizing any anatomical structure using just a few template
scans, yet its performance surpasses that of fully supervised models; 2)
MedLSAM not only aligns closely with the performance of SAM and its specialized
medical adaptations with manual prompts but achieves this with minimal reliance
on extreme point annotations across the entire dataset. Furthermore, MedLAM has
the potential to be seamlessly integrated with future 3D SAM models, paving the
way for enhanced performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10915">Revisiting Fine-Tuning Strategies for Self-supervised Medical Imaging Analysis. (arXiv:2307.10915v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Muhammad Osama Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yi Fang</a></p>
<p>Despite the rapid progress in self-supervised learning (SSL), end-to-end
fine-tuning still remains the dominant fine-tuning strategy for medical imaging
analysis. However, it remains unclear whether this approach is truly optimal
for effectively utilizing the pre-trained knowledge, especially considering the
diverse categories of SSL that capture different types of features. In this
paper, we present the first comprehensive study that discovers effective
fine-tuning strategies for self-supervised learning in medical imaging. After
developing strong contrastive and restorative SSL baselines that outperform
SOTA methods across four diverse downstream tasks, we conduct an extensive
fine-tuning analysis across multiple pre-training and fine-tuning datasets, as
well as various fine-tuning dataset sizes. Contrary to the conventional wisdom
of fine-tuning only the last few layers of a pre-trained network, we show that
fine-tuning intermediate layers is more effective, with fine-tuning the second
quarter (25-50%) of the network being optimal for contrastive SSL whereas
fine-tuning the third quarter (50-75%) of the network being optimal for
restorative SSL. Compared to the de-facto standard of end-to-end fine-tuning,
our best fine-tuning strategy, which fine-tunes a shallower network consisting
of the first three quarters (0-75%) of the pre-trained network, yields
improvements of as much as 5.48%. Additionally, using these insights, we
propose a simple yet effective method to leverage the complementary strengths
of multiple SSL models, resulting in enhancements of up to 3.57% compared to
using the best model alone. Hence, our fine-tuning strategies not only enhance
the performance of individual SSL models, but also enable effective utilization
of the complementary strengths offered by multiple SSL models, leading to
significant improvements in self-supervised medical imaging analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12327">End-to-end Hyperspectral Image Change Detection Network Based on Band Selection. (arXiv:2307.12327v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_Q/0/1/0/all/0/1">Qingren Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yuan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1">Chang Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1">Wei Xiang</a></p>
<p>For hyperspectral image change detection (HSI-CD), one key challenge is to
reduce band redundancy, as only a few bands are crucial for change detection
while other bands may be adverse to it. However, most existing HSI-CD methods
directly extract change feature from full-dimensional HSIs, suffering from a
degradation of feature discrimination. To address this issue, we propose an
end-to-end hyperspectral image change detection network with band selection
(ECDBS), which effectively retains the critical bands to promote change
detection. The main ingredients of the network are a deep learning based band
selection module and cascading band-specific spatial attention (BSA) blocks.
The band selection module can be seamlessly integrated with subsequent CD
models for joint optimization and end-to-end reasoning, rather than as a step
separate from change detection. The BSA block extracts features from each band
using a tailored strategy. Unlike the typically used feature extraction
strategy that uniformly processes all bands, the BSA blocks considers the
differences in feature distributions among widely spaced bands, thereupon
extracting more sufficient change feature. Experimental evaluations conducted
on three widely used HSI-CD datasets demonstrate the effectiveness and
superiority of our proposed method over other state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02463">Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&amp;3D Medical Data. (arXiv:2308.02463v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chaoyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoman Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a></p>
<p>In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM. We consider the construction of foundational models
from three perspectives, namely, dataset construction, model design, and
thorough evaluation. Our contribution can be concluded as follows: (i), we
construct a large-scale Medical Multi-modal Dataset, MedMD, which consists of
16M 2D and 3D medical scans with high-quality text descriptions or reports
across various data formats, modalities, and tasks, covering over 5000 distinct
diseases. To the best of our knowledge, this is the first large-scale,
high-quality, medical visual-language dataset, with both 2D and 3D scans; (ii),
we propose an architecture that enables visually conditioned generative
pre-training, i.e., allowing for integration of text input with 2D or 3D
medical scans, and generate responses for diverse radiologic tasks. The model
was initially pre-trained on MedMD and subsequently fine-tuned on the
domain-specific dataset, which is a radiologic cleaned version of MedMD,
containing 3M radiologic visual-language pairs, termed as RadMD; (iii), we
propose a new evaluation benchmark, RadBench, that comprises five tasks,
including modality recognition, disease diagnosis, visual question answering,
report generation and rationale diagnosis, aiming to comprehensively assess the
capability of foundation models in handling practical clinical problems. We
conduct both automatic and human evaluation on RadBench, in both cases, RadFM
outperforms existing multi-modal foundation models, that are publicaly
accessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V.
Additionally, we also adapt RadFM for different public benchmarks, surpassing
existing SOTAs on diverse datasets. All codes, data, and model checkpoint will
all be made publicly available to promote further research and development in
the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02340">Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm. (arXiv:2309.02340v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1">Alhasan Abdellatif</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsheikh_A/0/1/0/all/0/1">Ahmed H. Elsheikh</a></p>
<p>In this paper, we introduce a novel approach for generating texture images of
infinite resolutions using Generative Adversarial Networks (GANs) based on a
patch-by-patch paradigm. Existing texture synthesis techniques often rely on
generating a large-scale texture using a one-forward pass to the generating
model, this limits the scalability and flexibility of the generated images. In
contrast, the proposed approach trains GANs models on a single texture image to
generate relatively small patches that are locally correlated and can be
seamlessly concatenated to form a larger image while using a constant GPU
memory footprint. Our method learns the local texture structure and is able to
generate arbitrary-size textures, while also maintaining coherence and
diversity. The proposed method relies on local padding in the generator to
ensure consistency between patches and utilizes spatial stochastic modulation
to allow for local variations and diversity within the large-scale image.
Experimental results demonstrate superior scalability compared to existing
approaches while maintaining visual coherence of generated textures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05405">Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT. (arXiv:2309.05405v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1">Wentao Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tian_T/0/1/0/all/0/1">Tong Tian</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1">Weijin Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1">Lemeng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Haoyuan Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1">Huihua Yang</a></p>
<p>Abdominal organ and tumour segmentation has many important clinical
applications, such as organ quantification, surgical planning, and disease
diagnosis. However, manual assessment is inherently subjective with
considerable inter- and intra-expert variability. In the paper, we propose a
hybrid supervised framework, StMt, that integrates self-training and mean
teacher for the segmentation of abdominal organs and tumors using partially
labeled and unlabeled data. We introduce a two-stage segmentation pipeline and
whole-volume-based input strategy to maximize segmentation accuracy while
meeting the requirements of inference time and GPU memory usage. Experiments on
the validation set of FLARE2023 demonstrate that our method achieves excellent
segmentation performance as well as fast and low-resource model inference. Our
method achieved an average DSC score of 89.79\% and 45.55 \% for the organs and
lesions on the validation set and the average running time and area under GPU
memory-time cure are 11.25s and 9627.82MB, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06978">Differentiable JPEG: The Devil is in the Details. (arXiv:2309.06978v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1">Christoph Reich</a>, <a href="http://arxiv.org/find/cs/1/au:+Debnath_B/0/1/0/all/0/1">Biplob Debnath</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1">Deep Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1">Srimat Chakradhar</a></p>
<p>JPEG remains one of the most widespread lossy image coding methods. However,
the non-differentiable nature of JPEG restricts the application in deep
learning pipelines. Several differentiable approximations of JPEG have recently
been proposed to address this issue. This paper conducts a comprehensive review
of existing diff. JPEG approaches and identifies critical details that have
been missed by previous methods. To this end, we propose a novel diff. JPEG
approach, overcoming previous limitations. Our approach is differentiable
w.r.t. the input image, the JPEG quality, the quantization tables, and the
color conversion parameters. We evaluate the forward and backward performance
of our diff. JPEG approach against existing methods. Additionally, extensive
ablations are performed to evaluate crucial design choices. Our proposed diff.
JPEG resembles the (non-diff.) reference implementation best, significantly
surpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. For
strong compression rates, we can even improve PSNR by $9.51$dB. Strong
adversarial attack results are yielded by our diff. JPEG, demonstrating the
effective gradient approximation. Our code is available at
https://github.com/necla-ml/Diff-JPEG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09444">Tackling Heterogeneity in Medical Federated learning via Vision Transformers. (arXiv:2310.09444v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Darzi_E/0/1/0/all/0/1">Erfan Darzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yiqing Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1">Yangming Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sijtsema_N/0/1/0/all/0/1">Nanna M. Sijtsema</a>, <a href="http://arxiv.org/find/cs/1/au:+Ooijen_P/0/1/0/all/0/1">P.M.A van Ooijen</a></p>
<p>Optimization-based regularization methods have been effective in addressing
the challenges posed by data heterogeneity in medical federated learning,
particularly in improving the performance of underrepresented clients. However,
these methods often lead to lower overall model accuracy and slower convergence
rates. In this paper, we demonstrate that using Vision Transformers can
substantially improve the performance of underrepresented clients without a
significant trade-off in overall accuracy. This improvement is attributed to
the Vision transformer's ability to capture long-range dependencies within the
input data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16872">SonoSAMTrack -- Segment and Track Anything on Ultrasound Images. (arXiv:2310.16872v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ravishankar_H/0/1/0/all/0/1">Hariharan Ravishankar</a>, <a href="http://arxiv.org/find/eess/1/au:+Patil_R/0/1/0/all/0/1">Rohan Patil</a>, <a href="http://arxiv.org/find/eess/1/au:+Melapudi_V/0/1/0/all/0/1">Vikram Melapudi</a>, <a href="http://arxiv.org/find/eess/1/au:+Suthar_H/0/1/0/all/0/1">Harsh Suthar</a>, <a href="http://arxiv.org/find/eess/1/au:+Anzengruber_S/0/1/0/all/0/1">Stephan Anzengruber</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhatia_P/0/1/0/all/0/1">Parminder Bhatia</a>, <a href="http://arxiv.org/find/eess/1/au:+Taha_K/0/1/0/all/0/1">Kass-Hout Taha</a>, <a href="http://arxiv.org/find/eess/1/au:+Annangi_P/0/1/0/all/0/1">Pavan Annangi</a></p>
<p>In this paper, we present SonoSAMTrack - that combines a promptable
foundational model for segmenting objects of interest on ultrasound images
called SonoSAM, with a state-of-the art contour tracking model to propagate
segmentations on 2D+t and 3D ultrasound datasets. Fine-tuned and tested
exclusively on a rich, diverse set of objects from $\approx200$k ultrasound
image-mask pairs, SonoSAM demonstrates state-of-the-art performance on 7 unseen
ultrasound data-sets, outperforming competing methods by a significant margin.
We also extend SonoSAM to 2-D +t applications and demonstrate superior
performance making it a valuable tool for generating dense annotations and
segmentation of anatomical structures in clinical workflows. Further, to
increase practical utility of the work, we propose a two-step process of
fine-tuning followed by knowledge distillation to a smaller footprint model
without comprising the performance. We present detailed qualitative and
quantitative comparisons of SonoSAM with state-of-the-art methods showcasing
efficacy of the method. This is followed by demonstrating the reduction in
number of clicks in a dense video annotation problem of adult cardiac
ultrasound chamber segmentation using SonoSAMTrack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18583">Self-Supervised Multi-Modality Learning for Multi-Label Skin Lesion Classification. (arXiv:2310.18583v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1">Euijoon Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1">Lei Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinman Kim</a></p>
<p>The clinical diagnosis of skin lesion involves the analysis of dermoscopic
and clinical modalities. Dermoscopic images provide a detailed view of the
surface structures whereas clinical images offer a complementary macroscopic
information. The visual diagnosis of melanoma is also based on seven-point
checklist which involves identifying different visual attributes. Recently,
supervised learning approaches such as convolutional neural networks (CNNs)
have shown great performances using both dermoscopic and clinical modalities
(Multi-modality). The seven different visual attributes in the checklist are
also used to further improve the the diagnosis. The performances of these
approaches, however, are still reliant on the availability of large-scaled
labeled data. The acquisition of annotated dataset is an expensive and
time-consuming task, more so with annotating multi-attributes. To overcome this
limitation, we propose a self-supervised learning (SSL) algorithm for
multi-modality skin lesion classification. Our algorithm enables the
multi-modality learning by maximizing the similarities between paired
dermoscopic and clinical images from different views. In addition, we generate
surrogate pseudo-multi-labels that represent seven attributes via clustering
analysis. We also propose a label-relation-aware module to refine each
pseudo-label embedding and capture the interrelationships between
pseudo-multi-labels. We validated the effectiveness of our algorithm using
well-benchmarked seven-point skin lesion dataset. Our results show that our
algorithm achieved better performances than other state-of-the-art SSL
counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01655">Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification. (arXiv:2311.01655v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dammu_P/0/1/0/all/0/1">Preetam Prabhu Srikar Dammu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1">Chirag Shah</a></p>
<p>Often machine learning models tend to automatically learn associations
present in the training data without questioning their validity or
appropriateness. This undesirable property is the root cause of the
manifestation of spurious correlations, which render models unreliable and
prone to failure in the presence of distribution shifts. Research shows that
most methods attempting to remedy spurious correlations are only effective for
a model's known spurious associations. Current spurious correlation detection
algorithms either rely on extensive human annotations or are too restrictive in
their formulation. Moreover, they rely on strict definitions of visual
artifacts that may not apply to data produced by generative models, as they are
known to hallucinate contents that do not conform to standard specifications.
In this work, we introduce a general-purpose method that efficiently detects
potential spurious correlations, and requires significantly less human
interference in comparison to the prior art. Additionally, the proposed method
provides intuitive explanations while eliminating the need for pixel-level
annotations. We demonstrate the proposed method's tolerance to the peculiarity
of AI-generated images, which is a considerably challenging task, one where
most of the existing methods fall short. Consequently, our method is also
suitable for detecting spurious correlations that may propagate to downstream
applications originating from generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01766">Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weidong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shujun Li</a></p>
<p>Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02782">Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead. (arXiv:2311.02782v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yunkang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaonan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weiming Shen</a></p>
<p>Anomaly detection is a crucial task across different domains and data types.
However, existing anomaly detection models are often designed for specific
domains and modalities. This study explores the use of GPT-4V(ision), a
powerful visual-linguistic model, to address anomaly detection tasks in a
generic manner. We investigate the application of GPT-4V in multi-modality,
multi-domain anomaly detection tasks, including image, video, point cloud, and
time series data, across multiple application areas, such as industrial,
medical, logical, video, 3D anomaly detection, and localization tasks. To
enhance GPT-4V's performance, we incorporate different kinds of additional cues
such as class information, human expertise, and reference images as
prompts.Based on our experiments, GPT-4V proves to be highly effective in
detecting and explaining global and fine-grained semantic patterns in
zero/one-shot anomaly detection. This enables accurate differentiation between
normal and abnormal instances. Although we conducted extensive evaluations in
this study, there is still room for future evaluation to further exploit
GPT-4V's generic anomaly detection capacity from different aspects. These
include exploring quantitative metrics, expanding evaluation benchmarks,
incorporating multi-round interactions, and incorporating human feedback loops.
Nevertheless, GPT-4V exhibits promising performance in generic anomaly
detection and understanding, thus opening up a new avenue for anomaly
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04950">Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search. (arXiv:2311.04950v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1">Chaoyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+zhu_W/0/1/0/all/0/1">Wenwu zhu</a></p>
<p>Diffusion models have recently shown remarkable generation ability, achieving
state-of-the-art performance in many tasks. However, the high computational
cost is still a troubling problem for diffusion models. To tackle this problem,
we propose to automatically remove the structural redundancy in diffusion
models with our proposed Diffusion Distillation-based Block-wise Neural
Architecture Search (DiffNAS). Specifically, given a larger pretrained teacher,
we leverage DiffNAS to search for the smallest architecture which can achieve
on-par or even better performance than the teacher. Considering current
diffusion models are based on UNet which naturally has a block-wise structure,
we perform neural architecture search independently in each block, which
largely reduces the search space. Different from previous block-wise NAS
methods, DiffNAS contains a block-wise local search strategy and a retraining
strategy with a joint dynamic loss. Concretely, during the search process, we
block-wisely select the best subnet to avoid the unfairness brought by the
global search strategy used in previous works. When retraining the searched
architecture, we adopt a dynamic joint loss to maintain the consistency between
supernet training and subnet retraining, which also provides informative
objectives for each block and shortens the paths of gradient propagation. We
demonstrate this joint loss can effectively improve model performance. We also
prove the necessity of the dynamic adjustment of this loss. The experiments
show that our method can achieve significant computational reduction,
especially on latent diffusion models with about 50\% MACs and Parameter
reduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05237">Widely Applicable Strong Baseline for Sports Ball Detection and Tracking. (arXiv:2311.05237v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tarashima_S/0/1/0/all/0/1">Shuhei Tarashima</a>, <a href="http://arxiv.org/find/cs/1/au:+Haq_M/0/1/0/all/0/1">Muhammad Abdul Haq</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yushan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagawa_N/0/1/0/all/0/1">Norio Tagawa</a></p>
<p>In this work, we present a novel Sports Ball Detection and Tracking (SBDT)
method that can be applied to various sports categories. Our approach is
composed of (1) high-resolution feature extraction, (2) position-aware model
training, and (3) inference considering temporal consistency, all of which are
put together as a new SBDT baseline. Besides, to validate the
wide-applicability of our approach, we compare our baseline with 6
state-of-the-art SBDT methods on 5 datasets from different sports categories.
We achieve this by newly introducing two SBDT datasets, providing new ball
annotations for two datasets, and re-implementing all the methods to ease
extensive comparison. Experimental results demonstrate that our approach is
substantially superior to existing methods on all the sports categories covered
by the datasets. We believe our proposed method can play as a Widely Applicable
Strong Baseline (WASB) of SBDT, and our datasets and codebase will promote
future SBDT research. Datasets and codes are available at
https://github.com/nttcom/WASB-SBDT .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05371">Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data. (arXiv:2311.05371v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1">Yuxuan Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuntang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shwetak Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+McDuf_D/0/1/0/all/0/1">Daniel McDuf</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuzhe Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a></p>
<p>Recent advances in supervised deep learning techniques have demonstrated the
possibility to remotely measure human physiological vital signs (e.g.,
photoplethysmograph, heart rate) just from facial videos. However, the
performance of these methods heavily relies on the availability and diversity
of real labeled data. Yet, collecting large-scale real-world data with
high-quality labels is typically challenging and resource intensive, which also
raises privacy concerns when storing personal bio-metric data. Synthetic
video-based datasets (e.g., SCAMPS \cite{mcduff2022scamps}) with
photo-realistic synthesized avatars are introduced to alleviate the issues
while providing high-quality synthetic data. However, there exists a
significant gap between synthetic and real-world data, which hinders the
generalization of neural models trained on these synthetic datasets. In this
paper, we proposed several measures to add real-world noise to synthetic
physiological signals and corresponding facial videos. We experimented with
individual and combined augmentation methods and evaluated our framework on
three public real-world datasets. Our results show that we were able to reduce
the average MAE from 6.9 to 2.0.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06694">Comparative Multi-View Language Grounding. (arXiv:2311.06694v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1">Chancharik Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Abrar Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Corona_R/0/1/0/all/0/1">Rodolfo Corona</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>In this work, we consider the task of resolving object referents when given a
comparative language description. We present a Multi-view Approach to Grounding
in Context (MAGiC) that leverages transformers to pragmatically reason over
both objects given multiple image views and a language description. In contrast
to past efforts that attempt to connect vision and language for this task
without fully considering the resulting referential context, MAGiC makes use of
the comparative information by jointly reasoning over multiple views of both
object referent candidates and the referring language expression. We present an
analysis demonstrating that comparative reasoning contributes to SOTA
performance on the SNARE object reference task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07370">Classification of developmental and brain disorders via graph convolutional aggregation. (arXiv:2311.07370v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salim_I/0/1/0/all/0/1">Ibrahim Salim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1">A. Ben Hamza</a></p>
<p>While graph convolution based methods have become the de-facto standard for
graph representation learning, their applications to disease prediction tasks
remain quite limited, particularly in the classification of neurodevelopmental
and neurodegenerative brain disorders. In this paper, we introduce an
aggregator normalization graph convolutional network by leveraging aggregation
in graph sampling, as well as skip connections and identity mapping. The
proposed model learns discriminative graph node representations by
incorporating both imaging and non-imaging features into the graph nodes and
edges, respectively, with the aim of augmenting predictive capabilities and
providing a holistic perspective on the underlying mechanisms of brain
disorders. Skip connections enable the direct flow of information from the
input features to later layers of the network, while identity mapping helps
maintain the structural information of the graph during feature learning. We
benchmark our model against several recent baseline methods on two large
datasets, Autism Brain Imaging Data Exchange (ABIDE) and Alzheimer's Disease
Neuroimaging Initiative (ADNI), for the prediction of autism spectrum disorder
and Alzheimer's disease, respectively. Experimental results demonstrate the
competitive performance of our approach in comparison with recent baselines in
terms of several evaluation metrics, achieving relative improvements of 50% and
13.56% in classification accuracy over graph convolutional networks on ABIDE
and ADNI, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07622">Pretrain like Your Inference: Masked Tuning Improves Zero-Shot Composed Image Retrieval. (arXiv:2311.07622v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1">Hanjiang Lai</a></p>
<p>Zero-shot composed image retrieval (ZS-CIR), which aims to retrieve a target
image based on textual modifications to a reference image without triplet
labeling, has gained more and more attention. Current ZS-CIR research mainly
relies on two unlabeled pre-trained models: the vision-language model, e.g.,
CLIP, and the Pic2Word/textual inversion model. However, the pre-trained models
and CIR tasks have substantial discrepancies, where the pre-trained models
learn the similarities between vision and language but CIR aims to learn the
modifications of the image guided by text. In this paper, we introduce a novel
unlabeled and pre-trained masked tuning approach to reduce the gap between the
pre-trained model and the downstream CIR task. We first reformulate the
pre-trained vision-language contrastive learning as the CIR task, where we
randomly mask input image patches to generate $\langle$masked image, text,
image$\rangle$ triple from an image-text pair. Then, we propose a masked
tuning, which uses the text and the masked image to learn the modifications of
the original image. With such a simple design, it can learn to capture
fine-grained text-guided modifications. Extensive experimental results
demonstrate the significant superiority of our approach over the baseline
models on three ZS-CIR datasets, including FashionIQ, CIRR, and CIRCO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07634">ActiveDC: Distribution Calibration for Active Finetuning. (arXiv:2311.07634v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenshuai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhenhui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1">Jinzhou Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhong Wang</a></p>
<p>The pretraining-finetuning paradigm has gained popularity in various computer
vision tasks. In this paradigm, the emergence of active finetuning arises due
to the abundance of large-scale data and costly annotation requirements. Active
finetuning involves selecting a subset of data from an unlabeled pool for
annotation, facilitating subsequent finetuning. However, the use of a limited
number of training samples can lead to a biased distribution, potentially
resulting in model overfitting. In this paper, we propose a new method called
ActiveDC for the active finetuning tasks. Firstly, we select samples for
annotation by optimizing the distribution similarity between the subset to be
selected and the entire unlabeled pool in continuous space. Secondly, we
calibrate the distribution of the selected samples by exploiting implicit
category information in the unlabeled pool. The feature visualization provides
an intuitive sense of the effectiveness of our approach to distribution
calibration. We conducted extensive experiments on three image classification
datasets with different sampling ratios. The results indicate that ActiveDC
consistently outperforms the baseline performance in all image classification
tasks. The improvement is particularly significant when the sampling ratio is
low, with performance gains of up to 10%. Our code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08870">One-Shot Federated Learning with Classifier-Guided Diffusion Models. (arXiv:2311.08870v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mingzhao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Shangchao Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xiangyang Xue</a></p>
<p>One-shot federated learning (OSFL) has gained attention in recent years due
to its low communication cost. However, most of the existing methods require
auxiliary datasets or training generators, which hinders their practicality in
real-world scenarios. In this paper, we explore the novel opportunities that
diffusion models bring to OSFL and propose FedCADO, utilizing guidance from
client classifiers to generate data that complies with clients' distributions
and subsequently training the aggregated model on the server. Specifically, our
method involves targeted optimizations in two aspects. On one hand, we
conditionally edit the randomly sampled initial noises, embedding them with
specified semantics and distributions, resulting in a significant improvement
in both the quality and stability of generation. On the other hand, we employ
the BN statistics from the classifiers to provide detailed guidance during
generation. These tailored optimizations enable us to limitlessly generate
datasets, which closely resemble the distribution and quality of the original
client dataset. Our method effectively handles the heterogeneous client models
and the problems of non-IID features or labels. In terms of privacy protection,
our method avoids training any generator or transferring any auxiliary
information on clients, eliminating any additional privacy leakage risks.
Leveraging the extensive knowledge stored in the pre-trained diffusion model,
the synthetic datasets can assist us in surpassing the knowledge limitations of
the client samples, resulting in aggregation models that even outperform the
performance ceiling of centralized training in some cases, which is
convincingly demonstrated in the sufficient quantification and visualization
experiments conducted on three large-scale multi-domain image datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09093">Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions. (arXiv:2311.09093v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xingshuai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cappuccio_M/0/1/0/all/0/1">Massimiliano L. Cappuccio</a></p>
<p>Autonomous vehicle refers to a vehicle capable of perceiving its surrounding
environment and driving with little or no human driver input. The perception
system is a fundamental component which enables the autonomous vehicle to
collect data and extract relevant information from the environment to drive
safely. Benefit from the recent advances in computer vision, the perception
task can be achieved by using sensors, such as camera, LiDAR, radar, and
ultrasonic sensor. This paper reviews publications on computer vision and
autonomous driving that are published during the last ten years. In particular,
we first investigate the development of autonomous driving systems and
summarize these systems that are developed by the major automotive
manufacturers from different countries. Second, we investigate the sensors and
benchmark data sets that are commonly utilized for autonomous driving. Then, a
comprehensive overview of computer vision applications for autonomous driving
such as depth estimation, object detection, lane detection, and traffic sign
recognition are discussed. Additionally, we review public opinions and concerns
on autonomous vehicles. Based on the discussion, we analyze the current
technological challenges that autonomous vehicles meet with. Finally, we
present our insights and point out some promising directions for future
research. This paper will help the reader to understand autonomous vehicles
from the perspectives of academia and industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.10242">Ulixes: Facial Recognition Privacy with Adversarial Machine Learning. (arXiv:2010.10242v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cilloni_T/0/1/0/all/0/1">Thomas Cilloni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_C/0/1/0/all/0/1">Charles Walter</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_C/0/1/0/all/0/1">Charles Fleming</a></p>
<p>Facial recognition tools are becoming exceptionally accurate in identifying
people from images. However, this comes at the cost of privacy for users of
online services with photo management (e.g. social media platforms).
Particularly troubling is the ability to leverage unsupervised learning to
recognize faces even when the user has not labeled their images. In this paper
we propose Ulixes, a strategy to generate visually non-invasive facial noise
masks that yield adversarial examples, preventing the formation of identifiable
user clusters in the embedding space of facial encoders. This is applicable
even when a user is unmasked and labeled images are available online. We
demonstrate the effectiveness of Ulixes by showing that various classification
and clustering methods cannot reliably label the adversarial examples we
generate. We also study the effects of Ulixes in various black-box settings and
compare it to the current state of the art in adversarial machine learning.
Finally, we challenge the effectiveness of Ulixes against adversarially trained
models and show that it is robust to countermeasures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04216">MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos. (arXiv:2306.04216v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jielin Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiacheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">William Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Aditesh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_K/0/1/0/all/0/1">Karthik Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Claire Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a></p>
<p>Multimodal summarization with multimodal output (MSMO) has emerged as a
promising research direction. Nonetheless, numerous limitations exist within
existing public MSMO datasets, including insufficient upkeep, data
inaccessibility, limited size, and the absence of proper categorization, which
pose significant challenges to effective research. To address these challenges
and provide a comprehensive dataset for this new direction, we have
meticulously curated the MultiSum dataset. Our new dataset features (1)
Human-validated summaries for both video and textual content, providing
superior human instruction and labels for multimodal learning. (2)
Comprehensively and meticulously arranged categorization, spanning 17 principal
categories and 170 subcategories to encapsulate a diverse array of real-world
scenarios. (3) Benchmark tests performed on the proposed dataset to assess
varied tasks and methods, including video temporal segmentation, video
summarization, text summarization, and multimodal summarization. To champion
accessibility and collaboration, we release the MultiSum dataset and the data
collection tool as fully open-source resources, fostering transparency and
accelerating future developments. Our project website can be found at
https://multisum-dataset.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08811">Correlation-aware active learning for surgery video segmentation. (arXiv:2311.08811v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marquez_Neila_P/0/1/0/all/0/1">Pablo Marquez-Neila</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafii_Tari_H/0/1/0/all/0/1">Hedyeh Rafii-Tari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a></p>
<p>Semantic segmentation is a complex task that relies heavily on large amounts
of annotated image data. However, annotating such data can be time-consuming
and resource-intensive, especially in the medical domain. Active Learning (AL)
is a popular approach that can help to reduce this burden by iteratively
selecting images for annotation to improve the model performance. In the case
of video data, it is important to consider the model uncertainty and the
temporal nature of the sequences when selecting images for annotation. This
work proposes a novel AL strategy for surgery video segmentation, \COALSamp{},
COrrelation-aWare Active Learning. Our approach involves projecting images into
a latent space that has been fine-tuned using contrastive learning and then
selecting a fixed number of representative images from local clusters of video
frames. We demonstrate the effectiveness of this approach on two video datasets
of surgical instruments and three real-world video datasets. The datasets and
code will be made publicly available upon receiving necessary approvals.
</p>
</p>
</div>

    </div>
    </body>
    