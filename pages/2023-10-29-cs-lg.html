<!DOCTYPE html>
<html>
<head>
<title>2023-10-29-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2310.16842">Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chao Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1">Tianheng Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiele_G/0/1/0/all/0/1">Gregor Schiele</a></p>
<p>To process sensor data in the Internet of Things(IoTs), embedded deep
learning for 1-dimensional data is an important technique. In the past, CNNs
were frequently used because they are simple to optimise for special embedded
hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed
at energy-efficient inference on end devices. Using the traffic speed
prediction as a case study, a vanilla LSTM model with the optimised LSTM cell
achieves 17534 inferences per second while consuming only 3.8 $\mu$J per
inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It
achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy
efficient than existing approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16857">Improvement in Alzheimer&#x27;s Disease MRI Images Analysis by Convolutional Neural Networks Via Topological Optimization. (arXiv:2310.16857v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tan_P/0/1/0/all/0/1">Peiwen Tan</a></p>
<p>This research underscores the efficacy of Fourier topological optimization in
refining MRI imagery, thereby bolstering the classification precision of
Alzheimer's Disease through convolutional neural networks. Recognizing that MRI
scans are indispensable for neurological assessments, but frequently grapple
with issues like blurriness and contrast irregularities, the deployment of
Fourier topological optimization offered enhanced delineation of brain
structures, ameliorated noise, and superior contrast. The applied techniques
prioritized boundary enhancement, contrast and brightness adjustments, and
overall image lucidity. Employing CNN architectures VGG16, ResNet50,
InceptionV3, and Xception, the post-optimization analysis revealed a marked
elevation in performance. Conclusively, the amalgamation of Fourier topological
optimization with CNNs delineates a promising trajectory for the nuanced
classification of Alzheimer's Disease, portending a transformative impact on
its diagnostic paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16861">General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhangyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Cheng Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Stan Z. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Laurence T. Yang</a></p>
<p>The pre-training architectures of large language models encompass various
types, including autoencoding models, autoregressive models, and
encoder-decoder models. We posit that any modality can potentially benefit from
a large language model, as long as it undergoes vector quantization to become
discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which
seamlessly integrates autoencoding and autoregressive tasks in point cloud
transformer. This model is versatile, allowing fine-tuning for downstream point
cloud representation tasks, as well as unconditional and conditional generation
tasks. GPM enhances masked prediction in autoencoding through various forms of
mask padding tasks, leading to improved performance in point cloud
understanding. Additionally, GPM demonstrates highly competitive results in
unconditional point cloud generation tasks, even exhibiting the potential for
conditional generation tasks by modifying the input's conditional information.
Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves
superior performance in point cloud understanding tasks. Furthermore, the
integration of autoregressive and autoencoding within the same transformer
underscores its versatility across different downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16867">An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saadatinia_M/0/1/0/all/0/1">Mehrshad Saadatinia</a>, <a href="http://arxiv.org/find/cs/1/au:+Salimi_Badr_A/0/1/0/all/0/1">Armin Salimi-Badr</a></p>
<p>In this study, we leverage a deep learning-based method for the automatic
diagnosis of schizophrenia using EEG brain recordings. This approach utilizes
generative data augmentation, a powerful technique that enhances the accuracy
of the diagnosis. To enable the utilization of time-frequency features,
spectrograms were extracted from the raw signals. After exploring several
neural network architectural setups, a proper convolutional neural network
(CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN
with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two
different synthetic datasets were generated in order to augment the initial
dataset and address the over-fitting issue. The augmented dataset using VAE
achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a
lower loss value as well as a faster convergence. Finally, we addressed the
lack of trust in black-box models using the Local Interpretable Model-agnostic
Explanations (LIME) algorithm to determine the most important superpixels
(frequencies) in the diagnosis process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16870">MACP: Efficient Model Adaptation for Cooperative Perception. (arXiv:2310.16870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunsheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Juanwu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Can Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+ZHao_S/0/1/0/all/0/1">Sicheng ZHao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wenqian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziran Wang</a></p>
<p>Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception
capabilities of connected and automated vehicles (CAVs) by enabling information
sharing to "see through the occlusions", resulting in significant performance
improvements. However, developing and training complex multi-agent perception
models from scratch can be expensive and unnecessary when existing single-agent
models show remarkable generalization capabilities. In this paper, we propose a
new framework termed MACP, which equips a single-agent pre-trained model with
cooperation capabilities. We approach this objective by identifying the key
challenges of shifting from single-agent to cooperative settings, adapting the
model by freezing most of its parameters and adding a few lightweight modules.
We demonstrate in our experiments that the proposed framework can effectively
utilize cooperative observations and outperform other state-of-the-art
approaches in both simulated and real-world cooperative perception benchmarks
while requiring substantially fewer tunable parameters with reduced
communication costs. Our source code is available at
https://github.com/PurdueDigitalTwin/MACP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16912">Transformer-based Atmospheric Density Forecasting. (arXiv:2310.16912v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Briden_J/0/1/0/all/0/1">Julia Briden</a>, <a href="http://arxiv.org/find/physics/1/au:+Siew_P/0/1/0/all/0/1">Peng Mun Siew</a>, <a href="http://arxiv.org/find/physics/1/au:+Rodriguez_Fernandez_V/0/1/0/all/0/1">Victor Rodriguez-Fernandez</a>, <a href="http://arxiv.org/find/physics/1/au:+Linares_R/0/1/0/all/0/1">Richard Linares</a></p>
<p>As the peak of the solar cycle approaches in 2025 and the ability of a single
geomagnetic storm to significantly alter the orbit of Resident Space Objects
(RSOs), techniques for atmospheric density forecasting are vital for space
situational awareness. While linear data-driven methods, such as dynamic mode
decomposition with control (DMDc), have been used previously for forecasting
atmospheric density, deep learning-based forecasting has the ability to capture
nonlinearities in data. By learning multiple layer weights from historical
atmospheric density data, long-term dependencies in the dataset are captured in
the mapping between the current atmospheric density state and control input to
the atmospheric density state at the next timestep. This work improves upon
previous linear propagation methods for atmospheric density forecasting, by
developing a nonlinear transformer-based architecture for atmospheric density
forecasting. Empirical NRLMSISE-00 and JB2008, as well as physics-based TIEGCM
atmospheric density models are compared for forecasting with DMDc and with the
transformer-based propagator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16917">MimicTouch: Learning Human&#x27;s Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kelin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yunhai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Matthew Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Ye Zhao</a></p>
<p>In robotics and artificial intelligence, the integration of tactile
processing is becoming increasingly pivotal, especially in learning to execute
intricate tasks like alignment and insertion. However, existing works focusing
on tactile methods for insertion tasks predominantly rely on robot
teleoperation data and reinforcement learning, which do not utilize the rich
insights provided by human's control strategy guided by tactile feedback. For
utilizing human sensations, methodologies related to learning from humans
predominantly leverage visual feedback, often overlooking the invaluable
tactile feedback that humans inherently employ to finish complex manipulations.
Addressing this gap, we introduce "MimicTouch", a novel framework that mimics
human's tactile-guided control strategy. In this framework, we initially
collect multi-modal tactile datasets from human demonstrators, incorporating
human tactile-guided control strategies for task completion. The subsequent
step involves instructing robots through imitation learning using multi-modal
sensor data and retargeted human motions. To further mitigate the embodiment
gap between humans and robots, we employ online residual reinforcement learning
on the physical robot. Through comprehensive experiments, we validate the
safety of MimicTouch in transferring a latent policy learned through imitation
learning from human to robot. This ongoing work will pave the way for a broader
spectrum of tactile-guided robotic applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16936">Diagnosing Alzheimer&#x27;s Disease using Early-Late Multimodal Data Fusion with Jacobian Maps. (arXiv:2310.16936v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mustafa_Y/0/1/0/all/0/1">Yasmine Mustafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1">Tie Luo</a></p>
<p>Alzheimer's disease (AD) is a prevalent and debilitating neurodegenerative
disorder impacting a large aging population. Detecting AD in all its
presymptomatic and symptomatic stages is crucial for early intervention and
treatment. An active research direction is to explore machine learning methods
that harness multimodal data fusion to outperform human inspection of medical
scans. However, existing multimodal fusion models have limitations, including
redundant computation, complex architecture, and simplistic handling of missing
data. Moreover, the preprocessing pipelines of medical scans remain
inadequately detailed and are seldom optimized for individual subjects. In this
paper, we propose an efficient early-late fusion (ELF) approach, which
leverages a convolutional neural network for automated feature extraction and
random forests for their competitive performance on small datasets.
Additionally, we introduce a robust preprocessing pipeline that adapts to the
unique characteristics of individual subjects and makes use of whole brain
images rather than slices or patches. Moreover, to tackle the challenge of
detecting subtle changes in brain volume, we transform images into the Jacobian
domain (JD) to enhance both accuracy and robustness in our classification.
Using MRI and CT images from the OASIS-3 dataset, our experiments demonstrate
the effectiveness of the ELF approach in classifying AD into four stages with
an accuracy of 97.19%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16941">Exploring Behavior Discovery Methods for Heterogeneous Swarms of Limited-Capability Robots. (arXiv:2310.16941v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mattson_C/0/1/0/all/0/1">Connor Mattson</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">Jeremy C. Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Daniel S. Brown</a></p>
<p>We study the problem of determining the emergent behaviors that are possible
given a functionally heterogeneous swarm of robots with limited capabilities.
Prior work has considered behavior search for homogeneous swarms and proposed
the use of novelty search over either a hand-specified or learned behavior
space followed by clustering to return a taxonomy of emergent behaviors to the
user. In this paper, we seek to better understand the role of novelty search
and the efficacy of using clustering to discover novel emergent behaviors.
Through a large set of experiments and ablations, we analyze the effect of
representations, evolutionary search, and various clustering methods in the
search for novel behaviors in a heterogeneous swarm. Our results indicate that
prior methods fail to discover many interesting behaviors and that an iterative
human-in-the-loop discovery process discovers more behaviors than random
search, swarm chemistry, and automated behavior discovery. The combined
discoveries of our experiments uncover 23 emergent behaviors, 18 of which are
novel discoveries. To the best of our knowledge, these are the first known
emergent behaviors for heterogeneous swarms of computation-free agents. Videos,
code, and appendix are available at the project website:
https://sites.google.com/view/heterogeneous-bd-methods
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16944">Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1">Lewis Tunstall</a>, <a href="http://arxiv.org/find/cs/1/au:+Beeching_E/0/1/0/all/0/1">Edward Beeching</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_N/0/1/0/all/0/1">Nathan Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1">Nazneen Rajani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1">Kashif Rasul</a>, <a href="http://arxiv.org/find/cs/1/au:+Belkada_Y/0/1/0/all/0/1">Younes Belkada</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shengyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Werra_L/0/1/0/all/0/1">Leandro von Werra</a>, <a href="http://arxiv.org/find/cs/1/au:+Fourrier_C/0/1/0/all/0/1">Cl&#xe9;mentine Fourrier</a>, <a href="http://arxiv.org/find/cs/1/au:+Habib_N/0/1/0/all/0/1">Nathan Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarrazin_N/0/1/0/all/0/1">Nathan Sarrazin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanseviero_O/0/1/0/all/0/1">Omar Sanseviero</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Thomas Wolf</a></p>
<p>We aim to produce a smaller language model that is aligned to user intent.
Previous research has shown that applying distilled supervised fine-tuning
(dSFT) on larger models significantly improves task accuracy; however, these
models are unaligned, i.e. they do not respond well to natural prompts. To
distill this property, we experiment with the use of preference data from AI
Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model,
we apply distilled direct preference optimization (dDPO) to learn a chat model
with significantly improved intent alignment. The approach requires only a few
hours of training without any additional sampling during fine-tuning. The final
result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B
parameter models, and requires no human annotation. In particular, results on
MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access
RLHF-based model. Code, models, data, and tutorials for the system are
available at https://github.com/huggingface/alignment-handbook.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16945">Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lan_H/0/1/0/all/0/1">Hui Lan</a>, <a href="http://arxiv.org/find/stat/1/au:+Syrgkanis_V/0/1/0/all/0/1">Vasilis Syrgkanis</a></p>
<p>Accurate estimation of conditional average treatment effects (CATE) is at the
core of personalized decision making. While there is a plethora of models for
CATE estimation, model selection is a nontrivial task, due to the fundamental
problem of causal inference. Recent empirical work provides evidence in favor
of proxy loss metrics with double robust properties and in favor of model
ensembling. However, theoretical understanding is lacking. Direct application
of prior theoretical work leads to suboptimal oracle model selection rates due
to the non-convexity of the model selection problem. We provide regret rates
for the major existing CATE ensembling approaches and propose a new CATE model
ensembling approach based on Q-aggregation using the doubly robust loss. Our
main result shows that causal Q-aggregation achieves statistically optimal
oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and
$n$ samples), with the addition of higher-order estimation error terms related
to products of errors in the nuisance functions. Crucially, our regret rate
does not require that any of the candidate CATE models be close to the truth.
We validate our new method on many semi-synthetic datasets and also provide
extensions of our work to CATE model selection with instrumental variables and
unobserved confounding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16955">Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Aradhana Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Balashankar_A/0/1/0/all/0/1">Ananth Balashankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrahami_T/0/1/0/all/0/1">Thi Avrahami</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1">Alex Beutel</a></p>
<p>Real-world natural language processing systems need to be robust to human
adversaries. Collecting examples of human adversaries for training is an
effective but expensive solution. On the other hand, training on synthetic
attacks with small perturbations - such as word-substitution - does not
actually improve robustness to human adversaries. In this paper, we propose an
adversarial training framework that uses limited human adversarial examples to
generate more useful adversarial examples at scale. We demonstrate the
advantages of this system on the ANLI and hate speech detection benchmark
datasets - both collected via an iterative, adversarial
human-and-model-in-the-loop procedure. Compared to training only on observed
human attacks, also training on our synthetic adversarial examples improves
model robustness to future rounds. In ANLI, we see accuracy gains on the
current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of
human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate
speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a
future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the
distribution of existing human adversaries, meanwhile, degrade robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16958">Transferring a molecular foundation model for polymer property predictions. (arXiv:2310.16958v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kearney_L/0/1/0/all/0/1">Logan Kearney</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhowmik_D/0/1/0/all/0/1">Debsindhu Bhowmik</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_Z/0/1/0/all/0/1">Zachary Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Naskar_A/0/1/0/all/0/1">Amit K. Naskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gounley_J/0/1/0/all/0/1">John Gounley</a></p>
<p>Transformer-based large language models have remarkable potential to
accelerate design optimization for applications such as drug development and
materials discovery. Self-supervised pretraining of transformer models requires
large-scale datasets, which are often sparsely populated in topical areas such
as polymer science. State-of-the-art approaches for polymers conduct data
augmentation to generate additional samples but unavoidably incurs extra
computational costs. In contrast, large-scale open-source datasets are
available for small molecules and provide a potential solution to data scarcity
through transfer learning. In this work, we show that using transformers
pretrained on small molecules and fine-tuned on polymer properties achieve
comparable accuracy to those trained on augmented polymer datasets for a series
of benchmark prediction tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16959">Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning. (arXiv:2310.16959v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balashankar_A/0/1/0/all/0/1">Ananth Balashankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Aradhana Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1">Alex Beutel</a></p>
<p>As large language models (LLMs) are widely adopted, new safety issues and
policies emerge, to which existing safety classifiers do not generalize well.
If we have only observed a few examples of violations of a new safety rule, how
can we build a classifier to detect violations? In this paper, we study the
novel setting of domain-generalized few-shot learning for LLM-based text safety
classifiers. Unlike prior few-shot work, these new safety issues can be hard to
uncover and we do not get to choose the few examples. We demonstrate that
existing few-shot techniques do not perform well in this setting, and rather we
propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting
training data based on similar examples in prior existing rules. We empirically
show that our approach of similarity-based data-augmentation + prompt-tuning
(DAPT) consistently outperforms baselines that either do not rely on data
augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral
judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule
is loosely correlated with existing ones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16960">Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1">Huseyin A. Inan</a>, <a href="http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1">Arturs Backurs</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1">Varun Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1">Janardhan Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1">Robert Sim</a></p>
<p>Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16975">Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference. (arXiv:2310.16975v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zheyu Oliver Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Baptista_R/0/1/0/all/0/1">Ricardo Baptista</a>, <a href="http://arxiv.org/find/stat/1/au:+Marzouk_Y/0/1/0/all/0/1">Youssef Marzouk</a>, <a href="http://arxiv.org/find/stat/1/au:+Ruthotto_L/0/1/0/all/0/1">Lars Ruthotto</a>, <a href="http://arxiv.org/find/stat/1/au:+Verma_D/0/1/0/all/0/1">Deepanshu Verma</a></p>
<p>We present two neural network approaches that approximate the solutions of
static and dynamic conditional optimal transport (COT) problems, respectively.
Both approaches enable sampling and density estimation of conditional
probability distributions, which are core tasks in Bayesian inference. Our
methods represent the target conditional distributions as transformations of a
tractable reference distribution and, therefore, fall into the framework of
measure transport. COT maps are a canonical choice within this framework, with
desirable properties such as uniqueness and monotonicity. However, the
associated COT problems are computationally challenging, even in moderate
dimensions. To improve the scalability, our numerical algorithms leverage
neural networks to parameterize COT maps. Our methods exploit the structure of
the static and dynamic formulations of the COT problem. PCP-Map models
conditional transport maps as the gradient of a partially input convex neural
network (PICNN) and uses a novel numerical implementation to increase
computational efficiency compared to state-of-the-art alternatives. COT-Flow
models conditional transports via the flow of a regularized neural ODE; it is
slower to train but offers faster sampling. We demonstrate their effectiveness
and efficiency by comparing them with state-of-the-art approaches using
benchmark datasets and Bayesian inverse problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16978">The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">S M Atikur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibtisum_S/0/1/0/all/0/1">Sifat Ibtisum</a>, <a href="http://arxiv.org/find/cs/1/au:+Bazgir_E/0/1/0/all/0/1">Ehsan Bazgir</a>, <a href="http://arxiv.org/find/cs/1/au:+Barai_T/0/1/0/all/0/1">Tumpa Barai</a></p>
<p>The global need for effective disease diagnosis remains substantial, given
the complexities of various disease mechanisms and diverse patient symptoms. To
tackle these challenges, researchers, physicians, and patients are turning to
machine learning (ML), an artificial intelligence (AI) discipline, to develop
solutions. By leveraging sophisticated ML and AI methods, healthcare
stakeholders gain enhanced diagnostic and treatment capabilities. However,
there is a scarcity of research focused on ML algorithms for enhancing the
accuracy and computational efficiency. This research investigates the capacity
of machine learning algorithms to improve the transmission of heart rate data
in time series healthcare metrics, concentrating particularly on optimizing
accuracy and efficiency. By exploring various ML algorithms used in healthcare
applications, the review presents the latest trends and approaches in ML-based
disease diagnosis (MLBDD). The factors under consideration include the
algorithm utilized, the types of diseases targeted, the data types employed,
the applications, and the evaluation metrics. This review aims to shed light on
the prospects of ML in healthcare, particularly in disease diagnosis. By
analyzing the current literature, the study provides insights into
state-of-the-art methodologies and their performance metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16979">Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingchen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1">Niluthpol Chowdhury Mithun</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajvanshi_A/0/1/0/all/0/1">Abhinav Rajvanshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1">Han-Pang Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1">Supun Samarasekera</a></p>
<p>Deep learning-based solutions for semantic segmentation suffer from
significant performance degradation when tested on data with different
characteristics than what was used during the training. Adapting the models
using annotated data from the new domain is not always practical. Unsupervised
Domain Adaptation (UDA) approaches are crucial in deploying these models in the
actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ
a teacher-student self-training approach, where a teacher model is used to
generate pseudo-labels for the new data which in turn guide the training
process of the student model. Though this approach has seen a lot of success,
it suffers from the issue of noisy pseudo-labels being propagated in the
training process. To address this issue, we propose an auxiliary pseudo-label
refinement network (PRN) for online refining of the pseudo labels and also
localizing the pixels whose predicted labels are likely to be noisy. Being able
to improve the quality of pseudo labels and select highly reliable ones, PRN
helps self-training of segmentation models to be robust against pseudo label
noise propagation during different stages of adaptation. We evaluate our
approach on benchmark datasets with three different domain shifts, and our
approach consistently performs significantly better than the previous
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16981">Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark. (arXiv:2310.16981v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1">Lasse Hansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Seedat_N/0/1/0/all/0/1">Nabeel Seedat</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrovic_A/0/1/0/all/0/1">Andrija Petrovic</a></p>
<p>Synthetic data serves as an alternative in training machine learning models,
particularly when real-world data is limited or inaccessible. However, ensuring
that synthetic data mirrors the complex nuances of real-world data is a
challenging task. This paper addresses this issue by exploring the potential of
integrating data-centric AI techniques which profile the data to guide the
synthetic data generation process. Moreover, we shed light on the often ignored
consequences of neglecting these data profiles during synthetic data generation
-- despite seemingly high statistical fidelity. Subsequently, we propose a
novel framework to evaluate the integration of data profiles to guide the
creation of more representative synthetic data. In an empirical study, we
evaluate the performance of five state-of-the-art models for tabular data
generation on eleven distinct tabular datasets. The findings offer critical
insights into the successes and limitations of current synthetic data
generation techniques. Finally, we provide practical recommendations for
integrating data-centric insights into the synthetic data generation process,
with a specific focus on classification performance, model selection, and
feature selection. This study aims to reevaluate conventional approaches to
synthetic data generation and promote the application of data-centric AI
techniques in improving the quality and effectiveness of synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16986">Probabilistic Integral Circuits. (arXiv:2310.16986v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gala_G/0/1/0/all/0/1">Gennaro Gala</a>, <a href="http://arxiv.org/find/cs/1/au:+Campos_C/0/1/0/all/0/1">Cassio de Campos</a>, <a href="http://arxiv.org/find/cs/1/au:+Peharz_R/0/1/0/all/0/1">Robert Peharz</a>, <a href="http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1">Antonio Vergari</a>, <a href="http://arxiv.org/find/cs/1/au:+Quaeghebeur_E/0/1/0/all/0/1">Erik Quaeghebeur</a></p>
<p>Continuous latent variables (LVs) are a key ingredient of many generative
models, as they allow modelling expressive mixtures with an uncountable number
of components. In contrast, probabilistic circuits (PCs) are hierarchical
discrete mixtures represented as computational graphs composed of input, sum
and product units. Unlike continuous LV models, PCs provide tractable inference
but are limited to discrete LVs with categorical (i.e. unordered) states. We
bridge these model classes by introducing probabilistic integral circuits
(PICs), a new language of computational graphs that extends PCs with integral
units representing continuous LVs. In the first place, PICs are symbolic
computational graphs and are fully tractable in simple cases where analytical
integration is possible. In practice, we parameterise PICs with light-weight
neural nets delivering an intractable hierarchical continuous mixture that can
be approximated arbitrarily well with large PCs using numerical quadrature. On
several distribution estimation benchmarks, we show that such PIC-approximating
PCs systematically outperform PCs commonly learned via expectation-maximization
or SGD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16990">STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leon Liyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1">Aditya Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1">Dhivya Piraviperumal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Tien Dung Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzou_N/0/1/0/all/0/1">Nicholas Tzou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>In the context of a voice assistant system, steering refers to the phenomenon
in which a user issues a follow-up command attempting to direct or clarify a
previous turn. We propose STEER, a steering detection model that predicts
whether a follow-up turn is a user's attempt to steer the previous command.
Constructing a training dataset for steering use cases poses challenges due to
the cold-start problem. To overcome this, we developed heuristic rules to
sample opt-in usage data, approximating positive and negative samples without
any annotation. Our experimental results show promising performance in
identifying steering intent, with over 95% accuracy on our sampled data.
Moreover, STEER, in conjunction with our sampling strategy, aligns effectively
with real-world steering scenarios, as evidenced by its strong zero-shot
performance on a human-graded evaluation set. In addition to relying solely on
user transcripts as input, we introduce STEER+, an enhanced version of the
model. STEER+ utilizes a semantic parse tree to provide more context on
out-of-vocabulary words, such as named entities that often occur at the
sentence boundary. This further improves model performance, reducing error rate
in domains where entities frequently appear, such as messaging. Lastly, we
present a data analysis that highlights the improvement in user experience when
voice assistants support steering use cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16996">Towards Continually Learning Application Performance Models. (arXiv:2310.16996v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinurat_R/0/1/0/all/0/1">Ray A. O. Sinurat</a>, <a href="http://arxiv.org/find/cs/1/au:+Daram_A/0/1/0/all/0/1">Anurag Daram</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunawi_H/0/1/0/all/0/1">Haryadi S. Gunawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ross_R/0/1/0/all/0/1">Robert B. Ross</a>, <a href="http://arxiv.org/find/cs/1/au:+Madireddy_S/0/1/0/all/0/1">Sandeep Madireddy</a></p>
<p>Machine learning-based performance models are increasingly being used to
build critical job scheduling and application optimization decisions.
Traditionally, these models assume that data distribution does not change as
more samples are collected over time. However, owing to the complexity and
heterogeneity of production HPC systems, they are susceptible to hardware
degradation, replacement, and/or software patches, which can lead to drift in
the data distribution that can adversely affect the performance models. To this
end, we develop continually learning performance models that account for the
distribution drift, alleviate catastrophic forgetting, and improve
generalizability. Our best model was able to retain accuracy, regardless of
having to learn the new distribution of data inflicted by system changes, while
demonstrating a 2x improvement in the prediction accuracy of the whole data
sequence in comparison to the naive approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16999">Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaman_F/0/1/0/all/0/1">Fahim Ahmed Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaodong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1">Milan Sonka</a>, <a href="http://arxiv.org/find/cs/1/au:+Mudumbai_R/0/1/0/all/0/1">Raghuraman Mudumbai</a></p>
<p>We describe a method for verifying the output of a deep neural network for
medical image segmentation that is robust to several classes of random as well
as worst-case perturbations i.e. adversarial attacks. This method is based on a
general approach recently developed by the authors called ``Trust, but Verify"
wherein an auxiliary verification network produces predictions about certain
masked features in the input image using the segmentation as an input. A
well-designed auxiliary network will produce high-quality predictions when the
input segmentations are accurate, but will produce low-quality predictions when
the segmentations are incorrect. Checking the predictions of such a network
with the original image allows us to detect bad segmentations. However, to
ensure the verification method is truly robust, we need a method for checking
the quality of the predictions that does not itself rely on a black-box neural
network. Indeed, we show that previous methods for segmentation evaluation that
do use deep neural regression networks are vulnerable to false negatives i.e.
can inaccurately label bad segmentations as good. We describe the design of a
verification network that avoids such vulnerability and present results to
demonstrate its robustness compared to previous methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17002">Faster Recalibration of an Online Predictor via Approachability. (arXiv:2310.17002v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Okoroafor_P/0/1/0/all/0/1">Princewill Okoroafor</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleinberg_R/0/1/0/all/0/1">Robert Kleinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a></p>
<p>Predictive models in ML need to be trustworthy and reliable, which often at
the very least means outputting calibrated probabilities. This can be
particularly difficult to guarantee in the online prediction setting when the
outcome sequence can be generated adversarially. In this paper we introduce a
technique using Blackwell's approachability theorem for taking an online
predictive model which might not be calibrated and transforming its predictions
to calibrated predictions without much increase to the loss of the original
model. Our proposed algorithm achieves calibration and accuracy at a faster
rate than existing techniques <a href="/abs/1607.03594">arXiv:1607.03594</a> and is the first algorithm to
offer a flexible tradeoff between calibration error and accuracy in the online
setting. We demonstrate this by characterizing the space of jointly achievable
calibration and regret using our technique.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17009">Simulation based stacking. (arXiv:2310.17009v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1">Yuling Yao</a>, <a href="http://arxiv.org/find/stat/1/au:+Blancard_B/0/1/0/all/0/1">Bruno R&#xe9;galdo-Saint Blancard</a>, <a href="http://arxiv.org/find/stat/1/au:+Domke_J/0/1/0/all/0/1">Justin Domke</a></p>
<p>Simulation-based inference has been popular for amortized Bayesian
computation. It is typical to have more than one posterior approximation, from
different inference algorithms, different architectures, or simply the
randomness of initialization and stochastic gradients. With a provable
asymptotic guarantee, we present a general stacking framework to make use of
all available posterior approximations. Our stacking method is able to combine
densities, simulation draws, confidence intervals, and moments, and address the
overall precision, calibration, coverage, and bias at the same time. We
illustrate our method on several benchmark simulations and a challenging
cosmological inference task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17019">Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zentner_K/0/1/0/all/0/1">K.R. Zentner</a>, <a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1">Ryan Julian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1">Brian Ichter</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1">Gaurav S. Sukhatme</a></p>
<p>This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call "Language-World," which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large Language Models (LLMs) and those
using Deep Reinforcement Learning. Second, we introduce a method we call Plan
Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of
high-level plans using end-to-end demonstrations. Using Language-World, we show
that PCBC is able to achieve strong performance in a variety of few-shot
regimes, often achieving task generalization with as little as a single
demonstration. We have made Language-World available as open-source software at
https://github.com/krzentner/language-world/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17021">Streaming Factor Trajectory Learning for Temporal Tensor Decomposition. (arXiv:2310.17021v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Shikai Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shibo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1">Robert Kirby</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1">Shandian Zhe</a></p>
<p>Practical tensor data is often along with time information. Most existing
temporal decomposition approaches estimate a set of fixed factors for the
objects in each tensor mode, and hence cannot capture the temporal evolution of
the objects' representation. More important, we lack an effective approach to
capture such evolution from streaming data, which is common in real-world
applications. To address these issues, we propose Streaming Factor Trajectory
Learning (SFTL) for temporal tensor decomposition. We use Gaussian processes
(GPs) to model the trajectory of factors so as to flexibly estimate their
temporal evolution. To address the computational challenges in handling
streaming data, we convert the GPs into a state-space prior by constructing an
equivalent stochastic differential equation (SDE). We develop an efficient
online filtering algorithm to estimate a decoupled running posterior of the
involved factor states upon receiving new data. The decoupled estimation
enables us to conduct standard Rauch-Tung-Striebel smoothing to compute the
full posterior of all the trajectories in parallel, without the need for
revisiting any previous data. We have shown the advantage of SFTL in both
synthetic tasks and real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17022">Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1">Sidharth Mudgal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganapathy_H/0/1/0/all/0/1">Harish Ganapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">YaGuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yanping Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Heng-Tze Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1">Michael Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1">Trevor Strohman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1">Alex Beutel</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a></p>
<p>We propose controlled decoding (CD), a novel off-policy reinforcement
learning method to control the autoregressive generation from language models
towards high reward outcomes. CD solves an off-policy reinforcement learning
problem through a value function for the reward, which we call a prefix scorer.
The prefix scorer is used at inference time to steer the generation towards
higher reward outcomes. We show that the prefix scorer may be trained on
(possibly) off-policy data to predict the expected reward when decoding is
continued from a partially decoded response. We empirically demonstrate that CD
is effective as a control mechanism on Reddit conversations corpus. We also
show that the modularity of the design of CD makes it possible to control for
multiple rewards, effectively solving a multi-objective reinforcement learning
problem with no additional complexity. Finally, we show that CD can be applied
in a novel blockwise fashion at inference-time, again without the need for any
training-time changes, essentially bridging the gap between the popular
best-of-$K$ strategy and token-level reinforcement learning. This makes CD a
promising approach for alignment of language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17023">On the Identifiability and Interpretability of Gaussian Process Models. (arXiv:2310.17023v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1">Jiawen Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Mu_W/0/1/0/all/0/1">Wancen Mu</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1">Yun Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_D/0/1/0/all/0/1">Didong Li</a></p>
<p>In this paper, we critically examine the prevalent practice of using additive
mixtures of Mat\'ern kernels in single-output Gaussian process (GP) models and
explore the properties of multiplicative mixtures of Mat\'ern kernels for
multi-output GP models. For the single-output case, we derive a series of
theoretical results showing that the smoothness of a mixture of Mat\'ern
kernels is determined by the least smooth component and that a GP with such a
kernel is effectively equivalent to the least smooth kernel component.
Furthermore, we demonstrate that none of the mixing weights or parameters
within individual kernel components are identifiable. We then turn our
attention to multi-output GP models and analyze the identifiability of the
covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where
$K_0$ is a standard single output kernel such as Mat\'ern. We show that $A$ is
identifiable up to a multiplicative constant, suggesting that multiplicative
mixtures are well suited for multi-output tasks. Our findings are supported by
extensive simulations and real applications for both single- and multi-output
settings. This work provides insight into kernel selection and interpretation
for GP models, emphasizing the importance of choosing appropriate kernel
structures for different tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17032">Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting. (arXiv:2310.17032v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Khan_S/0/1/0/all/0/1">Saad Zafar Khan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Muzammil_N/0/1/0/all/0/1">Nazeefa Muzammil</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zaidi_S/0/1/0/all/0/1">Syed Mohammad Hassan Zaidi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Aljohani_A/0/1/0/all/0/1">Abdulah Jeza Aljohani</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Khan_H/0/1/0/all/0/1">Haibat Khan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ghafoor_S/0/1/0/all/0/1">Salman Ghafoor</a></p>
<p>Accurately forecasting solar power generation is crucial in the global
progression towards sustainable energy systems. In this study, we conduct a
meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and
classical Long Short-Term Memory (LSTM) models for solar power production
forecasting. Our controlled experiments reveal promising advantages of QLSTMs,
including accelerated training convergence and substantially reduced test loss
within the initial epoch compared to classical LSTMs. These empirical findings
demonstrate QLSTM's potential to swiftly assimilate complex time series
relationships, enabled by quantum phenomena like superposition. However,
realizing QLSTM's full capabilities necessitates further research into model
validation across diverse conditions, systematic hyperparameter optimization,
hardware noise resilience, and applications to correlated renewable forecasting
problems. With continued progress, quantum machine learning can offer a
paradigm shift in renewable energy time series prediction. This pioneering work
provides initial evidence substantiating quantum advantages over classical
LSTM, while acknowledging present limitations. Through rigorous benchmarking
grounded in real-world data, our study elucidates a promising trajectory for
quantum learning in renewable forecasting. Additional research and development
can further actualize this potential to achieve unprecedented accuracy and
reliability in predicting solar power generation worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17042">StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1">Juyoung Yun</a></p>
<p>In the rapidly advancing domain of deep learning optimization, this paper
unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded
Adam algorithm. Central to StochGradAdam is its gradient sampling technique.
This method not only ensures stable convergence but also leverages the
advantages of selective gradient consideration, fostering robust training by
potentially mitigating the effects of noisy or outlier data and enhancing the
exploration of the loss landscape for more dependable convergence. In both
image classification and segmentation tasks, StochGradAdam has demonstrated
superior performance compared to the traditional Adam optimizer. By judiciously
sampling a subset of gradients at each iteration, the optimizer is optimized
for managing intricate models. The paper provides a comprehensive exploration
of StochGradAdam's methodology, from its mathematical foundations to bias
correction strategies, heralding a promising advancement in deep learning
training techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17044">Learning to Rank for Active Learning via Multi-Task Bilevel Optimization. (arXiv:2310.17044v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zixin Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Si Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a></p>
<p>Active learning is a promising paradigm to reduce the labeling cost by
strategically requesting labels to improve model performance. However, existing
active learning methods often rely on expensive acquisition function to
compute, extensive modeling retraining and multiple rounds of interaction with
annotators. To address these limitations, we propose a novel approach for
active learning, which aims to select batches of unlabeled instances through a
learned surrogate model for data acquisition. A key challenge in this approach
is developing an acquisition function that generalizes well, as the history of
data, which forms part of the utility function's input, grows over time. Our
novel algorithmic contribution is a bilevel multi-task bilevel optimization
framework that predicts the relative utility -- measured by the validation
accuracy -- of different training sets, and ensures the learned acquisition
function generalizes effectively. For cases where validation accuracy is
expensive to evaluate, we introduce efficient interpolation-based surrogate
models to estimate the utility function, reducing the evaluation cost. We
demonstrate the performance of our approach through extensive experiments on
standard active classification benchmarks. By employing our learned utility
function, we show significant improvements over traditional techniques, paving
the way for more efficient and effective utility maximization in active
learning applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17054">BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs&#x27; Generation. (arXiv:2310.17054v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yufei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Felix Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Large language models (LLMs) such as GPT-3 have demonstrated a strong
capability to generate coherent and contextually relevant text. However, amidst
their successes, a crucial issue persists: their generated outputs still lack
commonsense at times. Moreover, fine-tuning the entire LLM towards more
commonsensical outputs is computationally expensive if not infeasible. In this
paper, we present a computation-efficient framework that steers a frozen
Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,
producing a plausible output that incorporates a list of concepts in a
meaningful way). Specifically, we first construct a reference-free evaluator
that assigns a sentence with a commonsensical score by grounding the sentence
to a dynamic commonsense knowledge base from four different relational aspects.
We then use the scorer as the oracle for commonsense knowledge, and extend the
controllable generation method called NADO to train an auxiliary head that
guides a fixed PTLM to better satisfy the oracle. We test our framework on a
series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two
constrained concept-to-sentence benchmarks. Human evaluation results
demonstrate that our method consistently leads to the most commonsensical
outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17056">Strategizing EV Charging and Renewable Integration in Texas. (arXiv:2310.17056v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mohammadi_M/0/1/0/all/0/1">Mohammad Mohammadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Thornburg_J/0/1/0/all/0/1">Jesse Thornburg</a></p>
<p>Exploring the convergence of electric vehicles (EVs), renewable energy, and
smart grid technologies in the context of Texas, this study addresses
challenges hindering the widespread adoption of EVs. Acknowledging their
environmental benefits, the research focuses on grid stability concerns,
uncoordinated charging patterns, and the complicated relationship between EVs
and renewable energy sources. Dynamic time warping (DTW) clustering and k-means
clustering methodologies categorize days based on total load and net load,
offering nuanced insights into daily electricity consumption and renewable
energy generation patterns. By establishing optimal charging and
vehicle-to-grid (V2G) windows tailored to specific load characteristics, the
study provides a sophisticated methodology for strategic decision-making in
energy consumption and renewable integration. The findings contribute to the
ongoing discourse on achieving a sustainable and resilient energy future
through the seamless integration of EVs into smart grids.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17064">math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saidi_H/0/1/0/all/0/1">Hassen Saidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Susmit Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahai_T/0/1/0/all/0/1">Tuhin Sahai</a></p>
<p>As artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
</p>
<p>While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17072">Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yonghyeon Lee</a></p>
<p>The Motion Manifold Primitive (MMP) produces, for a given task, a continuous
manifold of trajectories each of which can successfully complete the task. It
consists of the decoder function that parametrizes the manifold and the
probability density in the latent coordinate space. In this paper, we first
show that the MMP performance can significantly degrade due to the geometric
distortion in the latent space -- by distortion, we mean that similar motions
are not located nearby in the latent space. We then propose {\it Isometric
Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the
geometry of the manifold. For this purpose, we formulate and use a Riemannian
metric for the motion space (i.e., parametric curve space), which we call a
{\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding
motions and pushing manipulation tasks show that IMMP significantly outperforms
existing MMP methods. Code is available at
https://github.com/Gabe-YHLee/IMMP-public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17074">Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates. (arXiv:2310.17074v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Miao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Beining Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Difan Zou</a></p>
<p>In this work, we theoretically investigate the generalization properties of
neural networks (NN) trained by stochastic gradient descent (SGD) algorithm
with large learning rates. Under such a training regime, our finding is that,
the oscillation of the NN weights caused by the large learning rate SGD
training turns out to be beneficial to the generalization of the NN, which
potentially improves over the same NN trained by SGD with small learning rates
that converges more smoothly. In view of this finding, we call such a
phenomenon "benign oscillation". Our theory towards demystifying such a
phenomenon builds upon the feature learning perspective of deep learning.
Specifically, we consider a feature-noise data generation model that consists
of (i) weak features which have a small $\ell_2$-norm and appear in each data
point; (ii) strong features which have a larger $\ell_2$-norm but only appear
in a certain fraction of all data points; and (iii) noise. We prove that NNs
trained by oscillating SGD with a large learning rate can effectively learn the
weak features in the presence of those strong features. In contrast, NNs
trained by SGD with a small learning rate can only learn the strong features
but makes little progress in learning the weak features. Consequently, when it
comes to the new testing data which consist of only weak features, the NN
trained by oscillating SGD with a large learning rate could still make correct
predictions consistently, while the NN trained by small learning rate SGD
fails. Our theory sheds light on how large learning rate training benefits the
generalization of NNs. Experimental results demonstrate our finding on "benign
oscillation".
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17078">HCT: Hybrid Convnet-Transformer for Parkinson&#x27;s disease detection and severity prediction from gait. (arXiv:2310.17078v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naimi_S/0/1/0/all/0/1">Safwen Naimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouachir_W/0/1/0/all/0/1">Wassim Bouachir</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1">Guillaume-Alexandre Bilodeau</a></p>
<p>In this paper, we propose a novel deep learning method based on a new Hybrid
ConvNet-Transformer architecture to detect and stage Parkinson's disease (PD)
from gait data. We adopt a two-step approach by dividing the problem into two
sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy
versus parkinsonian patients. If the patient is parkinsonian, a multi-class
Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&amp;Y) score to
assess the PD severity stage. Our hybrid architecture exploits the strengths of
both Convolutional Neural Networks (ConvNets) and Transformers to accurately
detect PD and determine the severity stage. In particular, we take advantage of
ConvNets to capture local patterns and correlations in the data, while we
exploit Transformers for handling long-term dependencies in the input signal.
We show that our hybrid method achieves superior performance when compared to
other state-of-the-art methods, with a PD detection accuracy of 97% and a
severity staging accuracy of 87%. Our source code is available at:
https://github.com/SafwenNaimi
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17080">Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images. (arXiv:2310.17080v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naimi_S/0/1/0/all/0/1">Safwen Naimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Koubaa_O/0/1/0/all/0/1">Olfa Koubaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouachir_W/0/1/0/all/0/1">Wassim Bouachir</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1">Guillaume-Alexandre Bilodeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeddore_G/0/1/0/all/0/1">Gregory Jeddore</a>, <a href="http://arxiv.org/find/cs/1/au:+Baines_P/0/1/0/all/0/1">Patricia Baines</a>, <a href="http://arxiv.org/find/cs/1/au:+Correia_D/0/1/0/all/0/1">David Correia</a>, <a href="http://arxiv.org/find/cs/1/au:+Arsenault_A/0/1/0/all/0/1">Andre Arsenault</a></p>
<p>Lichens are symbiotic organisms composed of fungi, algae, and/or
cyanobacteria that thrive in a variety of environments. They play important
roles in carbon and nitrogen cycling, and contribute directly and indirectly to
biodiversity. Ecologists typically monitor lichens by using them as indicators
to assess air quality and habitat conditions. In particular, epiphytic lichens,
which live on trees, are key markers of air quality and environmental health. A
new method of monitoring epiphytic lichens involves using time-lapse cameras to
gather images of lichen populations. These cameras are used by ecologists in
Newfoundland and Labrador to subsequently analyze and manually segment the
images to determine lichen thalli condition and change. These methods are
time-consuming and susceptible to observer bias. In this work, we aim to
automate the monitoring of lichens over extended periods and to estimate their
biomass and condition to facilitate the task of ecologists. To accomplish this,
our proposed framework uses semantic segmentation with an effective training
approach to automate monitoring and biomass estimation of epiphytic lichens on
time-lapse images. We show that our method has the potential to significantly
improve the accuracy and efficiency of lichen population monitoring, making it
a valuable tool for forest ecologists and environmental scientists to evaluate
the impact of climate change on Canada's forests. To the best of our knowledge,
this is the first time that such an approach has been used to assist ecologists
in monitoring and analyzing epiphytic lichens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17086">Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Deqing Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tian-Qi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1">Vatsal Sharan</a></p>
<p>Transformers are remarkably good at in-context learning (ICL) -- learning
from demonstrations without parameter updates -- but how they perform ICL
remains a mystery. Recent work suggests that Transformers may learn in-context
by internally running Gradient Descent, a first-order optimization method. In
this paper, we instead demonstrate that Transformers learn to implement
higher-order optimization methods to perform ICL. Focusing on in-context linear
regression, we show that Transformers learn to implement an algorithm very
similar to Iterative Newton's Method, a higher-order optimization method,
rather than Gradient Descent. Empirically, we show that predictions from
successive Transformer layers closely match different iterations of Newton's
Method linearly, with each middle layer roughly computing 3 iterations. In
contrast, exponentially more Gradient Descent steps are needed to match an
additional Transformers layer; this suggests that Transformers have an
comparable rate of convergence with high-order methods such as Iterative
Newton, which are exponentially faster than Gradient Descent. We also show that
Transformers can learn in-context on ill-conditioned data, a setting where
Gradient Descent struggles but Iterative Newton succeeds. Finally, we show
theoretical results which support our empirical findings and have a close
correspondence with them: we prove that Transformers can implement $k$
iterations of Newton's method with $\mathcal{O}(k)$ layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17087">Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult. (arXiv:2310.17087v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenghao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1">Molei Tao</a></p>
<p>Large learning rates, when applied to gradient descent for nonconvex
optimization, yield various implicit biases including the edge of stability
(Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et
al., 2020). These phenomena cannot be well explained by classical optimization
theory. Though significant theoretical progress has been made in understanding
these implicit biases, it remains unclear for which objective functions would
they occur. This paper provides an initial step in answering this question,
namely that these implicit biases are in fact various tips of the same iceberg.
They occur when the objective function of optimization has some good
regularity, which, in combination with a provable preference of large learning
rate gradient descent for moving toward flatter regions, results in these
nontrivial dynamical phenomena. To establish this result, we develop a new
global convergence theory under large learning rates, for a family of nonconvex
functions without globally Lipschitz continuous gradient, which was typically
assumed in existing convergence analysis. A byproduct is the first
non-asymptotic convergence rate bound for large-learning-rate gradient descent
optimization of nonconvex functions. We also validate our theory with
experiments on neural networks, where different losses, activation functions,
and batch normalization all can significantly affect regularity and lead to
very different training dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17100">Network Design through Graph Neural Networks: Identifying Challenges and Improving Performance. (arXiv:2310.17100v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1">Donald Loveland</a>, <a href="http://arxiv.org/find/cs/1/au:+Caceres_R/0/1/0/all/0/1">Rajmonda Caceres</a></p>
<p>Graph Neural Network (GNN) research has produced strategies to modify a
graph's edges using gradients from a trained GNN, with the goal of network
design. However, the factors which govern gradient-based editing are
understudied, obscuring why edges are chosen and if edits are grounded in an
edge's importance. Thus, we begin by analyzing the gradient computation in
previous works, elucidating the factors that influence edits and highlighting
the potential over-reliance on structural properties. Specifically, we find
that edges can achieve high gradients due to structural biases, rather than
importance, leading to erroneous edits when the factors are unrelated to the
design task. To improve editing, we propose ORE, an iterative editing method
that (a) edits the highest scoring edges and (b) re-embeds the edited graph to
refresh gradients, leading to less biased edge choices. We empirically study
ORE through a set of proposed design tasks, each with an external validation
method, demonstrating that ORE improves upon previous methods by up to 50%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17110">LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. (arXiv:2310.17110v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yijian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Simin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>In an era marked by the increasing adoption of Large Language Models (LLMs)
for various tasks, there is a growing focus on exploring LLMs' capabilities in
handling web data, particularly graph data. Dynamic graphs, which capture
temporal network evolution patterns, are ubiquitous in real-world web data.
Evaluating LLMs' competence in understanding spatial-temporal information on
dynamic graphs is essential for their adoption in web applications, which
remains unexplored in the literature. In this paper, we bridge the gap via
proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic
graphs, to the best of our knowledge, for the first time. Specifically, we
propose the LLM4DyG benchmark, which includes nine specially designed tasks
considering the capability evaluation of LLMs from both temporal and spatial
dimensions. Then, we conduct extensive experiments to analyze the impacts of
different data generators, data statistics, prompting techniques, and LLMs on
the model performance. Finally, we propose Disentangled Spatial-Temporal
Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal
understanding abilities. Our main observations are: 1) LLMs have preliminary
spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph
tasks show increasing difficulties for LLMs as the graph size and density
increase, while not sensitive to the time span and data generation mechanism,
3) the proposed DST2 prompting method can help to improve LLMs'
spatial-temporal understanding abilities on dynamic graphs for most tasks. The
data and codes will be open-sourced at publication time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17114">On the Convergence of CART under Sufficient Impurity Decrease Condition. (arXiv:2310.17114v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1">Rahul Mazumder</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1">Haoyue Wang</a></p>
<p>The decision tree is a flexible machine learning model that finds its success
in numerous applications. It is usually fitted in a recursively greedy manner
using CART. In this paper, we investigate the convergence rate of CART under a
regression setting. First, we establish an upper bound on the prediction error
of CART under a sufficient impurity decrease (SID) condition
\cite{chi2022asymptotic} -- our result improves upon the known result by
\cite{chi2022asymptotic} under a similar assumption. Furthermore, we provide
examples that demonstrate the error bound cannot be further improved by more
than a constant or a logarithmic factor. Second, we introduce a set of easily
verifiable sufficient conditions for the SID condition. Specifically, we
demonstrate that the SID condition can be satisfied in the case of an additive
model, provided that the component functions adhere to a ``locally reverse
Poincar{\'e} inequality". We discuss several well-known function classes in
non-parametric estimation to illustrate the practical utility of this concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17120">Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1">Reshmi Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kajal_H/0/1/0/all/0/1">Harjeet Singh Kajal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1">Sharanya Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_D/0/1/0/all/0/1">Dhuri Shrivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Samyadeep Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1">Hansi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Soundararajan Srinivasan</a></p>
<p>Breaking down a document or a conversation into multiple contiguous segments
based on its semantic structure is an important and challenging problem in NLP,
which can assist many downstream tasks. However, current works on topic
segmentation often focus on segmentation of structured texts. In this paper, we
comprehensively analyze the generalization capabilities of state-of-the-art
topic segmentation models on unstructured texts. We find that: (a) Current
strategies of pre-training on a large corpus of structured text such as
Wiki-727K do not help in transferability to unstructured conversational data.
(b) Training from scratch with only a relatively small-sized dataset of the
target unstructured domain improves the segmentation results by a significant
margin. We stress-test our proposed Topic Segmentation approach by
experimenting with multiple loss functions, in order to mitigate effects of
imbalance in unstructured conversational datasets. Our empirical evaluation
indicates that Focal Loss function is a robust alternative to Cross-Entropy and
re-weighted Cross-Entropy loss function when segmenting unstructured and
semi-structured chats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17132">Unleashing the potential of GNNs via Bi-directional Knowledge Transfer. (arXiv:2310.17132v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhizhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhenfeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a></p>
<p>Based on the message-passing paradigm, there has been an amount of research
proposing diverse and impressive feature propagation mechanisms to improve the
performance of GNNs. However, less focus has been put on feature
transformation, another major operation of the message-passing framework. In
this paper, we first empirically investigate the performance of the feature
transformation operation in several typical GNNs. Unexpectedly, we notice that
GNNs do not completely free up the power of the inherent feature transformation
operation. By this observation, we propose the Bi-directional Knowledge
Transfer (BiKT), a plug-and-play approach to unleash the potential of the
feature transformation operations without modifying the original architecture.
Taking the feature transformation operation as a derived representation
learning model that shares parameters with the original GNN, the direct
prediction by this model provides a topological-agnostic knowledge feedback
that can further instruct the learning of GNN and the feature transformations
therein. On this basis, BiKT not only allows us to acquire knowledge from both
the GNN and its derived model but promotes each other by injecting the
knowledge into the other. In addition, a theoretical analysis is further
provided to demonstrate that BiKT improves the generalization bound of the GNNs
from the perspective of domain adaption. An extensive group of experiments on
up to 7 datasets with 5 typical GNNs demonstrates that BiKT brings up to 0.5% -
4% performance gain over the original GNN, which means a boosted GNN is
obtained. Meanwhile, the derived model also shows a powerful performance to
compete with or even surpass the original GNN, enabling us to flexibly apply it
independently to some other specific downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17137">Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kaiwen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wenger_J/0/1/0/all/0/1">Jonathan Wenger</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_H/0/1/0/all/0/1">Haydn Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1">Geoff Pleiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1">Jacob R. Gardner</a></p>
<p>Gaussian process (GP) hyperparameter optimization requires repeatedly solving
linear systems with $n \times n$ kernel matrices. To address the prohibitive
$\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative
numerical methods, like conjugate gradients (CG). However, as datasets increase
in magnitude, the corresponding kernel matrices become increasingly
ill-conditioned and still require $\mathcal{O}(n^2)$ space without
partitioning. Thus, while CG increases the size of datasets GPs can be trained
on, modern datasets reach scales beyond its applicability. In this work, we
propose an iterative method which only accesses subblocks of the kernel matrix,
effectively enabling \emph{mini-batching}. Our algorithm, based on alternating
projection, has $\mathcal{O}(n)$ per-iteration time and space complexity,
solving many of the practical challenges of scaling GPs to very large datasets.
Theoretically, we prove our method enjoys linear convergence and empirically we
demonstrate its robustness to ill-conditioning. On large-scale benchmark
datasets up to four million datapoints our approach accelerates training by a
factor of 2$\times$ to 27$\times$ compared to CG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17139">Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning. (arXiv:2310.17139v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zang_H/0/1/0/all/0/1">Hongyu Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leiji Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baigui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1">Riashat Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Combes_R/0/1/0/all/0/1">Remi Tachet des Combes</a>, <a href="http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1">Romain Laroche</a></p>
<p>While bisimulation-based approaches hold promise for learning robust state
representations for Reinforcement Learning (RL) tasks, their efficacy in
offline RL tasks has not been up to par. In some instances, their performance
has even significantly underperformed alternative methods. We aim to understand
why bisimulation methods succeed in online settings, but falter in offline
tasks. Our analysis reveals that missing transitions in the dataset are
particularly harmful to the bisimulation principle, leading to ineffective
estimation. We also shed light on the critical role of reward scaling in
bounding the scale of bisimulation measurements and of the value error they
induce. Based on these findings, we propose to apply the expectile operator for
representation learning to our offline RL setting, which helps to prevent
overfitting to incomplete data. Meanwhile, by introducing an appropriate reward
scaling strategy, we avoid the risk of feature collapse in representation
space. We implement these recommendations on two state-of-the-art
bisimulation-based algorithms, MICo and SimSR, and demonstrate performance
gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at
\url{https://github.com/zanghyu/Offline_Bisimulation}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17146">Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shengpu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1">Jenna Wiens</a></p>
<p>In applying reinforcement learning (RL) to high-stakes domains, quantitative
and qualitative evaluation using observational data can help practitioners
understand the generalization performance of new policies. However, this type
of off-policy evaluation (OPE) is inherently limited since offline data may not
reflect the distribution shifts resulting from the application of new policies.
On the other hand, online evaluation by collecting rollouts according to the
new policy is often infeasible, as deploying new policies in these domains can
be unsafe. In this work, we propose a semi-offline evaluation framework as an
intermediate step between offline and online evaluation, where human users
provide annotations of unobserved counterfactual trajectories. While tempting
to simply augment existing data with such annotations, we show that this naive
approach can lead to biased results. Instead, we design a new family of OPE
estimators based on importance sampling (IS) and a novel weighting scheme that
incorporate counterfactual annotations without introducing additional bias. We
analyze the theoretical properties of our approach, showing its potential to
reduce both bias and variance compared to standard IS estimators. Our analyses
reveal important practical considerations for handling biased, noisy, or
missing annotations. In a series of proof-of-concept experiments involving
bandits and a healthcare-inspired simulator, we demonstrate that our approach
outperforms purely offline IS estimators and is robust to imperfect
annotations. Our framework, combined with principled human-centered design of
annotation solicitation, can enable the application of RL in high-stakes
domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17149">Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiabin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1">Lianghao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chao Huang</a></p>
<p>Spatio-temporal graph neural networks (STGNNs) have gained popularity as a
powerful tool for effectively modeling spatio-temporal dependencies in diverse
real-world urban applications, including intelligent transportation and public
safety. However, the black-box nature of STGNNs limits their interpretability,
hindering their application in scenarios related to urban resource allocation
and policy formulation. To bridge this gap, we propose an Explainable
Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances
STGNNs with inherent explainability, enabling them to provide accurate
predictions and faithful explanations simultaneously. Our framework integrates
a unified spatio-temporal graph attention network with a positional information
fusion layer as the STG encoder and decoder, respectively. Furthermore, we
propose a structure distillation approach based on the Graph Information
Bottleneck (GIB) principle with an explainable objective, which is instantiated
by the STG encoder and decoder. Through extensive experiments, we demonstrate
that our STExplainer outperforms state-of-the-art baselines in terms of
predictive accuracy and explainability metrics (i.e., sparsity and fidelity) on
traffic and crime prediction tasks. Furthermore, our model exhibits superior
representation ability in alleviating data missing and sparsity issues. The
implementation code is available at: https://github.com/HKUDS/STExplainer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17152">Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_R/0/1/0/all/0/1">Rupsa Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Akkaya_Z/0/1/0/all/0/1">Zehra Akkaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Luitjens_J/0/1/0/all/0/1">Johanna Luitjens</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1">Pan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1">Valentina Pedoia</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1">Sharmila Majumdar</a></p>
<p>In the current study, our purpose is to evaluate the feasibility of applying
deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in
healthy controls scanned at 0.55T and compared with 3.0T. The current study
assesses the performance of standard in-practice bone, and cartilage
segmentation algorithms at 0.55T, both qualitatively and quantitatively, in
terms of comparing segmentation performance, areas of improvement, and
compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial
results demonstrate a usable to good technical feasibility of translating
existing quantitative deep-learning-based image segmentation techniques,
trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition
environment. Especially in terms of segmenting cartilage compartments, the
models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T
low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be
utilized for evaluating knee cartilage thickness and bone segmentations aided
by established DL algorithms trained at higher-field strengths out-of-the-box
initially. This could be utilized at the far-spread point-of-care locations
with a lack of radiologists available to manually segment low-field images, at
least till a decent base of low-field data pool is collated. With further
fine-tuning with manual labeling of low-field data or utilizing synthesized
higher SNR images from low-field images, OA biomarker quantification
performance is potentially guaranteed to be further improved.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17153">Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration. (arXiv:2310.17153v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Longlin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a></p>
<p>Semi-implicit variational inference (SIVI) has been introduced to expand the
analytical variational families by defining expressive semi-implicit
distributions in a hierarchical manner. However, the single-layer architecture
commonly used in current SIVI methods can be insufficient when the target
posterior has complicated structures. In this paper, we propose hierarchical
semi-implicit variational inference, called HSIVI, which generalizes SIVI to
allow more expressive multi-layer construction of semi-implicit distributions.
By introducing auxiliary distributions that interpolate between a simple base
distribution and the target distribution, the conditional layers can be trained
by progressively matching these auxiliary distributions one layer after
another. Moreover, given pre-trained score networks, HSIVI can be used to
accelerate the sampling process of diffusion models with the score matching
objective. We show that HSIVI significantly enhances the expressiveness of SIVI
on several Bayesian inference problems with complicated target distributions.
When used for diffusion model acceleration, we show that HSIVI can produce high
quality samples comparable to or better than the existing fast diffusion model
based samplers with a small number of function evaluations on various datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17157">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. (arXiv:2310.17157v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zichang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1">Tri Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Binhang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Anshumali Shrivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Christopher Re</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Beidi Chen</a></p>
<p>Large language models (LLMs) with hundreds of billions of parameters have
sparked a new wave of exciting AI applications. However, they are
computationally expensive at inference time. Sparsity is a natural approach to
reduce this cost, but existing methods either require costly retraining, have
to forgo LLM's in-context learning ability, or do not yield wall-clock time
speedup on modern hardware. We hypothesize that contextual sparsity, which are
small, input-dependent sets of attention heads and MLP parameters that yield
approximately the same output as the dense model for a given input, can address
these issues. We show that contextual sparsity exists, that it can be
accurately predicted, and that we can exploit it to speed up LLM inference in
wall-clock time without compromising LLM's quality or in-context learning
ability. Based on these insights, we propose DejaVu, a system that uses a
low-cost algorithm to predict contextual sparsity on the fly given inputs to
each layer, along with an asynchronous and hardware-aware implementation that
speeds up LLM inference. We validate that DejaVu can reduce the inference
latency of OPT-175B by over 2X compared to the state-of-the-art
FasterTransformer, and over 6X compared to the widely used Hugging Face
implementation, without compromising model quality. The code is available at
https://github.com/FMInference/DejaVu.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17159">MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift. (arXiv:2310.17159v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neo_D/0/1/0/all/0/1">Dexter Neo</a>, <a href="http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1">Stefan Winkler</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tsuhan Chen</a></p>
<p>We present a new loss function that addresses the out-of-distribution (OOD)
calibration problem. While many objective functions have been proposed to
effectively calibrate models in-distribution, our findings show that they do
not always fare well OOD. Based on the Principle of Maximum Entropy, we
incorporate helpful statistical constraints observed during training,
delivering better model calibration without sacrificing accuracy. We provide
theoretical analysis and show empirically that our method works well in
practice, achieving state-of-the-art calibration on both synthetic and
real-world benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17167">Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenkai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1">Krista A. Ehinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1">Tom Drummond</a></p>
<p>This paper introduces two key contributions aimed at improving the speed and
quality of images generated through inverse diffusion processes. The first
contribution involves reparameterizing the diffusion process in terms of the
angle on a quarter-circular arc between the image and noise, specifically
setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This
reparameterization eliminates two singularities and allows for the expression
of diffusion evolution as a well-behaved ordinary differential equation (ODE).
In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be
used effectively. The second contribution is to directly estimate both the
image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which
enables more stable calculations of the update step in the inverse diffusion
steps, as accurate estimation of both the image and noise are crucial at
different stages of the process. Together with these changes, our model
achieves faster generation, with the ability to converge on high-quality images
more quickly, and higher quality of the generated images, as measured by
metrics such as Frechet Inception Distance (FID), spatial Frechet Inception
Distance (sFID), precision, and recall.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17168">Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andaz_S/0/1/0/all/0/1">Sohrab Andaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenach_C/0/1/0/all/0/1">Carson Eisenach</a>, <a href="http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1">Dhruv Madeka</a>, <a href="http://arxiv.org/find/cs/1/au:+Torkkola_K/0/1/0/all/0/1">Kari Torkkola</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Randy Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dean Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1">Sham Kakade</a></p>
<p>In this paper we address the problem of learning and backtesting inventory
control policies in the presence of general arrival dynamics -- which we term
as a quantity-over-time arrivals model (QOT). We also allow for order
quantities to be modified as a post-processing step to meet vendor constraints
such as order minimum and batch size constraints -- a common practice in real
supply chains. To the best of our knowledge this is the first work to handle
either arbitrary arrival dynamics or an arbitrary downstream post-processing of
order quantities. Building upon recent work (Madeka et al., 2022) we similarly
formulate the periodic review inventory control problem as an exogenous
decision process, where most of the state is outside the control of the agent.
Madeka et al. (2022) show how to construct a simulator that replays historic
data to solve this class of problem. In our case, we incorporate a deep
generative model for the arrivals process as part of the history replay. By
formulating the problem as an exogenous decision process, we can apply results
from Madeka et al. (2022) to obtain a reduction to supervised learning.
Finally, we show via simulation studies that this approach yields statistically
significant improvements in profitability over production baselines. Using data
from an ongoing real-world A/B test, we show that Gen-QOT generalizes well to
off-policy data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17173">DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic. (arXiv:2310.17173v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neo_D/0/1/0/all/0/1">Dexter Neo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tsuhan Chen</a></p>
<p>We present a novel extension to the family of Soft Actor-Critic (SAC)
algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC
can be further improved via additional statistical constraints derived from a
surrogate critic policy. Furthermore, our findings suggests that these
constraints provide an added robustness against potential domain shifts, which
are essential for safe deployment of reinforcement learning agents in the
real-world. We provide theoretical analysis and show empirical results on low
data regimes for both in-distribution and out-of-distribution variants of Atari
2600 games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17176">A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1">Mrinal Kanti Dhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1">Mou Deb</a>, <a href="http://arxiv.org/find/cs/1/au:+Madhab_D/0/1/0/all/0/1">D. Madhab</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zeyun Yu</a></p>
<p>Accurate teeth segmentation and orientation are fundamental in modern oral
healthcare, enabling precise diagnosis, treatment planning, and dental implant
design. In this study, we present a comprehensive approach to teeth
segmentation and orientation from panoramic X-ray images, leveraging deep
learning techniques. We build our model based on FUSegNet, a popular model
originally developed for wound segmentation, and introduce modifications by
incorporating grid-based attention gates into the skip connections. We
introduce oriented bounding box (OBB) generation through principal component
analysis (PCA) for precise tooth orientation estimation. Evaluating our
approach on the publicly available DNS dataset, comprising 543 panoramic X-ray
images, we achieve the highest Intersection-over-Union (IoU) score of 82.43%
and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in
teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU)
score of 82.82%. We also conduct detailed analyses of individual tooth labels
and categorical performance, shedding light on strengths and weaknesses. The
proposed model's accuracy and versatility offer promising prospects for
improving dental diagnoses, treatment planning, and personalized healthcare in
the oral domain. Our generated OBB coordinates and codes are available at
https://github.com/mrinal054/Instance_teeth_segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17178">Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ugadiarov_L/0/1/0/all/0/1">Leonid Ugadiarov</a>, <a href="http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1">Aleksandr I. Panov</a></p>
<p>There have recently been significant advances in the problem of unsupervised
object-centric representation learning and its application to downstream tasks.
The latest works support the argument that employing disentangled object
representations in image-based object-centric reinforcement learning tasks
facilitates policy learning. We propose a novel object-centric reinforcement
learning algorithm combining actor-critic and model-based approaches to utilize
these representations effectively. In our approach, we use a transformer
encoder to extract object representations and graph neural networks to
approximate the dynamics of an environment. The proposed method fills a
research gap in developing efficient object-centric world models for
reinforcement learning settings that can be used for environments with discrete
or continuous action spaces. Our algorithm performs better in a visually
complex 3D robotic environment and a 2D environment with compositional
structure than the state-of-the-art model-free actor-critic algorithm built
upon transformer architecture and the state-of-the-art monolithic model-based
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17185">Adaptive important sampling for Deep Ritz. (arXiv:2310.17185v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiaoliang Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuancheng Zhou</a></p>
<p>We introduce an adaptive sampling method for the Deep Ritz method aimed at
solving partial differential equations (PDEs). Two deep neural networks are
used. One network is employed to approximate the solution of PDEs, while the
other one is a deep generative model used to generate new collocation points to
refine the training set. The adaptive sampling procedure consists of two main
steps. The first step is solving the PDEs using the Deep Ritz method by
minimizing an associated variational loss discretized by the collocation points
in the training set. The second step involves generating a new training set,
which is then used in subsequent computations to further improve the accuracy
of the current approximate solution. We treat the integrand in the variational
loss as an unnormalized probability density function (PDF) and approximate it
using a deep generative model called bounded KRnet. The new samples and their
associated PDF values are obtained from the bounded KRnet. With these new
samples and their associated PDF values, the variational loss can be
approximated more accurately by importance sampling. Compared to the original
Deep Ritz method, the proposed adaptive method improves accuracy, especially
for problems characterized by low regularity and high dimensionality. We
demonstrate the effectiveness of our new method through a series of numerical
experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17191">How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiahai Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1">Jacob Steinhardt</a></p>
<p>To correctly use in-context information, language models (LMs) must bind
entities to their attributes. For example, given a context describing a "green
square" and a "blue circle", LMs must bind the shapes to their respective
colors. We analyze LM representations and identify the binding ID mechanism: a
general mechanism for solving the binding problem, which we observe in every
sufficiently large model from the Pythia and LLaMA families. Using causal
interventions, we show that LMs' internal activations represent binding
information by attaching binding ID vectors to corresponding entities and
attributes. We further show that binding ID vectors form a continuous subspace,
in which distances between binding ID vectors reflect their discernability.
Overall, our results uncover interpretable strategies in LMs for representing
symbolic knowledge in-context, providing a step towards understanding general
in-context reasoning in large-scale LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17200">Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xingyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yaling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1">Huaming Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yu Zhao</a></p>
<p>Federated learning, a decentralized approach to machine learning, faces
significant challenges such as extensive communication overheads, slow
convergence, and unstable improvements. These challenges primarily stem from
the gradient variance due to heterogeneous client data distributions. To
address this, we introduce a novel Networked Control Variates (FedNCV)
framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO)
as a fundamental control variate unit in the FedNCV framework, implemented at
both client and server levels. At the client level, the RLOO control variate is
employed to optimize local gradient updates, mitigating the variance introduced
by data samples. Once relayed to the server, the RLOO-based estimator further
provides an unbiased and low-variance aggregated gradient, leading to robust
global updates. This dual-side application is formalized as a linear
combination of composite control variates. We provide a mathematical expression
capturing this integration of double control variates within FedNCV and present
three theoretical results with corresponding proofs. This unique dual structure
equips FedNCV to address data heterogeneity and scalability issues, thus
potentially paving the way for large-scale applications. Moreover, we tested
FedNCV on six diverse datasets under a Dirichlet distribution with {\alpha} =
0.1, and benchmarked its performance against six SOTA methods, demonstrating
its superiority.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17202">miditok: A Python package for MIDI file tokenization. (arXiv:2310.17202v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fradet_N/0/1/0/all/0/1">Nathan Fradet</a>, <a href="http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1">Jean-Pierre Briot</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhel_F/0/1/0/all/0/1">Fabien Chhel</a>, <a href="http://arxiv.org/find/cs/1/au:+Seghrouchni_A/0/1/0/all/0/1">Amal El Fallah Seghrouchni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutowski_N/0/1/0/all/0/1">Nicolas Gutowski</a></p>
<p>Recent progress in natural language processing has been adapted to the
symbolic music modality. Language models, such as Transformers, have been used
with symbolic music for a variety of tasks among which music generation,
modeling or transcription, with state-of-the-art performances. These models are
beginning to be used in production products. To encode and decode music for the
backbone model, they need to rely on tokenizers, whose role is to serialize
music into sequences of distinct elements called tokens. MidiTok is an
open-source library allowing to tokenize symbolic music with great flexibility
and extended features. It features the most popular music tokenizations, under
a unified API. It is made to be easily used and extensible for everyone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17209">Weakly-Supervised Surgical Phase Recognition. (arXiv:2310.17209v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hirsch_R/0/1/0/all/0/1">Roy Hirsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1">Regev Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1">Mathilde Caron</a>, <a href="http://arxiv.org/find/cs/1/au:+Golany_T/0/1/0/all/0/1">Tomer Golany</a>, <a href="http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1">Daniel Freedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1">Ehud Rivlin</a></p>
<p>A key element of computer-assisted surgery systems is phase recognition of
surgical videos. Existing phase recognition algorithms require frame-wise
annotation of a large number of videos, which is time and money consuming. In
this work we join concepts of graph segmentation with self-supervised learning
to derive a random-walk solution for per-frame phase prediction. Furthermore,
we utilize within our method two forms of weak supervision: sparse timestamps
or few-shot learning. The proposed algorithm enjoys low complexity and can
operate in lowdata regimes. We validate our method by running experiments with
the public Cholec80 dataset of laparoscopic cholecystectomy videos,
demonstrating promising performance in multiple setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17217">Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1">Chenze Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhengrui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17230">Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1">Alex Tamkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Taufeeque_M/0/1/0/all/0/1">Mohammad Taufeeque</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a></p>
<p>Understanding neural networks is challenging in part because of the dense,
continuous nature of their hidden states. We explore whether we can train
neural networks to have hidden states that are sparse, discrete, and more
interpretable by quantizing their continuous features into what we call
codebook features. Codebook features are produced by finetuning neural networks
with vector quantization bottlenecks at each layer, producing a network whose
hidden features are the sum of a small number of discrete vector codes chosen
from a larger codebook. Surprisingly, we find that neural networks can operate
under this extreme bottleneck with only modest degradation in performance. This
sparse, discrete bottleneck also provides an intuitive way of controlling
neural network behavior: first, find codes that activate when the desired
behavior is present, then activate those same codes during generation to elicit
that behavior. We validate our approach by training codebook Transformers on
several different datasets. First, we explore a finite state machine dataset
with far more hidden states than neurons. In this setting, our approach
overcomes the superposition problem by assigning states to distinct codes, and
we find that we can make the neural network behave as if it is in a different
state by activating the code for that state. Second, we train Transformer
language models with up to 410M parameters on two natural language datasets. We
identify codes in these models representing diverse, disentangled concepts
(ranging from negative emotions to months of the year) and find that we can
guide the model to generate different topics by activating the appropriate
codes during inference. Overall, codebook features appear to be a promising
unit of analysis and control for neural networks and interpretability. Our
codebase and models are open-sourced at
https://github.com/taufeeque9/codebook-features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17238">Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhaohui Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1">Kewei Tu</a></p>
<p>Entity and Relation Extraction (ERE) is an important task in information
extraction. Recent marker-based pipeline models achieve state-of-the-art
performance, but still suffer from the error propagation issue. Also, most of
current ERE models do not take into account higher-order interactions between
multiple entities and relations, while higher-order modeling could be
beneficial.In this work, we propose HyperGraph neural network for ERE
($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based
pipleline model). To alleviate error propagation,we use a high-recall pruner
mechanism to transfer the burden of entity identification and labeling from the
NER module to the joint module of our model. For higher-order modeling, we
build a hypergraph, where nodes are entities (provided by the span pruner) and
relations thereof, and hyperedges encode interactions between two different
relations or between a relation and its associated subject and object entities.
We then run a hypergraph neural network for higher-order inference by applying
message passing over the built hypergraph. Experiments on three widely used
benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant
improvements over the previous state-of-the-art PL-marker.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17245">CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiao-Hu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiao-Liang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shi-Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhen-Qiu Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao-Yin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1">Mei-Jiang Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tian-Yu Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">De-Xing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bo-Xian Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1">Zeng-Guang Hou</a></p>
<p>Offline reinforcement learning (RL) aims to optimize policy using collected
data without online interactions. Model-based approaches are particularly
appealing for addressing offline RL challenges due to their capability to
mitigate the limitations of offline data through data generation using models.
Prior research has demonstrated that introducing conservatism into the model or
Q-function during policy optimization can effectively alleviate the prevalent
distribution drift problem in offline RL. However, the investigation into the
impacts of conservatism in reward estimation is still lacking. This paper
proposes a novel model-based offline RL algorithm, Conservative Reward for
model-based Offline Policy optimization (CROP), which conservatively estimates
the reward in model training. To achieve a conservative reward estimation, CROP
simultaneously minimizes the estimation error and the reward of random actions.
Theoretical analysis shows that this conservative reward mechanism leads to a
conservative policy evaluation and helps mitigate distribution drift.
Experiments on D4RL benchmarks showcase that the performance of CROP is
comparable to the state-of-the-art baselines. Notably, CROP establishes an
innovative connection between offline and online RL, highlighting that offline
RL problems can be tackled by adopting online RL techniques to the empirical
Markov decision process trained with a conservative reward. The source code is
available with https://github.com/G0K0URURI/CROP.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17247">Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity. (arXiv:2310.17247v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1">Jack Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+ONeill_C/0/1/0/all/0/1">Charles O&#x27;Neill</a>, <a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1">Thang Bui</a></p>
<p>In some settings neural networks exhibit a phenomenon known as grokking,
where they achieve perfect or near-perfect accuracy on the validation set long
after the same performance has been achieved on the training set. In this
paper, we discover that grokking is not limited to neural networks but occurs
in other settings such as Gaussian process (GP) classification, GP regression
and linear regression. We also uncover a mechanism by which to induce grokking
on algorithmic datasets via the addition of dimensions containing spurious
information. The presence of the phenomenon in non-neural architectures
provides evidence that grokking is not specific to SGD or weight norm
regularisation. Instead, grokking may be possible in any setting where solution
search is guided by complexity and error. Based on this insight and further
trends we see in the training trajectories of a Bayesian neural network (BNN)
and GP regression model, we make progress towards a more general theory of
grokking. Specifically, we hypothesise that the phenomenon is governed by the
accessibility of certain regions in the error and complexity landscapes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17250">IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_A/0/1/0/all/0/1">Anh T. Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Viharos_Z/0/1/0/all/0/1">Zsolt J. Viharos</a></p>
<p>Machine learning is a powerful tool for extracting valuable information and
making various predictions from diverse datasets. Traditional algorithms rely
on well-defined input and output variables however, there are scenarios where
the distinction between the input and output variables and the underlying,
associated (input and output) layers of the model, are unknown. Neural
Architecture Search (NAS) and Feature Selection have emerged as promising
solutions in such scenarios. This research proposes IDENAS, an Internal
Dependency-based Exploration for Neural Architecture Search, integrating NAS
with feature selection. The methodology explores internal dependencies in the
complete parameter space for classification involving 1D sensor and 2D image
data as well. IDENAS employs a modified encoder-decoder model and the
Sequential Forward Search (SFS) algorithm, combining input-output configuration
search with embedded feature selection. Experimental results demonstrate
IDENASs superior performance in comparison to other algorithms, showcasing its
effectiveness in model development pipelines and automated machine learning. On
average, IDENAS achieved significant modelling improvements, underscoring its
significant contribution to advancing the state-of-the-art in neural
architecture search and feature selection integration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17256">fairret: a Framework for Differentiable Fairness Regularization Terms. (arXiv:2310.17256v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buyl_M/0/1/0/all/0/1">Maarten Buyl</a>, <a href="http://arxiv.org/find/cs/1/au:+Defrance_M/0/1/0/all/0/1">MaryBeth Defrance</a>, <a href="http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1">Tijl De Bie</a></p>
<p>Current tools for machine learning fairness only admit a limited range of
fairness definitions and have seen little integration with automatic
differentiation libraries, despite the central role these libraries play in
modern machine learning pipelines.
</p>
<p>We introduce a framework of fairness regularization terms (fairrets) which
quantify bias as modular objectives that are easily integrated in automatic
differentiation pipelines. By employing a general definition of fairness in
terms of linear-fractional statistics, a wide class of fairrets can be computed
efficiently. Experiments show the behavior of their gradients and their utility
in enforcing fairness with minimal loss of predictive power compared to
baselines. Our contribution includes a PyTorch implementation of the fairret
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17264">Variance of ML-based software fault predictors: are we really improving fault prediction?. (arXiv:2310.17264v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shahini_X/0/1/0/all/0/1">Xhulja Shahini</a>, <a href="http://arxiv.org/find/cs/1/au:+Bubel_D/0/1/0/all/0/1">Domenic Bubel</a>, <a href="http://arxiv.org/find/cs/1/au:+Metzger_A/0/1/0/all/0/1">Andreas Metzger</a></p>
<p>Software quality assurance activities become increasingly difficult as
software systems become more and more complex and continuously grow in size.
Moreover, testing becomes even more expensive when dealing with large-scale
systems. Thus, to effectively allocate quality assurance resources, researchers
have proposed fault prediction (FP) which utilizes machine learning (ML) to
predict fault-prone code areas. However, ML algorithms typically make use of
stochastic elements to increase the prediction models' generalizability and
efficiency of the training process. These stochastic elements, also known as
nondeterminism-introducing (NI) factors, lead to variance in the training
process and as a result, lead to variance in prediction accuracy and training
time. This variance poses a challenge for reproducibility in research. More
importantly, while fault prediction models may have shown good performance in
the lab (e.g., often-times involving multiple runs and averaging outcomes),
high variance of results can pose the risk that these models show low
performance when applied in practice. In this work, we experimentally analyze
the variance of a state-of-the-art fault prediction approach. Our experimental
results indicate that NI factors can indeed cause considerable variance in the
fault prediction models' accuracy. We observed a maximum variance of 10.10% in
terms of the per-class accuracy metric. We thus, also discuss how to deal with
such variance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17273">Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adachi_M/0/1/0/all/0/1">Masaki Adachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Planden_B/0/1/0/all/0/1">Brady Planden</a>, <a href="http://arxiv.org/find/cs/1/au:+Howey_D/0/1/0/all/0/1">David A. Howey</a>, <a href="http://arxiv.org/find/cs/1/au:+Maundet_K/0/1/0/all/0/1">Krikamol Maundet</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1">Michael A. Osborne</a>, <a href="http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1">Siu Lun Chau</a></p>
<p>Like many optimizers, Bayesian optimization often falls short of gaining user
trust due to opacity. While attempts have been made to develop human-centric
optimizers, they typically assume user knowledge is well-specified and
error-free, employing users mainly as supervisors of the optimization process.
We relax these assumptions and propose a more balanced human-AI partnership
with our Collaborative and Explainable Bayesian Optimization (CoExBO)
framework. Instead of explicitly requiring a user to provide a knowledge model,
CoExBO employs preference learning to seamlessly integrate human insights into
the optimization, resulting in algorithmic suggestions that resonate with user
preference. CoExBO explains its candidate selection every iteration to foster
trust, empowering users with a clearer grasp of the optimization. Furthermore,
CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with
extreme adversarial interventions, the algorithm converges asymptotically to a
vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI
teaming experiments in lithium-ion battery design, highlighting substantial
improvements over conventional methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17281">BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds. (arXiv:2310.17281v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sautier_C/0/1/0/all/0/1">Corentin Sautier</a>, <a href="http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1">Gilles Puy</a>, <a href="http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1">Alexandre Boulch</a>, <a href="http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1">Renaud Marlet</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1">Vincent Lepetit</a></p>
<p>We present a surprisingly simple and efficient method for self-supervision of
3D backbone on automotive Lidar point clouds. We design a contrastive loss
between features of Lidar scans captured in the same scene. Several such
approaches have been proposed in the literature from PointConstrast, which uses
a contrast at the level of points, to the state-of-the-art TARL, which uses a
contrast at the level of segments, roughly corresponding to objects. While the
former enjoys a great simplicity of implementation, it is surpassed by the
latter, which however requires a costly pre-processing. In BEVContrast, we
define our contrast at the level of 2D cells in the Bird's Eye View plane.
Resulting cell-level representations offer a good trade-off between the
point-level representations exploited in PointContrast and segment-level
representations exploited in TARL: we retain the simplicity of PointContrast
(cell representations are cheap to compute) while surpassing the performance of
TARL in downstream semantic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17303">Demonstration-Regularized RL. (arXiv:2310.17303v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tiapkin_D/0/1/0/all/0/1">Daniil Tiapkin</a>, <a href="http://arxiv.org/find/stat/1/au:+Belomestny_D/0/1/0/all/0/1">Denis Belomestny</a>, <a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1">Eric Moulines</a>, <a href="http://arxiv.org/find/stat/1/au:+Naumov_A/0/1/0/all/0/1">Alexey Naumov</a>, <a href="http://arxiv.org/find/stat/1/au:+Perrault_P/0/1/0/all/0/1">Pierre Perrault</a>, <a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1">Pierre Menard</a></p>
<p>Incorporating expert demonstrations has empirically helped to improve the
sample efficiency of reinforcement learning (RL). This paper quantifies
theoretically to what extent this extra information reduces RL's sample
complexity. In particular, we study the demonstration-regularized reinforcement
learning that leverages the expert demonstrations by KL-regularization for a
policy learned by behavior cloning. Our findings reveal that using
$N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal
policy at a sample complexity of order
$\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$
in finite and $\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2
N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is
the target precision, $H$ the horizon, $A$ the number of action, $S$ the number
of states in the finite case and $d$ the dimension of the feature space in the
linear case. As a by-product, we provide tight convergence guarantees for the
behaviour cloning procedure under general assumptions on the policy classes.
Additionally, we establish that demonstration-regularized methods are provably
efficient for reinforcement learning from human feedback (RLHF). In this
respect, we provide theoretical evidence showing the benefits of
KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid
pessimism injection by employing computationally feasible regularization to
handle reward estimation uncertainty, thus setting our approach apart from the
prior works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17325">C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiaxin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bang An</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuancheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a></p>
<p>Representation learning assumes that real-world data is generated by a few
semantically meaningful generative factors (i.e., sources of variation) and
aims to discover them in the latent space. These factors are expected to be
causally disentangled, meaning that distinct factors are encoded into separate
latent variables, and changes in one factor will not affect the values of the
others. Compared to statistical independence, causal disentanglement allows
more controllable data generation, improved robustness, and better
generalization. However, most existing work assumes unconfoundedness in the
discovery process, that there are no common causes to the generative factors
and thus obtain only statistical independence. In this paper, we recognize the
importance of modeling confounders in discovering causal generative factors.
Unfortunately, such factors are not identifiable without proper inductive bias.
We fill the gap by introducing a framework entitled Confounded-Disentanglement
(C-Disentanglement), the first framework that explicitly introduces the
inductive bias of confounder via labels from domain expertise. In addition, we
accordingly propose an approach to sufficiently identify the causally
disentangled factors under any inductive bias of the confounder. We conduct
extensive experiments on both synthetic and real-world datasets. Our method
demonstrates competitive results compared to various SOTA baselines in
obtaining causally disentangled features and downstream tasks under domain
shifts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17330">CQM: Curriculum Reinforcement Learning with a Quantized World Model. (arXiv:2310.17330v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seungjae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1">Daesol Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jonghae Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">H. Jin Kim</a></p>
<p>Recent curriculum Reinforcement Learning (RL) has shown notable progress in
solving complex tasks by proposing sequences of surrogate tasks. However, the
previous approaches often face challenges when they generate curriculum goals
in a high-dimensional space. Thus, they usually rely on manually specified goal
spaces. To alleviate this limitation and improve the scalability of the
curriculum, we propose a novel curriculum method that automatically defines the
semantic goal space which contains vital information for the curriculum
process, and suggests curriculum goals over it. To define the semantic goal
space, our method discretizes continuous observations via vector
quantized-variational autoencoders (VQ-VAE) and restores the temporal relations
between the discretized observations by a graph. Concurrently, ours suggests
uncertainty and temporal distance-aware curriculum goals that converges to the
final goals over the automatically composed goal space. We demonstrate that the
proposed method allows efficient explorations in an uninformed environment with
raw goal examples only. Also, ours outperforms the state-of-the-art curriculum
RL methods on data efficiency and performance, in various goal-reaching tasks
even with ego-centric visual inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17332">On Forecast Stability. (arXiv:2310.17332v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Godahewa_R/0/1/0/all/0/1">Rakshitha Godahewa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1">Christoph Bergmeir</a>, <a href="http://arxiv.org/find/cs/1/au:+Baz_Z/0/1/0/all/0/1">Zeynep Erkin Baz</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chengjun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhangdi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1">Salvador Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Benavides_D/0/1/0/all/0/1">Dario Benavides</a></p>
<p>Forecasts are typically not produced in a vacuum but in a business context,
where forecasts are generated on a regular basis and interact with each other.
For decisions, it may be important that forecasts do not change arbitrarily,
and are stable in some sense. However, this area has received only limited
attention in the forecasting literature. In this paper, we explore two types of
forecast stability that we call vertical stability and horizontal stability.
The existing works in the literature are only applicable to certain base models
and extending these frameworks to be compatible with any base model is not
straightforward. Furthermore, these frameworks can only stabilise the forecasts
vertically. To fill this gap, we propose a simple linear-interpolation-based
approach that is applicable to stabilise the forecasts provided by any base
model vertically and horizontally. The approach can produce both accurate and
stable forecasts. Using N-BEATS, Pooled Regression and LightGBM as the base
models, in our evaluation on four publicly available datasets, the proposed
framework is able to achieve significantly higher stability and/or accuracy
compared to a set of benchmarks including a state-of-the-art forecast
stabilisation method across three error metrics and six stability metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17335">A multi-artifact EEG denoising by frequency-based deep learning. (arXiv:2310.17335v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gabardi_M/0/1/0/all/0/1">Matteo Gabardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Saibene_A/0/1/0/all/0/1">Aurora Saibene</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasparini_F/0/1/0/all/0/1">Francesca Gasparini</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizzo_D/0/1/0/all/0/1">Daniele Rizzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1">Fabio Antonio Stella</a></p>
<p>Electroencephalographic (EEG) signals are fundamental to neuroscience
research and clinical applications such as brain-computer interfaces and
neurological disorder diagnosis. These signals are typically a combination of
neurological activity and noise, originating from various sources, including
physiological artifacts like ocular and muscular movements. Under this setting,
we tackle the challenge of distinguishing neurological activity from
noise-related sources. We develop a novel EEG denoising model that operates in
the frequency domain, leveraging prior knowledge about noise spectral features
to adaptively compute optimal convolutional filters for noise separation. The
model is trained to learn an empirical relationship connecting the spectral
characteristics of noise and noisy signal to a non-linear transformation which
allows signal denoising. Performance evaluation on the EEGdenoiseNet dataset
shows that the proposed model achieves optimal results according to both
temporal and spectral metrics. The model is found to remove physiological
artifacts from input EEG data, thus achieving effective EEG denoising. Indeed,
the model performance either matches or outperforms that achieved by benchmark
models, proving to effectively remove both muscle and ocular artifacts without
the need to perform any training on the particular type of artifact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17341">De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buin_A/0/1/0/all/0/1">Andrei Buin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1">Hung Yi Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadsden_S/0/1/0/all/0/1">S. Andrew Gadsden</a>, <a href="http://arxiv.org/find/cs/1/au:+Alderson_F/0/1/0/all/0/1">Faraz A. Alderson</a></p>
<p>We present here a combination of two networks, Recurrent Neural Networks
(RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction
generation using the novel Reaction Smiles-like representation of reactions
(CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks
are known for their autoregressive properties and are frequently used in
language modelling with direct application to SMILES generation. The relatively
novel TCNs possess similar properties with wide receptive field while obeying
the causality required for natural language processing (NLP). The combination
of both latent representations expressed through TCN and RNN results in an
overall better performance compared to RNN alone. Additionally, it is shown
that different fine-tuning protocols have a profound impact on generative scope
of the model when applied on a dataset of interest via transfer learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17355">Exploring the Trie of Rules: a fast data structure for the representation of association rules. (arXiv:2310.17355v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kudriavtsev_M/0/1/0/all/0/1">Mikhail Kudriavtsev</a>, <a href="http://arxiv.org/find/cs/1/au:+Bezbradica_D/0/1/0/all/0/1">Dr Marija Bezbradica</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarren_D/0/1/0/all/0/1">Dr Andrew McCarren</a></p>
<p>Association rule mining techniques can generate a large volume of sequential
data when implemented on transactional databases. Extracting insights from a
large set of association rules has been found to be a challenging process. When
examining a ruleset, the fundamental question is how to summarise and represent
meaningful mined knowledge efficiently. Many algorithms and strategies have
been developed to address issue of knowledge extraction; however, the
effectiveness of this process can be limited by the data structures. A better
data structure can sufficiently affect the speed of the knowledge extraction
process. This paper proposes a novel data structure, called the Trie of rules,
for storing a ruleset that is generated by association rule mining. The
resulting data structure is a prefix-tree graph structure made of pre-mined
rules. This graph stores the rules as paths within the prefix-tree in a way
that similar rules overlay each other. Each node in the tree represents a rule
where a consequent is this node, and an antecedent is a path from this node to
the root of the tree. The evaluation showed that the proposed representation
technique is promising. It compresses a ruleset with almost no data loss and
benefits in terms of time for basic operations such as searching for a specific
rule and sorting, which is the base for many knowledge discovery methods.
Moreover, our method demonstrated a significant improvement in traversing time,
achieving an 8-fold increase compared to traditional data structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17356">Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning. (arXiv:2310.17356v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Al_lahham_A/0/1/0/all/0/1">Anas Al-lahham</a>, <a href="http://arxiv.org/find/cs/1/au:+Theeb_O/0/1/0/all/0/1">Obaidah Theeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Elalem_K/0/1/0/all/0/1">Khaled Elalem</a>, <a href="http://arxiv.org/find/cs/1/au:+Alshawi_T/0/1/0/all/0/1">Tariq A. Alshawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alshebeili_S/0/1/0/all/0/1">Saleh A. Alshebeili</a></p>
<p>Ahead-of-time forecasting of the output power of power plants is essential
for the stability of the electricity grid and ensuring uninterrupted service.
However, forecasting renewable energy sources is difficult due to the chaotic
behavior of natural energy sources. This paper presents a new approach to
estimate short-term solar irradiance from sky images. The~proposed algorithm
extracts features from sky images and use learning-based techniques to estimate
the solar irradiance. The~performance of proposed machine learning (ML)
algorithm is evaluated using two publicly available datasets of sky images.
The~datasets contain over 350,000 images for an interval of 16 years, from 2004
to 2020, with the corresponding global horizontal irradiance (GHI) of each
image as the ground truth. Compared to the state-of-the-art computationally
heavy algorithms proposed in the literature, our approach achieves competitive
results with much less computational complexity for both nowcasting and
forecasting up to 4 h ahead of time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17360">Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning. (arXiv:2310.17360v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Junfeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhencheng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1">Roger Zimmermann</a></p>
<p>Spatio-temporal graph learning is a fundamental problem in the Web of Things
era, which enables a plethora of Web applications such as smart cities, human
mobility and climate analysis. Existing approaches tackle different learning
tasks independently, tailoring their models to unique task characteristics.
These methods, however, fall short of modeling intrinsic uncertainties in the
spatio-temporal data. Meanwhile, their specialized designs limit their
universality as general spatio-temporal learning solutions. In this paper, we
propose to model the learning tasks in a unified perspective, viewing them as
predictions based on conditional information with shared spatio-temporal
patterns. Based on this proposal, we introduce Unified Spatio-Temporal
Diffusion Models (USTD) to address the tasks uniformly within the
uncertainty-aware diffusion framework. USTD is holistically designed,
comprising a shared spatio-temporal encoder and attention-based denoising
networks that are task-specific. The shared encoder, optimized by a
pre-training strategy, effectively captures conditional spatio-temporal
patterns. The denoising networks, utilizing both cross- and self-attention,
integrate conditional dependencies and generate predictions. Opting for
forecasting and kriging as downstream tasks, we design Gated Attention (SGA)
and Temporal Gated Attention (TGA) for each task, with different emphases on
the spatial and temporal dimensions, respectively. By combining the advantages
of deterministic encoders and probabilistic diffusion models, USTD achieves
state-of-the-art performances compared to deterministic and probabilistic
baselines in both tasks, while also providing valuable uncertainty estimates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17378">Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Racz_D/0/1/0/all/0/1">D&#xe1;niel R&#xe1;cz</a>, <a href="http://arxiv.org/find/cs/1/au:+Petreczky_M/0/1/0/all/0/1">Mih&#xe1;ly Petreczky</a>, <a href="http://arxiv.org/find/cs/1/au:+Csertan_A/0/1/0/all/0/1">Andr&#xe1;s Csert&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Daroczy_B/0/1/0/all/0/1">B&#xe1;lint Dar&#xf3;czy</a></p>
<p>Recent advances in deep learning have given us some very promising results on
the generalization ability of deep neural networks, however literature still
lacks a comprehensive theory explaining why heavily over-parametrized models
are able to generalize well while fitting the training data. In this paper we
propose a PAC type bound on the generalization error of feedforward ReLU
networks via estimating the Rademacher complexity of the set of networks
available from an initial parameter vector via gradient descent. The key idea
is to bound the sensitivity of the network's gradient to perturbation of the
input data along the optimization trajectory. The obtained bound does not
explicitly depend on the depth of the network. Our results are experimentally
verified on the MNIST and CIFAR-10 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17383">On the recognition of the game type based on physiological signals and eye tracking. (arXiv:2310.17383v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Czekaj_L/0/1/0/all/0/1">&#x141;ukasz Czekaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Radzinski_L/0/1/0/all/0/1">&#x141;ukasz Radzinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolimaga_M/0/1/0/all/0/1">Mateusz Kolimaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Domaszewicz_J/0/1/0/all/0/1">Jakub Domaszewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitlowski_R/0/1/0/all/0/1">Robert Kit&#x142;owski</a>, <a href="http://arxiv.org/find/cs/1/au:+Szwoch_M/0/1/0/all/0/1">Mariusz Szwoch</a>, <a href="http://arxiv.org/find/cs/1/au:+Duch_W/0/1/0/all/0/1">W&#x142;odzis&#x142;aw Duch</a></p>
<p>Automated interpretation of signals yields many impressive applications from
the area of affective computing and human activity recognition (HAR). In this
paper we ask the question about possibility of cognitive activity recognition
on the base of particular set of signals. We use recognition of the game played
by the participant as a playground for exploration of the problem. We build
classifier of three different games (Space Invaders, Tetris, Tower Defence) and
inter-game pause. We validate classifier in the player-independent and
player-dependent scenario. We discuss the improvement in the player-dependent
scenario in the context of biometric person recognition. On the base of the
results obtained in game classification, we consider potential applications in
smart surveillance and quantified self.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17385">Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Achddou_J/0/1/0/all/0/1">Juliette Achddou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1">Nicol&#xf2; Cesa-Bianchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1">Pierre Laforgue</a></p>
<p>We study multitask online learning in a setting where agents can only
exchange information with their neighbors on an arbitrary communication
network. We introduce $\texttt{MT-CO}_2\texttt{OL}$, a decentralized algorithm
for this setting whose regret depends on the interplay between the task
similarities and the network structure. Our analysis shows that the regret of
$\texttt{MT-CO}_2\texttt{OL}$ is never worse (up to constants) than the bound
obtained when agents do not share information. On the other hand, our bounds
significantly improve when neighboring agents operate on similar tasks. In
addition, we prove that our algorithm can be made differentially private with a
negligible impact on the regret when the losses are linear. Finally, we provide
experimental support for our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17386">A Challenge in Reweighting Data with Bilevel Optimization. (arXiv:2310.17386v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ivanova_A/0/1/0/all/0/1">Anastasia Ivanova</a>, <a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1">Pierre Ablin</a></p>
<p>In many scenarios, one uses a large training set to train a model with the
goal of performing well on a smaller testing set with a different distribution.
Learning a weight for each data point of the training set is an appealing
solution, as it ideally allows one to automatically learn the importance of
each training point for generalization on the testing set. This task is usually
formalized as a bilevel optimization problem. Classical bilevel solvers are
based on a warm-start strategy where both the parameters of the models and the
data weights are learned at the same time. We show that this joint dynamic may
lead to sub-optimal solutions, for which the final data weights are very
sparse. This finding illustrates the difficulty of data reweighting and offers
a clue as to why this method is rarely used in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17394">Enhancing Graph Neural Networks with Structure-Based Prompt. (arXiv:2310.17394v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Q/0/1/0/all/0/1">Qingqing Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zeyuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiding Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1">Anfeng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a></p>
<p>Graph Neural Networks (GNNs) are powerful in learning semantics of graph
data. Recently, a new paradigm "pre-train, prompt" has shown promising results
in adapting GNNs to various tasks with less supervised data. The success of
such paradigm can be attributed to the more consistent objectives of
pre-training and task-oriented prompt tuning, where the pre-trained knowledge
can be effectively transferred to downstream tasks. However, an overlooked
issue of existing studies is that the structure information of graph is usually
exploited during pre-training for learning node representations, while
neglected in the prompt tuning stage for learning task-specific parameters. To
bridge this gap, we propose a novel structure-based prompting method for GNNs,
namely SAP, which consistently exploits structure information in both
pre-training and prompt tuning stages. In particular, SAP 1) employs a
dual-view contrastive learning to align the latent semantic spaces of node
attributes and graph structure, and 2) incorporates structure information in
prompted graph to elicit more pre-trained knowledge in prompt tuning. We
conduct extensive experiments on node classification and graph classification
tasks to show the effectiveness of SAP. Moreover, we show that SAP can lead to
better performance in more challenging few-shot scenarios on both homophilous
and heterophilous graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17403">Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheurer_E/0/1/0/all/0/1">Erik Scheurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1">Jenny Schmalfuss</a>, <a href="http://arxiv.org/find/cs/1/au:+Lis_A/0/1/0/all/0/1">Alexander Lis</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1">Andr&#xe9;s Bruhn</a></p>
<p>Adversarial patches undermine the reliability of optical flow predictions
when placed in arbitrary scene locations. Therefore, they pose a realistic
threat to real-world motion detection and its downstream applications.
Potential remedies are defense strategies that detect and remove adversarial
patches, but their influence on the underlying motion prediction has not been
investigated. In this paper, we thoroughly examine the currently available
detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art
optical flow methods, and illuminate their side effects on the quality and
robustness of the final flow predictions. In particular, we implement
defense-aware attacks to investigate whether current defenses are able to
withstand attacks that take the defense mechanism into account. Our experiments
yield two surprising results: Detect-and-remove defenses do not only lower the
optical flow quality on benign scenes, in doing so, they also harm the
robustness under patch attacks for all tested optical flow methods except
FlowNetC. As currently employed detect-and-remove defenses fail to deliver the
promised adversarial robustness for optical flow, they evoke a false sense of
security. The code is available at
https://github.com/cv-stuttgart/DetectionDefenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17404">Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1">Facundo Manuel Quiroga</a>, <a href="http://arxiv.org/find/cs/1/au:+Torrents_Barrena_J/0/1/0/all/0/1">Jordina Torrents-Barrena</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1">Laura Cristina Lanzarini</a>, <a href="http://arxiv.org/find/cs/1/au:+Puig_Valls_D/0/1/0/all/0/1">Domenec Puig-Valls</a></p>
<p>Invariances in neural networks are useful and necessary for many tasks.
However, the representation of the invariance of most neural network models has
not been characterized. We propose measures to quantify the invariance of
neural networks in terms of their internal representation. The measures are
efficient and interpretable, and can be applied to any neural network model.
They are also more sensitive to invariance than previously defined measures. We
validate the measures and their properties in the domain of affine
transformations and the CIFAR10 and MNIST datasets, including their stability
and interpretability. Using the measures, we perform a first analysis of CNN
models and show that their internal invariance is remarkably stable to random
weight initializations, but not to changes in dataset or transformation. We
believe the measures will enable new avenues of research in invariance
representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17405">Causal Modeling with Stationary Diffusions. (arXiv:2310.17405v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lorch_L/0/1/0/all/0/1">Lars Lorch</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>We develop a novel approach towards causal inference. Rather than structural
equations over a causal graph, we learn stochastic differential equations
(SDEs) whose stationary densities model a system's behavior under
interventions. These stationary diffusion models do not require the formalism
of causal graphs, let alone the common assumption of acyclicity. We show that
in several cases, they generalize to unseen interventions on their variables,
often better than classical approaches. Our inference method is based on a new
theoretical result that expresses a stationarity condition on the diffusion's
generator in a reproducing kernel Hilbert space. The resulting kernel deviation
from stationarity (KDS) is an objective function of independent interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17427">Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1">Franco Ronchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1">Facundo Manuel Quiroga</a>, <a href="http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1">C&#xe9;sar Estrebou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1">Laura Lanzarini</a></p>
<p>Automatic sign language recognition is an important topic within the areas of
human-computer interaction and machine learning. On the one hand, it poses a
complex challenge that requires the intervention of various knowledge areas,
such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people.
</p>
<p>This paper offers two main contributions: first, the creation of a database
of handshapes for the Argentinian Sign Language (LSA), which is a topic that
has barely been discussed so far. Secondly, a technique for image processing,
descriptor extraction and subsequent handshape classification using a
supervised adaptation of self-organizing maps that is called ProbSom. This
technique is compared to others in the state of the art, such as Support Vector
Machines (SVM), Random Forests, and Neural Networks.
</p>
<p>The database that was built contains 800 images with 16 LSA handshapes, and
is a first step towards building a comprehensive database of Argentinian signs.
The ProbSom-based neural classifier, using the proposed descriptor, achieved an
accuracy rate above 90%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17432">Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models. (arXiv:2310.17432v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goodier_J/0/1/0/all/0/1">Joseph Goodier</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_N/0/1/0/all/0/1">Neill D.F. Campbell</a></p>
<p>Out-of-Distribution detection between dataset pairs has been extensively
explored with generative models. We show that likelihood-based
Out-of-Distribution detection can be extended to diffusion models by leveraging
the fact that they, like other likelihood-based generative models, are
dramatically affected by the input sample complexity. Currently, all
Out-of-Distribution detection methods with Diffusion Models are
reconstruction-based. We propose a new likelihood ratio for Out-of-Distribution
detection with Deep Denoising Diffusion Models, which we call the Complexity
Corrected Likelihood Ratio. Our likelihood ratio is constructed using Evidence
Lower-Bound evaluations from an individual model at various noising levels. We
present results that are comparable to state-of-the-art Out-of-Distribution
detection methods with generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17437">Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language. (arXiv:2310.17437v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1">Franco Ronchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1">Facundo Manuel Quiroga</a>, <a href="http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1">C&#xe9;sar Estrebou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1">Laura Lanzarini</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosete_A/0/1/0/all/0/1">Alejandro Rosete</a></p>
<p>Automatic sign language recognition (SLR) is an important topic within the
areas of human-computer interaction and machine learning. On the one hand, it
poses a complex challenge that requires the intervention of various knowledge
areas, such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people, as well as the teaching of sign language for the hearing population.
</p>
<p>SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or
similar models to recognize signs. Such techniques exploit the sequential
ordering of frames to reduce the number of hypothesis. This paper presents a
general probabilistic model for sign classification that combines
sub-classifiers based on different types of features such as position, movement
and handshape. The model employs a bag-of-words approach in all classification
steps, to explore the hypothesis that ordering is not essential for
recognition. The proposed model achieved an accuracy rate of 97% on an
Argentinian Sign Language dataset containing 64 classes of signs and 3200
samples, providing some evidence that indeed recognition without ordering is
possible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17458">Coalitional Bargaining via Reinforcement Learning: An Application to Collaborative Vehicle Routing. (arXiv:2310.17458v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mak_S/0/1/0/all/0/1">Stephen Mak</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1">Tim Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostroumov_M/0/1/0/all/0/1">Michael Ostroumov</a>, <a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1">Alexandra Brintrup</a></p>
<p>Collaborative Vehicle Routing is where delivery companies cooperate by
sharing their delivery information and performing delivery requests on behalf
of each other. This achieves economies of scale and thus reduces cost,
greenhouse gas emissions, and road congestion. But which company should partner
with whom, and how much should each company be compensated? Traditional game
theoretic solution concepts, such as the Shapley value or nucleolus, are
difficult to calculate for the real-world problem of Collaborative Vehicle
Routing due to the characteristic function scaling exponentially with the
number of agents. This would require solving the Vehicle Routing Problem (an
NP-Hard problem) an exponential number of times. We therefore propose to model
this problem as a coalitional bargaining game where - crucially - agents are
not given access to the characteristic function. Instead, we implicitly reason
about the characteristic function, and thus eliminate the need to evaluate the
VRP an exponential number of times - we only need to evaluate it once. Our
contribution is that our decentralised approach is both scalable and considers
the self-interested nature of companies. The agents learn using a modified
Independent Proximal Policy Optimisation. Our RL agents outperform a strong
heuristic bot. The agents correctly identify the optimal coalitions 79% of the
time with an average optimality gap of 4.2% and reduction in run-time of 62%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17462">Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1">Daniel Kienzle</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1">Julian Lorenz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1">Katja Ludwig</a>, <a href="http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1">Rainer Lienhart</a></p>
<p>We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object's
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method's potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17463">Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation. (arXiv:2310.17463v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hess_K/0/1/0/all/0/1">Konstantin Hess</a>, <a href="http://arxiv.org/find/cs/1/au:+Melnychuk_V/0/1/0/all/0/1">Valentyn Melnychuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Frauen_D/0/1/0/all/0/1">Dennis Frauen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1">Stefan Feuerriegel</a></p>
<p>Treatment effect estimation in continuous time is crucial for personalized
medicine. However, existing methods for this task are limited to point
estimates of the potential outcomes, whereas uncertainty estimates have been
ignored. Needless to say, uncertainty quantification is crucial for reliable
decision-making in medical applications. To fill this gap, we propose a novel
Bayesian neural controlled differential equation (BNCDE) for treatment effect
estimation in continuous time. In our BNCDE, the time dimension is modeled
through a coupled system of neural controlled differential equations and neural
stochastic differential equations, where the neural stochastic differential
equations allow for tractable variational Bayesian inference. Thereby, for an
assigned sequence of treatments, our BNCDE provides meaningful posterior
predictive distributions of the potential outcomes. To the best of our
knowledge, ours is the first tailored neural method to provide uncertainty
estimates of treatment effects in continuous time. As such, our method is of
direct practical value for promoting reliable decision-making in medicine.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17467">The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ambrogioni_L/0/1/0/all/0/1">Luca Ambrogioni</a></p>
<p>Generative diffusion models have achieved spectacular performance in many
areas of generative modeling. While the fundamental ideas behind these models
come from non-equilibrium physics, in this paper we show that many aspects of
these models can be understood using the tools of equilibrium statistical
mechanics. Using this reformulation, we show that generative diffusion models
undergo second-order phase transitions corresponding to symmetry breaking
phenomena. We argue that this lead to a form of instability that lies at the
heart of their generative capabilities and that can be described by a set of
mean field critical exponents. We conclude by analyzing recent work connecting
diffusion models and associative memory networks in view of the thermodynamic
formulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17468">Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yang Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1">Dezhong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Peng Hu</a></p>
<p>Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17471">Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End Collaboration. (arXiv:2310.17471v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhiheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Howard H. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chenyuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Junshen Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Sihui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1">Tony Q. S. Quek</a></p>
<p>Future wireless communication networks are in a position to move beyond
data-centric, device-oriented connectivity and offer intelligent, immersive
experiences based on task-oriented connections, especially in the context of
the thriving development of pre-trained foundation models (PFM) and the
evolving vision of 6G native artificial intelligence (AI). Therefore,
redefining modes of collaboration between devices and servers and constructing
native intelligence libraries become critically important in 6G. In this paper,
we analyze the challenges of achieving 6G native AI from the perspectives of
data, intelligence, and networks. Then, we propose a 6G native AI framework
based on foundation models, provide a customization approach for intent-aware
PFM, present a construction of a task-oriented AI toolkit, and outline a novel
cloud-edge-end collaboration paradigm. As a practical use case, we apply this
framework for orchestration, achieving the maximum sum rate within a wireless
communication system, and presenting preliminary evaluation results. Finally,
we outline research directions for achieving native AI in 6G.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17477">Secure short-term load forecasting for smart grids with transformer-based federated learning. (arXiv:2310.17477v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sievers_J/0/1/0/all/0/1">Jonas Sievers</a>, <a href="http://arxiv.org/find/cs/1/au:+Blank_T/0/1/0/all/0/1">Thomas Blank</a></p>
<p>Electricity load forecasting is an essential task within smart grids to
assist demand and supply balance. While advanced deep learning models require
large amounts of high-resolution data for accurate short-term load predictions,
fine-grained load profiles can expose users' electricity consumption behaviors,
which raises privacy and security concerns. One solution to improve data
privacy is federated learning, where models are trained locally on private
data, and only the trained model parameters are merged and updated on a global
server. Therefore, this paper presents a novel transformer-based deep learning
approach with federated learning for short-term electricity load prediction. To
evaluate our results, we benchmark our federated learning architecture against
central and local learning and compare the performance of our model to long
short-term memory models and convolutional neural networks. Our simulations are
based on a dataset from a German university campus and show that
transformer-based forecasting is a promising alternative to state-of-the-art
models within federated learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17485">Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach. (arXiv:2310.17485v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mak_S/0/1/0/all/0/1">Stephen Mak</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1">Tim Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostroumov_M/0/1/0/all/0/1">Michael Ostroumov</a>, <a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1">Alexandra Brintrup</a></p>
<p>Collaborative vehicle routing occurs when carriers collaborate through
sharing their transportation requests and performing transportation requests on
behalf of each other. This achieves economies of scale, thus reducing cost,
greenhouse gas emissions and road congestion. But which carrier should partner
with whom, and how much should each carrier be compensated? Traditional game
theoretic solution concepts are expensive to calculate as the characteristic
function scales exponentially with the number of agents. This would require
solving the vehicle routing problem (NP-hard) an exponential number of times.
We therefore propose to model this problem as a coalitional bargaining game
solved using deep multi-agent reinforcement learning, where - crucially -
agents are not given access to the characteristic function. Instead, we
implicitly reason about the characteristic function; thus, when deployed in
production, we only need to evaluate the expensive post-collaboration vehicle
routing problem once. Our contribution is that we are the first to consider
both the route allocation problem and gain sharing problem simultaneously -
without access to the expensive characteristic function. Through decentralised
machine learning, our agents bargain with each other and agree to outcomes that
correlate well with the Shapley value - a fair profit allocation mechanism.
Importantly, we are able to achieve a reduction in run-time of 88%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17489">Bias in Evaluation Processes: An Optimization-Based Model. (arXiv:2310.17489v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1">L. Elisa Celis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Amit Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Anay Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1">Nisheeth K. Vishnoi</a></p>
<p>Biases with respect to socially-salient attributes of individuals have been
well documented in evaluation processes used in settings such as admissions and
hiring. We view such an evaluation process as a transformation of a
distribution of the true utility of an individual for a task to an observed
distribution and model it as a solution to a loss minimization problem subject
to an information constraint. Our model has two parameters that have been
identified as factors leading to biases: the resource-information trade-off
parameter in the information constraint and the risk-averseness parameter in
the loss function. We characterize the distributions that arise from our model
and study the effect of the parameters on the observed distribution. The
outputs of our model enrich the class of distributions that can be used to
capture variation across groups in the observed evaluations. We empirically
validate our model by fitting real-world datasets and use it to study the
effect of interventions in a downstream selection task. These results
contribute to an understanding of the emergence of bias in evaluation processes
and provide tools to guide the deployment of interventions to mitigate biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17491">FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Terence Jie Chua</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1">Kwok-Yan Lam</a></p>
<p>The emergence of foundation models, including language and vision models, has
reshaped AI's landscape, offering capabilities across various applications.
Deploying and fine-tuning these large models, like GPT-3 and BERT, presents
challenges, especially in the current foundation model era. We introduce
Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning
(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we
expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses
adapters, emulators, and PEFT for federated model tuning, enhancing model
privacy and memory efficiency. Adapters adjust pre-trained models, while
emulators give a compact representation of original models, addressing both
privacy and efficiency. Adaptable to various neural networks, our approach also
uses deep reinforcement learning for hyper-parameter optimization. We tested
FedPEAT in a unique scenario with a server participating in collaborative
federated tuning, showcasing its potential in tackling foundation model
challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17492">Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach. (arXiv:2310.17492v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Terence Jie Chua</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a></p>
<p>The efficient deployment and fine-tuning of foundation models are pivotal in
contemporary artificial intelligence. In this study, we present a
groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation
models, specifically designed to enhance local task performance on user
equipment (UE). Central to our approach is the innovative Emulator-Adapter
architecture, segmenting the foundation model into two cohesive modules. This
design not only conserves computational resources but also ensures adaptability
and fine-tuning efficiency for downstream tasks. Additionally, we introduce an
advanced resource allocation mechanism that is fine-tuned to the needs of the
Emulator-Adapter structure in decentralized settings. To address the challenges
presented by this system, we employ a hybrid multi-agent Deep Reinforcement
Learning (DRL) strategy, adept at handling mixed discrete-continuous action
spaces, ensuring dynamic and optimal resource allocations. Our comprehensive
simulations and validations underscore the practical viability of our approach,
demonstrating its robustness, efficiency, and scalability. Collectively, this
work offers a fresh perspective on deploying foundation models and balancing
computational efficiency with task proficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17496">Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Si_N/0/1/0/all/0/1">Nian Si</a></p>
<p>In modern recommendation systems, the standard pipeline involves training
machine learning models on historical data to predict user behaviors and
improve recommendations continuously. However, these data training loops can
introduce interference in A/B tests, where data generated by control and
treatment algorithms, potentially with different distributions, are combined.
To address these challenges, we introduce a novel approach called weighted
training. This approach entails training a model to predict the probability of
each data point appearing in either the treatment or control data and
subsequently applying weighted losses during model training. We demonstrate
that this approach achieves the least variance among all estimators without
causing shifts in the training distributions. Through simulation studies, we
demonstrate the lower bias and variance of our approach compared to other
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17498">CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1">Zhen Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zidi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Backdoor attack is a common threat to deep neural networks. During testing,
samples embedded with a backdoor trigger will be misclassified as an
adversarial target by a backdoored model, while samples without the backdoor
trigger will be correctly classified. In this paper, we present the first
certified backdoor detector (CBD), which is based on a novel, adjustable
conformal prediction scheme based on our proposed statistic local dominant
probability. For any classifier under inspection, CBD provides 1) a detection
inference, 2) the condition under which the attacks are guaranteed to be
detectable for the same classification domain, and 3) a probabilistic upper
bound for the false positive rate. Our theoretical results show that attacks
with triggers that are more resilient to test-time noise and have smaller
perturbation magnitudes are more likely to be detected with guarantees.
Moreover, we conduct extensive experiments on four benchmark datasets
considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves
comparable or even higher detection accuracy than state-of-the-art detectors,
and it in addition provides detection certification. Notably, for backdoor
attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which
achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%
(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true
positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and
TinyImageNet, respectively, with low false positive rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17499">The IMS Toucan System for the Blizzard Challenge 2023. (arXiv:2310.17499v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1">Florian Lux</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1">Julia Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1">Sarina Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bott_T/0/1/0/all/0/1">Thomas Bott</a>, <a href="http://arxiv.org/find/cs/1/au:+Schauffler_N/0/1/0/all/0/1">Nadja Schauffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1">Pavel Denisov</a>, <a href="http://arxiv.org/find/cs/1/au:+Schweitzer_A/0/1/0/all/0/1">Antje Schweitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1">Ngoc Thang Vu</a></p>
<p>For our contribution to the Blizzard Challenge 2023, we improved on the
system we submitted to the Blizzard Challenge 2021. Our approach entails a
rule-based text-to-phoneme processing system that includes rule-based
disambiguation of homographs in the French language. It then transforms the
phonemes to spectrograms as intermediate representations using a fast and
efficient non-autoregressive synthesis architecture based on Conformer and
Glow. A GAN based neural vocoder that combines recent state-of-the-art
approaches converts the spectrogram to the final wave. We carefully designed
the data processing, training, and inference procedures for the challenge data.
Our system identifier is G. Open source code and demo are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17502">Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions. (arXiv:2310.17502v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1">Florian Lux</a>, <a href="http://arxiv.org/find/cs/1/au:+Tilli_P/0/1/0/all/0/1">Pascal Tilli</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1">Sarina Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1">Ngoc Thang Vu</a></p>
<p>Customizing voice and speaking style in a speech synthesis system with
intuitive and fine-grained controls is challenging, given that little data with
appropriate labels is available. Furthermore, editing an existing human's voice
also comes with ethical concerns. In this paper, we propose a method to
generate artificial speaker embeddings that cannot be linked to a real human
while offering intuitive and fine-grained control over the voice and speaking
style of the embeddings, without requiring any labels for speaker or style. The
artificial and controllable embeddings can be fed to a speech synthesis system,
conditioned on embeddings of real humans during training, without sacrificing
privacy during inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17513">The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yuchen Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kangwook Lee</a></p>
<p>Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17526">Can large language models replace humans in the systematic review process? Evaluating GPT-4&#x27;s efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khraisha_Q/0/1/0/all/0/1">Qusai Khraisha</a>, <a href="http://arxiv.org/find/cs/1/au:+Put_S/0/1/0/all/0/1">Sophie Put</a>, <a href="http://arxiv.org/find/cs/1/au:+Kappenberg_J/0/1/0/all/0/1">Johanna Kappenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Warraitch_A/0/1/0/all/0/1">Azza Warraitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_K/0/1/0/all/0/1">Kristin Hadfield</a></p>
<p>Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4's capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17530">Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1">Laura Cabello</a>, <a href="http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1">Emanuele Bugliarello</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1">Stephanie Brandl</a>, <a href="http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1">Desmond Elliott</a></p>
<p>Pretrained machine learning models are known to perpetuate and even amplify
existing biases in data, which can result in unfair outcomes that ultimately
impact user experience. Therefore, it is crucial to understand the mechanisms
behind those prejudicial biases to ensure that model performance does not
result in discriminatory behaviour toward certain groups or populations. In
this work, we define gender bias as our case study. We quantify bias
amplification in pretraining and after fine-tuning on three families of
vision-and-language models. We investigate the connection, if any, between the
two learning stages, and evaluate how bias amplification reflects on model
performance. Overall, we find that bias amplification in pretraining and after
fine-tuning are independent. We then examine the effect of continued
pretraining on gender-neutral data, finding that this reduces group
disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without
significantly compromising task performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17531">Learning Regularized Graphon Mean-Field Games with Unknown Graphons. (arXiv:2310.17531v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fengzhuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1">Vincent Y. F. Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a></p>
<p>We design and analyze reinforcement learning algorithms for Graphon
Mean-Field Games (GMFGs). In contrast to previous works that require the
precise values of the graphons, we aim to learn the Nash Equilibrium (NE) of
the regularized GMFGs when the graphons are unknown. Our contributions are
threefold. First, we propose the Proximal Policy Optimization for GMFG
(GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$
after $T$ iterations with an estimation oracle, improving on a previous work by
Xie et al. (ICML, 2021). Second, using kernel embedding of distributions, we
design efficient algorithms to estimate the transition kernels, reward
functions, and graphons from sampled agents. Convergence rates are then derived
when the positions of the agents are either known or unknown. Results for the
combination of the optimization algorithm GMFG-PPO and the estimation algorithm
are then provided. These algorithms are the first specifically designed for
learning graphons from sampled agents. Finally, the efficacy of the proposed
algorithms are corroborated through simulations. These simulations demonstrate
that learning the unknown graphons reduces the exploitability effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17534">SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suya_F/0/1/0/all/0/1">Fnu Suya</a>, <a href="http://arxiv.org/find/cs/1/au:+Suri_A/0/1/0/all/0/1">Anshuman Suri</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tingwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Jingtao Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1">David Evans</a></p>
<p>Numerous works study black-box attacks on image classifiers. However, these
works make different assumptions on the adversary's knowledge and current
literature lacks a cohesive organization centered around the threat model. To
systematize knowledge in this area, we propose a taxonomy over the threat space
spanning the axes of feedback granularity, the access of interactive queries,
and the quality and quantity of the auxiliary data available to the attacker.
Our new taxonomy provides three key insights. 1) Despite extensive literature,
numerous under-explored threat spaces exist, which cannot be trivially solved
by adapting techniques from well-explored settings. We demonstrate this by
establishing a new state-of-the-art in the less-studied setting of access to
top-k confidence scores by adapting techniques from well-explored settings of
accessing the complete confidence vector, but show how it still falls short of
the more restrictive setting that only obtains the prediction label,
highlighting the need for more research. 2) Identification the threat model of
different attacks uncovers stronger baselines that challenge prior
state-of-the-art claims. We demonstrate this by enhancing an initially weaker
baseline (under interactive query access) via surrogate models, effectively
overturning claims in the respective paper. 3) Our taxonomy reveals
interactions between attacker knowledge that connect well to related areas,
such as model inversion and extraction attacks. We discuss how advances in
other areas can enable potentially stronger black-box attacks. Finally, we
emphasize the need for a more realistic assessment of attack success by
factoring in local attack runtime. This approach reveals the potential for
certain attacks to achieve notably higher success rates and the need to
evaluate attacks in diverse and harder settings, highlighting the need for
better selection criteria.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17537">Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity. (arXiv:2310.17537v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jaedong Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1">Zhang-Wei Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Eric Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Boopathy_A/0/1/0/all/0/1">Akhilan Boopathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiete_I/0/1/0/all/0/1">Ila Fiete</a></p>
<p>Deep reinforcement learning methods exhibit impressive performance on a range
of tasks but still struggle on hard exploration tasks in large environments
with sparse rewards. To address this, intrinsic rewards can be generated using
forward model prediction errors that decrease as the environment becomes known,
and incentivize an agent to explore novel states. While prediction-based
intrinsic rewards can help agents solve hard exploration tasks, they can suffer
from catastrophic forgetting and actually increase at visited states. We first
examine the conditions and causes of catastrophic forgetting in grid world
environments. We then propose a new method FARCuriosity, inspired by how humans
and animals learn. The method depends on fragmentation and recall: an agent
fragments an environment based on surprisal, and uses different local curiosity
modules (prediction-based intrinsic reward functions) for each fragment so that
modules are not trained on the entire environment. At each fragmentation event,
the agent stores the current module in long-term memory (LTM) and either
initializes a new module or recalls a previously stored module based on its
match with the current state. With fragmentation and recall, FARCuriosity
achieves less forgetting and better overall performance in games with varied
and heterogeneous environments in the Atari benchmark suite of tasks. Thus,
this work highlights the problem of catastrophic forgetting in prediction-based
curiosity methods and proposes a solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17538">Little Exploration is All You Need. (arXiv:2310.17538v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Henry H.H. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiaming Lu</a></p>
<p>The prevailing principle of "Optimism in the Face of Uncertainty" advocates
for the incorporation of an exploration bonus, generally assumed to be
proportional to the inverse square root of the visit count ($1/\sqrt{n}$),
where $n$ is the number of visits to a particular state-action pair. This
approach, however, exclusively focuses on "uncertainty," neglecting the
inherent "difficulty" of different options. To address this gap, we introduce a
novel modification of standard UCB algorithm in the multi-armed bandit problem,
proposing an adjusted bonus term of $1/n^\tau$, where $\tau &gt; 1/2$, that
accounts for task difficulty. Our proposed algorithm, denoted as UCB$^\tau$, is
substantiated through comprehensive regret and risk analyses, confirming its
theoretical robustness. Comparative evaluations with standard UCB and Thompson
Sampling algorithms on synthetic datasets demonstrate that UCB$^\tau$ not only
outperforms in efficacy but also exhibits lower risk across various
environmental conditions and hyperparameter settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17540">EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving. (arXiv:2310.17540v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jier Chen</a></p>
<p>Forecasting vehicular motions in autonomous driving requires a deep
understanding of agent interactions and the preservation of motion equivariance
under Euclidean geometric transformations. Traditional models often lack the
sophistication needed to handle the intricate dynamics inherent to autonomous
vehicles and the interaction relationships among agents in the scene. As a
result, these models have a lower model capacity, which then leads to higher
prediction errors and lower training efficiency. In our research, we employ
EqMotion, a leading equivariant particle, and human prediction model that also
accounts for invariant agent interactions, for the task of multi-agent vehicle
motion forecasting. In addition, we use a multi-modal prediction mechanism to
account for multiple possible future paths in a probabilistic manner. By
leveraging EqMotion, our model achieves state-of-the-art (SOTA) performance
with fewer parameters (1.2 million) and a significantly reduced training time
(less than 2 hours).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17544">Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tumay_A/0/1/0/all/0/1">Aysin Tumay</a>, <a href="http://arxiv.org/find/cs/1/au:+Aydin_M/0/1/0/all/0/1">Mustafa E. Aydin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozat_S/0/1/0/all/0/1">Suleyman S. Kozat</a></p>
<p>We study a novel ensemble approach for feature selection based on
hierarchical stacking in cases of non-stationarity and limited number of
samples with large number of features. Our approach exploits the co-dependency
between features using a hierarchical structure. Initially, a machine learning
model is trained using a subset of features, and then the model's output is
updated using another algorithm with the remaining features to minimize the
target loss. This hierarchical structure allows for flexible depth and feature
selection. By exploiting feature co-dependency hierarchically, our proposed
approach overcomes the limitations of traditional feature selection methods and
feature importance scores. The effectiveness of the approach is demonstrated on
synthetic and real-life datasets, indicating improved performance with
scalability and stability compared to the traditional methods and
state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17550">Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Andi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1">Mycal Tucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenny_E/0/1/0/all/0/1">Eoin Kenny</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaslavsky_N/0/1/0/all/0/1">Noga Zaslavsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1">Julie Shah</a></p>
<p>Neural networks often learn task-specific latent representations that fail to
generalize to novel settings or tasks. Conversely, humans learn discrete
representations (i.e., concepts or words) at a variety of abstraction levels
(e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based
on task. Inspired by this, we train neural models to generate a spectrum of
discrete representations, and control the complexity of the representations
(roughly, how many bits are allocated for encoding inputs) by tuning the
entropy of the distribution over representations. In finetuning experiments,
using only a small number of labeled examples for a new task, we show that (1)
tuning the representation to a task-appropriate complexity level supports the
highest finetuning performance, and (2) in a human-participant study, users
were able to identify the appropriate complexity level for a downstream task
using visualizations of discrete representations. Our results indicate a
promising direction for rapid model finetuning by leveraging human insight.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17552">Model-Based Runtime Monitoring with Interactive Imitation Learning. (arXiv:2310.17552v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huihan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dass_S/0/1/0/all/0/1">Shivin Dass</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuke Zhu</a></p>
<p>Robot learning methods have recently made great strides, but generalization
and robustness challenges still hinder their widespread deployment. Failing to
detect and address potential failures renders state-of-the-art learning systems
not combat-ready for high-stakes tasks. Recent advances in interactive
imitation learning have presented a promising framework for human-robot
teaming, enabling the robots to operate safely and continually improve their
performances over long-term deployments. Nonetheless, existing methods
typically require constant human supervision and preemptive feedback, limiting
their practicality in realistic domains. This work aims to endow a robot with
the ability to monitor and detect errors during task execution. We introduce a
model-based runtime monitoring algorithm that learns from deployment data to
detect system anomalies and anticipate failures. Unlike prior work that cannot
foresee future failures or requires failure experiences for training, our
method learns a latent-space dynamics model and a failure classifier, enabling
our method to simulate future action outcomes and detect out-of-distribution
and high-risk states preemptively. We train our method within an interactive
imitation learning framework, where it continually updates the model from the
experiences of the human-robot team collected using trustworthy deployments.
Consequently, our method reduces the human workload needed over time while
ensuring reliable task execution. Our method outperforms the baselines across
system-level and unit-test metrics, with 23% and 40% higher success rates in
simulation and on physical hardware, respectively. More information at
https://ut-austin-rpl.github.io/sirius-runtime-monitor/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17555">Interactive Robot Learning from Verbal Correction. (arXiv:2310.17555v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huihan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Alice Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuke Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1">Adith Swaminathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1">Andrey Kolobov</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Ching-An Cheng</a></p>
<p>The ability to learn and refine behavior after deployment has become ever
more important for robots as we design them to operate in unstructured
environments like households. In this work, we design a new learning system
based on large language model (LLM), OLAF, that allows everyday users to teach
a robot using verbal corrections when the robot makes mistakes, e.g., by saying
"Stop what you're doing. You should move closer to the cup." A key feature of
OLAF is its ability to update the robot's visuomotor neural policy based on the
verbal feedback to avoid repeating mistakes in the future. This is in contrast
to existing LLM-based robotic systems, which only follow verbal commands or
corrections but not learn from them. We demonstrate the efficacy of our design
in experiments where a user teaches a robot to perform long-horizon
manipulation tasks both in simulation and on physical hardware, achieving on
average 20.0% improvement in policy success rate. Videos and more results are
at https://ut-austin-rpl.github.io/olaf/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17556">Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient Descent. (arXiv:2310.17556v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yixiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a></p>
<p>We propose a new algorithm for efficiently solving the damped Fisher matrix
in large-scale scenarios where the number of parameters significantly exceeds
the number of available samples. This problem is fundamental for natural
gradient descent and stochastic reconfiguration. Our algorithm is based on
Cholesky decomposition and is generally applicable. Benchmark results show that
the algorithm is significantly faster than existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17558">Towards Matching Phones and Speech Representations. (arXiv:2310.17558v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Gene-Ping Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a></p>
<p>Learning phone types from phone instances has been a long-standing problem,
while still being open. In this work, we revisit this problem in the context of
self-supervised learning, and pose it as the problem of matching cluster
centroids to phone embeddings. We study two key properties that enable
matching, namely, whether cluster centroids of self-supervised representations
reduce the variability of phone instances and respect the relationship among
phones. We then use the matching result to produce pseudo-labels and introduce
a new loss function for improving self-supervised representations. Our
experiments show that the matching result captures the relationship among
phones. Training the new loss function jointly with the regular self-supervised
losses, such as APC and CPC, significantly improves the downstream phone
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17561">Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eisenmann_L/0/1/0/all/0/1">Lukas Eisenmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Monfared_Z/0/1/0/all/0/1">Zahra Monfared</a>, <a href="http://arxiv.org/find/cs/1/au:+Goring_N/0/1/0/all/0/1">Niclas Alexander G&#xf6;ring</a>, <a href="http://arxiv.org/find/cs/1/au:+Durstewitz_D/0/1/0/all/0/1">Daniel Durstewitz</a></p>
<p>Recurrent neural networks (RNNs) are popular machine learning tools for
modeling and forecasting sequential data and for inferring dynamical systems
(DS) from observed time series. Concepts from DS theory (DST) have variously
been used to further our understanding of both, how trained RNNs solve complex
tasks, and the training process itself. Bifurcations are particularly important
phenomena in DS, including RNNs, that refer to topological (qualitative)
changes in a system's dynamical behavior as one or more of its parameters are
varied. Knowing the bifurcation structure of an RNN will thus allow to deduce
many of its computational and dynamical properties, like its sensitivity to
parameter variations or its behavior during training. In particular,
bifurcations may account for sudden loss jumps observed in RNN training that
could severely impede the training process. Here we first mathematically prove
for a particular class of ReLU-based RNNs that certain bifurcations are indeed
associated with loss gradients tending toward infinity or zero. We then
introduce a novel heuristic algorithm for detecting all fixed points and
k-cycles in ReLU-based RNNs and their existence and stability regions, hence
bifurcation manifolds in parameter space. In contrast to previous numerical
algorithms for finding fixed points and common continuation methods, our
algorithm provides exact results and returns fixed points and cycles up to high
orders with surprisingly good scaling behavior. We exemplify the algorithm on
the analysis of the training process of RNNs, and find that the recently
introduced technique of generalized teacher forcing completely avoids certain
types of bifurcations in training. Thus, besides facilitating the DST analysis
of trained RNNs, our algorithm provides a powerful instrument for analyzing the
training process itself.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.01742">Online Estimation and Community Detection of Network Point Processes for Event Streams. (arXiv:2009.01742v3 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1">Guanhua Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ward_O/0/1/0/all/0/1">Owen G. Ward</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tian Zheng</a></p>
<p>A common goal in network modeling is to uncover the latent community
structure present among nodes. For many real-world networks, the true
connections consist of events arriving as streams, which are then aggregated to
form edges, ignoring the dynamic temporal component. A natural way to take
account of these temporal dynamics of interactions is to use point processes as
the foundation of network models for community detection. Computational
complexity hampers the scalability of such approaches to large sparse networks.
To circumvent this challenge, we propose a fast online variational inference
algorithm for estimating the latent structure underlying dynamic event arrivals
on a network, using continuous-time point process latent network models. We
describe this procedure for networks models capturing community structure. This
structure can be learned as new events are observed on the network, updating
the inferred community assignments. We investigate the theoretical properties
of such an inference scheme, and provide regret bounds on the loss function of
this procedure. The proposed inference procedure is then thoroughly compared,
using both simulation studies and real data, to non-online variants. We
demonstrate that online inference can obtain comparable performance, in terms
of community recovery, to non-online variants, while realising computational
gains. Our proposed inference framework can also be readily modified to
incorporate other popular network structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.11959">Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1">Yury Gorishniy</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1">Ivan Rubachev</a>, <a href="http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1">Valentin Khrulkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a></p>
<p>The existing literature on deep learning for tabular data proposes a wide
range of novel architectures and reports competitive results on various
datasets. However, the proposed models are usually not properly compared to
each other and existing works often use different benchmarks and experiment
protocols. As a result, it is unclear for both researchers and practitioners
what models perform best. Additionally, the field still lacks effective
baselines, that is, the easy-to-use models that provide competitive performance
across different problems.
</p>
<p>In this work, we perform an overview of the main families of DL architectures
for tabular data and raise the bar of baselines in tabular DL by identifying
two simple and powerful deep architectures. The first one is a ResNet-like
architecture which turns out to be a strong baseline that is often missing in
prior works. The second model is our simple adaptation of the Transformer
architecture for tabular data, which outperforms other solutions on most tasks.
Both models are compared to many existing architectures on a diverse set of
tasks under the same training and tuning protocols. We also compare the best DL
models with Gradient Boosted Decision Trees and conclude that there is still no
universally superior solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.07420">Optimal Scoring Rule Design under Partial Knowledge. (arXiv:2107.07420v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fang-Yi Yu</a></p>
<p>This paper studies the design of optimal proper scoring rules when the
principal has partial knowledge of an agent's signal distribution. Recent work
characterizes the proper scoring rules that maximize the increase of an agent's
payoff when the agent chooses to access a costly signal to refine a posterior
belief from her prior prediction, under the assumption that the agent's signal
distribution is fully known to the principal. In our setting, the principal
only knows about a set of distributions where the agent's signal distribution
belongs. We formulate the scoring rule design problem as a max-min optimization
that maximizes the worst-case increase in payoff across the set of
distributions.
</p>
<p>We propose an efficient algorithm to compute an optimal scoring rule when the
set of distributions is finite, and devise a fully polynomial-time
approximation scheme that accommodates various infinite sets of distributions.
We further remark that widely used scoring rules, such as the quadratic and log
rules, as well as previously identified optimal scoring rules under full
knowledge, can be far from optimal in our partial knowledge settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.11419">Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits. (arXiv:2107.11419v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Komiyama_J/0/1/0/all/0/1">Junpei Komiyama</a>, <a href="http://arxiv.org/find/stat/1/au:+Fouche_E/0/1/0/all/0/1">Edouard Fouch&#xe9;</a>, <a href="http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1">Junya Honda</a></p>
<p>We consider nonstationary multi-armed bandit problems where the model
parameters of the arms change over time. We introduce the adaptive resetting
bandit (ADR-bandit), a bandit algorithm class that leverages adaptive windowing
techniques from literature on data streams. We first provide new guarantees on
the quality of estimators resulting from adaptive windowing techniques, which
are of independent interest. Furthermore, we conduct a finite-time analysis of
ADR-bandit in two typical environments: an abrupt environment where changes
occur instantaneously and a gradual environment where changes occur
progressively. We demonstrate that ADR-bandit has nearly optimal performance
when abrupt or gradual changes occur in a coordinated manner that we call
global changes. We demonstrate that forced exploration is unnecessary when we
assume such global changes. Unlike the existing nonstationary bandit
algorithms, ADR-bandit has optimal performance in stationary environments as
well as nonstationary environments with global changes. Our experiments show
that the proposed algorithms outperform the existing approaches in synthetic
and real-world environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.14251">Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingbo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengmeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junfan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Accurate inference of fine-grained traffic flow from coarse-grained one is an
emerging yet crucial problem, which can help greatly reduce the number of the
required traffic monitoring sensors for cost savings. In this work, we notice
that traffic flow has a high correlation with road network, which was either
completely ignored or simply treated as an external factor in previous works.
To facilitate this problem, we propose a novel Road-Aware Traffic Flow
Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks
to fully learn the road-aware spatial distribution of fine-grained traffic
flow. Specifically, a multi-directional 1D convolutional layer is first
introduced to extract the semantic feature of the road network. Subsequently,
we incorporate the road network feature and coarse-grained flow feature to
regularize the short-range spatial distribution modeling of road-relative
traffic flow. Furthermore, we take the road network feature as a query to
capture the long-range spatial distribution of traffic flow with a transformer
architecture. Benefiting from the road-aware inference mechanism, our method
can generate high-quality fine-grained traffic flow maps. Extensive experiments
on three real-world datasets show that the proposed RATFM outperforms
state-of-the-art models under various scenarios. Our code and datasets are
released at {\url{https://github.com/luimoli/RATFM}}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.12458">Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Avalos_R/0/1/0/all/0/1">Rapha&#xeb;l Avalos</a>, <a href="http://arxiv.org/find/cs/1/au:+Reymond_M/0/1/0/all/0/1">Mathieu Reymond</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1">Ann Now&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1">Diederik M. Roijers</a></p>
<p>Many recent successful off-policy multi-agent reinforcement learning (MARL)
algorithms for cooperative partially observable environments focus on finding
factorized value functions, leading to convoluted network structures. Building
on the structure of independent Q-learners, our LAN algorithm takes a radically
different approach, leveraging a dueling architecture to learn for each agent a
decentralized best-response policies via individual advantage functions. The
learning is stabilized by a centralized critic whose primary objective is to
reduce the moving target problem of the individual advantages. The critic,
whose network's size is independent of the number of agents, is cast aside
after learning. Evaluation on the StarCraft II multi-agent challenge benchmark
shows that LAN reaches state-of-the-art performance and is highly scalable with
respect to the number of agents, opening up a promising alternative direction
for MARL research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.05556">On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1">Yury Gorishniy</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1">Ivan Rubachev</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a></p>
<p>Recently, Transformer-like deep architectures have shown strong performance
on tabular data problems. Unlike traditional models, e.g., MLP, these
architectures map scalar values of numerical features to high-dimensional
embeddings before mixing them in the main backbone. In this work, we argue that
embeddings for numerical features are an underexplored degree of freedom in
tabular DL, which allows constructing more powerful DL models and competing
with GBDT on some traditionally GBDT-friendly benchmarks. We start by
describing two conceptually different approaches to building embedding modules:
the first one is based on a piecewise linear encoding of scalar values, and the
second one utilizes periodic activations. Then, we empirically demonstrate that
these two approaches can lead to significant performance boosts compared to the
embeddings based on conventional blocks such as linear layers and ReLU
activations. Importantly, we also show that embedding numerical features is
beneficial for many backbones, not only for Transformers. Specifically, after
proper embeddings, simple MLP-like models can perform on par with the
attention-based architectures. Overall, we highlight embeddings for numerical
features as an important design aspect with good potential for further
improvements in tabular DL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.09249">Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1">Ling-Yu Duan</a></p>
<p>Federated Learning (FL) is an emerging distributed learning paradigm under
privacy constraint. Data heterogeneity is one of the main challenges in FL,
which results in slow convergence and degraded performance. Most existing
approaches only tackle the heterogeneity challenge by restricting the local
model update in client, ignoring the performance drop caused by direct global
model aggregation. Instead, we propose a data-free knowledge distillation
method to fine-tune the global model in the server (FedFTG), which relieves the
issue of direct model aggregation. Concretely, FedFTG explores the input space
of local models through a generator, and uses it to transfer the knowledge from
local models to the global model. Besides, we propose a hard sample mining
scheme to achieve effective knowledge distillation throughout the training. In
addition, we develop customized label sampling and class-level ensemble to
derive maximum utilization of knowledge, which implicitly mitigates the
distribution discrepancy across clients. Extensive experiments show that our
FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and
can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and
SCAFFOLD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15403">Neural Optimal Transport with General Cost Functionals. (arXiv:2205.15403v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Asadulaev_A/0/1/0/all/0/1">Arip Asadulaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1">Alexander Korotin</a>, <a href="http://arxiv.org/find/cs/1/au:+Egiazarian_V/0/1/0/all/0/1">Vage Egiazarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Mokrov_P/0/1/0/all/0/1">Petr Mokrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1">Evgeny Burnaev</a></p>
<p>We introduce a novel neural network-based algorithm to compute optimal
transport (OT) plans for general cost functionals. In contrast to common
Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more
flexibility and allow using auxiliary information, such as class labels, to
construct the required transport map. Existing methods for general costs are
discrete and have limitations in practice, i.e. they do not provide an
out-of-sample estimation. We address the challenge of designing a continuous OT
approach for general costs that generalizes to new data points in
high-dimensional spaces, such as images. Additionally, we provide the
theoretical error analysis for our recovered transport plans. As an
application, we construct a cost functional to map data distributions while
preserving the class-wise structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05794">Characterizing the Implicit Bias of Regularized SGD in Rank Minimization. (arXiv:2206.05794v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1">Tomer Galanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Siegel_Z/0/1/0/all/0/1">Zachary S. Siegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1">Aparna Gupte</a>, <a href="http://arxiv.org/find/cs/1/au:+Poggio_T/0/1/0/all/0/1">Tomaso Poggio</a></p>
<p>We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank
weight matrices when training deep neural networks. Our results show that
training neural networks with mini-batch SGD and weight decay causes a bias
towards rank minimization over the weight matrices. Specifically, we show, both
theoretically and empirically, that this bias is more pronounced when using
smaller batch sizes, higher learning rates, or increased weight decay.
Additionally, we predict and observe empirically that weight decay is necessary
to achieve this bias. Unlike previous literature, our analysis does not rely on
assumptions about the data, convergence, or optimality of the weight matrices
and applies to a wide range of neural network architectures of any width or
depth. Finally, we empirically investigate the connection between this bias and
generalization, finding that it has a marginal effect on generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05869">On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms. (arXiv:2206.05869v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Trang H. Tran</a></p>
<p>Stochastic gradient descent (SGD) algorithm is the method of choice in many
machine learning tasks thanks to its scalability and efficiency in dealing with
large-scale problems. In this paper, we focus on the shuffling version of SGD
which matches the mainstream practical heuristics. We show the convergence to a
global solution of shuffling SGD for a class of non-convex functions under
over-parameterized settings. Our analysis employs more relaxed non-convex
assumptions than previous literature. Nevertheless, we maintain the desired
computational complexity as shuffling SGD has achieved in the general convex
setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.04306">Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach. (arXiv:2207.04306v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belkhouja_T/0/1/0/all/0/1">Taha Belkhouja</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Doppa_J/0/1/0/all/0/1">Janardhan Rao Doppa</a></p>
<p>Safe deployment of time-series classifiers for real-world applications relies
on the ability to detect the data which is not generated from the same
distribution as training data. This task is referred to as out-of-distribution
(OOD) detection. We consider the novel problem of OOD detection for the
time-series domain. We discuss the unique challenges posed by time-series data
and explain why prior methods from the image domain will perform poorly.
Motivated by these challenges, this paper proposes a novel {\em Seasonal Ratio
Scoring (SRS)} approach. SRS consists of three key algorithmic steps. First,
each input is decomposed into class-wise semantic component and remainder.
Second, this decomposition is employed to estimate the class-wise conditional
likelihoods of the input and remainder using deep generative models. The
seasonal ratio score is computed from these estimates. Third, a threshold
interval is identified from the in-distribution data to detect OOD examples.
Experiments on diverse real-world benchmarks demonstrate that the SRS method is
well-suited for time-series OOD detection when compared to baseline methods.
Open-source code for SRS method is provided at
https://github.com/tahabelkhouja/SRS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.06589">Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungeun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kijung Yoon</a></p>
<p>Graph neural networks (GNNs) have become compelling models designed to
perform learning and inference on graph-structured data. However, little work
has been done to understand the fundamental limitations of GNNs for scaling to
larger graphs and generalizing to out-of-distribution (OOD) inputs. In this
paper, we use a random graph generator to systematically investigate how the
graph size and structural properties affect the predictive performance of GNNs.
We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of
multiple node update functions can improve the generalization performance of
GNNs when dealing with graphs of multimodal degree distributions. Accordingly,
we propose a multi-module GNN framework that allows the network to adapt
flexibly to new graphs by generalizing a single canonical nonlinear
transformation over aggregated inputs. Our results show that the multi-module
GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.09493">A framework for benchmarking clustering algorithms. (arXiv:2209.09493v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gagolewski_M/0/1/0/all/0/1">Marek Gagolewski</a></p>
<p>The evaluation of clustering algorithms can involve running them on a variety
of benchmark problems, and comparing their outputs to the reference,
ground-truth groupings provided by experts. Unfortunately, many research papers
and graduate theses consider only a small number of datasets. Also, the fact
that there can be many equally valid ways to cluster a given problem set is
rarely taken into account. In order to overcome these limitations, we have
developed a framework whose aim is to introduce a consistent methodology for
testing clustering algorithms. Furthermore, we have aggregated, polished, and
standardised many clustering benchmark dataset collections referred to across
the machine learning and data mining literature, and included new datasets of
different dimensionalities, sizes, and cluster types. An interactive datasets
explorer, the documentation of the Python API, a description of the ways to
interact with the framework from other programming languages such as R or
MATLAB, and other details are all provided at
&lt;https://clustering-benchmarks.gagolewski.com&gt;.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15543">Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty. (arXiv:2209.15543v3 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Brown_S/0/1/0/all/0/1">Stephen Brown</a>, <a href="http://arxiv.org/find/physics/1/au:+Rodi_W/0/1/0/all/0/1">William L. Rodi</a>, <a href="http://arxiv.org/find/physics/1/au:+Seracini_M/0/1/0/all/0/1">Marco Seracini</a>, <a href="http://arxiv.org/find/physics/1/au:+Gu_C/0/1/0/all/0/1">Chen Gu</a>, <a href="http://arxiv.org/find/physics/1/au:+Fehler_M/0/1/0/all/0/1">Michael Fehler</a>, <a href="http://arxiv.org/find/physics/1/au:+Faulds_J/0/1/0/all/0/1">James Faulds</a>, <a href="http://arxiv.org/find/physics/1/au:+Smith_C/0/1/0/all/0/1">Connor M. Smith</a>, <a href="http://arxiv.org/find/physics/1/au:+Treitel_S/0/1/0/all/0/1">Sven Treitel</a></p>
<p>We consider the application of machine learning to the evaluation of
geothermal resource potential. A supervised learning problem is defined where
maps of 10 geological and geophysical features within the state of Nevada, USA
are used to define geothermal potential across a broad region. We have
available a relatively small set of positive training sites (known resources or
active power plants) and negative training sites (known drill sites with
unsuitable geothermal conditions) and use these to constrain and optimize
artificial neural networks for this classification task. The main objective is
to predict the geothermal resource potential at unknown sites within a large
geographic area where the defining features are known. These predictions could
be used to target promising areas for further detailed investigations. We
describe the evolution of our work from defining a specific neural network
architecture to training and optimization trials. Upon analysis we expose the
inevitable problems of model variability and resulting prediction uncertainty.
Finally, to address these problems we apply the concept of Bayesian neural
networks, a heuristic approach to regularization in network training, and make
use of the practical interpretation of the formal uncertainty measures they
provide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09933">Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective. (arXiv:2210.09933v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_J/0/1/0/all/0/1">Jos&#xe9; Ribeiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1">Lucas Cardoso</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1">Ra&#xed;ssa Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Cirilo_V/0/1/0/all/0/1">Vitor Cirilo</a>, <a href="http://arxiv.org/find/cs/1/au:+Carneiro_N/0/1/0/all/0/1">N&#xed;kolas Carneiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1">Ronnie Alves</a></p>
<p>In recent years, XAI researchers have been formalizing proposals and
developing new methods to explain black box models, with no general consensus
in the community on which method to use to explain these models, with this
choice being almost directly linked to the popularity of a specific method.
Methods such as Ciu, Dalex, Eli5, Lofo, Shap and Skater emerged with the
proposal to explain black box models through global rankings of feature
relevance, which based on different methodologies, generate global explanations
that indicate how the model's inputs explain its predictions. In this context,
41 datasets, 4 tree-ensemble algorithms (Light Gradient Boosting, CatBoost,
Random Forest, and Gradient Boosting), and 6 XAI methods were used to support
the launch of a new XAI method, called eXirt, based on Item Response Theory -
IRT and aimed at tree-ensemble black box models that use tabular data referring
to binary classification problems. In the first set of analyses, the 164 global
feature relevance ranks of the eXirt were compared with 984 ranks of the other
XAI methods present in the literature, seeking to highlight their similarities
and differences. In a second analysis, exclusive explanations of the eXirt
based on Explanation-by-example were presented that help in understanding the
model trust. Thus, it was verified that eXirt is able to generate global
explanations of tree-ensemble models and also local explanations of instances
of models through IRT, showing how this consolidated theory can be used in
machine learning in order to obtain explainable and reliable models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10482">Effective Targeted Attacks for Adversarial Self-Supervised Learning. (arXiv:2210.10482v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minseon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1">Hyeonjeong Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1">Sooel Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a></p>
<p>Recently, unsupervised adversarial training (AT) has been highlighted as a
means of achieving robustness in models without any label information. Previous
studies in unsupervised AT have mostly focused on implementing self-supervised
learning (SSL) frameworks, which maximize the instance-wise classification loss
to generate adversarial examples. However, we observe that simply maximizing
the self-supervised training loss with an untargeted adversarial attack often
results in generating ineffective adversaries that may not help improve the
robustness of the trained model, especially for non-contrastive SSL frameworks
without negative examples. To tackle this problem, we propose a novel positive
mining for targeted adversarial attack to generate effective adversaries for
adversarial SSL frameworks. Specifically, we introduce an algorithm that
selects the most confusing yet similar target example for a given instance
based on entropy and similarity, and subsequently perturbs the given instance
towards the selected target. Our method demonstrates significant enhancements
in robustness when applied to non-contrastive SSL frameworks, and less but
consistent robustness improvements with contrastive SSL frameworks, on the
benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10485">Learning Transferable Adversarial Robust Representations via Multi-view Consistency. (arXiv:2210.10485v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minseon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1">Hyeonjeong Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dong Bok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a></p>
<p>Despite the success on few-shot learning problems, most meta-learned models
only focus on achieving good performance on clean examples and thus easily
break down when given adversarially perturbed samples. While some recent works
have shown that a combination of adversarial learning and meta-learning could
enhance the robustness of a meta-learner against adversarial attacks, they fail
to achieve generalizable adversarial robustness to unseen domains and tasks,
which is the ultimate goal of meta-learning. To address this challenge, we
propose a novel meta-adversarial multi-view representation learning framework
with dual encoders. Specifically, we introduce the discrepancy across the two
differently augmented samples of the same data instance by first updating the
encoder parameters with them and further imposing a novel label-free
adversarial attack to maximize their discrepancy. Then, we maximize the
consistency across the views to learn transferable robust representations
across domains and tasks. Through experimental validation on multiple
benchmarks, we demonstrate the effectiveness of our framework on few-shot
learning tasks from unseen domains, achieving over 10\% robust accuracy
improvements against previous adversarial meta-learning baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.17218">Artificial intelligence in government: Concepts, standards, and a unified framework. (arXiv:2210.17218v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Straub_V/0/1/0/all/0/1">Vincent J. Straub</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1">Deborah Morgan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bright_J/0/1/0/all/0/1">Jonathan Bright</a>, <a href="http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1">Helen Margetts</a></p>
<p>Recent advances in artificial intelligence (AI), especially in generative
language modelling, hold the promise of transforming government. Given the
advanced capabilities of new AI systems, it is critical that these are embedded
using standard operational procedures, clear epistemic criteria, and behave in
alignment with the normative expectations of society. Scholars in multiple
domains have subsequently begun to conceptualize the different forms that AI
applications may take, highlighting both their potential benefits and pitfalls.
However, the literature remains fragmented, with researchers in social science
disciplines like public administration and political science, and the
fast-moving fields of AI, ML, and robotics, all developing concepts in relative
isolation. Although there are calls to formalize the emerging study of AI in
government, a balanced account that captures the full depth of theoretical
perspectives needed to understand the consequences of embedding AI into a
public sector context is lacking. Here, we unify efforts across social and
technical disciplines by first conducting an integrative literature review to
identify and cluster 69 key terms that frequently co-occur in the
multidisciplinary study of AI. We then build on the results of this
bibliometric analysis to propose three new multifaceted concepts for
understanding and analysing AI-based systems for government (AI-GOV) in a more
unified way: (1) operational fitness, (2) epistemic alignment, and (3)
normative divergence. Finally, we put these concepts to work by using them as
dimensions in a conceptual typology of AI-GOV and connecting each with emerging
AI technical measurement standards to encourage operationalization, foster
cross-disciplinary dialogue, and stimulate debate among those aiming to rethink
government with AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.05321">Fairness and bias correction in machine learning for depression prediction: results from four study populations. (arXiv:2211.05321v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dang_V/0/1/0/all/0/1">Vien Ngoc Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cascarano_A/0/1/0/all/0/1">Anna Cascarano</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulder_R/0/1/0/all/0/1">Rosa H. Mulder</a>, <a href="http://arxiv.org/find/cs/1/au:+Cecil_C/0/1/0/all/0/1">Charlotte Cecil</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuluaga_M/0/1/0/all/0/1">Maria A. Zuluaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Gonzalez_J/0/1/0/all/0/1">Jer&#xf3;nimo Hern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href="http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1">Karim Lekadir</a></p>
<p>A significant level of stigma and inequality exists in mental healthcare,
especially in under-served populations. Inequalities are reflected in the data
collected for scientific purposes. When not properly accounted for, machine
learning (ML) models leart from data can reinforce these structural
inequalities or biases. Here, we present a systematic study of bias in ML
models designed to predict depression in four different case studies covering
different countries and populations. We find that standard ML approaches show
regularly biased behaviors. We also show that mitigation techniques, both
standard and our own post-hoc method, can be effective in reducing the level of
unfair bias. No single best ML model for depression prediction provides
equality of outcomes. This emphasizes the importance of analyzing fairness
during model selection and transparent reporting about the impact of debiasing
interventions. Finally, we provide practical recommendations to develop
bias-aware ML models for depression risk prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03654">Node-oriented Spectral Filtering for Graph Neural Networks. (arXiv:2212.03654v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhenfeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhizhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Youru Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a></p>
<p>Graph neural networks (GNNs) have shown remarkable performance on homophilic
graph data while being far less impressive when handling non-homophilic graph
data due to the inherent low-pass filtering property of GNNs. In general, since
real-world graphs are often complex mixtures of diverse subgraph patterns,
learning a universal spectral filter on the graph from the global perspective
as in most current works may still suffer from great difficulty in adapting to
the variation of local patterns. On the basis of the theoretical analysis of
local patterns, we rethink the existing spectral filtering methods and propose
the node-oriented spectral filtering for graph neural network (namely NFGNN).
By estimating the node-oriented spectral filter for each node, NFGNN is
provided with the capability of precise local node positioning via the
generalized translated operator, thus discriminating the variations of local
homophily patterns adaptively. Meanwhile, the utilization of
re-parameterization brings a good trade-off between global consistency and
local sensibility for learning the node-oriented spectral filters. Furthermore,
we theoretically analyze the localization property of NFGNN, demonstrating that
the signal after adaptive filtering is still positioned around the
corresponding node. Extensive experimental results demonstrate that the
proposed NFGNN achieves more favorable performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08951">Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chengmin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a></p>
<p>When perceiving the world from multiple viewpoints, humans have the ability
to reason about the complete objects in a compositional manner even when an
object is completely occluded from certain viewpoints. Meanwhile, humans are
able to imagine novel views after observing multiple viewpoints. Recent
remarkable advances in multi-view object-centric learning still leaves some
unresolved problems: 1) The shapes of partially or completely occluded objects
can not be well reconstructed. 2) The novel viewpoint prediction depends on
expensive viewpoint annotations rather than implicit rules in view
representations. In this paper, we introduce a time-conditioned generative
model for videos. To reconstruct the complete shape of an object accurately, we
enhance the disentanglement between the latent representations of objects and
views, where the latent representations of time-conditioned views are jointly
inferred with a Transformer and then are input to a sequential extension of
Slot Attention to learn object-centric representations. In addition, Gaussian
processes are employed as priors of view latent variables for video generation
and novel-view prediction without viewpoint annotations. Experiments on
multiple datasets demonstrate that the proposed model can make object-centric
video decomposition, reconstruct the complete shapes of occluded objects, and
make novel-view predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10813">Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1">Yijun Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_A/0/1/0/all/0/1">Anqi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nanguang Chen</a></p>
<p>The concern about underlying discrimination hidden in machine learning (ML)
models is increasing, as ML systems have been widely applied in more and more
real-world scenarios and any discrimination hidden in them will directly affect
human life. Many techniques have been developed to enhance fairness including
commonly-used group fairness measures and several fairness-aware methods
combining ensemble learning. However, existing fairness measures can only focus
on one aspect -- either group or individual fairness, and the hard
compatibility among them indicates a possibility of remaining biases even if
one of them is satisfied. Moreover, existing mechanisms to boost fairness
usually present empirical results to show validity, yet few of them discuss
whether fairness can be boosted with certain theoretical guarantees. To address
these issues, we propose a fairness quality measure named discriminative risk
to reflect both individual and group fairness aspects. Furthermore, we
investigate the properties of the proposed measure and propose first- and
second-order oracle bounds to show that fairness can be boosted via ensemble
combination with theoretical learning guarantees. The analysis is suitable for
both binary and multi-class classification. A pruning method is also proposed
to utilise our proposed measure and comprehensive experiments are conducted to
evaluate the effectiveness of the proposed methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11113">Finding Regions of Counterfactual Explanations via Robust Optimization. (arXiv:2301.11113v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maragno_D/0/1/0/all/0/1">Donato Maragno</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurtz_J/0/1/0/all/0/1">Jannis Kurtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rober_T/0/1/0/all/0/1">Tabea E. R&#xf6;ber</a>, <a href="http://arxiv.org/find/cs/1/au:+Goedhart_R/0/1/0/all/0/1">Rob Goedhart</a>, <a href="http://arxiv.org/find/cs/1/au:+Birbil_S/0/1/0/all/0/1">&#x15e;. Ilker Birbil</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertog_D/0/1/0/all/0/1">Dick den Hertog</a></p>
<p>Counterfactual explanations play an important role in detecting bias and
improving the explainability of data-driven classification models. A
counterfactual explanation (CE) is a minimal perturbed data point for which the
decision of the model changes. Most of the existing methods can only provide
one CE, which may not be achievable for the user. In this work we derive an
iterative method to calculate robust CEs, i.e. CEs that remain valid even after
the features are slightly perturbed. To this end, our method provides a whole
region of CEs allowing the user to choose a suitable recourse to obtain a
desired outcome. We use algorithmic ideas from robust optimization and prove
convergence results for the most common machine learning methods including
logistic regression, decision trees, random forests, and neural networks. Our
experiments show that our method can efficiently generate globally optimal
robust CEs for a variety of common data sets and classification models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11588">Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty. (arXiv:2301.11588v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Inatsu_Y/0/1/0/all/0/1">Yu Inatsu</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeno_S/0/1/0/all/0/1">Shion Takeno</a>, <a href="http://arxiv.org/find/stat/1/au:+Hanada_H/0/1/0/all/0/1">Hiroyuki Hanada</a>, <a href="http://arxiv.org/find/stat/1/au:+Iwata_K/0/1/0/all/0/1">Kazuki Iwata</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a></p>
<p>In this study, we propose a novel multi-objective Bayesian optimization
(MOBO) method to efficiently identify the Pareto front (PF) defined by risk
measures for black-box functions under the presence of input uncertainty (IU).
Existing BO methods for Pareto optimization in the presence of IU are
risk-specific or without theoretical guarantees, whereas our proposed method
addresses general risk measures and has theoretical guarantees. The basic idea
of the proposed method is to assume a Gaussian process (GP) model for the
black-box function and to construct high-probability bounding boxes for the
risk measures using the GP model. Furthermore, in order to reduce the
uncertainty of non-dominated bounding boxes, we propose a method of selecting
the next evaluation point using a maximin distance defined by the maximum value
of a quasi distance based on bounding boxes. As theoretical analysis, we prove
that the algorithm can return an arbitrary-accurate solution in a finite number
of iterations with high probability, for various risk measures such as Bayes
risk, worst-case risk, and value-at-risk. We also give a theoretical analysis
that takes into account approximation errors because there exist non-negligible
approximation errors (e.g., finite approximation of PFs and sampling-based
approximation of bounding boxes) in practice. We confirm that the proposed
method outperforms compared with existing methods not only in the setting with
IU but also in the setting of ordinary MOBO through numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12593">Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Queeney_J/0/1/0/all/0/1">James Queeney</a>, <a href="http://arxiv.org/find/cs/1/au:+Benosman_M/0/1/0/all/0/1">Mouhacine Benosman</a></p>
<p>Many real-world domains require safe decision making in uncertain
environments. In this work, we introduce a deep reinforcement learning
framework for approaching this important problem. We consider a distribution
over transition models, and apply a risk-averse perspective towards model
uncertainty through the use of coherent distortion risk measures. We provide
robustness guarantees for this framework by showing it is equivalent to a
specific class of distributionally robust safe reinforcement learning problems.
Unlike existing approaches to robustness in deep reinforcement learning,
however, our formulation does not involve minimax optimization. This leads to
an efficient, model-free implementation of our approach that only requires
standard data collection from a single training environment. In experiments on
continuous control tasks with safety constraints, we demonstrate that our
framework produces robust performance and safety at deployment time across a
range of perturbed test environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12906">Curvature Filtrations for Graph Generative Model Evaluation. (arXiv:2301.12906v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Southern_J/0/1/0/all/0/1">Joshua Southern</a>, <a href="http://arxiv.org/find/cs/1/au:+Wayland_J/0/1/0/all/0/1">Jeremy Wayland</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1">Michael Bronstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1">Bastian Rieck</a></p>
<p>Graph generative model evaluation necessitates understanding differences
between graphs on the distributional level. This entails being able to harness
salient attributes of graphs in an efficient manner. Curvature constitutes one
such property that has recently proved its utility in characterising graphs.
Its expressive properties, stability, and practical utility in model evaluation
remain largely unexplored, however. We combine graph curvature descriptors with
emerging methods from topological data analysis to obtain robust, expressive
descriptors for evaluating graph generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12930">Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing. (arXiv:2301.12930v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1">Oliver Schulte</a></p>
<p>A fundamental problem of causal discovery is cause-effect inference, learning
the correct causal direction between two random variables. Significant progress
has been made through modelling the effect as a function of its cause and a
noise term, which allows us to leverage assumptions about the generating
function class. The recently introduced heteroscedastic location-scale noise
functional models (LSNMs) combine expressive power with identifiability
guarantees. LSNM model selection based on maximizing likelihood achieves
state-of-the-art accuracy, when the noise distributions are correctly
specified. However, through an extensive empirical evaluation, we demonstrate
that the accuracy deteriorates sharply when the form of the noise distribution
is misspecified by the user. Our analysis shows that the failure occurs mainly
when the conditional variance in the anti-causal direction is smaller than that
in the causal direction. As an alternative, we find that causal model selection
through residual independence testing is much more robust to noise
misspecification and misleading conditional variance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13349">Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1">Ashok Cutkosky</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1">Ioannis Ch. Paschalidis</a></p>
<p>Motivated by the challenge of nonstationarity in sequential decision making,
we study Online Convex Optimization (OCO) under the coupling of two problem
structures: the domain is unbounded, and the comparator sequence
$u_1,\ldots,u_T$ is arbitrarily time-varying. As no algorithm can guarantee low
regret simultaneously against all comparator sequences, handling this setting
requires moving from minimax optimality to comparator adaptivity. That is,
sensible regret bounds should depend on certain complexity measures of the
comparator relative to one's prior knowledge.
</p>
<p>This paper achieves a new type of these adaptive regret bounds via a sparse
coding framework. The complexity of the comparator is measured by its energy
and its sparsity on a user-specified dictionary, which offers considerable
versatility. Equipped with a wavelet dictionary for example, our framework
improves the state-of-the-art bound (Jacobsen &amp; Cutkosky, 2022) by adapting to
both ($i$) the magnitude of the comparator average $||\bar
u||=||\sum_{t=1}^Tu_t/T||$, rather than the maximum $\max_t||u_t||$; and ($ii$)
the comparator variability $\sum_{t=1}^T||u_t-\bar u||$, rather than the
uncentered sum $\sum_{t=1}^T||u_t||$. Furthermore, our analysis is simpler due
to decoupling function approximation from regret minimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01681">Improving the Timing Resolution of Positron Emission Tomography Detectors Using Boosted Learning -- A Residual Physics Approach. (arXiv:2302.01681v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naunheim_S/0/1/0/all/0/1">Stephan Naunheim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_Y/0/1/0/all/0/1">Yannick Kuhl</a>, <a href="http://arxiv.org/find/cs/1/au:+Schug_D/0/1/0/all/0/1">David Schug</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_V/0/1/0/all/0/1">Volkmar Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1">Florian Mueller</a></p>
<p>Artificial intelligence (AI) is entering medical imaging, mainly enhancing
image reconstruction. Nevertheless, improvements throughout the entire
processing, from signal detection to computation, potentially offer significant
benefits. This work presents a novel and versatile approach to detector
optimization using machine learning (ML) and residual physics. We apply the
concept to positron emission tomography (PET), intending to improve the
coincidence time resolution (CTR). PET visualizes metabolic processes in the
body by detecting photons with scintillation detectors. Improved CTR
performance offers the advantage of reducing radioactive dose exposure for
patients. Modern PET detectors with sophisticated concepts and read-out
topologies represent complex physical and electronic systems requiring
dedicated calibration techniques. Traditional methods primarily depend on
analytical formulations successfully describing the main detector
characteristics. However, when accounting for higher-order effects, additional
complexities arise matching theoretical models to experimental reality. Our
work addresses this challenge by combining traditional calibration with AI and
residual physics, presenting a highly promising approach. We present a residual
physics-based strategy using gradient tree boosting and physics-guided data
generation. The explainable AI framework SHapley Additive exPlanations (SHAP)
was used to identify known physical effects with learned patterns. In addition,
the models were tested against basic physical laws. We were able to improve the
CTR significantly (more than 20%) for clinically relevant detectors of 19 mm
height, reaching CTRs of 185 ps (450-550 keV).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01757">RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion. (arXiv:2302.01757v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhuoqun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchant_N/0/1/0/all/0/1">Neil G. Marchant</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_K/0/1/0/all/0/1">Keane Lucas</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1">Lujo Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1">Olga Ohrimenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1">Benjamin I. P. Rubinstein</a></p>
<p>Randomized smoothing is a leading approach for constructing classifiers that
are certifiably robust against adversarial examples. Existing work on
randomized smoothing has focused on classifiers with continuous inputs, such as
images, where $\ell_p$-norm bounded adversaries are commonly studied. However,
there has been limited work for classifiers with discrete or variable-size
inputs, such as for source code, which require different threat models and
smoothing mechanisms. In this work, we adapt randomized smoothing for discrete
sequence classifiers to provide certified robustness against edit
distance-bounded adversaries. Our proposed smoothing mechanism randomized
deletion (RS-Del) applies random deletion edits, which are (perhaps
surprisingly) sufficient to confer robustness against adversarial deletion,
insertion and substitution edits. Our proof of certification deviates from the
established Neyman-Pearson approach, which is intractable in our setting, and
is instead organized around longest common subsequences. We present a case
study on malware detection--a binary classification problem on byte sequences
where classifier evasion is a well-established threat model. When applied to
the popular MalConv malware detection model, our smoothing mechanism RS-Del
achieves a certified accuracy of 91% at an edit distance radius of 128 bytes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02209">A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs. (arXiv:2302.02209v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xingyue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Orth_M/0/1/0/all/0/1">Miguel Romero Orth</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1">&#x130;smail &#x130;lkan Ceylan</a>, <a href="http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1">Pablo Barcel&#xf3;</a></p>
<p>Graph neural networks are prominent models for representation learning over
graph-structured data. While the capabilities and limitations of these models
are well-understood for simple graphs, our understanding remains incomplete in
the context of knowledge graphs. Our goal is to provide a systematic
understanding of the landscape of graph neural networks for knowledge graphs
pertaining to the prominent task of link prediction. Our analysis entails a
unifying perspective on seemingly unrelated models and unlocks a series of
other models. The expressive power of various models is characterized via a
corresponding relational Weisfeiler-Leman algorithm. This analysis is extended
to provide a precise logical characterization of the class of functions
captured by a class of graph neural networks. The theoretical findings
presented in this paper explain the benefits of some widely employed practical
design choices, which are validated empirically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03857">Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xilie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a></p>
<p>Adversarial contrastive learning (ACL) does not require expensive data
annotations but outputs a robust representation that withstands adversarial
attacks and also generalizes to a wide range of downstream tasks. However, ACL
needs tremendous running time to generate the adversarial variants of all
training data, which limits its scalability to large datasets. To speed up ACL,
this paper proposes a robustness-aware coreset selection (RCS) method. RCS does
not require label information and searches for an informative subset that
minimizes a representational divergence, which is the distance of the
representation between natural data and their virtual adversarial variants. The
vanilla solution of RCS via traversing all possible subsets is computationally
prohibitive. Therefore, we theoretically transform RCS into a surrogate problem
of submodular maximization, of which the greedy search is an efficient solution
with an optimality guarantee for the original problem. Empirically, our
comprehensive results corroborate that RCS can speed up ACL by a large margin
without significantly hurting the robustness transferability. Notably, to the
best of our knowledge, we are the first to conduct ACL efficiently on the
large-scale ImageNet-1K dataset to obtain an effective robust representation
via RCS. Our source code is at
https://github.com/GodXuxilie/Efficient_ACL_via_RCS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04089">ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1">Eldar Kurtic</a>, <a href="http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1">Elias Frantar</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>The breakthrough performance of large language models (LLMs) comes with major
computational footprints and high deployment costs. In this paper, we progress
towards resolving this problem by proposing a novel structured compression
approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art
accuracy-vs-speedup, while matching a set of desired target runtime speedups in
any given inference environment. Specifically, given a model, a dataset, an
inference environment, as well as a set of speedup targets, ZipLM iteratively
identifies and removes components with the worst loss-runtime trade-off. Unlike
prior methods that specialize in either the post-training/one-shot or the
gradual compression setting, and only for specific families of models such as
BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed
models across all these settings. Furthermore, ZipLM achieves superior results
for a fraction of the computational cost relative to prior distillation and
pruning techniques, making it a cost-effective approach for generating an
entire family of smaller, faster, and highly accurate models, guaranteed to
meet the desired inference specifications. In particular, ZipLM outperforms all
prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and
TinyBERT. Moreover, it matches the performance of the heavily optimized
MobileBERT model, obtained via extensive architecture search, by simply pruning
the baseline BERT-large model. When compressing GPT2, ZipLM outperforms
DistilGPT2 while being 60% smaller and 30% faster. Our code is available at:
https://github.com/IST-DASLab/ZipLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04449">Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yewen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1">Amos Azaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1">Tom M. Mitchell</a></p>
<p>High sample complexity has long been a challenge for RL. On the other hand,
humans learn to perform tasks not only from interaction or demonstrations, but
also by reading unstructured text documents, e.g., instruction manuals.
Instruction manuals and wiki pages are among the most abundant data that could
inform agents of valuable features and policies or task-specific environmental
dynamics and reward structures. Therefore, we hypothesize that the ability to
utilize human-written instruction manuals to assist learning policies for
specific tasks should lead to a more efficient and better-performing agent. We
propose the Read and Reward framework. Read and Reward speeds up RL algorithms
on Atari games by reading manuals released by the Atari game developers. Our
framework consists of a QA Extraction module that extracts and summarizes
relevant information from the manual and a Reasoning module that evaluates
object-agent interactions based on information from the manual. An auxiliary
reward is then provided to a standard A2C RL agent, when interaction is
detected. Experimentally, various RL algorithms obtain significant improvement
in performance and training speed when assisted by our design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04824">Lithium Metal Battery Quality Control via Transformer-CNN Segmentation. (arXiv:2302.04824v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quenum_J/0/1/0/all/0/1">Jerome Quenum</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenyuk_I/0/1/0/all/0/1">Iryna Zenyuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Ushizima_D/0/1/0/all/0/1">Daniela Ushizima</a></p>
<p>Lithium metal battery (LMB) has the potential to be the next-generation
battery system because of its high theoretical energy density. However, defects
known as dendrites are formed by heterogeneous lithium (Li) plating, which
hinders the development and utilization of LMBs. Non-destructive techniques to
observe the dendrite morphology often use X-ray computed tomography (XCT) to
provide cross-sectional views. To retrieve three-dimensional structures inside
a battery, image segmentation becomes essential to quantitatively analyze XCT
images. This work proposes a new semantic segmentation approach using a
transformer-based neural network called TransforCNN that is capable of
segmenting out dendrites from XCT data. In addition, we compare the performance
of the proposed TransforCNN with three other algorithms, such as U-Net, Y-Net,
and E-Net, consisting of an Ensemble Network model for XCT analysis. Our
results show the advantages of using TransforCNN when evaluating
over-segmentation metrics, such as mean Intersection over Union (mIoU) and mean
Dice Similarity Coefficient (mDSC) as well as through several qualitatively
comparative visualizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13534">Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms. (arXiv:2302.13534v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1">Tiancheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haipeng Luo</a></p>
<p>We study the problem of designing adaptive multi-armed bandit algorithms that
perform optimally in both the stochastic setting and the adversarial setting
simultaneously (often known as a best-of-both-world guarantee). A line of
recent works shows that when configured and analyzed properly, the
Follow-the-Regularized-Leader (FTRL) algorithm, originally designed for the
adversarial setting, can in fact optimally adapt to the stochastic setting as
well. Such results, however, critically rely on an assumption that there exists
one unique optimal arm. Recently, Ito (2021) took the first step to remove such
an undesirable uniqueness assumption for one particular FTRL algorithm with the
$\frac{1}{2}$-Tsallis entropy regularizer. In this work, we significantly
improve and generalize this result, showing that uniqueness is unnecessary for
FTRL with a broad family of regularizers and a new learning rate schedule. For
some regularizers, our regret bounds also improve upon prior results even when
uniqueness holds. We further provide an application of our results to the
decoupled exploration and exploitation problem, demonstrating that our
techniques are broadly applicable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00028">Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v6 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jakkala_K/0/1/0/all/0/1">Kalvik Jakkala</a>, <a href="http://arxiv.org/find/cs/1/au:+Akella_S/0/1/0/all/0/1">Srinivas Akella</a></p>
<p>The sensor placement problem is a common problem that arises when monitoring
correlated phenomena, such as temperature, precipitation, and salinity.
Existing approaches to this problem typically formulate it as the maximization
of information metrics, such as mutual information~(MI), and use optimization
methods such as greedy algorithms in discrete domains, and derivative-free
optimization methods such as genetic algorithms in continuous domains. However,
computing MI for sensor placement requires discretizing the environment, and
its computation cost depends on the size of the discretized environment. This
limitation restricts these approaches from scaling to large problems. We have
uncovered a novel connection between the sensor placement problem and sparse
Gaussian processes~(SGP). Our approach leverages SGPs and is gradient-based,
which allows us to efficiently find solution placements in continuous
environments. We generalize our method to also handle discrete environments.
Our experimental results on four real-world datasets demonstrate that our
approach generates sensor placements consistently on par with or better than
the prior state-of-the-art approaches in terms of both MI and reconstruction
quality, all while being significantly faster. Our computationally efficient
approach enables both large-scale sensor placement and fast robotic sensor
placement for informative path planning algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00198">Convolutional Visual Prompt for Robust Visual Perception. (arXiv:2303.00198v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yun-Yun Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chengzhi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Junfeng Yang</a></p>
<p>Vision models are often vulnerable to out-of-distribution (OOD) samples
without adapting. While visual prompts offer a lightweight method of
input-space adaptation for large-scale vision models, they rely on a
high-dimensional additive vector and labeled data. This leads to overfitting
when adapting models in a self-supervised test-time setting without labels. We
introduce convolutional visual prompts (CVP) for label-free test-time
adaptation for robust visual perception. The structured nature of CVP demands
fewer trainable parameters, less than 1\% compared to standard visual prompts,
combating overfitting. Extensive experiments and analysis on a wide variety of
OOD visual perception tasks show that our approach is effective, improving
robustness by up to 5.87% over several large-scale models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03284">The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Avalos_R/0/1/0/all/0/1">Raphael Avalos</a>, <a href="http://arxiv.org/find/cs/1/au:+Delgrange_F/0/1/0/all/0/1">Florent Delgrange</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1">Ann Now&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Guillermo A. P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1">Diederik M. Roijers</a></p>
<p>Partially Observable Markov Decision Processes (POMDPs) are used to model
environments where the full state cannot be perceived by an agent. As such the
agent needs to reason taking into account the past observations and actions.
However, simply remembering the full history is generally intractable due to
the exponential growth in the history space. Maintaining a probability
distribution that models the belief over what the true state is can be used as
a sufficient statistic of the history, but its computation requires access to
the model of the environment and is often intractable. While SOTA algorithms
use Recurrent Neural Networks to compress the observation-action history aiming
to learn a sufficient statistic, they lack guarantees of success and can lead
to sub-optimal policies. To overcome this, we propose the Wasserstein Belief
Updater, an RL algorithm that learns a latent model of the POMDP and an
approximation of the belief update. Our approach comes with theoretical
guarantees on the quality of our approximation ensuring that our outputted
beliefs allow for learning the optimal value function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05683">Hierarchical clustering with OWA-based linkages, the Lance-Williams formula, and dendrogram inversions. (arXiv:2303.05683v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gagolewski_M/0/1/0/all/0/1">Marek Gagolewski</a>, <a href="http://arxiv.org/find/stat/1/au:+Cena_A/0/1/0/all/0/1">Anna Cena</a>, <a href="http://arxiv.org/find/stat/1/au:+James_S/0/1/0/all/0/1">Simon James</a>, <a href="http://arxiv.org/find/stat/1/au:+Beliakov_G/0/1/0/all/0/1">Gleb Beliakov</a></p>
<p>Agglomerative hierarchical clustering based on Ordered Weighted Averaging
(OWA) operators not only generalises the single, complete, and average
linkages, but also includes intercluster distances based on a few nearest or
farthest neighbours, trimmed and winsorised means of pairwise point
similarities, amongst many others. We explore the relationships between the
famous Lance-Williams update formula and the extended OWA-based linkages with
weights generated via infinite coefficient sequences. Furthermore, we provide
some conditions for the weight generators to guarantee the resulting
dendrograms to be free from unaesthetic inversions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11207">Investigating Topological Order using Recurrent Neural Networks. (arXiv:2303.11207v3 [cond-mat.str-el] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Hibat_Allah_M/0/1/0/all/0/1">Mohamed Hibat-Allah</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Melko_R/0/1/0/all/0/1">Roger G. Melko</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Carrasquilla_J/0/1/0/all/0/1">Juan Carrasquilla</a></p>
<p>Recurrent neural networks (RNNs), originally developed for natural language
processing, hold great promise for accurately describing strongly correlated
quantum many-body systems. Here, we employ 2D RNNs to investigate two
prototypical quantum many-body Hamiltonians exhibiting topological order.
Specifically, we demonstrate that RNN wave functions can effectively capture
the topological order of the toric code and a Bose-Hubbard spin liquid on the
kagome lattice by estimating their topological entanglement entropies. We also
find that RNNs favor coherent superpositions of minimally-entangled states over
minimally-entangled states themselves. Overall, our findings demonstrate that
RNN wave functions constitute a powerful tool to study phases of matter beyond
Landau's symmetry-breaking paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11249">What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alexander_Y/0/1/0/all/0/1">Yotam Alexander</a>, <a href="http://arxiv.org/find/cs/1/au:+Vega_N/0/1/0/all/0/1">Nimrod De La Vega</a>, <a href="http://arxiv.org/find/cs/1/au:+Razin_N/0/1/0/all/0/1">Noam Razin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1">Nadav Cohen</a></p>
<p>The question of what makes a data distribution suitable for deep learning is
a fundamental open problem. Focusing on locally connected neural networks (a
prevalent family of architectures that includes convolutional and recurrent
neural networks as well as local self-attention models), we address this
problem by adopting theoretical tools from quantum physics. Our main
theoretical result states that a certain locally connected neural network is
capable of accurate prediction over a data distribution if and only if the data
distribution admits low quantum entanglement under certain canonical partitions
of features. As a practical application of this result, we derive a
preprocessing method for enhancing the suitability of a data distribution to
locally connected neural networks. Experiments with widespread models over
various datasets demonstrate our findings. We hope that our use of quantum
entanglement will encourage further adoption of tools from physics for formally
reasoning about the relation between deep learning and real-world data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06700">Control3Diff: Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiatao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qingzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1">Shuangfei Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Baoquan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1">Josh Susskind</a></p>
<p>Diffusion models have recently become the de-facto approach for generative
modeling in the 2D domain. However, extending diffusion models to 3D is
challenging due to the difficulties in acquiring 3D ground truth data for
training. On the other hand, 3D GANs that integrate implicit 3D representations
into GANs have shown remarkable 3D-aware generation when trained only on
single-view image datasets. However, 3D GANs do not provide straightforward
ways to precisely control image synthesis. To address these challenges, We
present Control3Diff, a 3D diffusion model that combines the strengths of
diffusion models and 3D GANs for versatile, controllable 3D-aware image
synthesis for single-view datasets. Control3Diff explicitly models the
underlying latent distribution (optionally conditioned on external inputs),
thus enabling direct control during the diffusion process. Moreover, our
approach is general and applicable to any type of controlling input, allowing
us to train it with the same diffusion objective without any auxiliary
supervision. We validate the efficacy of Control3Diff on standard image
generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various
conditioning inputs such as images, sketches, and text prompts. Please see the
project website (\url{https://jiataogu.me/control3diff}) for video comparisons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00633">Self-Evaluation Guided Beam Search for Reasoning. (arXiv:2305.00633v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiran Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1">Min-Yen Kan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qizhe Xie</a></p>
<p>Breaking down a problem into intermediate steps has demonstrated impressive
performance in Large Language Model (LLM) reasoning. However, the growth of the
reasoning chain introduces uncertainty and error accumulation, making it
challenging to elicit accurate final results. To tackle this challenge of
uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation
mechanism to guide and calibrate the reasoning process of LLMs. We propose a
decoding algorithm integrating the self-evaluation guidance via stochastic beam
search. The self-evaluation guidance serves as a better-calibrated automatic
criterion, facilitating an efficient search in the reasoning space and
resulting in superior prediction quality. Stochastic beam search balances
exploitation and exploration of the search space with temperature-controlled
randomness. Our approach surpasses the corresponding Codex-backboned baselines
in few-shot accuracy by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQuA,
and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on
arithmetic reasoning demonstrate the efficiency of our method in outperforming
the baseline methods with comparable computational budgets. Further analysis in
multi-step reasoning finds our self-evaluation guidance pinpoints logic
failures and leads to higher consistency and robustness. Our code is publicly
available at https://guideddecoding.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03598">NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1">Ma&#xeb;l Jullien</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1">Marco Valentino</a>, <a href="http://arxiv.org/find/cs/1/au:+Frost_H/0/1/0/all/0/1">Hannah Frost</a>, <a href="http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1">Paul O&#x27;Regan</a>, <a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1">Donal Landers</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1">Andr&#xe9; Freitas</a></p>
<p>How can we interpret and retrieve medical evidence to support clinical
decisions? Clinical trial reports (CTR) amassed over the years contain
indispensable information for the development of personalized medicine.
However, it is practically infeasible to manually inspect over 400,000+
clinical trial reports in order to find the best evidence for experimental
treatments. Natural Language Inference (NLI) offers a potential solution to
this problem, by allowing the scalable computation of textual entailment.
However, existing NLI models perform poorly on biomedical corpora, and
previously published datasets fail to capture the full complexity of inference
over CTRs. In this work, we present a novel resource to advance research on NLI
for reasoning on CTRs. The resource includes two main tasks. Firstly, to
determine the inference relation between a natural language statement, and a
CTR. Secondly, to retrieve supporting facts to justify the predicted relation.
We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these
tasks. Baselines on this corpus expose the limitations of existing NLI models,
with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To
the best of our knowledge, we are the first to design a task that covers the
interpretation of full CTRs. To encourage further work on this challenging
dataset, we make the corpus, competition leaderboard, website and code to
replicate the baseline experiments available at:
https://github.com/ai-systems/nli4ct
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08175">An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions. (arXiv:2305.08175v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yingtai Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1">Guanlin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Danfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1">Daniel Kifer</a></p>
<p>Noisy marginals are a common form of confidentiality-protecting data release
and are useful for many downstream tasks such as contingency table analysis,
construction of Bayesian networks, and even synthetic data generation. Privacy
mechanisms that provide unbiased noisy answers to linear queries (such as
marginals) are known as matrix mechanisms.
</p>
<p>We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian
noise that is both optimal and scalable. ResidualPlanner can optimize for many
loss functions that can be written as a convex function of marginal variances
(prior work was restricted to just one predefined objective function).
ResidualPlanner can optimize the accuracy of marginals in large scale settings
in seconds, even when the previous state of the art (HDMM) runs out of memory.
It even runs on datasets with 100 attributes in a couple of minutes.
Furthermore ResidualPlanner can efficiently compute variance/covariance values
for each marginal (prior methods quickly run out of memory, even for relatively
small datasets).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11531">Generalizing to new geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v3 [physics.ins-det] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1">Junze Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Ghosh_A/0/1/0/all/0/1">Aishik Ghosh</a>, <a href="http://arxiv.org/find/physics/1/au:+Smith_D/0/1/0/all/0/1">Dylan Smith</a>, <a href="http://arxiv.org/find/physics/1/au:+Baldi_P/0/1/0/all/0/1">Pierre Baldi</a>, <a href="http://arxiv.org/find/physics/1/au:+Whiteson_D/0/1/0/all/0/1">Daniel Whiteson</a></p>
<p>Generation of simulated detector response to collision products is crucial to
data analysis in particle physics, but computationally very expensive. One
subdetector, the calorimeter, dominates the computational time due to the high
granularity of its cells and complexity of the interactions. Generative models
can provide more rapid sample production, but currently require significant
effort to optimize performance for specific detector geometries, often
requiring many models to describe the varying cell sizes and arrangements,
without the ability to generalize to other geometries. We develop a
$\textit{geometry-aware}$ autoregressive model, which learns how the
calorimeter response varies with geometry, and is capable of generating
simulated responses to unseen geometries without additional training. The
geometry-aware model outperforms a baseline unaware model by over $50\%$ in
several metrics such as the Wasserstein distance between the generated and the
true distributions of key quantities which summarize the simulated response. A
single geometry-aware model could replace the hundreds of generative models
currently designed for calorimeter simulation by physicists analyzing data
collected at the Large Hadron Collider. This proof-of-concept study motivates
the design of a foundational model that will be a crucial tool for the study of
future detectors, dramatically reducing the large upfront investment usually
needed to develop generative calorimeter models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11685">Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jang_K/0/1/0/all/0/1">Kangwook Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Sungnyun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1">Hoirin Kim</a></p>
<p>Transformer-based speech self-supervised learning (SSL) models, such as
HuBERT, show surprising performance in various speech processing tasks.
However, huge number of parameters in speech SSL models necessitate the
compression to a more compact model for wider usage in academia or small
companies. In this study, we suggest to reuse attention maps across the
Transformer layers, so as to remove key and query parameters while retaining
the number of layers. Furthermore, we propose a novel masking distillation
strategy to improve the student model's speech representation quality. We
extend the distillation loss to utilize both masked and unmasked speech frames
to fully leverage the teacher model's high-quality representation. Our
universal compression strategy yields the student model that achieves phoneme
error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11982">Sequential Memory with Temporal Predictive Coding. (arXiv:2305.11982v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Tang_M/0/1/0/all/0/1">Mufeng Tang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Barron_H/0/1/0/all/0/1">Helen Barron</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bogacz_R/0/1/0/all/0/1">Rafal Bogacz</a></p>
<p>Forming accurate memory of sequential stimuli is a fundamental function of
biological agents. However, the computational mechanism underlying sequential
memory in the brain remains unclear. Inspired by neuroscience theories and
recent successes in applying predictive coding (PC) to \emph{static} memory
tasks, in this work we propose a novel PC-based model for \emph{sequential}
memory, called \emph{temporal predictive coding} (tPC). We show that our tPC
models can memorize and retrieve sequential inputs accurately with a
biologically plausible neural implementation. Importantly, our analytical study
reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN)
with an implicit statistical whitening process, which leads to more stable
performance in sequential memory tasks of structured inputs. Moreover, we find
that tPC exhibits properties consistent with behavioral observations and
theories in neuroscience, thereby strengthening its biological relevance. Our
work establishes a possible computational mechanism underlying sequential
memory in the brain that can also be theoretically interpreted using existing
memory model frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12283">Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods. (arXiv:2305.12283v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongze Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaocheng Li</a></p>
<p>In this paper, we consider the uncertainty quantification problem for
regression models. Specifically, we consider an individual calibration
objective for characterizing the quantiles of the prediction model. While such
an objective is well-motivated from downstream tasks such as newsvendor cost,
the existing methods have been largely heuristic and lack of statistical
guarantee in terms of individual calibration. We show via simple examples that
the existing methods focusing on population-level calibration guarantees such
as average calibration or sharpness can lead to harmful and unexpected results.
We propose simple nonparametric calibration methods that are agnostic of the
underlying prediction model and enjoy both computational efficiency and
statistical consistency. Our approach enables a better understanding of the
possibility of individual calibration, and we establish matching upper and
lower bounds for the calibration error of our proposed methods. Technically,
our analysis combines the nonparametric analysis with a covering number
argument for parametric analysis, which advances the existing theoretical
analyses in the literature of nonparametric density estimation and quantile
bandit problems. Importantly, the nonparametric perspective sheds new
theoretical insights into regression calibration in terms of the curse of
dimensionality and reconciles the existing results on the impossibility of
individual calibration. To our knowledge, we make the first effort to reach
both individual calibration and finite-sample guarantee with minimal
assumptions in terms of conformal prediction. Numerical experiments show the
advantage of such a simple approach under various metrics, and also under
covariates shift. We hope our work provides a simple benchmark and a starting
point of theoretical ground for future research on regression calibration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13082">Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates. (arXiv:2305.13082v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hanzely_S/0/1/0/all/0/1">Slavom&#xed;r Hanzely</a></p>
<p>In this paper, we propose the first sketch-and-project Newton method with
fast $\mathcal O(k^{-2})$ global convergence rate for self-concordant
functions. Our method, SGN, can be viewed in three ways: i) as a
sketch-and-project algorithm projecting updates of Newton method, ii) as a
cubically regularized Newton ethod in sketched subspaces, and iii) as a damped
Newton method in sketched subspaces. SGN inherits best of all three worlds:
cheap iteration costs of sketch-and-project methods, state-of-the-art $\mathcal
O(k^{-2})$ global convergence rate of full-rank Newton-like methods and the
algorithm simplicity of damped Newton methods. Finally, we demonstrate its
comparable empirical performance to baseline algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13552">Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsuchida_R/0/1/0/all/0/1">Russell Tsuchida</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1">Cheng Soon Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1">Dino Sejdinovic</a></p>
<p>Flexible models for probability distributions are an essential ingredient in
many machine learning tasks. We develop and investigate a new class of
probability distributions, which we call a Squared Neural Family (SNEFY),
formed by squaring the 2-norm of a neural network and normalising it with
respect to a base measure. Following the reasoning similar to the well
established connections between infinitely wide neural networks and Gaussian
processes, we show that SNEFYs admit closed form normalising constants in many
cases of interest, thereby resulting in flexible yet fully tractable density
models. SNEFYs strictly generalise classical exponential families, are closed
under conditioning, and have tractable marginal distributions. Their utility is
illustrated on a variety of density estimation, conditional density estimation,
and density estimation with missing data tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13632">Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yifu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1">Yftah Ziser</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1">Edoardo M. Ponti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a></p>
<p>Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14077">Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. (arXiv:2305.14077v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Haas_M/0/1/0/all/0/1">Moritz Haas</a>, <a href="http://arxiv.org/find/stat/1/au:+Holzmuller_D/0/1/0/all/0/1">David Holzm&#xfc;ller</a>, <a href="http://arxiv.org/find/stat/1/au:+Luxburg_U/0/1/0/all/0/1">Ulrike von Luxburg</a>, <a href="http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1">Ingo Steinwart</a></p>
<p>The success of over-parameterized neural networks trained to near-zero
training error has caused great interest in the phenomenon of benign
overfitting, where estimators are statistically consistent even though they
interpolate noisy training data. While benign overfitting in fixed dimension
has been established for some learning methods, current literature suggests
that for regression with typical kernel methods and wide neural networks,
benign overfitting requires a high-dimensional setting where the dimension
grows with the sample size. In this paper, we show that the smoothness of the
estimators, and not the dimension, is the key: benign overfitting is possible
if and only if the estimator's derivatives are large enough. We generalize
existing inconsistency results to non-interpolating models and more kernels to
show that benign overfitting with moderate derivatives is impossible in fixed
dimension. Conversely, we show that rate-optimal benign overfitting is possible
for regression with a sequence of spiky-smooth kernels with large derivatives.
Using neural tangent kernels, we translate our results to wide neural networks.
We prove that while infinite-width networks do not overfit benignly with the
ReLU activation, this can be fixed by adding small high-frequency fluctuations
to the activation function. Our experiments verify that such neural networks,
while overfitting, can indeed generalize well even on low-dimensional data
sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14267">SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models. (arXiv:2305.14267v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1">Martin Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1">Nelson Fernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Thuy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Gherbi_E/0/1/0/all/0/1">Elies Gherbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1">Hatem Hajri</a>, <a href="http://arxiv.org/find/cs/1/au:+Masmoudi_N/0/1/0/all/0/1">Nader Masmoudi</a></p>
<p>A potent class of generative models known as Diffusion Probabilistic Models
(DPMs) has become prominent. A forward diffusion process adds gradually noise
to data, while a model learns to gradually denoise. Sampling from pre-trained
DPMs is obtained by solving differential equations (DE) defined by the learnt
model, a process which has shown to be prohibitively slow. Numerous efforts on
speeding-up this process have consisted on crafting powerful ODE solvers.
Despite being quick, such solvers do not usually reach the optimal quality
achieved by available slow SDE solvers. Our goal is to propose SDE solvers that
reach optimal quality without requiring several hundreds or thousands of NFEs
to achieve that goal. We propose Stochastic Explicit Exponential
Derivative-free Solvers (SEEDS), improving and generalizing Exponential
Integrator approaches to the stochastic case on several frameworks. After
carefully analyzing the formulation of exact solutions of diffusion SDEs, we
craft SEEDS to analytically compute the linear part of such solutions. Inspired
by the Exponential Time-Differencing method, SEEDS use a novel treatment of the
stochastic components of solutions, enabling the analytical computation of
their variance, and contains high-order terms allowing to reach optimal quality
sampling $\sim3$-$5\times$ faster than previous SDE methods. We validate our
approach on several image generation benchmarks, showing that SEEDS outperform
or are competitive with previous SDE solvers. Contrary to the latter, SEEDS are
derivative and training free, and we fully prove strong convergence guarantees
for them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14858">Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zixuan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiaqi Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hanqing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1">David Z. Pan</a></p>
<p>Transformers have achieved great success in machine learning applications.
Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root
Mean Square Normalization (RMSNorm), play a critical role in accelerating and
stabilizing the training of Transformers. While LayerNorm recenters and
rescales input vectors, RMSNorm only rescales the vectors by their RMS value.
Despite being more computationally efficient, RMSNorm may compromise the
representation ability of Transformers. There is currently no consensus
regarding the preferred normalization technique, as some models employ
LayerNorm while others utilize RMSNorm, especially in recent large language
models. It is challenging to convert Transformers with one normalization to the
other type. While there is an ongoing disagreement between the two
normalization types, we propose a solution to unify two mainstream Transformer
architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent
redundant mean information in the main branch of Pre-LN Transformers, we can
reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose
the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a
lossless compression of the zero-mean vectors. We formally establish the
equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in
both training and inference. It implies that Pre-LN Transformers can be
substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the
same arithmetic functionality along with free efficiency improvement.
Experiments demonstrate that we can reduce the training and inference time of
Pre-LN Transformers by 1% - 10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14943">Learning Rate Free Bayesian Inference in Constrained Domains. (arXiv:2305.14943v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sharrock_L/0/1/0/all/0/1">Louis Sharrock</a>, <a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1">Lester Mackey</a>, <a href="http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1">Christopher Nemeth</a></p>
<p>We introduce a suite of new particle-based algorithms for sampling on
constrained domains which are entirely learning rate free. Our approach
leverages coin betting ideas from convex optimisation, and the viewpoint of
constrained sampling as a mirrored optimisation problem on the space of
probability measures. Based on this viewpoint, we also introduce a unifying
framework for several existing constrained sampling algorithms, including
mirrored Langevin dynamics and mirrored Stein variational gradient descent. We
demonstrate the performance of our algorithms on a range of numerical examples,
including sampling from targets on the simplex, sampling with fairness
constraints, and constrained sampling problems in post-selection inference. Our
results indicate that our algorithms achieve competitive performance with
existing constrained sampling methods, without the need to tune any
hyperparameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15807">Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness. (arXiv:2305.15807v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chzhen_E/0/1/0/all/0/1">Evgenii Chzhen</a> (LMO, CELESTE), <a href="http://arxiv.org/find/stat/1/au:+Giraud_C/0/1/0/all/0/1">Christophe Giraud</a> (LMO, CELESTE), <a href="http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Stoltz_G/0/1/0/all/0/1">Gilles Stoltz</a> (LMO, CELESTE, HEC Paris)</p>
<p>We consider contextual bandit problems with knapsacks [CBwK], a problem where
at each round, a scalar reward is obtained and vector-valued costs are
suffered. The learner aims to maximize the cumulative rewards while ensuring
that the cumulative costs are lower than some predetermined cost constraints.
We assume that contexts come from a continuous set, that costs can be signed,
and that the expected reward and cost functions, while unknown, may be
uniformly estimated -- a typical assumption in the literature. In this setting,
total cost constraints had so far to be at least of order $T^{3/4}$, where $T$
is the number of rounds, and were even typically assumed to depend linearly on
$T$. We are however motivated to use CBwK to impose a fairness constraint of
equalized average costs between groups: the budget associated with the
corresponding cost constraints should be as close as possible to the natural
deviations, of order $\sqrt{T}$. To that end, we introduce a dual strategy
based on projected-gradient-descent updates, that is able to deal with
total-cost constraints of the order of $\sqrt{T}$ up to poly-logarithmic terms.
This strategy is more direct and simpler than existing strategies in the
literature. It relies on a careful, adaptive, tuning of the step size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16150">Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franceschi_J/0/1/0/all/0/1">Jean-Yves Franceschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1">Mike Gartrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1">Ludovic Dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1">Thibaut Issenhuth</a>, <a href="http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1">Emmanuel de B&#xe9;zenac</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Micka&#xeb;l Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1">Alain Rakotomamonjy</a></p>
<p>Particle-based deep generative models, such as gradient flows and score-based
diffusion models, have recently gained traction thanks to their striking
performance. Their principle of displacing particle distributions using
differential equations is conventionally seen as opposed to the previously
widespread generative adversarial networks (GANs), which involve training a
pushforward generator network. In this paper we challenge this interpretation,
and propose a novel framework that unifies particle and adversarial generative
models by framing generator training as a generalization of particle models.
This suggests that a generator is an optional addition to any such generative
model. Consequently, integrating a generator into a score-based diffusion model
and training a GAN without a generator naturally emerge from our framework. We
empirically test the viability of these original models as proofs of concepts
of potential applications of our framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16264">Scaling Data-Constrained Language Models. (arXiv:2305.16264v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1">Niklas Muennighoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1">Boaz Barak</a>, <a href="http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1">Teven Le Scao</a>, <a href="http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1">Aleksandra Piktus</a>, <a href="http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1">Nouamane Tazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1">Sampo Pyysalo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Thomas Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a></p>
<p>The current trend of scaling language models involves increasing both
parameter count and training dataset size. Extrapolating this trend suggests
that training dataset size may soon be limited by the amount of text data
available on the internet. Motivated by this limit, we investigate scaling
language models in data-constrained regimes. Specifically, we run a large set
of experiments varying the extent of data repetition and compute budget,
ranging up to 900 billion training tokens and 9 billion parameter models. We
find that with constrained data for a fixed compute budget, training with up to
4 epochs of repeated data yields negligible changes to loss compared to having
unique data. However, with more repetition, the value of adding compute
eventually decays to zero. We propose and empirically validate a scaling law
for compute optimality that accounts for the decreasing value of repeated
tokens and excess parameters. Finally, we experiment with approaches mitigating
data scarcity, including augmenting the training dataset with code data or
removing commonly used filters. Models and datasets from our 400 training runs
are freely available at https://github.com/huggingface/datablations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16427">Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seleznova_M/0/1/0/all/0/1">Mariia Seleznova</a>, <a href="http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1">Dana Weitzner</a>, <a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1">Raja Giryes</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1">Gitta Kutyniok</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_H/0/1/0/all/0/1">Hung-Hsu Chou</a></p>
<p>This work bridges two important concepts: the Neural Tangent Kernel (NTK),
which captures the evolution of deep neural networks (DNNs) during training,
and the Neural Collapse (NC) phenomenon, which refers to the emergence of
symmetry and structure in the last-layer features of well-trained
classification DNNs. We adopt the natural assumption that the empirical NTK
develops a block structure aligned with the class labels, i.e., samples within
the same class have stronger correlations than samples from different classes.
Under this assumption, we derive the dynamics of DNNs trained with mean squared
(MSE) loss and break them into interpretable phases. Moreover, we identify an
invariant that captures the essence of the dynamics, and use it to prove the
emergence of NC in DNNs with block-structured NTK. We provide large-scale
numerical experiments on three common DNN architectures and three benchmark
datasets to support our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16905">Improving Neural Additive Models with Bayesian Principles. (arXiv:2305.16905v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bouchiat_K/0/1/0/all/0/1">Kouroche Bouchiat</a>, <a href="http://arxiv.org/find/stat/1/au:+Immer_A/0/1/0/all/0/1">Alexander Immer</a>, <a href="http://arxiv.org/find/stat/1/au:+Yeche_H/0/1/0/all/0/1">Hugo Y&#xe8;che</a>, <a href="http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1">Gunnar R&#xe4;tsch</a>, <a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1">Vincent Fortuin</a></p>
<p>Neural additive models (NAMs) can improve the interpretability of deep neural
networks by handling input features in separate additive sub-networks. However,
they lack inherent mechanisms that provide calibrated uncertainties and enable
selection of relevant features and interactions. Approaching NAMs from a
Bayesian perspective, we enhance them in three primary ways, namely by a)
providing credible intervals for the individual additive sub-networks; b)
estimating the marginal likelihood to perform an implicit selection of features
via an empirical Bayes procedure; and c) enabling a ranking of feature pairs as
candidates for second-order interaction in fine-tuned models. In particular, we
develop Laplace-approximated NAMs (LA-NAMs), which show improved empirical
performance on tabular datasets and challenging real-world medical tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17380">No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1">Tiancheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouyer_C/0/1/0/all/0/1">Chlo&#xe9; Rouyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1">William Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chen-Yu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haipeng Luo</a></p>
<p>Existing online learning algorithms for adversarial Markov Decision Processes
achieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if the
loss functions are chosen arbitrarily by an adversary, with the caveat that the
transition function has to be fixed. This is because it has been shown that
adversarial transition functions make no-regret learning impossible. Despite
such impossibility results, in this work, we develop algorithms that can handle
both adversarial losses and adversarial transitions, with regret increasing
smoothly in the degree of maliciousness of the adversary. More concretely, we
first propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} +
C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial the
transition functions are and can be at most ${O}(T)$. While this algorithm
itself requires knowledge of $C^{\textsf{P}}$, we further develop a black-box
reduction approach that removes this requirement. Moreover, we also show that
further refinements of the algorithm not only maintains the same regret bound,
but also simultaneously adapts to easier environments (where losses are
generated in a certain stochastically constrained manner as in Jin et al.
[2021]) and achieves $\widetilde{{O}}(U + \sqrt{UC^{\textsf{L}}} +
C^{\textsf{P}})$ regret, where $U$ is some standard gap-dependent coefficient
and $C^{\textsf{L}}$ is the amount of corruption on losses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18353">Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tosato_N/0/1/0/all/0/1">Niccol&#xf2; Tosato</a>, <a href="http://arxiv.org/find/cs/1/au:+Basile_L/0/1/0/all/0/1">Lorenzo Basile</a>, <a href="http://arxiv.org/find/cs/1/au:+Ballarin_E/0/1/0/all/0/1">Emanuele Ballarin</a>, <a href="http://arxiv.org/find/cs/1/au:+Alteriis_G/0/1/0/all/0/1">Giuseppe de Alteriis</a>, <a href="http://arxiv.org/find/cs/1/au:+Cazzaniga_A/0/1/0/all/0/1">Alberto Cazzaniga</a>, <a href="http://arxiv.org/find/cs/1/au:+Ansuini_A/0/1/0/all/0/1">Alessio Ansuini</a></p>
<p>The Backpropagation algorithm has often been criticised for its lack of
biological realism. In an attempt to find a more biologically plausible
alternative, the recently introduced Forward-Forward algorithm replaces the
forward and backward passes of Backpropagation with two forward passes. In this
work, we show that the internal representations obtained by the Forward-Forward
algorithm can organise into category-specific ensembles exhibiting high
sparsity - i.e. composed of an extremely low number of active units. This
situation is reminiscent of what has been observed in cortical sensory areas,
where neuronal ensembles are suggested to serve as the functional building
blocks for perception and action. Interestingly, while this sparse pattern does
not typically arise in models trained with standard Backpropagation, it can
emerge in networks trained with Backpropagation on the same objective proposed
for the Forward-Forward algorithm. These results suggest that the learning
procedure proposed by Forward-Forward may be superior to Backpropagation in
modelling learning in the cortex, even when a backward pass is used.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19470">Label Embedding via Low-Coherence Matrices. (arXiv:2305.19470v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1">Clayton Scott</a></p>
<p>Label embedding is a framework for multiclass classification problems where
each label is represented by a distinct vector of some fixed dimension, and
training involves matching model output to the vector representing the correct
label. While label embedding has been successfully applied in extreme
classification and zero-shot learning, and offers both computational and
statistical advantages, its theoretical foundations remain poorly understood.
This work presents an analysis of label embedding in the context of extreme
multiclass classification, where the number of classes $C$ is very large. We
present an excess risk bound that reveals a trade-off between computational and
statistical efficiency, quantified via the coherence of the embedding matrix.
We further show that under the Massart noise condition, the statistical penalty
for label embedding vanishes with sufficiently low coherence. Our analysis
supports an algorithm that is simple, scalable, and easily parallelizable, and
experimental results demonstrate its effectiveness in large-scale applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19693">Spontaneous Symmetry Breaking in Generative Diffusion Models. (arXiv:2305.19693v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raya_G/0/1/0/all/0/1">Gabriel Raya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambrogioni_L/0/1/0/all/0/1">Luca Ambrogioni</a></p>
<p>Generative diffusion models have recently emerged as a leading approach for
generating high-dimensional data. In this paper, we show that the dynamics of
these models exhibit a spontaneous symmetry breaking that divides the
generative dynamics into two distinct phases: 1) A linear steady-state dynamics
around a central fixed-point and 2) an attractor dynamics directed towards the
data manifold. These two "phases" are separated by the change in stability of
the central fixed-point, with the resulting window of instability being
responsible for the diversity of the generated samples. Using both theoretical
and empirical evidence, we show that an accurate simulation of the early
dynamics does not significantly contribute to the final generation, since early
fluctuations are reverted to the central fixed point. To leverage this insight,
we propose a Gaussian late initialization scheme, which significantly improves
model performance, achieving up to 3x FID improvements on fast samplers, while
also increasing sample diversity (e.g., racial composition of generated CelebA
images). Our work offers a new way to understand the generative dynamics of
diffusion models that has the potential to bring about higher performance and
less biased fast-samplers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20081">Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1">Bingyi Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Chao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1">Tianyu Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a></p>
<p>Offline reinforcement learning (RL) aims to learn optimal policies from
offline datasets, where the parameterization of policies is crucial but often
overlooked. Recently, Diffsuion-QL significantly boosts the performance of
offline RL by representing a policy with a diffusion model, whose success
relies on a parametrized Markov Chain with hundreds of steps for sampling.
However, Diffusion-QL suffers from two critical limitations. 1) It is
computationally inefficient to forward and backward through the whole Markov
chain during training. 2) It is incompatible with maximum likelihood-based RL
algorithms (e.g., policy gradient methods) as the likelihood of diffusion
models is intractable. Therefore, we propose efficient diffusion policy (EDP)
to overcome these two challenges. EDP approximately constructs actions from
corrupted ones at training to avoid running the sampling chain. We conduct
extensive experiments on the D4RL benchmark. The results show that EDP can
reduce the diffusion policy training time from 5 days to 5 hours on
gym-locomotion tasks. Moreover, we show that EDP is compatible with various
offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on
D4RL by large margins over previous methods. Our code is available at
https://github.com/sail-sg/edp.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00650">Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction. (arXiv:2306.00650v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marsden_R/0/1/0/all/0/1">Robert A. Marsden</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1">Mario D&#xf6;bler</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a></p>
<p>Since distribution shifts are likely to occur during test-time and can
drastically decrease the model's performance, online test-time adaptation (TTA)
continues to update the model after deployment, leveraging the current test
data. Clearly, a method proposed for online TTA has to perform well for all
kinds of environmental conditions. By introducing the variable factors domain
non-stationarity and temporal correlation, we first unfold all practically
relevant settings and define the entity as universal TTA. We want to highlight
that this is the first work that covers such a broad spectrum, which is
indispensable for the use in practice. To tackle the problem of universal TTA,
we identify and highlight several challenges a self-training based method has
to deal with: 1) model bias and the occurrence of trivial solutions when
performing entropy minimization on varying sequence lengths with and without
multiple domain shifts, 2) loss of generalization which exacerbates the
adaptation to multiple domain shifts and the occurrence of catastrophic
forgetting, and 3) performance degradation due to shifts in class prior. To
prevent the model from becoming biased, we leverage a dataset and
model-agnostic certainty and diversity weighting. In order to maintain
generalization and prevent catastrophic forgetting, we propose to continually
weight-average the source and adapted model. To compensate for disparities in
the class prior during test-time, we propose an adaptive prior correction
scheme that reweights the model's predictions. We evaluate our approach, named
ROID, on a wide range of settings, datasets, and models, setting new standards
in the field of universal TTA. Code is available at:
https://github.com/mariodoebler/test-time-adaptation
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01874">SACSoN: Scalable Autonomous Control for Social Navigation. (arXiv:2306.01874v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hirose_N/0/1/0/all/0/1">Noriaki Hirose</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Dhruv Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1">Ajay Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>Machine learning provides a powerful tool for building socially compliant
robotic systems that go beyond simple predictive models of human behavior. By
observing and understanding human interactions from past experiences, learning
can enable effective social navigation behaviors directly from data. In this
paper, our goal is to develop methods for training policies for socially
unobtrusive navigation, such that robots can navigate among humans in ways that
don't disturb human behavior. We introduce a definition for such behavior based
on the counterfactual perturbation of the human: if the robot had not intruded
into the space, would the human have acted in the same way? By minimizing this
counterfactual perturbation, we can induce robots to behave in ways that do not
alter the natural behavior of humans in the shared space. Instantiating this
principle requires training policies to minimize their effect on human
behavior, and this in turn requires data that allows us to model the behavior
of humans in the presence of robots. Therefore, our approach is based on two
key contributions. First, we collect a large dataset where an indoor mobile
robot interacts with human bystanders. Second, we utilize this dataset to train
policies that minimize counterfactual perturbation. We provide supplementary
videos and make publicly available the largest-of-its-kind visual navigation
dataset on our project page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04220">Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianyuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shoucheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youfang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Li Jiang</a></p>
<p>Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.Code is available at: https://github.com/pcheng2/TSRL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04618">Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lifan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1">Ganqu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hongcheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_F/0/1/0/all/0/1">Fangyuan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xingyi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>This paper reexamines the research on out-of-distribution (OOD) robustness in
the field of NLP. We find that the distribution shift settings in previous
studies commonly lack adequate challenges, hindering the accurate evaluation of
OOD robustness. To address these issues, we propose a benchmark construction
protocol that ensures clear differentiation and challenging distribution
shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution
robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we
conduct a series of experiments on pre-trained language models for analysis and
evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the
relationship between in-distribution (ID) and OOD performance. We identify
three typical types that unveil the inner learning mechanism, which could
potentially facilitate the forecasting of OOD robustness, correlating with the
advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and
find that, despite exhibiting some effectiveness in specific cases, they do not
offer significant improvement compared to vanilla fine-tuning. Further, we
evaluate 5 LLMs with various adaptation paradigms and find that when sufficient
ID data is available, fine-tuning domain-specific models outperform LLMs on ID
examples significantly. However, in the case of OOD instances, prioritizing
LLMs with in-context learning yields better results. We identify that both
fine-tuned small models and LLMs face challenges in effectively addressing
downstream tasks. The code is public at
\url{https://github.com/lifan-yuan/OOD_NLP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05557">On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v3 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1">Donald Loveland</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heimann_M/0/1/0/all/0/1">Mark Heimann</a>, <a href="http://arxiv.org/find/cs/1/au:+Fish_B/0/1/0/all/0/1">Benjamin Fish</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaub_M/0/1/0/all/0/1">Michael T. Shaub</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1">Danai Koutra</a></p>
<p>Graph Neural Network (GNN) research has highlighted a relationship between
high homophily (i.e., the tendency of nodes of the same class to connect) and
strong predictive performance in node classification. However, recent work has
found the relationship to be more nuanced, demonstrating that simple GNNs can
learn in certain heterophilous settings. To resolve these conflicting findings
and align closer to real-world datasets, we go beyond the assumption of a
global graph homophily level and study the performance of GNNs when the local
homophily level of a node deviates from the global homophily level. Through
theoretical and empirical analysis, we systematically demonstrate how shifts in
local homophily can introduce performance degradation, leading to performance
discrepancies across local homophily levels. We ground the practical
implications of this work through granular analysis on five real-world datasets
with varying global homophily levels, demonstrating that (a) GNNs can fail to
generalize to test nodes that deviate from the global homophily of a graph, and
(b) high local homophily does not necessarily confer high performance for a
node. We further show that GNNs designed for globally heterophilous graphs can
alleviate performance discrepancy by improving performance across local
homophily levels, offering a new perspective on how these GNNs achieve stronger
global performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07163">A Batch-to-Online Transformation under Random-Order Model. (arXiv:2306.07163v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jing Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshida_Y/0/1/0/all/0/1">Yuichi Yoshida</a></p>
<p>We introduce a transformation framework that can be utilized to develop
online algorithms with low $\epsilon$-approximate regret in the random-order
model from offline approximation algorithms. We first give a general reduction
theorem that transforms an offline approximation algorithm with low average
sensitivity to an online algorithm with low $\epsilon$-approximate regret. We
then demonstrate that offline approximation algorithms can be transformed into
a low-sensitivity version using a coreset construction method. To showcase the
versatility of our approach, we apply it to various problems, including online
$(k,z)$-clustering, online matrix approximation, and online regression, and
successfully achieve polylogarithmic $\epsilon$-approximate regret for each
problem. Moreover, we show that in all three cases, our algorithm also enjoys
low inconsistency, which may be desired in some online applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07273">Gaussian Membership Inference Privacy. (arXiv:2306.07273v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leemann_T/0/1/0/all/0/1">Tobias Leemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1">Martin Pawelczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>We propose a novel and practical privacy notion called $f$-Membership
Inference Privacy ($f$-MIP), which explicitly considers the capabilities of
realistic adversaries under the membership inference attack threat model.
Consequently, $f$-MIP offers interpretable privacy guarantees and improved
utility (e.g., better classification accuracy). In particular, we derive a
parametric family of $f$-MIP guarantees that we refer to as $\mu$-Gaussian
Membership Inference Privacy ($\mu$-GMIP) by theoretically analyzing likelihood
ratio-based membership inference attacks on stochastic gradient descent (SGD).
Our analysis highlights that models trained with standard SGD already offer an
elementary level of MIP. Additionally, we show how $f$-MIP can be amplified by
adding noise to gradient updates. Our analysis further yields an analytical
membership inference attack that offers two distinct advantages over previous
approaches. First, unlike existing state-of-the-art attacks that require
training hundreds of shadow models, our attack does not require any shadow
model. Second, our analytical attack enables straightforward auditing of our
privacy notion $f$-MIP. Finally, we quantify how various hyperparameters (e.g.,
batch size, number of model parameters) and specific data characteristics
determine an attacker's ability to accurately infer a point's membership in the
training set. We demonstrate the effectiveness of our method on models trained
on vision and tabular datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07923">Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits. (arXiv:2306.07923v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lequn Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1">Akshay Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1">Aleksandrs Slivkins</a></p>
<p>We consider offline policy optimization (OPO) in contextual bandits, where
one is given a fixed dataset of logged interactions. While pessimistic
regularizers are typically used to mitigate distribution shift, prior
implementations thereof are either specialized or computationally inefficient.
We present the first general oracle-efficient algorithm for pessimistic OPO: it
reduces to supervised learning, leading to broad applicability. We obtain
statistical guarantees analogous to those for prior pessimistic approaches. We
instantiate our approach for both discrete and continuous actions and perform
experiments in both settings, showing advantage over unregularized OPO across a
wide range of configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08645">Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis. (arXiv:2306.08645v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhiyu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xuli Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xiangyang Xue</a></p>
<p>Diffusion models (DMs) have recently gained attention with state-of-the-art
performance in text-to-image synthesis. Abiding by the tradition in deep
learning, DMs are trained and evaluated on the images with fixed sizes.
However, users are demanding for various images with specific sizes and various
aspect ratio. This paper focuses on adapting text-to-image diffusion models to
handle such variety while maintaining visual fidelity. First we observe that,
during the synthesis, lower resolution images suffer from incomplete object
portrayal, while higher resolution images exhibit repetitively disordered
presentation. Next, we establish a statistical relationship indicating that
attention entropy changes with token quantity, suggesting that models aggregate
spatial information in proportion to image resolution. The subsequent
interpretation on our observations is that objects are incompletely depicted
due to limited spatial information for low resolutions, while repetitively
disorganized presentation arises from redundant spatial information for high
resolutions. From this perspective, we propose a scaling factor to alleviate
the change of attention entropy and mitigate the defective pattern observed.
Extensive experimental results validate the efficacy of the proposed scaling
factor, enabling models to achieve better visual effects, image quality, and
text alignment. Notably, these improvements are achieved without additional
training or fine-tuning techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13004">Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalra_A/0/1/0/all/0/1">Akansha Kalra</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Daniel S. Brown</a></p>
<p>There is an increasing interest in learning reward functions that model human
preferences. However, many frameworks use blackbox learning methods that, while
expressive, are difficult to interpret. We propose and evaluate a novel
approach for learning expressive and interpretable reward functions from
preferences using Differentiable Decision Trees (DDTs). Our experiments across
several domains, including CartPole, Visual Gridworld environments and Atari
games, provide evidence that that the tree structure of our learned reward
function is useful in determining the extent to which the reward function is
aligned with human preferences. We provide experimental evidence that reward
DDTs can achieve competitive performance when compared with larger capacity
deep neural network reward functions. We also observe that the choice between
soft and hard (argmax) output of reward DDT reveals a tension between wanting
highly shaped rewards to ensure good RL performance, while also wanting
simpler, more interpretable rewards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14534">CEIL: Generalized Contextual Imitation Learning. (arXiv:2306.14534v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Li He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yachen Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a></p>
<p>In this paper, we present \textbf{C}ont\textbf{E}xtual \textbf{I}mitation
\textbf{L}earning~(CEIL), a general and broadly applicable algorithm for
imitation learning (IL). Inspired by the formulation of hindsight information
matching, we derive CEIL by explicitly learning a hindsight embedding function
together with a contextual policy using the hindsight embeddings. To achieve
the expert matching objective for IL, we advocate for optimizing a contextual
variable such that it biases the contextual policy towards mimicking expert
behaviors. Beyond the typical learning from demonstrations (LfD) setting, CEIL
is a generalist that can be effectively applied to multiple settings including:
1)~learning from observations (LfO), 2)~offline IL, 3)~cross-domain IL
(mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate
CEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline).
Compared to prior state-of-the-art baselines, we show that CEIL is more
sample-efficient in most online IL tasks and achieves better or competitive
performances in offline tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15012">Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Blancard_B/0/1/0/all/0/1">Bruno R&#xe9;galdo-Saint Blancard</a>, <a href="http://arxiv.org/find/stat/1/au:+Eickenberg_M/0/1/0/all/0/1">Michael Eickenberg</a></p>
<p>Separating signals from an additive mixture may be an unnecessarily hard
problem when one is only interested in specific properties of a given signal.
In this work, we tackle simpler "statistical component separation" problems
that focus on recovering a predefined set of statistical descriptors of a
target signal from a noisy mixture. Assuming access to samples of the noise
process, we investigate a method devised to match the statistics of the
solution candidate corrupted by noise samples with those of the observed
mixture. We first analyze the behavior of this method using simple examples
with analytically tractable calculations. Then, we apply it in an image
denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based
descriptors on astrophysics and ImageNet data. In the case of 1), we show that
our method better recovers the descriptors of the target data than a standard
denoising method in most situations. Additionally, despite not constructed for
this purpose, it performs surprisingly well in terms of peak signal-to-noise
ratio on full signal reconstruction. In comparison, representation 2) appears
less suitable for image denoising. Finally, we extend this method by
introducing a diffusive stepwise algorithm which gives a new perspective to the
initial method and leads to promising results for image denoising under
specific circumstances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01951">A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kothapalli_V/0/1/0/all/0/1">Vignesh Kothapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tirer_T/0/1/0/all/0/1">Tom Tirer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1">Joan Bruna</a></p>
<p>Graph neural networks (GNNs) have become increasingly popular for
classification tasks on graph-structured data. Yet, the interplay between graph
topology and feature evolution in GNNs is not well understood. In this paper,
we focus on node-wise classification, illustrated with community detection on
stochastic block model graphs, and explore the feature evolution through the
lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep
classifiers (e.g. for image classification) beyond the zero training error
point, NC demonstrates a reduction in the deepest features' within-class
variability and an increased alignment of their class means to certain
symmetric structures. We start with an empirical study that shows that a
decrease in within-class variability is also prevalent in the node-wise
classification setting, however, not to the extent observed in the
instance-wise case. Then, we theoretically study this distinction.
Specifically, we show that even an "optimistic" mathematical model requires
that the graphs obey a strict structural condition in order to possess a
minimizer with exact collapse. Interestingly, this condition is viable also for
heterophilic graphs and relates to recent empirical studies on settings with
improved GNNs' generalization. Furthermore, by studying the gradient dynamics
of the theoretical model, we provide reasoning for the partial collapse
observed empirically. Finally, we present a study on the evolution of within-
and between-class feature variability across layers of a well-trained GNN and
contrast the behavior with spectral methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04110">Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Iakovlev_V/0/1/0/all/0/1">Valerii Iakovlev</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1">Markus Heinonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahdesmaki_H/0/1/0/all/0/1">Harri L&#xe4;hdesm&#xe4;ki</a></p>
<p>We introduce a novel grid-independent model for learning partial differential
equations (PDEs) from noisy and partial observations on irregular
spatiotemporal grids. We propose a space-time continuous latent neural PDE
model with an efficient probabilistic framework and a novel encoder design for
improved data efficiency and grid independence. The latent state dynamics are
governed by a PDE model that combines the collocation method and the method of
lines. We employ amortized variational inference for approximate posterior
estimation and utilize a multiple shooting technique for enhanced training
speed and stability. Our model demonstrates state-of-the-art performance on
complex synthetic and real-world datasets, overcoming limitations of previous
approaches and effectively handling partially-observed data. The proposed model
outperforms recent methods, showing its potential to advance data-driven PDE
modeling and enabling robust, grid-independent modeling of complex
partially-observed dynamic processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04204">Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory. (arXiv:2307.04204v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Minhak Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1">Chulhee Yun</a></p>
<p>Cohen et al. (2021) empirically study the evolution of the largest eigenvalue
of the loss Hessian, also known as sharpness, along the gradient descent (GD)
trajectory and observe the Edge of Stability (EoS) phenomenon. The sharpness
increases at the early phase of training (referred to as progressive
sharpening), and eventually saturates close to the threshold of $2 /
\text{(step size)}$. In this paper, we start by demonstrating through empirical
studies that when the EoS phenomenon occurs, different GD trajectories (after a
proper reparameterization) align on a specific bifurcation diagram independent
of initialization. We then rigorously prove this trajectory alignment
phenomenon for a two-layer fully-connected linear network and a single-neuron
nonlinear network trained with a single data point. Our trajectory alignment
analysis establishes both progressive sharpening and EoS phenomena,
encompassing and extending recent findings in the literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05891">PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Char_I/0/1/0/all/0/1">Ian Char</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jeff Schneider</a></p>
<p>Deep reinforcement learning (RL) has shown immense potential for learning to
control systems through data alone. However, one challenge deep RL faces is
that the full state of the system is often not observable. When this is the
case, the policy needs to leverage the history of observations to infer the
current state. At the same time, differences between the training and testing
environments makes it critical for the policy not to overfit to the sequence of
observations it sees at training time. As such, there is an important balancing
act between having the history encoder be flexible enough to extract relevant
information, yet be robust to changes in the environment. To strike this
balance, we look to the PID controller for inspiration. We assert the PID
controller's success shows that only summing and differencing are needed to
accumulate information over time for many control tasks. Following this
principle, we propose two architectures for encoding history: one that directly
uses PID features and another that extends these core ideas and can be used in
arbitrary control tasks. When compared with prior approaches, our encoders
produce policies that are often more robust and achieve better performance on a
variety of tracking tasks. Going beyond tracking tasks, our policies achieve
1.7x better performance on average over previous state-of-the-art methods on a
suite of locomotion control tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07063">Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1">Yiren Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chongyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1">Soroush Vosoughi</a></p>
<p>We present a novel methodology aimed at optimizing the application of frozen
large language models (LLMs) for resource-intensive vision-language (VL)
pre-training. The current paradigm uses visual features as prompts to guide
language models, with a focus on determining the most relevant visual features
for corresponding text. Our approach diverges by concentrating on the language
component, specifically identifying the optimal prompts to align with visual
features. We introduce the Prompt-Transformer (P-Former), a model that predicts
these ideal prompts, which is trained exclusively on linguistic data, bypassing
the need for image-text pairings. This strategy subtly bifurcates the
end-to-end VL training process into an additional, separate stage. Our
experiments reveal that our framework significantly enhances the performance of
a robust image-to-text baseline (BLIP-2), and effectively narrows the
performance gap between models trained with either 4M or 129M image-text pairs.
Importantly, our framework is modality-agnostic and flexible in terms of
architectural design, as validated by its successful application in a video
learning task using varied base modules. The code will be made available at
https://github.com/yiren-jian/BLIText.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07907">Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation. (arXiv:2307.07907v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenhao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Laixi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1">Yuejie Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a></p>
<p>Robustness has been extensively studied in reinforcement learning (RL) to
handle various forms of uncertainty such as random perturbations, rare events,
and malicious attacks. In this work, we consider one critical type of
robustness against spurious correlation, where different portions of the state
do not have correlations induced by unobserved confounders. These spurious
correlations are ubiquitous in real-world tasks, for instance, a self-driving
car usually observes heavy traffic in the daytime and light traffic at night
due to unobservable human activity. A model that learns such useless or even
harmful correlation could catastrophically fail when the confounder in the test
case deviates from the training one. Although motivated, enabling robustness
against spurious correlation poses significant challenges since the uncertainty
set, shaped by the unobserved confounder and causal structure, is difficult to
characterize and identify. Existing robust algorithms that assume simple and
unstructured uncertainty sets are therefore inadequate to address this
challenge. To solve this issue, we propose Robust State-Confounded Markov
Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in
avoiding learning spurious correlations compared with other robust RL
counterparts. We also design an empirical algorithm to learn the robust optimal
policy for RSC-MDPs, which outperforms all baselines in eight realistic
self-driving and manipulation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10350">Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thao Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1">Samir Yitzhak Gadre</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1">Gabriel Ilharco</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Sewoong Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1">Ludwig Schmidt</a></p>
<p>Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp's large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity. The synthetic captions used in our experiments are now available on
HuggingFace.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10922">Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1">Kanchana Ranasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael Ryoo</a></p>
<p>Recent contrastive language image pre-training has led to learning highly
transferable and robust image representations. However, adapting these models
to video domains with minimal supervision remains an open problem. We explore a
simple step in that direction, using language tied self-supervised learning to
adapt an image CLIP model to the video domain. A backbone modified for temporal
modeling is trained under self-distillation settings with train objectives
operating in an action concept space. Feature vectors of various action
concepts extracted from a language encoder using relevant textual prompts
construct this space. We introduce two train objectives, concept distillation
and concept alignment, that retain generality of original representations while
enforcing relations between actions and their attributes. Our approach improves
zero-shot and linear probing performance on three action recognition
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14338">TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023. (arXiv:2307.14338v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1">Yury Gorishniy</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1">Ivan Rubachev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kartashev_N/0/1/0/all/0/1">Nikolay Kartashev</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlenskii_D/0/1/0/all/0/1">Daniil Shlenskii</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotelnikov_A/0/1/0/all/0/1">Akim Kotelnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a></p>
<p>Deep learning (DL) models for tabular data problems (e.g. classification,
regression) are currently receiving increasingly more attention from
researchers. However, despite the recent efforts, the non-DL algorithms based
on gradient-boosted decision trees (GBDT) remain a strong go-to solution for
these problems. One of the research directions aimed at improving the position
of tabular DL involves designing so-called retrieval-augmented models. For a
target object, such models retrieve other objects (e.g. the nearest neighbors)
from the available training data and use their features and labels to make a
better prediction.
</p>
<p>In this work, we present TabR -- essentially, a feed-forward network with a
custom k-Nearest-Neighbors-like component in the middle. On a set of public
benchmarks with datasets up to several million objects, TabR marks a big step
forward for tabular DL: it demonstrates the best average performance among
tabular DL models, becomes the new state-of-the-art on several datasets, and
even outperforms GBDT models on the recently proposed "GBDT-friendly" benchmark
(see Figure 1). Among the important findings and technical details powering
TabR, the main ones lie in the attention-like mechanism that is responsible for
retrieving the nearest neighbors and extracting valuable signal from them. In
addition to the much higher performance, TabR is simple and significantly more
efficient compared to prior retrieval-based tabular DL models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00855">A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing highly cited and impactful publications across six decades. (arXiv:2308.00855v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ezugwu_A/0/1/0/all/0/1">Absalom E. Ezugwu</a>, <a href="http://arxiv.org/find/cs/1/au:+Greeff_J/0/1/0/all/0/1">Japie Greeff</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1">Yuh-Shan Ho</a></p>
<p>Machine learning (ML) has emerged as a prominent field of research in
computer science and other related fields, thereby driving advancements in
other domains of interest. As the field continues to evolve, it is crucial to
understand the landscape of highly cited publications to identify key trends,
influential authors, and significant contributions made thus far. In this
paper, we present a comprehensive bibliometric analysis of highly cited ML
publications. We collected a dataset consisting of the top-cited papers from
reputable ML conferences and journals, covering a period of several years from
1959 to 2022. We employed various bibliometric techniques to analyze the data,
including citation analysis, co-authorship analysis, keyword analysis, and
publication trends. Our findings reveal the most influential papers, highly
cited authors, and collaborative networks within the machine learning
community. We identify popular research themes and uncover emerging topics that
have recently gained significant attention. Furthermore, we examine the
geographical distribution of highly cited publications, highlighting the
dominance of certain countries in ML research. By shedding light on the
landscape of highly cited ML publications, our study provides valuable insights
for researchers, policymakers, and practitioners seeking to understand the key
developments and trends in this rapidly evolving field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07983">Monte Carlo guided Diffusion for Bayesian linear inverse problems. (arXiv:2308.07983v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cardoso_G/0/1/0/all/0/1">Gabriel Cardoso</a>, <a href="http://arxiv.org/find/stat/1/au:+Idrissi_Y/0/1/0/all/0/1">Yazid Janati El Idrissi</a>, <a href="http://arxiv.org/find/stat/1/au:+Corff_S/0/1/0/all/0/1">Sylvain Le Corff</a>, <a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1">Eric Moulines</a></p>
<p>Ill-posed linear inverse problems arise frequently in various applications,
from computational photography to medical imaging. A recent line of research
exploits Bayesian inference with informative priors to handle the ill-posedness
of such problems. Amongst such priors, score-based generative models (SGM) have
recently been successfully applied to several different inverse problems. In
this study, we exploit the particular structure of the prior defined by the SGM
to define a sequence of intermediate linear inverse problems. As the noise
level decreases, the posteriors of these inverse problems get closer to the
target posterior of the original inverse problem. To sample from this sequence
of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The
proposed algorithm, MCGDiff, is shown to be theoretically grounded and we
provide numerical simulations showing that it outperforms competing baselines
when dealing with ill-posed inverse problems in a Bayesian setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13633">Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Duong_L/0/1/0/all/0/1">Lyndon R. Duong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Simoncelli_E/0/1/0/all/0/1">Eero P. Simoncelli</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chklovskii_D/0/1/0/all/0/1">Dmitri B. Chklovskii</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lipshutz_D/0/1/0/all/0/1">David Lipshutz</a></p>
<p>Neurons in early sensory areas rapidly adapt to changing sensory statistics,
both by normalizing the variance of their individual responses and by reducing
correlations between their responses. Together, these transformations may be
viewed as an adaptive form of statistical whitening. Existing mechanistic
models of adaptive whitening exclusively use either synaptic plasticity or gain
modulation as the biological substrate for adaptation; however, on their own,
each of these models has significant limitations. In this work, we unify these
approaches in a normative multi-timescale mechanistic model that adaptively
whitens its responses with complementary computational roles for synaptic
plasticity and gain modulation. Gains are modified on a fast timescale to adapt
to the current statistical context, whereas synapses are modified on a slow
timescale to match structural properties of the input statistics that are
invariant across contexts. Our model is derived from a novel multi-timescale
whitening objective that factorizes the inverse whitening matrix into basis
vectors, which correspond to synaptic weights, and a diagonal matrix, which
corresponds to neuronal gains. We test our model on synthetic and natural
datasets and find that the synapses learn optimal configurations over long
timescales that enable adaptive whitening on short timescales using gain
modulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01270">COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Denize_J/0/1/0/all/0/1">Julien Denize</a>, <a href="http://arxiv.org/find/cs/1/au:+Liashuha_M/0/1/0/all/0/1">Mykola Liashuha</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1">Jaonary Rabarisoa</a>, <a href="http://arxiv.org/find/cs/1/au:+Orcesi_A/0/1/0/all/0/1">Astrid Orcesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Herault_R/0/1/0/all/0/1">Romain H&#xe9;rault</a></p>
<p>We present COMEDIAN, a novel pipeline to initialize spatiotemporal
transformers for action spotting, which involves self-supervised learning and
knowledge distillation. Action spotting is a timestamp-level temporal action
detection task. Our pipeline consists of three steps, with two initialization
stages. First, we perform self-supervised initialization of a spatial
transformer using short videos as input. Additionally, we initialize a temporal
transformer that enhances the spatial transformer's outputs with global context
through knowledge distillation from a pre-computed feature bank aligned with
each short video segment. In the final step, we fine-tune the transformers to
the action spotting task. The experiments, conducted on the SoccerNet-v2
dataset, demonstrate state-of-the-art performance and validate the
effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several
advantages of our pretraining pipeline, including improved performance and
faster convergence compared to non-pretrained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03004">A Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Ze Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lei Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yinghuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a></p>
<p>A recent empirical observation (Li et al., 2022b) of activation sparsity in
MLP blocks offers an opportunity to drastically reduce computation costs for
free. Although having attributed it to training dynamics, existing theoretical
explanations of activation sparsity are restricted to shallow networks, small
training steps and special training, despite its emergence in deep models
standardly trained for a large number of steps. To fill these gaps, we propose
the notion of gradient sparsity as one source of activation sparsity and a
theoretical explanation based on it that sees sparsity a necessary step to
adversarial robustness w.r.t. hidden features and parameters, which is
approximately the flatness of minima for well-learned models. The theory
applies to standardly trained LayerNorm-ed MLPs, and further to Transformers or
other architectures trained with weight noises. Eliminating other sources of
flatness except for sparsity, we discover the phenomenon that the ratio between
the largest and smallest non-zero singular values of weight matrices is small.
When discussing the emergence of this spectral concentration, we use random
matrix theory (RMT) as a powerful tool to analyze stochastic gradient noises.
Validational experiments are conducted to verify our gradient-sparsity-based
explanation. We propose two plug-and-play modules for both training and
finetuning for sparsity. Experiments on ImageNet-1k and C4 demonstrate their
50% sparsity improvements, indicating further potential cost reduction in both
training and inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07593">Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chamma_A/0/1/0/all/0/1">Ahmad Chamma</a> (1 and 2 and 3), <a href="http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1">Denis A. Engemann</a> (4), <a href="http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1">Bertrand Thirion</a> (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)</p>
<p>Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An experiment on real-world data analysis in a
large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14597">Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahn_N/0/1/0/all/0/1">Nate Rahn</a>, <a href="http://arxiv.org/find/cs/1/au:+DOro_P/0/1/0/all/0/1">Pierluca D&#x27;Oro</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiltzer_H/0/1/0/all/0/1">Harley Wiltzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1">Pierre-Luc Bacon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1">Marc G. Bellemare</a></p>
<p>Deep reinforcement learning agents for continuous control are known to
exhibit significant instability in their performance over time. In this work,
we provide a fresh perspective on these behaviors by studying the return
landscape: the mapping between a policy and a return. We find that popular
algorithms traverse noisy neighborhoods of this landscape, in which a single
update to the policy parameters leads to a wide range of returns. By taking a
distributional view of these returns, we map the landscape, characterizing
failure-prone regions of policy space and revealing a hidden dimension of
policy quality. We show that the landscape exhibits surprising structure by
finding simple paths in parameter space which improve the stability of a
policy. To conclude, we develop a distribution-aware procedure which finds such
paths, navigating away from noisy neighborhoods in order to improve the
robustness of a policy. Taken together, our results provide new insight into
the optimization, evaluation, and design of agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17310">Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jiayuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Borovykh_A/0/1/0/all/0/1">Anastasia Borovykh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayou_S/0/1/0/all/0/1">Soufiane Hayou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shokri_R/0/1/0/all/0/1">Reza Shokri</a></p>
<p>We introduce a new analytical framework to quantify the changes in a machine
learning algorithm's output distribution following the inclusion of a few data
points in its training set, a notion we define as leave-one-out
distinguishability (LOOD). This problem is key to measuring data
**memorization** and **information leakage** in machine learning, and the
**influence** of training data points on model predictions. We illustrate how
our method broadens and refines existing empirical measures of memorization and
privacy risks associated with training data. We use Gaussian processes to model
the randomness of machine learning algorithms, and validate LOOD with extensive
empirical analysis of information leakage using membership inference attacks.
Our theoretical framework enables us to investigate the causes of information
leakage and where the leakage is high. For example, we analyze the influence of
activation functions, on data memorization. Additionally, our method allows us
to optimize queries that disclose the most significant information about the
training data in the leave-one-out setting. We illustrate how optimal queries
can be used for accurate **reconstruction** of training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00270">SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bang An</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yongjian Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a></p>
<p>The problem of urban event ranking aims at predicting the top-k most risky
locations of future events such as traffic accidents and crimes. This problem
is of fundamental importance to public safety and urban administration
especially when limited resources are available. The problem is, however,
challenging due to complex and dynamic spatio-temporal correlations between
locations, uneven distribution of urban events in space, and the difficulty to
correctly rank nearby locations with similar features. Prior works on event
forecasting mostly aim at accurately predicting the actual risk score or counts
of events for all the locations. Rankings obtained as such usually have low
quality due to prediction errors. Learning-to-rank methods directly optimize
measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot
handle the spatiotemporal autocorrelation existing among locations. In this
paper, we bridge the gap by proposing a novel spatial event ranking approach
named SpatialRank. SpatialRank features adaptive graph convolution layers that
dynamically learn the spatiotemporal dependencies across locations from data.
In addition, the model optimizes through surrogates a hybrid NDCG loss with a
spatial component to better rank neighboring spatial locations. We design an
importance-sampling with a spatial filtering algorithm to effectively evaluate
the loss during training. Comprehensive experiments on three real-world
datasets demonstrate that SpatialRank can effectively identify the top riskiest
locations of crimes and traffic accidents and outperform state-of-art methods
in terms of NDCG by up to 12.7%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01551">Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1">Guy Blanc</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_J/0/1/0/all/0/1">Jane Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Pabbaraju_C/0/1/0/all/0/1">Chirag Pabbaraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Sullivan_C/0/1/0/all/0/1">Colin Sullivan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1">Li-Yang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1">Mo Tiwari</a></p>
<p>We propose a simple generalization of standard and empirically successful
decision tree learning algorithms such as ID3, C4.5, and CART. These
algorithms, which have been central to machine learning for decades, are greedy
in nature: they grow a decision tree by iteratively splitting on the best
attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as
possible splits instead of just the single best attribute. We demonstrate,
theoretically and empirically, the power of this simple generalization. We
first prove a {\sl greediness hierarchy theorem} showing that for every $k \in
\mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there
are data distributions for which the former achieves accuracy $1-\varepsilon$,
whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then
show, through extensive experiments, that Top-$k$ outperforms the two main
approaches to decision tree learning: classic greedy algorithms and more recent
"optimal decision tree" algorithms. On one hand, Top-$k$ consistently enjoys
significant accuracy gains over greedy algorithms across a wide range of
benchmarks. On the other hand, Top-$k$ is markedly more scalable than optimal
decision tree algorithms and is able to handle dataset and feature set sizes
that remain far beyond the reach of these algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02255">MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tony Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiacheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1">Michel Galley</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02987">Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions. (arXiv:2310.02987v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xufeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Alacaoglu_A/0/1/0/all/0/1">Ahmet Alacaoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_J/0/1/0/all/0/1">Jelena Diakonikolas</a></p>
<p>Machine learning approaches relying on such criteria as adversarial
robustness or multi-agent settings have raised the need for solving
game-theoretic equilibrium problems. Of particular relevance to these
applications are methods targeting finite-sum structure, which generically
arises in empirical variants of learning problems in these contexts. Further,
methods with computable approximation errors are highly desirable, as they
provide verifiable exit criteria. Motivated by these applications, we study
finite-sum monotone inclusion problems, which model broad classes of
equilibrium problems. Our main contributions are variants of the classical
Halpern iteration that employ variance reduction to obtain improved complexity
guarantees in which $n$ component operators in the finite sum are ``on
average'' either cocoercive or Lipschitz continuous and monotone, with
parameter $L$. The resulting oracle complexity of our methods, which provide
guarantees for the last iterate and for a (computable) operator norm residual,
is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improves
upon existing methods by a factor up to $\sqrt{n}$. This constitutes the first
variance reduction-type result for general finite-sum monotone inclusions and
for more specific problems such as convex-concave optimization when operator
norm residual is the optimality measure. We further argue that, up to
poly-logarithmic factors, this complexity is unimprovable in the monotone
Lipschitz setting; i.e., the provided result is near-optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05858">DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jingliang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Liming Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiaxin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengbo Eben Li</a></p>
<p>Reinforcement learning (RL) has proven to be highly effective in tackling
complex decision-making and control tasks. However, prevalent model-free RL
methods often face severe performance degradation due to the well-known
overestimation issue. In response to this problem, we recently introduced an
off-policy RL algorithm, called distributional soft actor-critic (DSAC or
DSAC-v1), which can effectively improve the value estimation accuracy by
learning a continuous Gaussian value distribution. Nonetheless, standard DSAC
has its own shortcomings, including occasionally unstable learning processes
and needs for task-specific reward scaling, which may hinder its overall
performance and adaptability in some special tasks. This paper further
introduces three important refinements to standard DSAC in order to address
these shortcomings. These refinements consist of critic gradient adjusting,
twin value distribution learning, and variance-based target return clipping.
The modified RL algorithm is named as DSAC with three refinements (DSAC-T or
DSAC-v2), and its performances are systematically evaluated on a diverse set of
benchmark tasks. Without any task-specific hyperparameter tuning, DSAC-T
surpasses a lot of mainstream model-free RL algorithms, including SAC, TD3,
DDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T, unlike
its standard version, ensures a highly stable learning process and delivers
similar performance across varying reward scales.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12350">Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_N/0/1/0/all/0/1">Nan Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiuling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wendy Hui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1">Violet Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1">Yue Ning</a></p>
<p>Graph Neural Networks (GNNs) have been widely used for various types of graph
data processing and analytical tasks in different domains. Training GNNs over
centralized graph data can be infeasible due to privacy concerns and regulatory
restrictions. Thus, federated learning (FL) becomes a trending solution to
address this challenge in a distributed learning paradigm. However, as GNNs may
inherit historical bias from training data and lead to discriminatory
predictions, the bias of local models can be easily propagated to the global
model in distributed settings. This poses a new challenge in mitigating bias in
federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair
Federated Graph Neural Network, that enhances group fairness of federated GNNs.
As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN
aims to mitigate both types of bias under federated settings. First, we provide
theoretical insights on the connection between data bias in a training graph
and statistical fairness metrics of the trained GNN models. Based on the
theoretical analysis, we design $\text{F}^2$GNN which contains two key
components: a fairness-aware local model update scheme that enhances group
fairness of the local models on the client side, and a fairness-weighted global
model update scheme that takes both data bias and fairness metrics of local
models into consideration in the aggregation process. We evaluate
$\text{F}^2$GNN empirically versus a number of baseline methods, and
demonstrate that $\text{F}^2$GNN outperforms these baselines in terms of both
fairness and model accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13268">DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kaiwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianfei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>Diffusion probabilistic models (DPMs) have exhibited excellent performance
for high-fidelity image generation while suffering from inefficient sampling.
Recent works accelerate the sampling procedure by proposing fast ODE solvers
that leverage the specific ODE form of DPMs. However, they highly rely on
specific parameterization during inference (such as noise/data prediction),
which might not be the optimal choice. In this work, we propose a novel
formulation towards the optimal parameterization during sampling that minimizes
the first-order discretization error of the ODE solution. Based on such
formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs
by introducing several coefficients efficiently computed on the pretrained
model, which we call \textit{empirical model statistics}. We further
incorporate multistep methods and a predictor-corrector framework, and propose
some techniques for improving sample quality at small numbers of function
evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3
achieves consistently better or comparable performance in both unconditional
and conditional sampling with both pixel-space and latent-space DPMs,
especially in 5$\sim$10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE)
on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable
Diffusion, bringing a speed-up of 15\%$\sim$30\% compared to previous
state-of-the-art training-free methods. Code is available at
\url{https://github.com/thu-ml/DPM-Solver-v3}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13923">Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation. (arXiv:2310.13923v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jianing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Geng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiangchao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a></p>
<p>Out-of-distribution (OOD) detection is important for deploying reliable
machine learning models on real-world applications. Recent advances in outlier
exposure have shown promising results on OOD detection via fine-tuning model
with informatively sampled auxiliary outliers. However, previous methods assume
that the collected outliers can be sufficiently large and representative to
cover the boundary between ID and OOD data, which might be impractical and
challenging. In this work, we propose a novel framework, namely, Diversified
Outlier Exposure (DivOE), for effective OOD detection via informative
extrapolation based on the given auxiliary outliers. Specifically, DivOE
introduces a new learning objective, which diversifies the auxiliary
distribution by explicitly synthesizing more informative outliers for
extrapolation during training. It leverages a multi-step optimization method to
generate novel outliers beyond the original ones, which is compatible with many
variants of outlier exposure. Extensive experiments and analyses have been
conducted to characterize and demonstrate the effectiveness of the proposed
DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14814">Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Odonnat_A/0/1/0/all/0/1">Ambroise Odonnat</a>, <a href="http://arxiv.org/find/cs/1/au:+Feofanov_V/0/1/0/all/0/1">Vasilii Feofanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1">Ievgen Redko</a></p>
<p>Self-training is a well-known approach for semi-supervised learning. It
consists of iteratively assigning pseudo-labels to unlabeled data for which the
model is confident and treating them as labeled examples. For neural networks,
softmax prediction probabilities are often used as a confidence measure,
despite the fact that they are known to be overconfident, even for wrong
predictions. This phenomenon is particularly intensified in the presence of
sample selection bias, i.e., when data labeling is subject to some constraint.
To address this issue, we propose a novel confidence measure, called
$\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of
linear classifiers. We provide the theoretical analysis of our approach by
studying stationary points and describing the relationship between the
diversity of the individual members and their performance. We empirically
demonstrate the benefit of our confidence measure for three different
pseudo-labeling policies on classification datasets of various data modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15694">COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1">Lin Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuanzhao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruifeng Xu</a></p>
<p>The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Fitting (COPF), in which we estimate a series of optimal policies using
the Monte Carlo method, and then continually fit the policy sequence with the
function regularization. COPF involves a single learning phase and doesn't
necessitate complex reinforcement learning. Importantly, it shares the
capability with RLHF to learn from unlabeled data, making it flexible for
continual preference learning. Our experimental results show that COPF
outperforms strong Continuous learning (CL) baselines when it comes to
consistently aligning with human preferences on different tasks and domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15890">Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aketi_S/0/1/0/all/0/1">Sai Aparna Aketi</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>The current state-of-the-art decentralized learning algorithms mostly assume
the data distribution to be Independent and Identically Distributed (IID).
However, in practical scenarios, the distributed datasets can have
significantly heterogeneous data distributions across the agents. In this work,
we present a novel approach for decentralized learning on heterogeneous data,
where data-free knowledge distillation through contrastive loss on
cross-features is utilized to improve performance. Cross-features for a pair of
neighboring agents are the features (i.e., last hidden layer activations)
obtained from the data of an agent with respect to the model parameters of the
other agent. We demonstrate the effectiveness of the proposed technique through
an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10,
CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and
network topologies. Our experiments show that the proposed method achieves
superior performance (0.2-4% improvement in test accuracy) compared to other
existing techniques for decentralized learning on heterogeneous data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16638">Robust Covariate Shift Adaptation for Density-Ratio Estimation. (arXiv:2310.16638v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kato_M/0/1/0/all/0/1">Masahiro Kato</a></p>
<p>Consider a scenario where we have access to train data with both covariates
and outcomes while test data only contains covariates. In this scenario, our
primary aim is to predict the missing outcomes of the test data. With this
objective in mind, we train parametric regression models under a covariate
shift, where covariate distributions are different between the train and test
data. For this problem, existing studies have proposed covariate shift
adaptation via importance weighting using the density ratio. This approach
averages the train data losses, each weighted by an estimated ratio of the
covariate densities between the train and test data, to approximate the
test-data risk. Although it allows us to obtain a test-data risk minimizer, its
performance heavily relies on the accuracy of the density ratio estimation.
Moreover, even if the density ratio can be consistently estimated, the
estimation errors of the density ratio also yield bias in the estimators of the
regression model's parameters of interest. To mitigate these challenges, we
introduce a doubly robust estimator for covariate shift adaptation via
importance weighting, which incorporates an additional estimator for the
regression function. Leveraging double machine learning techniques, our
estimator reduces the bias arising from the density ratio estimation errors. We
demonstrate the asymptotic distribution of the regression parameter estimator.
Notably, our estimator remains consistent if either the density ratio estimator
or the regression function is consistent, showcasing its robustness against
potential errors in density ratio estimation. Finally, we confirm the soundness
of our proposed method via simulation studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16639">Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving. (arXiv:2310.16639v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Echterhoff_J/0/1/0/all/0/1">Jessica Echterhoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1">An Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kyungtae Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelraouf_A/0/1/0/all/0/1">Amr Abdelraouf</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rohit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1">Julian McAuley</a></p>
<p>Concept bottleneck models have been successfully used for explainable machine
learning by encoding information within the model with a set of human-defined
concepts. In the context of human-assisted or autonomous driving,
explainability models can help user acceptance and understanding of decisions
made by the autonomous vehicle, which can be used to rationalize and explain
driver or vehicle behavior. We propose a new approach using concept bottlenecks
as visual features for control command predictions and explanations of user and
vehicle behavior. We learn a human-understandable concept layer that we use to
explain sequential driving scenes while learning vehicle control commands. This
approach can then be used to determine whether a change in a preferred gap or
steering commands from a human (or autonomous vehicle) is led by an external
stimulus or change in preferences. We achieve competitive performance to latent
visual features while gaining interpretability within our model setup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16779">Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1">Jongheon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a></p>
<p>Along with recent diffusion models, randomized smoothing has become one of a
few tangible approaches that offers adversarial robustness to models at scale,
e.g., those of large pre-trained models. Specifically, one can perform
randomized smoothing on any classifier via a simple "denoise-and-classify"
pipeline, so-called denoised smoothing, given that an accurate denoiser is
available - such as diffusion model. In this paper, we present scalable methods
to address the current trade-off between certified robustness and accuracy in
denoised smoothing. Our key idea is to "selectively" apply smoothing among
multiple noise scales, coined multi-scale smoothing, which can be efficiently
implemented with a single diffusion model. This approach also suggests a new
objective to compare the collective robustness of multi-scale smoothed
classifiers, and questions which representation of diffusion model would
maximize the objective. To address this, we propose to further fine-tune
diffusion model (a) to perform consistent denoising whenever the original image
is recoverable, but (b) to generate rather diverse outputs otherwise. Our
experiments show that the proposed multi-scale smoothing scheme combined with
diffusion fine-tuning enables strong certified robustness available with high
noise level while maintaining its accuracy closer to non-smoothed classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16826">Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping. (arXiv:2310.16826v2 [astro-ph.EP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Pena_Asensio_E/0/1/0/all/0/1">Eloy Pe&#xf1;a-Asensio</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Trigo_Rodriguez_J/0/1/0/all/0/1">Josep M. Trigo-Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Grebol_Tomas_P/0/1/0/all/0/1">Pau Gr&#xe8;bol-Tom&#xe0;s</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Regordosa_Avellana_D/0/1/0/all/0/1">David Regordosa-Avellana</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Rimola_A/0/1/0/all/0/1">Albert Rimola</a></p>
<p>In recent decades, the use of optical detection systems for meteor studies
has increased dramatically, resulting in huge amounts of data being analyzed.
Automated meteor detection tools are essential for studying the continuous
meteoroid incoming flux, recovering fresh meteorites, and achieving a better
understanding of our Solar System. Concerning meteor detection, distinguishing
false positives between meteor and non-meteor images has traditionally been
performed by hand, which is significantly time-consuming. To address this
issue, we developed a fully automated pipeline that uses Convolutional Neural
Networks (CNNs) to classify candidate meteor detections. Our new method is able
to detect meteors even in images that contain static elements such as clouds,
the Moon, and buildings. To accurately locate the meteor within each frame, we
employ the Gradient-weighted Class Activation Mapping (Grad-CAM) technique.
This method facilitates the identification of the region of interest by
multiplying the activations from the last convolutional layer with the average
of the gradients across the feature map of that layer. By combining these
findings with the activation map derived from the first convolutional layer, we
effectively pinpoint the most probable pixel location of the meteor. We trained
and evaluated our model on a large dataset collected by the Spanish Meteor
Network (SPMN) and achieved a precision of 98\%. Our new methodology presented
here has the potential to reduce the workload of meteor scientists and station
operators and improve the accuracy of meteor tracking and classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.01992">Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bouniot_Q/0/1/0/all/0/1">Quentin Bouniot</a>, <a href="http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1">Ievgen Redko</a>, <a href="http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1">Romaric Audigier</a>, <a href="http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1">Ang&#xe9;lique Loesch</a>, <a href="http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1">Amaury Habrard</a></p>
<p>In this paper, we consider the framework of multi-task representation (MTR)
learning where the goal is to use source tasks to learn a representation that
reduces the sample complexity of solving a target task. We start by reviewing
recent advances in MTR theory and show that they can provide novel insights for
popular meta-learning algorithms when analyzed within this framework. In
particular, we highlight a fundamental difference between gradient-based and
metric-based algorithms in practice and put forward a theoretical analysis to
explain it. Finally, we use the derived insights to improve the performance of
meta-learning methods via a new spectral-based regularization term and confirm
its efficiency through experimental studies on few-shot classification
benchmarks. To the best of our knowledge, this is the first contribution that
puts the most recent learning bounds of MTR theory into practice for the task
of few-shot classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00990">A weighted-variance variational autoencoder model for speech enhancement. (arXiv:2211.00990v2 [cs.SD] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golmakani_A/0/1/0/all/0/1">Ali Golmakani</a> (MULTISPEECH), <a href="http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1">Mostafa Sadeghi</a> (MULTISPEECH), <a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1">Xavier Alameda-Pineda</a> (ROBOTLEARN), <a href="http://arxiv.org/find/cs/1/au:+Serizel_R/0/1/0/all/0/1">Romain Serizel</a> (MULTISPEECH)</p>
<p>We address speech enhancement based on variational autoencoders, which
involves learning a speech prior distribution in the time-frequency (TF)
domain. A zero-mean complex-valued Gaussian distribution is usually assumed for
the generative model, where the speech information is encoded in the variance
as a function of a latent variable. In contrast to this commonly used approach,
we propose a weighted variance generative model, where the contribution of each
spectrogram time-frame in parameter learning is weighted. We impose a Gamma
prior distribution on the weights, which would effectively lead to a Student's
t-distribution instead of Gaussian for speech generative modeling. We develop
efficient training and speech enhancement algorithms based on the proposed
generative model. Our experimental results on spectrogram auto-encoding and
speech enhancement demonstrate the effectiveness and robustness of the proposed
approach compared to the standard unweighted variance model.
</p>
</p>
</div>

    </div>
    </body>
    