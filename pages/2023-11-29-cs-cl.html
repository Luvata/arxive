<!DOCTYPE html>
<html>
<head>
<title>2023-11-29-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.14674">Emotion-Oriented Behavior Model Using Deep Learning. (arXiv:2311.14674v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raza_M/0/1/0/all/0/1">Muhammad Arslan Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1">Muhammad Shoaib Farooq</a>, <a href="http://arxiv.org/find/cs/1/au:+Khelifi_A/0/1/0/all/0/1">Adel Khelifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvi_A/0/1/0/all/0/1">Atif Alvi</a></p>
<p>Emotions, as a fundamental ingredient of any social interaction, lead to
behaviors that represent the effectiveness of the interaction through facial
expressions and gestures in humans. Hence an agent must possess the social and
cognitive abilities to understand human social parameters and behave
accordingly. However, no such emotion-oriented behavior model is presented yet
in the existing research. The emotion prediction may generate appropriate
agents' behaviors for effective interaction using conversation modality.
Considering the importance of emotions, and behaviors, for an agent's social
interaction, an Emotion-based Behavior model is presented in this paper for
Socio-cognitive artificial agents. The proposed model is implemented using
tweets data trained on multiple models like Long Short-Term Memory (LSTM),
Convolution Neural Network (CNN) and Bidirectional Encoder Representations from
Transformers (BERT) for emotion prediction with an average accuracy of 92%, and
55% respectively. Further, using emotion predictions from CNN-LSTM, the
behavior module responds using facial expressions and gestures using Behavioral
Markup Language (BML). The accuracy of emotion-based behavior predictions is
statistically validated using the 2-tailed Pearson correlation on the data
collected from human users through questionnaires. Analysis shows that all
emotion-based behaviors accurately depict human-like gestures and facial
expressions based on the significant correlation at the 0.01 and 0.05 levels.
This study is a steppingstone to a multi-faceted artificial agent interaction
based on emotion-oriented behaviors. Cognition has significance regarding
social interaction among humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14677">Filter bubbles and affective polarization in user-personalized large language model outputs. (arXiv:2311.14677v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lazovich_T/0/1/0/all/0/1">Tomo Lazovich</a></p>
<p>Echoing the history of search engines and social media content rankings, the
advent of large language models (LLMs) has led to a push for increased
personalization of model outputs to individual users. In the past, personalized
recommendations and ranking systems have been linked to the development of
filter bubbles (serving content that may confirm a user's existing biases) and
affective polarization (strong negative sentiment towards those with differing
views). In this work, we explore how prompting a leading large language model,
ChatGPT-3.5, with a user's political affiliation prior to asking factual
questions about public figures and organizations leads to differing results. We
observe that left-leaning users tend to receive more positive statements about
left-leaning political figures and media outlets, while right-leaning users see
more positive statements about right-leaning entities. This pattern holds
across presidential candidates, members of the U.S. Senate, and media
organizations with ratings from AllSides. When qualitatively evaluating some of
these outputs, there is evidence that particular facts are included or excluded
based on the user's political affiliation. These results illustrate that
personalizing LLMs based on user demographics carry the same risks of affective
polarization and filter bubbles that have been seen in other personalized
internet technologies. This ``failure mode" should be monitored closely as
there are more attempts to monetize and personalize these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14685">Comprehensive Assessment of Toxicity in ChatGPT. (arXiv:2311.14685v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Boyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xinyue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_W/0/1/0/all/0/1">Wai Man Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_Z/0/1/0/all/0/1">Zeyang Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Salem_A/0/1/0/all/0/1">Ahmed Salem</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1">Michael Backes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a></p>
<p>Moderating offensive, hateful, and toxic language has always been an
important but challenging topic in the domain of safe use in NLP. The emerging
large language models (LLMs), such as ChatGPT, can potentially further
accentuate this threat. Previous works have discovered that ChatGPT can
generate toxic responses using carefully crafted inputs. However, limited
research has been done to systematically examine when ChatGPT generates toxic
responses. In this paper, we comprehensively evaluate the toxicity in ChatGPT
by utilizing instruction-tuning datasets that closely align with real-world
scenarios. Our results show that ChatGPT's toxicity varies based on different
properties and settings of the prompts, including tasks, domains, length, and
languages. Notably, prompts in creative writing tasks can be 2x more likely
than others to elicit toxic responses. Prompting in German and Portuguese can
also double the response toxicity. Additionally, we discover that certain
deliberately toxic prompts, designed in earlier studies, no longer yield
harmful responses. We hope our discoveries can guide model developers to better
regulate these AI systems and the users to avoid undesirable outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14693">Benefits and Harms of Large Language Models in Digital Mental Health. (arXiv:2311.14693v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1">Munmun De Choudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Pendse_S/0/1/0/all/0/1">Sachin R. Pendse</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Neha Kumar</a></p>
<p>The past decade has been transformative for mental health research and
practice. The ability to harness large repositories of data, whether from
electronic health records (EHR), mobile devices, or social media, has revealed
a potential for valuable insights into patient experiences, promising early,
proactive interventions, as well as personalized treatment plans. Recent
developments in generative artificial intelligence, particularly large language
models (LLMs), show promise in leading digital mental health to uncharted
territory. Patients are arriving at doctors' appointments with information
sourced from chatbots, state-of-the-art LLMs are being incorporated in medical
software and EHR systems, and chatbots from an ever-increasing number of
startups promise to serve as AI companions, friends, and partners. This article
presents contemporary perspectives on the opportunities and risks posed by LLMs
in the design, development, and implementation of digital mental health tools.
We adopt an ecological framework and draw on the affordances offered by LLMs to
discuss four application areas -- care-seeking behaviors from individuals in
need of care, community care provision, institutional and medical care
provision, and larger care ecologies at the societal level. We engage in a
thoughtful consideration of whether and how LLM-based technologies could or
should be employed for enhancing mental health. The benefits and harms our
article surfaces could serve to help shape future research, advocacy, and
regulatory efforts focused on creating more responsible, user-friendly,
equitable, and secure LLM-based tools for mental health treatment and
intervention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14699">Ontology Learning Using Formal Concept Analysis and WordNet. (arXiv:2311.14699v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1">Bryar A. Hassan</a></p>
<p>Manual ontology construction takes time, resources, and domain specialists.
Supporting a component of this process for automation or semi-automation would
be good. This project and dissertation provide a Formal Concept Analysis and
WordNet framework for learning concept hierarchies from free texts. The process
has steps. First, the document is Part-Of-Speech labeled, then parsed to
produce sentence parse trees. Verb/noun dependencies are derived from parse
trees next. After lemmatizing, pruning, and filtering the word pairings, the
formal context is created. The formal context may contain some erroneous and
uninteresting pairs because the parser output may be erroneous, not all derived
pairs are interesting, and it may be large due to constructing it from a large
free text corpus. Deriving lattice from the formal context may take longer,
depending on the size and complexity of the data. Thus, decreasing formal
context may eliminate erroneous and uninteresting pairs and speed up idea
lattice derivation. WordNet-based and Frequency-based approaches are tested.
Finally, we compute formal idea lattice and create a classical concept
hierarchy. The reduced concept lattice is compared to the original to evaluate
the outcomes. Despite several system constraints and component discrepancies
that may prevent logical conclusion, the following data imply idea hierarchies
in this project and dissertation are promising. First, the reduced idea lattice
and original concept have commonalities. Second, alternative language or
statistical methods can reduce formal context size. Finally, WordNet-based and
Frequency-based approaches reduce formal context differently, and the order of
applying them is examined to reduce context efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14703">ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management. (arXiv:2311.14703v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Angela Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1">Mert Yuksekgonul</a>, <a href="http://arxiv.org/find/cs/1/au:+Guild_J/0/1/0/all/0/1">Joshua Guild</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Joseph C. Wu</a></p>
<p>Recent breakthroughs in large language models (LLMs) have led to their rapid
dissemination and widespread use. One early application has been to medicine,
where LLMs have been investigated to streamline clinical workflows and
facilitate clinical analysis and decision-making. However, a leading barrier to
the deployment of Artificial Intelligence (AI) and in particular LLMs has been
concern for embedded gender and racial biases. Here, we evaluate whether a
leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical
management of acute coronary syndrome (ACS). We find that specifying patients
as female, African American, or Hispanic resulted in a decrease in guideline
recommended medical management, diagnosis, and symptom management of ACS. Most
notably, the largest disparities were seen in the recommendation of coronary
angiography or stress testing for the diagnosis and further intervention of ACS
and recommendation of high intensity statins. These disparities correlate with
biases that have been observed clinically and have been implicated in the
differential gender and racial morbidity and mortality outcomes of ACS and
coronary artery disease. Furthermore, we find that the largest disparities are
seen during unstable angina, where fewer explicit clinical guidelines exist.
Finally, we find that through asking ChatGPT 3.5 to explain its reasoning prior
to providing an answer, we are able to improve clinical accuracy and mitigate
instances of gender and racial biases. This is among the first studies to
demonstrate that the gender and racial biases that LLMs exhibit do in fact
affect clinical management. Additionally, we demonstrate that existing
strategies that improve LLM performance not only improve LLM performance in
clinical management, but can also be used to mitigate gender and racial biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14708">Large Language Model-Driven Classroom Flipping: Empowering Student-Centric Peer Questioning with Flipped Interaction. (arXiv:2311.14708v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chee Wei Tan</a></p>
<p>Reciprocal questioning is essential for effective teaching and learning,
fostering active engagement and deeper understanding through collaborative
interactions, especially in large classrooms. Can large language model (LLM),
such as OpenAI's GPT (Generative Pre-trained Transformer) series, assist in
this? This paper investigates a pedagogical approach of classroom flipping
based on flipped interaction in LLMs. Flipped interaction involves using
language models to prioritize generating questions instead of answers to
prompts. We demonstrate how traditional classroom flipping techniques,
including Peer Instruction and Just-in-Time Teaching (JiTT), can be enhanced
through flipped interaction techniques, creating student-centric questions for
hybrid teaching. In particular, we propose a workflow to integrate prompt
engineering with clicker and JiTT quizzes by a poll-prompt-quiz routine and a
quiz-prompt-discuss routine to empower students to self-regulate their learning
capacity and enable teachers to swiftly personalize training pathways. We
develop an LLM-driven chatbot software that digitizes various elements of
classroom flipping and facilitates the assessment of students using these
routines to deliver peer-generated questions. We have applied our LLM-driven
chatbot software for teaching both undergraduate and graduate students from
2020 to 2022, effectively useful for bridging the gap between teachers and
students in remote teaching during the COVID-19 pandemic years. In particular,
LLM-driven classroom flipping can be particularly beneficial in large class
settings to optimize teaching pace and enable engaging classroom experiences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14722">Zero-Shot Question Answering over Financial Documents using Large Language Models. (arXiv:2311.14722v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phogat_K/0/1/0/all/0/1">Karmvir Singh Phogat</a>, <a href="http://arxiv.org/find/cs/1/au:+Harsha_C/0/1/0/all/0/1">Chetan Harsha</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasaratha_S/0/1/0/all/0/1">Sridhar Dasaratha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishna_S/0/1/0/all/0/1">Shashishekar Ramakrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Puranam_S/0/1/0/all/0/1">Sai Akhil Puranam</a></p>
<p>We introduce a large language model (LLM) based approach to answer complex
questions requiring multi-hop numerical reasoning over financial reports. While
LLMs have exhibited remarkable performance on various natural language and
reasoning tasks, complex reasoning problems often rely on few-shot prompts that
require carefully crafted examples. In contrast, our approach uses novel
zero-shot prompts that guide the LLM to encode the required reasoning into a
Python program or a domain specific language. The generated program is then
executed by a program interpreter, thus mitigating the limitations of LLM in
performing accurate arithmetic calculations.
</p>
<p>We evaluate the proposed approach on three financial datasets using some of
the recently developed generative pretrained transformer (GPT) models and
perform comparisons with various zero-shot baselines. The experimental results
demonstrate that our approach significantly improves the accuracy for all the
LLMs over their respective baselines. We provide a detailed analysis of the
results, generating insights to support our findings. The success of our
approach demonstrates the enormous potential to extract complex domain specific
numerical reasoning by designing zero-shot prompts to effectively exploit the
knowledge embedded in LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14727">Optimal Strategies to Perform Multilingual Analysis of Social Content for a Novel Dataset in the Tourism Domain. (arXiv:2311.14727v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masson_M/0/1/0/all/0/1">Maxime Masson</a>, <a href="http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1">Rodrigo Agerri</a>, <a href="http://arxiv.org/find/cs/1/au:+Sallaberry_C/0/1/0/all/0/1">Christian Sallaberry</a>, <a href="http://arxiv.org/find/cs/1/au:+Bessagnet_M/0/1/0/all/0/1">Marie-Noelle Bessagnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacayrelle_A/0/1/0/all/0/1">Annig Le Parc Lacayrelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Roose_P/0/1/0/all/0/1">Philippe Roose</a></p>
<p>The rising influence of social media platforms in various domains, including
tourism, has highlighted the growing need for efficient and automated natural
language processing (NLP) approaches to take advantage of this valuable
resource. However, the transformation of multilingual, unstructured, and
informal texts into structured knowledge often poses significant challenges.
</p>
<p>In this work, we evaluate and compare few-shot, pattern-exploiting and
fine-tuning machine learning techniques on large multilingual language models
(LLMs) to establish the best strategy to address the lack of annotated data for
3 common NLP tasks in the tourism domain: (1) Sentiment Analysis, (2) Named
Entity Recognition, and (3) Fine-grained Thematic Concept Extraction (linked to
a semantic resource). Furthermore, we aim to ascertain the quantity of
annotated examples required to achieve good performance in those 3 tasks,
addressing a common challenge encountered by NLP researchers in the
construction of domain-specific datasets.
</p>
<p>Extensive experimentation on a newly collected and annotated multilingual
(French, English, and Spanish) dataset composed of tourism-related tweets shows
that current few-shot learning techniques allow us to obtain competitive
results for all three tasks with very little annotation data: 5 tweets per
label (15 in total) for Sentiment Analysis, 10% of the tweets for location
detection (around 160) and 13% (200 approx.) of the tweets annotated with
thematic concepts, a highly fine-grained sequence labeling task based on an
inventory of 315 classes.
</p>
<p>This comparative analysis, grounded in a novel dataset, paves the way for
applying NLP to new domain-specific applications, reducing the need for manual
annotations and circumventing the complexities of rule-based, ad hoc solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14729">App for Resume-Based Job Matching with Speech Interviews and Grammar Analysis: A Review. (arXiv:2311.14729v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1">Tanmay Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardeshi_Y/0/1/0/all/0/1">Yuvraj Pardeshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_Y/0/1/0/all/0/1">Yash Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakat_V/0/1/0/all/0/1">Vaishnvi Sakat</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhirud_S/0/1/0/all/0/1">Sapana Bhirud</a></p>
<p>Through the advancement in natural language processing (NLP), specifically in
speech recognition, fully automated complex systems functioning on voice input
have started proliferating in areas such as home automation. These systems have
been termed Automatic Speech Recognition Systems (ASR). In this review paper,
we explore the feasibility of an end-to-end system providing speech and text
based natural language processing for job interview preparation as well as
recommendation of relevant job postings. We also explore existing
recommender-based systems and note their limitations. This literature review
would help us identify the approaches and limitations of the various similar
use-cases of NLP technology for our upcoming project.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14730">MemoryCompanion: A Smart Healthcare Solution to Empower Efficient Alzheimer&#x27;s Care Via Unleashing Generative AI. (arXiv:2311.14730v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lifei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1">Yeonie Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yi Fang</a></p>
<p>With the rise of Large Language Models (LLMs), notably characterized by GPT
frameworks, there emerges a catalyst for novel healthcare applications. Earlier
iterations of chatbot caregivers, though existent, have yet to achieve a
dimension of human-like authenticity. This paper unveils `MemoryCompanion' a
pioneering digital health solution explicitly tailored for Alzheimer's disease
(AD) patients and their caregivers. Drawing upon the nuances of GPT technology
and prompt engineering, MemoryCompanion manifests a personalized caregiving
paradigm, fostering interactions via voice-cloning and talking-face mechanisms
that resonate with the familiarity of known companions. Using advanced
prompt-engineering, the system intricately adapts to each patient's distinct
profile, curating its content and communication style accordingly. This
approach strives to counteract prevalent issues of social isolation and
loneliness frequently observed in AD demographics. Our methodology, grounded in
its innovative design, addresses both the caregiving and technological
challenges intrinsic to this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14736">Data Diversity Matters for Robust Instruction Tuning. (arXiv:2311.14736v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bukharin_A/0/1/0/all/0/1">Alexander Bukharin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a></p>
<p>Instruction tuning has emerged as a key step in aligning large language
models. One of the central challenges of instruction tuning is dataset
selection, as the composition of the instruction tuning dataset can
significantly impact downstream performance. In particular, researchers have
hypothesized that dataset diversity and dataset quality are important
indicators of downstream performance. However, it is not clear how to
automatically select high quality and diverse data or how exactly quality and
diversity affect instruction following ability. To resolve these issues, we
propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT
provides a principled algorithm to control dataset diversity and quality,
allowing us to conduct an in depth study on the effect of diversity and quality
on instruction tuning performance. From this study we draw two key insights (1)
there is a natural tradeoff between dataset diversity and quality and (2)
increasing dataset diversity significantly improves the worst case instruction
following performance, therefore improving robustness. We validate the
performance of QDIT on several large scale instruction tuning datasets, where
we find it can improve worst case performance by 18% while maintaining or
improving average performance compared to quality driven baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14737">Positional Description Matters for Transformers Arithmetic. (arXiv:2311.14737v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1">Ruoqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1">S&#xe9;bastien Bubeck</a>, <a href="http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1">Ronen Eldan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yin Tat Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a></p>
<p>Transformers, central to the successes in modern Natural Language Processing,
often falter on arithmetic tasks despite their vast capabilities --which
paradoxically include remarkable coding abilities. We observe that a crucial
challenge is their naive reliance on positional information to solve arithmetic
problems with a small number of digits, leading to poor performance on larger
numbers. Herein, we delve deeper into the role of positional encoding, and
propose several ways to fix the issue, either by modifying the positional
encoding directly, or by modifying the representation of the arithmetic task to
leverage standard positional encoding differently. We investigate the value of
these modifications for three tasks: (i) classical multiplication, (ii) length
extrapolation in addition, and (iii) addition in natural language context. For
(i) we train a small model on a small dataset (100M parameters and 300k
samples) with remarkable aptitude in (direct, no scratchpad) 15 digits
multiplication and essentially perfect up to 12 digits, while usual training in
this context would give a model failing at 4 digits multiplication. In the
experiments on addition, we use a mere 120k samples to demonstrate: for (ii)
extrapolation from 10 digits to testing on 12 digits numbers while usual
training would have no extrapolation, and for (iii) almost perfect accuracy up
to 5 digits while usual training would be correct only up to 3 digits (which is
essentially memorization with a training set of 120k samples).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14740">AutoKG: Efficient Automated Knowledge Graph Generation for Language Models. (arXiv:2311.14740v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bohan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1">Andrea L. Bertozzi</a></p>
<p>Traditional methods of linking large language models (LLMs) to knowledge
bases via the semantic similarity search often fall short of capturing complex
relational dynamics. To address these limitations, we introduce AutoKG, a
lightweight and efficient approach for automated knowledge graph (KG)
construction. For a given knowledge base consisting of text blocks, AutoKG
first extracts keywords using a LLM and then evaluates the relationship weight
between each pair of keywords using graph Laplace learning. We employ a hybrid
search scheme combining vector similarity and graph-based associations to
enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a
more comprehensive and interconnected knowledge retrieval mechanism compared to
the semantic similarity search, thereby enhancing the capabilities of LLMs in
generating more insightful and relevant outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14741">@ve: A Chatbot for Latin. (arXiv:2311.14741v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bendel_O/0/1/0/all/0/1">Oliver Bendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndiaye_K/0/1/0/all/0/1">Karim N&#x27;diaye</a></p>
<p>Dead, extinct, and endangered languages have been preserved primarily through
audio conservation and the collection and digitization of scripts and have been
promoted through targeted language acquisition efforts. Another possibility
would be to build conversational agents that can master these languages. This
would provide an artificial, active conversational partner which has knowledge
of the vocabulary and grammar, and one learns with it in a different way. The
chatbot @ve, with which one can communicate in Latin, was developed in
2022/2023 based on GPT-3.0. It was additionally equipped with a manually
created knowledge base. After conceptual groundwork, this paper presents the
preparation and implementation of the project. In addition, it summarizes the
test that a Latin expert conducted with the chatbot. A critical discussion
elaborates advantages and disadvantages. @ve could be a new tool for teaching
Latin in a memorable and entertaining way through dialogue. However, the
present implementation is still too prone to glitches for stand-alone use -
i.e., without the accompaniment of a teacher. The use of GPT-4 could be a
solution as well as the extension of the knowledge base. In conclusion, it can
be argued that conversational agents are an innovative approach to promoting
and preserving languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align an LLM. These reward models are additionally used
at inference-time to estimate how well LLM responses adhere to those desired
behaviors. However, there is little work measuring how robust these reward
models are to distribution shifts. In this work, we evaluate how reward model
performance - measured via accuracy and calibration (i.e. alignment between
accuracy and confidence) - is affected by distribution shift. We show novel
calibration patterns and accuracy drops due to OOD prompts and responses, and
that the reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting in order to detect these
distribution shifts in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14788">Evaluating Large Language Models through Gender and Racial Stereotypes. (arXiv:2311.14788v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1">Ananya Malik</a></p>
<p>Language Models have ushered a new age of AI gaining traction within the NLP
community as well as amongst the general population. AI's ability to make
predictions, generations and its applications in sensitive decision-making
scenarios, makes it even more important to study these models for possible
biases that may exist and that can be exaggerated. We conduct a quality
comparative study and establish a framework to evaluate language models under
the premise of two kinds of biases: gender and race, in a professional setting.
We find out that while gender bias has reduced immensely in newer models, as
compared to older ones, racial bias still exists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14808">Data-to-Text Bilingual Generation. (arXiv:2311.14808v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lapalme_G/0/1/0/all/0/1">Guy Lapalme</a></p>
<p>This document illustrates the use of pyrealb for generating two parallel
texts (English and French) from a single source of data. The data selection and
text organisation processes are shared between the two languages. only language
dependent word and phrasing choices are distinct processes. The realized texts
thus convey identical information in both languages without the risk of being
lost in translation. This is especially important in cases where strict and
simultaneous bilingualism is required. We first present the types of
applications targeted by this approach and how the pyrealb English and French
realizer can be used for achieving this goal in a natural way. We describe an
object-oriented organization to ensure a convenient realization in both
languages. To illustrate the process, different types of applications are then
briefly sketched with links to the source code. A brief comparison of the text
generation is given with the output of an instance of a GPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14835">Weak Alignment Supervision from Hybrid Model Improves End-to-end ASR. (arXiv:2311.14835v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jintao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yingbo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1">Zoltan Tuske</a></p>
<p>In this paper, we aim to create weak alignment supervision to aid the
end-to-end modeling. Towards this end, we use the existing hybrid ASR system to
produce triphone alignments of the training audios. We then create a
cross-entropy loss at a certain layer of the encoder using the derived
alignments. In contrast to the general one-hot cross-entropy losses with or
without loss weighting, here we use a cross-entropy loss with a label smoothing
parameter to regularize the supervision. As a comparison, we also conduct the
experiments with one-hot cross-entropy losses and CTC losses with loss
weighting. The results show that placing the weak alignment supervision with
the label smoothing parameter of 0.5 at the third encoder layer outperforms the
other two approaches and leads to about 5% relative WER reduction on the
TED-LIUM 2 dataset over the baseline. We see similar improvements when applying
the method out-of-the-box on a Tagalog end-to-end ASR system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14836">Custom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion. (arXiv:2311.14836v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamble_A/0/1/0/all/0/1">Anand Kamble</a>, <a href="http://arxiv.org/find/cs/1/au:+Tathe_A/0/1/0/all/0/1">Aniket Tathe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumbharkar_S/0/1/0/all/0/1">Suyash Kumbharkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhandare_A/0/1/0/all/0/1">Atharva Bhandare</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Anirban C. Mitra</a></p>
<p>This paper proposes two innovative methodologies to construct customized
Common Voice datasets for low-resource languages like Hindi. The first
methodology leverages Bark, a transformer-based text-to-audio model developed
by Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to
enhance Bark's performance. The second methodology employs Retrieval-Based
Voice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both
methodologies contribute to the advancement of ASR technology and offer
valuable insights into addressing the challenges of constructing customized
Common Voice datasets for under-resourced languages. Furthermore, they provide
a pathway to achieving high-quality, personalized voice generation for a range
of applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14838">OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models. (arXiv:2311.14838v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1">Nikolay Bogoychev</a>, <a href="http://arxiv.org/find/cs/1/au:+Linde_J/0/1/0/all/0/1">Jelmer van der Linde</a>, <a href="http://arxiv.org/find/cs/1/au:+Nail_G/0/1/0/all/0/1">Graeme Nail</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1">Barry Haddow</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaragoza_Bernabeu_J/0/1/0/all/0/1">Jaume Zaragoza-Bernabeu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramirez_Sanchez_G/0/1/0/all/0/1">Gema Ram&#xed;rez-S&#xe1;nchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Weymann_L/0/1/0/all/0/1">Lukas Weymann</a>, <a href="http://arxiv.org/find/cs/1/au:+Mateiu_T/0/1/0/all/0/1">Tudor Nicolae Mateiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1">Jind&#x159;ich Helcl</a>, <a href="http://arxiv.org/find/cs/1/au:+Aulamo_M/0/1/0/all/0/1">Mikko Aulamo</a></p>
<p>Developing high quality machine translation systems is a labour intensive,
challenging and confusing process for newcomers to the field. We present a pair
of tools OpusCleaner and OpusTrainer that aim to simplify the process, reduce
the amount of work and lower the entry barrier for newcomers.
</p>
<p>OpusCleaner is a data downloading, cleaning, and proprocessing toolkit. It is
designed to allow researchers to quickly download, visualise and preprocess
bilingual (or monolingual) data that comes from many different sources, each of
them with different quality, issues, and unique filtering/preprocessing
requirements.
</p>
<p>OpusTrainer is a data scheduling and data augmenting tool aimed at building
large scale, robust machine translation systems and large language models. It
features deterministic data mixing from many different sources, on-the-fly data
augmentation and more.
</p>
<p>Using these tools, we showcase how we can use it to create high quality
machine translation model robust to noisy user input; multilingual models and
terminology aware models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14865">Improving Cross-Domain Hate Speech Generalizability with Emotion Knowledge. (arXiv:2311.14865v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Shi Yin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gauch_S/0/1/0/all/0/1">Susan Gauch</a></p>
<p>Reliable automatic hate speech (HS) detection systems must adapt to the
in-flow of diverse new data to curtail hate speech. However, hate speech
detection systems commonly lack generalizability in identifying hate speech
dissimilar to data used in training, impeding their robustness in real-world
deployments. In this work, we propose a hate speech generalization framework
that leverages emotion knowledge in a multitask architecture to improve the
generalizability of hate speech detection in a cross-domain setting. We
investigate emotion corpora with varying emotion categorical scopes to
determine the best corpus scope for supplying emotion knowledge to foster
generalized hate speech detection. We further assess the relationship between
using pretrained Transformers models adapted for hate speech and its effect on
our emotion-enriched hate speech generalization model. We perform extensive
experiments on six publicly available datasets sourced from different online
domains and show that our emotion-enriched HS detection generalization method
demonstrates consistent generalization improvement in cross-domain evaluation,
increasing generalization performance up to 18.1% and average cross-domain
performance up to 8.5%, according to the F1 measure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14871">Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses. (arXiv:2311.14871v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1">Linzi Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Hackinen_B/0/1/0/all/0/1">Brad Hackinen</a>, <a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1">Giuseppe Carenini</a></p>
<p>U.S. Federal Regulators receive over one million comment letters each year
from businesses, interest groups, and members of the public, all advocating for
changes to proposed regulations. These comments are believed to have
wide-ranging impacts on public policy. However, measuring the impact of
specific comments is challenging because regulators are required to respond to
comments but they do not have to specify which comments they are addressing. In
this paper, we propose a simple yet effective solution to this problem by using
an iterative contrastive method to train a neural model aiming for matching
text from public comments to responses written by regulators. We demonstrate
that our proposal substantially outperforms a set of selected text-matching
baselines on a human-annotated test set. Furthermore, it delivers performance
comparable to the most advanced gigantic language model (i.e., GPT-4), and is
more cost-effective when handling comments and regulator responses matching in
larger scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14901">Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance. (arXiv:2311.14901v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanlin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1">Yong Xiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Juhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongong Ji</a></p>
<p>Code search engine is an essential tool in software development. Many code
search methods have sprung up, focusing on the overall ranking performance of
code search. In this paper, we study code search from another perspective by
analyzing the bias of code search models. Biased code search engines provide
poor user experience, even though they show promising overall performance. Due
to different development conventions (e.g., prefer long queries or
abbreviations), some programmers will find the engine useful, while others may
find it hard to get desirable search results. To mitigate biases, we develop a
general debiasing framework that employs reranking to calibrate search results.
It can be easily plugged into existing engines and handle new code search
biases discovered in the future. Experiments show that our framework can
effectively reduce biases. Meanwhile, the overall ranking performance of code
search gets improved after debiasing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14919">Faster Minimum Bayes Risk Decoding with Confidence-based Pruning. (arXiv:2311.14919v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Julius Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1">Andreas Vlachos</a></p>
<p>Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest
expected utility over the model distribution for some utility function. It has
been shown to improve accuracy over beam search in conditional language
generation problems and especially neural machine translation, in both human
and automatic evaluations. However, the standard sampling-based algorithm for
MBR is substantially more computationally expensive than beam search, requiring
a large number of samples as well as a quadratic number of calls to the utility
function, limiting its applicability. We describe an algorithm for MBR which
gradually grows the number of samples used to estimate the utility while
pruning hypotheses that are unlikely to have the highest utility according to
confidence estimates obtained with bootstrap sampling. Our method requires
fewer samples and drastically reduces the number of calls to the utility
function compared to standard MBR while being statistically indistinguishable
in terms of accuracy. We demonstrate the effectiveness of our approach in
experiments on three language pairs, using chrF++ and COMET as
utility/evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14949">Vector-Quantized Prompt Learning for Paraphrase Generation. (arXiv:2311.14949v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haotian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peidong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianggen Liu</a></p>
<p>Deep generative modeling of natural languages has achieved many successes,
such as producing fluent sentences and translating from one language into
another. However, the development of generative modeling techniques for
paraphrase generation still lags behind largely due to the challenges in
addressing the complex conflicts between expression diversity and semantic
preservation. This paper proposes to generate diverse and high-quality
paraphrases by exploiting the pre-trained models with instance-dependent
prompts. To learn generalizable prompts, we assume that the number of abstract
transforming patterns of paraphrase generation (governed by prompts) is finite
and usually not large. Therefore, we present vector-quantized prompts as the
cues to control the generation of pre-trained models. Extensive experiments
demonstrate that the proposed method achieves new state-of-art results on three
benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release
all the code upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14966">Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains. (arXiv:2311.14966v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1">Chia-Chien Hung</a>, <a href="http://arxiv.org/find/cs/1/au:+Rim_W/0/1/0/all/0/1">Wiem Ben Rim</a>, <a href="http://arxiv.org/find/cs/1/au:+Frost_L/0/1/0/all/0/1">Lindsay Frost</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruckner_L/0/1/0/all/0/1">Lars Bruckner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1">Carolin Lawrence</a></p>
<p>High-risk domains pose unique challenges that require language models to
provide accurate and safe responses. Despite the great success of large
language models (LLMs), such as ChatGPT and its variants, their performance in
high-risk domains remains unclear. Our study delves into an in-depth analysis
of the performance of instruction-tuned LLMs, focusing on factual accuracy and
safety adherence. To comprehensively assess the capabilities of LLMs, we
conduct experiments on six NLP datasets including question answering and
summarization tasks within two high-risk domains: legal and medical. Further
qualitative analysis highlights the existing limitations inherent in current
LLMs when evaluating in high-risk domains. This underscores the essential
nature of not only improving LLM capabilities but also prioritizing the
refinement of domain-specific metrics, and embracing a more human-centric
approach to enhance safety and factual reliability. Our findings advance the
field toward the concerns of properly evaluating LLMs in high-risk domains,
aiming to steer the adaptability of LLMs in fulfilling societal obligations and
aligning with forthcoming regulations, such as the EU AI Act.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15016">E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation. (arXiv:2311.15016v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1">Fengyi Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Quan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhendong Mao</a></p>
<p>Achieving empathy is a crucial step toward humanized dialogue systems.
Current approaches for empathetic dialogue generation mainly perceive an
emotional label to generate an empathetic response conditioned on it, which
simply treat emotions independently, but ignore the intrinsic emotion
correlation in dialogues, resulting in inaccurate emotion perception and
unsuitable response generation. In this paper, we propose a novel emotion
correlation enhanced empathetic dialogue generation framework, which
comprehensively realizes emotion correlation learning, utilization, and
supervising. Specifically, a multi-resolution emotion graph is devised to
capture context-based emotion interactions from different resolutions, further
modeling emotion correlation. Then we propose an emotion correlation enhanced
decoder, with a novel correlation-aware aggregation and soft/hard strategy,
respectively improving the emotion perception and response generation.
Experimental results on the benchmark dataset demonstrate the superiority of
our model in both empathetic perception and expression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15023">Offensive Language Identification in Transliterated and Code-Mixed Bangla. (arXiv:2311.15023v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1">Md Nishat Raihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanmoy_U/0/1/0/all/0/1">Umma Hani Tanmoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1">Anika Binte Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+North_K/0/1/0/all/0/1">Kai North</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1">Tharindu Ranasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1">Antonios Anastasopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1">Marcos Zampieri</a></p>
<p>Identifying offensive content in social media is vital for creating safe
online communities. Several recent studies have addressed this problem by
creating datasets for various languages. In this paper, we explore offensive
language identification in texts with transliterations and code-mixing,
linguistic phenomena common in multilingual societies, and a known challenge
for NLP systems. We introduce TB-OLID, a transliterated Bangla offensive
language dataset containing 5,000 manually annotated comments. We train and
fine-tune machine learning models on TB-OLID, and we evaluate their results on
this dataset. Our results show that English pre-trained transformer-based
models, such as fBERT and HateBERT achieve the best performance on this
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15029">nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence Inciting Text Detection in Bangla. (arXiv:2311.15029v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1">Md Nishat Raihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1">Dhiman Goswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Puspo_S/0/1/0/all/0/1">Sadiya Sayara Chowdhury Puspo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1">Marcos Zampieri</a></p>
<p>In this paper, we discuss the nlpBDpatriots entry to the shared task on
Violence Inciting Text Detection (VITD) organized as part of the first workshop
on Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task
is to identify and classify the violent threats, that provoke further unlawful
violent acts. Our best-performing approach for the task is two-step
classification using back translation and multilinguality which ranked 6th out
of 27 teams with a macro F1 score of 0.74.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15032">nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis. (arXiv:2311.15032v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1">Dhiman Goswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1">Md Nishat Raihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Puspo_S/0/1/0/all/0/1">Sadiya Sayara Chowdhury Puspo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1">Marcos Zampieri</a></p>
<p>In this paper, we discuss the nlpBDpatriots entry to the shared task on
Sentiment Analysis of Bangla Social Media Posts organized at the first workshop
on Bangla Language Processing (BLP) co-located with EMNLP. The main objective
of this task is to identify the polarity of social media content using a Bangla
dataset annotated with positive, neutral, and negative labels provided by the
shared task organizers. Our best system for this task is a transfer learning
approach with data augmentation which achieved a micro F1 score of 0.71. Our
best system ranked 12th among 30 teams that participated in the competition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15054">Detection of developmental language disorder in Cypriot Greek children using a machine learning neural network algorithm. (arXiv:2311.15054v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Georgiou_G/0/1/0/all/0/1">Georgios P. Georgiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1">Elena Theodorou</a></p>
<p>Children with developmental language disorder (DLD) encounter difficulties in
acquiring various language structures. Early identification and intervention
are crucial to prevent negative long-term outcomes impacting the academic,
social, and emotional development of children. The study aims to develop an
automated method for the identification of DLD using artificial intelligence,
specifically a neural network machine learning algorithm. This protocol is
applied for the first time in Cypriot Greek children, which is generally
considered underresearched in the context of DLD. The neural network model was
trained using perceptual and production data elicited from children with DLD
and healthy controls. The k-fold technique was used to crossvalidate the
algorithm. The performance of the model was evaluated using metrics such as
accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability
to make accurate predictions on a set of unseen data. The results demonstrated
high classification values for all metrics (between 0.92 and 0.98), indicating
the high accuracy of the neural model in classifying children with DLD.
Additionally, the variable importance analysis revealed that the language
production skills of children had a more significant impact on the performance
of the model compared to perception skills. Neural networks represent powerful
tools for detecting DLD, providing early and quick assessments of the disorder,
and having the potential to improve clinical outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15055">Automatically Finding and Categorizing Replication Studies. (arXiv:2311.15055v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruiter_B/0/1/0/all/0/1">Bob de Ruiter</a></p>
<p>In many fields of experimental science, papers that failed to replicate
continue to be cited as a result of the poor discoverability of replication
studies. As a first step to creating a system that automatically finds
replication studies for a given paper, 334 replication studies and 344
replicated studies were collected. Replication studies could be identified in
the dataset based on text content at a higher rate than chance (AUROC = 0.886).
</p>
<p>Additionally, successful replication studies could be distinguished from
failed replication studies at a higher rate than chance (AUROC = 0.664).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.00225">Learning From How Humans Correct. (arXiv:2102.00225v17 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tong Guo</a></p>
<p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we need to relabel
the noisy data in our dataset for our industry application. The experiment
result shows that our learn-on-correction method improve the classification
accuracy from 91.7% to 92.5% in test dataset. The 91.7% accuracy is trained on
the corrected dataset, which improve the baseline from 83.3% to 91.7% in test
dataset. The accuracy under human evaluation achieves more than 97%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.10021">Neuradicon: operational representation learning of neuroimaging reports. (arXiv:2107.10021v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Watkins_H/0/1/0/all/0/1">Henry Watkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1">Robert Gray</a>, <a href="http://arxiv.org/find/cs/1/au:+Julius_A/0/1/0/all/0/1">Adam Julius</a>, <a href="http://arxiv.org/find/cs/1/au:+Mah_Y/0/1/0/all/0/1">Yee-Haur Mah</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinaya_W/0/1/0/all/0/1">Walter H.L. Pinaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_P/0/1/0/all/0/1">Paul Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1">Ashwani Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Engleitner_H/0/1/0/all/0/1">Holger Engleitner</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1">Jorge Cardoso</a>, <a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1">Sebastien Ourselin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rees_G/0/1/0/all/0/1">Geraint Rees</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaeger_R/0/1/0/all/0/1">Rolf Jaeger</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1">Parashkev Nachev</a></p>
<p>Radiological reports typically summarize the content and interpretation of
imaging studies in unstructured form that precludes quantitative analysis. This
limits the monitoring of radiological services to throughput undifferentiated
by content, impeding specific, targeted operational optimization. Here we
present Neuradicon, a natural language processing (NLP) framework for
quantitative analysis of neuroradiological reports. Our framework is a hybrid
of rule-based and artificial intelligence models to represent neurological
reports in succinct, quantitative form optimally suited to operational
guidance. We demonstrate the application of Neuradicon to operational
phenotyping of a corpus of 336,569 reports, and report excellent
generalizability across time and two independent healthcare institutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10498">PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. (arXiv:2206.10498v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1">Karthik Valmeekam</a>, <a href="http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1">Matthew Marquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1">Alberto Olmo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1">Sarath Sreedharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1">Subbarao Kambhampati</a></p>
<p>Generating plans of action, and reasoning about change have long been
considered a core competence of intelligent agents. It is thus no surprise that
evaluating the planning and reasoning capabilities of large language models
(LLMs) has become a hot topic of research. Most claims about LLM planning
capabilities are however based on common sense tasks-where it becomes hard to
tell whether LLMs are planning or merely retrieving from their vast world
knowledge. There is a strong need for systematic and extensible planning
benchmarks with sufficient diversity to evaluate whether LLMs have innate
planning capabilities. Motivated by this, we propose PlanBench, an extensible
benchmark suite based on the kinds of domains used in the automated planning
community, especially in the International Planning Competition, to test the
capabilities of LLMs in planning or reasoning about actions and change.
PlanBench provides sufficient diversity in both the task domains and the
specific planning capabilities. Our studies also show that on many critical
capabilities-including plan generation-LLM performance falls quite short, even
with the SOTA models. PlanBench can thus function as a useful marker of
progress of LLMs in planning and reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.04568">The Impact of Data Corruption on Named Entity Recognition for Low-resourced Languages. (arXiv:2208.04568v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fokam_M/0/1/0/all/0/1">Manuel Fokam</a>, <a href="http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1">Michael Beukman</a></p>
<p>Data availability and quality are major challenges in natural language
processing for low-resourced languages. In particular, there is significantly
less data available than for higher-resourced languages. This data is also
often of low quality, rife with errors, invalid text or incorrect annotations.
Many prior works focus on dealing with these problems, either by generating
synthetic data, or filtering out low-quality parts of datasets. We instead
investigate these factors more deeply, by systematically measuring the effect
of data quantity and quality on the performance of pre-trained language models
in a low-resourced setting. Our results show that having fewer
completely-labelled sentences is significantly better than having more
sentences with missing labels; and that models can perform remarkably well with
only 10% of the training data. Importantly, these results are consistent across
ten low-resource languages, English, and four pre-trained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06373">InferEM: Inferring the Speaker&#x27;s Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1">Guoqing Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhigang Zeng</a></p>
<p>Current approaches to empathetic response generation typically encode the
entire dialogue history directly and put the output into a decoder to generate
friendly feedback. These methods focus on modelling contextual information but
neglect capturing the direct intention of the speaker. We argue that the last
utterance in the dialogue empirically conveys the intention of the speaker.
Consequently, we propose a novel model named InferEM for empathetic response
generation. We separately encode the last utterance and fuse it with the entire
dialogue through the multi-head attention based intention fusion module to
capture the speaker's intention. Besides, we utilize previous utterances to
predict the last utterance, which simulates human's psychology to guess what
the interlocutor may speak in advance. To balance the optimizing rates of the
utterance prediction and response generation, a multi-task learning strategy is
designed for InferEM. Experimental results demonstrate the plausibility and
validity of InferEM in improving empathetic expression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10013">DocAsRef: An Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely. (arXiv:2212.10013v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1">Forrest Sheng Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1">Ruixuan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1">Ge Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yinfei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hebi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1">Minghui Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Youbiao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a></p>
<p>Automated summary quality assessment falls into two categories:
reference-based and reference-free. Reference-based metrics, historically
deemed more accurate due to the additional information provided by
human-written references, are limited by their reliance on human input. In this
paper, we hypothesize that the comparison methodologies used by some
reference-based metrics to evaluate a system summary against its corresponding
reference can be effectively adapted to assess it against its source document,
thereby transforming these metrics into reference-free ones. Experimental
results support this hypothesis. After being repurposed reference-freely, the
zero-shot BERTScore using the pretrained DeBERTa-large-MNLI model of &lt;0.5B
parameters consistently outperforms its original reference-based version across
various aspects on the SummEval and Newsroom datasets. It also excels in
comparison to most existing reference-free metrics and closely competes with
zero-shot summary evaluators based on GPT-3.5.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15445">IRFL: Image Recognition of Figurative Language. (arXiv:2303.15445v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1">Ron Yosef</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1">Yonatan Bitton</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1">Dafna Shahaf</a></p>
<p>Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge. In this work, we develop the Image Recognition of
Figurative Language (IRFL) dataset. We leverage human annotation and an
automatic pipeline we created to generate a multimodal dataset, and introduce
two novel tasks as a benchmark for multimodal figurative language
understanding. We experimented with state-of-the-art vision and language models
and found that the best (22%) performed substantially worse than humans (97%).
We release our dataset, benchmark, and code, in hopes of driving the
development of models that can better understand figurative language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04947">Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1">Tao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Junwen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1">Siddhartha Brahma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1">Joshua Ainslie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kenton Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yanqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1">Nan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1">Vincent Y. Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuexin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Wei Chang</a></p>
<p>We propose Conditional Adapter (CoDA), a parameter-efficient transfer
learning method that also improves inference efficiency. CoDA generalizes
beyond standard adapter approaches to enable a new way of balancing speed and
accuracy using conditional computation. Starting with an existing dense
pretrained model, CoDA adds sparse activation together with a small number of
new parameters and a light-weight training phase. Our experiments demonstrate
that the CoDA approach provides an unexpectedly efficient way to transfer
knowledge. Across a variety of language, vision, and speech tasks, CoDA
achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter
approaches with moderate to no accuracy loss and the same parameter efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07637">Text2Cohort: Facilitating Intuitive Access to Biomedical Data with Natural Language Cohort Discovery. (arXiv:2305.07637v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1">Pranav Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanhere_A/0/1/0/all/0/1">Adway Kanhere</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1">Paul H. Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1">Vishwa S. Parekh</a></p>
<p>The Imaging Data Commons (IDC) is a cloud-based database that provides
researchers with open access to cancer imaging data, with the goal of
facilitating collaboration. However, cohort discovery within the IDC database
has a significant technical learning curve. Recently, large language models
(LLM) have demonstrated exceptional utility for natural language processing
tasks. We developed Text2Cohort, a LLM-powered toolkit to facilitate
user-friendly natural language cohort discovery in the IDC. Our method
translates user input into IDC queries using grounding techniques and returns
the query's response. We evaluate Text2Cohort on 50 natural language inputs,
from information extraction to cohort discovery. Our toolkit successfully
generated responses with an 88% accuracy and 0.94 F1 score. We demonstrate that
Text2Cohort can enable researchers to discover and curate cohorts on IDC with
high levels of accuracy using natural language in a more intuitive and
user-friendly way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09620">AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction. (arXiv:2305.09620v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junsol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byungkyu Lee</a></p>
<p>Large language models (LLMs) that produce human-like responses have begun to
revolutionize research practices in the social sciences. This paper shows how
we can integrate LLMs and social surveys to accurately predict individual
responses to survey questions that were not asked before. We develop a novel
methodological framework to personalize LLMs by considering the meaning of
survey questions derived from their text, the latent beliefs of individuals
inferred from their response patterns, and the temporal contexts across
different survey periods through fine-tuning LLMs with survey data. Using the
General Social Survey from 1972 to 2021, we show that the fine-tuned model
based on Alpaca-7b can predict individual responses to survey questions that
are partially missing as well as entirely missing. The remarkable prediction
capabilities allow us to fill in missing trends with high confidence and
pinpoint when public attitudes changed, such as the rising support for same-sex
marriage. We discuss practical constraints, socio-demographic representation,
and ethical concerns regarding individual autonomy and privacy when using LLMs
for opinion prediction. This study demonstrates that LLMs and surveys can
mutually enhance each other's capabilities: LLMs broaden survey potential,
while surveys improve the alignment of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11853">How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shuaichen Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fosler_Lussier_E/0/1/0/all/0/1">Eric Fosler-Lussier</a></p>
<p>Large language models (LLMs) with in-context learning have demonstrated
remarkable capability in the text-to-SQL task. Previous research has prompted
LLMs with various demonstration-retrieval strategies and intermediate reasoning
steps to enhance the performance of LLMs. However, those works often employ
varied strategies when constructing the prompt text for text-to-SQL inputs,
such as databases and demonstration examples. This leads to a lack of
comparability in both the prompt constructions and their primary contributions.
Furthermore, selecting an effective prompt construction has emerged as a
persistent problem for future research. To address this limitation, we
comprehensively investigate the impact of prompt constructions across various
settings and provide insights into prompt constructions for future text-to-SQL
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13547">Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Haoqi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1">Qihuang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhiliang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1">Xin Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Text classification tasks often encounter few shot scenarios with limited
labeled data, and addressing data scarcity is crucial. Data augmentation with
mixup has shown to be effective on various text classification tasks. However,
most of the mixup methods do not consider the varying degree of learning
difficulty in different stages of training and generate new samples with one
hot labels, resulting in the model over confidence. In this paper, we propose a
self evolution learning (SE) based mixup approach for data augmentation in text
classification, which can generate more adaptive and model friendly pesudo
samples for the model training. SE focuses on the variation of the model's
learning ability. To alleviate the model confidence, we introduce a novel
instance specific label smoothing approach, which linearly interpolates the
model's output and one hot labels of the original samples to generate new soft
for label mixing up. Through experimental analysis, in addition to improving
classification accuracy, we demonstrate that SE also enhances the model's
generalize ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14930">In-Context Impersonation Reveals Large Language Models&#x27; Strengths and Biases. (arXiv:2305.14930v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1">Leonard Salewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaniz_S/0/1/0/all/0/1">Stephan Alaniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rio_Torto_I/0/1/0/all/0/1">Isabel Rio-Torto</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1">Eric Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1">Zeynep Akata</a></p>
<p>In everyday conversations, humans can take on different roles and adapt their
vocabulary to their chosen roles. We explore whether LLMs can take on, that is
impersonate, different roles when they generate text in-context. We ask LLMs to
assume different personas before solving vision and language tasks. We do this
by prefixing the prompt with a persona that is associated either with a social
identity or domain expertise. In a multi-armed bandit task, we find that LLMs
pretending to be children of different ages recover human-like developmental
stages of exploration. In a language-based reasoning task, we find that LLMs
impersonating domain experts perform better than LLMs impersonating non-domain
experts. Finally, we test whether LLMs' impersonations are complementary to
visual information when describing different categories. We find that
impersonation can improve performance: an LLM prompted to be a bird expert
describes birds better than one prompted to be a car expert. However,
impersonation can also uncover LLMs' biases: an LLM prompted to be a man
describes cars better than one prompted to be a woman. These findings
demonstrate that LLMs are capable of taking on diverse roles and that this
in-context impersonation can be used to uncover their hidden strengths and
biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03081">Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs. (arXiv:2306.03081v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lew_A/0/1/0/all/0/1">Alexander K. Lew</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhi_Xuan_T/0/1/0/all/0/1">Tan Zhi-Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Grand_G/0/1/0/all/0/1">Gabriel Grand</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1">Vikash K. Mansinghka</a></p>
<p>Even after fine-tuning and reinforcement learning, large language models
(LLMs) can be difficult, if not impossible, to control reliably with prompts
alone. We propose a new inference-time approach to enforcing syntactic and
semantic constraints on the outputs of LLMs, called sequential Monte Carlo
(SMC) steering. The key idea is to specify language generation tasks as
posterior inference problems in a class of discrete probabilistic sequence
models, and replace standard decoding with sequential Monte Carlo inference.
For a computational cost similar to that of beam search, SMC can steer LLMs to
solve diverse tasks, including infilling, generation under syntactic
constraints, and prompt intersection. To facilitate experimentation with SMC
steering, we present a probabilistic programming library, LLaMPPL
(https://github.com/probcomp/hfppl), for concisely specifying new generation
tasks as language model probabilistic programs, and automating steering of
LLaMA-family Transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11845">Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gerling_C/0/1/0/all/0/1">Christopher Gerling</a>, <a href="http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1">Stefan Lessmann</a></p>
<p>Traditional banks face increasing competition from FinTechs in the rapidly
evolving financial ecosystem. Raising operational efficiency is vital to
address this challenge. Our study aims to improve the efficiency of
document-intensive business processes in banking. To that end, we first review
the landscape of business documents in the retail segment. Banking documents
often contain text, layout, and visuals, suggesting that document analytics and
process automation require more than plain natural language processing (NLP).
To verify this and assess the incremental value of visual cues when processing
business documents, we compare a recently proposed multimodal model called
LayoutXLM to powerful text classifiers (e.g., BERT) and large language models
(e.g., GPT) in a case study related to processing company register extracts.
The results confirm that incorporating layout information in a model
substantially increases its performance. Interestingly, we also observed that
more than 75% of the best model performance (in terms of the F1 score) can be
achieved with as little as 30% of the training data. This shows that the demand
for data labeled data to set up a multi-modal model can be moderate, which
simplifies real-world applications of multimodal document analytics. Our study
also sheds light on more specific practices in the scope of calibrating a
multimodal banking document classifier, including the need for fine-tuning. In
sum, the paper contributes original empirical evidence on the effectiveness and
efficiency of multi-model models for document processing in the banking
business and offers practical guidance on how to unlock this potential in
day-to-day operations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15176">RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1">Katherine A. Keith</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sergey Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a>, <a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1">Jonathan Bragg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1">Rohit Bhattacharya</a></p>
<p>Confounding is a significant obstacle to unbiased estimation of causal
effects from observational data. For settings with high-dimensional covariates
-- such as text data, genomics, or the behavioral social sciences --
researchers have proposed methods to adjust for confounding by adapting machine
learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In
this work, we build on a promising empirical evaluation strategy that
simplifies evaluation design and uses real data: subsampling randomized
controlled trials (RCTs) to create confounded observational datasets while
using the average causal effects from the RCTs as ground-truth. We contribute a
new sampling algorithm, which we call RCT rejection sampling, and provide
theoretical guarantees that causal identification holds in the observational
data to allow for valid comparisons to the ground-truth RCT. Using synthetic
data, we show our algorithm indeed results in low bias when oracle estimators
are evaluated on the confounded samples, which is not always the case for a
previously proposed algorithm. In addition to this identification result, we
highlight several finite data considerations for evaluation designers who plan
to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data
considerations with a novel, real-world RCT -- which we release publicly --
consisting of approximately 70k observations and text data as high-dimensional
covariates. Together, these contributions build towards a broader agenda of
improved empirical evaluation for causal estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15992">Towards Codable Watermarking for Injecting Multi-bit Information to LLM. (arXiv:2307.15992v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lean Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenkai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Deli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xu Sun</a></p>
<p>As large language models (LLMs) generate texts with increasing fluency and
realism, there is a growing need to identify the source of texts to prevent the
abuse of LLMs. Text watermarking techniques have proven reliable in
distinguishing whether a text is generated by LLMs by injecting hidden patterns
into the generated texts. However, we argue that existing watermarking methods
for LLMs are encoding-inefficient (only contain one bit of information -
whether it is generated from an LLM or not) and cannot flexibly meet the
diverse information encoding needs (such as encoding model version, generation
time, user id, etc.) in different LLMs application scenarios. In this work, we
conduct the first systematic study on the topic of Codable Text Watermarking
for LLMs (CTWL) that allows text watermarks to carry more customizable
information. First of all, we study the taxonomy of LLM watermarking technology
and give a mathematical formulation for CTWL. Additionally, we provide a
comprehensive evaluation system for CTWL: (1) watermarking success rate, (2)
robustness against various corruptions, (3) coding rate of payload information,
(4) encoding and decoding efficiency, (5) impacts on the quality of the
generated text. To meet the requirements of these non-Pareto-improving metrics,
we devise a CTWL method named Balance-Marking, based on the motivation of
ensuring that available and unavailable vocabularies for encoding information
have approximately equivalent probabilities. Compared to the random vocabulary
partitioning extended from the existing work, a probability-balanced vocabulary
partition can significantly improve the quality of the generated text.
Extensive experimental results have shown that our method outperforms a direct
baseline under comprehensive evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00802">GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatzikyriakidis_S/0/1/0/all/0/1">Stergios Chatzikyriakidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Qwaider_C/0/1/0/all/0/1">Chatrine Qwaider</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolokousis_I/0/1/0/all/0/1">Ilias Kolokousis</a>, <a href="http://arxiv.org/find/cs/1/au:+Koula_C/0/1/0/all/0/1">Christina Koula</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadakis_D/0/1/0/all/0/1">Dimitris Papadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakellariou_E/0/1/0/all/0/1">Efthymia Sakellariou</a></p>
<p>In this paper, we present a dataset for the computational study of a number
of Modern Greek dialects. It consists of raw text data from four dialects of
Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is
of considerable size, albeit imbalanced, and presents the first attempt to
create large scale dialectal resources of this type for Modern Greek dialects.
We then use the dataset to perform dialect idefntification. We experiment with
traditional ML algorithms, as well as simple DL architectures. The results show
very good performance on the task, potentially revealing that the dialects in
question have distinct enough characteristics allowing even simple ML models to
perform well on the task. Error analysis is performed for the top performing
algorithms showing that in a number of cases the errors are due to insufficient
dataset cleaning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10819">Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zekun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baolin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pengcheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xifeng Yan</a></p>
<p>Large Language Models (LLMs) have demonstrated exceptional proficiency in
instruction-following, becoming increasingly crucial across various
applications. However, this capability brings with it the risk of prompt
injection attacks, where attackers inject instructions into LLMs' input to
elicit undesirable actions or content. Understanding the robustness of LLMs
against such attacks is vital for their safe implementation. In this work, we
establish a benchmark to evaluate the robustness of instruction-following LLMs
against prompt injection attacks. Our objective is to determine the extent to
which LLMs can be influenced by injected instructions and their ability to
differentiate between these injected and original target instructions. Through
extensive experiments with leading instruction-following LLMs, we uncover
significant vulnerabilities in their robustness to such attacks. Our results
indicate that some models are overly tuned to follow any embedded instructions
in the prompt, overly focusing on the latter parts of the prompt without fully
grasping the entire context. By contrast, models with a better grasp of the
context and instruction-following capabilities will potentially be more
susceptible to compromise by injected instructions. This underscores the need
to shift the focus from merely enhancing LLMs' instruction-following
capabilities to improving their overall comprehension of prompts and
discernment of instructions that are appropriate to follow. We hope our
in-depth analysis offers insights into the underlying causes of these
vulnerabilities, aiding in the development of future solutions. Code and data
are available at
https://github.com/Leezekun/instruction-following-robustness-eval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14306">Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yuansheng Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Sichao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+wu_X/0/1/0/all/0/1">Xinyu wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Hui Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuli Zhou</a></p>
<p>Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01947">TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models. (arXiv:2309.01947v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1">Yuan Shangguan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haichuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Danni Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chunyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fathullah_Y/0/1/0/all/0/1">Yassir Fathullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalmia_A/0/1/0/all/0/1">Ayushi Dalmia</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1">Raghuraman Krishnamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1">Ozlem Kalinli</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Junteng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1">Jay Mahadeokar</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1">Mike Seltzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a></p>
<p>Automatic Speech Recognition (ASR) models need to be optimized for specific
hardware before they can be deployed on devices. This can be done by tuning the
model's hyperparameters or exploring variations in its architecture.
Re-training and re-validating models after making these changes can be a
resource-intensive task. This paper presents TODM (Train Once Deploy Many), a
new approach to efficiently train many sizes of hardware-friendly on-device ASR
models with comparable GPU-hours to that of a single training job. TODM
leverages insights from prior work on Supernet, where Recurrent Neural Network
Transducer (RNN-T) models share weights within a Supernet. It reduces layer
sizes and widths of the Supernet to obtain subnetworks, making them smaller
models suitable for all hardware types. We introduce a novel combination of
three techniques to improve the outcomes of the TODM Supernet: adaptive
dropouts, an in-place Alpha-divergence knowledge distillation, and the use of
ScaledAdam optimizer. We validate our approach by comparing Supernet-trained
versus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using
LibriSpeech. Results demonstrate that our TODM Supernet either matches or
surpasses the performance of manually tuned models by up to a relative of 3%
better in word error rate (WER), while efficiently keeping the cost of training
many models at a small constant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17255">Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaoyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hastings_J/0/1/0/all/0/1">Janna Hastings</a>, <a href="http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1">Ernesto Jim&#xe9;nez-Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1">Vanessa L&#xf3;pez</a>, <a href="http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1">Pierre Monnin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pesquita_C/0/1/0/all/0/1">Catia Pesquita</a>, <a href="http://arxiv.org/find/cs/1/au:+Skoda_P/0/1/0/all/0/1">Petr &#x160;koda</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamma_V/0/1/0/all/0/1">Valentina Tamma</a></p>
<p>The term life sciences refers to the disciplines that study living organisms
and life processes, and include chemistry, biology, medicine, and a range of
other related disciplines. Research efforts in life sciences are heavily
data-driven, as they produce and consume vast amounts of scientific data, much
of which is intrinsically relational and graph-structured.
</p>
<p>The volume of data and the complexity of scientific concepts and relations
referred to therein promote the application of advanced knowledge-driven
technologies for managing and interpreting data, with the ultimate aim to
advance scientific discovery.
</p>
<p>In this survey and position paper, we discuss recent developments and
advances in the use of graph-based technologies in life sciences and set out a
vision for how these technologies will impact these fields into the future. We
focus on three broad topics: the construction and management of Knowledge
Graphs (KGs), the use of KGs and associated technologies in the discovery of
new knowledge, and the use of KGs in artificial intelligence applications to
support explanations (explainable AI). We select a few exemplary use cases for
each topic, discuss the challenges and open research questions within these
topics, and conclude with a perspective and outlook that summarizes the
overarching challenges and their potential solutions as a guide for future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01825">Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zahweh_M/0/1/0/all/0/1">Mohamad Hasan Zahweh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1">Hasan Nasrallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1">Mustafa Shukor</a>, <a href="http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1">Ghaleb Faour</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1">Ali J. Ghandour</a></p>
<p>Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01828">Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shreim_H/0/1/0/all/0/1">Hossein Shreim</a>, <a href="http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1">Abdul Karim Gizzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1">Ali J. Ghandour</a></p>
<p>eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01889">Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei Zaharia</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a></p>
<p>Transformers have emerged as the architecture of choice for many
state-of-the-art AI models, showcasing exceptional performance across a wide
range of AI applications. However, the memory demands imposed by Transformers
limit their ability to handle long sequences, thereby posing challenges in
utilizing videos, actions, and other long-form sequences and modalities in
complex environments. We present a novel approach, Ring Attention with
Blockwise Transformers (Ring Attention), which leverages blockwise computation
of self-attention and feedforward to distribute long sequences across multiple
devices while fully overlapping the communication of key-value blocks with the
computation of blockwise attention. Our approach enables training and inference
of sequences that are up to device count times longer than those achievable by
prior memory-efficient Transformers, without resorting to approximations or
incurring additional communication and computation overheads. Extensive
experiments on language modeling and reinforcement learning tasks demonstrate
the effectiveness of our approach in allowing millions of tokens context size
and improving performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04914">Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks. (arXiv:2310.04914v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1">Avinash Madasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhiwandiwalla_A/0/1/0/all/0/1">Anahita Bhiwandiwalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1">Vasudev Lal</a></p>
<p>Foundational multimodal models pre-trained on large scale image-text pairs or
video-text pairs or both have shown strong generalization abilities on
downstream tasks. However unlike image-text models, pretraining video-text
models is always not feasible due to the difficulty in collecting large-scale
clean and aligned data, and exponential computational costs involved in the
pretraining phase. Therefore, the pertinent question to ask is: Can image-text
models be adapted to video tasks and is there any benefit to using these models
over pretraining directly on videos? In this work, we focus on this question by
proposing a detailed study on the generalization abilities of image-text models
when evaluated on video understanding tasks in a zero-shot setting. We
investigate 9 foundational image-text models on a diverse set of video tasks
that include video action recognition (video AR), video retrieval (video RT),
video question answering (video QA), video multiple choice (video MC) and video
captioning (video CP). Our experiments show that image-text models exhibit
impressive performance on video AR, video RT and video MC. Furthermore, they
perform moderately on video captioning and poorly on video QA. These findings
shed a light on the benefits of adapting foundational image-text models to an
array of video tasks while avoiding the costly pretraining step.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06422">Large Language Models for Propaganda Detection. (arXiv:2310.06422v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sprenkamp_K/0/1/0/all/0/1">Kilian Sprenkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1">Daniel Gordon Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Zavolokina_L/0/1/0/all/0/1">Liudmila Zavolokina</a></p>
<p>The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06627">What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models. (arXiv:2310.06627v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Letian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaotong Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhongkai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1">Yongshuo Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bingchen Zhao</a></p>
<p>Counterfactual reasoning, a fundamental aspect of human cognition, involves
contemplating alternatives to established facts or past events, significantly
enhancing our abilities in planning and decision-making. In light of the
advancements in current multi-modal large language models, we explore their
effectiveness in counterfactual reasoning. To facilitate this investigation, we
introduce a novel dataset, C-VQA, specifically designed to test the
counterfactual reasoning capabilities of modern multi-modal large language
models. This dataset is constructed by infusing original questions with
counterfactual presuppositions, spanning various types such as numerical and
boolean queries. It encompasses a mix of real and synthetic data, representing
a wide range of difficulty levels. Our thorough evaluations of contemporary
vision-language models using this dataset have revealed substantial performance
drops, with some models showing up to a 40\% decrease, highlighting a
significant gap between current models and human-like vision reasoning
capabilities. We hope our dataset will serve as a vital benchmark for
evaluating the counterfactual reasoning capabilities of models. Code and
dataset are publicly available at https://bzhao.me/C-VQA/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08540">Do pretrained Transformers Really Learn In-context by Gradient Descent?. (arXiv:2310.08540v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lingfeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Aayush Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1">Daniel Khashabi</a></p>
<p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?
Several recent works draw analogies between the dynamics of GD and the emergent
behavior of ICL in large language models. However, these works make assumptions
far from the realistic natural language setting in which language models are
trained. Therefore, such discrepancies between theory and practice necessitate
further investigation to validate their applicability.
</p>
<p>We start by highlighting the assumptions in prior works that construct
Transformer weights to simulate gradient descent. Their experiments with
training Transformers on ICL objective, inconsistencies in the order
sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity
to parameter changes are some examples of mismatch from the real-world setting.
</p>
<p>Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural
setting. We conduct comprehensive empirical analyses on language models
pretrained on natural data (LLaMa-7B). Our comparisons on various performance
metrics highlight the inconsistent behavior of ICL and GD as a function of
various factors such as datasets, models, and the number of demonstrations. We
observe that ICL and GD modify the output distribution of language models
differently. These results indicate that the equivalence between ICL and GD is
an open hypothesis, requires nuanced considerations, and calls for further
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10520">Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuxiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1">Guanting Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiran Xu</a></p>
<p>Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time-consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods. Our code has been released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14505">Sentiment analysis with adaptive multi-head attention in Transformer. (arXiv:2310.14505v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanfei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Demeter_D/0/1/0/all/0/1">David Demeter</a></p>
<p>We propose a novel framework based on the attention mechanism to identify the
sentiment of a movie review document. Previous efforts on deep neural networks
with attention mechanisms focus on encoder and decoder with fixed numbers of
multi-head attention. Therefore, we need a mechanism to stop the attention
process automatically if no more useful information can be read from the
memory.In this paper, we propose an adaptive multi-head attention architecture
(AdaptAttn) which varies the number of attention heads based on length of
sentences. AdaptAttn has a data preprocessing step where each document is
classified into any one of the three bins small, medium or large based on
length of the sentence. The document classified as small goes through two heads
in each layer, the medium group passes four heads and the large group is
processed by eight heads. We examine the merit of our model on the Stanford
large movie review dataset. The experimental results show that the F1 score
from our model is on par with the baseline model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18332">WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models. (arXiv:2310.18332v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jun-Yan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhi-Qi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingdong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wangmeng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xianhui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1">Xiaoyang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zengke Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yusen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yifeng Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a></p>
<p>This paper introduces WordArt Designer, a user-driven framework for artistic
typography synthesis, relying on the Large Language Model (LLM). The system
incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo
modules. 1) The LLM Engine, empowered by the LLM (e.g., GPT-3.5), interprets
user inputs and generates actionable prompts for the other modules, thereby
transforming abstract concepts into tangible designs. 2) The SemTypo module
optimizes font designs using semantic concepts, striking a balance between
artistic transformation and readability. 3) Building on the semantic layout
provided by the SemTypo module, the StyTypo module creates smooth, refined
images. 4) The TexTypo module further enhances the design's aesthetics through
texture rendering, enabling the generation of inventive textured fonts.
Notably, WordArt Designer highlights the fusion of generative AI with artistic
typography. Experience its capabilities on ModelScope:
https://www.modelscope.cn/studios/WordArt/WordArt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18387">OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive Language Identification. (arXiv:2310.18387v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1">Dhiman Goswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1">Md Nishat Raihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_A/0/1/0/all/0/1">Antara Mahmud</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1">Antonios Anastasopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1">Marcos Zampieri</a></p>
<p>Code-mixing is a well-studied linguistic phenomenon when two or more
languages are mixed in text or speech. Several works have been conducted on
building datasets and performing downstream NLP tasks on code-mixed data.
Although it is not uncommon to observe code-mixing of three or more languages,
most available datasets in this domain contain code-mixed data from only two
languages. In this paper, we introduce OffMix-3L, a novel offensive language
identification dataset containing code-mixed data from three different
languages. We experiment with several models on this dataset and observe that
BanglishBERT outperforms other transformer-based models and GPT-3.5.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19106">PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators. (arXiv:2310.19106v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sulc_A/0/1/0/all/0/1">Antonin Sulc</a>, <a href="http://arxiv.org/find/cs/1/au:+Kammering_R/0/1/0/all/0/1">Raimund Kammering</a>, <a href="http://arxiv.org/find/cs/1/au:+Eichler_A/0/1/0/all/0/1">Annika Eichler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilksen_T/0/1/0/all/0/1">Tim Wilksen</a></p>
<p>Navigating the landscape of particle accelerators has become increasingly
challenging with recent surges in contributions. These intricate devices
challenge comprehension, even within individual facilities. To address this, we
introduce PACuna, a fine-tuned language model refined through publicly
available accelerator resources like conferences, pre-prints, and books. We
automated data collection and question generation to minimize expert
involvement and make the data publicly available. PACuna demonstrates
proficiency in addressing intricate accelerator questions, validated by
experts. Our approach shows adapting language models to scientific domains by
fine-tuning technical texts and auto-generated corpora capturing the latest
developments can further produce pre-trained models to answer some intricate
questions that commercially available assistants cannot and can serve as
intelligent assistants for individual facilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19736">Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zishan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Renren Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yufei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Dan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Supryadi/0/1/0/all/0/1">Supryadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Linhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1">Bojian Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities across
a broad spectrum of tasks. They have attracted significant attention and been
deployed in numerous downstream applications. Nevertheless, akin to a
double-edged sword, LLMs also present potential risks. They could suffer from
private data leaks or yield inappropriate, harmful, or misleading content.
Additionally, the rapid progress of LLMs raises concerns about the potential
emergence of superintelligent systems without adequate safeguards. To
effectively capitalize on LLM capacities as well as ensure their safe and
beneficial development, it is critical to conduct a rigorous and comprehensive
evaluation of LLMs.
</p>
<p>This survey endeavors to offer a panoramic perspective on the evaluation of
LLMs. We categorize the evaluation of LLMs into three major groups: knowledge
and capability evaluation, alignment evaluation and safety evaluation. In
addition to the comprehensive review on the evaluation methodologies and
benchmarks on these three aspects, we collate a compendium of evaluations
pertaining to LLMs' performance in specialized domains, and discuss the
construction of comprehensive evaluation platforms that cover LLM evaluations
on capabilities, alignment, safety, and applicability.
</p>
<p>We hope that this comprehensive overview will stimulate further research
interests in the evaluation of LLMs, with the ultimate goal of making
evaluation serve as a cornerstone in guiding the responsible development of
LLMs. We envision that this will channel their evolution into a direction that
maximizes societal benefit while minimizing potential risks. A curated list of
related papers has been publicly available at
https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05419">Mirror: A Universal Framework for Various Information Extraction Tasks. (arXiv:2311.05419v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1">Tong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Junfei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zijian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mengsong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guoliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1">Xiaoye Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhefeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1">Baoxing Huai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a></p>
<p>Sharing knowledge between information extraction tasks has always been a
challenge due to the diverse data formats and task variations. Meanwhile, this
divergence leads to information waste and increases difficulties in building
complex applications in real scenarios. Recent studies often formulate IE tasks
as a triplet extraction problem. However, such a paradigm does not support
multi-span and n-ary extraction, leading to weak versatility. To this end, we
reorganize IE problems into unified multi-slot tuples and propose a universal
framework for various IE tasks, namely Mirror. Specifically, we recast existing
IE tasks as a multi-span cyclic graph extraction problem and devise a
non-autoregressive graph decoding algorithm to extract all spans in a single
step. It is worth noting that this graph structure is incredibly versatile, and
it supports not only complex IE tasks, but also machine reading comprehension
and classification tasks. We manually construct a corpus containing 57 datasets
for model pretraining, and conduct experiments on 30 datasets across 8
downstream tasks. The experimental results demonstrate that our model has
decent compatibility and outperforms or reaches competitive performance with
SOTA systems under few-shot and zero-shot settings. The code, model weights,
and pretraining corpus are available at https://github.com/Spico197/Mirror .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06401">Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs. (arXiv:2311.06401v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_B/0/1/0/all/0/1">Benjamin C. Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kannampallil_T/0/1/0/all/0/1">Thomas Kannampallil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seunghwan Kim</a></p>
<p>EHR audit logs are a highly granular stream of events that capture clinician
activities, and is a significant area of interest for research in
characterizing clinician workflow on the electronic health record (EHR).
Existing techniques to measure the complexity of workflow through EHR audit
logs (audit logs) involve time- or frequency-based cross-sectional aggregations
that are unable to capture the full complexity of a EHR session. We briefly
evaluate the usage of transformer-based tabular language model (tabular LM) in
measuring the entropy or disorderedness of action sequences within workflow and
release the evaluated models publicly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07590">Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Scheurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1">Mikita Balesni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1">Marius Hobbhahn</a></p>
<p>We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11435">Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India. (arXiv:2311.11435v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1">Milind Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaushik_A/0/1/0/all/0/1">Abhishek Kaushik</a></p>
<p>In March 2020, the World Health Organisation declared COVID-19 a global
pandemic as it spread to nearly every country. By mid-2021, India had
introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure
successful vaccination in a densely populated country like India, understanding
public sentiment was crucial. Social media, particularly Reddit with over 430
million users, played a vital role in disseminating information. This study
employs data mining techniques to analyze Reddit data and gauge Indian
sentiments towards COVID-19 vaccines. Using Python's Text Blob library,
comments are annotated to assess general sentiments. Results show that most
Reddit users in India expressed neutrality about vaccination, posing a
challenge for the Indian government's efforts to vaccinate a significant
portion of the population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11509">Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information. (arXiv:2311.11509v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhengmian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Saayan Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1">Viswanathan Swaminathan</a></p>
<p>In recent years, Large Language Models (LLM) have emerged as pivotal tools in
various applications. However, these models are susceptible to adversarial
prompt attacks, where attackers can carefully curate input strings that lead to
undesirable outputs. The inherent vulnerability of LLMs stems from their
input-output mechanisms, especially when presented with intensely
out-of-distribution (OOD) inputs. This paper proposes a token-level detection
method to identify adversarial prompts, leveraging the LLM's capability to
predict the next token's probability. We measure the degree of the model's
perplexity and incorporate neighboring token information to encourage the
detection of contiguous adversarial prompt sequences. As a result, we propose
two methods: one that identifies each token as either being part of an
adversarial prompt or not, and another that estimates the probability of each
token being part of an adversarial prompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12986">Unsupervised Graph Attention Autoencoder for Attributed Networks using K-means Loss. (arXiv:2311.12986v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bekkaira_A/0/1/0/all/0/1">Abdelfateh Bekkaira</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellaouar_S/0/1/0/all/0/1">Slimane Bellaouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Oulad_Naoui_S/0/1/0/all/0/1">Slimane Oulad-Naoui</a></p>
<p>Several natural phenomena and complex systems are often represented as
networks. Discovering their community structure is a fundamental task for
understanding these networks. Many algorithms have been proposed, but recently,
Graph Neural Networks (GNN) have emerged as a compelling approach for enhancing
this task.In this paper, we introduce a simple, efficient, and
clustering-oriented model based on unsupervised \textbf{G}raph Attention
\textbf{A}uto\textbf{E}ncoder for community detection in attributed networks
(GAECO). The proposed model adeptly learns representations from both the
network's topology and attribute information, simultaneously addressing dual
objectives: reconstruction and community discovery. It places a particular
emphasis on discovering compact communities by robustly minimizing clustering
errors. The model employs k-means as an objective function and utilizes a
multi-head Graph Attention Auto-Encoder for decoding the representations.
Experiments conducted on three datasets of attributed networks show that our
method surpasses state-of-the-art algorithms in terms of NMI and ARI.
Additionally, our approach scales effectively with the size of the network,
making it suitable for large-scale applications. The implications of our
findings extend beyond biological network interpretation and social network
analysis, where knowledge of the fundamental community structure is essential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13534">LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1">Shitao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peitian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xingrun Xing</a></p>
<p>The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose a novel method which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging (namely LM-Cocktail), where the fine-tuned language model
is merged with the pre-trained base model or the peer models from other domains
through weighted average. Despite simplicity, LM-Cocktail is surprisingly
effective: the resulted model is able to achieve a strong empirical performance
in the whole scope of general tasks while preserving a superior capacity in its
targeted domain. We conduct comprehensive experiments with LLama and BGE model
on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14353">Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation. (arXiv:2311.14353v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1">Yasumasa Kano</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1">Katsuhito Sudoh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1">Satoshi Nakamura</a></p>
<p>Simultaneous translation is a task in which the translation begins before the
end of an input speech segment. Its evaluation should be conducted based on
latency in addition to quality, and for users, the smallest possible amount of
latency is preferable. Most existing metrics measure latency based on the start
timings of partial translations and ignore their duration. This means such
metrics do not penalize the latency caused by long translation output, which
delays the comprehension of users and subsequent translations. In this work, we
propose a novel latency evaluation metric for simultaneous translation called
\emph{Average Token Delay} (ATD) that focuses on the duration of partial
translations. We demonstrate its effectiveness through analyses simulating
user-side latency based on Ear-Voice Span (EVS). In our experiment, ATD had the
highest correlation with EVS among baseline latency metrics under most
conditions.
</p>
</p>
</div>

    </div>
    </body>
    