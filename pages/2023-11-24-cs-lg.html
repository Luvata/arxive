<!DOCTYPE html>
<html>
<head>
<title>2023-11-24-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.12804">Towards the generation of synchronized and believable non-verbal facial behaviors of a talking virtual agent. (arXiv:2311.12804v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delbosc_A/0/1/0/all/0/1">Alice Delbosc</a> (TALEP, LIS, AMU), <a href="http://arxiv.org/find/cs/1/au:+Ochs_M/0/1/0/all/0/1">Magalie Ochs</a> (LIS, AMU, TALEP), <a href="http://arxiv.org/find/cs/1/au:+Sabouret_N/0/1/0/all/0/1">Nicolas Sabouret</a> (LISN), <a href="http://arxiv.org/find/cs/1/au:+Ravenet_B/0/1/0/all/0/1">Brian Ravenet</a> (LISN), <a href="http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1">St&#xe9;phane Ayache</a> (AMU, LIS, QARMA)</p>
<p>This paper introduces a new model to generate rhythmically relevant
non-verbal facial behaviors for virtual agents while they speak. The model
demonstrates perceived performance comparable to behaviors directly extracted
from the data and replayed on a virtual agent, in terms of synchronization with
speech and believability. Interestingly, we found that training the model with
two different sets of data, instead of one, did not necessarily improve its
performance. The expressiveness of the people in the dataset and the shooting
conditions are key elements. We also show that employing an adversarial model,
in which fabricated fake examples are introduced during the training phase,
increases the perception of synchronization with speech. A collection of videos
demonstrating the results and code can be accessed at:
https://github.com/aldelb/non_verbal_facial_animation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12806">MatGD: Materials Graph Digitizer. (arXiv:2311.12806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaewoong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonseok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jihan Kim</a></p>
<p>We have developed MatGD (Material Graph Digitizer), which is a tool for
digitizing a data line from scientific graphs. The algorithm behind the tool
consists of four steps: (1) identifying graphs within subfigures, (2)
separating axes and data sections, (3) discerning the data lines by eliminating
irrelevant graph objects and matching with the legend, and (4) data extraction
and saving. From the 62,534 papers in the areas of batteries, catalysis, and
MOFs, 501,045 figures were mined. Remarkably, our tool showcased performance
with over 99% accuracy in legend marker and text detection. Moreover, its
capability for data line separation stood at 66%, which is much higher compared
to other existing figure mining tools. We believe that this tool will be
integral to collecting both past and future data from publications, and these
data can be used to train various machine learning models that can enhance
material predictions and new materials discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12807">Reducing the Environmental Impact of Wireless Communication via Probabilistic Machine Learning. (arXiv:2311.12807v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koblitz_A/0/1/0/all/0/1">A. Ryo Koblitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Maggi_L/0/1/0/all/0/1">Lorenzo Maggi</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1">Matthew Andrews</a></p>
<p>Machine learning methods are increasingly adopted in communications problems,
particularly those arising in next generation wireless settings. Though seen as
a key climate mitigation and societal adaptation enabler, communications
related energy consumption is high and is expected to grow in future networks
in spite of anticipated efficiency gains in 6G due to exponential
communications traffic growth. To make meaningful climate mitigation impact in
the communications sector, a mindset shift away from maximizing throughput at
all cost and towards prioritizing energy efficiency is needed. Moreover, this
must be adopted in both existing (without incurring further embodied carbon
costs through equipment replacement) and future network infrastructure, given
the long development time of mobile generations. To that end, we present
summaries of two such problems, from both current and next generation network
specifications, where probabilistic inference methods were used to great
effect: using Bayesian parameter tuning we are able to safely reduce the energy
consumption of existing hardware on a live communications network by $11\%$
whilst maintaining operator specified performance envelopes; through
spatiotemporal Gaussian process surrogate modeling we reduce the overhead in a
next generation hybrid beamforming system by over $60\%$, greatly improving the
networks' ability to target highly mobile users such as autonomous vehicles.
The Bayesian paradigm is itself helpful in terms of energy usage, since
training a Bayesian optimization model can require much less computation than,
say, training a deep neural network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12810">Combining low-dose CT-based radiomics and metabolomics for early lung cancer screening support. (arXiv:2311.12810v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zyla_J/0/1/0/all/0/1">Joanna Zyla</a>, <a href="http://arxiv.org/find/cs/1/au:+Marczyk_M/0/1/0/all/0/1">Michal Marczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Prazuch_W/0/1/0/all/0/1">Wojciech Prazuch</a>, <a href="http://arxiv.org/find/cs/1/au:+Socha_M/0/1/0/all/0/1">Marek Socha</a>, <a href="http://arxiv.org/find/cs/1/au:+Suwalska_A/0/1/0/all/0/1">Aleksandra Suwalska</a>, <a href="http://arxiv.org/find/cs/1/au:+Durawa_A/0/1/0/all/0/1">Agata Durawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jelitto_Gorska_M/0/1/0/all/0/1">Malgorzata Jelitto-Gorska</a>, <a href="http://arxiv.org/find/cs/1/au:+Dziadziuszko_K/0/1/0/all/0/1">Katarzyna Dziadziuszko</a>, <a href="http://arxiv.org/find/cs/1/au:+Szurowska_E/0/1/0/all/0/1">Edyta Szurowska</a>, <a href="http://arxiv.org/find/cs/1/au:+Rzyman_W/0/1/0/all/0/1">Witold Rzyman</a>, <a href="http://arxiv.org/find/cs/1/au:+Widlak_P/0/1/0/all/0/1">Piotr Widlak</a>, <a href="http://arxiv.org/find/cs/1/au:+Polanska_J/0/1/0/all/0/1">Joanna Polanska</a></p>
<p>Due to its predominantly asymptomatic or mildly symptomatic progression, lung
cancer is often diagnosed in advanced stages, resulting in poorer survival
rates for patients. As with other cancers, early detection significantly
improves the chances of successful treatment. Early diagnosis can be
facilitated through screening programs designed to detect lung tissue tumors
when they are still small, typically around 3mm in size. However, the analysis
of extensive screening program data is hampered by limited access to medical
experts. In this study, we developed a procedure for identifying potential
malignant neoplastic lesions within lung parenchyma. The system leverages
machine learning (ML) techniques applied to two types of measurements: low-dose
Computed Tomography-based radiomics and metabolomics. Using data from two
Polish screening programs, two ML algorithms were tested, along with various
integration methods, to create a final model that combines both modalities to
support lung cancer screening.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12812">Personalization of Affective Models to Enable Neuropsychiatric Digital Precision Health Interventions: A Feasibility Study. (arXiv:2311.12812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kargarandehkordi_A/0/1/0/all/0/1">Ali Kargarandehkordi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaisti_M/0/1/0/all/0/1">Matti Kaisti</a>, <a href="http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1">Peter Washington</a></p>
<p>Mobile digital therapeutics for autism spectrum disorder (ASD) often target
emotion recognition and evocation, which is a challenge for children with ASD.
While such mobile applications often use computer vision machine learning (ML)
models to guide the adaptive nature of the digital intervention, a single model
is usually deployed and applied to all children. Here, we explore the potential
of model personalization, or training a single emotion recognition model per
person, to improve the performance of these underlying emotion recognition
models used to guide digital health therapies for children with ASD. We
conducted experiments on the Emognition dataset, a video dataset of human
subjects evoking a series of emotions. For a subset of 10 individuals in the
dataset with a sufficient representation of at least two ground truth emotion
labels, we trained a personalized version of three classical ML models on a set
of 51 features extracted from each video frame. We measured the importance of
each facial feature for all personalized models and observed differing ranked
lists of top features across subjects, motivating the need for model
personalization. We then compared the personalized models against a generalized
model trained using data from all 10 participants. The mean F1-scores achieved
by the personalized models were 90.48%, 92.66%, and 86.40%, respectively. By
contrast, the mean F1-scores reached by non-personalized models trained on
different human subjects and evaluated using the same test set were 88.55%,
91.78%, and 80.42%, respectively. The personalized models outperformed the
generalized models for 7 out of 10 participants. PCA analyses on the remaining
3 participants revealed relatively facial configuration differences between
emotion labels within each subject, suggesting that personalized ML will fail
when the variation among data points within a subjects data is too low.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12813">Targeted Activation Penalties Help CNNs Ignore Spurious Signals. (arXiv:2311.12813v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dekai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Matthew Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1">Francesca Toni</a></p>
<p>Neural networks (NNs) can learn to rely on spurious signals in the training
data, leading to poor generalisation. Recent methods tackle this problem by
training NNs with additional ground-truth annotations of such signals. These
methods may, however, let spurious signals re-emerge in deep convolutional NNs
(CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the
same problem by penalising activations to control the re-emergence of spurious
signals in deep CNNs, while also lowering training times and memory usage. In
addition, ground-truth annotations can be expensive to obtain. We show that TAP
still works well with annotations generated by pre-trained models as effective
substitutes of ground-truth annotations. We demonstrate the power of TAP
against two state-of-the-art baselines on the MNIST benchmark and on two
clinical image datasets, using four different CNN architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12814">HydraScreen: A Generalizable Structure-Based Deep Learning Approach to Drug Discovery. (arXiv:2311.12814v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Prat_A/0/1/0/all/0/1">Alvaro Prat</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Aty_H/0/1/0/all/0/1">Hisham Abdel Aty</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kamuntavicius_G/0/1/0/all/0/1">Gintautas Kamuntavi&#x10d;ius</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Paquet_T/0/1/0/all/0/1">Tanya Paquet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Norvaisas_P/0/1/0/all/0/1">Povilas Norvai&#x161;as</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gasparotto_P/0/1/0/all/0/1">Piero Gasparotto</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tal_R/0/1/0/all/0/1">Roy Tal</a></p>
<p>We propose HydraScreen, a deep-learning approach that aims to provide a
framework for more robust machine-learning-accelerated drug discovery.
HydraScreen utilizes a state-of-the-art 3D convolutional neural network,
designed for the effective representation of molecular structures and
interactions in protein-ligand binding. We design an end-to-end pipeline for
high-throughput screening and lead optimization, targeting applications in
structure-based drug design. We assess our approach using established public
benchmarks based on the CASF 2016 core set, achieving top-tier results in
affinity and pose prediction (Pearson's r = 0.86, RMSE = 1.15, Top-1 = 0.95).
Furthermore, we utilize a novel interaction profiling approach to identify
potential biases in the model and dataset to boost interpretability and support
the unbiased nature of our method. Finally, we showcase HydraScreen's capacity
to generalize across unseen proteins and ligands, offering directions for
future development of robust machine learning scoring functions. HydraScreen
(accessible at https://hydrascreen.ro5.ai) provides a user-friendly GUI and a
public API, facilitating easy assessment of individual protein-ligand
complexes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12816">Evolution of Convolutional Neural Network (CNN): Compute vs Memory bandwidth for Edge AI. (arXiv:2311.12816v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chenna_D/0/1/0/all/0/1">Dwith Chenna</a></p>
<p>Convolutional Neural Networks (CNNs) have greatly influenced the field of
Embedded Vision and Edge Artificial Intelligence (AI), enabling powerful
machine learning capabilities on resource-constrained devices. This article
explores the relationship between CNN compute requirements and memory bandwidth
in the context of Edge AI. We delve into the historical progression of CNN
architectures, from the early pioneering models to the current state-of-the-art
designs, highlighting the advancements in compute-intensive operations. We
examine the impact of increasing model complexity on both computational
requirements and memory access patterns. The paper presents a comparison
analysis of the evolving trade-off between compute demands and memory bandwidth
requirements in CNNs. This analysis provides insights into designing efficient
architectures and potential hardware accelerators in enhancing CNN performance
on edge devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12819">Fixing the problems of deep neural networks will require better training data and learning algorithms. (arXiv:2311.12819v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1">Drew Linsley</a>, <a href="http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1">Thomas Serre</a></p>
<p>Bowers and colleagues argue that DNNs are poor models of biological vision
because they often learn to rival human accuracy by relying on strategies that
differ markedly from those of humans. We show that this problem is worsening as
DNNs are becoming larger-scale and increasingly more accurate, and prescribe
methods for building DNNs that can reliably model biological vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12821">Advancing The Rate-Distortion-Computation Frontier For Neural Image Compression. (arXiv:2311.12821v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1">David Minnen</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnston_N/0/1/0/all/0/1">Nick Johnston</a></p>
<p>The rate-distortion performance of neural image compression models has
exceeded the state-of-the-art for non-learned codecs, but neural codecs are
still far from widespread deployment and adoption. The largest obstacle is
having efficient models that are feasible on a wide variety of consumer
hardware. Comparative research and evaluation is difficult due to the lack of
standard benchmarking platforms and due to variations in hardware architectures
and test environments. Through our rate-distortion-computation (RDC) study we
demonstrate that neither floating-point operations (FLOPs) nor runtime are
sufficient on their own to accurately rank neural compression methods. We also
explore the RDC frontier, which leads to a family of model architectures with
the best empirical trade-off between computational requirements and RD
performance. Finally, we identify a novel neural compression architecture that
yields state-of-the-art RD performance with rate savings of 23.1% over BPG
(7.0% over VTM and 3.0% over ELIC) without requiring significantly more FLOPs
than other learning-based codecs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12825">A PSO Based Method to Generate Actionable Counterfactuals for High Dimensional Data. (arXiv:2311.12825v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1">Shashank Shekhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Salim_A/0/1/0/all/0/1">Asif Salim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansode_A/0/1/0/all/0/1">Adesh Bansode</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinturkar_V/0/1/0/all/0/1">Vivaswan Jinturkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Anirudha Nayak</a></p>
<p>Counterfactual explanations (CFE) are methods that explain a machine learning
model by giving an alternate class prediction of a data point with some minimal
changes in its features. It helps the users to identify their data attributes
that caused an undesirable prediction like a loan or credit card rejection. We
describe an efficient and an actionable counterfactual (CF) generation method
based on particle swarm optimization (PSO). We propose a simple objective
function for the optimization of the instance-centric CF generation problem.
The PSO brings in a lot of flexibility in terms of carrying out multi-objective
optimization in large dimensions, capability for multiple CF generation, and
setting box constraints or immutability of data attributes. An algorithm is
proposed that incorporates these features and it enables greater control over
the proximity and sparsity properties over the generated CFs. The proposed
algorithm is evaluated with a set of action-ability metrics in real-world
datasets, and the results were superior compared to that of the
state-of-the-arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12831">ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets. (arXiv:2311.12831v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Kaiyuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoli Wang</a></p>
<p>Due to its conceptual simplicity and generality, compressive neural
representation has emerged as a promising alternative to traditional
compression methods for managing massive volumetric datasets. The
state-of-the-art neural compression solution, neurcomp, however, utilizes a
single large multilayer perceptron (MLP) to encode the global volume, incurring
slow training and inference. This paper presents an efficient compressive
neural representation (ECNR) solution that improves upon neurcomp to handle
large-scale time-varying datasets. At the heart of our approach is a multiscale
structure that uses the Laplacian pyramid for adaptive signal fitting via
implicit neural representation. We leverage multiple small MLPs at each scale
for fitting local content or residual blocks. By assigning similar blocks to
the same MLP via size uniformization, we enable balanced parallelization among
MLPs to significantly speed up training and inference. A deep compression
strategy is then employed to compact the resulting model. We demonstrate the
effectiveness of ECNR with multiple datasets and compare it with neurcomp and
two state-of-the-art conventional compression methods (SZ3 and TTHRESH). Our
results position ECNR as a promising alternative to neurcomp for scientific
data compression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12836">AI-based association analysis for medical imaging using latent-space geometric confounder correction. (arXiv:2311.12836v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianjing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vernooij_M/0/1/0/all/0/1">Meike W. Vernooij</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1">Eppo B. Wolvius</a>, <a href="http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1">Gennady V. Roshchupkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1">Esther E. Bron</a></p>
<p>AI has greatly enhanced medical image analysis, yet its use in
epidemiological population imaging studies remains limited due to visualization
challenges in non-linear models and lack of confounder control. Addressing
this, we introduce an AI method emphasizing semantic feature interpretation and
resilience against multiple confounders. Our approach's merits are tested in
three scenarios: extracting confounder-free features from a 2D synthetic
dataset; examining the association between prenatal alcohol exposure and
children's facial shapes using 3D mesh data; exploring the relationship between
global cognition and brain images with a 3D MRI dataset. Results confirm our
method effectively reduces confounder influences, establishing less confounded
associations. Additionally, it provides a unique visual representation,
highlighting specific image alterations due to identified correlations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12839">A Review of Deep Reinforcement Learning in Serverless Computing: Function Scheduling and Resource Auto-Scaling. (arXiv:2311.12839v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majid_A/0/1/0/all/0/1">Amjad Yousef Majid</a>, <a href="http://arxiv.org/find/cs/1/au:+Marin_E/0/1/0/all/0/1">Eduard Marin</a></p>
<p>In the rapidly evolving field of serverless computing, efficient function
scheduling and resource scaling are critical for optimizing performance and
cost. This paper presents a comprehensive review of the application of Deep
Reinforcement Learning (DRL) techniques in these areas. We begin by providing
an overview of serverless computing, highlighting its benefits and challenges,
with a particular focus on function scheduling and resource scaling. We then
delve into the principles of deep reinforcement learning (DRL) and its
potential for addressing these challenges. A systematic review of recent
studies applying DRL to serverless computing is presented, covering various
algorithms, models, and performances. Our analysis reveals that DRL, with its
ability to learn and adapt from an environment, shows promising results in
improving the efficiency of function scheduling and resource scaling in
serverless computing. However, several challenges remain, including the need
for more realistic simulation environments, handling of cold starts, and the
trade-off between learning time and scheduling performance. We conclude by
discussing potential future directions for this research area, emphasizing the
need for more robust DRL models, better benchmarking methods, and the
exploration of multi-agent reinforcement learning for more complex serverless
architectures. This review serves as a valuable resource for researchers and
practitioners aiming to understand and advance the application of DRL in
serverless computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12850">Meticulously Selecting 1% of the Dataset for Pre-training! Generating Differentially Private Images Data with Semantics Query. (arXiv:2311.12850v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kecen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xinwen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a></p>
<p>Differential Privacy (DP) image data synthesis, which leverages the DP
technique to generate synthetic data to replace the sensitive data, allowing
organizations to share and utilize synthetic images without privacy concerns.
Previous methods incorporate the advanced techniques of generative models and
pre-training on a public dataset to produce exceptional DP image data, but
suffer from problems of unstable training and massive computational resource
demands. This paper proposes a novel DP image synthesis method, termed
PRIVIMAGE, which meticulously selects pre-training data, promoting the
efficient creation of DP datasets with high fidelity and utility. PRIVIMAGE
first establishes a semantic query function using a public dataset. Then, this
function assists in querying the semantic distribution of the sensitive
dataset, facilitating the selection of data from the public dataset with
analogous semantics for pre-training. Finally, we pre-train an image generative
model using the selected data and then fine-tune this model on the sensitive
dataset using Differentially Private Stochastic Gradient Descent (DP-SGD).
PRIVIMAGE allows us to train a lightly parameterized generative model, reducing
the noise in the gradient during DP-SGD training and enhancing training
stability. Extensive experiments demonstrate that PRIVIMAGE uses only 1% of the
public dataset for pre-training and 7.6% of the parameters in the generative
model compared to the state-of-the-art method, whereas achieves superior
synthetic performance and conserves more computational resources. On average,
PRIVIMAGE achieves 30.1% lower FID and 12.6% higher Classification Accuracy
than the state-of-the-art method. The replication package and datasets can be
accessed online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12856">Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer. (arXiv:2311.12856v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_N/0/1/0/all/0/1">Namkyeong Lee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Noh_H/0/1/0/all/0/1">Heewoong Noh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kim_S/0/1/0/all/0/1">Sungwon Kim</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hyun_D/0/1/0/all/0/1">Dongmin Hyun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Na_G/0/1/0/all/0/1">Gyoung S. Na</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Park_C/0/1/0/all/0/1">Chanyoung Park</a></p>
<p>The density of states (DOS) is a spectral property of crystalline materials,
which provides fundamental insights into various characteristics of the
materials. While previous works mainly focus on obtaining high-quality
representations of crystalline materials for DOS prediction, we focus on
predicting the DOS from the obtained representations by reflecting the nature
of DOS: DOS determines the general distribution of states as a function of
energy. That is, DOS is not solely determined by the crystalline material but
also by the energy levels, which has been neglected in previous works. In this
paper, we propose to integrate heterogeneous information obtained from the
crystalline materials and the energies via a multi-modal transformer, thereby
modeling the complex relationships between the atoms in the crystalline
materials and various energy levels for DOS prediction. Moreover, we propose to
utilize prompts to guide the model to learn the crystal structural
system-specific interactions between crystalline materials and energies.
Extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS,
with various real-world scenarios demonstrate the superiority of
DOSTransformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12858">RAEDiff: Denoising Diffusion Probabilistic Models Based Reversible Adversarial Examples Self-Generation and Self-Recovery. (arXiv:2311.12858v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1">Fan Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaoyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xuefeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhuo Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yan Zhao</a></p>
<p>Collected and annotated datasets, which are obtained through extensive
efforts, are effective for training Deep Neural Network (DNN) models. However,
these datasets are susceptible to be misused by unauthorized users, resulting
in infringement of Intellectual Property (IP) rights owned by the dataset
creators. Reversible Adversarial Exsamples (RAE) can help to solve the issues
of IP protection for datasets. RAEs are adversarial perturbed images that can
be restored to the original. As a cutting-edge approach, RAE scheme can serve
the purposes of preventing unauthorized users from engaging in malicious model
training, as well as ensuring the legitimate usage of authorized users.
Nevertheless, in the existing work, RAEs still rely on the embedded auxiliary
information for restoration, which may compromise their adversarial abilities.
In this paper, a novel self-generation and self-recovery method, named as
RAEDiff, is introduced for generating RAEs based on a Denoising Diffusion
Probabilistic Models (DDPM). It diffuses datasets into a Biased Gaussian
Distribution (BGD) and utilizes the prior knowledge of the DDPM for generating
and recovering RAEs. The experimental results demonstrate that RAEDiff
effectively self-generates adversarial perturbations for DNN models, including
Artificial Intelligence Generated Content (AIGC) models, while also exhibiting
significant self-recovery capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12859">Joint Multi-View Collaborative Clustering. (arXiv:2311.12859v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalafaoui_Y/0/1/0/all/0/1">Yasser Khalafaoui</a> (Alteca, ETIS), <a href="http://arxiv.org/find/cs/1/au:+Matei_B/0/1/0/all/0/1">Basarab Matei</a> (LIPN), <a href="http://arxiv.org/find/cs/1/au:+Grozavu_N/0/1/0/all/0/1">Nistor Grozavu</a> (ETIS), <a href="http://arxiv.org/find/cs/1/au:+Lovisetto_M/0/1/0/all/0/1">Martino Lovisetto</a> (Alteca)</p>
<p>Data is increasingly being collected from multiple sources and described by
multiple views. These multi-view data provide richer information than
traditional single-view data. Fusing the former for specific tasks is an
essential component of multi-view clustering. Since the goal of multi-view
clustering algorithms is to discover the common latent structure shared by
multiple views, the majority of proposed solutions overlook the advantages of
incorporating knowledge derived from horizontal collaboration between
multi-view data and the final consensus. To fill this gap, we propose the Joint
Multi-View Collaborative Clustering (JMVCC) solution, which involves the
generation of basic partitions using Non-negative Matrix Factorization (NMF)
and the horizontal collaboration principle, followed by the fusion of these
local partitions using ensemble clustering. Furthermore, we propose a weighting
method to reduce the risk of negative collaboration (i.e., views with low
quality) during the generation and fusion of local partitions. The experimental
results, which were obtained using a variety of data sets, demonstrate that
JMVCC outperforms other multi-view clustering algorithms and is robust to noisy
views.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12860">On the stability, correctness and plausibility of visual explanation methods based on feature importance. (arXiv:2311.12860v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Darme_R/0/1/0/all/0/1">Romain Xu-Darme</a> (LSL, LIG), <a href="http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1">Jenny Benois-Pineau</a> (LaBRI), <a href="http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1">Romain Giot</a> (LaBRI), <a href="http://arxiv.org/find/cs/1/au:+Quenot_G/0/1/0/all/0/1">Georges Qu&#xe9;not</a> (LIG), <a href="http://arxiv.org/find/cs/1/au:+Chihani_Z/0/1/0/all/0/1">Zakaria Chihani</a> (LSL), <a href="http://arxiv.org/find/cs/1/au:+Rousset_M/0/1/0/all/0/1">Marie-Christine Rousset</a> (LIG), <a href="http://arxiv.org/find/cs/1/au:+Zhukov_A/0/1/0/all/0/1">Alexey Zhukov</a> (LaBRI)</p>
<p>In the field of Explainable AI, multiples evaluation metrics have been
proposed in order to assess the quality of explanation methods w.r.t. a set of
desired properties. In this work, we study the articulation between the
stability, correctness and plausibility of explanations based on feature
importance for image classifiers. We show that the existing metrics for
evaluating these properties do not always agree, raising the issue of what
constitutes a good evaluation metric for explanations. Finally, in the
particular case of stability and correctness, we show the possible limitations
of some evaluation metrics and propose new ones that take into account the
local behaviour of the model under test.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12862">TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs. (arXiv:2311.12862v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haotian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhijian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1">Ke Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhongming Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiuyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guohao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Song Han</a></p>
<p>Sparse convolution plays a pivotal role in emerging workloads, including
point cloud processing in AR/VR, autonomous driving, and graph understanding in
recommendation systems. Since the computation pattern is sparse and irregular,
specialized high-performance kernels are required. Existing GPU libraries offer
two dataflow types for sparse convolution. The gather-GEMM-scatter dataflow is
easy to implement but not optimal in performance, while the dataflows with
overlapped computation and memory access (e.g.implicit GEMM) are highly
performant but have very high engineering costs. In this paper, we introduce
TorchSparse++, a new GPU library that achieves the best of both worlds. We
create a highly efficient Sparse Kernel Generator that generates performant
sparse convolution kernels at less than one-tenth of the engineering cost of
the current state-of-the-art system. On top of this, we design the Sparse
Autotuner, which extends the design space of existing sparse convolution
libraries and searches for the best dataflow configurations for training and
inference workloads. Consequently, TorchSparse++ achieves 2.9x, 3.3x, 2.2x and
1.7x measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art
MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is
1.2-1.3x faster than SpConv v2 in mixed precision training across seven
representative autonomous driving benchmarks. It also seamlessly supports graph
convolutions, achieving 2.6-7.6x faster inference speed compared with
state-of-the-art graph deep learning libraries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12864">OptScaler: A Hybrid Proactive-Reactive Framework for Robust Autoscaling in the Cloud. (arXiv:2311.12864v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zou_D/0/1/0/all/0/1">Ding Zou</a>, <a href="http://arxiv.org/find/math/1/au:+Lu_W/0/1/0/all/0/1">Wei Lu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_Z/0/1/0/all/0/1">Zhibo Zhu</a>, <a href="http://arxiv.org/find/math/1/au:+Lu_X/0/1/0/all/0/1">Xingyu Lu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_X/0/1/0/all/0/1">Xiaojin Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Liu_K/0/1/0/all/0/1">Kangyu Liu</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1">Haiqing Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_K/0/1/0/all/0/1">Kefan Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Sun_R/0/1/0/all/0/1">Renen Sun</a></p>
<p>Autoscaling is a vital mechanism in cloud computing that supports the
autonomous adjustment of computing resources under dynamic workloads. A primary
goal of autoscaling is to stabilize resource utilization at a desirable level,
thus reconciling the need for resource-saving with the satisfaction of Service
Level Objectives (SLOs). Existing proactive autoscaling methods anticipate the
future workload and scale the resources in advance, whereas the reliability may
suffer from prediction deviations arising from the frequent fluctuations and
noise of cloud workloads; reactive methods rely on real-time system feedback,
while the hysteretic nature of reactive methods could cause violations of the
rigorous SLOs. To this end, this paper presents OptScaler, a hybrid autoscaling
framework that integrates the power of both proactive and reactive methods for
regulating CPU utilization. Specifically, the proactive module of OptScaler
consists of a sophisticated workload prediction model and an optimization
model, where the former provides reliable inputs to the latter for making
optimal scaling decisions. The reactive module provides a self-tuning estimator
of CPU utilization to the optimization model. We embed Model Predictive Control
(MPC) mechanism and robust optimization techniques into the optimization model
to further enhance its reliability. Numerical results have demonstrated the
superiority of both the workload prediction model and the hybrid framework of
OptScaler in the scenario of online services compared to prevalent reactive,
proactive, or hybrid autoscalers. OptScaler has been successfully deployed at
Alipay, supporting the autoscaling of applets in the world-leading payment
platform.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12866">Modular Blended Attention Network for Video Question Answering. (arXiv:2311.12866v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingjie Zhou</a></p>
<p>In multimodal machine learning tasks, it is due to the complexity of the
assignments that the network structure, in most cases, is assembled in a
sophisticated way. The holistic architecture can be separated into several
logical parts according to the respective ends that the modules are devised to
achieve. As the number of modalities of information representation increases,
constructing ad hoc subnetworks for processing the data from divergent
modalities while mediating the fusion of different information types has become
a cumbersome and expensive problem. In this paper, we present an approach to
facilitate the question with a reusable and composable neural unit; by
connecting the units in series or parallel, the arduous network constructing of
multimodal machine learning tasks will be accomplished in a much
straightforward way. Additionally, through parameter sharing (weights
replication) among the units, the space complexity will be significantly
reduced. We have conducted experiments on three commonly used datasets; our
method achieves impressive performance compared to several video QA baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12869">Progression and Challenges of IoT in Healthcare: A Short Review. (arXiv:2311.12869v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">S M Atikur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibtisum_S/0/1/0/all/0/1">Sifat Ibtisum</a>, <a href="http://arxiv.org/find/cs/1/au:+Podder_P/0/1/0/all/0/1">Priya Podder</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1">S. M. Saokat Hossain</a></p>
<p>Smart healthcare, an integral element of connected living, plays a pivotal
role in fulfilling a fundamental human need. The burgeoning field of smart
healthcare is poised to generate substantial revenue in the foreseeable future.
Its multifaceted framework encompasses vital components such as the Internet of
Things (IoT), medical sensors, artificial intelligence (AI), edge and cloud
computing, as well as next-generation wireless communication technologies. Many
research papers discuss smart healthcare and healthcare more broadly. Numerous
nations have strategically deployed the Internet of Medical Things (IoMT)
alongside other measures to combat the propagation of COVID-19. This combined
effort has not only enhanced the safety of frontline healthcare workers but has
also augmented the overall efficacy in managing the pandemic, subsequently
reducing its impact on human lives and mortality rates. Remarkable strides have
been made in both applications and technology within the IoMT domain. However,
it is imperative to acknowledge that this technological advancement has
introduced certain challenges, particularly in the realm of security. The rapid
and extensive adoption of IoMT worldwide has magnified issues related to
security and privacy. These encompass a spectrum of concerns, ranging from
replay attacks, man-in-the-middle attacks, impersonation, privileged insider
threats, remote hijacking, password guessing, and denial of service (DoS)
attacks, to malware incursions. In this comprehensive review, we undertake a
comparative analysis of existing strategies designed for the detection and
prevention of malware in IoT environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12871">An Embodied Generalist Agent in 3D World. (arXiv:2311.12871v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiangyong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1">Silong Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Linghu_X/0/1/0/all/0/1">Xiongkun Linghu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Puhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1">Baoxiong Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siyuan Huang</a></p>
<p>Leveraging massive knowledge and learning schemes from large language models
(LLMs), recent machine learning models show notable successes in building
generalist agents that exhibit the capability of general-purpose task solving
in diverse domains, including natural language processing, computer vision, and
robotics. However, a significant challenge remains as these models exhibit
limited ability in understanding and interacting with the 3D world. We argue
this limitation significantly hinders the current models from performing
real-world tasks and further achieving general intelligence. To this end, we
introduce an embodied multi-modal and multi-task generalist agent that excels
in perceiving, grounding, reasoning, planning, and acting in the 3D world. Our
proposed agent, referred to as LEO, is trained with shared LLM-based model
architectures, objectives, and weights in two stages: (i) 3D vision-language
alignment and (ii) 3D vision-language-action instruction tuning. To facilitate
the training, we meticulously curate and generate an extensive dataset
comprising object-level and scene-level multi-modal tasks with exceeding scale
and complexity, necessitating a deep understanding of and interaction with the
3D world. Through rigorous experiments, we demonstrate LEO's remarkable
proficiency across a wide spectrum of tasks, including 3D captioning, question
answering, embodied reasoning, embodied navigation, and robotic manipulation.
Our ablation results further provide valuable insights for the development of
future embodied generalist agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12872">The Case for Universal Basic Computing Power. (arXiv:2311.12872v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yue Zhu</a></p>
<p>The Universal Basic Computing Power (UBCP) initiative ensures global, free
access to a set amount of computing power specifically for AI research and
development (R&amp;D). This initiative comprises three key elements. First, UBCP
must be cost free, with its usage limited to AI R&amp;D and minimal additional
conditions. Second, UBCP should continually incorporate the state of the art AI
advancements, including efficiently distilled, compressed, and deployed
training data, foundational models, benchmarks, and governance tools. Lastly,
it's essential for UBCP to be universally accessible, ensuring convenience for
all users. We urge major stakeholders in AI development large platforms, open
source contributors, and policymakers to prioritize the UBCP initiative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12874">SpecHD: Hyperdimensional Computing Framework for FPGA-based Mass Spectrometry Clustering. (arXiv:2311.12874v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Pinge_S/0/1/0/all/0/1">Sumukh Pinge</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xu_W/0/1/0/all/0/1">Weihong Xu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kang_J/0/1/0/all/0/1">Jaeyoung Kang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_T/0/1/0/all/0/1">Tianqi Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Moshiri_N/0/1/0/all/0/1">Neima Moshiri</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bittremieux_W/0/1/0/all/0/1">Wout Bittremieux</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Rosing_T/0/1/0/all/0/1">Tajana Rosing</a></p>
<p>Mass spectrometry-based proteomics is a key enabler for personalized
healthcare, providing a deep dive into the complex protein compositions of
biological systems. This technology has vast applications in biotechnology and
biomedicine but faces significant computational bottlenecks. Current
methodologies often require multiple hours or even days to process extensive
datasets, particularly in the domain of spectral clustering. To tackle these
inefficiencies, we introduce SpecHD, a hyperdimensional computing (HDC)
framework supplemented by an FPGA-accelerated architecture with integrated
near-storage preprocessing. Utilizing streamlined binary operations in an HDC
environment, SpecHD capitalizes on the low-latency and parallel capabilities of
FPGAs. This approach markedly improves clustering speed and efficiency, serving
as a catalyst for real-time, high-throughput data analysis in future healthcare
applications. Our evaluations demonstrate that SpecHD not only maintains but
often surpasses existing clustering quality metrics while drastically cutting
computational time. Specifically, it can cluster a large-scale human proteome
dataset-comprising 25 million MS/MS spectra and 131 GB of MS data-in just 5
minutes. With energy efficiency exceeding 31x and a speedup factor that spans a
range of 6x to 54x over existing state of-the-art solutions, SpecHD emerges as
a promising solution for the rapid analysis of mass spectrometry data with
great implications for personalized healthcare.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12875">Nav-Q: Quantum Deep Reinforcement Learning for Collision-Free Navigation of Self-Driving Cars. (arXiv:2311.12875v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Sinha_A/0/1/0/all/0/1">Akash Sinha</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Macaluso_A/0/1/0/all/0/1">Antonio Macaluso</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Klusch_M/0/1/0/all/0/1">Matthias Klusch</a></p>
<p>The challenge of collision-free navigation (CFN) for self-driving cars is an
NP-hard problem addressed through Deep Reinforcement Learning (DRL). Despite
the effectiveness of DRL methods, their application demands significant
computing resources and prolonged training periods to establish a resilient
agent. On the other hand, quantum reinforcement learning algorithms have
recently demonstrated faster convergence and improved stability in simple,
non-real-world environments. However, their application in the real-world CFN
domain has not been explored, and their direct adaptation would require a
quantum computing device onboard the vehicle for testing.
</p>
<p>In this work, we propose Nav-Q, the first quantum-supported DRL algorithm for
CFN of self-driving cars, that leverages quantum computation for improving the
training performance without the requirement for onboard quantum hardware.
Nav-Q is based on the actor-critic approach, where the critic is implemented
using a hybrid quantum-classical algorithm suitable for near-term quantum
devices. We assess the performance of Nav-Q using the CARLA driving simulator,
a de facto standard benchmark for evaluating state-of-the-art DRL methods. Our
empirical evaluations showcase that Nav-Q surpasses its classical counterpart
not only in terms of training stability but also, in certain instances, with
respect to the convergence rate when analyzing the Reward vs. Episode curve.
This enhancement is accomplished without negatively impacting the learned
policy by the agent. Furthermore, we assess Nav-Q in relation to effective
dimension, unveiling that the incorporation of a quantum component results in a
model possessing greater descriptive power compared to classical baselines.
Finally, we evaluate the performance of Nav-Q using noisy quantum simulation,
observing that the quantum noise enhances the exploratory tendencies of the
agent during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12876">Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification. (arXiv:2311.12876v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Corral_J/0/1/0/all/0/1">Jos&#xe9; Mar&#xed;a Rodr&#xed;guez Corral</a>, <a href="http://arxiv.org/find/eess/1/au:+Civit_Masot_J/0/1/0/all/0/1">Javier Civit-Masot</a>, <a href="http://arxiv.org/find/eess/1/au:+Luna_Perejon_F/0/1/0/all/0/1">Francisco Luna-Perej&#xf3;n</a>, <a href="http://arxiv.org/find/eess/1/au:+Diaz_Cano_I/0/1/0/all/0/1">Ignacio D&#xed;az-Cano</a>, <a href="http://arxiv.org/find/eess/1/au:+Morgado_Estevez_A/0/1/0/all/0/1">Arturo Morgado-Est&#xe9;vez</a>, <a href="http://arxiv.org/find/eess/1/au:+Dominguez_Morales_M/0/1/0/all/0/1">Manuel Dom&#xed;nguez-Morales</a></p>
<p>In this work, we evaluate the energy usage of fully embedded medical
diagnosis aids based on both segmentation and classification of medical images
implemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis
based on color fundus images as an example to show the possibility of
performing segmentation and classification in real time on embedded boards and
to highlight the different energy requirements of the studied implementations.
</p>
<p>Several other works develop the use of segmentation and feature extraction
techniques to detect glaucoma, among many other pathologies, with deep neural
networks. Memory limitations and low processing capabilities of embedded
accelerated systems (EAS) limit their use for deep network-based system
training. However, including specific acceleration hardware, such as NVIDIA's
Maxwell GPU or Google's Edge TPU, enables them to perform inferences using
complex pre-trained networks in very reasonable times.
</p>
<p>In this study, we evaluate the timing and energy performance of two EAS
equipped with Machine Learning (ML) accelerators executing an example
diagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)
segmentation, the obtained prediction times per image are under 29 and 43 ms
using Edge TPUs and Maxwell GPUs, respectively. Prediction times for the
classification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell
GPUs, respectively. Regarding energy usage, in approximate terms, for OD
segmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,
respectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and
70 mJ, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12878">Adaptive Bayesian Learning with Action and State-Dependent Signal Variance. (arXiv:2311.12878v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hou_K/0/1/0/all/0/1">Kaiwen Hou</a></p>
<p>This manuscript presents an advanced framework for Bayesian learning by
incorporating action and state-dependent signal variances into decision-making
models. This framework is pivotal in understanding complex data-feedback loops
and decision-making processes in various economic systems. Through a series of
examples, we demonstrate the versatility of this approach in different
contexts, ranging from simple Bayesian updating in stable environments to
complex models involving social learning and state-dependent uncertainties. The
paper uniquely contributes to the understanding of the nuanced interplay
between data, actions, outcomes, and the inherent uncertainty in economic
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12879">MiniAnDE: a reduced AnDE ensemble to deal with microarray data. (arXiv:2311.12879v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Torrijos_P/0/1/0/all/0/1">Pablo Torrijos</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gamez_J/0/1/0/all/0/1">Jos&#xe9; A. G&#xe1;mez</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Puerta_J/0/1/0/all/0/1">Jos&#xe9; M. Puerta</a></p>
<p>This article focuses on the supervised classification of datasets with a
large number of variables and a small number of instances. This is the case,
for example, for microarray data sets commonly used in bioinformatics. Complex
classifiers that require estimating statistics over many variables are not
suitable for this type of data. Probabilistic classifiers with low-order
probability tables, e.g. NB and AODE, are good alternatives for dealing with
this type of data. AODE usually improves NB in accuracy, but suffers from high
spatial complexity since $k$ models, each with $n+1$ variables, are included in
the AODE ensemble. In this paper, we propose MiniAnDE, an algorithm that
includes only a small number of heterogeneous base classifiers in the ensemble,
i.e., each model only includes a different subset of the $k$ predictive
variables. Experimental evaluation shows that using MiniAnDE classifiers on
microarray data is feasible and outperforms NB and other ensembles such as
bagging and random forest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12880">Weak-Form Latent Space Dynamics Identification. (arXiv:2311.12880v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tran_A/0/1/0/all/0/1">April Tran</a>, <a href="http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1">Xiaolong He</a>, <a href="http://arxiv.org/find/eess/1/au:+Messenger_D/0/1/0/all/0/1">Daniel A. Messenger</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1">Youngsoo Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Bortz_D/0/1/0/all/0/1">David M. Bortz</a></p>
<p>Recent work in data-driven modeling has demonstrated that a weak formulation
of model equations enhances the noise robustness of a wide range of
computational methods. In this paper, we demonstrate the power of the weak form
to enhance the LaSDI (Latent Space Dynamics Identification) algorithm, a
recently developed data-driven reduced order modeling technique.
</p>
<p>We introduce a weak form-based version WLaSDI (Weak-form Latent Space
Dynamics Identification). WLaSDI first compresses data, then projects onto the
test functions and learns the local latent space models. Notably, WLaSDI
demonstrates significantly enhanced robustness to noise. With WLaSDI, the local
latent space is obtained using weak-form equation learning techniques. Compared
to the standard sparse identification of nonlinear dynamics (SINDy) used in
LaSDI, the variance reduction of the weak form guarantees a robust and precise
latent space recovery, hence allowing for a fast, robust, and accurate
simulation. We demonstrate the efficacy of WLaSDI vs. LaSDI on several common
benchmark examples including viscid and inviscid Burgers', radial advection,
and heat conduction. For instance, in the case of 1D inviscid Burgers'
simulations with the addition of up to 100% Gaussian white noise, the relative
error remains consistently below 6% for WLaSDI, while it can exceed 10,000% for
LaSDI. Similarly, for radial advection simulations, the relative errors stay
below 15% for WLaSDI, in stark contrast to the potential errors of up to
10,000% with LaSDI. Moreover, speedups of several orders of magnitude can be
obtained with WLaSDI. For example applying WLaSDI to 1D Burgers' yields a 140X
speedup compared to the corresponding full order model.
</p>
<p>Python code to reproduce the results in this work is available at
(https://github.com/MathBioCU/PyWSINDy_ODE) and
(https://github.com/MathBioCU/PyWLaSDI).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12884">Identifying DNA Sequence Motifs Using Deep Learning. (arXiv:2311.12884v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Poddar_A/0/1/0/all/0/1">Asmita Poddar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Uzun_V/0/1/0/all/0/1">Vladimir Uzun</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tunbridge_E/0/1/0/all/0/1">Elizabeth Tunbridge</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Haerty_W/0/1/0/all/0/1">Wilfried Haerty</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nevado_Holgado_A/0/1/0/all/0/1">Alejo Nevado-Holgado</a></p>
<p>Splice sites play a crucial role in gene expression, and accurate prediction
of these sites in DNA sequences is essential for diagnosing and treating
genetic disorders. We address the challenge of splice site prediction by
introducing DeepDeCode, an attention-based deep learning sequence model to
capture the long-term dependencies in the nucleotides in DNA sequences. We
further propose using visualization techniques for accurate identification of
sequence motifs, which enhance the interpretability and trustworthiness of
DeepDeCode. We compare DeepDeCode to other state-of-the-art methods for splice
site prediction and demonstrate its accuracy, explainability and efficiency.
Given the results of our methodology, we expect that it can used for healthcare
applications to reason about genomic processes and be extended to discover new
splice sites and genomic regulatory elements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12888">Acceleration and Implicit Regularization in Gaussian Phase Retrieval. (arXiv:2311.12888v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Maunu_T/0/1/0/all/0/1">Tyler Maunu</a>, <a href="http://arxiv.org/find/math/1/au:+Molina_Fructuoso_M/0/1/0/all/0/1">Martin Molina-Fructuoso</a></p>
<p>We study accelerated optimization methods in the Gaussian phase retrieval
problem. In this setting, we prove that gradient methods with Polyak or
Nesterov momentum have similar implicit regularization to gradient descent.
This implicit regularization ensures that the algorithms remain in a nice
region, where the cost function is strongly convex and smooth despite being
nonconvex in general. This ensures that these accelerated methods achieve
faster rates of convergence than gradient descent. Experimental evidence
demonstrates that the accelerated methods converge faster than gradient descent
in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12889">Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge. (arXiv:2311.12889v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bowen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zhijun Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1">Camillo Jose Taylor</a></p>
<p>This work presents an enhanced approach to generating scene graphs by
incorporating a relationship hierarchy and commonsense knowledge. Specifically,
we propose a Bayesian classification head that exploits an informative
hierarchical structure. It jointly predicts the super-category or type of
relationship between the two objects, along with the detailed relationship
under each super-category. We design a commonsense validation pipeline that
uses a large language model to critique the results from the scene graph
prediction system and then use that feedback to enhance the model performance.
The system requires no external large language model assistance at test time,
making it more convenient for practical applications. Experiments on the Visual
Genome and the OpenImage V6 datasets demonstrate that harnessing hierarchical
relationships enhances the model performance by a large margin. The proposed
Bayesian head can also be incorporated as a portable module in existing scene
graph generation algorithms to improve their results. In addition, the
commonsense validation enables the model to generate an extensive set of
reasonable predictions beyond dataset annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12892">IMJENSE: Scan-specific Implicit Representation for Joint Coil Sensitivity and Image Estimation in Parallel MRI. (arXiv:2311.12892v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1">Ruimin Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1">Qing Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1">Jie Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+She_H/0/1/0/all/0/1">Huajun She</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chunlei Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yuyao Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1">Hongjiang Wei</a></p>
<p>Parallel imaging is a commonly used technique to accelerate magnetic
resonance imaging (MRI) data acquisition. Mathematically, parallel MRI
reconstruction can be formulated as an inverse problem relating the sparsely
sampled k-space measurements to the desired MRI image. Despite the success of
many existing reconstruction algorithms, it remains a challenge to reliably
reconstruct a high-quality image from highly reduced k-space measurements.
Recently, implicit neural representation has emerged as a powerful paradigm to
exploit the internal information and the physics of partially acquired data to
generate the desired object. In this study, we introduced IMJENSE, a
scan-specific implicit neural representation-based method for improving
parallel MRI reconstruction. Specifically, the underlying MRI image and coil
sensitivities were modeled as continuous functions of spatial coordinates,
parameterized by neural networks and polynomials, respectively. The weights in
the networks and coefficients in the polynomials were simultaneously learned
directly from sparsely acquired k-space measurements, without fully sampled
ground truth data for training. Benefiting from the powerful continuous
representation and joint estimation of the MRI image and coil sensitivities,
IMJENSE outperforms conventional image or k-space domain reconstruction
algorithms. With extremely limited calibration data, IMJENSE is more stable
than supervised calibrationless and calibration-based deep-learning methods.
Results show that IMJENSE robustly reconstructs the images acquired at
5$\mathbf{\times}$ and 6$\mathbf{\times}$ accelerations with only 4 or 8
calibration lines in 2D Cartesian acquisitions, corresponding to 22.0% and
19.5% undersampling rates. The high-quality results and scanning specificity
make the proposed method hold the potential for further accelerating the data
acquisition of parallel MRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12894">Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval. (arXiv:2311.12894v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiu-Shen Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xuhao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yuxin Peng</a></p>
<p>Our work focuses on tackling large-scale fine-grained image retrieval as
ranking the images depicting the concept of interests (i.e., the same
sub-category labels) highest based on the fine-grained details in the query. It
is desirable to alleviate the challenges of both fine-grained nature of small
inter-class variations with large intra-class variations and explosive growth
of fine-grained data for such a practical task. In this paper, we propose
attribute-aware hashing networks with self-consistency for generating
attribute-aware hash codes to not only make the retrieval process efficient,
but also establish explicit correspondences between hash codes and visual
attributes. Specifically, based on the captured visual representations by
attention, we develop an encoder-decoder structure network of a reconstruction
task to unsupervisedly distill high-level attribute-specific vectors from the
appearance-specific visual representations without attribute annotations. Our
models are also equipped with a feature decorrelation constraint upon these
attribute vectors to strengthen their representative abilities. Then, driven by
preserving original entities' similarity, the required hash codes can be
generated from these attribute-specific vectors and thus become
attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we
consider the model design from the perspective of the self-consistency
principle and propose to further enhance models' self-consistency by equipping
an additional image reconstruction path. Comprehensive quantitative experiments
under diverse empirical settings on six fine-grained retrieval datasets and two
generic retrieval datasets show the superiority of our models over competing
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12901">From Microbes to Methane: AI-Based Predictive Modeling of Feed Additive Efficacy in Dairy Cows. (arXiv:2311.12901v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Altshuler_Y/0/1/0/all/0/1">Yaniv Altshuler</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chebach_T/0/1/0/all/0/1">Tzruya Calvao Chebach</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cohen_S/0/1/0/all/0/1">Shalom Cohen</a></p>
<p>In an era of increasing pressure to achieve sustainable agriculture, the
optimization of livestock feed for enhancing yield and minimizing environmental
impact is a paramount objective. This study presents a pioneering approach
towards this goal, using rumen microbiome data to predict the efficacy of feed
additives in dairy cattle.
</p>
<p>We collected an extensive dataset that includes methane emissions from 2,190
Holstein cows distributed across 34 distinct sites. The cows were divided into
control and experimental groups in a double-blind, unbiased manner, accounting
for variables such as age, days in lactation, and average milk yield. The
experimental groups were administered one of four leading commercial feed
additives: Agolin, Kexxtone, Allimax, and Relyon. Methane emissions were
measured individually both before the administration of additives and over a
subsequent 12-week period. To develop our predictive model for additive
efficacy, rumen microbiome samples were collected from 510 cows from the same
herds prior to the study's onset. These samples underwent deep metagenomic
shotgun sequencing, yielding an average of 15.7 million reads per sample.
Utilizing innovative artificial intelligence techniques we successfully
estimated the efficacy of these feed additives across different farms. The
model's robustness was further confirmed through validation with independent
cohorts, affirming its generalizability and reliability.
</p>
<p>Our results underscore the transformative capability of using targeted feed
additive strategies to both optimize dairy yield and milk composition, and to
significantly reduce methane emissions. Specifically, our predictive model
demonstrates a scenario where its application could guide the assignment of
additives to farms where they are most effective. In doing so, we could achieve
an average potential reduction of over 27\% in overall emissions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12902">Local Convolution Enhanced Global Fourier Neural Operator For Multiscale Dynamic Spaces Prediction. (arXiv:2311.12902v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuanle Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yue Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tielin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bo Xu</a></p>
<p>Neural operators extend the capabilities of traditional neural networks by
allowing them to handle mappings between function spaces for the purpose of
solving partial differential equations (PDEs). One of the most notable methods
is the Fourier Neural Operator (FNO), which is inspired by Green's function
method and approximate operator kernel directly in the frequency domain. In
this work, we focus on predicting multiscale dynamic spaces, which is
equivalent to solving multiscale PDEs. Multiscale PDEs are characterized by
rapid coefficient changes and solution space oscillations, which are crucial
for modeling atmospheric convection and ocean circulation. To solve this
problem, models should have the ability to capture rapid changes and process
them at various scales. However, the FNO only approximates kernels in the
low-frequency domain, which is insufficient when solving multiscale PDEs. To
address this challenge, we propose a novel hierarchical neural operator that
integrates improved Fourier layers with attention mechanisms, aiming to capture
all details and handle them at various scales. These mechanisms complement each
other in the frequency domain and encourage the model to solve multiscale
problems. We perform experiments on dynamic spaces governed by forward and
reverse problems of multiscale elliptic equations, Navier-Stokes equations and
some other physical scenarios, and reach superior performance in existing PDE
benchmarks, especially equations characterized by rapid coefficient variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12904">Learning to Compute Gr\&quot;obner Bases. (arXiv:2311.12904v1 [math.AC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kera_H/0/1/0/all/0/1">Hiroshi Kera</a>, <a href="http://arxiv.org/find/math/1/au:+Ishihara_Y/0/1/0/all/0/1">Yuki Ishihara</a>, <a href="http://arxiv.org/find/math/1/au:+Kambe_Y/0/1/0/all/0/1">Yuta Kambe</a>, <a href="http://arxiv.org/find/math/1/au:+Vaccon_T/0/1/0/all/0/1">Tristan Vaccon</a>, <a href="http://arxiv.org/find/math/1/au:+Yokoyama_K/0/1/0/all/0/1">Kazuhiro Yokoyama</a></p>
<p>Solving a polynomial system, or computing an associated Gr\"obner basis, has
been a fundamental task in computational algebra. However, it is also known for
its notoriously expensive computational cost -- doubly exponential time
complexity in the number of variables in the worst case. In this paper, we
achieve for the first time Gr\"obner basis computation through the training of
a transformer. The training requires many pairs of a polynomial system and the
associated Gr\"obner basis, thus motivating us to address two novel algebraic
problems: random generation of Gr\"obner bases and the transformation of them
into non-Gr\"obner polynomial systems, termed as \textit{backward Gr\"obner
problem}. We resolve these problems with zero-dimensional radical ideals, the
ideals appearing in various applications. The experiments show that in the
five-variate case, the proposed dataset generation method is five orders of
magnitude faster than a naive approach, overcoming a crucial challenge in
learning to compute Gr\"obner bases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12905">Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer. (arXiv:2311.12905v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1">Zheqi Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a
new target domain by actively selecting a limited number of target data to
annotate.This setting neglects the more practical scenario where training data
are collected from multiple sources. This motivates us to target a new and
challenging setting of knowledge transfer that extends ADA from a single source
domain to multiple source domains, termed Multi-source Active Domain Adaptation
(MADA). Not surprisingly, we find that most traditional ADA methods cannot work
directly in such a setting, mainly due to the excessive domain gap introduced
by all the source domains and thus their uncertainty-aware sample selection can
easily become miscalibrated under the multi-domain shifts. Considering this, we
propose a Dynamic integrated uncertainty valuation framework(Detective) that
comprehensively consider the domain shift between multi-source domains and
target domain to detect the informative target samples. Specifically, the
leverages a dynamic Domain Adaptation(DA) model that learns how to adapt the
model's parameters to fit the union of multi-source domains. This enables an
approximate single-source domain modeling by the dynamic model. We then
comprehensively measure both domain uncertainty and predictive uncertainty in
the target domain to detect informative target samples using evidential deep
learning, thereby mitigating uncertainty miscalibration. Furthermore, we
introduce a contextual diversity-aware calculator to enhance the diversity of
the selected samples. Experiments demonstrate that our solution outperforms
existing methods by a considerable margin on three domain adaptation
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12906">Nonlinear System Identification of Swarm of UAVs Using Deep Learning Methods. (arXiv:2311.12906v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yazdannik_S/0/1/0/all/0/1">Saman Yazdannik</a>, <a href="http://arxiv.org/find/cs/1/au:+Tayefi_M/0/1/0/all/0/1">Morteza Tayefi</a>, <a href="http://arxiv.org/find/cs/1/au:+Farrokh_M/0/1/0/all/0/1">Mojtaba Farrokh</a></p>
<p>This study designs and evaluates multiple nonlinear system identification
techniques for modeling the UAV swarm system in planar space. learning methods
such as RNNs, CNNs, and Neural ODE are explored and compared. The objective is
to forecast future swarm trajectories by accurately approximating the nonlinear
dynamics of the swarm model. The modeling process is performed using both
transient and steady-state data from swarm simulations. Results show that the
combination of Neural ODE with a well-trained model using transient data is
robust for varying initial conditions and outperforms other learning methods in
accurately predicting swarm stability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12908">Diffusion Model Alignment Using Direct Preference Optimization. (arXiv:2311.12908v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1">Bram Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1">Meihua Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1">Rafael Rafailov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Linqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1">Aaron Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1">Senthil Purushwalkam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1">Nikhil Naik</a></p>
<p>Large language models (LLMs) are fine-tuned using human comparison data with
Reinforcement Learning from Human Feedback (RLHF) methods to make them better
aligned with users' preferences. In contrast to LLMs, human preference learning
has not been widely explored in text-to-image diffusion models; the best
existing approach is to fine-tune a pretrained model using carefully curated
high quality images and captions to improve visual appeal and text alignment.
We propose Diffusion-DPO, a method to align diffusion models to human
preferences by directly optimizing on human comparison data. Diffusion-DPO is
adapted from the recently developed Direct Preference Optimization (DPO), a
simpler alternative to RLHF which directly optimizes a policy that best
satisfies human preferences under a classification objective. We re-formulate
DPO to account for a diffusion model notion of likelihood, utilizing the
evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic
dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model
of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with
Diffusion-DPO. Our fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement
model in human evaluation, improving visual appeal and prompt alignment. We
also develop a variant that uses AI feedback and has comparable performance to
training on human preferences, opening the door for scaling of diffusion model
alignment methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12909">Non-Sequential Ensemble Kalman Filtering using Distributed Arrays. (arXiv:2311.12909v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Travelletti_C/0/1/0/all/0/1">C&#xe9;dric Travelletti</a>, <a href="http://arxiv.org/find/stat/1/au:+Franke_J/0/1/0/all/0/1">J&#xf6;rg Franke</a>, <a href="http://arxiv.org/find/stat/1/au:+Ginsbourger_D/0/1/0/all/0/1">David Ginsbourger</a>, <a href="http://arxiv.org/find/stat/1/au:+Bronnimann_S/0/1/0/all/0/1">Stefan Br&#xf6;nnimann</a></p>
<p>This work introduces a new, distributed implementation of the Ensemble Kalman
Filter (EnKF) that allows for non-sequential assimilation of large datasets in
high-dimensional problems. The traditional EnKF algorithm is computationally
intensive and exhibits difficulties in applications requiring interaction with
the background covariance matrix, prompting the use of methods like sequential
assimilation which can introduce unwanted consequences, such as dependency on
observation ordering. Our implementation leverages recent advancements in
distributed computing to enable the construction and use of the full model
error covariance matrix in distributed memory, allowing for single-batch
assimilation of all observations and eliminating order dependencies.
Comparative performance assessments, involving both synthetic and real-world
paleoclimatic reconstruction applications, indicate that the new,
non-sequential implementation outperforms the traditional, sequential one.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12915">Neural-Integrated Meshfree (NIM) Method: A differentiable programming-based hybrid solver for computational mechanics. (arXiv:2311.12915v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1">Honghui Du</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">QiZhi He</a></p>
<p>We present the neural-integrated meshfree (NIM) method, a differentiable
programming-based hybrid meshfree approach within the field of computational
mechanics. NIM seamlessly integrates traditional physics-based meshfree
discretization techniques with deep learning architectures. It employs a hybrid
approximation scheme, NeuroPU, to effectively represent the solution by
combining continuous DNN representations with partition of unity (PU) basis
functions associated with the underlying spatial discretization. This
neural-numerical hybridization not only enhances the solution representation
through functional space decomposition but also reduces both the size of DNN
model and the need for spatial gradient computations based on automatic
differentiation, leading to a significant improvement in training efficiency.
Under the NIM framework, we propose two truly meshfree solvers: the strong
form-based NIM (S-NIM) and the local variational form-based NIM (V-NIM). In the
S-NIM solver, the strong-form governing equation is directly considered in the
loss function, while the V-NIM solver employs a local Petrov-Galerkin approach
that allows the construction of variational residuals based on arbitrary
overlapping subdomains. This ensures both the satisfaction of underlying
physics and the preservation of meshfree property. We perform extensive
numerical experiments on both stationary and transient benchmark problems to
assess the effectiveness of the proposed NIM methods in terms of accuracy,
scalability, generalizability, and convergence properties. Moreover,
comparative analysis with other physics-informed machine learning methods
demonstrates that NIM, especially V-NIM, significantly enhances both accuracy
and efficiency in end-to-end predictive capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12917">Orchard: building large cancer phylogenies using stochastic combinatorial search. (arXiv:2311.12917v1 [q-bio.PE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kulman_E/0/1/0/all/0/1">E. Kulman</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kuang_R/0/1/0/all/0/1">R. Kuang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Morris_Q/0/1/0/all/0/1">Q. Morris</a></p>
<p>Phylogenies depicting the evolutionary history of genetically heterogeneous
subpopulations of cells from the same cancer i.e., cancer phylogenies, provide
useful insights about cancer development and inform treatment. Cancer
phylogenies can be reconstructed using data obtained from bulk DNA sequencing
of multiple tissue samples from the same cancer. We introduce Orchard, a fast
algorithm that reconstructs cancer phylogenies using point mutations detected
in bulk DNA sequencing data. Orchard constructs cancer phylogenies
progressively, one point mutation at a time, ultimately sampling complete
phylogenies from a posterior distribution implied by the bulk DNA data. Orchard
reconstructs more plausible phylogenies than state-of-the-art cancer phylogeny
reconstruction methods on 90 simulated cancers and 14 B-progenitor acute
lymphoblastic leukemias (B-ALLs). These results demonstrate that Orchard
accurately reconstructs cancer phylogenies with up to 300 mutations. We then
introduce a simple graph based clustering algorithm that uses a reconstructed
phylogeny to infer unique groups of mutations i.e., mutation clusters, that
characterize the genetic differences between cancer cell populations, and show
that this approach is competitive with state-of-the-art mutation clustering
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12918">Deep Learning-Based Real-Time Quality Control of Standard Video Compression for Live Streaming. (arXiv:2311.12918v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mortaheb_M/0/1/0/all/0/1">Matin Mortaheb</a>, <a href="http://arxiv.org/find/eess/1/au:+Khojastepour_M/0/1/0/all/0/1">Mohammad A. Amir Khojastepour</a>, <a href="http://arxiv.org/find/eess/1/au:+Chakradhar_S/0/1/0/all/0/1">Srimat T. Chakradhar</a>, <a href="http://arxiv.org/find/eess/1/au:+Ulukus_S/0/1/0/all/0/1">Sennur Ulukus</a></p>
<p>Ensuring high-quality video content for wireless users has become
increasingly vital. Nevertheless, maintaining a consistent level of video
quality faces challenges due to the fluctuating encoded bitrate, primarily
caused by dynamic video content, especially in live streaming scenarios. Video
compression is typically employed to eliminate unnecessary redundancies within
and between video frames, thereby reducing the required bandwidth for video
transmission. The encoded bitrate and the quality of the compressed video
depend on encoder parameters, specifically, the quantization parameter (QP).
Poor choices of encoder parameters can result in reduced bandwidth efficiency
and high likelihood of non-conformance. Non-conformance refers to the violation
of the peak signal-to-noise ratio (PSNR) constraint for an encoded video
segment. To address these issues, a real-time deep learning-based H.264
controller is proposed. This controller dynamically estimates the optimal
encoder parameters based on the content of a video chunk with minimal delay.
The objective is to maintain video quality in terms of PSNR above a specified
threshold while minimizing the average bitrate of the compressed video.
Experimental results, conducted on both QCIF dataset and a diverse range of
random videos from public datasets, validate the effectiveness of this
approach. Notably, it achieves improvements of up to 2.5 times in average
bandwidth usage compared to the state-of-the-art adaptive bitrate video
streaming, with a negligible non-conformance probability below $10^{-2}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12929">Hierarchical Learning for Quantum ML: Novel Training Technique for Large-Scale Variational Quantum Circuits. (arXiv:2311.12929v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Gharibyan_H/0/1/0/all/0/1">Hrant Gharibyan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Su_V/0/1/0/all/0/1">Vincent Su</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tepanyan_H/0/1/0/all/0/1">Hayk Tepanyan</a></p>
<p>We present hierarchical learning, a novel variational architecture for
efficient training of large-scale variational quantum circuits. We test and
benchmark our technique for distribution loading with quantum circuit born
machines (QCBMs). With QCBMs, probability distributions are loaded into the
squared amplitudes of computational basis vectors represented by bitstrings.
Our key insight is to take advantage of the fact that the most significant
(qu)bits have a greater effect on the final distribution and can be learned
first. One can think of it as a generalization of layerwise learning, where
some parameters of the variational circuit are learned first to prevent the
phenomena of barren plateaus. We briefly review adjoint methods for computing
the gradient, in particular for loss functions that are not expectation values
of observables. We first compare the role of connectivity in the variational
ansatz for the task of loading a Gaussian distribution on nine qubits, finding
that 2D connectivity greatly outperforms qubits arranged on a line. Based on
our observations, we then implement this strategy on large-scale numerical
experiments with GPUs, training a QCBM to reproduce a 3-dimensional
multivariate Gaussian distribution on 27 qubits up to $\sim4\%$ total variation
distance. Though barren plateau arguments do not strictly apply here due to the
objective function not being tied to an observable, this is to our knowledge
the first practical demonstration of variational learning on large numbers of
qubits. We also demonstrate hierarchical learning as a resource-efficient way
to load distributions for existing quantum hardware (IBM's 7 and 27 qubit
devices) in tandem with Fire Opal optimizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12943">InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions. (arXiv:2311.12943v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kedia_K/0/1/0/all/0/1">Kushal Kedia</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1">Atiksh Bhardwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_P/0/1/0/all/0/1">Prithwish Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1">Sanjiban Choudhury</a></p>
<p>In collaborative human-robot manipulation, a robot must predict human intents
and adapt its actions accordingly to smoothly execute tasks. However, the
human's intent in turn depends on actions the robot takes, creating a
chicken-or-egg problem. Prior methods ignore such inter-dependency and instead
train marginal intent prediction models independent of robot actions. This is
because training conditional models is hard given a lack of paired human-robot
interaction datasets.
</p>
<p>Can we instead leverage large-scale human-human interaction data that is more
easily accessible? Our key insight is to exploit a correspondence between human
and robot actions that enables transfer learning from human-human to
human-robot data. We propose a novel architecture, InteRACT, that pre-trains a
conditional intent prediction model on large human-human datasets and
fine-tunes on a small human-robot dataset. We evaluate on a set of real-world
collaborative human-robot manipulation tasks and show that our conditional
model improves over various marginal baselines. We also introduce new
techniques to tele-operate a 7-DoF robot arm and collect a diverse range of
human-robot collaborative manipulation data, which we open-source.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12944">DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks. (arXiv:2311.12944v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1">Daksh Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Chamola_V/0/1/0/all/0/1">Vinay Chamola</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Sandeep Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeadally_S/0/1/0/all/0/1">Sherali Zeadally</a></p>
<p>The power requirements posed by the fifth-generation and beyond cellular
networks are an important constraint in network deployment and require
energy-efficient solutions. In this work, we propose a novel user load transfer
approach using airborne base stations (BS), mounted on drones, for reliable and
secure power redistribution across the micro-grid network comprising green
small cell BSs. Depending on the user density and the availability of an aerial
BS, the energy requirement of a cell with an energy deficit is accommodated by
migrating the aerial BS from a high-energy to a low-energy cell. The proposed
hybrid drone-based framework integrates long short-term memory with unique cost
functions using an evolutionary neural network for drones and BSs, and
efficiently manages energy and load redistribution. The proposed algorithm
reduces power outages at BSs and maintains consistent throughput stability,
thereby demonstrating its capability to boost the reliability and robustness of
wireless communication systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12956">Innovative Horizons in Aerial Imagery: LSKNet Meets DiffusionDet for Advanced Object Detection. (arXiv:2311.12956v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharshar_A/0/1/0/all/0/1">Ahmed Sharshar</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsun_A/0/1/0/all/0/1">Aleksandr Matsun</a></p>
<p>In the realm of aerial image analysis, object detection plays a pivotal role,
with significant implications for areas such as remote sensing, urban planning,
and disaster management. This study addresses the inherent challenges in this
domain, notably the detection of small objects, managing densely packed
elements, and accounting for diverse orientations. We present an in-depth
evaluation of an object detection model that integrates the Large Selective
Kernel Network (LSKNet)as its backbone with the DiffusionDet head, utilizing
the iSAID dataset for empirical analysis. Our approach encompasses the
introduction of novel methodologies and extensive ablation studies. These
studies critically assess various aspects such as loss functions, box
regression techniques, and classification strategies to refine the model's
precision in object detection. The paper details the experimental application
of the LSKNet backbone in synergy with the DiffusionDet heads, a combination
tailored to meet the specific challenges in aerial image object detection. The
findings of this research indicate a substantial enhancement in the model's
performance, especially in the accuracy-time tradeoff. The proposed model
achieves a mean average precision (MAP) of approximately 45.7%, which is a
significant improvement, outperforming the RCNN model by 4.7% on the same
dataset. This advancement underscores the effectiveness of the proposed
modifications and sets a new benchmark in aerial image analysis, paving the way
for more accurate and efficient object detection methodologies. The code is
publicly available at https://github.com/SashaMatsun/LSKDiffDet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12970">Clustered Policy Decision Ranking. (arXiv:2311.12970v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levin_M/0/1/0/all/0/1">Mark Levin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1">Hana Chockler</a></p>
<p>Policies trained via reinforcement learning (RL) are often very complex even
for simple tasks. In an episode with n time steps, a policy will make n
decisions on actions to take, many of which may appear non-intuitive to the
observer. Moreover, it is not clear which of these decisions directly
contribute towards achieving the reward and how significant their contribution
is. Given a trained policy, we propose a black-box method based on statistical
covariance estimation that clusters the states of the environment and ranks
each cluster according to the importance of decisions made in its states. We
compare our measure against a previous statistical fault localization based
ranking procedure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12997">How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks. (arXiv:2311.12997v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1">Rahul Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Khona_M/0/1/0/all/0/1">Mikail Khona</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1">Robert P. Dick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1">Ekdeep Singh Lubana</a></p>
<p>Transformers trained on huge text corpora exhibit a remarkable set of
capabilities, e.g., performing simple logical operations. Given the inherent
compositional nature of language, one can expect the model to learn to compose
these capabilities, potentially yielding a combinatorial explosion of what
operations it can perform on an input. Motivated by the above, we aim to assess
in this paper "how capable can a transformer become?". Specifically, we train
autoregressive Transformer models on a data-generating process that involves
compositions of a set of well-defined monolithic capabilities. Through a series
of extensive and systematic experiments on this data-generating process, we
show that: (1) autoregressive Transformers can learn compositional structures
from the training data and generalize to exponentially or even combinatorially
many functions; (2) composing functions by generating intermediate outputs is
more effective at generalizing to unseen compositions, compared to generating
no intermediate outputs; (3) the training data has a significant impact on the
model's ability to compose unseen combinations of functions; and (4) the
attention layers in the latter half of the model are critical to
compositionality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12999">CovarNav: Machine Unlearning via Model Inversion and Covariance Navigation. (arXiv:2311.12999v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1">Ali Abbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrash_C/0/1/0/all/0/1">Chayne Thrash</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbari_E/0/1/0/all/0/1">Elaheh Akbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Daniel Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1">Soheil Kolouri</a></p>
<p>The rapid progress of AI, combined with its unprecedented public adoption and
the propensity of large neural networks to memorize training data, has given
rise to significant data privacy concerns. To address these concerns, machine
unlearning has emerged as an essential technique to selectively remove the
influence of specific training data points on trained models. In this paper, we
approach the machine unlearning problem through the lens of continual learning.
Given a trained model and a subset of training data designated to be forgotten
(i.e., the "forget set"), we introduce a three-step process, named CovarNav, to
facilitate this forgetting. Firstly, we derive a proxy for the model's training
data using a model inversion attack. Secondly, we mislabel the forget set by
selecting the most probable class that deviates from the actual ground truth.
Lastly, we deploy a gradient projection method to minimize the cross-entropy
loss on the modified forget set (i.e., learn incorrect labels for this set)
while preventing forgetting of the inverted samples. We rigorously evaluate
CovarNav on the CIFAR-10 and Vggface2 datasets, comparing our results with
recent benchmarks in the field and demonstrating the efficacy of our proposed
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13015">Fast and Interpretable Mortality Risk Scores for Critical Care Patients. (arXiv:2311.13015v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chloe Qinyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1">Muhang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1">Lesia Semenova</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiachang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jack Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarpa_J/0/1/0/all/0/1">Joseph Scarpa</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a></p>
<p>Prediction of mortality in intensive care unit (ICU) patients is an important
task in critical care medicine. Prior work in creating mortality risk models
falls into two major categories: domain-expert-created scoring systems, and
black box machine learning (ML) models. Both of these have disadvantages: black
box models are unacceptable for use in hospitals, whereas manual creation of
models (including hand-tuning of logistic regression parameters) relies on
humans to perform high-dimensional constrained optimization, which leads to a
loss in performance. In this work, we bridge the gap between accurate black box
models and hand-tuned interpretable models. We build on modern interpretable ML
techniques to design accurate and interpretable mortality risk scores. We
leverage the largest existing public ICU monitoring datasets, namely the MIMIC
III and eICU datasets. By evaluating risk across medical centers, we are able
to study generalization across domains. In order to customize our risk score
models, we develop a new algorithm, GroupFasterRisk, which has several
important benefits: (1) it uses hard sparsity constraint, allowing users to
directly control the number of features; (2) it incorporates group sparsity to
allow more cohesive models; (3) it allows for monotonicity correction on models
for including domain knowledge; (4) it produces many equally-good models at
once, which allows domain experts to choose among them. GroupFasterRisk creates
its risk scores within hours, even on the large datasets we study here.
GroupFasterRisk's risk scores perform better than risk scores currently used in
hospitals, and have similar prediction performance to black box ML models
(despite being much sparser). Because GroupFasterRisk produces a variety of
risk scores and handles constraints, it allows design flexibility, which is the
key enabler of practical and trustworthy model creation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13022">Unsupervised Multimodal Surface Registration with Geometric Deep Learning. (arXiv:2311.13022v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suliman_M/0/1/0/all/0/1">Mohamed A. Suliman</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1">Logan Z. J. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Fawaz_A/0/1/0/all/0/1">Abdulah Fawaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_E/0/1/0/all/0/1">Emma C. Robinson</a></p>
<p>This paper introduces GeoMorph, a novel geometric deep-learning framework
designed for image registration of cortical surfaces. The registration process
consists of two main steps. First, independent feature extraction is performed
on each input surface using graph convolutions, generating low-dimensional
feature representations that capture important cortical surface
characteristics. Subsequently, features are registered in a deep-discrete
manner to optimize the overlap of common structures across surfaces by learning
displacements of a set of control points. To ensure smooth and biologically
plausible deformations, we implement regularization through a deep conditional
random field implemented with a recurrent neural network. Experimental results
demonstrate that GeoMorph surpasses existing deep-learning methods by achieving
improved alignment with smoother deformations. Furthermore, GeoMorph exhibits
competitive performance compared to classical frameworks. Such versatility and
robustness suggest strong potential for various neuroscience applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13028">DMLR: Data-centric Machine Learning Research -- Past, Present and Future. (arXiv:2311.13028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oala_L/0/1/0/all/0/1">Luis Oala</a>, <a href="http://arxiv.org/find/cs/1/au:+Maskey_M/0/1/0/all/0/1">Manil Maskey</a>, <a href="http://arxiv.org/find/cs/1/au:+Bat_Leah_L/0/1/0/all/0/1">Lilith Bat-Leah</a>, <a href="http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1">Alicia Parrish</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurel_N/0/1/0/all/0/1">Nezihe Merve G&#xfc;rel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1">Tzu-Sheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1">Rotem Dror</a>, <a href="http://arxiv.org/find/cs/1/au:+Brajovic_D/0/1/0/all/0/1">Danilo Brajovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1">Xiaozhe Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1">Max Bartolo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojas_W/0/1/0/all/0/1">William A Gaviria Rojas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hileman_R/0/1/0/all/0/1">Ryan Hileman</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliment_R/0/1/0/all/0/1">Rainier Aliment</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1">Michael W. Mahoney</a>, <a href="http://arxiv.org/find/cs/1/au:+Risdal_M/0/1/0/all/0/1">Meg Risdal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1">Matthew Lease</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_D/0/1/0/all/0/1">Debojyoti Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Northcutt_C/0/1/0/all/0/1">Curtis G Northcutt</a>, <a href="http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1">Cody Coleman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hancock_B/0/1/0/all/0/1">Braden Hancock</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_B/0/1/0/all/0/1">Bernard Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1">Girmaw Abebe Tadesse</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1">Bojan Karla&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaa_A/0/1/0/all/0/1">Ahmed Alaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieng_A/0/1/0/all/0/1">Adji Bousso Dieng</a>, <a href="http://arxiv.org/find/cs/1/au:+Noy_N/0/1/0/all/0/1">Natasha Noy</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1">Vijay Janapa Reddi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Paritosh_P/0/1/0/all/0/1">Praveen Paritosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bollacker_K/0/1/0/all/0/1">Kurt Bollacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1">Lora Aroyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1">Joaquin Vanschoren</a>, <a href="http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1">Isabelle Guyon</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattson_P/0/1/0/all/0/1">Peter Mattson</a></p>
<p>Drawing from discussions at the inaugural DMLR workshop at ICML 2023 and
meetings prior, in this report we outline the relevance of community engagement
and infrastructure development for the creation of next-generation public
datasets that will advance machine learning science. We chart a path forward as
a collective effort to sustain the creation and maintenance of these datasets
and methods towards positive scientific, societal and business impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13036">Favour: FAst Variance Operator for Uncertainty Rating. (arXiv:2311.13036v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahle_T/0/1/0/all/0/1">Thomas D. Ahle</a>, <a href="http://arxiv.org/find/cs/1/au:+Karimi_S/0/1/0/all/0/1">Sahar Karimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1">Peter Tak Peter Tang</a></p>
<p>Bayesian Neural Networks (BNN) have emerged as a crucial approach for
interpreting ML predictions. By sampling from the posterior distribution, data
scientists may estimate the uncertainty of an inference. Unfortunately many
inference samples are often needed, the overhead of which greatly hinder BNN's
wide adoption. To mitigate this, previous work proposed propagating the first
and second moments of the posterior directly through the network. However, on
its own this method is even slower than sampling, so the propagated variance
needs to be approximated such as assuming independence between neural nodes.
The resulting trade-off between quality and inference time did not match even
plain Monte Carlo sampling.
</p>
<p>Our contribution is a more principled variance propagation framework based on
"spiked covariance matrices", which smoothly interpolates between quality and
inference time. This is made possible by a new fast algorithm for updating a
diagonal-plus-low-rank matrix approximation under various operations. We tested
our algorithm against sampling based MC Dropout and Variational Inference on a
number of downstream uncertainty themed tasks, such as calibration and
out-of-distribution testing. We find that Favour is as fast as performing 2-3
inference samples, while matching the performance of 10-100 samples.
</p>
<p>In summary, this work enables the use of BNN in the realm of performance
critical tasks where they have previously been out of reach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13038">Synaptic Sampling of Neural Networks. (arXiv:2311.13038v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aimone_J/0/1/0/all/0/1">James B. Aimone</a>, <a href="http://arxiv.org/find/cs/1/au:+Severa_W/0/1/0/all/0/1">William Severa</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1">J. Darby Smith</a></p>
<p>Probabilistic artificial neural networks offer intriguing prospects for
enabling the uncertainty of artificial intelligence methods to be described
explicitly in their function; however, the development of techniques that
quantify uncertainty by well-understood methods such as Monte Carlo sampling
has been limited by the high costs of stochastic sampling on deterministic
computing hardware. Emerging computing systems that are amenable to
hardware-level probabilistic computing, such as those that leverage stochastic
devices, may make probabilistic neural networks more feasible in the
not-too-distant future. This paper describes the scANN technique --
\textit{sampling (by coinflips) artificial neural networks} -- which enables
neural networks to be sampled directly by treating the weights as Bernoulli
coin flips. This method is natively well suited for probabilistic computing
techniques that focus on tunable stochastic devices, nearly matches fully
deterministic performance while also describing the uncertainty of correct and
incorrect neural network outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13046">Do we listen to what we are told? An empirical study on human behaviour during the COVID-19 pandemic: neural networks vs. regression analysis. (arXiv:2311.13046v1 [econ.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Heluo_Y/0/1/0/all/0/1">Yuxi Heluo</a>, <a href="http://arxiv.org/find/econ/1/au:+Wang_K/0/1/0/all/0/1">Kexin Wang</a>, <a href="http://arxiv.org/find/econ/1/au:+Robson_C/0/1/0/all/0/1">Charles W. Robson</a></p>
<p>In this work, we contribute the first visual open-source empirical study on
human behaviour during the COVID-19 pandemic, in order to investigate how
compliant a general population is to mask-wearing-related public-health policy.
Object-detection-based convolutional neural networks, regression analysis and
multilayer perceptrons are combined to analyse visual data of the Viennese
public during 2020. We find that mask-wearing-related government regulations
and public-transport announcements encouraged correct mask-wearing-behaviours
during the COVID-19 pandemic. Importantly, changes in announcement and
regulation contents led to heterogeneous effects on people's behaviour.
Comparing the predictive power of regression analysis and neural networks, we
demonstrate that the latter produces more accurate predictions of population
reactions during the COVID-19 pandemic. Our use of regression modelling also
allows us to unearth possible causal pathways underlying societal behaviour.
Since our findings highlight the importance of appropriate communication
contents, our results will facilitate more effective non-pharmaceutical
interventions to be developed in future. Adding to the literature, we
demonstrate that regression modelling and neural networks are not mutually
exclusive but instead complement each other.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13050">Multi-fidelity Bayesian Optimization in Engineering Design. (arXiv:2311.13050v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_B/0/1/0/all/0/1">Bach Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruda Zhang</a></p>
<p>Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian
optimization (BO), MF BO has found a niche in solving expensive engineering
design optimization problems, thanks to its advantages in incorporating
physical and mathematical understandings of the problems, saving resources,
addressing exploitation-exploration trade-off, considering uncertainty, and
processing parallel computing. The increasing number of works dedicated to MF
BO suggests the need for a comprehensive review of this advanced optimization
technique. In this paper, we survey recent developments of two essential
ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition
functions. We first categorize the existing MF modeling methods and MFO
strategies to locate MF BO in a large family of surrogate-based optimization
and MFO algorithms. We then exploit the common properties shared between the
methods from each ingredient of MF BO to describe important GP-based MF
surrogate models and review various acquisition functions. By doing so, we
expect to provide a structured understanding of MF BO. Finally, we attempt to
reveal important aspects that require further research for applications of MF
BO in solving intricate yet important design optimization problems, including
constrained optimization, high-dimensional optimization, optimization under
uncertainty, and multi-objective optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13052">Novel OCT mosaicking pipeline with Feature- and Pixel-based registration. (arXiv:2311.13052v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiacheng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1">Dewei Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1">Yuankai K. Tao</a>, <a href="http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1">Ipek Oguz</a></p>
<p>High-resolution Optical Coherence Tomography (OCT) images are crucial for
ophthalmology studies but are limited by their relatively narrow field of view
(FoV). Image mosaicking is a technique for aligning multiple overlapping images
to obtain a larger FoV. Current mosaicking pipelines often struggle with
substantial noise and considerable displacement between the input sub-fields.
In this paper, we propose a versatile pipeline for stitching multi-view
OCT/OCTA \textit{en face} projection images. Our method combines the strengths
of learning-based feature matching and robust pixel-based registration to align
multiple images effectively. Furthermore, we advance the application of a
trained foundational model, Segment Anything Model (SAM), to validate
mosaicking results in an unsupervised manner. The efficacy of our pipeline is
validated using an in-house dataset and a large public dataset, where our
method shows superior performance in terms of both accuracy and computational
efficiency. We also made our evaluation tool for image mosaicking and the
corresponding pipeline publicly available at
\url{https://github.com/MedICL-VU/OCT-mosaicking}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13059">A note on estimating the dimension from a random geometric graph. (arXiv:2311.13059v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Atamanchuk_C/0/1/0/all/0/1">Caelan Atamanchuk</a>, <a href="http://arxiv.org/find/stat/1/au:+Devroye_L/0/1/0/all/0/1">Luc Devroye</a>, <a href="http://arxiv.org/find/stat/1/au:+Lugosi_G/0/1/0/all/0/1">Gabor Lugosi</a></p>
<p>Let $G_n$ be a random geometric graph with vertex set $[n]$ based on $n$
i.i.d.\ random vectors $X_1,\ldots,X_n$ drawn from an unknown density $f$ on
$\R^d$. An edge $(i,j)$ is present when $\|X_i -X_j\| \le r_n$, for a given
threshold $r_n$ possibly depending upon $n$, where $\| \cdot \|$ denotes
Euclidean distance. We study the problem of estimating the dimension $d$ of the
underlying space when we have access to the adjacency matrix of the graph but
do not know $r_n$ or the vectors $X_i$. The main result of the paper is that
there exists an estimator of $d$ that converges to $d$ in probability as $n \to
\infty$ for all densities with $\int f^5 &lt; \infty$ whenever $n^{3/2} r_n^d \to
\infty$ and $r_n = o(1)$. The conditions allow very sparse graphs since when
$n^{3/2} r_n^d \to 0$, the graph contains isolated edges only, with high
probability. We also show that, without any condition on the density, a
consistent estimator of $d$ exists when $n r_n^d \to \infty$ and $r_n = o(1)$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13073">FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline. (arXiv:2311.13073v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1">Vladimir Arkhipkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaheen_Z/0/1/0/all/0/1">Zein Shaheen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1">Viacheslav Vasilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Dakhova_E/0/1/0/all/0/1">Elizaveta Dakhova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a></p>
<p>Multimedia generation approaches occupy a prominent place in artificial
intelligence research. Text-to-image models achieved high-quality results over
the last few years. However, video synthesis methods recently started to
develop. This paper presents a new two-stage latent diffusion text-to-video
generation architecture based on the text-to-image diffusion model. The first
stage concerns keyframes synthesis to figure the storyline of a video, while
the second one is devoted to interpolation frames generation to make movements
of the scene and objects smooth. We compare several temporal conditioning
approaches for keyframes generation. The results show the advantage of using
separate temporal blocks over temporal layers in terms of metrics reflecting
video generation quality aspects and human preference. The design of our
interpolation model significantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we evaluate different
configurations of MoVQ-based video decoding scheme to improve consistency and
achieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare our
pipeline with existing solutions and achieve top-2 scores overall and top-1
among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:
https://ai-forever.github.io/kandinsky-video/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13081">Learning to Fly in Seconds. (arXiv:2311.13081v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eschmann_J/0/1/0/all/0/1">Jonas Eschmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Albani_D/0/1/0/all/0/1">Dario Albani</a>, <a href="http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1">Giuseppe Loianno</a></p>
<p>Learning-based methods, particularly Reinforcement Learning (RL), hold great
promise for streamlining deployment, enhancing performance, and achieving
generalization in the control of autonomous multirotor aerial vehicles. Deep RL
has been able to control complex systems with impressive fidelity and agility
in simulation but the simulation-to-reality transfer often brings a
hard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively
long training times. In this work, we propose a novel asymmetric
actor-critic-based architecture coupled with a highly reliable RL-based
training paradigm for end-to-end quadrotor control. We show how curriculum
learning and a highly optimized simulator enhance sample complexity and lead to
fast training times. To precisely discuss the challenges related to
low-level/end-to-end multirotor control, we also introduce a taxonomy that
classifies the existing levels of control abstractions as well as
non-linearities and domain parameters. Our framework enables
Simulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18
seconds of training on a consumer-grade laptop as well as its deployment on
microcontrollers to control a multirotor under real-time guarantees. Finally,
our solution exhibits competitive performance in trajectory tracking, as
demonstrated through various experimental comparisons with existing
state-of-the-art control solutions using a real Crazyflie nano quadrotor. We
open source the code including a very fast multirotor dynamics simulator that
can simulate about 5 months of flight per second on a laptop GPU. The fast
training times and deployment to a cheap, off-the-shelf quadrotor lower the
barriers to entry and help democratize the research and development of these
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13087">Predict-Then-Optimize by Proxy: Learning Joint Models of Prediction and Optimization. (arXiv:2311.13087v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotary_J/0/1/0/all/0/1">James Kotary</a>, <a href="http://arxiv.org/find/cs/1/au:+Vito_V/0/1/0/all/0/1">Vincenzo Di Vito</a>, <a href="http://arxiv.org/find/cs/1/au:+Christopher_J/0/1/0/all/0/1">Jacob Christopher</a>, <a href="http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1">Pascal Van Hentenryck</a>, <a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1">Ferdinando Fioretto</a></p>
<p>Many real-world decision processes are modeled by optimization problems whose
defining parameters are unknown and must be inferred from observable data. The
Predict-Then-Optimize framework uses machine learning models to predict unknown
parameters of an optimization problem from features before solving. Recent
works show that decision quality can be improved in this setting by solving and
differentiating the optimization problem in the training loop, enabling
end-to-end training with loss functions defined directly on the resulting
decisions. However, this approach can be inefficient and requires handcrafted,
problem-specific rules for backpropagation through the optimization step. This
paper proposes an alternative method, in which optimal solutions are learned
directly from the observable features by predictive models. The approach is
generic, and based on an adaptation of the Learning-to-Optimize paradigm, from
which a rich variety of existing techniques can be employed. Experimental
evaluations show the ability of several Learning-to-Optimize methods to provide
efficient, accurate, and flexible solutions to an array of challenging
Predict-Then-Optimize problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13091">Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise. (arXiv:2311.13091v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a></p>
<p>The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model's
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise's instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13094">Newton-CG methods for nonconvex unconstrained optimization with H\&quot;older continuous Hessian. (arXiv:2311.13094v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+He_C/0/1/0/all/0/1">Chuan He</a>, <a href="http://arxiv.org/find/math/1/au:+Lu_Z/0/1/0/all/0/1">Zhaosong Lu</a></p>
<p>In this paper we consider a nonconvex unconstrained optimization problem
minimizing a twice differentiable objective function with H\"older continuous
Hessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG)
method for finding an approximate first-order stationary point (FOSP) of this
problem, assuming the associated the H\"older parameters are explicitly known.
Then we develop a parameter-free Newton-CG method without requiring any prior
knowledge of these parameters. To the best of our knowledge, this method is the
first parameter-free second-order method achieving the best-known iteration and
operation complexity for finding an approximate FOSP of this problem.
Furthermore, we propose a Newton-CG method for finding an approximate
second-order stationary point (SOSP) of the considered problem with high
probability and establish its iteration and operation complexity. Finally, we
present preliminary numerical results to demonstrate the superior practical
performance of our parameter-free Newton-CG method over a well-known
regularized Newton method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13099">PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF. (arXiv:2311.13099v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yintong Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1">Tianjia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a></p>
<p>We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13102">Detecting out-of-distribution text using topological features of transformer-based language models. (arXiv:2311.13102v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pollano_A/0/1/0/all/0/1">Andres Pollano</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1">Anupam Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Simmons_A/0/1/0/all/0/1">Anj Simmons</a></p>
<p>We attempt to detect out-of-distribution (OOD) text samples though applying
Topological Data Analysis (TDA) to attention maps in transformer-based language
models. We evaluate our proposed TDA-based approach for out-of-distribution
detection on BERT, a transformer-based language model, and compare the to a
more traditional OOD approach based on BERT CLS embeddings. We found that our
TDA approach outperforms the CLS embedding approach at distinguishing
in-distribution data (politics and entertainment news articles from HuffPost)
from far out-of-domain samples (IMDB reviews), but its effectiveness
deteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business
news articles from HuffPost) datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13110">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?. (arXiv:2311.13110v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1">Sam Buchanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1">Druv Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tianzhe Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Shengbang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1">Hao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuexiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1">Benjamin D. Haeffele</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a></p>
<p>In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13118">Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements. (arXiv:2311.13118v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1">Alejandro Rodriguez Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivas_P/0/1/0/all/0/1">Pablo Rivas</a></p>
<p>This project tackles the pressing issue of human trafficking in online C2C
marketplaces through advanced Natural Language Processing (NLP) techniques. We
introduce a novel methodology for generating pseudo-labeled datasets with
minimal supervision, serving as a rich resource for training state-of-the-art
NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and
Organized Activity Detection (OAD), we employ cutting-edge Transformer models
for analysis. A key contribution is the implementation of an interpretability
framework using Integrated Gradients, providing explainable insights crucial
for law enforcement. This work not only fills a critical gap in the literature
but also offers a scalable, machine learning-driven approach to combat human
exploitation online. It serves as a foundation for future research and
practical applications, emphasizing the role of machine learning in addressing
complex social issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13133">LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms. (arXiv:2311.13133v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1">Aditi Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Havens_S/0/1/0/all/0/1">Sam Havens</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohmann_J/0/1/0/all/0/1">Jeremey Dohmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1">Alex Trott</a>, <a href="http://arxiv.org/find/cs/1/au:+Portes_J/0/1/0/all/0/1">Jacob Portes</a></p>
<p>Large Language Models are traditionally finetuned on large instruction
datasets. However recent studies suggest that small, high-quality datasets can
suffice for general purpose instruction following. This lack of consensus
surrounding finetuning best practices is in part due to rapidly diverging
approaches to LLM evaluation. In this study, we ask whether a small amount of
diverse finetuning samples can improve performance on both traditional
perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We
finetune open-source MPT-7B and MPT-30B models on instruction finetuning
datasets of various sizes ranging from 1k to 60k samples. We find that subsets
of 1k-6k instruction finetuning samples are sufficient to achieve good
performance on both (1) traditional NLP benchmarks and (2) model-based
evaluation. Finally, we show that mixing textbook-style and open-ended QA
finetuning datasets optimizes performance on both evaluation paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13147">Optimal Transport with Cyclic Symmetry. (arXiv:2311.13147v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Takeda_S/0/1/0/all/0/1">Shoichiro Takeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Akagi_Y/0/1/0/all/0/1">Yasunori Akagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Marumo_N/0/1/0/all/0/1">Naoki Marumo</a>, <a href="http://arxiv.org/find/cs/1/au:+Niwa_K/0/1/0/all/0/1">Kenta Niwa</a></p>
<p>We propose novel fast algorithms for optimal transport (OT) utilizing a
cyclic symmetry structure of input data. Such OT with cyclic symmetry appears
universally in various real-world examples: image processing, urban planning,
and graph processing. Our main idea is to reduce OT to a small optimization
problem that has significantly fewer variables by utilizing cyclic symmetry and
various optimization techniques. On the basis of this reduction, our algorithms
solve the small optimization problem instead of the original OT. As a result,
our algorithms obtain the optimal solution and the objective function value of
the original OT faster than solving the original OT directly. In this paper,
our focus is on two crucial OT formulations: the linear programming OT (LOT)
and the strongly convex-regularized OT, which includes the well-known
entropy-regularized OT (EROT). Experiments show the effectiveness of our
algorithms for LOT and EROT in synthetic/real-world data that has a
strict/approximate cyclic symmetry structure. Through theoretical and
experimental results, this paper successfully introduces the concept of
symmetry into the OT research field for the first time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13154">Testing Closeness of Multivariate Distributions via Ramsey Theory. (arXiv:2311.13154v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sihan Liu</a></p>
<p>We investigate the statistical task of closeness (or equivalence) testing for
multidimensional distributions. Specifically, given sample access to two
unknown distributions $\mathbf p, \mathbf q$ on $\mathbb R^d$, we want to
distinguish between the case that $\mathbf p=\mathbf q$ versus $\|\mathbf
p-\mathbf q\|_{A_k} &gt; \epsilon$, where $\|\mathbf p-\mathbf q\|_{A_k}$ denotes
the generalized ${A}_k$ distance between $\mathbf p$ and $\mathbf q$ --
measuring the maximum discrepancy between the distributions over any collection
of $k$ disjoint, axis-aligned rectangles. Our main result is the first
closeness tester for this problem with {\em sub-learning} sample complexity in
any fixed dimension and a nearly-matching sample complexity lower bound.
</p>
<p>In more detail, we provide a computationally efficient closeness tester with
sample complexity $O\left((k^{6/7}/ \mathrm{poly}_d(\epsilon))
\log^d(k)\right)$. On the lower bound side, we establish a qualitatively
matching sample complexity lower bound of
$\Omega(k^{6/7}/\mathrm{poly}(\epsilon))$, even for $d=2$. These sample
complexity bounds are surprising because the sample complexity of the problem
in the univariate setting is $\Theta(k^{4/5}/\mathrm{poly}(\epsilon))$. This
has the interesting consequence that the jump from one to two dimensions leads
to a substantial increase in sample complexity, while increases beyond that do
not.
</p>
<p>As a corollary of our general $A_k$ tester, we obtain $d_{\mathrm
TV}$-closeness testers for pairs of $k$-histograms on $\mathbb R^d$ over a
common unknown partition, and pairs of uniform distributions supported on the
union of $k$ unknown disjoint axis-aligned rectangles.
</p>
<p>Both our algorithm and our lower bound make essential use of tools from
Ramsey theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13159">Multi-Objective Optimization via Wasserstein-Fisher-Rao Gradient Flow. (arXiv:2311.13159v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yinuo Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tesi Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gangwani_T/0/1/0/all/0/1">Tanmay Gangwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangi_A/0/1/0/all/0/1">Anshuka Rangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmanian_H/0/1/0/all/0/1">Holakou Rahmanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1">Lexing Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Subhajit Sanyal</a></p>
<p>Multi-objective optimization (MOO) aims to optimize multiple, possibly
conflicting objectives with widespread applications. We introduce a novel
interacting particle method for MOO inspired by molecular dynamics simulations.
Our approach combines overdamped Langevin and birth-death dynamics,
incorporating a "dominance potential" to steer particles toward global Pareto
optimality. In contrast to previous methods, our method is able to relocate
dominated particles, making it particularly adept at managing Pareto fronts of
complicated geometries. Our method is also theoretically grounded as a
Wasserstein-Fisher-Rao gradient flow with convergence guarantees. Extensive
experiments confirm that our approach outperforms state-of-the-art methods on
challenging synthetic and real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13163">Have Your Cake and Eat It Too: Toward Efficient and Accurate Split Federated Learning. (arXiv:2311.13163v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1">Dengke Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Ming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1">Zeke Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaofei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingsong Chen</a></p>
<p>Due to its advantages in resource constraint scenarios, Split Federated
Learning (SFL) is promising in AIoT systems. However, due to data heterogeneity
and stragglers, SFL suffers from the challenges of low inference accuracy and
low efficiency. To address these issues, this paper presents a novel SFL
approach, named Sliding Split Federated Learning (S$^2$FL), which adopts an
adaptive sliding model split strategy and a data balance-based training
mechanism. By dynamically dispatching different model portions to AIoT devices
according to their computing capability, S$^2$FL can alleviate the low training
efficiency caused by stragglers. By combining features uploaded by devices with
different data distributions to generate multiple larger batches with a uniform
distribution for back-propagation, S$^2$FL can alleviate the performance
degradation caused by data heterogeneity. Experimental results demonstrate
that, compared to conventional SFL, S$^2$FL can achieve up to 16.5\% inference
accuracy improvement and 3.54X training acceleration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13166">AdaptiveFL: Adaptive Heterogeneous Federated Learning for Resource-Constrained AIoT Systems. (arXiv:2311.13166v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Chentao Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Ming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zekai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaofei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingsong Chen</a></p>
<p>Although Federated Learning (FL) is promising to enable collaborative
learning among Artificial Intelligence of Things (AIoT) devices, it suffers
from the problem of low classification performance due to various heterogeneity
factors (e.g., computing capacity, memory size) of devices and uncertain
operating environments. To address these issues, this paper introduces an
effective FL approach named AdaptiveFL based on a novel fine-grained width-wise
model pruning strategy, which can generate various heterogeneous local models
for heterogeneous AIoT devices. By using our proposed reinforcement
learning-based device selection mechanism, AdaptiveFL can adaptively dispatch
suitable heterogeneous models to corresponding AIoT devices on the fly based on
their available resources for local training. Experimental results show that,
compared to state-of-the-art methods, AdaptiveFL can achieve up to 16.83%
inference improvements for both IID and non-IID scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13169">SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape. (arXiv:2311.13169v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hua Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kuang-Hung Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorov_I/0/1/0/all/0/1">Igor Fedorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wei Wen</a></p>
<p>Neural Architecture Search (NAS) has become a widely used tool for automating
neural network design. While one-shot NAS methods have successfully reduced
computational requirements, they often require extensive training. On the other
hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate
architecture's test performance but has two limitations: (1) inability to use
the information gained as a network improves with training and (2) unreliable
performance, particularly in complex domains like RecSys, due to the
multi-modal data inputs and complex architecture configurations. To synthesize
the benefits of both methods, we introduce a "sub-one-shot" paradigm that
serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the
supernet is trained using only a small subset of the training data, a phase we
refer to as "warm-up." Within this framework, we present SiGeo, a proxy founded
on a novel theoretical framework that connects the supernet warm-up with the
efficacy of the proxy. Extensive experiments have shown that SiGeo, with the
benefit of warm-up, consistently outperforms state-of-the-art NAS proxies on
various established NAS benchmarks. When a supernet is warmed up, it can
achieve comparable performance to weight-sharing one-shot NAS methods, but with
a significant reduction ($\sim 60$\%) in computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13171">ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization. (arXiv:2311.13171v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Prateek Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Parameter-efficient fine-tuning (PEFT) techniques make it possible to
efficiently adapt a language model to create "expert" models that specialize to
new tasks or domains. Recent techniques in model merging and compositional
generalization leverage these expert models by dynamically composing modules to
improve zero/few-shot generalization. Despite the efficiency of PEFT methods,
the size of expert models can make it onerous to retrieve expert models per
query over high-latency networks like the Internet or serve multiple experts on
a single GPU. To address these issues, we present ComPEFT, a novel method for
compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT
employs sparsification and ternary quantization to reduce the size of the PEFT
module without performing any additional retraining while preserving or
enhancing model performance. In extensive evaluation across T5, T0, and
LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression
ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale -
stronger models exhibit higher compressibility and better performance. For
example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on
MMLU with a storage size reduction of up to 26x. In addition, we show that the
compressed experts produced by ComPEFT maintain few-shot compositional
generalization capabilities, facilitate efficient communication and
computation, and exhibit enhanced performance when merged. Lastly, we provide
an analysis of different method components, compare it with other PEFT methods,
and test ComPEFT's efficacy for compressing the residual of full-finetuning.
Our code is available at https://github.com/prateeky2806/compeft.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13174">SecureCut: Federated Gradient Boosting Decision Trees with Efficient Machine Unlearning. (arXiv:2311.13174v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bowen Li Jie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chentao Wu</a></p>
<p>In response to legislation mandating companies to honor the \textit{right to
be forgotten} by erasing user data, it has become imperative to enable data
removal in Vertical Federated Learning (VFL) where multiple parties provide
private features for model training. In VFL, data removal, i.e.,
\textit{machine unlearning}, often requires removing specific features across
all samples under privacy guarentee in federated learning. To address this
challenge, we propose \methname, a novel Gradient Boosting Decision Tree (GBDT)
framework that effectively enables both \textit{instance unlearning} and
\textit{feature unlearning} without the need for retraining from scratch.
Leveraging a robust GBDT structure, we enable effective data deletion while
reducing degradation of model performance. Extensive experimental results on
popular datasets demonstrate that our method achieves superior model utility
and forgetfulness compared to \textit{state-of-the-art} methods. To our best
knowledge, this is the first work that investigates machine unlearning in VFL
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13180">Provably Efficient High-Dimensional Bandit Learning with Batched Feedbacks. (arXiv:2311.13180v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1">Jianqing Fan</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ye_C/0/1/0/all/0/1">Chenlu Ye</a></p>
<p>We study high-dimensional multi-armed contextual bandits with batched
feedback where the $T$ steps of online interactions are divided into $L$
batches. In specific, each batch collects data according to a policy that
depends on previous batches and the rewards are revealed only at the end of the
batch. Such a feedback structure is popular in applications such as
personalized medicine and online advertisement, where the online data often do
not arrive in a fully serial manner. We consider high-dimensional and linear
settings where the reward function of the bandit model admits either a sparse
or low-rank structure and ask how small a number of batches are needed for a
comparable performance with fully dynamic data in which $L = T$. For these
settings, we design a provably sample-efficient algorithm which achieves a $
\mathcal{\tilde O}(s_0^2 \log^2 T)$ regret in the sparse case and $
\mathcal{\tilde O} ( r ^2 \log^2 T)$ regret in the low-rank case, using only $L
= \mathcal{O}( \log T)$ batches. Here $s_0$ and $r$ are the sparsity and rank
of the reward parameter in sparse and low-rank cases, respectively, and $
\mathcal{\tilde O}(\cdot)$ omits logarithmic factors involving the feature
dimensions. In other words, our algorithm achieves regret bounds comparable to
those in fully sequential setting with only $\mathcal{O}( \log T)$ batches. Our
algorithm features a novel batch allocation method that adjusts the batch sizes
according to the estimation accuracy within each batch and cumulative regret.
Furthermore, we also conduct experiments with synthetic and real-world data to
validate our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13184">AS-LLM: When Algorithm Selection Meets Large Language Model. (arXiv:2311.13184v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jibin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1">Kay Chen Tan</a></p>
<p>Algorithm selection aims to identify the most suitable algorithm for solving
a specific problem before execution, which has become a critical process of the
AutoML. Current mainstream algorithm selection techniques rely heavily on
feature representations of various problems and employ the performance of each
algorithm as supervised information. However, there is a significant research
gap concerning the consideration of algorithm features. This gap is primarily
attributed to the inherent complexity of algorithms, making it particularly
challenging to find a universally effective feature extraction method that is
applicable across a diverse range of algorithms. Unfortunately, neglecting this
aspect undoubtedly impacts the accuracy of algorithm selection and indirectly
necessitates an increased volume of problem data for training purposes. This
paper takes a significant stride towards addressing this gap by proposing an
approach that integrates algorithm representation into the algorithm selection
process. Specifically, our proposed model employs distinct modules to extract
representations of both problems and algorithms, where the algorithm
representation leverages the capabilities of pre-trained LLMs in the realm of
code comprehension. Following the extraction of embedding vectors for both
algorithms and problems, the most suitable algorithm is determined through
calculations of matching degrees. Our experiments not only validate the
effectiveness of the proposed model but also showcase the performance of
different embedded pre-trained LLMs, which suggests that the proposed algorithm
selection framework holds the potential to serve as a baseline task for
evaluating the code representation capabilities of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13188">Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation. (arXiv:2311.13188v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taesan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1">Taekyoon Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Junui Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yelim Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Mincheol Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyunam Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1">Sungil Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1">Hyungjun Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minsung Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1">Jaegul Choo</a></p>
<p>This paper investigates Cross-Domain Sequential Recommendation (CDSR), a
promising method that uses information from multiple domains (more than three)
to generate accurate and diverse recommendations, and takes into account the
sequential nature of user interactions. The effectiveness of these systems
often depends on the complex interplay among the multiple domains. In this
dynamic landscape, the problem of negative transfer arises, where heterogeneous
knowledge between dissimilar domains leads to performance degradation due to
differences in user preferences across these domains. As a remedy, we propose a
new CDSR framework that addresses the problem of negative transfer by assessing
the extent of negative transfer from one domain to another and adaptively
assigning low weight values to the corresponding prediction losses. To this
end, the amount of negative transfer is estimated by measuring the marginal
contribution of each domain to model performance based on a cooperative game
theory. In addition, a hierarchical contrastive learning approach that
incorporates information from the sequence of coarse-level categories into that
of fine-level categories (e.g., item level) when implementing contrastive
learning was developed to mitigate negative transfer. Despite the potentially
low relevance between domains at the fine-level, there may be higher relevance
at the category level due to its generalised and broader preferences. We show
that our model is superior to prior works in terms of model performance on two
real-world datasets across ten different domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13225">NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU Heterogeneous Environments. (arXiv:2311.13225v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1">Xin Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiange Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Chunyu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Hao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yu Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Ge Yu</a></p>
<p>Graph Neural Networks (GNNs) have demonstrated outstanding performance in
various applications. Existing frameworks utilize CPU-GPU heterogeneous
environments to train GNN models and integrate mini-batch and sampling
techniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous
environments, we can divide sample-based GNN training into three steps: sample,
gather, and train. Existing GNN systems use different task orchestrating
methods to employ each step on CPU or GPU. After extensive experiments and
analysis, we find that existing task orchestrating methods fail to fully
utilize the heterogeneous resources, limited by inefficient CPU processing or
GPU resource contention. In this paper, we propose NeutronOrch, a system for
sample-based GNN training that incorporates a layer-based task orchestrating
method and ensures balanced utilization of the CPU and GPU. NeutronOrch
decouples the training process by layer and pushes down the training task of
the bottom layer to the CPU. This significantly reduces the computational load
and memory footprint of GPU training. To avoid inefficient CPU processing,
NeutronOrch only offloads the training of frequently accessed vertices to the
CPU and lets GPU reuse their embeddings with bounded staleness. Furthermore,
NeutronOrch provides a fine-grained pipeline design for the layer-based task
orchestrating method, fully overlapping different tasks on heterogeneous
resources while strictly guaranteeing bounded staleness. The experimental
results show that compared with the state-of-the-art GNN systems, NeutronOrch
can achieve up to 4.61x performance speedup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13231">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model. (arXiv:2311.13231v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jian Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiafei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chunjiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weihan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a></p>
<p>Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13244">Hard Label Black Box Node Injection Attack on Graph Neural Networks. (arXiv:2311.13244v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zihao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guofeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jingchen Tang</a></p>
<p>While graph neural networks have achieved state-of-the-art performances in
many real-world tasks including graph classification and node classification,
recent works have demonstrated they are also extremely vulnerable to
adversarial attacks. Most previous works have focused on attacking node
classification networks under impractical white-box scenarios. In this work, we
will propose a non-targeted Hard Label Black Box Node Injection Attack on Graph
Neural Networks, which to the best of our knowledge, is the first of its kind.
Under this setting, more real world tasks can be studied because our attack
assumes no prior knowledge about (1): the model architecture of the GNN we are
attacking; (2): the model's gradients; (3): the output logits of the target GNN
model. Our attack is based on an existing edge perturbation attack, from which
we restrict the optimization process to formulate a node injection attack. In
the work, we will evaluate the performance of the attack using three datasets,
COIL-DEL, IMDB-BINARY, and NCI1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13250">Towards Hetero-Client Federated Multi-Task Learning. (arXiv:2311.13250v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuxiang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Suizhi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuwen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirejiding_S/0/1/0/all/0/1">Shalayiding Sirejiding</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yue Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongtao Lu</a></p>
<p>Federated Learning (FL) enables joint training across distributed clients
using their local data privately. Federated Multi-Task Learning (FMTL) builds
on FL to handle multiple tasks, assuming model congruity that identical model
architecture is deployed in each client. To relax this assumption and thus
extend real-world applicability, we introduce a novel problem setting,
Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse
task setups. The main challenge of HC-FMTL is the model incongruity issue that
invalidates conventional aggregation methods. It also escalates the
difficulties in accurate model aggregation to deal with data and task
heterogeneity inherent in FMTL. To address these challenges, we propose the
FedHCA$^2$ framework, which allows for federated training of personalized
models by modeling relationships among heterogeneous clients. Drawing on our
theoretical insights into the difference between multi-task and federated
optimization, we propose the Hyper Conflict-Averse Aggregation scheme to
mitigate conflicts during encoder updates. Additionally, inspired by task
interaction in MTL, the Hyper Cross Attention Aggregation scheme uses
layer-wise cross attention to enhance decoder interactions while alleviating
model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for
each client to customize personalized parameter updates. Extensive experiments
demonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios
compared to representative methods. Our code will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13258">ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation. (arXiv:2311.13258v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Manling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1">Derek Hoiem</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>State-of-the-art vision-language models (VLMs) still have limited performance
in structural knowledge extraction, such as relations between objects. In this
work, we present ViStruct, a training framework to learn VLMs for effective
visual structural knowledge extraction. Two novel designs are incorporated.
First, we propose to leverage the inherent structure of programming language to
depict visual structural information. This approach enables explicit and
consistent representation of visual structural information of multiple
granularities, such as concepts, relations, and events, in a well-organized
structured format. Second, we introduce curriculum-based learning for VLMs to
progressively comprehend visual structures, from fundamental visual concepts to
intricate event structures. Our intuition is that lower-level knowledge may
contribute to complex visual structure understanding. Furthermore, we compile
and release a collection of datasets tailored for visual structural knowledge
extraction. We adopt a weakly-supervised approach to directly generate visual
event structures from captions for ViStruct training, capitalizing on abundant
image-caption pairs from the web. In experiments, we evaluate ViStruct on
visual structure prediction tasks, demonstrating its effectiveness in improving
the understanding of visual structures. The code is public at
\url{https://github.com/Yangyi-Chen/vi-struct}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13261">Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides. (arXiv:2311.13261v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hoibo_M/0/1/0/all/0/1">Maren H&#xf8;ib&#xf8;</a>, <a href="http://arxiv.org/find/eess/1/au:+Pedersen_A/0/1/0/all/0/1">Andr&#xe9; Pedersen</a>, <a href="http://arxiv.org/find/eess/1/au:+Dale_V/0/1/0/all/0/1">Vibeke Grotnes Dale</a>, <a href="http://arxiv.org/find/eess/1/au:+Berget_S/0/1/0/all/0/1">Sissel Marie Berget</a>, <a href="http://arxiv.org/find/eess/1/au:+Ytterhus_B/0/1/0/all/0/1">Borgny Ytterhus</a>, <a href="http://arxiv.org/find/eess/1/au:+Lindskog_C/0/1/0/all/0/1">Cecilia Lindskog</a>, <a href="http://arxiv.org/find/eess/1/au:+Wik_E/0/1/0/all/0/1">Elisabeth Wik</a>, <a href="http://arxiv.org/find/eess/1/au:+Akslen_L/0/1/0/all/0/1">Lars A. Akslen</a>, <a href="http://arxiv.org/find/eess/1/au:+Reinertsen_I/0/1/0/all/0/1">Ingerid Reinertsen</a>, <a href="http://arxiv.org/find/eess/1/au:+Smistad_E/0/1/0/all/0/1">Erik Smistad</a>, <a href="http://arxiv.org/find/eess/1/au:+Valla_M/0/1/0/all/0/1">Marit Valla</a></p>
<p>Digital pathology enables automatic analysis of histopathological sections
using artificial intelligence (AI). Automatic evaluation could improve
diagnostic efficiency and help find associations between morphological features
and clinical outcome. For development of such prediction models, identifying
invasive epithelial cells, and separating these from benign epithelial cells
and in situ lesions would be the first step. In this study, we aimed to develop
an AI model for segmentation of epithelial cells in sections from breast
cancer. We generated epithelial ground truth masks by restaining hematoxylin
and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'
annotations. HE/CK image pairs were used to train a convolutional neural
network, and data augmentation was used to make the model more robust. Tissue
microarrays (TMAs) from 839 patients, and whole slide images from two patients
were used for training and evaluation of the models. The sections were derived
from four cohorts of breast cancer patients. TMAs from 21 patients from a fifth
cohort was used as a second test set. In quantitative evaluation, a mean Dice
score of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial
cells, and in situ lesions, respectively, were achieved. In qualitative scoring
(0-5) by pathologists, results were best for all epithelium and invasive
epithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in
situ lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in
HE stained breast cancer slides well, but further work is needed for accurate
division between the classes. Immunohistochemistry, together with pathologists'
annotations, enabled the creation of accurate ground truths. The model is made
freely available in FastPathology and the code is available at
https://github.com/AICAN-Research/breast-epithelium-segmentation
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13265">Improved identification accuracy in equation learning via comprehensive $\boldsymbol{R^2}$-elimination and Bayesian model selection. (arXiv:2311.13265v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Nickelsen_D/0/1/0/all/0/1">Daniel Nickelsen</a>, <a href="http://arxiv.org/find/stat/1/au:+Bah_B/0/1/0/all/0/1">Bubacarr Bah</a></p>
<p>In the field of equation learning, exhaustively considering all possible
equations derived from a basis function dictionary is infeasible. Sparse
regression and greedy algorithms have emerged as popular approaches to tackle
this challenge. However, the presence of multicollinearity poses difficulties
for sparse regression techniques, and greedy steps may inadvertently exclude
terms of the true equation, leading to reduced identification accuracy. In this
article, we present an approach that strikes a balance between
comprehensiveness and efficiency in equation learning. Inspired by stepwise
regression, our approach combines the coefficient of determination, $R^2$, and
the Bayesian model evidence, $p(\boldsymbol y|\mathcal M)$, in a novel way. Our
procedure is characterized by a comprehensive search with just a minor
reduction of the model space at each iteration step. With two flavors of our
approach and the adoption of $p(\boldsymbol y|\mathcal M)$ for bi-directional
stepwise regression, we present a total of three new avenues for equation
learning. Through three extensive numerical experiments involving random
polynomials and dynamical systems, we compare our approach against four
state-of-the-art methods and two standard approaches. The results demonstrate
that our comprehensive search approach surpasses all other methods in terms of
identification accuracy. In particular, the second flavor of our approach
establishes an efficient overfitting penalty solely based on $R^2$, which
achieves highest rates of exact equation recovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13267">FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem in Federated Learning. (arXiv:2311.13267v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seongyoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) is a collaborative method for training models while
preserving data privacy in decentralized settings. However, FL encounters
challenges related to data heterogeneity, which can result in performance
degradation. In our study, we observe that as data heterogeneity increases,
feature representation in the FedAVG model deteriorates more significantly
compared to classifier weight. Additionally, we observe that as data
heterogeneity increases, the gap between higher feature norms for observed
classes, obtained from local models, and feature norms of unobserved classes
widens, in contrast to the behavior of classifier weight norms. This widening
gap extends to encompass the feature norm disparities between local and the
global models. To address these issues, we introduce Federated Averaging with
Feature Normalization Update (FedFN), a straightforward learning method. We
demonstrate the superior performance of FedFN through extensive experiments,
even when applied to pretrained ResNet18. Subsequently, we confirm the
applicability of FedFN to foundation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13279">Comprehensive Evaluation of GNN Training Systems: A Data Management Perspective. (arXiv:2311.13279v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Hao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yajiong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1">Xin Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiange Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yu Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Ge Yu</a></p>
<p>Many Graph Neural Network (GNN) training systems have emerged recently to
support efficient GNN training. Since GNNs embody complex data dependencies
between training samples, the training of GNNs should address distinct
challenges different from DNN training in data management, such as data
partitioning, batch preparation for mini-batch training, and data transferring
between CPUs and GPUs. These factors, which take up a large proportion of
training time, make data management in GNN training more significant. This
paper reviews GNN training from a data management perspective and provides a
comprehensive analysis and evaluation of the representative approaches. We
conduct extensive experiments on various benchmark datasets and show many
interesting and valuable results. We also provide some practical tips learned
from these experiments, which are helpful for designing GNN training systems in
the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13285">Improving performance of heart rate time series classification by grouping subjects. (arXiv:2311.13285v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beekhuizen_M/0/1/0/all/0/1">Michael Beekhuizen</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Naseri_A/0/1/0/all/0/1">Arman Naseri</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Tax_D/0/1/0/all/0/1">David Tax</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Bilt_I/0/1/0/all/0/1">Ivo van der Bilt</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Reinders_M/0/1/0/all/0/1">Marcel Reinders</a> (1) ((1) Delft University of Technology, (2) Haga Teaching Hospital)</p>
<p>Unlike the more commonly analyzed ECG or PPG data for activity
classification, heart rate time series data is less detailed, often noisier and
can contain missing data points. Using the BigIdeasLab_STEP dataset, which
includes heart rate time series annotated with specific tasks performed by
individuals, we sought to determine if general classification was achievable.
Our analyses showed that the accuracy is sensitive to the choice of
window/stride size. Moreover, we found variable classification performances
between subjects due to differences in the physical structure of their hearts.
Various techniques were used to minimize this variability. First of all,
normalization proved to be a crucial step and significantly improved the
performance. Secondly, grouping subjects and performing classification inside a
group helped to improve performance and decrease inter-subject variability.
Finally, we show that including handcrafted features as input to a deep
learning (DL) network improves the classification performance further.
Together, these findings indicate that heart rate time series can be utilized
for classification tasks like predicting activity. However, normalization or
grouping techniques need to be chosen carefully to minimize the issue of
subject variability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13293">The Influence of Neural Networks on Hydropower Plant Management in Agriculture: Addressing Challenges and Exploring Untapped Opportunities. (arXiv:2311.13293v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1">C. Coelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1">M. Fernanda P. Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferras_L/0/1/0/all/0/1">L.L. Ferr&#xe1;s</a></p>
<p>Hydropower plants are crucial for stable renewable energy and serve as vital
water sources for sustainable agriculture. However, it is essential to assess
the current water management practices associated with hydropower plant
management software. A key concern is the potential conflict between
electricity generation and agricultural water needs. Prioritising water for
electricity generation can reduce irrigation availability in agriculture during
crucial periods like droughts, impacting crop yields and regional food
security. Coordination between electricity and agricultural water allocation is
necessary to ensure optimal and environmentally sound practices. Neural
networks have become valuable tools for hydropower plant management, but their
black-box nature raises concerns about transparency in decision making.
Additionally, current approaches often do not take advantage of their potential
to create a system that effectively balances water allocation.
</p>
<p>This work is a call for attention and highlights the potential risks of
deploying neural network-based hydropower plant management software without
proper scrutiny and control. To address these concerns, we propose the adoption
of the Agriculture Conscious Hydropower Plant Management framework, aiming to
maximise electricity production while prioritising stable irrigation for
agriculture. We also advocate reevaluating government-imposed minimum water
guidelines for irrigation to ensure flexibility and effective water allocation.
Additionally, we suggest a set of regulatory measures to promote model
transparency and robustness, certifying software that makes conscious and
intelligent water allocation decisions, ultimately safeguarding agriculture
from undue strain during droughts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13294">Probabilistic Inference in Reinforcement Learning Done Right. (arXiv:2311.13294v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tarbouriech_J/0/1/0/all/0/1">Jean Tarbouriech</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1">Tor Lattimore</a>, <a href="http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1">Brendan O&#x27;Donoghue</a></p>
<p>A popular perspective in Reinforcement learning (RL) casts the problem as
probabilistic inference on a graphical model of the Markov decision process
(MDP). The core object of study is the probability of each state-action pair
being visited under the optimal policy. Previous approaches to approximate this
quantity can be arbitrarily poor, leading to algorithms that do not implement
genuine statistical inference and consequently do not perform well in
challenging problems. In this work, we undertake a rigorous Bayesian treatment
of the posterior probability of state-action optimality and clarify how it
flows through the MDP. We first reveal that this quantity can indeed be used to
generate a policy that explores efficiently, as measured by regret.
Unfortunately, computing it is intractable, so we derive a new variational
Bayesian approximation yielding a tractable convex optimization problem and
establish that the resulting policy also explores efficiently. We call our
approach VAPOR and show that it has strong connections to Thompson sampling,
K-learning, and maximum entropy exploration. We conclude with some experiments
demonstrating the performance advantage of a deep RL version of VAPOR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13319">Deep Learning for Vascular Segmentation and Applications in Phase Contrast Tomography Imaging. (arXiv:2311.13319v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yagis_E/0/1/0/all/0/1">Ekin Yagis</a>, <a href="http://arxiv.org/find/eess/1/au:+Aslani_S/0/1/0/all/0/1">Shahab Aslani</a>, <a href="http://arxiv.org/find/eess/1/au:+Jain_Y/0/1/0/all/0/1">Yashvardhan Jain</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yang Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Rahmani_S/0/1/0/all/0/1">Shahrokh Rahmani</a>, <a href="http://arxiv.org/find/eess/1/au:+Brunet_J/0/1/0/all/0/1">Joseph Brunet</a>, <a href="http://arxiv.org/find/eess/1/au:+Bellier_A/0/1/0/all/0/1">Alexandre Bellier</a>, <a href="http://arxiv.org/find/eess/1/au:+Werlein_C/0/1/0/all/0/1">Christopher Werlein</a>, <a href="http://arxiv.org/find/eess/1/au:+Ackermann_M/0/1/0/all/0/1">Maximilian Ackermann</a>, <a href="http://arxiv.org/find/eess/1/au:+Jonigk_D/0/1/0/all/0/1">Danny Jonigk</a>, <a href="http://arxiv.org/find/eess/1/au:+Tafforeau_P/0/1/0/all/0/1">Paul Tafforeau</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_P/0/1/0/all/0/1">Peter D Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Walsh_C/0/1/0/all/0/1">Claire Walsh</a></p>
<p>Automated blood vessel segmentation is vital for biomedical imaging, as
vessel changes indicate many pathologies. Still, precise segmentation is
difficult due to the complexity of vascular structures, anatomical variations
across patients, the scarcity of annotated public datasets, and the quality of
images. We present a thorough literature review, highlighting the state of
machine learning techniques across diverse organs. Our goal is to provide a
foundation on the topic and identify a robust baseline model for application to
vascular segmentation in a new imaging modality, Hierarchical Phase Contrast
Tomography (HiP CT). Introduced in 2020 at the European Synchrotron Radiation
Facility, HiP CT enables 3D imaging of complete organs at an unprecedented
resolution of ca. 20mm per voxel, with the capability for localized zooms in
selected regions down to 1mm per voxel without sectioning. We have created a
training dataset with double annotator validated vascular data from three
kidneys imaged with HiP CT in the context of the Human Organ Atlas Project.
Finally, utilising the nnU Net model, we conduct experiments to assess the
models performance on both familiar and unseen samples, employing vessel
specific metrics. Our results show that while segmentations yielded reasonably
high scores such as clDice values ranging from 0.82 to 0.88, certain errors
persisted. Large vessels that collapsed due to the lack of hydrostatic pressure
(HiP CT is an ex vivo technique) were segmented poorly. Moreover, decreased
connectivity in finer vessels and higher segmentation errors at vessel
boundaries were observed. Such errors obstruct the understanding of the
structures by interrupting vascular tree connectivity. Through our review and
outputs, we aim to set a benchmark for subsequent model evaluations using
various modalities, especially with the HiP CT imaging database.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13321">Revisiting Supervision for Continual Representation Learning. (arXiv:2311.13321v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1">Daniel Marczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1">Sebastian Cygert</a>, <a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1">Bart&#x142;omiej Twardowski</a></p>
<p>In the field of continual learning, models are designed to learn tasks one
after the other. While most research has centered on supervised continual
learning, recent studies have highlighted the strengths of self-supervised
continual representation learning. The improved transferability of
representations built with self-supervised methods is often associated with the
role played by the multi-layer perceptron projector. In this work, we depart
from this observation and reexamine the role of supervision in continual
representation learning. We reckon that additional information, such as human
annotations, should not deteriorate the quality of representations. Our
findings show that supervised models when enhanced with a multi-layer
perceptron head, can outperform self-supervised models in continual
representation learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13326">Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1">Woosung Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1">Insu Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yuntae Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Gimin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woo Chang Kim</a></p>
<p>Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13341">Learning principle and mathematical realization of the learning mechanism in the brain. (arXiv:2311.13341v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katayose_T/0/1/0/all/0/1">Taisuke Katayose</a></p>
<p>While deep learning has achieved remarkable success, there is no clear
explanation about why it works so well. In order to discuss this question
quantitatively, we need a mathematical framework that explains what learning is
in the first place. After several considerations, we succeeded in constructing
a mathematical framework that can provide a unified understanding of all types
of learning, including deep learning and learning in the brain. We call it
learning principle, and it follows that all learning is equivalent to
estimating the probability of input data. We not only derived this principle,
but also mentioned its application to actual machine learning models. For
example, we found that conventional supervised learning is equivalent to
estimating conditional probabilities, and succeeded in making supervised
learning more effective and generalized. We also proposed a new method of
defining the values of estimated probability using differentiation, and showed
that unsupervised learning can be performed on arbitrary dataset without any
prior knowledge. Namely, this method is a general-purpose machine learning in
the true sense. Moreover, we succeeded in describing the learning mechanism in
the brain by considering the time evolution of a fully or partially connected
model and applying this new method. The learning principle provides solutions
to many unsolved problems in deep learning and cognitive neuroscience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13348">MergeSFL: Split Federated Learning with Feature Merging and Batch Size Regulation. (arXiv:2311.13348v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yunming Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongli Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhiwei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1">Chunming Qiao</a></p>
<p>Recently, federated learning (FL) has emerged as a popular technique for edge
AI to mine valuable knowledge in edge computing (EC) systems. To mitigate the
computing/communication burden on resource-constrained workers and protect
model privacy, split federated learning (SFL) has been released by integrating
both data and model parallelism. Despite resource limitations, SFL still faces
two other critical challenges in EC, i.e., statistical heterogeneity and system
heterogeneity. To address these challenges, we propose a novel SFL framework,
termed MergeSFL, by incorporating feature merging and batch size regulation in
SFL. Concretely, feature merging aims to merge the features from workers into a
mixed feature sequence, which is approximately equivalent to the features
derived from IID data and is employed to promote model accuracy. While batch
size regulation aims to assign diverse and suitable batch sizes for
heterogeneous workers to improve training efficiency. Moreover, MergeSFL
explores to jointly optimize these two strategies upon their coupled
relationship to better enhance the performance of SFL. Extensive experiments
are conducted on a physical platform with 80 NVIDIA Jetson edge devices, and
the experimental results show that MergeSFL can improve the final model
accuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared
to the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13349">REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints. (arXiv:2311.13349v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Corti_F/0/1/0/all/0/1">Francesco Corti</a>, <a href="http://arxiv.org/find/cs/1/au:+Maag_B/0/1/0/all/0/1">Balz Maag</a>, <a href="http://arxiv.org/find/cs/1/au:+Schauer_J/0/1/0/all/0/1">Joachim Schauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Pferschy_U/0/1/0/all/0/1">Ulrich Pferschy</a>, <a href="http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1">Olga Saukh</a></p>
<p>Deep models deployed on edge devices frequently encounter resource
variability, which arises from fluctuating energy levels, timing constraints,
or prioritization of other critical tasks within the system. State-of-the-art
machine learning pipelines generate resource-agnostic models, not capable to
adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks
(REDS) to tackle model adaptation to variable resources. In contrast to the
state-of-the-art, REDS use structured sparsity constructively by exploiting
permutation invariance of neurons, which allows for hardware-specific
optimizations. Specifically, REDS achieve computational efficiency by (1)
skipping sequential computational blocks identified by a novel iterative
knapsack optimizer, and (2) leveraging simple math to re-arrange the order of
operations in REDS computational graph to take advantage of the data cache.
REDS support conventional deep networks frequently deployed on the edge and
provide computational benefits even for small and simple networks. We evaluate
REDS on six benchmark architectures trained on the Google Speech Commands,
FMNIST and CIFAR10 datasets, and test on four off-the-shelf mobile and embedded
hardware platforms. We provide a theoretical result and empirical evidence for
REDS outstanding performance in terms of submodels' test set accuracy, and
demonstrate an adaptation time in response to dynamic resource constraints of
under 40$\mu$s, utilizing a 2-layer fully-connected network on Arduino Nano 33
BLE Sense.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13350">Fact-based Court Judgment Prediction. (arXiv:2311.13350v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nigam_S/0/1/0/all/0/1">Shubham Kumar Nigam</a>, <a href="http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1">Aniket Deroy</a></p>
<p>This extended abstract extends the research presented in "ILDC for CJPE:
Indian Legal Documents Corpus for Court Judgment Prediction and Explanation"
\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within
the context of Indian legal documents. We introduce two distinct problem
variations: one based solely on facts, and another combining facts with rulings
from lower courts (RLC). Our research aims to enhance early-phase case outcome
prediction, offering significant benefits to legal professionals and the
general public. The results, however, indicated a performance decline compared
to the original ILDC for CJPE study, even after implementing various weightage
schemes in our DELSumm algorithm. Additionally, using only facts for legal
judgment prediction with different transformer models yielded results inferior
to the state-of-the-art outcomes reported in the "ILDC for CJPE" study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13355">Unified Classification and Rejection: A One-versus-All Framework. (arXiv:2311.13355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhen Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xu-Yao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-Lin Liu</a></p>
<p>Classifying patterns of known classes and rejecting ambiguous and novel (also
called as out-of-distribution (OOD)) inputs are involved in open world pattern
recognition. Deep neural network models usually excel in closed-set
classification while performing poorly in rejecting OOD. To tackle this
problem, numerous methods have been designed to perform open set recognition
(OSR) or OOD rejection/detection tasks. Previous methods mostly take
post-training score transformation or hybrid models to ensure low scores on OOD
inputs while separating known classes. In this paper, we attempt to build a
unified framework for building open set classifiers for both classification and
OOD rejection. We formulate the open set recognition of $ K $-known-class as a
$ (K + 1) $-class classification problem with model trained on known-class
samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all
(OVA) binary classification tasks and binding some parameters, we show that
combining the scores of OVA classifiers can give $ (K + 1) $-class posterior
probabilities, which enables classification and OOD rejection in a unified
framework. To maintain the closed-set classification accuracy of the OVA
trained classifier, we propose a hybrid training strategy combining OVA loss
and multi-class cross-entropy loss. We implement the OVA framework and hybrid
training strategy on the recently proposed convolutional prototype network.
Experiments on popular OSR and OOD detection datasets demonstrate that the
proposed framework, using a single multi-class classifier, yields competitive
performance in closed-set classification, OOD detection, and misclassification
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13374">An Empirical Study of Uncertainty Estimation Techniques for Detecting Drift in Data Streams. (arXiv:2311.13374v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Winter_A/0/1/0/all/0/1">Anton Winter</a>, <a href="http://arxiv.org/find/cs/1/au:+Jourdan_N/0/1/0/all/0/1">Nicolas Jourdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wirth_T/0/1/0/all/0/1">Tristan Wirth</a>, <a href="http://arxiv.org/find/cs/1/au:+Knauthe_V/0/1/0/all/0/1">Volker Knauthe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1">Arjan Kuijper</a></p>
<p>In safety-critical domains such as autonomous driving and medical diagnosis,
the reliability of machine learning models is crucial. One significant
challenge to reliability is concept drift, which can cause model deterioration
over time. Traditionally, drift detectors rely on true labels, which are often
scarce and costly. This study conducts a comprehensive empirical evaluation of
using uncertainty values as substitutes for error rates in detecting drifts,
aiming to alleviate the reliance on labeled post-deployment data. We examine
five uncertainty estimation methods in conjunction with the ADWIN detector
across seven real-world datasets. Our results reveal that while the SWAG method
exhibits superior calibration, the overall accuracy in detecting drifts is not
notably impacted by the choice of uncertainty estimation method, with even the
most basic method demonstrating competitive performance. These findings offer
valuable insights into the practical applicability of uncertainty-based drift
detection in real-world, safety-critical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13381">Confidant: Customizing Transformer-based LLMs via Collaborative Edge Training. (arXiv:2311.13381v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuxuan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qianqian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1">Yuanchao Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shibo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a></p>
<p>Transformer-based large language models (LLMs) have demonstrated impressive
capabilities in a variety of natural language processing (NLP) tasks.
Nonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge
devices with limited computing, memory, and energy budgets. In this paper, we
propose Confidant, a multi-backend collaborative training framework for
customizing state-of-the-art LLMs on commodity mobile devices like smartphones.
Confidant partitions an LLM into several sub-models so that each fits into a
mobile device's memory. A pipeline parallel training mechanism is further
developed to ensure fast and efficient distributed training. In addition, we
propose a novel backend scheduler to allocate different attention heads to
heterogeneous compute hardware, including mobile CPU and GPUs, to maximize the
compute resource utilization on each edge device. Our preliminary experimental
results show that Confidant achieves at most 45.3% memory reduction and 8.03x
inference speedup in practical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13411">Bayesian inference of a new Mallows model for characterising symptom sequences applied in primary progressive aphasia. (arXiv:2311.13411v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taylor_B/0/1/0/all/0/1">Beatrice Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Shand_C/0/1/0/all/0/1">Cameron Shand</a>, <a href="http://arxiv.org/find/cs/1/au:+Hardy_C/0/1/0/all/0/1">Chris J. D. Hardy</a>, <a href="http://arxiv.org/find/cs/1/au:+Oxtoby_N/0/1/0/all/0/1">Neil Oxtoby</a></p>
<p>Machine learning models offer the potential to understand diverse datasets in
a data-driven way, powering insights into individual disease experiences and
ensuring equitable healthcare. In this study, we explore Bayesian inference for
characterising symptom sequences, and the associated modelling challenges. We
adapted the Mallows model to account for partial rankings and right-censored
data, employing custom MCMC fitting. Our evaluation, encompassing synthetic
data and a primary progressive aphasia dataset, highlights the model's efficacy
in revealing mean orderings and estimating ranking variance. This holds the
potential to enhance clinical comprehension of symptom occurrence. However, our
work encounters limitations concerning model scalability and small dataset
sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13414">From Images to Connections: Can DQN with GNNs learn the Strategic Game of Hex?. (arXiv:2311.13414v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1">Yannik Keller</a>, <a href="http://arxiv.org/find/cs/1/au:+Bluml_J/0/1/0/all/0/1">Jannis Bl&#xfc;ml</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_G/0/1/0/all/0/1">Gopika Sudhakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>The gameplay of strategic board games such as chess, Go and Hex is often
characterized by combinatorial, relational structures -- capturing distinct
interactions and non-local patterns -- and not just images. Nonetheless, most
common self-play reinforcement learning (RL) approaches simply approximate
policy and value functions using convolutional neural networks (CNN). A key
feature of CNNs is their relational inductive bias towards locality and
translational invariance. In contrast, graph neural networks (GNN) can encode
more complicated and distinct relational structures. Hence, we investigate the
crucial question: Can GNNs, with their ability to encode complex connections,
replace CNNs in self-play reinforcement learning? To this end, we do a
comparison with Hex -- an abstract yet strategically rich board game -- serving
as our experimental platform. Our findings reveal that GNNs excel at dealing
with long range dependency situations in game states and are less prone to
overfitting, but also showing a reduced proficiency in discerning local
patterns. This suggests a potential paradigm shift, signaling the use of
game-specific structures to reshape self-play reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13431">Extracting individual variable information for their decoupling, direct mutual information and multi-feature Granger causality. (arXiv:2311.13431v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Duda_J/0/1/0/all/0/1">Jarek Duda</a></p>
<p>Working with multiple variables they usually contain difficult to control
complex dependencies. This article proposes extraction of their individual
information, e.g. $\overline{X|Y}$ as random variable containing information
from $X$, but with removed information about $Y$, by using $(x,y)
\leftrightarrow (\bar{x}=\textrm{CDF}_{X|Y=y}(x),y)$ reversible normalization.
One application can be decoupling of individual information of variables:
reversibly transform $(X_1,\ldots,X_n)\leftrightarrow(\tilde{X}_1,\ldots
\tilde{X}_n)$ together containing the same information, but being independent:
$\forall_{i\neq j} \tilde{X}_i\perp \tilde{X}_j, \tilde{X}_i\perp X_j$. It
requires detailed models of complex conditional probability distributions - it
is generally a difficult task, but here can be done through multiple dependency
reducing iterations, using imperfect methods (here HCR: Hierarchical
Correlation Reconstruction). It could be also used for direct mutual
information - evaluating direct information transfer: without use of
intermediate variables. For causality direction there is discussed
multi-feature Granger causality, e.g. to trace various types of individual
information transfers between such decoupled variables, including propagation
time (delay).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13434">Recurrent neural networks and transfer learning for elasto-plasticity in woven composites. (arXiv:2311.13434v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Ghane_E/0/1/0/all/0/1">Ehsan Ghane</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fagerstrom_M/0/1/0/all/0/1">Martin Fagerstr&#xf6;m</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Mirkhalaf_M/0/1/0/all/0/1">Mohsen Mirkhalaf</a></p>
<p>As a surrogate for computationally intensive meso-scale simulation of woven
composites, this article presents Recurrent Neural Network (RNN) models.
Leveraging the power of transfer learning, the initialization challenges and
sparse data issues inherent in cyclic shear strain loads are addressed in the
RNN models. A mean-field model generates a comprehensive data set representing
elasto-plastic behavior. In simulations, arbitrary six-dimensional strain
histories are used to predict stresses under random walking as the source task
and cyclic loading conditions as the target task. Incorporating sub-scale
properties enhances RNN versatility. In order to achieve accurate predictions,
the model uses a grid search method to tune network architecture and
hyper-parameter configurations. The results of this study demonstrate that
transfer learning can be used to effectively adapt the RNN to varying strain
conditions, which establishes its potential as a useful tool for modeling
path-dependent responses in woven composites.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13443">Guided Flows for Generative Modeling and Decision Making. (arXiv:2311.13443v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qinqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">Matt Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaul_N/0/1/0/all/0/1">Neta Shaul</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1">Yaron Lipman</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ricky T. Q. Chen</a></p>
<p>Classifier-free guidance is a key component for improving the performance of
conditional generative models for many downstream tasks. It drastically
improves the quality of samples produced, but has so far only been used for
diffusion models. Flow Matching (FM), an alternative simulation-free approach,
trains Continuous Normalizing Flows (CNFs) based on regressing vector fields.
It remains an open question whether classifier-free guidance can be performed
for Flow Matching models, and to what extent does it improve performance. In
this paper, we explore the usage of Guided Flows for a variety of downstream
applications involving conditional image generation, speech synthesis, and
reinforcement learning. In particular, we are the first to apply flow models to
the offline reinforcement learning setting. We also show that Guided Flows
significantly improves the sample quality in image generation and zero-shot
text-to-speech synthesis, and can make use of drastically low amounts of
computation without affecting the agent's overall performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13445">Transfer Attacks and Defenses for Large Language Models on Coding Tasks. (arXiv:2311.13445v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangal_R/0/1/0/all/0/1">Ravi Mangal</a>, <a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1">Matt Fredrikson</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_L/0/1/0/all/0/1">Limin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1">Corina Pasareanu</a></p>
<p>Modern large language models (LLMs), such as ChatGPT, have demonstrated
impressive capabilities for coding tasks including writing and reasoning about
code. They improve upon previous neural network models of code, such as
code2seq or seq2seq, that already demonstrated competitive results when
performing tasks such as code summarization and identifying code
vulnerabilities. However, these previous code models were shown vulnerable to
adversarial examples, i.e. small syntactic perturbations that do not change the
program's semantics, such as the inclusion of "dead code" through false
conditions or the addition of inconsequential print statements, designed to
"fool" the models. LLMs can also be vulnerable to the same adversarial
perturbations but a detailed study on this concern has been lacking so far. In
this paper we aim to investigate the effect of adversarial perturbations on
coding tasks with LLMs. In particular, we study the transferability of
adversarial examples, generated through white-box attacks on smaller code
models, to LLMs. Furthermore, to make the LLMs more robust against such
adversaries without incurring the cost of retraining, we propose prompt-based
defenses that involve modifying the prompt to include additional information
such as examples of adversarially perturbed code and explicit instructions for
reversing adversarial perturbations. Our experiments show that adversarial
examples obtained with a smaller code model are indeed transferable, weakening
the LLMs' performance. The proposed defenses show promise in improving the
model's resilience, paving the way to more robust defensive solutions for LLMs
in code-related applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13447">Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates. (arXiv:2311.13447v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Menart_M/0/1/0/all/0/1">Michael Menart</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1">Enayat Ullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1">Raman Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1">Raef Bassily</a>, <a href="http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1">Crist&#xf3;bal Guzm&#xe1;n</a></p>
<p>We study private empirical risk minimization (ERM) problem for losses
satisfying the $(\gamma,\kappa)$-Kurdyka-{\L}ojasiewicz (KL) condition. The
Polyak-{\L}ojasiewicz (PL) condition is a special case of this condition when
$\kappa=2$. Specifically, we study this problem under the constraint of $\rho$
zero-concentrated differential privacy (zCDP). When $\kappa\in[1,2]$ and the
loss function is Lipschitz and smooth over a sufficiently large region, we
provide a new algorithm based on variance reduced gradient descent that
achieves the rate
$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ on the
excess empirical risk, where $n$ is the dataset size and $d$ is the dimension.
We further show that this rate is nearly optimal. When $\kappa \geq 2$ and the
loss is instead Lipschitz and weakly convex, we show it is possible to achieve
the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$
with a private implementation of the proximal point method. When the KL
parameters are unknown, we provide a novel modification and analysis of the
noisy gradient descent algorithm and show that this algorithm achieves a rate
of
$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{\frac{2\kappa}{4-\kappa}}\big)$
adaptively, which is nearly optimal when $\kappa = 2$. We further show that,
without assuming the KL condition, the same gradient descent algorithm can
achieve fast convergence to a stationary point when the gradient stays
sufficiently large during the run of the algorithm. Specifically, we show that
this algorithm can approximate stationary points of Lipschitz, smooth (and
possibly nonconvex) objectives with rate as fast as
$\tilde{O}\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)$ and never worse than
$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{1/2}\big)$. The latter
rate matches the best known rate for methods that do not rely on variance
reduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13454">Explaining high-dimensional text classifiers. (arXiv:2311.13454v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Melamed_O/0/1/0/all/0/1">Odelia Melamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1">Rich Caruana</a></p>
<p>Explainability has become a valuable tool in the last few years, helping
humans better understand AI-guided decisions. However, the classic
explainability tools are sometimes quite limited when considering
high-dimensional inputs and neural network classifiers. We present a new
explainability method using theoretically proven high-dimensional properties in
neural network classifiers. We present two usages of it: 1) On the classical
sentiment analysis task for the IMDB reviews dataset, and 2) our
Malware-Detection task for our PowerShell scripts dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13459">The Tempered Hilbert Simplex Distance and Its Application To Non-linear Embeddings of TEMs. (arXiv:2311.13459v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1">Ehsan Amid</a>, <a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1">Frank Nielsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1">Richard Nock</a>, <a href="http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1">Manfred K. Warmuth</a></p>
<p>Tempered Exponential Measures (TEMs) are a parametric generalization of the
exponential family of distributions maximizing the tempered entropy function
among positive measures subject to a probability normalization of their power
densities. Calculus on TEMs relies on a deformed algebra of arithmetic
operators induced by the deformed logarithms used to define the tempered
entropy. In this work, we introduce three different parameterizations of finite
discrete TEMs via Legendre functions of the negative tempered entropy function.
In particular, we establish an isometry between such parameterizations in terms
of a generalization of the Hilbert log cross-ratio simplex distance to a
tempered Hilbert co-simplex distance. Similar to the Hilbert geometry, the
tempered Hilbert distance is characterized as a $t$-symmetrization of the
oriented tempered Funk distance. We motivate our construction by introducing
the notion of $t$-lengths of smooth curves in a tautological Finsler manifold.
We then demonstrate the properties of our generalized structure in different
settings and numerically examine the quality of its differentiable
approximations for optimization in machine learning settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13460">Multi-Objective Bayesian Optimization with Active Preference Learning. (arXiv:2311.13460v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ozaki_R/0/1/0/all/0/1">Ryota Ozaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_K/0/1/0/all/0/1">Kazuki Ishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanzaki_Y/0/1/0/all/0/1">Youhei Kanzaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1">Shinya Suzuki</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeno_S/0/1/0/all/0/1">Shion Takeno</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Karasuyama_M/0/1/0/all/0/1">Masayuki Karasuyama</a></p>
<p>There are a lot of real-world black-box optimization problems that need to
optimize multiple criteria simultaneously. However, in a multi-objective
optimization (MOO) problem, identifying the whole Pareto front requires the
prohibitive search cost, while in many practical scenarios, the decision maker
(DM) only needs a specific solution among the set of the Pareto optimal
solutions. We propose a Bayesian optimization (BO) approach to identifying the
most preferred solution in the MOO with expensive objective functions, in which
a Bayesian preference model of the DM is adaptively estimated by an interactive
manner based on the two types of supervisions called the pairwise preference
and improvement request. To explore the most preferred solution, we define an
acquisition function in which the uncertainty both in the objective functions
and the DM preference is incorporated. Further, to minimize the interaction
cost with the DM, we also propose an active learning strategy for the
preference estimation. We empirically demonstrate the effectiveness of our
proposed method through the benchmark function optimization and the
hyper-parameter optimization problems for machine learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13466">Accelerating Inference in Molecular Diffusion Models with Latent Representations of Protein Structure. (arXiv:2311.13466v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Dunn_I/0/1/0/all/0/1">Ian Dunn</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Koes_D/0/1/0/all/0/1">David Ryan Koes</a></p>
<p>Diffusion generative models have emerged as a powerful framework for
addressing problems in structural biology and structure-based drug design.
These models operate directly on 3D molecular structures. Due to the
unfavorable scaling of graph neural networks (GNNs) with graph size as well as
the relatively slow inference speeds inherent to diffusion models, many
existing molecular diffusion models rely on coarse-grained representations of
protein structure to make training and inference feasible. However, such
coarse-grained representations discard essential information for modeling
molecular interactions and impair the quality of generated structures. In this
work, we present a novel GNN-based architecture for learning latent
representations of molecular structure. When trained end-to-end with a
diffusion model for de novo ligand design, our model achieves comparable
performance to one with an all-atom protein representation while exhibiting a
3-fold reduction in inference time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13469">Span-Based Optimal Sample Complexity for Average Reward MDPs. (arXiv:2311.13469v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zurek_M/0/1/0/all/0/1">Matthew Zurek</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yudong Chen</a></p>
<p>We study the sample complexity of learning an $\varepsilon$-optimal policy in
an average-reward Markov decision process (MDP) under a generative model. We
establish the complexity bound $\widetilde{O}\left(SA\frac{H}{\varepsilon^2}
\right)$, where $H$ is the span of the bias function of the optimal policy and
$SA$ is the cardinality of the state-action space. Our result is the first that
is minimax optimal (up to log factors) in all parameters $S,A,H$ and
$\varepsilon$, improving on existing work that either assumes uniformly bounded
mixing times for all policies or has suboptimal dependence on the parameters.
</p>
<p>Our result is based on reducing the average-reward MDP to a discounted MDP.
To establish the optimality of this reduction, we develop improved bounds for
$\gamma$-discounted MDPs, showing that
$\widetilde{O}\left(SA\frac{H}{(1-\gamma)^2\varepsilon^2} \right)$ samples
suffice to learn a $\varepsilon$-optimal policy in weakly communicating MDPs
under the regime that $\gamma \geq 1 - \frac{1}{H}$, circumventing the
well-known lower bound of
$\widetilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\varepsilon^2} \right)$ for
general $\gamma$-discounted MDPs. Our analysis develops upper bounds on certain
instance-dependent variance parameters in terms of the span parameter. These
bounds are tighter than those based on the mixing time or diameter of the MDP
and may be of broader use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13471">Comparative Analysis of Linear Regression, Gaussian Elimination, and LU Decomposition for CT Real Estate Purchase Decisions. (arXiv:2311.13471v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xilin Cheng</a></p>
<p>This paper presents a comprehensive evaluation of three distinct
computational algorithms applied to the decision-making process of real estate
purchases. Specifically, we analyze the efficacy of Linear Regression from
Scikit-learn library, Gaussian Elimination with partial pivoting, and LU
Decomposition in predicting the advisability of buying a house in the State of
Connecticut based on a set of financial and market-related parameters. The
algorithms' performances were compared using a dataset encompassing
town-specific details, yearly data, interest rates, and median sale ratios. Our
results demonstrate significant differences in predictive accuracy, with Linear
Regression and LU Decomposition providing the most reliable recommendations and
Gaussian Elimination showing limitations in stability and performance. The
study's findings emphasize the importance of algorithm selection in predictive
analytic and offer insights into the practical applications of computational
methods in real estate investment strategies. By evaluating model efficacy
through metrics such as R-squared scores and Mean Squared Error, we provide a
nuanced understanding of each method's strengths and weaknesses, contributing
valuable knowledge to the fields of real estate analysis and predictive
modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13475">Machine Translation to Control Formality Features in the Target Language. (arXiv:2311.13475v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1">Harshita Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_P/0/1/0/all/0/1">Prashasta Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyowon Lee</a></p>
<p>Formality plays a significant role in language communication, especially in
low-resource languages such as Hindi, Japanese and Korean. These languages
utilise formal and informal expressions to convey messages based on social
contexts and relationships. When a language translation technique is used to
translate from a source language that does not pertain the formality (e.g.
English) to a target language that does, there is a missing information on
formality that could be a challenge in producing an accurate outcome. This
research explores how this issue should be resolved when machine learning
methods are used to translate from English to languages with formality, using
Hindi as the example data. This was done by training a bilingual model in a
formality-controlled setting and comparing its performance with a pre-trained
multilingual model in a similar setting. Since there are not a lot of training
data with ground truth, automated annotation techniques were employed to
increase the data size. The primary modeling approach involved leveraging
transformer models, which have demonstrated effectiveness in various natural
language processing tasks. We evaluate the official formality accuracy(ACC) by
comparing the predicted masked tokens with the ground truth. This metric
provides a quantitative measure of how well the translations align with the
desired outputs. Our study showcases a versatile translation strategy that
considers the nuances of formality in the target language, catering to diverse
language communication needs and scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13485">Deep-learning-based acceleration of MRI for radiotherapy planning of pediatric patients with brain tumors. (arXiv:2311.13485v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Alam_S/0/1/0/all/0/1">Shahinur Alam</a>, <a href="http://arxiv.org/find/eess/1/au:+Uh_J/0/1/0/all/0/1">Jinsoo Uh</a>, <a href="http://arxiv.org/find/eess/1/au:+Dresner_A/0/1/0/all/0/1">Alexander Dresner</a>, <a href="http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1">Chia-ho Hua</a>, <a href="http://arxiv.org/find/eess/1/au:+Khairy_K/0/1/0/all/0/1">Khaled Khairy</a></p>
<p>Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic and
radiotherapy (RT) planning tool, offering detailed insights into the anatomy of
the human body. The extensive scan time is stressful for patients, who must
remain motionless in a prolonged imaging procedure that prioritizes reduction
of imaging artifacts. This is challenging for pediatric patients who may
require measures for managing voluntary motions such as anesthesia. Several
computational approaches reduce scan time (fast MRI), by recording fewer
measurements and digitally recovering full information via post-acquisition
reconstruction. However, most fast MRI approaches were developed for diagnostic
imaging, without addressing reconstruction challenges specific to RT planning.
In this work, we developed a deep learning-based method (DeepMRIRec) for MRI
reconstruction from undersampled data acquired with RT-specific receiver coil
arrangements. We evaluated our method against fully sampled data of T1-weighted
MR images acquired from 73 children with brain tumors/surgical beds using loop
and posterior coils (12 channels), with and without applying virtual
compression of coil elements. DeepMRIRec reduced scanning time by a factor of
four producing a structural similarity score surpassing the evaluated
state-of-the-art method (0.960 vs 0.896), thereby demonstrating its potential
for accelerating MRI scanning for RT planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13490">Benchmarking Toxic Molecule Classification using Graph Neural Networks and Few Shot Learning. (arXiv:2311.13490v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Mehta_B/0/1/0/all/0/1">Bhavya Mehta</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kothari_K/0/1/0/all/0/1">Kush Kothari</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nambiar_R/0/1/0/all/0/1">Reshmika Nambiar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shrawne_S/0/1/0/all/0/1">Seema Shrawne</a></p>
<p>Traditional methods like Graph Convolutional Networks (GCNs) face challenges
with limited data and class imbalance, leading to suboptimal performance in
graph classification tasks during toxicity prediction of molecules as a whole.
To address these issues, we harness the power of Graph Isomorphic Networks,
Multi Headed Attention and Free Large-scale Adversarial Augmentation separately
on Graphs for precisely capturing the structural data of molecules and their
toxicological properties. Additionally, we incorporate Few-Shot Learning to
improve the model's generalization with limited annotated samples. Extensive
experiments on a diverse toxicology dataset demonstrate that our method
achieves an impressive state-of-art AUC-ROC value of 0.816, surpassing the
baseline GCN model by 11.4%. This highlights the significance of our proposed
methodology and Few Shot Learning in advancing Toxic Molecular Classification,
with the potential to enhance drug discovery and environmental risk assessment
processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13491">Grad-Shafranov equilibria via data-free physics informed neural networks. (arXiv:2311.13491v1 [physics.plasm-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Jang_B/0/1/0/all/0/1">Byoungchan Jang</a>, <a href="http://arxiv.org/find/physics/1/au:+Kaptanoglu_A/0/1/0/all/0/1">Alan A. Kaptanoglu</a>, <a href="http://arxiv.org/find/physics/1/au:+Gaur_R/0/1/0/all/0/1">Rahul Gaur</a>, <a href="http://arxiv.org/find/physics/1/au:+Pan_S/0/1/0/all/0/1">Shaw Pan</a>, <a href="http://arxiv.org/find/physics/1/au:+Landreman_M/0/1/0/all/0/1">Matt Landreman</a>, <a href="http://arxiv.org/find/physics/1/au:+Dorland_W/0/1/0/all/0/1">William Dorland</a></p>
<p>A large number of magnetohydrodynamic (MHD) equilibrium calculations are
often required for uncertainty quantification, optimization, and real-time
diagnostic information, making MHD equilibrium codes vital to the field of
plasma physics. In this paper, we explore a method for solving the
Grad-Shafranov equation by using Physics-Informed Neural Networks (PINNs). For
PINNs, we optimize neural networks by directly minimizing the residual of the
PDE as a loss function. We show that PINNs can accurately and effectively solve
the Grad-Shafranov equation with several different boundary conditions. We also
explore the parameter space by varying the size of the model, the learning
rate, and boundary conditions to map various trade-offs such as between
reconstruction error and computational speed. Additionally, we introduce a
parameterized PINN framework, expanding the input space to include variables
such as pressure, aspect ratio, elongation, and triangularity in order to
handle a broader range of plasma scenarios within a single network.
Parametrized PINNs could be used in future work to solve inverse problems such
as shape optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13495">Current Topological and Machine Learning Applications for Bias Detection in Text. (arXiv:2311.13495v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farrelly_C/0/1/0/all/0/1">Colleen Farrelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_Y/0/1/0/all/0/1">Yashbir Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hathaway_Q/0/1/0/all/0/1">Quincy A. Hathaway</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlsson_G/0/1/0/all/0/1">Gunnar Carlsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1">Ashok Choudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_R/0/1/0/all/0/1">Rahul Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1">Gianfranco Doretto</a>, <a href="http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1">Yassine Himeur</a>, <a href="http://arxiv.org/find/cs/1/au:+Atalls_S/0/1/0/all/0/1">Shadi Atalls</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1">Wathiq Mansoor</a></p>
<p>Institutional bias can impact patient outcomes, educational attainment, and
legal system navigation. Written records often reflect bias, and once bias is
identified; it is possible to refer individuals for training to reduce bias.
Many machine learning tools exist to explore text data and create predictive
models that can search written records to identify real-time bias. However, few
previous studies investigate large language model embeddings and geometric
models of biased text data to understand geometry's impact on bias modeling
accuracy. To overcome this issue, this study utilizes the RedditBias database
to analyze textual biases. Four transformer models, including BERT and RoBERTa
variants, were explored. Post-embedding, t-SNE allowed two-dimensional
visualization of data. KNN classifiers differentiated bias types, with lower
k-values proving more effective. Findings suggest BERT, particularly mini BERT,
excels in bias classification, while multilingual models lag. The
recommendation emphasizes refining monolingual models and exploring
domain-specific biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13502">Bitformer: An efficient Transformer with bitwise operation-based attention for Big Data Analytics at low-cost low-precision devices. (arXiv:2311.13502v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_G/0/1/0/all/0/1">Gaoxiang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junkai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoying Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yongxin Zhu</a></p>
<p>In the current landscape of large models, the Transformer stands as a
cornerstone, playing a pivotal role in shaping the trajectory of modern models.
However, its application encounters challenges attributed to the substantial
computational intricacies intrinsic to its attention mechanism. Moreover, its
reliance on high-precision floating-point operations presents specific hurdles,
particularly evident in computation-intensive scenarios such as edge computing
environments. These environments, characterized by resource-constrained devices
and a preference for lower precision, necessitate innovative solutions.
</p>
<p>To tackle the exacting data processing demands posed by edge devices, we
introduce the Bitformer model, an inventive extension of the Transformer
paradigm. Central to this innovation is a novel attention mechanism that
adeptly replaces conventional floating-point matrix multiplication with bitwise
operations. This strategic substitution yields dual advantages. Not only does
it maintain the attention mechanism's prowess in capturing intricate long-range
information dependencies, but it also orchestrates a profound reduction in the
computational complexity inherent in the attention operation. The transition
from an $O(n^2d)$ complexity, typical of floating-point operations, to an
$O(n^2T)$ complexity characterizing bitwise operations, substantiates this
advantage. Notably, in this context, the parameter $T$ remains markedly smaller
than the conventional dimensionality parameter $d$.
</p>
<p>The Bitformer model in essence endeavors to reconcile the indomitable
requirements of modern computing landscapes with the constraints posed by edge
computing scenarios. By forging this innovative path, we bridge the gap between
high-performing models and resource-scarce environments, thus unveiling a
promising trajectory for further advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13507">Applying Dimensionality Reduction as Precursor to LSTM-CNN Models for Classifying Imagery and Motor Signals in ECoG-Based BCIs. (arXiv:2311.13507v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bafana_S/0/1/0/all/0/1">Soham Bafana</a></p>
<p>Motor impairments, frequently caused by neurological incidents like strokes
or traumatic brain injuries, present substantial obstacles in rehabilitation
therapy. This research aims to elevate the field by optimizing motor imagery
classification algorithms within Brain-Computer Interfaces (BCIs). By improving
the efficiency of BCIs, we offer a novel approach that holds significant
promise for enhancing motor rehabilitation outcomes. Utilizing unsupervised
techniques for dimensionality reduction, namely Uniform Manifold Approximation
and Projection (UMAP) coupled with K-Nearest Neighbors (KNN), we evaluate the
necessity of employing supervised methods such as Long Short-Term Memory (LSTM)
and Convolutional Neural Networks (CNNs) for classification tasks. Importantly,
participants who exhibited high KNN scores following UMAP dimensionality
reduction also achieved high accuracy in supervised deep learning (DL) models.
Due to individualized model requirements and massive neural training data,
dimensionality reduction becomes an effective preprocessing step that minimizes
the need for extensive data labeling and supervised deep learning techniques.
This approach has significant implications not only for targeted therapies in
motor dysfunction but also for addressing regulatory, safety, and reliability
concerns in the rapidly evolving BCI field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13508">Naturalness of Attention: Revisiting Attention in Code Language Models. (arXiv:2311.13508v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1">Mootez Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_T/0/1/0/all/0/1">Tushar Sharma</a></p>
<p>Language models for code such as CodeBERT offer the capability to learn
advanced source code representation, but their opacity poses barriers to
understanding of captured properties. Recent attention analysis studies provide
initial interpretability insights by focusing solely on attention weights
rather than considering the wider context modeling of Transformers. This study
aims to shed some light on the previously ignored factors of the attention
mechanism beyond the attention weights. We conduct an initial empirical study
analyzing both attention distributions and transformed representations in
CodeBERT. Across two programming languages, Java and Python, we find that the
scaled transformation norms of the input better capture syntactic structure
compared to attention weights alone. Our analysis reveals characterization of
how CodeBERT embeds syntactic code properties. The findings demonstrate the
importance of incorporating factors beyond just attention weights for
rigorously understanding neural code models. This lays the groundwork for
developing more interpretable models and effective uses of attention mechanisms
in program analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13531">Leveraging CNNs and Ensemble Learning for Automated Disaster Image Classification. (arXiv:2311.13531v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rathod_A/0/1/0/all/0/1">Archit Rathod</a>, <a href="http://arxiv.org/find/cs/1/au:+Pariawala_V/0/1/0/all/0/1">Veer Pariawala</a>, <a href="http://arxiv.org/find/cs/1/au:+Surana_M/0/1/0/all/0/1">Mokshit Surana</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_K/0/1/0/all/0/1">Kumkum Saxena</a></p>
<p>Natural disasters act as a serious threat globally, requiring effective and
efficient disaster management and recovery. This paper focuses on classifying
natural disaster images using Convolutional Neural Networks (CNNs). Multiple
CNN architectures were built and trained on a dataset containing images of
earthquakes, floods, wildfires, and volcanoes. A stacked CNN ensemble approach
proved to be the most effective, achieving 95% accuracy and an F1 score going
up to 0.96 for individual classes. Tuning hyperparameters of individual models
for optimization was critical to maximize the models' performance. The stacking
of CNNs with XGBoost acting as the meta-model utilizes the strengths of the CNN
and ResNet models to improve the overall accuracy of the classification.
Results obtained from the models illustrated the potency of CNN-based models
for automated disaster image classification. This lays the foundation for
expanding these techniques to build robust systems for disaster response,
damage assessment, and recovery management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13538">Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhicheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yinya Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jing Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jing Tang</a></p>
<p>Existing work has found that the prompt engineering heavily influences the
performance of large language models (LLMs). Chain-of-thought (CoT), as a
popular prompt engineering technique, prompted LLMs using in-context examples
with reasoning steps. In current studies, the few-shot examples of CoT are
generally handcrafted by humans. However, how the text style of in-context
examples influence the outputs of LLMs still remains under-explored. This paper
presents a novel and effective approach, named \textbf{AlignCoT}, to improve
the reasoning capability of LLMs by aligning the in-context examples with the
native style of LLMs. ``Native'' refers to the inherent characteristic style of
LLMs which can be probed by original zero-shot scenarios. AlignCoT is
orthogonal to other prompt engineering methods, making it easy to combine with
state-of-the-art techniques to further improve the LLMs' performance. We
conduct extensive and comprehensive experiments on several benchmarks. The
empirical results demonstrate that our AlignCoTsignificantly improves
performance over the carefully handcrafted in-context examples. For instance,
with GPT-3.5-turbo, we observed a +2.5\% improvement on GSM8K. Furthermore, our
AlignCoT consistently improve the performance when combined with other
state-of-the-art prompt engineering methods. The source code and dataset will
be available at
\href{https://github.com/yangzhch6/AlignCoT}{https://github.com/yangzhch6/AlignCoT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13539">Learned Nonlinear Predictor for Critically Sampled 3D Point Cloud Attribute Compression. (arXiv:2311.13539v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Do_T/0/1/0/all/0/1">Tam Thuc Do</a>, <a href="http://arxiv.org/find/eess/1/au:+Chou_P/0/1/0/all/0/1">Philip A. Chou</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheung_G/0/1/0/all/0/1">Gene Cheung</a></p>
<p>We study 3D point cloud attribute compression via a volumetric approach:
assuming point cloud geometry is known at both encoder and decoder, parameters
$\theta$ of a continuous attribute function $f: \mathbb{R}^3 \mapsto
\mathbb{R}$ are quantized to $\hat{\theta}$ and encoded, so that discrete
samples $f_{\hat{\theta}}(\mathbf{x}_i)$ can be recovered at known 3D points
$\mathbf{x}_i \in \mathbb{R}^3$ at the decoder. Specifically, we consider a
nested sequences of function subspaces $\mathcal{F}^{(p)}_{l_0} \subseteq
\cdots \subseteq \mathcal{F}^{(p)}_L$, where $\mathcal{F}_l^{(p)}$ is a family
of functions spanned by B-spline basis functions of order $p$, $f_l^*$ is the
projection of $f$ on $\mathcal{F}_l^{(p)}$ and encoded as low-pass coefficients
$F_l^*$, and $g_l^*$ is the residual function in orthogonal subspace
$\mathcal{G}_l^{(p)}$ (where $\mathcal{G}_l^{(p)} \oplus \mathcal{F}_l^{(p)} =
\mathcal{F}_{l+1}^{(p)}$) and encoded as high-pass coefficients $G_l^*$. In
this paper, to improve coding performance over [1], we study predicting
$f_{l+1}^*$ at level $l+1$ given $f_l^*$ at level $l$ and encoding of $G_l^*$
for the $p=1$ case (RAHT($1$)). For the prediction, we formalize RAHT(1) linear
prediction in MPEG-PCC in a theoretical framework, and propose a new nonlinear
predictor using a polynomial of bilateral filter. We derive equations to
efficiently compute the critically sampled high-pass coefficients $G_l^*$
amenable to encoding. We optimize parameters in our resulting feed-forward
network on a large training set of point clouds by minimizing a rate-distortion
Lagrangian. Experimental results show that our improved framework outperformed
the MPEG G-PCC predictor by $11$ to $12\%$ in bit rate reduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13541">Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nahshan_Y/0/1/0/all/0/1">Yury Nahshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampeas_J/0/1/0/all/0/1">Joseph Kampeas</a>, <a href="http://arxiv.org/find/cs/1/au:+Haleva_E/0/1/0/all/0/1">Emir Haleva</a></p>
<p>Transformer models have achieved remarkable results in a wide range of
applications. However, their scalability is hampered by the quadratic time and
memory complexity of the self-attention mechanism concerning the sequence
length. This limitation poses a substantial obstacle when dealing with long
documents or high-resolution images. In this work, we study the self-attention
mechanism by analyzing the distribution of the attention matrix and its
concentration ability. Furthermore, we propose instruments to measure these
quantities and introduce a novel self-attention mechanism, Linear Log-Normal
Attention, designed to emulate the distribution and concentration behavior of
the original self-attention. Our experimental results on popular natural
language benchmarks reveal that our proposed Linear Log-Normal Attention
outperforms other linearized attention alternatives, offering a promising
avenue for enhancing the scalability of transformer models. Our code is
available in supplementary materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13548">Efficient Numerical Integration in Reproducing Kernel Hilbert Spaces via Leverage Scores Sampling. (arXiv:2311.13548v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chatalic_A/0/1/0/all/0/1">Antoine Chatalic</a>, <a href="http://arxiv.org/find/stat/1/au:+Schreuder_N/0/1/0/all/0/1">Nicolas Schreuder</a>, <a href="http://arxiv.org/find/stat/1/au:+Vito_E/0/1/0/all/0/1">Ernesto De Vito</a>, <a href="http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1">Lorenzo Rosasco</a></p>
<p>In this work we consider the problem of numerical integration, i.e.,
approximating integrals with respect to a target probability measure using only
pointwise evaluations of the integrand. We focus on the setting in which the
target distribution is only accessible through a set of $n$ i.i.d.
observations, and the integrand belongs to a reproducing kernel Hilbert space.
We propose an efficient procedure which exploits a small i.i.d. random subset
of $m&lt;n$ samples drawn either uniformly or using approximate leverage scores
from the initial observations. Our main result is an upper bound on the
approximation error of this procedure for both sampling strategies. It yields
sufficient conditions on the subsample size to recover the standard (optimal)
$n^{-1/2}$ rate while reducing drastically the number of functions evaluations,
and thus the overall computational cost. Moreover, we obtain rates with respect
to the number $m$ of evaluations of the integrand which adapt to its
smoothness, and match known optimal rates for instance for Sobolev spaces. We
illustrate our theoretical findings with numerical experiments on real
datasets, which highlight the attractive efficiency-accuracy tradeoff of our
method compared to existing randomized and greedy quadrature methods. We note
that, the problem of numerical integration in RKHS amounts to designing a
discrete approximation of the kernel mean embedding of the target distribution.
As a consequence, direct applications of our results also include the efficient
computation of maximum mean discrepancies between distributions and the design
of efficient kernel-based tests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13552">A Unified Framework for Trace-induced Quantum Kernels. (arXiv:2311.13552v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Gan_B/0/1/0/all/0/1">Beng Yee Gan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Leykam_D/0/1/0/all/0/1">Daniel Leykam</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Thanasilp_S/0/1/0/all/0/1">Supanut Thanasilp</a></p>
<p>Quantum kernel methods are promising candidates for achieving a practical
quantum advantage for certain machine learning tasks. Similar to classical
machine learning, an exact form of a quantum kernel is expected to have a great
impact on the model performance. In this work we combine all trace-induced
quantum kernels, including the commonly-used global fidelity and local
projected quantum kernels, into a common framework. We show how generalized
trace-induced quantum kernels can be constructed as combinations of the
fundamental building blocks we coin "Lego" kernels, which impose an inductive
bias on the resulting quantum models. We relate the expressive power and
generalization ability to the number of non-zero weight Lego kernels and
propose a systematic approach to increase the complexity of a quantum kernel
model, leading to a new form of the local projected kernels that require fewer
quantum resources in terms of the number of quantum gates and measurement
shots. We show numerically that models based on local projected kernels can
achieve comparable performance to the global fidelity quantum kernel. Our work
unifies existing quantum kernels and provides a systematic framework to compare
their properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13569">Combinatorial Optimization with Policy Adaptation using Latent Space Search. (arXiv:2311.13569v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chalumeau_F/0/1/0/all/0/1">Felix Chalumeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Surana_S/0/1/0/all/0/1">Shikha Surana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonnet_C/0/1/0/all/0/1">Clement Bonnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1">Nathan Grinsztajn</a>, <a href="http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1">Arnu Pretorius</a>, <a href="http://arxiv.org/find/cs/1/au:+Laterre_A/0/1/0/all/0/1">Alexandre Laterre</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1">Thomas D. Barrett</a></p>
<p>Combinatorial Optimization underpins many real-world applications and yet,
designing performant algorithms to solve these complex, typically NP-hard,
problems remains a significant research challenge. Reinforcement Learning (RL)
provides a versatile framework for designing heuristics across a broad spectrum
of problem domains. However, despite notable progress, RL has not yet
supplanted industrial solvers as the go-to solution. Current approaches
emphasize pre-training heuristics that construct solutions but often rely on
search procedures with limited variance, such as stochastically sampling
numerous solutions from a single policy or employing computationally expensive
fine-tuning of the policy on individual problem instances. Building on the
intuition that performant search at inference time should be anticipated during
pre-training, we propose COMPASS, a novel RL approach that parameterizes a
distribution of diverse and specialized policies conditioned on a continuous
latent space. We evaluate COMPASS across three canonical problems - Travelling
Salesman, Capacitated Vehicle Routing, and Job-Shop Scheduling - and
demonstrate that our search strategy (i) outperforms state-of-the-art
approaches on 11 standard benchmarking tasks and (ii) generalizes better,
surpassing all other approaches on a set of 18 procedurally transformed
instance distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13571">Ball Mill Fault Prediction Based on Deep Convolutional Auto-Encoding Network. (arXiv:2311.13571v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1">Xinkun Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yonggang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xinwu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peilong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">LiYe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">JanFeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yuan Pan</a></p>
<p>Ball mills play a critical role in modern mining operations, making their
bearing failures a significant concern due to the potential loss of production
efficiency and economic consequences. This paper presents an anomaly detection
method based on Deep Convolutional Auto-encoding Neural Networks (DCAN) for
addressing the issue of ball mill bearing fault detection. The proposed
approach leverages vibration data collected during normal operation for
training, overcoming challenges such as labeling issues and data imbalance
often encountered in supervised learning methods. DCAN includes the modules of
convolutional feature extraction and transposed convolutional feature
reconstruction, demonstrating exceptional capabilities in signal processing and
feature extraction. Additionally, the paper describes the practical deployment
of the DCAN-based anomaly detection model for bearing fault detection,
utilizing data from the ball mill bearings of Wuhan Iron &amp; Steel Resources
Group and fault data from NASA's bearing vibration dataset. Experimental
results validate the DCAN model's reliability in recognizing fault vibration
patterns. This method holds promise for enhancing bearing fault detection
efficiency, reducing production interruptions, and lowering maintenance costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13580">$\sigma$-PCA: a unified neural model for linear and nonlinear principal component analysis. (arXiv:2311.13580v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1">Fahdi Kanavati</a>, <a href="http://arxiv.org/find/cs/1/au:+Katsnith_L/0/1/0/all/0/1">Lucy Katsnith</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsuneki_M/0/1/0/all/0/1">Masayuki Tsuneki</a></p>
<p>Linear principal component analysis (PCA), nonlinear PCA, and linear
independent component analysis (ICA) -- those are three methods with
single-layer autoencoder formulations for learning linear transformations from
data. Linear PCA learns orthogonal transformations (rotations) that orient axes
to maximise variance, but it suffers from a subspace rotational indeterminacy:
it fails to find a unique rotation for axes that share the same variance. Both
nonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational
to permutational by maximising statistical independence under the assumption of
unit variance. The main difference between them is that nonlinear PCA only
learns rotations while linear ICA learns not just rotations but any linear
transformation with unit variance. The relationship between all three can be
understood by the singular value decomposition of the linear ICA transformation
into a sequence of rotation, scale, rotation. Linear PCA learns the first
rotation; nonlinear PCA learns the second. The scale is simply the inverse of
the standard deviations. The problem is that, in contrast to linear PCA,
conventional nonlinear PCA cannot be used directly on the data to learn the
first rotation, the first being special as it reduces dimensionality and orders
by variances. In this paper, we have identified the cause, and as a solution we
propose $\sigma$-PCA: a unified neural model for linear and nonlinear PCA as
single-layer autoencoders. One of its key ingredients: modelling not just the
rotation but also the scale -- the variances. This model bridges the disparity
between linear and nonlinear PCA. And so, like linear PCA, it can learn a
semi-orthogonal transformation that reduces dimensionality and orders by
variances, but, unlike linear PCA, it does not suffer from rotational
indeterminacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13583">Adaptive Sampling for Deep Learning via Efficient Nonparametric Proxies. (arXiv:2311.13583v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1">Shabnam Daghaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1">Benjamin Coleman</a>, <a href="http://arxiv.org/find/cs/1/au:+Geordie_B/0/1/0/all/0/1">Benito Geordie</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Anshumali Shrivastava</a></p>
<p>Data sampling is an effective method to improve the training speed of neural
networks, with recent results demonstrating that it can even break the neural
scaling laws. These results critically rely on high-quality scores to estimate
the importance of an input to the network. We observe that there are two
dominant strategies: static sampling, where the scores are determined before
training, and dynamic sampling, where the scores can depend on the model
weights. Static algorithms are computationally inexpensive but less effective
than their dynamic counterparts, which can cause end-to-end slowdown due to
their need to explicitly compute losses. To address this problem, we propose a
novel sampling distribution based on nonparametric kernel regression that
learns an effective importance score as the neural network trains. However,
nonparametric regression models are too computationally expensive to accelerate
end-to-end training. Therefore, we develop an efficient sketch-based
approximation to the Nadaraya-Watson estimator. Using recent techniques from
high-dimensional statistics and randomized algorithms, we prove that our
Nadaraya-Watson sketch approximates the estimator with exponential convergence
guarantees. Our sampling algorithm outperforms the baseline in terms of
wall-clock time and accuracy on four datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13584">On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates. (arXiv:2311.13584v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bruno_S/0/1/0/all/0/1">Stefano Bruno</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1">Dong-Young Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Akyildiz_O/0/1/0/all/0/1">&#xd6;mer Deniz Akyildiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabanis_S/0/1/0/all/0/1">Sotirios Sabanis</a></p>
<p>We provide full theoretical guarantees for the convergence behaviour of
diffusion-based generative models under the assumption of strongly logconcave
data distributions while our approximating class of functions used for score
estimation is made of Lipschitz continuous functions. We demonstrate via a
motivating example, sampling from a Gaussian distribution with unknown mean,
the powerfulness of our approach. In this case, explicit estimates are provided
for the associated optimization problem, i.e. score approximation, while these
are combined with the corresponding sampling estimates. As a result, we obtain
the best known upper bound estimates in terms of key quantities of interest,
such as the dimension and rates of convergence, for the Wasserstein-2 distance
between the data distribution (Gaussian with unknown mean) and our sampling
algorithm.
</p>
<p>Beyond the motivating example and in order to allow for the use of a diverse
range of stochastic optimizers, we present our results using an $L^2$-accurate
score estimation assumption, which crucially is formed under an expectation
with respect to the stochastic optimizer and our novel auxiliary process that
uses only known information. This approach yields the best known convergence
rate for our sampling algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13587">A Survey of Serverless Machine Learning Model Inference. (arXiv:2311.13587v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kojs_K/0/1/0/all/0/1">Kamil Kojs</a></p>
<p>Recent developments in Generative AI, Computer Vision, and Natural Language
Processing have led to an increased integration of AI models into various
products. This widespread adoption of AI requires significant efforts in
deploying these models in production environments. When hosting machine
learning models for real-time predictions, it is important to meet defined
Service Level Objectives (SLOs), ensuring reliability, minimal downtime, and
optimizing operational costs of the underlying infrastructure. Large machine
learning models often demand GPU resources for efficient inference to meet
SLOs. In the context of these trends, there is growing interest in hosting AI
models in a serverless architecture while still providing GPU access for
inference tasks. This survey aims to summarize and categorize the emerging
challenges and optimization opportunities for large-scale deep learning serving
systems. By providing a novel taxonomy and summarizing recent trends, we hope
that this survey could shed light on new optimization perspectives and motivate
novel works in large-scale deep learning serving systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13589">Risk-sensitive Markov Decision Process and Learning under General Utility Functions. (arXiv:2311.13589v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhengqi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Renyuan Xu</a></p>
<p>Reinforcement Learning (RL) has gained substantial attention across diverse
application domains and theoretical investigations. Existing literature on RL
theory largely focuses on risk-neutral settings where the decision-maker learns
to maximize the expected cumulative reward. However, in practical scenarios
such as portfolio management and e-commerce recommendations, decision-makers
often persist in heterogeneous risk preferences subject to outcome
uncertainties, which can not be well-captured by the risk-neural framework.
Incorporating these preferences can be approached through utility theory, yet
the development of risk-sensitive RL under general utility functions remains an
open question for theoretical exploration.
</p>
<p>In this paper, we consider a scenario where the decision-maker seeks to
optimize a general utility function of the cumulative reward in the framework
of a Markov decision process (MDP). To facilitate the Dynamic Programming
Principle and Bellman equation, we enlarge the state space with an additional
dimension that accounts for the cumulative reward. We propose a discretized
approximation scheme to the MDP under enlarged state space, which is tractable
and key for algorithmic design. We then propose a modified value iteration
algorithm that employs an epsilon-covering over the space of cumulative reward.
When a simulator is accessible, our algorithm efficiently learns a near-optimal
policy with guaranteed sample complexity. In the absence of a simulator, our
algorithm, designed with an upper-confidence-bound exploration approach,
identifies a near-optimal policy while ensuring a guaranteed regret bound. For
both algorithms, we match the theoretical lower bounds for the risk-neutral
setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13594">Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kopf_L/0/1/0/all/0/1">Laura Kopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1">Shinichi Nakajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1">Marius Kloft</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>Deep Neural Networks (DNNs) demonstrated remarkable capabilities in learning
complex hierarchical data representations, but the nature of these
representations remains largely unknown. Existing global explainability
methods, such as Network Dissection, face limitations such as reliance on
segmentation masks, lack of statistical significance testing, and high
computational demands. We propose Inverse Recognition (INVERT), a scalable
approach for connecting learned representations with human-understandable
concepts by leveraging their capacity to discriminate between these concepts.
In contrast to prior work, INVERT is capable of handling diverse types of
neurons, exhibits less computational complexity, and does not rely on the
availability of segmentation masks. Moreover, INVERT provides an interpretable
metric assessing the alignment between the representation and its corresponding
explanation and delivering a measure of statistical significance, emphasizing
its utility and credibility. We demonstrate the applicability of INVERT in
various scenarios, including the identification of representations affected by
spurious correlations, and the interpretation of the hierarchical structure of
decision-making within the models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13595">Covariance alignment: from maximum likelihood estimation to Gromov-Wasserstein. (arXiv:2311.13595v1 [math.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Han_Y/0/1/0/all/0/1">Yanjun Han</a>, <a href="http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1">Philippe Rigollet</a>, <a href="http://arxiv.org/find/math/1/au:+Stepaniants_G/0/1/0/all/0/1">George Stepaniants</a></p>
<p>Feature alignment methods are used in many scientific disciplines for data
pooling, annotation, and comparison. As an instance of a permutation learning
problem, feature alignment presents significant statistical and computational
challenges. In this work, we propose the covariance alignment model to study
and compare various alignment methods and establish a minimax lower bound for
covariance alignment that has a non-standard dimension scaling because of the
presence of a nuisance parameter. This lower bound is in fact minimax optimal
and is achieved by a natural quasi MLE. However, this estimator involves a
search over all permutations which is computationally infeasible even when the
problem has moderate size. To overcome this limitation, we show that the
celebrated Gromov-Wasserstein algorithm from optimal transport which is more
amenable to fast implementation even on large-scale problems is also minimax
optimal. These results give the first statistical justification for the
deployment of the Gromov-Wasserstein algorithm in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13600">ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs. (arXiv:2311.13600v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1">Viraj Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1">Nataniel Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1">Forrester Cole</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1">Erika Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1">Svetlana Lazebnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1">Varun Jampani</a></p>
<p>Methods for finetuning generative models for concept-driven personalization
generally achieve strong results for subject-driven or style-driven generation.
Recently, low-rank adaptations (LoRA) have been proposed as a
parameter-efficient way of achieving concept-driven personalization. While
recent work explores the combination of separate LoRAs to achieve joint
generation of learned styles and subjects, existing techniques do not reliably
address the problem; they often compromise either subject fidelity or style
fidelity. We propose ZipLoRA, a method to cheaply and effectively merge
independently trained style and subject LoRAs in order to achieve generation of
any user-provided subject in any user-provided style. Experiments on a wide
range of subject and style combinations show that ZipLoRA can generate
compelling results with meaningful improvements over baselines in subject and
style fidelity while preserving the ability to recontextualize. Project page:
https://ziplora.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13601">Visual In-Context Prompting. (arXiv:2311.13601v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1">Qing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1">Tianhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shilong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huaizhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>In-context prompting in large language models (LLMs) has become a prevalent
approach to improve zero-shot capabilities, but this idea is less explored in
the vision domain. Existing visual prompting methods focus on referring
segmentation to segment the most relevant object, falling short of addressing
many generic vision tasks like open-set segmentation and detection. In this
paper, we introduce a universal visual in-context prompting framework for both
tasks. In particular, we build on top of an encoder-decoder architecture, and
develop a versatile prompt encoder to support a variety of prompts like
strokes, boxes, and points. We further enhance it to take an arbitrary number
of reference image segments as the context. Our extensive explorations show
that the proposed visual in-context prompting elicits extraordinary referring
and generic segmentation capabilities to refer and detect, yielding competitive
performance to close-set in-domain datasets and showing promising results on
many open-set segmentation datasets. By joint training on COCO and SA-1B, our
model achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be
available at https://github.com/UX-Decoder/DINOv.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2006.09365">Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing. (arXiv:2006.09365v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1">Sai Praneeth Karimireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1">Martin Jaggi</a></p>
<p>In Byzantine robust distributed or federated learning, a central server wants
to train a machine learning model over data distributed across multiple
workers. However, a fraction of these workers may deviate from the prescribed
algorithm and send arbitrary messages. While this problem has received
significant attention recently, most current defenses assume that the workers
have identical data. For realistic cases when the data across workers are
heterogeneous (non-iid), we design new attacks which circumvent current
defenses, leading to significant loss of performance. We then propose a simple
bucketing scheme that adapts existing robust algorithms to heterogeneous
datasets at a negligible computational cost. We also theoretically and
experimentally validate our approach, showing that combining bucketing with
existing robust algorithms is effective against challenging attacks. Our work
is the first to establish guaranteed convergence for the non-iid Byzantine
robust problem under realistic assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.00527">Gates Are Not What You Need in RNNs. (arXiv:2108.00527v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zakovskis_R/0/1/0/all/0/1">Ronalds Zakovskis</a>, <a href="http://arxiv.org/find/cs/1/au:+Draguns_A/0/1/0/all/0/1">Andis Draguns</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaile_E/0/1/0/all/0/1">Eliza Gaile</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozolins_E/0/1/0/all/0/1">Emils Ozolins</a>, <a href="http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1">Karlis Freivalds</a></p>
<p>Recurrent neural networks have flourished in many areas. Consequently, we can
see new RNN cells being developed continuously, usually by creating or using
gates in a new, original way. But what if we told you that gates in RNNs are
redundant? In this paper, we propose a new recurrent cell called Residual
Recurrent Unit (RRU) which beats traditional cells and does not employ a single
gate. It is based on the residual shortcut connection, linear transformations,
ReLU, and normalization. To evaluate our cell's effectiveness, we compare its
performance against the widely-used GRU and LSTM cells and the recently
proposed Mogrifier LSTM on several tasks including, polyphonic music modeling,
language modeling, and sentiment analysis. Our experiments show that RRU
outperforms the traditional gated units on most of these tasks. Also, it has
better robustness to parameter selection, allowing immediate application in new
tasks without much tuning. We have implemented the RRU in TensorFlow, and the
code is made available at https://github.com/LUMII-Syslab/RRU .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.08494">Learning continuous models for continuous physics. (arXiv:2202.08494v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishnapriyan_A/0/1/0/all/0/1">Aditi S. Krishnapriyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Queiruga_A/0/1/0/all/0/1">Alejandro F. Queiruga</a>, <a href="http://arxiv.org/find/cs/1/au:+Erichson_N/0/1/0/all/0/1">N. Benjamin Erichson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1">Michael W. Mahoney</a></p>
<p>Dynamical systems that evolve continuously over time are ubiquitous
throughout science and engineering. Machine learning (ML) provides data-driven
approaches to model and predict the dynamics of such systems. A core issue with
this approach is that ML models are typically trained on discrete data, using
ML methodologies that are not aware of underlying continuity properties. This
results in models that often do not capture any underlying continuous dynamics
-- either of the system of interest, or indeed of any related system. To
address this challenge, we develop a convergence test based on numerical
analysis theory. Our test verifies whether a model has learned a function that
accurately approximates an underlying continuous dynamics. Models that fail
this test fail to capture relevant dynamics, rendering them of limited utility
for many scientific prediction tasks; while models that pass this test enable
both better interpolation and better extrapolation in multiple ways. Our
results illustrate how principled numerical analysis methods can be coupled
with existing ML training/testing methodologies to validate models for science
and engineering applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10209">Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy. (arXiv:2202.10209v5 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1">Seira Hidano</a>, <a href="http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1">Takao Murakami</a></p>
<p>Differentially private GNNs (Graph Neural Networks) have been recently
studied to provide high accuracy in various tasks on graph data while strongly
protecting user privacy. In particular, a recent study proposes an algorithm to
protect each user's feature vector in an attributed graph with LDP (Local
Differential Privacy), a strong privacy notion without a trusted third party.
However, this algorithm does not protect edges (friendships) in a social graph,
hence cannot protect user privacy in unattributed graphs. How to provide strong
privacy with high accuracy in unattributed graphs remains open.
</p>
<p>In this paper, we propose a novel LDP algorithm called the DPRR
(Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our
DPRR preserves each user's degree hence a graph structure while providing edge
LDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic
edge sampling, where each user's sampling probability is automatically tuned
using the Laplacian mechanism to preserve the degree information under edge
LDP. We also propose a privacy budget allocation method to make the noise in
both Warner's RR and the Laplacian mechanism small. We focus on graph
classification as a task of GNNs and evaluate the DPRR using three social graph
datasets. Our experimental results show that the DPRR significantly outperforms
three baselines and provides accuracy close to a non-private algorithm in all
datasets with a reasonable privacy budget, e.g., epsilon=1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.06450">The effect of speech pathology on automatic speaker verification -- a large-scale study. (arXiv:2204.06450v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1">Soroosh Tayebi Arasteh</a>, <a href="http://arxiv.org/find/cs/1/au:+Weise_T/0/1/0/all/0/1">Tobias Weise</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuster_M/0/1/0/all/0/1">Maria Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Noeth_E/0/1/0/all/0/1">Elmar Noeth</a>, <a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seung Hee Yang</a></p>
<p>Navigating the challenges of data-driven speech processing, one of the
primary hurdles is accessing reliable pathological speech data. While public
datasets appear to offer solutions, they come with inherent risks of potential
unintended exposure of patient health information via re-identification
attacks. Using a comprehensive real-world pathological speech corpus, with over
n=3,800 test subjects spanning various age groups and speech disorders, we
employed a deep-learning-driven automatic speaker verification (ASV) approach.
This resulted in a notable mean equal error rate (EER) of 0.89% with a standard
deviation of 0.06%, outstripping traditional benchmarks. Our comprehensive
assessments demonstrate that pathological speech overall faces heightened
privacy breach risks compared to healthy speech. Specifically, adults with
dysphonia are at heightened re-identification risks, whereas conditions like
dysarthria yield results comparable to those of healthy speakers. Crucially,
speech intelligibility does not influence the ASV system's performance metrics.
In pediatric cases, particularly those with cleft lip and palate, the recording
environment plays a decisive role in re-identification. Merging data across
pathological types led to a marked EER decrease, suggesting the potential
benefits of pathological diversity in ASV, accompanied by a logarithmic boost
in ASV effectiveness. In essence, this research sheds light on the dynamics
between pathological speech and speaker verification, emphasizing its crucial
role in safeguarding patient confidentiality in our increasingly digitized
healthcare era.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.02645">Discovering stochastic dynamical equations from biological time series data. (arXiv:2205.02645v4 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Nabeel_A/0/1/0/all/0/1">Arshed Nabeel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Karichannavar_A/0/1/0/all/0/1">Ashwin Karichannavar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Palathingal_S/0/1/0/all/0/1">Shuaib Palathingal</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jhawar_J/0/1/0/all/0/1">Jitesh Jhawar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bruckner_D/0/1/0/all/0/1">David B. Br&#xfc;ckner</a>, <a href="http://arxiv.org/find/q-bio/1/au:+M%2E_D/0/1/0/all/0/1">Danny Raj M.</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Guttal_V/0/1/0/all/0/1">Vishwesha Guttal</a></p>
<p>Stochastic differential equations (SDEs) are an important framework to model
dynamics with randomness, as is common in most biological systems. The inverse
problem of integrating these models with empirical data remains a major
challenge. Here, we present a software package, PyDaDDy (Python Library for
Data Driven Dynamics) that takes time series data as an input and outputs an
interpretable SDE. We achieve this by combining traditional approaches from
stochastic calculus literature with state-of-the-art equation discovery
techniques. We validate our approach on synthetic datasets, and demonstrate the
generality and applicability of the method on two real-world datasets of vastly
different spatiotemporal scales: (i) collective movement of fish school where
stochasticity plays a crucial role, and (ii) confined migration of a single
cell, primarily following a relaxed oscillation. We make the method available
as an easy-to-use, open-source Python package, PyDaddy (Python Library for Data
Driven Dynamics).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05077">Tensor Train for Global Optimization Problems in Robotics. (arXiv:2206.05077v5 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1">Suhan Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Lembono_T/0/1/0/all/0/1">Teguh Lembono</a>, <a href="http://arxiv.org/find/cs/1/au:+Loew_T/0/1/0/all/0/1">Tobias Loew</a>, <a href="http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1">Sylvain Calinon</a></p>
<p>The convergence of many numerical optimization techniques is highly dependent
on the initial guess given to the solver. To address this issue, we propose a
novel approach that utilizes tensor methods to initialize existing optimization
solvers near global optima. Our method does not require access to a database of
good solutions. We first transform the cost function, which depends on both
task parameters and optimization variables, into a probability density
function. Unlike existing approaches, the joint probability distribution of the
task parameters and optimization variables is approximated using the Tensor
Train model, which enables efficient conditioning and sampling. We treat the
task parameters as random variables, and for a given task, we generate samples
for decision variables from the conditional distribution to initialize the
optimization solver. Our method can produce multiple solutions (when they
exist) faster than existing methods. We first evaluate the approach on
benchmark functions for numerical optimization that are hard to solve using
gradient-based optimization solvers with a naive initialization. The results
show that the proposed method can generate samples close to global optima and
from multiple modes. We then demonstrate the generality and relevance of our
framework to robotics by applying it to inverse kinematics with obstacles and
motion planning problems with a 7-DoF manipulator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.12261">GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1">Guoqing Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhigang Zeng</a></p>
<p>Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.13933">Using Taylor-Approximated Gradients to Improve the Frank-Wolfe Method for Empirical Risk Minimization. (arXiv:2208.13933v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zikai Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Freund_R/0/1/0/all/0/1">Robert M. Freund</a></p>
<p>The Frank-Wolfe method has become increasingly useful in statistical and
machine learning applications, due to the structure-inducing properties of the
iterates, and especially in settings where linear minimization over the
feasible set is more computationally efficient than projection. In the setting
of Empirical Risk Minimization -- one of the fundamental optimization problems
in statistical and machine learning -- the computational effectiveness of
Frank-Wolfe methods typically grows linearly in the number of data observations
$n$. This is in stark contrast to the case for typical stochastic projection
methods. In order to reduce this dependence on $n$, we look to second-order
smoothness of typical smooth loss functions (least squares loss and logistic
loss, for example) and we propose amending the Frank-Wolfe method with Taylor
series-approximated gradients, including variants for both deterministic and
stochastic settings. Compared with current state-of-the-art methods in the
regime where the optimality tolerance $\varepsilon$ is sufficiently small, our
methods are able to simultaneously reduce the dependence on large $n$ while
obtaining optimal convergence rates of Frank-Wolfe methods, in both the convex
and non-convex settings. We also propose a novel adaptive step-size approach
for which we have computational guarantees. Last of all, we present
computational experiments which show that our methods exhibit very significant
speed-ups over existing methods on real-world datasets for both convex and
non-convex binary classification problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14605">Looking at the posterior: accuracy and uncertainty of neural-network predictions. (arXiv:2211.14605v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1">H. Linander</a>, <a href="http://arxiv.org/find/cs/1/au:+Balabanov_O/0/1/0/all/0/1">O. Balabanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">H. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehlig_B/0/1/0/all/0/1">B. Mehlig</a></p>
<p>Bayesian inference can quantify uncertainty in the predictions of neural
networks using posterior distributions for model parameters and network output.
By looking at these posterior distributions, one can separate the origin of
uncertainty into aleatoric and epistemic contributions. One goal of uncertainty
quantification is to inform on prediction accuracy. Here we show that
prediction accuracy depends on both epistemic and aleatoric uncertainty in an
intricate fashion that cannot be understood in terms of marginalized
uncertainty distributions alone. How the accuracy relates to epistemic and
aleatoric uncertainties depends not only on the model architecture, but also on
the properties of the dataset. We discuss the significance of these results for
active learning and introduce a novel acquisition function that outperforms
common uncertainty-based methods. To arrive at our results, we approximated the
posteriors using deep ensembles, for fully-connected, convolutional and
attention-based neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02931">Leveraging Different Learning Styles for Improved Knowledge Distillation in Biomedical Imaging. (arXiv:2212.02931v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niyaz_U/0/1/0/all/0/1">Usma Niyaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sambyal_A/0/1/0/all/0/1">Abhishek Singh Sambyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1">Deepti R. Bathula</a></p>
<p>Learning style refers to a type of training mechanism adopted by an
individual to gain new knowledge. As suggested by the VARK model, humans have
different learning preferences, like Visual (V), Auditory (A), Read/Write (R),
and Kinesthetic (K), for acquiring and effectively processing information. Our
work endeavors to leverage this concept of knowledge diversification to improve
the performance of model compression techniques like Knowledge Distillation
(KD) and Mutual Learning (ML). Consequently, we use a single-teacher and
two-student network in a unified framework that not only allows for the
transfer of knowledge from teacher to students (KD) but also encourages
collaborative learning between students (ML). Unlike the conventional approach,
where the teacher shares the same knowledge in the form of predictions or
feature representations with the student network, our proposed approach employs
a more diversified strategy by training one student with predictions and the
other with feature maps from the teacher. We further extend this knowledge
diversification by facilitating the exchange of predictions and feature maps
between the two student networks, enriching their learning experiences. We have
conducted comprehensive experiments with three benchmark datasets for both
classification and segmentation tasks using two different network architecture
combinations. These experimental results demonstrate that knowledge
diversification in a combined KD and ML framework outperforms conventional KD
or ML techniques (with similar network configuration) that only use predictions
with an average improvement of 2%. Furthermore, consistent improvement in
performance across different tasks, with various network architectures, and
over state-of-the-art techniques establishes the robustness and
generalizability of the proposed model
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08131">Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies. (arXiv:2212.08131v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sujit_S/0/1/0/all/0/1">Shivakanth Sujit</a>, <a href="http://arxiv.org/find/cs/1/au:+Braga_P/0/1/0/all/0/1">Pedro H. M. Braga</a>, <a href="http://arxiv.org/find/cs/1/au:+Bornschein_J/0/1/0/all/0/1">Jorg Bornschein</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1">Samira Ebrahimi Kahou</a></p>
<p>Reinforcement learning (RL) has shown great promise with algorithms learning
in environments with large state and action spaces purely from scalar reward
signals. A crucial challenge for current deep RL algorithms is that they
require a tremendous amount of environment interactions for learning. This can
be infeasible in situations where such interactions are expensive; such as in
robotics. Offline RL algorithms try to address this issue by bootstrapping the
learning process from existing logged data without needing to interact with the
environment from the very beginning. While online RL algorithms are typically
evaluated as a function of the number of environment interactions, there exists
no single established protocol for evaluating offline RL methods.In this paper,
we propose a sequential approach to evaluate offline RL algorithms as a
function of the training set size and thus by their data efficiency. Sequential
evaluation provides valuable insights into the data efficiency of the learning
process and the robustness of algorithms to distribution changes in the dataset
while also harmonizing the visualization of the offline and online learning
phases. Our approach is generally applicable and easy to implement. We compare
several existing offline RL algorithms using this approach and present insights
from a variety of tasks and offline datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05326">Scalable Real-Time Recurrent Learning Using Columnar-Constructive Networks. (arXiv:2302.05326v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javed_K/0/1/0/all/0/1">Khurram Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1">Haseeb Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1">Rich Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Martha White</a></p>
<p>Constructing states from sequences of observations is an important component
of reinforcement learning agents. One solution for state construction is to use
recurrent neural networks. Back-propagation through time (BPTT), and real-time
recurrent learning (RTRL) are two popular gradient-based methods for recurrent
learning. BPTT requires complete trajectories of observations before it can
compute the gradients and is unsuitable for online updates. RTRL can do online
updates but scales poorly to large networks. In this paper, we propose two
constraints that make RTRL scalable. We show that by either decomposing the
network into independent modules or learning the network in stages, we can make
RTRL scale linearly with the number of parameters. Unlike prior scalable
gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms
do not add noise or bias to the gradient estimate. Instead, they trade off the
functional capacity of the network for computationally efficient learning. We
demonstrate the effectiveness of our approach over Truncated-BPTT on a
prediction benchmark inspired by animal learning and by doing policy evaluation
of pre-trained policies for Atari 2600 games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11381">Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes. (arXiv:2302.11381v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Johnson_E/0/1/0/all/0/1">Emmeran Johnson</a>, <a href="http://arxiv.org/find/math/1/au:+Pike_Burke_C/0/1/0/all/0/1">Ciara Pike-Burke</a>, <a href="http://arxiv.org/find/math/1/au:+Rebeschini_P/0/1/0/all/0/1">Patrick Rebeschini</a></p>
<p>Policy Mirror Descent (PMD) is a general family of algorithms that covers a
wide range of novel and fundamental methods in reinforcement learning.
Motivated by the instability of policy iteration (PI) with inexact policy
evaluation, PMD algorithmically regularises the policy improvement step of PI.
With exact policy evaluation, PI is known to converge linearly with a rate
given by the discount factor $\gamma$ of a Markov Decision Process. In this
work, we bridge the gap between PI and PMD with exact policy evaluation and
show that the dimension-free $\gamma$-rate of PI can be achieved by the general
family of unregularised PMD algorithms under an adaptive step-size. We show
that both the rate and step-size are unimprovable for PMD: we provide matching
lower bounds that demonstrate that the $\gamma$-rate is optimal for PMD methods
as well as PI, and that the adaptive step-size is necessary for PMD to achieve
it. Our work is the first to relate PMD to rate-optimality and step-size
necessity. Our study of the convergence of PMD avoids the use of the
performance difference lemma, which leads to a direct analysis of independent
interest. We also extend the analysis to the inexact setting and establish the
first dimension-optimal sample complexity for unregularised PMD under a
generative model, improving upon the best-known result.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01959">Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taehoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bohyung Han</a></p>
<p>We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07647">LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision. (arXiv:2304.07647v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiani Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_M/0/1/0/all/0/1">Mayur Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08837">Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers. (arXiv:2304.08837v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Cao_J/0/1/0/all/0/1">John Cao</a>, <a href="http://arxiv.org/find/math/1/au:+Niazi_M/0/1/0/all/0/1">Muhammad Umar B. Niazi</a>, <a href="http://arxiv.org/find/math/1/au:+Barreau_M/0/1/0/all/0/1">Matthieu Barreau</a>, <a href="http://arxiv.org/find/math/1/au:+Johansson_K/0/1/0/all/0/1">Karl Henrik Johansson</a></p>
<p>This paper presents a novel observer-based approach to detect and isolate
faulty sensors in nonlinear systems. The proposed sensor fault detection and
isolation (s-FDI) method applies to a general class of nonlinear systems. Our
focus is on s-FDI for two types of faults: complete failure and sensor
degradation. The key aspect of this approach lies in the utilization of a
neural network-based Kazantzis-Kravaris/Luenberger (KKL) observer. The neural
network is trained to learn the dynamics of the observer, enabling accurate
output predictions of the system. Sensor faults are detected by comparing the
actual output measurements with the predicted values. If the difference
surpasses a theoretical threshold, a sensor fault is detected. To identify and
isolate which sensor is faulty, we compare the numerical difference of each
sensor meassurement with an empirically derived threshold. We derive both
theoretical and empirical thresholds for detection and isolation, respectively.
Notably, the proposed approach is robust to measurement noise and system
uncertainties. Its effectiveness is demonstrated through numerical simulations
of sensor faults in a network of Kuramoto oscillators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13318">A principled deep learning approach for geological facies generation. (arXiv:2305.13318v2 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Bhavsar_F/0/1/0/all/0/1">Ferdinand Bhavsar</a>, <a href="http://arxiv.org/find/physics/1/au:+Desassis_N/0/1/0/all/0/1">Nicolas Desassis</a>, <a href="http://arxiv.org/find/physics/1/au:+Ors_F/0/1/0/all/0/1">Fabien Ors</a>, <a href="http://arxiv.org/find/physics/1/au:+Romary_T/0/1/0/all/0/1">Thomas Romary</a></p>
<p>The simulation of geological facies in an unobservable volume is essential in
various geoscience applications. Given the complexity of the problem, deep
generative learning is a promising approach to overcome the limitations of
traditional geostatistical simulation models, in particular their lack of
physical realism. This research aims to investigate the application of
generative adversarial networks and deep variational inference for
conditionally simulating meandering channels in underground volumes. In this
paper, we review the generative deep learning approaches, in particular the
adversarial ones and the stabilization techniques that aim to facilitate their
training. The proposed approach is tested on 2D and 3D simulations generated by
the stochastic process-based model Flumy. Morphological metrics are utilized to
compare our proposed method with earlier iterations of generative adversarial
networks. The results indicate that by utilizing recent stabilization
techniques, generative adversarial networks can efficiently sample from target
data distributions. Moreover, we demonstrate the ability to simulate
conditioned simulations through the latent variable model property of the
proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13536">Subspace-Configurable Networks. (arXiv:2305.13536v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1">Olga Saukh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaoxi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1">Lothar Thiele</a></p>
<p>While the deployment of deep learning models on edge devices is increasing,
these models often lack robustness when faced with dynamic changes in sensed
data. This can be attributed to sensor drift, or variations in the data
compared to what was used during offline training due to factors such as
specific sensor placement or naturally changing sensing conditions. Hence,
achieving the desired robustness necessitates the utilization of either an
invariant architecture or specialized training approaches, like data
augmentation. Alternatively, input transformations can be treated as a domain
shift problem, and solved by post-deployment model adaptation. In this paper,
we train a parameterized subspace of configurable networks, where an optimal
network for a particular parameter setting is part of this subspace. The
obtained subspace is low-dimensional and has a surprisingly simple structure
even for complex, non-invertible transformations of the input, leading to an
exceptionally high efficiency of subspace-configurable networks (SCNs) when
limited storage and computing resources are at stake. We evaluate SCNs on a
wide range of standard datasets, architectures, and transformations, and
demonstrate their power on resource-constrained IoT devices, where they can
take up to 2.4 times less RAM and be 7.6 times faster at inference time than a
model that achieves the same test set accuracy, yet is trained with data
augmentations to cover the desired range of input transformations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14032">Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v4 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bae_S/0/1/0/all/0/1">Sangmin Bae</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">June-Woo Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Cho_W/0/1/0/all/0/1">Won-Yang Cho</a>, <a href="http://arxiv.org/find/eess/1/au:+Baek_H/0/1/0/all/0/1">Hyerim Baek</a>, <a href="http://arxiv.org/find/eess/1/au:+Son_S/0/1/0/all/0/1">Soyoun Son</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_B/0/1/0/all/0/1">Byungjo Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Ha_C/0/1/0/all/0/1">Changwan Ha</a>, <a href="http://arxiv.org/find/eess/1/au:+Tae_K/0/1/0/all/0/1">Kyongpil Tae</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Sungnyun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Respiratory sound contains crucial information for the early diagnosis of
fatal lung diseases. Since the COVID-19 pandemic, there has been a growing
interest in contact-free medical care based on electronic stethoscopes. To this
end, cutting-edge deep learning models have been developed to diagnose lung
diseases; however, it is still challenging due to the scarcity of medical data.
In this study, we demonstrate that the pretrained model on large-scale visual
and audio datasets can be generalized to the respiratory sound classification
task. In addition, we introduce a straightforward Patch-Mix augmentation, which
randomly mixes patches between different samples, with Audio Spectrogram
Transformer (AST). We further propose a novel and effective Patch-Mix
Contrastive Learning to distinguish the mixed representations in the latent
space. Our method achieves state-of-the-art performance on the ICBHI dataset,
outperforming the prior leading score by an improvement of 4.08%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15851">On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v3 [stat.CO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bardenet_R/0/1/0/all/0/1">R&#xe9;mi Bardenet</a>, <a href="http://arxiv.org/find/stat/1/au:+Fanuel_M/0/1/0/all/0/1">Micha&#xeb;l Fanuel</a>, <a href="http://arxiv.org/find/stat/1/au:+Feller_A/0/1/0/all/0/1">Alexandre Feller</a></p>
<p>DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since
then, they have been widely used as models and subsampling tools in statistics
and computer science. Most applications require sampling from a DPP, and given
their quantum origin, it is natural to wonder whether sampling a DPP on a
quantum computer is easier than on a classical one. We focus here on DPPs over
a finite state space, which are distributions over the subsets of
$\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla
sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and
$\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank
of the kernel matrix. A large first part of the current paper consists in
explaining why the state-of-the-art in quantum simulation of fermionic systems
already yields quantum DPP sampling algorithms. We then modify existing quantum
circuits, and discuss their insertion in a full DPP sampling pipeline that
starts from practical kernel specifications. The bottom line is that, with $P$
(classical) parallel processors, we can divide the preprocessing cost by $P$
and build a quantum circuit with $\mathcal{O}(Nr)$ gates that sample a given
DPP, with depth varying from $\mathcal{O}(N)$ to $\mathcal{O}(r\log N)$
depending on qubit-communication constraints on the target machine. We also
connect existing work on the simulation of superconductors to Pfaffian point
processes, which generalize DPPs and would be a natural addition to the machine
learner's toolbox. In particular, we describe "projective" Pfaffian point
processes, the cardinality of which has constant parity, almost surely.
Finally, the circuits are empirically validated on a classical simulator and on
5-qubit IBM machines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16213">ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. (arXiv:2305.16213v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yikai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1">Fan Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chongxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16854">Channel and Gradient-Importance Aware Device Scheduling for Over-the-Air Federated Learning. (arXiv:2305.16854v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuchang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+lin_Z/0/1/0/all/0/1">Zehong lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuyi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Shi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a></p>
<p>Federated learning (FL) is a popular privacy-preserving distributed training
scheme, where multiple devices collaborate to train machine learning models by
uploading local model updates. To improve communication efficiency,
over-the-air computation (AirComp) has been applied to FL, which leverages
analog modulation to harness the superposition property of radio waves such
that numerous devices can upload their model updates concurrently for
aggregation. However, the uplink channel noise incurs considerable model
aggregation distortion, which is critically determined by the device scheduling
and compromises the learned model performance. In this paper, we propose a
probabilistic device scheduling framework for over-the-air FL, named PO-FL, to
mitigate the negative impact of channel noise, where each device is scheduled
according to a certain probability and its model update is reweighted using
this probability in aggregation. We prove the unbiasedness of this aggregation
scheme and demonstrate the convergence of PO-FL on both convex and non-convex
loss functions. Our convergence bounds unveil that the device scheduling
affects the learning performance through the communication distortion and
global update variance. Based on the convergence analysis, we further develop a
channel and gradient-importance aware algorithm to optimize the device
scheduling probabilities in PO-FL. Extensive simulation results show that the
proposed PO-FL framework with channel and gradient-importance awareness
achieves faster convergence and produces better models than baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00560">Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification. (arXiv:2306.00560v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Ziliang Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jonnarth_A/0/1/0/all/0/1">Arvi Jonnarth</a>, <a href="http://arxiv.org/find/cs/1/au:+Eldesokey_A/0/1/0/all/0/1">Abdelrahman Eldesokey</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnander_J/0/1/0/all/0/1">Joakim Johnander</a>, <a href="http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1">Bastian Wandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Forssen_P/0/1/0/all/0/1">Per-Erik Forssen</a></p>
<p>Computer vision systems that are deployed in safety-critical applications
need to quantify their output uncertainty. We study regression from images to
parameter values and here it is common to detect uncertainty by predicting
probability distributions. In this context, we investigate the
regression-by-classification paradigm which can represent multimodal
distributions, without a prior assumption on the number of modes. Through
experiments on a specifically designed synthetic dataset, we demonstrate that
traditional loss functions lead to poor probability distribution estimates and
severe overconfidence, in the absence of full ground truth distributions. In
order to alleviate these issues, we propose hinge-Wasserstein -- a simple
improvement of the Wasserstein loss that reduces the penalty for weak secondary
modes during training. This enables prediction of complex distributions with
multiple modes, and allows training on datasets where full ground truth
distributions are not available. In extensive experiments, we show that the
proposed loss leads to substantially better uncertainty estimation on two
challenging computer vision tasks: horizon line detection and stereo disparity
estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04225">Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kinfu_K/0/1/0/all/0/1">Kaleab A. Kinfu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1">Rene Vidal</a></p>
<p>While Convolutional Neural Networks (CNNs) have been widely successful in 2D
human pose estimation, Vision Transformers (ViTs) have emerged as a promising
alternative to CNNs, boosting state-of-the-art performance. However, the
quadratic computational complexity of ViTs has limited their applicability for
processing high-resolution images. In this paper, we propose three methods for
reducing ViT's computational complexity, which are based on selecting and
processing a small number of most informative patches while disregarding
others. The first two methods leverage a lightweight pose estimation network to
guide the patch selection process, while the third method utilizes a set of
learnable joint tokens to ensure that the selected patches contain the most
important information about body joints. Experiments across six benchmarks show
that our proposed methods achieve a significant reduction in computational
complexity, ranging from 30% to 44%, with only a minimal drop in accuracy
between 0% and 3.5%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04889">ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qimin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhiqin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a></p>
<p>We present ShaDDR, an example-based deep generative neural network which
produces a high-resolution textured 3D shape through geometry detailization and
conditional texture generation applied to an input coarse voxel shape. Trained
on a small set of detailed and textured exemplar shapes, our method learns to
detailize the geometry via multi-resolution voxel upsampling and generate
textures on voxel surfaces via differentiable rendering against exemplar
texture images from a few views. The generation is interactive, taking less
than 1 second to produce a 3D model with voxel resolutions up to 512^3. The
generated shape preserves the overall structure of the input coarse voxel
model, while the style of the generated geometric details and textures can be
manipulated through learned latent codes. In the experiments, we show that our
method can generate higher-resolution shapes with plausible and improved
geometric details and clean textures compared to prior works. Furthermore, we
showcase the ability of our method to learn geometric details and textures from
shapes reconstructed from real-world photos. In addition, we have developed an
interactive modeling application to demonstrate the generalizability of our
method to various user inputs and the controllability it offers, allowing users
to interactively sculpt a coarse voxel shape to define the overall structure of
the detailized 3D shape. Code and data are available at
https://github.com/qiminchen/ShaDDR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06202">NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1">Anwar Said</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayrak_R/0/1/0/all/0/1">Roza G. Bayrak</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabbir_M/0/1/0/all/0/1">Mudassir Shabbir</a>, <a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1">Daniel Moyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Catie Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsoukos_X/0/1/0/all/0/1">Xenofon Koutsoukos</a></p>
<p>Machine learning provides a valuable tool for analyzing high-dimensional
functional neuroimaging data, and is proving effective in predicting various
neurological conditions, psychiatric disorders, and cognitive patterns. In
functional magnetic resonance imaging (MRI) research, interactions between
brain regions are commonly modeled using graph-based representations. The
potency of graph machine learning methods has been established across myriad
domains, marking a transformative step in data interpretation and predictive
modeling. Yet, despite their promise, the transposition of these techniques to
the neuroimaging domain has been challenging due to the expansive number of
potential preprocessing pipelines and the large parameter search space for
graph-based dataset construction. In this paper, we introduce NeuroGraph, a
collection of graph-based neuroimaging datasets, and demonstrated its utility
for predicting multiple categories of behavioral and cognitive traits. We delve
deeply into the dataset generation search space by crafting 35 datasets that
encompass static and dynamic brain connectivity, running in excess of 15
baseline methods for benchmarking. Additionally, we provide generic frameworks
for learning on both static and dynamic graphs. Our extensive experiments lead
to several key observations. Notably, using correlation vectors as node
features, incorporating larger number of regions of interest, and employing
sparser graphs lead to improved performance. To foster further advancements in
graph-based data driven neuroimaging analysis, we offer a comprehensive
open-source Python package that includes the benchmark datasets, baseline
implementations, model training, and standard evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08280">Differentially Private Wireless Federated Learning Using Orthogonal Sequences. (arXiv:2306.08280v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xizixiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Ruiquan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Cong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1">H. Vincent Poor</a></p>
<p>We propose a privacy-preserving uplink over-the-air computation (AirComp)
method, termed FLORAS, for single-input single-output (SISO) wireless federated
learning (FL) systems. From the perspective of communication designs, FLORAS
eliminates the requirement of channel state information at the transmitters
(CSIT) by leveraging the properties of orthogonal sequences. From the privacy
perspective, we prove that FLORAS offers both item-level and client-level
differential privacy (DP) guarantees. Moreover, by properly adjusting the
system parameters, FLORAS can flexibly achieve different DP levels at no
additional cost. A new FL convergence bound is derived which, combined with the
privacy guarantees, allows for a smooth tradeoff between the achieved
convergence rate and differential privacy levels. Experimental results
demonstrate the advantages of FLORAS compared with the baseline AirComp method,
and validate that the analytical results can guide the design of
privacy-preserving FL with different tradeoff requirements on the model
convergence and privacy levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16430">DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khabbazan_B/0/1/0/all/0/1">Bahareh Khabbazan</a>, <a href="http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1">Marc Riera</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1">Antonio Gonz&#xe1;lez</a></p>
<p>Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the
storage and computational complexity by decreasing the arithmetical precision
of activations and weights, a.k.a. tensors. Efficient hardware architectures
employ linear quantization to enable the deployment of recent DNNs onto
embedded systems and mobile devices. However, linear uniform quantization
cannot usually reduce the numerical precision to less than 8 bits without
sacrificing high performance in terms of model accuracy. The performance loss
is due to the fact that tensors do not follow uniform distributions. In this
paper, we show that a significant amount of tensors fit into an exponential
distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors
with an adaptive scheme that achieves the best trade-off between numerical
precision and accuracy loss. The experimental results show that DNA-TEQ
provides a much lower quantization bit-width compared to previous proposals,
resulting in an average compression ratio of 40% over the linear INT8 baseline,
with negligible accuracy loss and without retraining the DNNs. Besides, DNA-TEQ
leads the way in performing dot-product operations in the exponential domain,
which saves 66% of energy consumption on average for a set of widely used DNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17301">Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shijun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hongkai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yimin Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Haomin Zhou</a></p>
<p>In this work, a comprehensive numerical study involving analysis and
experiments shows why a two-layer neural network has difficulties handling high
frequencies in approximation and learning when machine precision and
computation cost are important factors in real practice. In particular, the
following basic computational issues are investigated: (1) the minimal
numerical error one can achieve given a finite machine precision, (2) the
computation cost to achieve a given accuracy, and (3) stability with respect to
perturbations. The key to the study is the conditioning of the representation
and its learning dynamics. Explicit answers to the above questions with
numerical verifications are presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03357">Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiyuan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1">Yiming Ying</a></p>
<p>Many machine learning tasks can be formulated as a stochastic compositional
optimization (SCO) problem such as reinforcement learning, AUC maximization,
and meta-learning, where the objective function involves a nested composition
associated with an expectation. While a significant amount of studies has been
devoted to studying the convergence behavior of SCO algorithms, there is little
work on understanding their generalization, i.e., how these learning algorithms
built from training examples would behave on future test examples. In this
paper, we provide the stability and generalization analysis of stochastic
compositional gradient descent algorithms through the lens of algorithmic
stability in the framework of statistical learning theory. Firstly, we
introduce a stability concept called compositional uniform stability and
establish its quantitative relation with generalization for SCO problems. Then,
we establish the compositional uniform stability results for two popular
stochastic compositional gradient descent algorithms, namely SCGD and SCSC.
Finally, we derive dimension-independent excess risk bounds for SCGD and SCSC
by trade-offing their stability results and optimization errors. To the best of
our knowledge, these are the first-ever-known results on stability and
generalization analysis of stochastic compositional gradient descent
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11280">Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Negoescu_D/0/1/0/all/0/1">Diana M. Negoescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_H/0/1/0/all/0/1">Humberto Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Orjany_S/0/1/0/all/0/1">Saad Eddin Al Orjany</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jilei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lut_Y/0/1/0/all/0/1">Yuliia Lut</a>, <a href="http://arxiv.org/find/cs/1/au:+Tandra_R/0/1/0/all/0/1">Rahul Tandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaowen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xinyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Douglas_Z/0/1/0/all/0/1">Zach Douglas</a>, <a href="http://arxiv.org/find/cs/1/au:+Nolkha_V/0/1/0/all/0/1">Vidita Nolkha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahammad_P/0/1/0/all/0/1">Parvez Ahammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Samorodnitsky_G/0/1/0/all/0/1">Gennady Samorodnitsky</a></p>
<p>We introduce Epsilon*, a new privacy metric for measuring the privacy risk of
a single model instance prior to, during, or after deployment of privacy
mitigation strategies. The metric requires only black-box access to model
predictions, does not require training data re-sampling or model re-training,
and can be used to measure the privacy risk of models not trained with
differential privacy. Epsilon* is a function of true positive and false
positive rates in a hypothesis test used by an adversary in a membership
inference attack. We distinguish between quantifying the privacy loss of a
trained model instance, which we refer to as empirical privacy, and quantifying
the privacy loss of the training mechanism which produces this model instance.
Existing approaches in the privacy auditing literature provide lower bounds for
the latter, while our metric provides an empirical lower bound for the former
by relying on an (${\epsilon}$, ${\delta}$)-type of quantification of the
privacy of the trained model instance. We establish a relationship between
these lower bounds and show how to implement Epsilon* to avoid numerical and
noise amplification instability. We further show in experiments on benchmark
public data sets that Epsilon* is sensitive to privacy risk mitigation by
training with differential privacy (DP), where the value of Epsilon* is reduced
by up to 800% compared to the Epsilon* values of non-DP trained baseline
models. This metric allows privacy auditors to be independent of model owners,
and enables visualizing the privacy-utility landscape to make informed
decisions regarding the trade-offs between model privacy and utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11494">Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1">Marcel Kollovieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1">Abdul Fatir Ansari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohlke_Schneider_M/0/1/0/all/0/1">Michael Bohlke-Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Zschiegner_J/0/1/0/all/0/1">Jasper Zschiegner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a></p>
<p>Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally-trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13390">Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1">Klaus Broelemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>Counterfactual Explanations (CEs) are an important tool in Algorithmic
Recourse for addressing two questions: 1. What are the crucial factors that led
to an automated prediction/decision? 2. How can these factors be changed to
achieve a more favorable outcome from a user's perspective? Thus, guiding the
user's interaction with AI systems by proposing easy-to-understand explanations
and easy-to-attain feasible changes is essential for the trustworthy adoption
and long-term acceptance of AI systems. In the literature, various methods have
been proposed to generate CEs, and different quality measures have been
suggested to evaluate these methods. However, the generation of CEs is usually
computationally expensive, and the resulting suggestions are unrealistic and
thus non-actionable. In this paper, we introduce a new method to generate CEs
for a pre-trained binary classifier by first shaping the latent space of an
autoencoder to be a mixture of Gaussian distributions. CEs are then generated
in latent space by linear interpolation between the query sample and the
centroid of the target class. We show that our method maintains the
characteristics of the input sample during the counterfactual search. In
various experiments, we show that the proposed method is competitive based on
different quality measures on image and tabular datasets -- efficiently returns
results that are closer to the original data manifold compared to three
state-of-the-art methods, which are essential for realistic high-dimensional
machine learning applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09936">BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a></p>
<p>Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14606">On the Tradeoff between Privacy Preservation and Byzantine-Robustness in Decentralized Learning. (arXiv:2308.14606v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haoxiang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Heng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1">Qing Ling</a></p>
<p>This paper jointly considers privacy preservation and Byzantine-robustness in
decentralized learning. In a decentralized network, honest-but-curious agents
faithfully follow the prescribed algorithm, but expect to infer their
neighbors' private data from messages received during the learning process,
while dishonest-and-Byzantine agents disobey the prescribed algorithm, and
deliberately disseminate wrong messages to their neighbors so as to bias the
learning process. For this novel setting, we investigate a generic
privacy-preserving and Byzantine-robust decentralized stochastic gradient
descent (SGD) framework, in which Gaussian noise is injected to preserve
privacy and robust aggregation rules are adopted to counteract Byzantine
attacks. We analyze its learning error and privacy guarantee, discovering an
essential tradeoff between privacy preservation and Byzantine-robustness in
decentralized learning -- the learning error caused by defending against
Byzantine attacks is exacerbated by the Gaussian noise added to preserve
privacy. For a class of state-of-the-art robust aggregation rules, we give
unified analysis of the "mixing abilities". Building upon this analysis, we
reveal how the "mixing abilities" affect the tradeoff between privacy
preservation and Byzantine-robustness. The theoretical results provide
guidelines for achieving a favorable tradeoff with proper design of robust
aggregation rules. Numerical experiments are conducted and corroborate our
theoretical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00846">pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation. (arXiv:2309.00846v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sreenivas_M/0/1/0/all/0/1">Manogna Sreenivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_G/0/1/0/all/0/1">Goirik Chakrabarty</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Soma Biswas</a></p>
<p>Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling
models to perform well in real-world scenarios, where test data distribution
differs from training. In this work, we propose a novel approach called pseudo
Source guided Target Clustering (pSTarC) addressing the relatively unexplored
area of TTA under real-world domain shifts. This method draws inspiration from
target clustering techniques and exploits the source classifier for generating
pseudo-source samples. The test samples are strategically aligned with these
pseudo-source samples, facilitating their clustering and thereby enhancing TTA
performance. pSTarC operates solely within the fully test-time adaptation
protocol, removing the need for actual source data. Experimental validation on
a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126,
CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant
improvements in prediction accuracy along with efficient computational
requirements. Furthermore, we also demonstrate the universality of the pSTarC
framework by showing its effectiveness for the continuous TTA framework. The
source code for our method is available at https://manogna-s.github.io/pstarc
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01897">Inferring Actual Treatment Pathways from Patient Records. (arXiv:2309.01897v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilkins_Caruana_A/0/1/0/all/0/1">Adrian Wilkins-Caruana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1">Madhushi Bandara</a>, <a href="http://arxiv.org/find/cs/1/au:+Musial_K/0/1/0/all/0/1">Katarzyna Musial</a>, <a href="http://arxiv.org/find/cs/1/au:+Catchpoole_D/0/1/0/all/0/1">Daniel Catchpoole</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_P/0/1/0/all/0/1">Paul J. Kennedy</a></p>
<p>Treatment pathways are step-by-step plans outlining the recommended medical
care for specific diseases; they get revised when different treatments are
found to improve patient outcomes. Examining health records is an important
part of this revision process, but inferring patients' actual treatments from
health data is challenging due to complex event-coding schemes and the absence
of pathway-related annotations. This study aims to infer the actual treatment
steps for a particular patient group from administrative health records (AHR) -
a common form of tabular healthcare data - and address several technique- and
methodology-based gaps in treatment pathway-inference research. We introduce
Defrag, a method for examining AHRs to infer the real-world treatment steps for
a particular patient group. Defrag learns the semantic and temporal meaning of
healthcare event sequences, allowing it to reliably infer treatment steps from
complex healthcare data. To our knowledge, Defrag is the first
pathway-inference method to utilise a neural network (NN), an approach made
possible by a novel, self-supervised learning objective. We also developed a
testing and validation framework for pathway inference, which we use to
characterise and evaluate Defrag's pathway inference ability and compare
against baselines. We demonstrate Defrag's effectiveness by identifying
best-practice pathway fragments for breast cancer, lung cancer, and melanoma in
public healthcare records. Additionally, we use synthetic data experiments to
demonstrate the characteristics of the Defrag method, and to compare Defrag to
several baselines where it significantly outperforms non-NN-based methods.
Defrag significantly outperforms several existing pathway-inference methods and
offers an innovative and effective approach for inferring treatment pathways
from AHRs. Open-source code is provided to encourage further research in this
area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07675">Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1">Mehdi Zadem</a>, <a href="http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1">Sergio Mover</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1">Sao Mai Nguyen</a></p>
<p>Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this paper, we propose a
developmental mechanism for goal discovery via an emergent representation that
abstracts (i.e., groups together) sets of environment states that have similar
roles in the task. We introduce a Feudal HRL algorithm that concurrently learns
both the goal representation and a hierarchical policy. The algorithm uses
symbolic reachability analysis for neural networks to approximate the
transition relation among sets of states and to refine the goal representation.
We evaluate our approach on complex navigation tasks, showing the learned
representation is interpretable, transferrable and results in data efficient
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08549">HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1">Minh-Hao Van</a>, <a href="http://arxiv.org/find/cs/1/au:+Carey_A/0/1/0/all/0/1">Alycia N. Carey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xintao Wu</a></p>
<p>While numerous defense methods have been proposed to prohibit potential
poisoning attacks from untrusted data sources, most research works only defend
against specific attacks, which leaves many avenues for an adversary to
exploit. In this work, we propose an efficient and robust training approach to
defend against data poisoning attacks based on influence functions, named
Healthy Influential-Noise based Training. Using influence functions, we craft
healthy noise that helps to harden the classification model against poisoning
attacks without significantly affecting the generalization ability on test
data. In addition, our method can perform effectively when only a subset of the
training data is modified, instead of the current method of adding noise to all
examples that has been used in several previous works. We conduct comprehensive
evaluations over two image datasets with state-of-the-art poisoning attacks
under different realistic attack scenarios. Our empirical results show that
HINT can efficiently protect deep learning models against the effect of both
untargeted and targeted poisoning attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11983">Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1">Zheng Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1">Ting Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethu_V/0/1/0/all/0/1">Vidhyasaharan Sethu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1">Beena Ahmed</a></p>
<p>Connectionist temporal classification (CTC) is commonly adopted for sequence
modeling tasks like speech recognition, where it is necessary to preserve order
between the input and target sequences. However, CTC is only applied to
deterministic sequence models, where the latent space is discontinuous and
sparse, which in turn makes them less capable of handling data variability when
compared to variational models. In this paper, we integrate CTC with a
variational model and derive loss functions that can be used to train more
generalizable sequence models that preserve order. Specifically, we derive two
versions of the novel variational CTC based on two reasonable assumptions, the
first being that the variational latent variables at each time step are
conditionally independent; and the second being that these latent variables are
Markovian. We show that both loss functions allow direct optimization of the
variational lower bound for the model log-likelihood, and present
computationally tractable forms for implementing them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16020">GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cepeda_V/0/1/0/all/0/1">Vicente Vivanco Cepeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1">Gaurav Kumar Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a></p>
<p>Worldwide Geo-localization aims to pinpoint the precise location of images
taken anywhere on Earth. This task has considerable challenges due to immense
variation in geographic landscapes. The image-to-image retrieval-based
approaches fail to solve this problem on a global scale as it is not feasible
to construct a large gallery of images covering the entire world. Instead,
existing approaches divide the globe into discrete geographic cells,
transforming the problem into a classification task. However, their performance
is limited by the predefined classes and often results in inaccurate
localizations when an image's location significantly deviates from its class
center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP's location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich
high-dimensional feature suitable to use even beyond geo-localization. To the
best of our knowledge, this is the first work employing GPS encoding for
geo-localization. We demonstrate the efficacy of our method via extensive
experiments and ablations on benchmark datasets. We achieve competitive
performance with just 20% of training data, highlighting its effectiveness even
in limited-data settings. Furthermore, we qualitatively demonstrate
geo-localization using a text query by leveraging CLIP backbone of our image
encoder. The project webpage is available at:
https://vicentevivan.github.io/GeoCLIP
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01012">Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chapman_J/0/1/0/all/0/1">James Chapman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wells_L/0/1/0/all/0/1">Lennie Wells</a>, <a href="http://arxiv.org/find/cs/1/au:+Aguila_A/0/1/0/all/0/1">Ana Lawry Aguila</a></p>
<p>The Canonical Correlation Analysis (CCA) family of methods is foundational in
multi-view learning. Regularised linear CCA methods can be seen to generalise
Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue
Problem (GEP) framework. However, classical algorithms for these linear methods
are computationally infeasible for large-scale data. Extensions to Deep CCA
show great promise, but current training procedures are slow and complicated.
First we propose a novel unconstrained objective that characterizes the top
subspace of GEPs. Our core contribution is a family of fast algorithms for
stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying
stochastic gradient descent (SGD) to the corresponding CCA objectives. These
methods show far faster convergence and recover higher correlations than the
previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This
speed allows us to perform a first-of-its-kind PLS analysis of an extremely
large biomedical dataset from the UK Biobank, with over 33,000 individuals and
500,000 variants. Finally, we not only match the performance of `CCA-family'
Self-Supervised Learning (SSL) methods on CIFAR-10 and CIFAR-100 with minimal
hyper-parameter tuning, but also establish the first solid theoretical links to
classical CCA, laying the groundwork for future insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03789">Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Rubin_N/0/1/0/all/0/1">Noa Rubin</a>, <a href="http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1">Inbar Seroussi</a>, <a href="http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1">Zohar Ringel</a></p>
<p>A key property of deep neural networks (DNNs) is their ability to learn new
features during training. This intriguing aspect of deep learning stands out
most clearly in recently reported Grokking phenomena. While mainly reflected as
a sudden increase in test accuracy, Grokking is also believed to be a beyond
lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here
we apply a recent development in the theory of feature learning, the adaptive
kernel approach, to two teacher-student models with cubic-polynomial and
modular addition teachers. We provide analytical predictions on feature
learning and Grokking properties of these models and demonstrate a mapping
between Grokking and the theory of phase transitions. We show that after
Grokking, the state of the DNN is analogous to the mixed phase following a
first-order phase transition. In this mixed phase, the DNN generates useful
internal representations of the teacher that are sharply distinct from those
before the transition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08897">Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Jina Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1">Youngtaek Hong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1">Dawun Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1">Yeonggul Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1">Jaeik Jeon</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_S/0/1/0/all/0/1">Sihyeon Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_T/0/1/0/all/0/1">Taekgeun Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1">Yeonyee E. Yoon</a>, <a href="http://arxiv.org/find/eess/1/au:+Moon_I/0/1/0/all/0/1">Inki Moon</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seung-Ah Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1">Hyuk-Jae Chang</a></p>
<p>Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12036">A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azar_M/0/1/0/all/0/1">Mohammad Gheshlaghi Azar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/cs/1/au:+Piot_B/0/1/0/all/0/1">Bilal Piot</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daniel Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a></p>
<p>The prevalent deployment of learning from human preferences through
reinforcement learning (RLHF) relies on two important approximations: the first
assumes that pairwise preferences can be substituted with pointwise rewards.
The second assumes that a reward model trained on these pointwise rewards can
generalize from collected data to out-of-distribution data sampled by the
policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an
approach that bypasses the second approximation and learn directly a policy
from collected data without the reward modelling stage. However, this method
still heavily relies on the first approximation.
</p>
<p>In this paper we try to gain a deeper theoretical understanding of these
practical algorithms. In particular we derive a new general objective called
$\Psi$PO for learning from human preferences that is expressed in terms of
pairwise preferences and therefore bypasses both approximations. This new
general objective allows us to perform an in-depth analysis of the behavior of
RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential
pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$
simply to Identity, for which we can derive an efficient optimisation
procedure, prove performance guarantees and demonstrate its empirical
superiority to DPO on some illustrative examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12942">On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nowak_F/0/1/0/all/0/1">Franz Nowak</a>, <a href="http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1">Anej Svete</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Li Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>This work investigates the computational expressivity of language models
(LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992)
famously showed that RNNs with rational weights and hidden states and unbounded
computation time are Turing complete. However, LMs define weightings over
strings in addition to just (unweighted) language membership and the analysis
of the computational power of RNN LMs (RLMs) should reflect this. We extend the
Turing completeness result to the probabilistic case, showing how a rationally
weighted RLM with unbounded computation time can simulate any deterministic
probabilistic Turing machine (PTM) with rationally weighted transitions. Since,
in practice, RLMs work in real-time, processing a symbol at every time step, we
treat the above result as an upper bound on the expressivity of RLMs. We also
provide a lower bound by showing that under the restriction to real-time
computation, such models can simulate deterministic real-time rational PTMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13458">Correspondence learning between morphologically different robots via task demonstrations. (arXiv:2310.13458v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aktas_H/0/1/0/all/0/1">Hakan Aktas</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagai_Y/0/1/0/all/0/1">Yukie Nagai</a>, <a href="http://arxiv.org/find/cs/1/au:+Asada_M/0/1/0/all/0/1">Minoru Asada</a>, <a href="http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1">Erhan Oztop</a>, <a href="http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1">Emre Ugur</a></p>
<p>We observe a large variety of robots in terms of their bodies, sensors, and
actuators. Given the commonalities in the skill sets, teaching each skill to
each different robot independently is inefficient and not scalable when the
large variety in the robotic landscape is considered. If we can learn the
correspondences between the sensorimotor spaces of different robots, we can
expect a skill that is learned in one robot can be more directly and easily
transferred to other robots. In this paper, we propose a method to learn
correspondences among two or more robots that may have different morphologies.
To be specific, besides robots with similar morphologies with different degrees
of freedom, we show that a fixed-based manipulator robot with joint control and
a differential drive mobile robot can be addressed within the proposed
framework. To set up the correspondence among the robots considered, an initial
base task is demonstrated to the robots to achieve the same goal. Then, a
common latent representation is learned along with the individual robot
policies for achieving the goal. After the initial learning stage, the
observation of a new task execution by one robot becomes sufficient to generate
a latent space representation pertaining to the other robots to achieve the
same task. We verified our system in a set of experiments where the
correspondence between robots is learned (1) when the robots need to follow the
same paths to achieve the same task, (2) when the robots need to follow
different trajectories to achieve the same task, and (3) when complexities of
the required sensorimotor trajectories are different for the robots. We also
provide a proof-of-the-concept realization of correspondence learning between a
real manipulator robot and a simulated mobile robot.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15290">Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1">Muhang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bernie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1">Allan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shiyi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Anru R. Zhang</a></p>
<p>Electronic Health Records (EHRs) are rich sources of patient-level data,
including laboratory tests, medications, and diagnoses, offering valuable
resources for medical data analysis. However, concerns about privacy often
restrict access to EHRs, hindering downstream analysis. Researchers have
explored various methods for generating privacy-preserving EHR data. In this
study, we introduce a new method for generating diverse and realistic synthetic
EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We
conducted experiments on six datasets, comparing our proposed method with eight
existing methods. Our results demonstrate that our approach significantly
outperforms all existing methods in terms of data utility while requiring less
training effort. Our approach also enhances downstream medical data analysis by
providing diverse and realistic synthetic EHR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19274">Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks. (arXiv:2310.19274v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jaehong Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_R/0/1/0/all/0/1">Rasool Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">WaiChing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Wei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukerji_T/0/1/0/all/0/1">Tapan Mukerji</a></p>
<p>This study presents a Graph Neural Networks (GNNs)-based approach for
predicting the effective elastic moduli of rocks from their digital CT-scan
images. We use the Mapper algorithm to transform 3D digital rock images into
graph datasets, encapsulating essential geometrical information. These graphs,
after training, prove effective in predicting elastic moduli. Our GNN model
shows robust predictive capabilities across various graph sizes derived from
various subcube dimensions. Not only does it perform well on the test dataset,
but it also maintains high prediction accuracy for unseen rocks and unexplored
subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)
reveals the superior performance of GNNs in predicting unseen rock properties.
Moreover, the graph representation of microstructures significantly reduces GPU
memory requirements (compared to the grid representation for CNNs), enabling
greater flexibility in the batch size selection. This work demonstrates the
potential of GNN models in enhancing the prediction accuracy of rock properties
and boosting the efficiency of digital rock analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19680">Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Soon-Jae Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1">Chang-Sung Jeong</a></p>
<p>Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies have been
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes PLM-integrated NMT
(PiNMT) model to overcome the identified problems. PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieve
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20477">Exploring Practitioner Perspectives On Training Data Attribution Explanations. (arXiv:2310.20477v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1">Elisa Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortukov_E/0/1/0/all/0/1">Evgenii Kortukov</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jean Y. Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seong Joon Oh</a></p>
<p>Explainable AI (XAI) aims to provide insight into opaque model reasoning to
humans and as such is an interdisciplinary field by nature. In this paper, we
interviewed 10 practitioners to understand the possible usability of training
data attribution (TDA) explanations and to explore the design space of such an
approach. We confirmed that training data quality is often the most important
factor for high model performance in practice and model developers mainly rely
on their own experience to curate data. End-users expect explanations to
enhance their interaction with the model and do not necessarily prioritise but
are open to training data as a means of explanation. Within our participants,
we found that TDA explanations are not well-known and therefore not used. We
urge the community to focus on the utility of TDA techniques from the
human-machine collaboration perspective and broaden the TDA evaluation to
reflect common use cases in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00474">Diffusion models for probabilistic programming. (arXiv:2311.00474v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dirmeier_S/0/1/0/all/0/1">Simon Dirmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1">Fernando Perez-Cruz</a></p>
<p>We propose Diffusion Model Variational Inference (DMVI), a novel method for
automated approximate inference in probabilistic programming languages (PPLs).
DMVI utilizes diffusion models as variational approximations to the true
posterior distribution by deriving a novel bound to the marginal likelihood
objective used in Bayesian modelling. DMVI is easy to implement, allows
hassle-free inference in PPLs without the drawbacks of, e.g., variational
inference using normalizing flows, and does not make any constraints on the
underlying neural network model. We evaluate DMVI on a set of common Bayesian
models and show that its posterior inferences are in general more accurate than
those of contemporary methods used in PPLs while having a similar computational
cost and requiring less manual tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01483">FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zihan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yue Gao</a></p>
<p>Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01588">Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v2 [astro-ph.CO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Roncoli_A/0/1/0/all/0/1">Andrea Roncoli</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ciprijanovic_A/0/1/0/all/0/1">Aleksandra &#x106;iprijanovi&#x107;</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Voetberg_M/0/1/0/all/0/1">Maggie Voetberg</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1">Francisco Villaescusa-Navarro</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1">Brian Nord</a></p>
<p>Deep learning models have been shown to outperform methods that rely on
summary statistics, like the power spectrum, in extracting information from
complex cosmological data sets. However, due to differences in the subgrid
physics implementation and numerical approximations across different simulation
suites, models trained on data from one cosmological simulation show a drop in
performance when tested on another. Similarly, models trained on any of the
simulations would also likely experience a drop in performance when applied to
observational data. Training on data from two different suites of the CAMELS
hydrodynamic cosmological simulations, we examine the generalization
capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing
GNNs, we capitalize on their capacity to capture structured scale-free
cosmological information from galaxy distributions. Moreover, by including
unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable
our models to extract domain-invariant features. We demonstrate that DA-GNN
achieves higher accuracy and robustness on cross-dataset tasks (up to $28\%$
better relative error and up to almost an order of magnitude better $\chi^2$).
Using data visualizations, we show the effects of domain adaptation on proper
latent space data alignment. This shows that DA-GNNs are a promising method for
extracting domain-independent cosmological information, a vital step toward
robust deep learning for real cosmic survey data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02921">Edge2Node: Reducing Edge Prediction to Node Classification. (arXiv:2311.02921v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahmati_Z/0/1/0/all/0/1">Zahed Rahmati</a></p>
<p>Despite the success of graph neural network models in node classification,
edge prediction (the task of predicting missing or potential links between
nodes in a graph) remains a challenging problem for these models. A common
approach for edge prediction is to first obtain the embeddings of two nodes,
and then a predefined scoring function is used to predict the existence of an
edge between the two nodes. Here, we introduce a preliminary idea called
Edge2Node which suggests to directly obtain an embedding for each edge, without
the need for a scoring function. This idea wants to create a new graph H based
on the graph G given for the edge prediction task, and then suggests reducing
the edge prediction task on G to a node classification task on H. We anticipate
that this introductory method could stimulate further investigations for edge
prediction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08228">Counterfactual Explanation for Regression via Disentanglement in Latent Space. (arXiv:2311.08228v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1">Klaus Broelemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>Counterfactual Explanations (CEs) help address the question: How can the
factors that influence the prediction of a predictive model be changed to
achieve a more favorable outcome from a user's perspective? Thus, they bear the
potential to guide the user's interaction with AI systems since they represent
easy-to-understand explanations. To be applicable, CEs need to be realistic and
actionable. In the literature, various methods have been proposed to generate
CEs. However, the majority of research on CEs focuses on classification
problems where questions like "What should I do to get my rejected loan
approved?" are raised. In practice, answering questions like "What should I do
to increase my salary?" are of a more regressive nature. In this paper, we
introduce a novel method to generate CEs for a pre-trained regressor by first
disentangling the label-relevant from the label-irrelevant dimensions in the
latent space. CEs are then generated by combining the label-irrelevant
dimensions and the predefined output. The intuition behind this approach is
that the ideal counterfactual search should focus on the label-irrelevant
characteristics of the input and suggest changes toward target-relevant
characteristics. Searching in the latent space could help achieve this goal. We
show that our method maintains the characteristics of the query sample during
the counterfactual search. In various experiments, we demonstrate that the
proposed method is competitive based on different quality measures on image and
tabular datasets in regression problem settings. It efficiently returns results
closer to the original data manifold compared to three state-of-the-art
methods, which is essential for realistic high-dimensional machine learning
applications. Our code will be made available as an open-source package upon
the publication of this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08936">Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness. (arXiv:2311.08936v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emam_A/0/1/0/all/0/1">Ahmed Emam</a>, <a href="http://arxiv.org/find/cs/1/au:+Farag_M/0/1/0/all/0/1">Mohamed Farag</a>, <a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1">Ribana Roscher</a></p>
<p>Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10242">Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers. (arXiv:2311.10242v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengesi_S/0/1/0/all/0/1">Staphord Bengesi</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Sayed_H/0/1/0/all/0/1">Hoda El-Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1">Md Kamruzzaman Sarker</a>, <a href="http://arxiv.org/find/cs/1/au:+Houkpati_Y/0/1/0/all/0/1">Yao Houkpati</a>, <a href="http://arxiv.org/find/cs/1/au:+Irungu_J/0/1/0/all/0/1">John Irungu</a>, <a href="http://arxiv.org/find/cs/1/au:+Oladunni_T/0/1/0/all/0/1">Timothy Oladunni</a></p>
<p>The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10500">From Principle to Practice: Vertical Data Minimization for Machine Learning. (arXiv:2311.10500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Staab_R/0/1/0/all/0/1">Robin Staab</a>, <a href="http://arxiv.org/find/cs/1/au:+Jovanovic_N/0/1/0/all/0/1">Nikola Jovanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1">Mislav Balunovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a></p>
<p>Aiming to train and deploy predictive models, organizations collect large
amounts of detailed client data, risking the exposure of private information in
the event of a breach. To mitigate this, policymakers increasingly demand
compliance with the data minimization (DM) principle, restricting data
collection to only that data which is relevant and necessary for the task.
Despite regulatory pressure, the problem of deploying machine learning models
that obey DM has so far received little attention. In this work, we address
this challenge in a comprehensive manner. We propose a novel vertical DM (vDM)
workflow based on data generalization, which by design ensures that no
full-resolution client data is collected during training and deployment of
models, benefiting client privacy by reducing the attack surface in case of a
breach. We formalize and study the corresponding problem of finding
generalizations that both maximize data utility and minimize empirical privacy
risk, which we quantify by introducing a diverse set of policy-aligned
adversarial scenarios. Finally, we propose a range of baseline vDM algorithms,
as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that
outperforms all baselines across several settings. We plan to release our code
as a publicly available library, helping advance the standardization of DM for
machine learning. Overall, we believe our work can help lay the foundation for
further exploration and adoption of DM principles in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10747">Safety-aware Causal Representation for Trustworthy Reinforcement Learning in Autonomous Driving. (arXiv:2311.10747v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haohong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenhao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yaru Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiacheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yuming Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a></p>
<p>In the domain of autonomous driving, the Learning from Demonstration (LfD)
paradigm has exhibited notable efficacy in addressing sequential
decision-making problems. However, consistently achieving safety in varying
traffic contexts, especially in safety-critical scenarios, poses a significant
challenge due to the long-tailed and unforeseen scenarios absent from offline
datasets. In this paper, we introduce the saFety-aware strUctured Scenario
representatION (FUSION), a pioneering methodology conceived to facilitate the
learning of an adaptive end-to-end driving policy by leveraging structured
scenario information. FUSION capitalizes on the causal relationships between
decomposed reward, cost, state, and action space, constructing a framework for
structured sequential reasoning under dynamic traffic environments. We conduct
rigorous evaluations in two typical real-world settings of distribution shift
in autonomous vehicles, demonstrating the good balance between safety cost and
utility reward of FUSION compared to contemporary state-of-the-art safety-aware
LfD baselines. Empirical evidence under diverse driving scenarios attests that
FUSION significantly enhances the safety and generalizability of autonomous
driving agents, even in the face of challenging and unseen environments.
Furthermore, our ablation studies reveal noticeable improvements in the
integration of causal representation into the safe offline RL problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10863">Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks. (arXiv:2311.10863v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haojun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zihe Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1">Yiannis Kantaros</a></p>
<p>Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10986">EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge. (arXiv:2311.10986v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bufang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lixing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_N/0/1/0/all/0/1">Neiwen Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhenyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_G/0/1/0/all/0/1">Guoliang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Shuai_X/0/1/0/all/0/1">Xian Shuai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a></p>
<p>Deep Learning (DL) models have been widely deployed on IoT devices with the
help of advancements in DL algorithms and chips. However, the limited resources
of edge devices make these on-device DL models hard to be generalizable to
diverse environments and tasks. Although the recently emerged foundation models
(FMs) show impressive generalization power, how to effectively leverage the
rich knowledge of FMs on resource-limited edge devices is still not explored.
In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with
open-set recognition capability. EdgeFM selectively uploads unlabeled data to
query the FM on the cloud and customizes the specific knowledge and
architectures for edge models. Meanwhile, EdgeFM conducts dynamic model
switching at run-time taking into account both data uncertainty and dynamic
network variations, which ensures the accuracy always close to the original FM.
We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on
three public datasets and two self-collected datasets. Results show that EdgeFM
can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy
increase compared with the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11254">BOIS: Bayesian Optimization of Interconnected Systems. (arXiv:2311.11254v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gonzalez_L/0/1/0/all/0/1">Leonardo D. Gonz&#xe1;lez</a>, <a href="http://arxiv.org/find/stat/1/au:+Zavala_V/0/1/0/all/0/1">Victor M. Zavala</a></p>
<p>Bayesian optimization (BO) has proven to be an effective paradigm for the
global optimization of expensive-to-sample systems. One of the main advantages
of BO is its use of Gaussian processes (GPs) to characterize model uncertainty
which can be leveraged to guide the learning and search process. However, BO
typically treats systems as black-boxes and this limits the ability to exploit
structural knowledge (e.g., physics and sparse interconnections). Composite
functions of the form $f(x, y(x))$, wherein GP modeling is shifted from the
performance function $f$ to an intermediate function $y$, offer an avenue for
exploiting structural knowledge. However, the use of composite functions in a
BO framework is complicated by the need to generate a probability density for
$f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is
nonlinear it is not possible to obtain a closed-form expression). Previous work
has handled this issue using sampling techniques; these are easy to implement
and flexible but are computationally intensive. In this work, we introduce a
new paradigm which allows for the efficient use of composite functions in BO;
this uses adaptive linearizations of $f$ to obtain closed-form expressions for
the statistical moments of the composite function. We show that this simple
approach (which we call BOIS) enables the exploitation of structural knowledge,
such as that arising in interconnected systems as well as systems that embed
multiple GP models and combinations of physics and GP models. Using a chemical
process optimization case study, we benchmark the effectiveness of BOIS against
standard BO and sampling approaches. Our results indicate that BOIS achieves
performance gains and accurately captures the statistics of composite
functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11772">A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology. (arXiv:2311.11772v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolflein_G/0/1/0/all/0/1">Georg W&#xf6;lflein</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferber_D/0/1/0/all/0/1">Dyke Ferber</a>, <a href="http://arxiv.org/find/cs/1/au:+Meneghetti_A/0/1/0/all/0/1">Asier Rabasco Meneghetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Nahhas_O/0/1/0/all/0/1">Omar S. M. El Nahhas</a>, <a href="http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1">Daniel Truhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Carrero_Z/0/1/0/all/0/1">Zunamys I. Carrero</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1">David J. Harrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1">Ognjen Arandjelovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1">Jakob N. Kather</a></p>
<p>Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11819">Generalized super-resolution 4D Flow MRI $\unicode{x2013}$ using ensemble learning to extend across the cardiovascular system. (arXiv:2311.11819v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ericsson_L/0/1/0/all/0/1">Leon Ericsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Hjalmarsson_A/0/1/0/all/0/1">Adam Hjalmarsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Akbar_M/0/1/0/all/0/1">Muhammad Usman Akbar</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferdian_E/0/1/0/all/0/1">Edward Ferdian</a>, <a href="http://arxiv.org/find/eess/1/au:+Bonini_M/0/1/0/all/0/1">Mia Bonini</a>, <a href="http://arxiv.org/find/eess/1/au:+Hardy_B/0/1/0/all/0/1">Brandon Hardy</a>, <a href="http://arxiv.org/find/eess/1/au:+Schollenberger_J/0/1/0/all/0/1">Jonas Schollenberger</a>, <a href="http://arxiv.org/find/eess/1/au:+Aristova_M/0/1/0/all/0/1">Maria Aristova</a>, <a href="http://arxiv.org/find/eess/1/au:+Winter_P/0/1/0/all/0/1">Patrick Winter</a>, <a href="http://arxiv.org/find/eess/1/au:+Burris_N/0/1/0/all/0/1">Nicholas Burris</a>, <a href="http://arxiv.org/find/eess/1/au:+Fyrdahl_A/0/1/0/all/0/1">Alexander Fyrdahl</a>, <a href="http://arxiv.org/find/eess/1/au:+Sigfridsson_A/0/1/0/all/0/1">Andreas Sigfridsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Schnell_S/0/1/0/all/0/1">Susanne Schnell</a>, <a href="http://arxiv.org/find/eess/1/au:+Figueroa_C/0/1/0/all/0/1">C. Alberto Figueroa</a>, <a href="http://arxiv.org/find/eess/1/au:+Nordsletten_D/0/1/0/all/0/1">David Nordsletten</a>, <a href="http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1">Alistair A. Young</a>, <a href="http://arxiv.org/find/eess/1/au:+Marlevi_D/0/1/0/all/0/1">David Marlevi</a></p>
<p>4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12068">Enhancing Novel Object Detection via Cooperative Foundational Models. (arXiv:2311.12068v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1">Rohit Bharadwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a></p>
<p>In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12166">Creating Temporally Correlated High-Resolution Power Injection Profiles Using Physics-Aware GAN. (arXiv:2311.12166v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shah_H/0/1/0/all/0/1">Hritik Gopal Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Azimian_B/0/1/0/all/0/1">Behrouz Azimian</a>, <a href="http://arxiv.org/find/eess/1/au:+Pal_A/0/1/0/all/0/1">Anamitra Pal</a></p>
<p>Traditional smart meter measurements lack the granularity needed for
real-time decision-making. To address this practical problem, we create a
generative adversarial networks (GAN) model that enforces temporal consistency
on its high-resolution outputs via hard inequality constraints using a convex
optimization layer. A unique feature of our GAN model is that it is trained
solely on slow timescale aggregated power information obtained from historical
smart meter data. The results demonstrate that the model can successfully
create minutely interval temporally-correlated instantaneous power injection
profiles from 15-minute average power consumption information. This innovative
approach, emphasizing inter-neuron constraints, offers a promising avenue for
improved high-speed state estimation in distribution systems and enhances the
applicability of data-driven solutions for monitoring such systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12198">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. (arXiv:2311.12198v2 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianyi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zeshun Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a></p>
<p>We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or
any other geometry embedding, highlighting the principle of "what you see is
what you simulate (WS$^2$)." Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12526">Neural Network Pruning by Gradient Descent. (arXiv:2311.12526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ruyi Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiang Zhang</a></p>
<p>The rapid increase in the parameters of deep learning models has led to
significant costs, challenging computational efficiency and model
interpretability. In this paper, we introduce a novel and straightforward
neural network pruning framework that incorporates the Gumbel-Softmax
technique. This framework enables the simultaneous optimization of a network's
weights and topology in an end-to-end process using stochastic gradient
descent. Empirical results demonstrate its exceptional compression capability,
maintaining high accuracy on the MNIST dataset with only 0.15\% of the original
network parameters. Moreover, our framework enhances neural network
interpretability, not only by allowing easy extraction of feature importance
directly from the pruned network but also by enabling visualization of feature
symmetry and the pathways of information propagation from features to outcomes.
Although the pruning strategy is learned through deep learning, it is
surprisingly intuitive and understandable, focusing on selecting key
representative features and exploiting data patterns to achieve extreme sparse
pruning. We believe our method opens a promising new avenue for deep learning
pruning and the creation of interpretable machine learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12538">In-Context Learning Functions with Varying Number of Minima. (arXiv:2311.12538v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1">David Oniani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanshan Wang</a></p>
<p>Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12550">Explainable Anomaly Detection using Masked Latent Generative Modeling. (arXiv:2311.12550v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Daesoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Malacarne_S/0/1/0/all/0/1">Sara Malacarne</a>, <a href="http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1">Erlend Aune</a></p>
<p>We present a novel time series anomaly detection method that achieves
excellent detection accuracy while offering a superior level of explainability.
Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted
from the cutting-edge time series generation method known as TimeVQVAE. The
prior model is trained on the discrete latent space of a time-frequency domain.
Notably, the dimensional semantics of the time-frequency domain are preserved
in the latent space, enabling us to compute anomaly scores across different
frequency bands, which provides a better insight into the detected anomalies.
Additionally, the generative nature of the prior model allows for sampling
likely normal states for detected anomalies, enhancing the explainability of
the detected anomalies through counterfactuals. Our experimental evaluation on
the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD
significantly surpasses the existing methods in terms of detection accuracy and
explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11166">From Optimization to Control: Quasi Policy Iteration. (arXiv:2311.11166v1 [math.OC] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kolarijani_M/0/1/0/all/0/1">Mohammad Amin Sharifi Kolarijani</a>, <a href="http://arxiv.org/find/math/1/au:+Esfahani_P/0/1/0/all/0/1">Peyman Mohajerin Esfahani</a></p>
<p>Recent control algorithms for Markov decision processes (MDPs) have been
designed using an implicit analogy with well-established optimization
algorithms. In this paper, we make this analogy explicit across four problem
classes with a unified solution characterization. This novel framework, in
turn, allows for a systematic transformation of algorithms from one domain to
the other. In particular, we identify equivalent optimization and control
algorithms that have already been pointed out in the existing literature, but
mostly in a scattered way. With this unifying framework in mind, we then
exploit two linear structural constraints specific to MDPs for approximating
the Hessian in a second-order-type algorithm from optimization, namely,
Anderson mixing. This leads to a novel first-order control algorithm that
modifies the standard value iteration (VI) algorithm by incorporating two new
directions and adaptive step sizes. While the proposed algorithm, coined as
quasi-policy iteration, has the same computational complexity as VI, it
interestingly exhibits an empirical convergence behavior similar to policy
iteration with a very low sensitivity to the discount factor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11642">Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging. (arXiv:2311.11642v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muqeet_A/0/1/0/all/0/1">Abdul Muqeet</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyuchul Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Bumsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yohan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungrae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woonggon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwang Hee Lee</a></p>
<p>Video face re-aging deals with altering the apparent age of a person to the
target age in videos. This problem is challenging due to the lack of paired
video datasets maintaining temporal consistency in identity and age. Most
re-aging methods process each image individually without considering the
temporal consistency of videos. While some existing works address the issue of
temporal coherence through video facial attribute manipulation in latent space,
they often fail to deliver satisfactory performance in age transformation. To
tackle the issues, we propose (1) a novel synthetic video dataset that features
subjects across a diverse range of age groups; (2) a baseline architecture
designed to validate the effectiveness of our proposed dataset, and (3) the
development of three novel metrics tailored explicitly for evaluating the
temporal consistency of video re-aging techniques. Our comprehensive
experiments on public datasets, such as VFHQ and CelebV-HQ, show that our
method outperforms the existing approaches in terms of both age transformation
and temporal consistency.
</p>
</p>
</div>

    </div>
    </body>
    