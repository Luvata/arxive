<!DOCTYPE html>
<html>
<head>
<title>2024-01-04-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.00859">Federated Multi-View Synthesizing for Metaverse. (arXiv:2401.00859v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yiyu Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Qin_Z/0/1/0/all/0/1">Zhijin Qin</a>, <a href="http://arxiv.org/find/eess/1/au:+Tao_X/0/1/0/all/0/1">Xiaoming Tao</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1">Geoffrey Ye Li</a></p>
<p>The metaverse is expected to provide immersive entertainment, education, and
business applications. However, virtual reality (VR) transmission over wireless
networks is data- and computation-intensive, making it critical to introduce
novel solutions that meet stringent quality-of-service requirements. With
recent advances in edge intelligence and deep learning, we have developed a
novel multi-view synthesizing framework that can efficiently provide
computation, storage, and communication resources for wireless content delivery
in the metaverse. We propose a three-dimensional (3D)-aware generative model
that uses collections of single-view images. These single-view images are
transmitted to a group of users with overlapping fields of view, which avoids
massive content transmission compared to transmitting tiles or whole 3D models.
We then present a federated learning approach to guarantee an efficient
learning process. The training performance can be improved by characterizing
the vertical and horizontal data samples with a large latent feature space,
while low-latency communication can be achieved with a reduced number of
transmitted parameters during federated learning. We also propose a federated
transfer learning framework to enable fast domain adaptation to different
target domains. Simulation results have demonstrated the effectiveness of our
proposed federated multi-view synthesizing framework for VR content delivery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00869">FlashVideo: A Framework for Swift Inference in Text-to-Video Generation. (arXiv:2401.00869v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Bin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_l/0/1/0/all/0/1">le Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Caiwen Ding</a></p>
<p>In the evolving field of machine learning, video generation has witnessed
significant advancements with autoregressive-based transformer models and
diffusion models, known for synthesizing dynamic and realistic scenes. However,
these models often face challenges with prolonged inference times, even for
generating short video clips such as GIFs. This paper introduces FlashVideo, a
novel framework tailored for swift Text-to-Video generation. FlashVideo
represents the first successful adaptation of the RetNet architecture for video
generation, bringing a unique approach to the field. Leveraging the
RetNet-based architecture, FlashVideo reduces the time complexity of inference
from $\mathcal{O}(L^2)$ to $\mathcal{O}(L)$ for a sequence of length $L$,
significantly accelerating inference speed. Additionally, we adopt a
redundant-free frame interpolation method, enhancing the efficiency of frame
interpolation. Our comprehensive experiments demonstrate that FlashVideo
achieves a $\times9.17$ efficiency improvement over a traditional
autoregressive-based transformer model, and its inference speed is of the same
order of magnitude as that of BERT-based transformer models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00871">PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance Fields. (arXiv:2401.00871v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qingan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1">Huangying Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Changjiang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiangyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuzhong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Ziyue Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lantao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a></p>
<p>Identifying spatially complete planar primitives from visual data is a
crucial task in computer vision. Prior methods are largely restricted to either
2D segment recovery or simplifying 3D structures, even with extensive plane
annotations. We present PlanarNeRF, a novel framework capable of detecting
dense 3D planes through online learning. Drawing upon the neural field
representation, PlanarNeRF brings three major contributions. First, it enhances
3D plane detection with concurrent appearance and geometry knowledge. Second, a
lightweight plane fitting module is proposed to estimate plane parameters.
Third, a novel global memory bank structure with an update mechanism is
introduced, ensuring consistent cross-frame correspondence. The flexible
architecture of PlanarNeRF allows it to function in both 2D-supervised and
self-supervised solutions, in each of which it can effectively learn from
sparse training signals, significantly improving training efficiency. Through
extensive experiments, we demonstrate the effectiveness of PlanarNeRF in
various scenarios and remarkable improvement over existing works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00873">A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1">Emanuele Sansone</a>, <a href="http://arxiv.org/find/cs/1/au:+Manhaeve_R/0/1/0/all/0/1">Robin Manhaeve</a></p>
<p>Self-supervised learning is a popular and powerful method for utilizing large
amounts of unlabeled data, for which a wide variety of training objectives have
been proposed in the literature. In this study, we perform a Bayesian analysis
of state-of-the-art self-supervised learning objectives, elucidating the
underlying probabilistic graphical models in each class and presenting a
standardized methodology for their derivation from first principles. The
analysis also indicates a natural means of integrating self-supervised learning
with likelihood-based generative models. We instantiate this concept within the
realm of cluster-based self-supervised learning and energy models, introducing
a novel lower bound which is proven to reliably penalize the most important
failure modes. Furthermore, this newly proposed lower bound enables the
training of a standard backbone architecture without the necessity for
asymmetric elements such as stop gradients, momentum encoders, or specialized
clustering layers - typically introduced to avoid learning trivial solutions.
Our theoretical findings are substantiated through experiments on synthetic and
real-world data, including SVHN, CIFAR10, and CIFAR100, thus showing that our
objective function allows to outperform existing self-supervised learning
strategies in terms of clustering, generation and out-of-distribution detection
performance by a wide margin. We also demonstrate that GEDI can be integrated
into a neural-symbolic framework to mitigate the reasoning shortcut problem and
to learn higher quality symbolic representations thanks to the enhanced
classification performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00877">Improving the Stability of Diffusion Models for Content Consistent Super-Resolution. (arXiv:2401.00877v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1">Lingchen Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_R/0/1/0/all/0/1">Rongyuan Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengqiang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yong_H/0/1/0/all/0/1">Hongwei Yong</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>The generative priors of pre-trained latent diffusion models have
demonstrated great potential to enhance the perceptual quality of image
super-resolution (SR) results. Unfortunately, the existing diffusion
prior-based SR methods encounter a common problem, i.e., they tend to generate
rather different outputs for the same low-resolution image with different noise
samples. Such stochasticity is desired for text-to-image generation tasks but
problematic for SR tasks, where the image contents are expected to be well
preserved. To improve the stability of diffusion prior-based SR, we propose to
employ the diffusion models to refine image structures, while employing the
generative adversarial training to enhance image fine details. Specifically, we
propose a non-uniform timestep learning strategy to train a compact diffusion
network, which has high efficiency and stability to reproduce the image main
structures, and finetune the pre-trained decoder of variational auto-encoder
(VAE) by adversarial training for detail enhancement. Extensive experiments
show that our proposed method, namely content consistent super-resolution
(CCSR), can significantly reduce the stochasticity of diffusion prior-based SR,
improving the content consistency of SR outputs and speeding up the image
generation process. Codes and models can be found at
{https://github.com/csslc/CCSR}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00889">3D Human Pose Perception from Egocentric Stereo Videos. (arXiv:2401.00889v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akada_H/0/1/0/all/0/1">Hiroyasu Akada</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a></p>
<p>While head-mounted devices are becoming more compact, they provide egocentric
views with significant self-occlusions of the device user. Hence, existing
methods often fail to accurately estimate complex 3D poses from egocentric
views. In this work, we propose a new transformer-based framework to improve
egocentric stereo 3D human pose estimation, which leverages the scene
information and temporal context of egocentric stereo videos. Specifically, we
utilize 1) depth features from our 3D scene reconstruction module with
uniformly sampled windows of egocentric stereo frames, and 2) human joint
queries enhanced by temporal features of the video inputs. Our method is able
to accurately estimate human poses even in challenging scenarios, such as
crouching and sitting. Furthermore, we introduce two new benchmark datasets,
i.e., UnrealEgo2 and UnrealEgo-RW (RealWorld). The proposed datasets offer a
much larger number of egocentric stereo views with a wider variety of human
motions than the existing datasets, allowing comprehensive evaluation of
existing and upcoming methods. Our extensive experiments show that the proposed
approach significantly outperforms previous methods. We will release
UnrealEgo2, UnrealEgo-RW, and trained models on our project page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00894">Balanced Multi-modal Federated Learning via Cross-Modal Infiltration. (arXiv:2401.00894v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yunfeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenchao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haozhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiaqi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Song Guo</a></p>
<p>Federated learning (FL) underpins advancements in privacy-preserving
distributed computing by collaboratively training neural networks without
exposing clients' raw data. Current FL paradigms primarily focus on uni-modal
data, while exploiting the knowledge from distributed multimodal data remains
largely unexplored. Existing multimodal FL (MFL) solutions are mainly designed
for statistical or modality heterogeneity from the input side, however, have
yet to solve the fundamental issue,"modality imbalance", in distributed
conditions, which can lead to inadequate information exploitation and
heterogeneous knowledge aggregation on different modalities.In this paper, we
propose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework
that effectively alleviates modality imbalance and knowledge heterogeneity via
knowledge transfer from the global dominant modality. To avoid the loss of
information in the weak modality due to merely imitating the behavior of
dominant modality, we design the two-projector module to integrate the
knowledge from dominant modality while still promoting the local feature
exploitation of weak modality. In addition, we introduce a class-wise
temperature adaptation scheme to achieve fair performance across different
classes. Extensive experiments over popular datasets are conducted and give us
a gratifying confirmation of the proposed framework for fully exploring the
information of each modality in MFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00896">TrailBlazer: Trajectory Control for Diffusion-Based Video Generation. (arXiv:2401.00896v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wan-Duo Kurt Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1">J.P. Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1">W. Bastiaan Kleijn</a></p>
<p>Within recent approaches to text-to-video (T2V) generation, achieving
controllability in the synthesized video is often a challenge. Typically, this
issue is addressed by providing low-level per-frame guidance in the form of
edge maps, depth maps, or an existing video to be altered. However, the process
of obtaining such guidance can be labor-intensive. This paper focuses on
enhancing controllability in video synthesis by employing straightforward
bounding boxes to guide the subject in various ways, all without the need for
neural network training, finetuning, optimization at inference time, or the use
of pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a
pre-trained (T2V) model, and easy to implement. The subject is directed by a
bounding box through the proposed spatial and temporal attention map editing.
Moreover, we introduce the concept of keyframing, allowing the subject
trajectory and overall appearance to be guided by both a moving bounding box
and corresponding prompts, without the need to provide a detailed mask. The
method is efficient, with negligible additional computation relative to the
underlying pre-trained model. Despite the simplicity of the bounding box
guidance, the resulting motion is surprisingly natural, with emergent effects
including perspective and movement toward the virtual camera as the box size
increases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00897">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Luyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zedong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lirong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Cheng Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baigui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Stan Z. Li</a></p>
<p>As the deep learning revolution marches on, self-supervised learning has
garnered increasing attention in recent years thanks to its remarkable
representation learning ability and the low dependence on labeled data. Among
these varied self-supervised techniques, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training. This paradigm enables deep models to
learn robust representations and has demonstrated exceptional performance in
the context of computer vision, natural language processing, and other
modalities. In this survey, we present a comprehensive review of the masked
modeling framework and its methodology. We elaborate on the details of
techniques within masked modeling, including diverse masking strategies,
recovering targets, network architectures, and more. Then, we systematically
investigate its wide-ranging applications across domains. Furthermore, we also
explore the commonalities and differences between masked modeling methods in
different fields. Toward the end of this paper, we conclude by discussing the
limitations of current techniques and point out several potential avenues for
advancing masked modeling research. A paper list project with this survey is
available at \url{https://github.com/Lupin1998/Awesome-MIM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00901">Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding. (arXiv:2401.00901v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1">Syed Talal Wasim</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a></p>
<p>Video grounding aims to localize a spatio-temporal section in a video
corresponding to an input text query. This paper addresses a critical
limitation in current video grounding methodologies by introducing an
Open-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent
closed-set approaches that struggle with open-vocabulary scenarios due to
limited training data and predefined vocabularies, our model leverages
pre-trained representations from foundational spatial grounding models. This
empowers it to effectively bridge the semantic gap between natural language and
diverse visual content, achieving strong performance in closed-set and
open-vocabulary settings. Our contributions include a novel spatio-temporal
video grounding model, surpassing state-of-the-art results in closed-set
evaluations on multiple datasets and demonstrating superior performance in
open-vocabulary scenarios. Notably, the proposed model outperforms
state-of-the-art methods in closed-set settings on VidSTG (Declarative and
Interrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in
open-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model
surpasses the recent best-performing models by $4.26$ m_vIoU and $1.83\%$
accuracy, demonstrating its efficacy in handling diverse linguistic and visual
concepts for improved video understanding. Our codes will be released at
https://github.com/TalalWasim/Video-GroundingDINO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00909">Taming Mode Collapse in Score Distillation for Text-to-3D Generation. (arXiv:2401.00909v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1">Sreyas Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1">Forrest Iandola</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1">Rakesh Ranjan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yilei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a></p>
<p>Despite the remarkable performance of score distillation in text-to-3D
generation, such techniques notoriously suffer from view inconsistency issues,
also known as "Janus" artifact, where the generated objects fake each view with
multiple front faces. Although empirically effective methods have approached
this problem via score debiasing or prompt engineering, a more rigorous
perspective to explain and tackle this problem remains elusive. In this paper,
we reveal that the existing score distillation-based text-to-3D generation
frameworks degenerate to maximal likelihood seeking on each view independently
and thus suffer from the mode collapse problem, manifesting as the Janus
artifact in practice. To tame mode collapse, we improve score distillation by
re-establishing in entropy term in the corresponding variational objective,
which is applied to the distribution of rendered images. Maximizing the entropy
encourages diversity among different views in generated 3D assets, thereby
mitigating the Janus problem. Based on this new objective, we derive a new
update rule for 3D score distillation, dubbed Entropic Score Distillation
(ESD). We theoretically reveal that ESD can be simplified and implemented by
just adopting the classifier-free guidance trick upon variational score
distillation. Although embarrassingly straightforward, our extensive
experiments successfully demonstrate that ESD can be an effective treatment for
Janus artifacts in score distillation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00910">WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1">Saravanabalagi Ramachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Cibik_N/0/1/0/all/0/1">Nathaniel Cibik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1">Ganesh Sistu</a>, <a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1">John McDonald</a></p>
<p>Motion segmentation is a complex yet indispensable task in autonomous
driving. The challenges introduced by the ego-motion of the cameras, radial
distortion in fisheye lenses, and the need for temporal consistency make the
task more complicated, rendering traditional and standard Convolutional Neural
Network (CNN) approaches less effective. The consequent laborious data
labeling, representation of diverse and uncommon scenarios, and extensive data
capture requirements underscore the imperative of synthetic data for improving
machine learning model performance. To this end, we employ the PD-WoodScape
synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye
dataset. Thus, we present the WoodScape fisheye motion segmentation challenge
for autonomous driving, held as part of the CVPR 2023 Workshop on
Omnidirectional Computer Vision (OmniCV). As one of the first competitions
focused on fisheye motion segmentation, we aim to explore and evaluate the
potential and impact of utilizing synthetic data in this domain. In this paper,
we provide a detailed analysis on the competition which attracted the
participation of 112 global teams and a total of 234 submissions. This study
delineates the complexities inherent in the task of motion segmentation,
emphasizes the significance of fisheye datasets, articulate the necessity for
synthetic datasets and the resultant domain gap they engender, outlining the
foundational blueprint for devising successful solutions. Subsequently, we
delve into the details of the baseline experiments and winning methods
evaluating their qualitative and quantitative results, providing with useful
insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00912">ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention. (arXiv:2401.00912v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Chenhang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruihuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guowen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>Window-based transformers have demonstrated strong ability in large-scale
point cloud understanding by capturing context-aware representations with
affordable attention computation in a more localized manner. However, because
of the sparse nature of point clouds, the number of voxels per window varies
significantly. Current methods partition the voxels in each window into
multiple subsets of equal size, which cost expensive overhead in sorting and
padding the voxels, making them run slower than sparse convolution based
methods. In this paper, we present ScatterFormer, which, for the first time to
our best knowledge, could directly perform attention on voxel sets with
variable length. The key of ScatterFormer lies in the innovative Scatter Linear
Attention (SLA) module, which leverages the linear attention mechanism to
process in parallel all voxels scattered in different windows. Harnessing the
hierarchical computation units of the GPU and matrix blocking algorithm, we
reduce the latency of the proposed SLA module to less than 1 ms on moderate
GPUs. Besides, we develop a cross-window interaction module to simultaneously
enhance the local representation and allow the information flow across windows,
eliminating the need for window shifting. Our proposed ScatterFormer
demonstrates 73 mAP (L2) on the large-scale Waymo Open Dataset and 70.5 NDS on
the NuScenes dataset, running at an outstanding detection rate of 28 FPS. Code
is available at https://github.com/skyhehe123/ScatterFormer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00921">Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence. (arXiv:2401.00921v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruizhuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Linzhi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jiani Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Weihong Deng</a></p>
<p>Self-supervised pre-training paradigms have been extensively explored in the
field of skeleton-based action recognition. In particular, methods based on
masked prediction have pushed the performance of pre-training to a new height.
However, these methods take low-level features, such as raw joint coordinates
or temporal motion, as prediction targets for the masked regions, which is
suboptimal. In this paper, we show that using high-level contextualized
features as prediction targets can achieve superior performance. Specifically,
we propose Skeleton2vec, a simple and efficient self-supervised 3D action
representation learning framework, which utilizes a transformer-based teacher
encoder taking unmasked training samples as input to create latent
contextualized representations as prediction targets. Benefiting from the
self-attention mechanism, the latent representations generated by the teacher
encoder can incorporate the global context of the entire training samples,
leading to a richer training task. Additionally, considering the high temporal
correlations in skeleton sequences, we propose a motion-aware tube masking
strategy which divides the skeleton sequence into several tubes and performs
persistent masking within each tube based on motion priors, thus forcing the
model to build long-range spatio-temporal connections and focus on
action-semantic richer regions. Extensive experiments on NTU-60, NTU-120, and
PKU-MMD datasets demonstrate that our proposed Skeleton2vec outperforms
previous methods and achieves state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00926">Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Ben Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changmiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xianjun Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuxing Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1">Feiwei Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yu Gao</a></p>
<p>In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00929">GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation. (arXiv:2401.00929v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziqing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pengwei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Rui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1">Li Yi</a></p>
<p>This paper presents GenH2R, a framework for learning generalizable
vision-based human-to-robot (H2R) handover skills. The goal is to equip robots
with the ability to reliably receive objects with unseen geometry handed over
by humans in various complex trajectories. We acquire such generalizability by
learning H2R handover at scale with a comprehensive solution including
procedural simulation assets creation, automated demonstration generation, and
effective imitation learning. We leverage large-scale 3D model repositories,
dexterous grasp generation methods, and curve-based 3D animation to create an
H2R handover simulation environment named \simabbns, surpassing the number of
scenes in existing simulators by three orders of magnitude. We further
introduce a distillation-friendly demonstration generation method that
automatically generates a million high-quality demonstrations suitable for
learning. Finally, we present a 4D imitation learning method augmented by a
future forecasting objective to distill demonstrations into a visuo-motor
handover policy. Experimental evaluations in both simulators and the real world
demonstrate significant improvements (at least +10\% success rate) over
baselines in all cases. The project page is https://GenH2R.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00935">Boundary Attention: Learning to Find Faint Boundaries at Any Resolution. (arXiv:2401.00935v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Polansky_M/0/1/0/all/0/1">Mia Gaia Polansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1">Charles Herrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1">Junhwa Hur</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Deqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbin_D/0/1/0/all/0/1">Dor Verbin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1">Todd Zickler</a></p>
<p>We present a differentiable model that explicitly models boundaries --
including contours, corners and junctions -- using a new mechanism that we call
boundary attention. We show that our model provides accurate results even when
the boundary signal is very weak or is swamped by noise. Compared to previous
classical methods for finding faint boundaries, our model has the advantages of
being differentiable; being scalable to larger images; and automatically
adapting to an appropriate level of geometric detail in each part of an image.
Compared to previous deep methods for finding boundaries via end-to-end
training, it has the advantages of providing sub-pixel precision, being more
resilient to noise, and being able to process any image at its native
resolution and aspect ratio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00964">Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition. (arXiv:2401.00964v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Strohmayer_J/0/1/0/all/0/1">Julian Strohmayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampel_M/0/1/0/all/0/1">Martin Kampel</a></p>
<p>The recognition of human activities based on WiFi Channel State Information
(CSI) enables contactless and visual privacy-preserving sensing in indoor
environments. However, poor model generalization, due to varying environmental
conditions and sensing hardware, is a well-known problem in this space. To
address this issue, in this work, data augmentation techniques commonly used in
image-based learning are applied to WiFi CSI to investigate their effects on
model generalization performance in cross-scenario and cross-system settings.
In particular, we focus on the generalization between line-of-sight (LOS) and
non-line-of-sight (NLOS) through-wall scenarios, as well as on the
generalization between different antenna systems, which remains under-explored.
We collect and make publicly available a dataset of CSI amplitude spectrograms
of human activities. Utilizing this data, an ablation study is conducted in
which activity recognition models based on the EfficientNetV2 architecture are
trained, allowing us to assess the effects of each augmentation on model
generalization performance. The gathered results show that specific
combinations of simple data augmentation techniques applied to CSI amplitude
data can significantly improve cross-scenario and cross-system generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00971">Efficient Multi-domain Text Recognition Deep Neural Network Parameterization with Residual Adapters. (arXiv:2401.00971v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chao_J/0/1/0/all/0/1">Jiayou Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wei Zhu</a></p>
<p>Recent advancements in deep neural networks have markedly enhanced the
performance of computer vision tasks, yet the specialized nature of these
networks often necessitates extensive data and high computational power.
Addressing these requirements, this study presents a novel neural network model
adept at optical character recognition (OCR) across diverse domains, leveraging
the strengths of multi-task learning to improve efficiency and generalization.
The model is designed to achieve rapid adaptation to new domains, maintain a
compact size conducive to reduced computational resource demand, ensure high
accuracy, retain knowledge from previous learning experiences, and allow for
domain-specific performance improvements without the need to retrain entirely.
Rigorous evaluation on open datasets has validated the model's ability to
significantly lower the number of trainable parameters without sacrificing
performance, indicating its potential as a scalable and adaptable solution in
the field of computer vision, particularly for applications in optical text
recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00979">3D Visibility-aware Generalizable Neural Radiance Fields for Interacting Hands. (arXiv:2401.00979v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hanhui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zejun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a></p>
<p>Neural radiance fields (NeRFs) are promising 3D representations for scenes,
objects, and humans. However, most existing methods require multi-view inputs
and per-scene training, which limits their real-life applications. Moreover,
current methods focus on single-subject cases, leaving scenes of interacting
hands that involve severe inter-hand occlusions and challenging view variations
remain unsolved. To tackle these issues, this paper proposes a generalizable
visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,
given an image of interacting hands as input, our VA-NeRF first obtains a
mesh-based representation of hands and extracts their corresponding geometric
and textural features. Subsequently, a feature fusion module that exploits the
visibility of query points and mesh vertices is introduced to adaptively merge
features of both hands, enabling the recovery of features in unseen areas.
Additionally, our VA-NeRF is optimized together with a novel discriminator
within an adversarial learning paradigm. In contrast to conventional
discriminators that predict a single real/fake label for the synthesized image,
the proposed discriminator generates a pixel-wise visibility map, providing
fine-grained supervision for unseen areas and encouraging the VA-NeRF to
improve the visual quality of synthesized images. Experiments on the
Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms
conventional NeRFs significantly. Project Page:
\url{https://github.com/XuanHuang0/VANeRF}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00986">Real-Time Object Detection in Occluded Environment with Background Cluttering Effects Using Deep Learning. (arXiv:2401.00986v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aamir_S/0/1/0/all/0/1">Syed Muhammad Aamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hongbin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Malak Abid Ali Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aaqib_M/0/1/0/all/0/1">Muhammad Aaqib</a></p>
<p>Detection of small, undetermined moving objects or objects in an occluded
environment with a cluttered background is the main problem of computer vision.
This greatly affects the detection accuracy of deep learning models. To
overcome these problems, we concentrate on deep learning models for real-time
detection of cars and tanks in an occluded environment with a cluttered
background employing SSD and YOLO algorithms and improved precision of
detection and reduce problems faced by these models. The developed method makes
the custom dataset and employs a preprocessing technique to clean the noisy
dataset. For training the developed model we apply the data augmentation
technique to balance and diversify the data. We fine-tuned, trained, and
evaluated these models on the established dataset by applying these techniques
and highlighting the results we got more accurately than without applying these
techniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are
higher than YOLO V3 and YOLO V4. Furthermore, by employing various techniques
like data enhancement, noise reduction, parameter optimization, and model
fusion we improve the effectiveness of detection and recognition. We further
added a counting algorithm, and target attributes experimental comparison, and
made a graphical user interface system for the developed model with features of
object counting, alerts, status, resolution, and frame per second.
Subsequently, to justify the importance of the developed method analysis of
YOLO V3, V4, and SSD were incorporated. Which resulted in the overall
completion of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00988">Holistic Autonomous Driving Understanding by Bird&#x27;s-Eye-View Injected Multi-Modal Large Models. (arXiv:2401.00988v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xinpeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jinahua Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaomeng Li</a></p>
<p>The rise of multimodal large language models (MLLMs) has spurred interest in
language-based driving tasks. However, existing research typically focuses on
limited tasks and often omits key multi-view and temporal information which is
crucial for robust autonomous driving. To bridge these gaps, we introduce
NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17
subtasks, where each task demands holistic information (e.g., temporal,
multi-view, and spatial), significantly elevating the challenge level. To
obtain NuInstruct, we propose a novel SQL-based method to generate
instruction-response pairs automatically, which is inspired by the driving
logical progression of humans. We further present BEV-InMLLM, an end-to-end
method for efficiently deriving instruction-aware Bird's-Eye-View (BEV)
features, language-aligned for large language models. BEV-InMLLM integrates
multi-view, spatial awareness, and temporal semantics to enhance MLLMs'
capabilities on NuInstruct tasks. Moreover, our proposed BEV injection module
is a plug-and-play method for existing MLLMs. Our experiments on NuInstruct
demonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.
around 9% improvement on various tasks. We plan to release our NuInstruct for
future research development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00989">Diversity-aware Buffer for Coping with Temporally Correlated Data Streams in Online Test-time Adaptation. (arXiv:2401.00989v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1">Mario D&#xf6;bler</a>, <a href="http://arxiv.org/find/cs/1/au:+Marencke_F/0/1/0/all/0/1">Florian Marencke</a>, <a href="http://arxiv.org/find/cs/1/au:+Marsden_R/0/1/0/all/0/1">Robert A. Marsden</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a></p>
<p>Since distribution shifts are likely to occur after a model's deployment and
can drastically decrease the model's performance, online test-time adaptation
(TTA) continues to update the model during test-time, leveraging the current
test data. In real-world scenarios, test data streams are not always
independent and identically distributed (i.i.d.). Instead, they are frequently
temporally correlated, making them non-i.i.d. Many existing methods struggle to
cope with this scenario. In response, we propose a diversity-aware and
category-balanced buffer that can simulate an i.i.d. data stream, even in
non-i.i.d. scenarios. Combined with a diversity and entropy-weighted entropy
loss, we show that a stable adaptation is possible on a wide range of
corruptions and natural domain shifts, based on ImageNet. We achieve
state-of-the-art results on most considered benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01002">AI Mobile Application for Archaeological Dating of Bronze Dings. (arXiv:2401.01002v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chuntao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_R/0/1/0/all/0/1">Ruihua Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jiafu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Rixin Zhou</a></p>
<p>We develop an AI application for archaeological dating of bronze Dings. A
classification model is employed to predict the period of the input Ding, and a
detection model is used to show the feature parts for making a decision of
archaeological dating. To train the two deep learning models, we collected a
large number of Ding images from published materials, and annotated the period
and the feature parts on each image by archaeological experts. Furthermore, we
design a user system and deploy our pre-trained models based on the platform of
WeChat Mini Program for ease of use. Only need a smartphone installed WeChat
APP, users can easily know the result of intelligent archaeological dating, the
feature parts, and other reference artifacts, by taking a photo of a bronze
Ding. To use our application, please scan this QR code by WeChat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01003">Rink-Agnostic Hockey Rink Registration. (arXiv:2401.01003v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jia Cheng Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1">Mohammad Javad Shafiee</a>, <a href="http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1">David A. Clausi</a></p>
<p>Hockey rink registration is a useful tool for aiding and automating sports
analysis. When combined with player tracking, it can provide location
information of players on the rink by estimating a homography matrix that can
warp broadcast video frames onto an overhead template of the rink, or vice
versa. However, most existing techniques require accurate ground truth
information, which can take many hours to annotate, and only work on the
trained rink types. In this paper, we propose a generalized rink registration
pipeline that, once trained, can be applied to both seen and unseen rink types
with only an overhead rink template and the video frame as inputs. Our pipeline
uses domain adaptation techniques, semi-supervised learning, and synthetic data
during training to achieve this ability and overcome the lack of non-NHL
training data. The proposed method is evaluated on both NHL (source) and
non-NHL (target) rink data and the results demonstrate that our approach can
generalize to non-NHL rinks, while maintaining competitive performance on NHL
rinks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01008">Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hunter_R/0/1/0/all/0/1">Rosco Hunter</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1">&#x141;ukasz Dudziak</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1">Mohamed S. Abdelfattah</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Abhinav Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sourav Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongkai Wen</a></p>
<p>Text-to-image diffusion models have demonstrated unprecedented abilities at
flexible and realistic image synthesis. However, the iterative process required
to produce a single image is costly and incurs a high latency, prompting
researchers to further investigate its efficiency. Typically, improvements in
latency have been achieved in two ways: (1) training smaller models through
knowledge distillation (KD); and (2) adopting techniques from ODE-theory to
facilitate larger step sizes. In contrast, we propose a training-free approach
that does not alter the step-size of the sampler. Specifically, we find the
repeated calculation of attention maps to be both costly and redundant;
therefore, we propose a structured reuse of attention maps during sampling. Our
initial reuse policy is motivated by rudimentary ODE-theory, which suggests
that reuse is most suitable late in the sampling procedure. After noting a
number of limitations in this theoretical approach, we empirically search for a
better policy. Unlike methods that rely on KD, our reuse policies can easily be
adapted to a variety of setups in a plug-and-play manner. Furthermore, when
applied to Stable Diffusion-1.5, our reuse policies reduce latency with minimal
repercussions on sample quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01010">Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt. (arXiv:2401.01010v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1">Qiang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Ying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1">Bin-Bin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a></p>
<p>Unsupervised Anomaly Detection (UAD) with incremental training is crucial in
industrial manufacturing, as unpredictable defects make obtaining sufficient
labeled data infeasible. However, continual learning methods primarily rely on
supervised annotations, while the application in UAD is limited due to the
absence of supervision. Current UAD methods train separate models for different
classes sequentially, leading to catastrophic forgetting and a heavy
computational burden. To address this issue, we introduce a novel Unsupervised
Continual Anomaly Detection framework called UCAD, which equips the UAD with
continual learning capability through contrastively-learned prompts. In the
proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a
concise key-prompt-knowledge memory bank to guide task-invariant `anomaly'
model predictions using task-specific `normal' knowledge. Moreover,
Structure-based Contrastive Learning (SCL) is designed with the Segment
Anything Model (SAM) to improve prompt learning and anomaly segmentation
results. Specifically, by treating SAM's masks as structure, we draw features
within the same mask closer and push others apart for general feature
representations. We conduct comprehensive experiments and set the benchmark on
unsupervised continual anomaly detection and segmentation, demonstrating that
our method is significantly better than anomaly detection methods, even with
rehearsal training. The code will be available at
https://github.com/shirowalker/UCAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01018">Small Bird Detection using YOLOv7 with Test-Time Augmentation. (arXiv:2401.01018v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shigematsu_K/0/1/0/all/0/1">Kosuke Shigematsu</a></p>
<p>In this paper, we propose a method specifically aimed at improving small bird
detection for the Small Object Detection Challenge for Spotting Birds 2023.
Utilizing YOLOv7 model with test-time augmentation, our approach involves
increasing the input resolution, incorporating multiscale inference,
considering flipped images during the inference process, and employing weighted
boxes fusion to merge detection results. We rigorously explore the impact of
each technique on detection performance. Experimental results demonstrate
significant improvements in detection accuracy. Our method achieved a top score
in the Development category, with a public AP of 0.732 and a private AP of
27.2, both at IoU=0.5.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01021">Class Relevance Learning For Out-of-distribution Detection. (arXiv:2401.01021v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1">Butian Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Liguang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1">Tin Lun Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yangsheng Xu</a></p>
<p>Image classification plays a pivotal role across diverse applications, yet
challenges persist when models are deployed in real-world scenarios. Notably,
these models falter in detecting unfamiliar classes that were not incorporated
during classifier training, a formidable hurdle for safe and effective
real-world model deployment, commonly known as out-of-distribution (OOD)
detection. While existing techniques, like max logits, aim to leverage logits
for OOD identification, they often disregard the intricate interclass
relationships that underlie effective detection. This paper presents an
innovative class relevance learning method tailored for OOD detection. Our
method establishes a comprehensive class relevance learning framework,
strategically harnessing interclass relationships within the OOD pipeline. This
framework significantly augments OOD detection capabilities. Extensive
experimentation on diverse datasets, encompassing generic image classification
datasets (Near OOD and Far OOD datasets), demonstrates the superiority of our
method over state-of-the-art alternatives for OOD detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01032">A Comparison of Bounding Box and Landmark Detection Methods for Video-Based Heart Rate Estimation. (arXiv:2401.01032v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1">Laurence Liang</a></p>
<p>Remote Photoplethysmography (rPPG) uses the cyclic variation of skin tone on
a person's forehead region to estimate that person's heart rate. This paper
compares two methods: a bounding box-based method and a
landmark-detection-based method to estimate heart rate, and discovered that the
landmark-based approach has a smaller variance in terms of model results with a
standard deviation that is more than 4 times smaller (4.171 compared to
18.720).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01035">Online Continual Domain Adaptation for Semantic Image Segmentation Using Internal Representations. (arXiv:2401.01035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1">Serban Stan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a></p>
<p>Semantic segmentation models trained on annotated data fail to generalize
well when the input data distribution changes over extended time period,
leading to requiring re-training to maintain performance. Classic Unsupervised
domain adaptation (UDA) attempts to address a similar problem when there is
target domain with no annotated data points through transferring knowledge from
a source domain with annotated data. We develop an online UDA algorithm for
semantic segmentation of images that improves model generalization on
unannotated domains in scenarios where source data access is restricted during
adaptation. We perform model adaptation is by minimizing the distributional
distance between the source latent features and the target features in a shared
embedding space. Our solution promotes a shared domain-agnostic latent feature
space between the two domains, which allows for classifier generalization on
the target dataset. To alleviate the need of access to source samples during
adaptation, we approximate the source latent feature distribution via an
appropriate surrogate distribution, in this case a Gassian mixture model (GMM).
We evaluate our approach on well established semantic segmentation datasets and
demonstrate it compares favorably against state-of-the-art (SOTA) UDA semantic
segmentation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01042">Relating Events and Frames Based on Self-Supervised Learning and Uncorrelated Conditioning for Unsupervised Domain Adaptation. (arXiv:2401.01042v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a>, <a href="http://arxiv.org/find/cs/1/au:+Jian_D/0/1/0/all/0/1">Dayuan Jian</a></p>
<p>Event-based cameras provide accurate and high temporal resolution
measurements for performing computer vision tasks in challenging scenarios,
such as high-dynamic range environments and fast-motion maneuvers. Despite
their advantages, utilizing deep learning for event-based vision encounters a
significant obstacle due to the scarcity of annotated data caused by the
relatively recent emergence of event-based cameras. To overcome this
limitation, leveraging the knowledge available from annotated data obtained
with conventional frame-based cameras presents an effective solution based on
unsupervised domain adaptation. We propose a new algorithm tailored for
adapting a deep neural network trained on annotated frame-based data to
generalize well on event-based unannotated data. Our approach incorporates
uncorrelated conditioning and self-supervised learning in an adversarial
learning scheme to close the gap between the two source and target domains. By
applying self-supervised learning, the algorithm learns to align the
representations of event-based data with those from frame-based camera data,
thereby facilitating knowledge transfer.Furthermore, the inclusion of
uncorrelated conditioning ensures that the adapted model effectively
distinguishes between event-based and conventional data, enhancing its ability
to classify event-based images accurately.Through empirical experimentation and
evaluation, we demonstrate that our algorithm surpasses existing approaches
designed for the same purpose using two benchmarks. The superior performance of
our solution is attributed to its ability to effectively utilize annotated data
from frame-based cameras and transfer the acquired knowledge to the event-based
vision domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01065">BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Dafeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhengyu Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Changwei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1">Chengkai Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1">Peng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1">Kun Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jingchen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yixing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a></p>
<p>The demand for the retrieval of complex scene data in autonomous driving is
increasing, especially as passenger vehicles have been equipped with the
ability to navigate urban settings, with the imperative to address long-tail
scenarios. Meanwhile, under the pre-existing two dimensional image retrieval
method, some problems may arise with scene retrieval, such as lack of global
feature representation and subpar text retrieval ability. To address these
issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-Eye
View(BEV) retrieval methodology that utilizes descriptive text as an input to
retrieve corresponding scenes. This methodology applies the semantic feature
extraction abilities of a large language model (LLM) to facilitate zero-shot
retrieval of extensive text descriptions, and incorporates semi-structured
information from a knowledge graph to improve the semantic richness and variety
of the language embedding. Our experiments result in 87.66% accuracy on
NuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases in
our paper support that our retrieval method is also indicated to be effective
in identifying certain long-tail corner scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01066">DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in Nighttime Semantic Segmentation. (arXiv:2401.01066v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fanding Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zihao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wenhui Zhou</a></p>
<p>Due to the poor illumination and the difficulty in annotating, nighttime
conditions pose a significant challenge for autonomous vehicle perception
systems. Unsupervised domain adaptation (UDA) has been widely applied to
semantic segmentation on such images to adapt models from normal conditions to
target nighttime-condition domains. Self-training (ST) is a paradigm in UDA,
where a momentum teacher is utilized for pseudo-label prediction, but a
confirmation bias issue exists. Because the one-directional knowledge transfer
from a single teacher is insufficient to adapt to a large domain shift. To
mitigate this issue, we propose to alleviate domain gap by incrementally
considering style influence and illumination change. Therefore, we introduce a
one-stage Dual-Teacher Bi-directional Self-training (DTBS) framework for smooth
knowledge transfer and feedback. Based on two teacher models, we present a
novel pipeline to respectively decouple style and illumination shift. In
addition, we propose a new Re-weight exponential moving average (EMA) to merge
the knowledge of style and illumination factors, and provide feedback to the
student model. In this way, our method can be embedded in other UDA methods to
enhance their performance. For example, the Cityscapes to ACDC night task
yielded 53.8 mIoU (\%), which corresponds to an improvement of +5\% over the
previous state-of-the-art. The code is available at
\url{https://github.com/hf618/DTBS}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01074">AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis. (arXiv:2401.01074v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qiuhui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yi Hong</a></p>
<p>Medical data collected for making a diagnostic decision are typically
multi-modal and provide complementary perspectives of a subject. A
computer-aided diagnosis system welcomes multi-modal inputs; however, how to
effectively fuse such multi-modal data is a challenging task and attracts a lot
of attention in the medical research field. In this paper, we propose a
transformer-based framework, called Alifuse, for aligning and fusing
multi-modal medical data. Specifically, we convert images and unstructured and
structured texts into vision and language tokens, and use intramodal and
intermodal attention mechanisms to learn holistic representations of all
imaging and non-imaging data for classification. We apply Alifuse to classify
Alzheimer's disease and obtain state-of-the-art performance on five public
datasets, by outperforming eight baselines. The source code will be available
online later.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01075">Depth-discriminative Metric Learning for Monocular 3D Object Detection. (arXiv:2401.01075v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1">Wonhyeok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Mingyu Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sunghoon Im</a></p>
<p>Monocular 3D object detection poses a significant challenge due to the lack
of depth information in RGB images. Many existing methods strive to enhance the
object depth estimation performance by allocating additional parameters for
object depth estimation, utilizing extra modules or data. In contrast, we
introduce a novel metric learning scheme that encourages the model to extract
depth-discriminative features regardless of the visual attributes without
increasing inference time and model size. Our method employs the
distance-preserving function to organize the feature space manifold in relation
to ground-truth object depth. The proposed (K, B, eps)-quasi-isometric loss
leverages predetermined pairwise distance restriction as guidance for adjusting
the distance among object descriptors without disrupting the non-linearity of
the natural feature manifold. Moreover, we introduce an auxiliary head for
object-wise depth estimation, which enhances depth quality while maintaining
the inference time. The broad applicability of our method is demonstrated
through experiments that show improvements in overall performance when
integrated into various baselines. The results show that our method
consistently improves the performance of various baselines by 23.51% and 5.78%
on average across KITTI and Waymo, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01093">Exploring Hyperspectral Anomaly Detection with Human Vision: A Small Target Aware Detector. (arXiv:2401.01093v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jitao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a></p>
<p>Hyperspectral anomaly detection (HAD) aims to localize pixel points whose
spectral features differ from the background. HAD is essential in scenarios of
unknown or camouflaged target features, such as water quality monitoring, crop
growth monitoring and camouflaged target detection, where prior information of
targets is difficult to obtain. Existing HAD methods aim to objectively detect
and distinguish background and anomalous spectra, which can be achieved almost
effortlessly by human perception. However, the underlying processes of human
visual perception are thought to be quite complex. In this paper, we analyze
hyperspectral image (HSI) features under human visual perception, and transfer
the solution process of HAD to the more robust feature space for the first
time. Specifically, we propose a small target aware detector (STAD), which
introduces saliency maps to capture HSI features closer to human visual
perception. STAD not only extracts more anomalous representations, but also
reduces the impact of low-confidence regions through a proposed small target
filter (STF). Furthermore, considering the possibility of HAD algorithms being
applied to edge devices, we propose a full connected network to convolutional
network knowledge distillation strategy. It can learn the spectral and spatial
features of the HSI while lightening the network. We train the network on the
HAD100 training set and validate the proposed method on the HAD100 test set.
Our method provides a new solution space for HAD that is closer to human visual
perception with high confidence. Sufficient experiments on real HSI with
multiple method comparisons demonstrate the excellent performance and unique
potential of the proposed method. The code is available at
https://github.com/majitao-xd/STAD-HAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01097">Robust single-particle cryo-EM image denoising and restoration. (arXiv:2401.01097v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tengfei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">ShiYu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xin Zhao</a></p>
<p>Cryo-electron microscopy (cryo-EM) has achieved near-atomic level resolution
of biomolecules by reconstructing 2D micrographs. However, the resolution and
accuracy of the reconstructed particles are significantly reduced due to the
extremely low signal-to-noise ratio (SNR) and complex noise structure of
cryo-EM images. In this paper, we introduce a diffusion model with
post-processing framework to effectively denoise and restore single particle
cryo-EM images. Our method outperforms the state-of-the-art (SOTA) denoising
methods by effectively removing structural noise that has not been addressed
before. Additionally, more accurate and high-resolution three-dimensional
reconstruction structures can be obtained from denoised cryo-EM images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01102">Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing. (arXiv:2401.01102v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1">Zhe Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuexiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiaoying Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Wenhan Luo</a></p>
<p>Face recognition systems have raised concerns due to their vulnerability to
different presentation attacks, and system security has become an increasingly
critical concern. Although many face anti-spoofing (FAS) methods perform well
in intra-dataset scenarios, their generalization remains a challenge. To
address this issue, some methods adopt domain adversarial training (DAT) to
extract domain-invariant features. However, the competition between the encoder
and the domain discriminator can cause the network to be difficult to train and
converge. In this paper, we propose a domain adversarial attack (DAA) method to
mitigate the training instability problem by adding perturbations to the input
images, which makes them indistinguishable across domains and enables domain
alignment. Moreover, since models trained on limited data and types of attacks
cannot generalize well to unknown attacks, we propose a dual perceptual and
generative knowledge distillation framework for face anti-spoofing that
utilizes pre-trained face-related models containing rich face priors.
Specifically, we adopt two different face-related models as teachers to
transfer knowledge to the target student model. The pre-trained teacher models
are not from the task of face anti-spoofing but from perceptual and generative
tasks, respectively, which implicitly augment the data. By combining both DAA
and dual-teacher knowledge distillation, we develop a dual teacher knowledge
distillation with domain alignment framework (DTDA) for face anti-spoofing. The
advantage of our proposed method has been verified through extensive ablation
studies and comparison with state-of-the-art methods on public datasets across
multiple protocols.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01107">CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series. (arXiv:2401.01107v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tianyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zejia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jackelyn Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1">Ram Rajagopal</a></p>
<p>Urban transformations have profound societal impact on both individuals and
communities at large. Accurately assessing these shifts is essential for
understanding their underlying causes and ensuring sustainable urban planning.
Traditional measurements often encounter constraints in spatial and temporal
granularity, failing to capture real-time physical changes. While street view
imagery, capturing the heartbeat of urban spaces from a pedestrian point of
view, can add as a high-definition, up-to-date, and on-the-ground visual proxy
of urban change. We curate the largest street view time series dataset to date,
and propose an end-to-end change detection model to effectively capture
physical alterations in the built environment at scale. We demonstrate the
effectiveness of our proposed method by benchmark comparisons with previous
literature and implementing it at the city-wide level. Our approach has the
potential to supplement existing dataset and serve as a fine-grained and
accurate assessment of urban change.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01117">Q-Refine: A Perceptual Quality Refiner for AI-Generated Image. (arXiv:2401.01117v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1">Hongkun Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1">Lei Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaohong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weisi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a></p>
<p>With the rapid evolution of the Text-to-Image (T2I) model in recent years,
their unsatisfactory generation result has become a challenge. However,
uniformly refining AI-Generated Images (AIGIs) of different qualities not only
limited optimization capabilities for low-quality AIGIs but also brought
negative optimization to high-quality AIGIs. To address this issue, a
quality-award refiner named Q-Refine is proposed. Based on the preference of
the Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA)
metric to guide the refining process for the first time, and modify images of
different qualities through three adaptive pipelines. Experimental shows that
for mainstream T2I models, Q-Refine can perform effective optimization to AIGIs
of different qualities. It can be a general refiner to optimize AIGIs from both
fidelity and aesthetic quality levels, thus expanding the application of the
T2I generation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01128">SSP: A Simple and Safe automatic Prompt engineering method towards realistic image synthesis on LVM. (arXiv:2401.01128v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Weijin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianzhi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jiawen Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1">Fuji Ren</a></p>
<p>Recently, text-to-image (T2I) synthesis has undergone significant
advancements, particularly with the emergence of Large Language Models (LLM)
and their enhancement in Large Vision Models (LVM), greatly enhancing the
instruction-following capabilities of traditional T2I models. Nevertheless,
previous methods focus on improving generation quality but introduce unsafe
factors into prompts. We explore that appending specific camera descriptions to
prompts can enhance safety performance. Consequently, we propose a simple and
safe prompt engineering method (SSP) to improve image generation quality by
providing optimal camera descriptions. Specifically, we create a dataset from
multi-datasets as original prompts. To select the optimal camera, we design an
optimal camera matching approach and implement a classifier for original
prompts capable of automatically matching. Appending camera descriptions to
original prompts generates optimized prompts for further LVM image generation.
Experiments demonstrate that SSP improves semantic consistency by an average of
16% compared to others and safety metrics by 48.9%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01130">Joint Generative Modeling of Scene Graphs and Images via Diffusion Models. (arXiv:2401.01130v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bicheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Renjie Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lele Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1">Leonid Sigal</a></p>
<p>In this paper, we present a novel generative task: joint scene graph - image
generation. While previous works have explored image generation conditioned on
scene graphs or layouts, our task is distinctive and important as it involves
generating scene graphs themselves unconditionally from noise, enabling
efficient and interpretable control for image generation. Our task is
challenging, requiring the generation of plausible scene graphs with
heterogeneous attributes for nodes (objects) and edges (relations among
objects), including continuous object bounding boxes and discrete object and
relation categories. We introduce a novel diffusion model, DiffuseSG, that
jointly models the adjacency matrix along with heterogeneous node and edge
attributes. We explore various types of encodings for the categorical data,
relaxing it into a continuous space. With a graph transformer being the
denoiser, DiffuseSG successively denoises the scene graph representation in a
continuous space and discretizes the final representation to generate the clean
scene graph. Additionally, we introduce an IoU regularization to enhance the
empirical performance. Our model significantly outperforms existing methods in
scene graph generation on the Visual Genome and COCO-Stuff datasets, both on
standard and newly introduced metrics that better capture the problem
complexity. Moreover, we demonstrate the additional benefits of our model in
two downstream applications: 1) excelling in a series of scene graph completion
tasks, and 2) improving scene graph detection models by using extra training
samples generated from DiffuseSG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01134">Hybrid Pooling and Convolutional Network for Improving Accuracy and Training Convergence Speed in Object Detection. (arXiv:2401.01134v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shiwen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hai Wu</a></p>
<p>This paper introduces HPC-Net, a high-precision and rapidly convergent object
detection network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01160">Train-Free Segmentation in MRI with Cubical Persistent Homology. (arXiv:2401.01160v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Francois_A/0/1/0/all/0/1">Anton Fran&#xe7;ois</a>, <a href="http://arxiv.org/find/eess/1/au:+Tinarrage_R/0/1/0/all/0/1">Rapha&#xeb;l Tinarrage</a></p>
<p>We describe a new general method for segmentation in MRI scans using
Topological Data Analysis (TDA), offering several advantages over traditional
machine learning approaches. It works in three steps, first identifying the
whole object to segment via automatic thresholding, then detecting a
distinctive subset whose topology is known in advance, and finally deducing the
various components of the segmentation. Although convoking classical ideas of
TDA, such an algorithm has never been proposed separately from deep learning
methods. To achieve this, our approach takes into account, in addition to the
homology of the image, the localization of representative cycles, a piece of
information that seems never to have been exploited in this context. In
particular, it offers the ability to perform segmentation without the need for
large annotated data sets. TDA also provides a more interpretable and stable
framework for segmentation by explicitly mapping topological features to
segmentation components. By adapting the geometric object to be detected, the
algorithm can be adjusted to a wide range of data segmentation challenges. We
carefully study the examples of glioblastoma segmentation in brain MRI, where a
sphere is to be detected, as well as myocardium in cardiac MRI, involving a
cylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are
circles. We compare our method to state-of-the-art algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01163">NU-Class Net: A Novel Deep Learning-based Approach for Video Quality Enhancement. (arXiv:2401.01163v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moghaddam_P/0/1/0/all/0/1">Parham Zilouchian Moghaddam</a>, <a href="http://arxiv.org/find/cs/1/au:+Modarressi_M/0/1/0/all/0/1">Mehdi Modarressi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1">MohammadAmin Sadeghi</a></p>
<p>Video content has experienced a surge in popularity, asserting its dominance
over internet traffic and Internet of Things (IoT) networks. Video compression
has long been regarded as the primary means of efficiently managing the
substantial multimedia traffic generated by video-capturing devices.
Nevertheless, video compression algorithms entail significant computational
demands in order to achieve substantial compression ratios. This complexity
presents a formidable challenge when implementing efficient video coding
standards in resource-constrained embedded systems, such as IoT edge node
cameras. To tackle this challenge, this paper introduces NU-Class Net, an
innovative deep-learning model designed to mitigate compression artifacts
stemming from lossy compression codecs. This enhancement significantly elevates
the perceptible quality of low-bit-rate videos. By employing the NU-Class Net,
the video encoder within the video-capturing node can reduce output quality,
thereby generating low-bit-rate videos and effectively curtailing both
computation and bandwidth requirements at the edge. On the decoder side, which
is typically less encumbered by resource limitations, NU-Class Net is applied
after the video decoder to compensate for artifacts and approximate the quality
of the original video. Experimental results affirm the efficacy of the proposed
model in enhancing the perceptible quality of videos, especially those streamed
at low bit rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01164">Distilling Local Texture Features for Colorectal Tissue Classification in Low Data Regimes. (arXiv:2401.01164v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Demidov_D/0/1/0/all/0/1">Dmitry Demidov</a>, <a href="http://arxiv.org/find/cs/1/au:+Majzoub_R/0/1/0/all/0/1">Roba Al Majzoub</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Amandeep Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Khan</a></p>
<p>Multi-class colorectal tissue classification is a challenging problem that is
typically addressed in a setting, where it is assumed that ample amounts of
training data is available. However, manual annotation of fine-grained
colorectal tissue samples of multiple classes, especially the rare ones like
stromal tumor and anal cancer is laborious and expensive. To address this, we
propose a knowledge distillation-based approach, named KD-CTCNet, that
effectively captures local texture information from few tissue samples, through
a distillation loss, to improve the standard CNN features. The resulting
enriched feature representation achieves improved classification performance
specifically in low data regimes. Extensive experiments on two public datasets
of colorectal tissues reveal the merits of the proposed contributions, with a
consistent gain achieved over different approaches across low data settings.
The code and models are publicly available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01173">En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data. (arXiv:2401.01173v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Men_Y/0/1/0/all/0/1">Yifang Men</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Biwen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Miaomiao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zhouhui Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a></p>
<p>We present En3D, an enhanced generative scheme for sculpting high-quality 3D
human avatars. Unlike previous works that rely on scarce 3D datasets or limited
2D collections with imbalanced viewing angles and imprecise pose priors, our
approach aims to develop a zero-shot 3D generative scheme capable of producing
visually realistic, geometrically accurate and content-wise diverse 3D humans
without relying on pre-existing 3D or 2D assets. To address this challenge, we
introduce a meticulously crafted workflow that implements accurate physical
modeling to learn the enhanced 3D generative model from synthetic 2D data.
During inference, we integrate optimization modules to bridge the gap between
realistic appearances and coarse 3D shapes. Specifically, En3D comprises three
modules: a 3D generator that accurately models generalizable 3D humans with
realistic appearance from synthesized balanced, diverse, and structured human
images; a geometry sculptor that enhances shape quality using multi-view normal
constraints for intricate human anatomy; and a texturing module that
disentangles explicit texture maps with fidelity and editability, leveraging
semantical UV partitioning and a differentiable rasterizer. Experimental
results show that our approach significantly outperforms prior works in terms
of image quality, geometry accuracy and content diversity. We also showcase the
applicability of our generated avatars for animation and editing, as well as
the scalability of our approach for content-style free adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01175">Learning Surface Scattering Parameters From SAR Images Using Differentiable Ray Tracing. (arXiv:2401.01175v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jiangtao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Luomei_Y/0/1/0/all/0/1">Yixiang Luomei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a></p>
<p>Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex
scenes has consistently presented a significant research challenge. The
development of a microwave-domain surface scattering model and its
reversibility are poised to play a pivotal role in enhancing the authenticity
of SAR image simulations and facilitating the reconstruction of target
parameters. Drawing inspiration from the field of computer graphics, this paper
proposes a surface microwave rendering model that comprehensively considers
both Specular and Diffuse contributions. The model is analytically represented
by the coherent spatially varying bidirectional scattering distribution
function (CSVBSDF) based on the Kirchhoff approximation (KA) and the
perturbation method (SPM). And SAR imaging is achieved through the synergistic
combination of ray tracing and fast mapping projection techniques. Furthermore,
a differentiable ray tracing (DRT) engine based on SAR images was constructed
for CSVBSDF surface scattering parameter learning. Within this SAR image
simulation engine, the use of differentiable reverse ray tracing enables the
rapid estimation of parameter gradients from SAR images. The effectiveness of
this approach has been validated through simulations and comparisons with real
SAR images. By learning the surface scattering parameters, substantial
enhancements in SAR image simulation performance under various observation
conditions have been demonstrated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01178">GBSS:a global building semantic segmentation dataset for large-scale remote sensing building extraction. (arXiv:2401.01178v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yuping Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiayi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhen Zhang</a></p>
<p>Semantic segmentation techniques for extracting building footprints from
high-resolution remote sensing images have been widely used in many fields such
as urban planning. However, large-scale building extraction demands higher
diversity in training samples. In this paper, we construct a Global Building
Semantic Segmentation (GBSS) dataset (The dataset will be released), which
comprises 116.9k pairs of samples (about 742k buildings) from six continents.
There are significant variations of building samples in terms of size and
style, so the dataset can be a more challenging benchmark for evaluating the
generalization and robustness of building semantic segmentation models. We
validated through quantitative and qualitative comparisons between different
datasets, and further confirmed the potential application in the field of
transfer learning by conducting experiments on subsets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01179">Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jiuming Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Sibo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1">Rossella Arcucci</a></p>
<p>Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01180">Accurate and Efficient Urban Street Tree Inventory with Deep Learning on Mobile Phone Imagery. (arXiv:2401.01180v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Asim Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawaz_U/0/1/0/all/0/1">Umair Nawaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ulhaq_A/0/1/0/all/0/1">Anwaar Ulhaq</a>, <a href="http://arxiv.org/find/cs/1/au:+Gondal_I/0/1/0/all/0/1">Iqbal Gondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1">Sajid Javed</a></p>
<p>Deforestation, a major contributor to climate change, poses detrimental
consequences such as agricultural sector disruption, global warming, flash
floods, and landslides. Conventional approaches to urban street tree inventory
suffer from inaccuracies and necessitate specialised equipment. To overcome
these challenges, this paper proposes an innovative method that leverages deep
learning techniques and mobile phone imaging for urban street tree inventory.
Our approach utilises a pair of images captured by smartphone cameras to
accurately segment tree trunks and compute the diameter at breast height (DBH).
Compared to traditional methods, our approach exhibits several advantages,
including superior accuracy, reduced dependency on specialised equipment, and
applicability in hard-to-reach areas. We evaluated our method on a
comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with
an error rate of less than 2.5%. Our method holds significant potential for
substantially improving forest management practices. By enhancing the accuracy
and efficiency of tree inventory, our model empowers urban management to
mitigate the adverse effects of deforestation and climate change.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01181">Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label Classification. (arXiv:2401.01181v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xuelin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1">Dongqi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiawei Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiuxin Cao</a></p>
<p>Identifying labels that did not appear during training, known as multi-label
zero-shot learning, is a non-trivial task in computer vision. To this end,
recent studies have attempted to explore the multi-modal knowledge of
vision-language pre-training (VLP) models by knowledge distillation, allowing
to recognize unseen labels in an open-vocabulary manner. However, experimental
evidence shows that knowledge distillation is suboptimal and provides limited
performance gain in unseen label prediction. In this paper, a novel query-based
knowledge sharing paradigm is proposed to explore the multi-modal knowledge
from the pretrained VLP model for open-vocabulary multi-label classification.
Specifically, a set of learnable label-agnostic query tokens is trained to
extract critical vision knowledge from the input image, and further shared
across all labels, allowing them to select tokens of interest as visual clues
for recognition. Besides, we propose an effective prompt pool for robust label
embedding, and reformulate the standard ranking learning into a form of
classification to allow the magnitude of feature vectors for matching, which
both significantly benefit label recognition. Experimental results show that
our framework significantly outperforms state-of-the-art methods on zero-shot
task by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01199">JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example. (arXiv:2401.01199v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1">Benedetta Tondi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1">Mauro Barni</a></p>
<p>Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01200">Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms. (arXiv:2401.01200v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loss_F/0/1/0/all/0/1">Flavio P. Loss</a>, <a href="http://arxiv.org/find/cs/1/au:+Cunha_P/0/1/0/all/0/1">Pedro H. da Cunha</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocha_M/0/1/0/all/0/1">Matheus B. Rocha</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanoni_M/0/1/0/all/0/1">Madson Poltronieri Zanoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Lima_L/0/1/0/all/0/1">Leandro M. de Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Nascimento_I/0/1/0/all/0/1">Isadora Tavares Nascimento</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezende_I/0/1/0/all/0/1">Isabella Rezende</a>, <a href="http://arxiv.org/find/cs/1/au:+Canuto_T/0/1/0/all/0/1">Tania R. P. Canuto</a>, <a href="http://arxiv.org/find/cs/1/au:+Vieira_L/0/1/0/all/0/1">Luciana de Paula Vieira</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossoni_R/0/1/0/all/0/1">Renan Rossoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1">Maria C. S. Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Frasson_P/0/1/0/all/0/1">Patricia Lyra Frasson</a>, <a href="http://arxiv.org/find/cs/1/au:+Romao_W/0/1/0/all/0/1">Wanderson Rom&#xe3;o</a>, <a href="http://arxiv.org/find/cs/1/au:+Filgueiras_P/0/1/0/all/0/1">Paulo R. Filgueiras</a>, <a href="http://arxiv.org/find/cs/1/au:+Krohling_R/0/1/0/all/0/1">Renato A. Krohling</a></p>
<p>Skin lesions are classified in benign or malignant. Among the malignant,
melanoma is a very aggressive cancer and the major cause of deaths. So, early
diagnosis of skin cancer is very desired. In the last few years, there is a
growing interest in computer aided diagnostic (CAD) using most image and
clinical data of the lesion. These sources of information present limitations
due to their inability to provide information of the molecular structure of the
lesion. NIR spectroscopy may provide an alternative source of information to
automated CAD of skin lesions. The most commonly used techniques and
classification algorithms used in spectroscopy are Principal Component Analysis
(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support
Vector Machines (SVM). Nonetheless, there is a growing interest in applying the
modern techniques of machine and deep learning (MDL) to spectroscopy. One of
the main limitations to apply MDL to spectroscopy is the lack of public
datasets. Since there is no public dataset of NIR spectral data to skin
lesions, as far as we know, an effort has been made and a new dataset named
NIR-SC-UFES, has been collected, annotated and analyzed generating the
gold-standard for classification of NIR spectral data to skin cancer. Next, the
machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional
neural network (1D-CNN) were investigated to classify cancer and non-cancer
skin lesions. Experimental results indicate the best performance obtained by
LightGBM with pre-processing using standard normal variate (SNV), feature
extraction providing values of 0.839 for balanced accuracy, 0.851 for recall,
0.852 for precision, and 0.850 for F-score. The obtained results indicate the
first steps in CAD of skin lesions aiming the automated triage of patients with
skin lesions in vivo using NIR spectral data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01201">Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans. (arXiv:2401.01201v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venturini_L/0/1/0/all/0/1">Lorenzo Venturini</a>, <a href="http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1">Samuel Budd</a>, <a href="http://arxiv.org/find/cs/1/au:+Farruggia_A/0/1/0/all/0/1">Alfonso Farruggia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_R/0/1/0/all/0/1">Robert Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1">Jacqueline Matthew</a>, <a href="http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1">Thomas G. Day</a>, <a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1">Bernhard Kainz</a>, <a href="http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1">Reza Razavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajnal_J/0/1/0/all/0/1">Jo V. Hajnal</a></p>
<p>The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images. In this
paper, we introduce a paradigm shift that attains human-level performance in
biometric measurement by aggregating automatically extracted biometrics from
every frame across an entire scan, with no need for operator intervention. We
use a convolutional neural network to classify each frame of an ultrasound
video recording. We then measure fetal biometrics in every frame where
appropriate anatomy is visible. We use a Bayesian method to estimate the true
value of each biometric from a large number of measurements and
probabilistically reject outliers. We performed a retrospective experiment on
1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,
estimated fetal biometrics in those scans and compared our estimates to the
measurements sonographers took during the scan. Our method achieves human-level
performance in estimating fetal biometrics and estimates well-calibrated
credible intervals in which the true biometric value is expected to lie.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01207">Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation. (arXiv:2401.01207v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Renshuai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1">Bowen Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhipeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Changjie Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1">Tangjie Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xuan Cheng</a></p>
<p>In human-centric content generation, the pre-trained text-to-image models
struggle to produce user-wanted portrait images, which retain the identity of
individuals while exhibiting diverse expressions. This paper introduces our
efforts towards personalized face generation. To this end, we propose a novel
multi-modal face generation framework, capable of simultaneous
identity-expression control and more fine-grained expression synthesis. Our
expression control is so sophisticated that it can be specialized by the
fine-grained emotional vocabulary. We devise a novel diffusion model that can
undertake the task of simultaneously face swapping and reenactment. Due to the
entanglement of identity and expression, it's nontrivial to separately and
precisely control them in one framework, thus has not been explored yet. To
overcome this, we propose several innovative designs in the conditional
diffusion model, including balancing identity and expression encoder, improved
midpoint sampling, and explicitly background conditioning. Extensive
experiments have demonstrated the controllability and scalability of the
proposed framework, in comparison with state-of-the-art text-to-image, face
swapping, and face reenactment methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01208">FGENet: Fine-Grained Extraction Network for Congested Crowd Counting. (arXiv:2401.01208v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hao-Yuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiang-Yi Wei</a></p>
<p>Crowd counting has gained significant popularity due to its practical
applications. However, mainstream counting methods ignore precise individual
localization and suffer from annotation noise because of counting from
estimating density maps. Additionally, they also struggle with high-density
images.To address these issues, we propose an end-to-end model called
Fine-Grained Extraction Network (FGENet). Different from methods estimating
density maps, FGENet directly learns the original coordinate points that
represent the precise localization of individuals.This study designs a fusion
module, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature
maps extracted by the backbone of FGENet. The fused features are then passed to
both regression and classification heads, where the former provides predicted
point coordinates for a given image, and the latter determines the confidence
level for each predicted point being an individual. At the end, FGENet
establishes correspondences between prediction points and ground truth points
by employing the Hungarian algorithm. For training FGENet, we design a robust
loss function, named Three-Task Combination (TTC), to mitigate the impact of
annotation noise. Extensive experiments are conducted on four widely used crowd
counting datasets. Experimental results demonstrate the effectiveness of
FGENet. Notably, our method achieves a remarkable improvement of 3.14 points in
Mean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its
superiority over the existing state-of-the-art methods. Even more impressively,
FGENet surpasses previous benchmarks on the UCF\_CC\_50 dataset with an
astounding enhancement of 30.16 points in MAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01214">YOLO algorithm with hybrid attention feature pyramid network for solder joint defect detection. (arXiv:2401.01214v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ang_L/0/1/0/all/0/1">Li Ang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahim_S/0/1/0/all/0/1">Siti Khatijah Nor Abdul Rahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamzah_R/0/1/0/all/0/1">Raseeda Hamzah</a>, <a href="http://arxiv.org/find/cs/1/au:+Aminuddin_R/0/1/0/all/0/1">Raihah Aminuddin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousheng_G/0/1/0/all/0/1">Gao Yousheng</a></p>
<p>Traditional manual detection for solder joint defect is no longer applied
during industrial production due to low efficiency, inconsistent evaluation,
high cost and lack of real-time data. A new approach has been proposed to
address the issues of low accuracy, high false detection rates and
computational cost of solder joint defect detection in surface mount technology
of industrial scenarios. The proposed solution is a hybrid attention mechanism
designed specifically for the solder joint defect detection algorithm to
improve quality control in the manufacturing process by increasing the accuracy
while reducing the computational cost. The hybrid attention mechanism comprises
a proposed enhanced multi-head self-attention and coordinate attention
mechanisms increase the ability of attention networks to perceive contextual
information and enhances the utilization range of network features. The
coordinate attention mechanism enhances the connection between different
channels and reduces location information loss. The hybrid attention mechanism
enhances the capability of the network to perceive long-distance position
information and learn local features. The improved algorithm model has good
detection ability for solder joint defect detection, with mAP reaching 91.5%,
4.3% higher than the You Only Look Once version 5 algorithm and better than
other comparative algorithms. Compared to other versions, mean Average
Precision, Precision, Recall, and Frame per Seconds indicators have also
improved. The improvement of detection accuracy can be achieved while meeting
real-time detection requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01216">Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise. (arXiv:2401.01216v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qinglong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yanbin Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pengyuan Zhou</a></p>
<p>Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01219">Distribution Matching for Multi-Task Learning of Classification Tasks: a Large-Scale Study on Faces &amp; Beyond. (arXiv:2401.01219v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1">Dimitrios Kollias</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharmanska_V/0/1/0/all/0/1">Viktoriia Sharmanska</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1">Stefanos Zafeiriou</a></p>
<p>Multi-Task Learning (MTL) is a framework, where multiple related tasks are
learned jointly and benefit from a shared representation space, or parameter
transfer. To provide sufficient learning support, modern MTL uses annotated
data with full, or sufficiently large overlap across tasks, i.e., each input
sample is annotated for all, or most of the tasks. However, collecting such
annotations is prohibitive in many real applications, and cannot benefit from
datasets available for individual tasks. In this work, we challenge this setup
and show that MTL can be successful with classification tasks with little, or
non-overlapping annotations, or when there is big discrepancy in the size of
labeled data per task. We explore task-relatedness for co-annotation and
co-training, and propose a novel approach, where knowledge exchange is enabled
between the tasks via distribution matching. To demonstrate the general
applicability of our method, we conducted diverse case studies in the domains
of affective computing, face recognition, species recognition, and shopping
item classification using nine datasets. Our large-scale study of affective
tasks for basic expression recognition and facial action unit detection
illustrates that our approach is network agnostic and brings large performance
improvements compared to the state-of-the-art in both tasks and across all
studied databases. In all case studies, we show that co-training via
task-relatedness is advantageous and prevents negative transfer (which occurs
when MT model's performance is worse than that of at least one single-task
model).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01227">IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rabea_M/0/1/0/all/0/1">Mahmoud Rabea</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_H/0/1/0/all/0/1">Hanya Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoud_S/0/1/0/all/0/1">Sohaila Mahmoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Sayed_N/0/1/0/all/0/1">Nourhan Sayed</a></p>
<p>The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there's always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce "IdentiFace" which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01244">Temporal Adaptive RGBT Tracking with Modality Prompt. (arXiv:2401.01244v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaotao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Meng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1">Dian Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a></p>
<p>RGBT tracking has been widely used in various fields such as robotics,
surveillance processing, and autonomous driving. Existing RGBT trackers fully
explore the spatial information between the template and the search region and
locate the target based on the appearance matching results. However, these RGBT
trackers have very limited exploitation of temporal information, either
ignoring temporal information or exploiting it through online sampling and
training. The former struggles to cope with the object state changes, while the
latter neglects the correlation between spatial and temporal information. To
alleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking
framework, named as TATrack. TATrack has a spatio-temporal two-stream structure
and captures temporal information by an online updated template, where the
two-stream structure refers to the multi-modal feature extraction and
cross-modal interaction for the initial template and the online update template
respectively. TATrack contributes to comprehensively exploit spatio-temporal
information and multi-modal information for target localization. In addition,
we design a spatio-temporal interaction (STI) mechanism that bridges two
branches and enables cross-modal interaction to span longer time scales.
Extensive experiments on three popular RGBT tracking benchmarks show that our
method achieves state-of-the-art performance, while running at real-time speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01247">Deep Learning-Based Computational Model for Disease Identification in Cocoa Pods (Theobroma cacao L.). (arXiv:2401.01247v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vera_D/0/1/0/all/0/1">Darlyn Buena&#xf1;o Vera</a>, <a href="http://arxiv.org/find/cs/1/au:+Oviedo_B/0/1/0/all/0/1">Byron Oviedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Casanova_W/0/1/0/all/0/1">Washington Chiriboga Casanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambrano_Vega_C/0/1/0/all/0/1">Cristian Zambrano-Vega</a></p>
<p>The early identification of diseases in cocoa pods is an important task to
guarantee the production of high-quality cocoa. The use of artificial
intelligence techniques such as machine learning, computer vision and deep
learning are promising solutions to help identify and classify diseases in
cocoa pods. In this paper we introduce the development and evaluation of a deep
learning computational model applied to the identification of diseases in cocoa
pods, focusing on "monilia" and "black pod" diseases. An exhaustive review of
state-of-the-art of computational models was carried out, based on scientific
articles related to the identification of plant diseases using computer vision
and deep learning techniques. As a result of the search, EfficientDet-Lite4, an
efficient and lightweight model for object detection, was selected. A dataset,
including images of both healthy and diseased cocoa pods, has been utilized to
train the model to detect and pinpoint disease manifestations with considerable
accuracy. Significant enhancements in the model training and evaluation
demonstrate the capability of recognizing and classifying diseases through
image analysis. Furthermore, the functionalities of the model were integrated
into an Android native mobile with an user-friendly interface, allowing to
younger or inexperienced farmers a fast and accuracy identification of health
status of cocoa pods
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01256">VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM. (arXiv:2401.01256v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1">Fuchen Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zhaofan Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1">Ting Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1">Tao Mei</a></p>
<p>The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoDrafter, for content-consistent multi-scene video
generation. Technically, VideoDrafter leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoDrafter identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoDrafter outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoDrafter outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01272">MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic Communication. (arXiv:2401.01272v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yingbin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yaping Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaodong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Binhong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Ping Zhang</a></p>
<p>Vector quantization-based image semantic communication systems have
successfully boosted transmission efficiency, but face a challenge with
conflicting requirements between codebook design and digital constellation
modulation. Traditional codebooks need a wide index range, while modulation
favors few discrete states. To address this, we propose a multilevel generative
semantic communication system with a two-stage training framework. In the first
stage, we train a high-quality codebook, using a multi-head octonary codebook
(MOC) to compress the index range. We also integrate a residual vector
quantization (RVQ) mechanism for effective multilevel communication. In the
second stage, a noise reduction block (NRB) based on Swin Transformer is
introduced, coupled with the multilevel codebook from the first stage, serving
as a high-quality semantic knowledge base (SKB) for generative feature
restoration. Experimental results highlight MOC-RVQ's superior performance over
methods like BPG or JPEG, even without channel error correction coding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01286">A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yunzhi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zekun Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shengyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jintian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yuansheng Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jia-Chen Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pengjun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1">Lei Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaowei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01288">Physics-informed Generalizable Wireless Channel Modeling with Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges. (arXiv:2401.01288v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1">Ethan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haijian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1">Mingyue Ji</a></p>
<p>Channel modeling is fundamental in advancing wireless systems and has thus
attracted considerable research focus. Recent trends have seen a growing
reliance on data-driven techniques to facilitate the modeling process and yield
accurate channel predictions. In this work, we first provide a concise overview
of data-driven channel modeling methods, highlighting their limitations.
Subsequently, we introduce the concept and advantages of physics-informed
neural network (PINN)-based modeling and a summary of recent contributions in
this area. Our findings demonstrate that PINN-based approaches in channel
modeling exhibit promising attributes such as generalizability,
interpretability, and robustness. We offer a comprehensive architecture for
PINN methodology, designed to inform and inspire future model development. A
case-study of our recent work on precise indoor channel prediction with
semantic segmentation and deep learning is presented. The study concludes by
addressing the challenges faced and suggesting potential research directions in
this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01303">Integrating Edges into U-Net Models with Explainable Activation Maps for Brain Tumor Segmentation using MR Images. (arXiv:2401.01303v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sahayam_S/0/1/0/all/0/1">Subin Sahayam</a>, <a href="http://arxiv.org/find/eess/1/au:+Jayaraman_U/0/1/0/all/0/1">Umarani Jayaraman</a></p>
<p>Manual delineation of tumor regions from magnetic resonance (MR) images is
time-consuming, requires an expert, and is prone to human error. In recent
years, deep learning models have been the go-to approach for the segmentation
of brain tumors. U-Net and its' variants for semantic segmentation of medical
images have achieved good results in the literature. However, U-Net and its'
variants tend to over-segment tumor regions and may not accurately segment the
tumor edges. The edges of the tumor are as important as the tumor regions for
accurate diagnosis, surgical precision, and treatment planning. In the proposed
work, the authors aim to extract edges from the ground truth using a
derivative-like filter followed by edge reconstruction to obtain an edge ground
truth in addition to the brain tumor ground truth. Utilizing both ground
truths, the author studies several U-Net and its' variant architectures with
and without tumor edges ground truth as a target along with the tumor ground
truth for brain tumor segmentation. The author used the BraTS2020 benchmark
dataset to perform the study and the results are tabulated for the dice and
Hausdorff95 metrics. The mean and median metrics are calculated for the whole
tumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the
baseline U-Net and its variants, the models that learned edges along with the
tumor regions performed well in core tumor regions in both training and
validation datasets. The improved performance of edge-trained models trained on
baseline models like U-Net and V-Net achieved performance similar to baseline
state-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target
trained models are capable of generating edge maps that can be useful for
treatment planning. Additionally, for further explainability of the results,
the activation map generated by the hybrid MR-U-Net has been studied.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01339">Street Gaussians for Modeling Dynamic Urban Scenes. (arXiv:2401.01339v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yunzhi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haotong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chenxu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haiyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1">Kun Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_X/0/1/0/all/0/1">Xianpeng Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaowei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Sida Peng</a></p>
<p>This paper aims to tackle the problem of modeling dynamic urban street scenes
from monocular videos. Recent methods extend NeRF by incorporating tracked
vehicle poses to animate vehicles, enabling photo-realistic view synthesis of
dynamic urban street scenes. However, significant limitations are their slow
training and rendering speed, coupled with the critical need for high precision
in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene
representation that tackles all these limitations. Specifically, the dynamic
urban street is represented as a set of point clouds equipped with semantic
logits and 3D Gaussians, each associated with either a foreground vehicle or
the background. To model the dynamics of foreground object vehicles, each
object point cloud is optimized with optimizable tracked poses, along with a
dynamic spherical harmonics model for the dynamic appearance. The explicit
representation allows easy composition of object vehicles and background, which
in turn allows for scene editing operations and rendering at 133 FPS
(1066$\times$1600 resolution) within half an hour of training. The proposed
method is evaluated on multiple challenging benchmarks, including KITTI and
Waymo Open datasets. Experiments show that the proposed method consistently
outperforms state-of-the-art methods across all datasets. Furthermore, the
proposed representation delivers performance on par with that achieved using
precise ground-truth poses, despite relying only on poses from an off-the-shelf
tracker. The code is available at https://zju3dv.github.io/street_gaussians/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.02206">Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v8 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Talbot_M/0/1/0/all/0/1">Morgan B. Talbot</a>, <a href="http://arxiv.org/find/cs/1/au:+Zawar_R/0/1/0/all/0/1">Rushikesh Zawar</a>, <a href="http://arxiv.org/find/cs/1/au:+Badkundri_R/0/1/0/all/0/1">Rohil Badkundri</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengmi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1">Gabriel Kreiman</a></p>
<p>Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close to this
ability. When tasked with learning to classify objects by training on
non-repeating video frames in temporal order (online stream learning), models
that learn well from shuffled datasets catastrophically forget old knowledge
upon learning new stimuli. We propose a new continual learning algorithm,
Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by
replaying feature maps reconstructed by combining generic parts. CRUMB
concatenates trainable and re-usable "memory block" vectors to compositionally
reconstruct feature map tensors in convolutional neural networks. Storing the
indices of memory blocks used to reconstruct new stimuli enables memories of
the stimuli to be replayed during later tasks. This reconstruction mechanism
also primes the neural network to minimize catastrophic forgetting by biasing
it towards attending to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images, while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the state-of-the-art. Our code is
available at https://github.com/MorganBDT/crumb.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01923">Recovering 3D Human Mesh from Monocular Images: A Survey. (arXiv:2203.01923v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yating Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a></p>
<p>Estimating human pose and shape from monocular images is a long-standing
problem in computer vision. Since the release of statistical body models, 3D
human mesh recovery has been drawing broader attention. With the same goal of
obtaining well-aligned and physically plausible mesh results, two paradigms
have been developed to overcome challenges in the 2D-to-3D lifting process: i)
an optimization-based paradigm, where different data terms and regularization
terms are exploited as optimization objectives; and ii) a regression-based
paradigm, where deep learning techniques are embraced to solve the problem in
an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving
the quality of 3D mesh labels for a wide range of datasets. Though remarkable
progress has been achieved in the past decade, the task is still challenging
due to flexible body motions, diverse appearances, complex environments, and
insufficient in-the-wild annotations. To the best of our knowledge, this is the
first survey that focuses on the task of monocular 3D human mesh recovery. We
start with the introduction of body models and then elaborate recovery
frameworks and training objectives by providing in-depth analyses of their
strengths and weaknesses. We also summarize datasets, evaluation metrics, and
benchmark results. Open issues and future directions are discussed in the end,
hoping to motivate researchers and facilitate their research in this area. A
regularly updated project page can be found at
https://github.com/tinatiansjz/hmr-survey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.13165">YOLO and Mask R-CNN for Vehicle Number Plate Identification. (arXiv:2207.13165v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ganjoo_S/0/1/0/all/0/1">Siddharth Ganjoo</a></p>
<p>License plate scanners have grown in popularity in parking lots during the
past few years. In order to quickly identify license plates, traditional plate
recognition devices used in parking lots employ a fixed source of light and
shooting angles. For skewed angles, such as license plate images taken with
ultra-wide angle or fisheye lenses, deformation of the license plate
recognition plate can also be quite severe, impairing the ability of standard
license plate recognition systems to identify the plate. Mask RCNN gadget that
may be utilised for oblique pictures and various shooting angles. The results
of the experiments show that the suggested design will be capable of
classifying license plates with bevel angles larger than 0/60. Character
recognition using the suggested Mask R-CNN approach has advanced significantly
as well. The proposed Mask R-CNN method has also achieved significant progress
in character recognition, which is tilted more than 45 degrees as compared to
the strategy of employing the YOLOv2 model. Experiment results also suggest
that the methodology presented in the open data plate collecting is better than
other techniques (known as the AOLP dataset).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.03754">Exploring Long- and Short-Range Temporal Information for Learned Video Compression. (arXiv:2208.03754v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Huairui Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1">Zhenzhong Chen</a></p>
<p>Learned video compression methods have gained a variety of interest in the
video coding community since they have matched or even exceeded the
rate-distortion (RD) performance of traditional video codecs. However, many
current learning-based methods are dedicated to utilizing short-range temporal
information, thus limiting their performance. In this paper, we focus on
exploiting the unique characteristics of video content and further exploring
temporal information to enhance compression performance. Specifically, for
long-range temporal information exploitation, we propose temporal prior that
can update continuously within the group of pictures (GOP) during inference. In
that case temporal prior contains valuable temporal information of all decoded
images within the current GOP. As for short-range temporal information, we
propose a progressive guided motion compensation to achieve robust and
effective compensation. In detail, we design a hierarchical structure to
achieve multi-scale compensation. More importantly, we use optical flow
guidance to generate pixel offsets between feature maps at each scale, and the
compensation results at each scale will be used to guide the following scale's
compensation. Sufficient experimental results demonstrate that our method can
obtain better RD performance than state-of-the-art video compression
approaches. The code is publicly available on:
https://github.com/Huairui/LSTVC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.06950">Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v8 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1">Ruihan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a></p>
<p>This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content'' latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture'' variables characterizing the
diffusion process are synthesized at decoding time. We show that the model's
performance can be tuned toward perceptual metrics of interest. Our extensive
experiments involving multiple datasets and image quality assessment metrics
show that our approach yields stronger reported FID scores than the GAN-based
model, while also yielding competitive performance with VAE-based models in
several distortion metrics. Furthermore, training the diffusion with
$\mathcal{X}$-parameterization enables high-quality reconstructions in only a
handful of decoding steps, greatly affecting the model's practicality. Our code
is available at: \url{https://github.com/buggyyang/CDC_compression}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04718">On the Application of Efficient Neural Mapping to Real-Time Indoor Localisation for Unmanned Ground Vehicles. (arXiv:2211.04718v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holder_C/0/1/0/all/0/1">Christopher J. Holder</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1">Muhammad Shafique</a></p>
<p>Global localisation from visual data is a challenging problem applicable to
many robotics domains. Prior works have shown that neural networks can be
trained to map images of an environment to absolute camera pose within that
environment, learning an implicit neural mapping in the process. In this work
we evaluate the applicability of such an approach to real-world robotics
scenarios, demonstrating that by constraining the problem to 2-dimensions and
significantly increasing the quantity of training data, a compact model capable
of real-time inference on embedded platforms can be used to achieve
localisation accuracy of several centimetres. We deploy our trained model
onboard a UGV platform, demonstrating its effectiveness in a waypoint
navigation task, wherein it is able to localise with a mean accuracy of 9cm at
a rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or
220fps on a desktop GPU. Along with this work we will release a novel
localisation dataset comprising simulated and real environments, each with
training samples numbering in the tens of thousands.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00330">Reliable Joint Segmentation of Retinal Edema Lesions in OCT Images. (arXiv:2212.00330v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1">Chun-Mei Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zou_K/0/1/0/all/0/1">Ke Zou</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yanyu Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Meng_Q/0/1/0/all/0/1">Qingquan Meng</a>, <a href="http://arxiv.org/find/eess/1/au:+Goh_R/0/1/0/all/0/1">Rick Siow Mong Goh</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a></p>
<p>Focusing on the complicated pathological features, such as blurred
boundaries, severe scale differences between symptoms, background noise
interference, etc., in the task of retinal edema lesions joint segmentation
from OCT images and enabling the segmentation results more reliable. In this
paper, we propose a novel reliable multi-scale wavelet-enhanced transformer
network, which can provide accurate segmentation results with reliability
assessment. Specifically, aiming at improving the model's ability to learn the
complex pathological features of retinal edema lesions in OCT images, we
develop a novel segmentation backbone that integrates a wavelet-enhanced
feature extractor network and a multi-scale transformer module of our newly
designed. Meanwhile, to make the segmentation results more reliable, a novel
uncertainty segmentation head based on the subjective logical evidential theory
is introduced to generate the final segmentation results with a corresponding
overall uncertainty evaluation score map. We conduct comprehensive experiments
on the public database of AI-Challenge 2018 for retinal edema lesions
segmentation, and the results show that our proposed method achieves better
segmentation accuracy with a high degree of reliability as compared to other
state-of-the-art segmentation approaches. The code will be released on:
https://github.com/LooKing9218/ReliableRESeg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08965">PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via Cross-modal Distillation and Super-Voxel Clustering. (arXiv:2304.08965v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zisheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongbin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weitao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhipeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1">Haihong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baigui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1">Wenxiong Kang</a></p>
<p>Semantic segmentation of point clouds usually requires exhausting efforts of
human annotations, hence it attracts wide attention to the challenging topic of
learning from unlabeled or weaker forms of annotations. In this paper, we take
the first attempt for fully unsupervised semantic segmentation of point clouds,
which aims to delineate semantically meaningful objects without any form of
annotations. Previous works of unsupervised pipeline on 2D images fails in this
task of point clouds, due to: 1) Clustering Ambiguity caused by limited
magnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity
caused by the irregular sparsity of point cloud. Therefore, we propose a novel
framework, PointDC, which is comprised of two steps that handle the
aforementioned problems respectively: Cross-Modal Distillation (CMD) and
Super-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual
features are back-projected to the 3D space and aggregated to a unified point
feature to distill the training of the point representation. In the second
stage of SVC, the point features are aggregated to super-voxels and then fed to
the iterative clustering process for excavating semantic classes. PointDC
yields a significant improvement over the prior state-of-the-art unsupervised
methods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic
segmentation benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02803">Tensor PCA from basis in tensor space. (arXiv:2305.02803v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Turchetti_C/0/1/0/all/0/1">Claudio Turchetti</a></p>
<p>The aim of this paper is to present a mathematical framework for tensor PCA.
The proposed approach is able to overcome the limitations of previous methods
that extract a low dimensional subspace by iteratively solving an optimization
problem. The core of the proposed approach is the derivation of a basis in
tensor space from a real self-adjoint tensor operator, thus reducing the
problem of deriving a basis to an eigenvalue problem. Three different cases
have been studied to derive: i) a basis from a self-adjoint tensor operator;
ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence
between eigenvalue equation for a real self-adjoint tensor operator and
standard matrix eigenvalue equation has been proven. For all the three cases
considered, a subspace approach has been adopted to derive a tensor PCA.
Experiments on image datasets validate the proposed mathematical framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07490">ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter. (arXiv:2305.07490v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zhengqing Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a></p>
<p>In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists' descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11300">RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing. (arXiv:2306.11300v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zilun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tiancheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jianwei Yin</a></p>
<p>Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01530">Tomato Maturity Recognition with Convolutional Transformers. (arXiv:2307.01530v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Asim Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1">Taimur Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafay_M/0/1/0/all/0/1">Muhammad Shafay</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1">Israa Fahmy</a>, <a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1">Naoufel Werghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1">Lakmal Seneviratne</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_I/0/1/0/all/0/1">Irfan Hussain</a></p>
<p>Tomatoes are a major crop worldwide, and accurately classifying their
maturity is important for many agricultural applications, such as harvesting,
grading, and quality control. In this paper, the authors propose a novel method
for tomato maturity classification using a convolutional transformer. The
convolutional transformer is a hybrid architecture that combines the strengths
of convolutional neural networks (CNNs) and transformers. Additionally, this
study introduces a new tomato dataset named KUTomaData, explicitly designed to
train deep-learning models for tomato segmentation and classification.
KUTomaData is a compilation of images sourced from a greenhouse in the UAE,
with approximately 700 images available for training and testing. The dataset
is prepared under various lighting conditions and viewing perspectives and
employs different mobile camera sensors, distinguishing it from existing
datasets. The contributions of this paper are threefold:Firstly, the authors
propose a novel method for tomato maturity classification using a modular
convolutional transformer. Secondly, the authors introduce a new tomato image
dataset that contains images of tomatoes at different maturity levels. Lastly,
the authors show that the convolutional transformer outperforms
state-of-the-art methods for tomato maturity classification. The effectiveness
of the proposed framework in handling cluttered and occluded tomato instances
was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno
Annotated Tomato, as benchmarks. The evaluation results across these three
datasets demonstrate the exceptional performance of our proposed framework,
surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean
average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated
Tomato, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09330">Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots. (arXiv:2307.09330v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1">Daniel Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_A/0/1/0/all/0/1">Ashley Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1">Remco Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gleicher_M/0/1/0/all/0/1">Michael Gleicher</a>, <a href="http://arxiv.org/find/cs/1/au:+Landesberger_T/0/1/0/all/0/1">Tatiana von Landesberger</a></p>
<p>We investigate the ability of individuals to visually validate statistical
models in terms of their fit to the data. While visual model estimation has
been studied extensively, visual model validation remains under-investigated.
It is unknown how well people are able to visually validate models, and how
their performance compares to visual and computational estimation. As a
starting point, we conducted a study across two populations (crowdsourced and
volunteers). Participants had to both visually estimate (i.e, draw) and
visually validate (i.e., accept or reject) the frequently studied model of
averages. Across both populations, the level of accuracy of the models that
were considered valid was lower than the accuracy of the estimated models. We
find that participants' validation and estimation were unbiased. Moreover,
their natural critical point between accepting and rejecting a given mean value
is close to the boundary of its 95% confidence interval, indicating that the
visually perceived confidence interval corresponds to a common statistical
standard. Our work contributes to the understanding of visual model validation
and opens new research opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10875">Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification. (arXiv:2307.10875v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junchi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Henghui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Changsheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chee Yeow Meng</a></p>
<p>With the growth of 3D sensing technology, deep learning system for 3D point
clouds has become increasingly important, especially in applications like
autonomous vehicles where safety is a primary concern. However, there are also
growing concerns about the reliability of these systems when they encounter
noisy point clouds, whether occurring naturally or introduced with malicious
intent. This paper highlights the challenges of point cloud classification
posed by various forms of noise, from simple background noise to malicious
backdoor attacks that can intentionally skew model predictions. While there's
an urgent need for optimized point cloud denoising, current point outlier
removal approaches, an essential step for denoising, rely heavily on
handcrafted strategies and are not adapted for higher-level tasks, such as
classification. To address this issue, we introduce an innovative point outlier
cleansing method that harnesses the power of downstream classification models.
By employing gradient-based attribution analysis, we define a novel concept:
point risk. Drawing inspiration from tail risk minimization in finance, we
recast the outlier removal process as an optimization problem, named PointCVaR.
Extensive experiments show that our proposed technique not only robustly
filters diverse point cloud outliers but also consistently and significantly
enhances existing robust methods for point cloud classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04789">Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection. (arXiv:2308.04789v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chaoqin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Aofan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a></p>
<p>Anomaly detection has gained considerable attention due to its broad range of
applications, particularly in industrial defect detection. To address the
challenges of data collection, researchers have introduced zero-/few-shot
anomaly detection techniques that require minimal normal images for each
category. However, complex industrial scenarios often involve multiple objects,
presenting a significant challenge. In light of this, we propose a
straightforward yet powerful multi-scale memory comparison framework for
zero-/few-shot anomaly detection. Our approach employs a global memory bank to
capture features across the entire image, while an individual memory bank
focuses on simplified scenes containing a single object. The efficacy of our
method is validated by its remarkable achievement of 4th place in the zero-shot
track and 2nd place in the few-shot track of the Visual Anomaly and Novelty
Detection (VAND) competition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01141">VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders. (arXiv:2309.01141v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siteng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yachen Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Honggang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>Large-scale text-to-image diffusion models have shown impressive capabilities
for generative tasks by leveraging strong vision-language alignment from
pre-training. However, most vision-language discriminative tasks require
extensive fine-tuning on carefully-labeled datasets to acquire such alignment,
with great cost in time and computing resources. In this work, we explore
directly applying a pre-trained generative diffusion model to the challenging
discriminative task of visual grounding without any fine-tuning and additional
training dataset. Specifically, we propose VGDiffZero, a simple yet effective
zero-shot visual grounding framework based on text-to-image diffusion models.
We also design a comprehensive region-scoring method considering both global
and local contexts of each isolated proposal. Extensive experiments on RefCOCO,
RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on
zero-shot visual grounding. Our code is available at
https://github.com/xuyang-liu16/VGDiffZero.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09496">CLIP-based Synergistic Knowledge Transfer for Text-based Person Retrieval. (arXiv:2309.09496v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yating Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zimo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qingmin Liao</a></p>
<p>Text-based Person Retrieval (TPR) aims to retrieve the target person images
given a textual query. The primary challenge lies in bridging the substantial
gap between vision and language modalities, especially when dealing with
limited large-scale datasets. In this paper, we introduce a CLIP-based
Synergistic Knowledge Transfer (CSKT) approach for TPR. Specifically, to
explore the CLIP's knowledge on input side, we first propose a Bidirectional
Prompts Transferring (BPT) module constructed by text-to-image and
image-to-text bidirectional prompts and coupling projections. Secondly, Dual
Adapters Transferring (DAT) is designed to transfer knowledge on output side of
Multi-Head Attention (MHA) in vision and language. This synergistic two-way
collaborative mechanism promotes the early-stage feature fusion and efficiently
exploits the existing knowledge of CLIP. CSKT outperforms the state-of-the-art
approaches across three benchmark datasets when the training parameters merely
account for 7.4% of the entire model, demonstrating its remarkable efficiency,
effectiveness and generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10399">Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carloni_G/0/1/0/all/0/1">Gianluca Carloni</a>, <a href="http://arxiv.org/find/cs/1/au:+Colantonio_S/0/1/0/all/0/1">Sara Colantonio</a></p>
<p>We present a novel technique to discover and exploit weak causal signals
directly from images via neural networks for classification purposes. This way,
we model how the presence of a feature in one part of the image affects the
appearance of another feature in a different part of the image. Our method
consists of a convolutional neural network backbone and a causality-factors
extractor module, which computes weights to enhance each feature map according
to its causal influence in the scene. We develop different architecture
variants and empirically evaluate all the models on two public datasets of
prostate MRI images and breast histopathology slides for cancer diagnosis. We
study the effectiveness of our module both in fully-supervised and few-shot
learning, we assess its addition to existing attention-based solutions, we
conduct ablation studies, and investigate the explainability of our models via
class activation maps. Our findings show that our lightweight block extracts
meaningful information and improves the overall classification, together with
producing more robust predictions that focus on relevant parts of the image.
That is crucial in medical imaging, where accurate and reliable classifications
are essential for effective diagnosis and treatment planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11715">Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal. (arXiv:2309.11715v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tian Yi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jia Wei Yao</a></p>
<p>Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02067">Content Bias in Deep Learning Image Age Approximation: A new Approach Towards better Explainability. (arXiv:2310.02067v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jochl_R/0/1/0/all/0/1">Robert J&#xf6;chl</a>, <a href="http://arxiv.org/find/cs/1/au:+Uhl_A/0/1/0/all/0/1">Andreas Uhl</a></p>
<p>In the context of temporal image forensics, it is not evident that a neural
network, trained on images from different time-slots (classes), exploits solely
image age related features. Usually, images taken in close temporal proximity
(e.g., belonging to the same age class) share some common content properties.
Such content bias can be exploited by a neural network. In this work, a novel
approach is proposed that evaluates the influence of image content. This
approach is verified using synthetic images (where content bias can be ruled
out) with an age signal embedded. Based on the proposed approach, it is shown
that a deep learning approach proposed in the context of age classification is
most likely highly dependent on the image content. As a possible
countermeasure, two different models from the field of image steganalysis,
along with three different preprocessing techniques to increase the
signal-to-noise ratio (age signal to image content), are evaluated using the
proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01187">SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1">Renan A. Rojas-Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1">Ali Etemad</a>, <a href="http://arxiv.org/find/cs/1/au:+Bijamov_A/0/1/0/all/0/1">Alex Bijamov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morningstar_W/0/1/0/all/0/1">Warren R. Morningstar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1">Philip Andrew Mansfield</a></p>
<p>Self-supervised learning relies heavily on data augmentation to extract
meaningful representations from unlabeled images. While existing
state-of-the-art augmentation pipelines incorporate a wide range of primitive
transformations, these often disregard natural image structure. Thus, augmented
samples can exhibit degraded semantic information and low stylistic diversity,
affecting downstream performance of self-supervised representations. To
overcome this, we propose SASSL: Style Augmentations for Self Supervised
Learning, a novel augmentation technique based on Neural Style Transfer. The
method decouples semantic and stylistic attributes in images and applies
transformations exclusively to the style while preserving content, generating
diverse augmented samples that better retain their semantic properties.
Experimental results show our technique achieves a top-1 classification
performance improvement of more than 2% on ImageNet compared to the
well-established MoCo v2. We also measure transfer learning performance across
five diverse datasets, observing significant improvements of up to 3.75%. Our
experiments indicate that decoupling style from content information and
transferring style across datasets to diversify augmentations can significantly
improve downstream performance of self-supervised representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04028">ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with Implicit Neural Representations. (arXiv:2312.04028v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingwu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Di Huang</a></p>
<p>Accurate representations of 3D faces are of paramount importance in various
computer vision and graphics applications. However, the challenges persist due
to the limitations imposed by data discretization and model linearity, which
hinder the precise capture of identity and expression clues in current studies.
This paper presents a novel 3D morphable face model, named ImFace++, to learn a
sophisticated and continuous space with implicit neural representations.
ImFace++ first constructs two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
which simultaneously facilitate the automatic learning of correspondences
across diverse facial shapes. To capture more sophisticated facial details, a
refinement displacement field within the template space is further
incorporated, enabling a fine-grained learning of individual-specific facial
details. Furthermore, a Neural Blend-Field is designed to reinforce the
representation capabilities through adaptive blending of an array of local
fields. In addition to ImFace++, we have devised an improved learning strategy
to extend expression embeddings, allowing for a broader range of expression
variations. Comprehensive qualitative and quantitative evaluations demonstrate
that ImFace++ significantly advances the state-of-the-art in terms of both face
reconstruction fidelity and correspondence accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06738">InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following. (arXiv:2312.06738v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shufan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1">Harkanwar Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>The ability to provide fine-grained control for generating and editing visual
imagery has profound implications for computer vision and its applications.
Previous works have explored extending controllability in two directions:
instruction tuning with text-based prompts and multi-modal conditioning.
However, these works make one or more unnatural assumptions on the number
and/or type of modality inputs used to express controllability. We propose
InstructAny2Pix, a flexible multi-modal instruction-following system that
enables users to edit an input image using instructions involving audio,
images, and text. InstructAny2Pix consists of three building blocks that
facilitate this capability: a multi-modal encoder that encodes different
modalities such as images and audio into a unified latent space, a diffusion
model that learns to decode representations in this latent space into images,
and a multi-modal LLM that can understand instructions involving multiple
images and audio pieces and generate a conditional embedding of the desired
output, which can be used by the diffusion decoder. Additionally, to facilitate
training efficiency and improve generation quality, we include an additional
refinement prior module that enhances the visual quality of LLM outputs. These
designs are critical to the performance of our system. We demonstrate that our
system can perform a series of novel instruction-guided editing tasks. The code
is available at https://github.com/jacklishufan/InstructAny2Pix.git
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07418">Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023). (arXiv:2312.07418v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parajuli_K/0/1/0/all/0/1">Kabita Parajuli</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Shashidhar Ram Joshi</a></p>
<p>Video captioning in Nepali, a language written in the Devanagari script,
presents a unique challenge due to the lack of existing academic work in this
domain. This work develops a novel encoder-decoder paradigm for Nepali video
captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models
are used in the model to produce related textual descriptions based on features
retrieved from video frames using CNNs. Using Google Translate and manual
post-editing, a Nepali video captioning dataset is generated from the Microsoft
Research Video Description Corpus (MSVD) dataset created using Google
Translate, and manual post-editing work. The efficacy of the model for
Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE
measures, which are used to assess its performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08983">Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting. (arXiv:2312.08983v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunze Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1">Li Yi</a></p>
<p>We focus on the human-humanoid interaction task optionally with an object. We
propose a new task named online full-body motion reaction synthesis, which
generates humanoid reactions based on the human actor's motions. The previous
work only focuses on human interaction without objects and generates body
reactions without hand. Besides, they also do not consider the task as an
online setting, which means the inability to observe information beyond the
current moment in practical situations. To support this task, we construct two
datasets named HHI and CoChair and propose a unified method. Specifically, we
propose to construct a social affordance representation. We first select a
social affordance carrier and use SE(3)-Equivariant Neural Networks to learn
the local frame for the carrier, then we canonicalize the social affordance.
Besides, we propose a social affordance forecasting scheme to enable the
reactor to predict based on the imagined future. Experiments demonstrate that
our approach can effectively generate high-quality reactions on HHI and
CoChair. Furthermore, we also validate our method on existing human interaction
datasets Interhuman and Chi3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10144">Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vouitsis_N/0/1/0/all/0/1">No&#xeb;l Vouitsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhaoyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1">Satya Krishna Gorti</a>, <a href="http://arxiv.org/find/cs/1/au:+Villecroze_V/0/1/0/all/0/1">Valentin Villecroze</a>, <a href="http://arxiv.org/find/cs/1/au:+Cresswell_J/0/1/0/all/0/1">Jesse C. Cresswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Guangwei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1">Gabriel Loaiza-Ganem</a>, <a href="http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1">Maksims Volkovs</a></p>
<p>The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11035">Multi-Moving Camera Pedestrian Tracking with a New Dataset and Global Link Model. (arXiv:2312.11035v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuanghong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qingxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Cairong Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a></p>
<p>Ensuring driving safety for autonomous vehicles has become increasingly
crucial, highlighting the need for systematic tracking of pedestrians on the
road. Most vehicles are equipped with visual sensors, however, the large-scale
visual dataset from different agents has not been well studied yet. Basically,
most of the multi-target multi-camera (MTMC) tracking systems are composed of
two modules: single camera tracking (SCT) and inter-camera tracking (ICT). To
reliably coordinate between them, MTMC tracking has been a very complicated
task, while tracking across multi-moving cameras makes it even more
challenging. In this paper, we focus on multi-target multi-moving camera
(MTMMC) tracking, which is attracting increasing attention from the research
community. Observing there are few datasets for MTMMC tracking, we collect a
new dataset, called Multi-Moving Camera Track (MMCT), which contains sequences
under various driving scenarios. To address the common problems of identity
switch easily faced by most existing SCT trackers, especially for moving
cameras due to ego-motion between the camera and targets, a lightweight
appearance-free global link model, called Linker, is proposed to mitigate the
identity switch by associating two disjoint tracklets of the same target into a
complete trajectory within the same camera. Incorporated with Linker, existing
SCT trackers generally obtain a significant improvement. Moreover, a strong
baseline approach of re-identification (Re-ID) is effectively incorporated to
extract robust appearance features under varying surroundings for pedestrian
association across moving cameras for ICT, resulting in a much improved MTMMC
tracking system, which can constitute a step further towards coordinated mining
of multiple moving cameras. The dataset is available at
https://github.com/dhu-mmct/DHU-MMCT}{https://github.com/dhu-mmct/DHU-MMCT .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11231">Global Feature Pyramid Network. (arXiv:2312.11231v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1">Weilin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Ming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yonggui Lin</a></p>
<p>The visual feature pyramid has proven its effectiveness and efficiency in
target detection tasks. Yet, current methodologies tend to overly emphasize
inter-layer feature interaction, neglecting the crucial aspect of intra-layer
feature adjustment. Experience underscores the significant advantages of
intra-layer feature interaction in enhancing target detection tasks. While some
approaches endeavor to learn condensed intra-layer feature representations
using attention mechanisms or visual transformers, they overlook the
incorporation of global information interaction. This oversight results in
increased false detections and missed targets.To address this critical issue,
this paper introduces the Global Feature Pyramid Network (GFPNet), an augmented
version of PAFPN that integrates global information for enhanced target
detection. Specifically, we leverage a lightweight MLP to capture global
feature information, utilize the VNC encoder to process these features, and
employ a parallel learnable mechanism to extract intra-layer features from the
input image. Building on this foundation, we retain the PAFPN method to
facilitate inter-layer feature interaction, extracting rich feature details
across various levels.Compared to conventional feature pyramids, GFPN not only
effectively focuses on inter-layer feature information but also captures global
feature details, fostering intra-layer feature interaction and generating a
more comprehensive and impactful feature representation. GFPN consistently
demonstrates performance improvements over object detection baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11460">Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response. (arXiv:2312.11460v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1">Junfeng Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiawei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Liu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1">Jiangmiao Pang</a></p>
<p>Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot's explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot's
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot's proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12028">EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Siamul Karim Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tinsley_P/0/1/0/all/0/1">Patrick Tinsley</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitcheff_M/0/1/0/all/0/1">Mahsa Mitcheff</a>, <a href="http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1">Patrick Flynn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1">Kevin W. Bowyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1">Adam Czajka</a></p>
<p>Synthesis of same-identity biometric iris images, both for existing and
non-existing identities while preserving the identity across a wide range of
pupil sizes, is complex due to intricate iris muscle constriction mechanism,
requiring a precise model of iris non-linear texture deformations to be
embedded into the synthesis pipeline. This paper presents the first method of
fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris
images. This approach is capable of synthesizing images of irises with
different pupil sizes representing non-existing identities as well as
non-linearly deforming the texture of iris images of existing subjects given
the segmentation mask of the target iris image. Iris recognition experiments
suggest that the proposed deformation model not only preserves the identity
when changing the pupil size but offers better similarity between same-identity
iris samples with significant differences in pupil size, compared to
state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation
models. Two immediate applications of the proposed approach are: (a) synthesis
of, or enhancement of the existing biometric datasets for iris recognition,
mimicking those acquired with iris sensors, and (b) helping forensic human
experts in examining iris image pairs with significant differences in pupil
dilation. Source codes and weights of the models are made available with the
paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17050">KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching. (arXiv:2312.17050v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1">Huanjing Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1">Zifan Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingyu Yang</a></p>
<p>Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)
based SR by utilizing the telephoto image (Ref) to assist the super-resolution
of the low-resolution wide-angle image (LR input). Different from general
RefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)
area. However, current dual-lens SR methods rarely utilize these specific
characteristics and directly perform dense matching between the LR input and
Ref. Due to the resolution gap between LR and Ref, the matching may miss the
best-matched candidate and destroy the consistent structures in the overlapped
FoV area. Different from them, we propose to first align the Ref with the
center region (namely the overlapped FoV area) of the LR input by combining
global warping and local warping to make the aligned Ref be sharp and
consistent. Then, we formulate the aligned Ref and LR center as value-key
pairs, and the corner region of the LR is formulated as queries. In this way,
we propose a kernel-free matching strategy by matching between the LR-corner
(query) and LR-center (key) regions, and the corresponding aligned Ref (value)
can be warped to the corner region of the target. Our kernel-free matching
strategy avoids the resolution gap between LR and Ref, which makes our network
have better generalization ability. In addition, we construct a DuSR-Real
dataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.
Experiments on three datasets demonstrate that our method outperforms the
second-best method by a large margin. Our code and dataset are available at
https://github.com/ZifanCui/KeDuSR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00029">6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation. (arXiv:2401.00029v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Li Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1">Haoxuan Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yujun Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>Estimating the 6D object pose from a single RGB image often involves noise
and indeterminacy due to challenges such as occlusions and cluttered
backgrounds. Meanwhile, diffusion models have shown appealing performance in
generating high-quality images from random noise with high indeterminacy
through step-by-step denoising. Inspired by their denoising capability, we
propose a novel diffusion-based framework (6D-Diff) to handle the noise and
indeterminacy in object pose estimation for better performance. In our
framework, to establish accurate 2D-3D correspondence, we formulate 2D
keypoints detection as a reverse diffusion (denoising) process. To facilitate
such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion
process and condition the reverse process on the object features. Extensive
experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00374">EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked Audio Gesture Modeling. (arXiv:2401.00374v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haiyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Becherini_G/0/1/0/all/0/1">Giorgio Becherini</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yichen Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1">Mingyang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">You Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1">Naoya Iwamoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Bo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a></p>
<p>We propose EMAGE, a framework to generate full-body human gestures from audio
and masked gestures, encompassing facial, local body, hands, and global
movements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a new
mesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body with
FLAME head parameters and further refines the modeling of head, neck, and
finger movements, offering a community-standardized, high-quality 3D motion
captured dataset. EMAGE leverages masked body gesture priors during training to
boost inference performance. It involves a Masked Audio Gesture Transformer,
facilitating joint training on audio-to-gesture generation and masked gesture
reconstruction to effectively encode audio and body gesture hints. Encoded body
hints from masked gestures are then separately employed to generate facial and
body movements. Moreover, EMAGE adaptively merges speech features from the
audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance
the results' fidelity and diversity. Experiments demonstrate that EMAGE
generates holistic gestures with state-of-the-art performance and is flexible
in accepting predefined spatial-temporal gesture inputs, generating complete,
audio-synchronized results. Our code and dataset are available at
https://pantomatrix.github.io/EMAGE/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00616">GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields. (arXiv:2401.00616v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xiao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zongxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Shuai Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.
</p>
</p>
</div>

    </div>
    </body>
    