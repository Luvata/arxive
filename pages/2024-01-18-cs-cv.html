<!DOCTYPE html>
<html>
<head>
<title>2024-01-18-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2010.12190">Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1">Kun Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1">Qinghua Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingwen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jia Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1">Feipeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaolin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the
images generated by adversarial attacks, which raises researches on the
adversarial robustness of DNNs. A series of methods represented by the
adversarial training and its variants have proven as one of the most effective
techniques in enhancing the DNN robustness. Generally, adversarial training
focuses on enriching the training data by involving perturbed data. Such data
augmentation effect of the involved perturbed data in adversarial training does
not contribute to the robustness of DNN itself and usually suffers from clean
accuracy drop. Towards the robustness of DNN itself, we in this paper propose a
novel defense that aims at augmenting the model in order to learn features that
are adaptive to diverse inputs, including adversarial examples. More
specifically, to augment the model, multiple paths are embedded into the
network, and an orthogonality constraint is imposed on these paths to guarantee
the diversity among them. A margin-maximization loss is then designed to
further boost such DIversity via Orthogonality (DIO). In this way, the proposed
DIO augments the model and enhances the robustness of DNN itself as the learned
features can be corrected by these mutually-orthogonal paths. Extensive
empirical results on various data sets, structures and attacks verify the
stronger adversarial robustness of the proposed DIO utilizing model
augmentation. Besides, DIO can also be flexibly combined with different data
augmentation techniques (e.g., TRADES and DDPM), further promoting robustness
gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.00650">Decomposition, Compression, and Synthesis (DCS)-based Video Coding: A Neural Exploration via Resolution-Adaptive Learning. (arXiv:2012.00650v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Ming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1">Dandan Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fengqing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhan Ma</a></p>
<p>Inspired by the facts that retinal cells actually segregate the visual scene
into different attributes (e.g., spatial details, temporal motion) for
respective neuronal processing, we propose to first decompose the input video
into respective spatial texture frames (STF) at its native spatial resolution
that preserve the rich spatial details, and the other temporal motion frames
(TMF) at a lower spatial resolution that retain the motion smoothness; then
compress them together using any popular video coder; and finally synthesize
decoded STFs and TMFs for high-fidelity video reconstruction at the same
resolution as its native input. This work simply applies the bicubic resampling
in decomposition and HEVC compliant codec in compression, and puts the focus on
the synthesis part. For resolution-adaptive synthesis, a motion compensation
network (MCN) is devised on TMFs to efficiently align and aggregate temporal
motion features that will be jointly processed with corresponding STFs using a
non-local texture transfer network (NL-TTN) to better augment spatial details,
by which the compression and resolution resampling noises can be effectively
alleviated with better rate-distortion efficiency. Such "Decomposition,
Compression, Synthesis (DCS)" based scheme is codec agnostic, currently
exemplifying averaged $\approx$1 dB PSNR gain or $\approx$25% BD-rate saving,
against the HEVC anchor using reference software. In addition, experimental
comparisons to the state-of-the-art methods and ablation studies are conducted
to further report the efficiency and generalization of DCS algorithm, promising
an encouraging direction for future video coding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.14249">Marine Snow Removal Benchmarking Dataset. (arXiv:2103.14249v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaneko_R/0/1/0/all/0/1">Reina Kaneko</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1">Yuya Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Ueda_T/0/1/0/all/0/1">Takumi Ueda</a>, <a href="http://arxiv.org/find/cs/1/au:+Higashi_H/0/1/0/all/0/1">Hiroshi Higashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1">Yuichi Tanaka</a></p>
<p>This paper introduces a new benchmarking dataset for marine snow removal of
underwater images. Marine snow is one of the main degradation sources of
underwater images that are caused by small particles, e.g., organic matter and
sand, between the underwater scene and photosensors. We mathematically model
two typical types of marine snow from the observations of real underwater
images. The modeled artifacts are synthesized with underwater images to
construct large-scale pairs of ground truth and degraded images to calculate
objective qualities for marine snow removal and to train a deep neural network.
We propose two marine snow removal tasks using the dataset and show the first
benchmarking results of marine snow removal. The Marine Snow Removal
Benchmarking Dataset is publicly available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.12056">Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1">Shawn L. Beaulieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1">Jeff Clune</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1">Nick Cheney</a></p>
<p>Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow's machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning and retaining new information without being exposed to it
arbitrarily often. Past efforts to engineer such systems have sought to build
or regulate artificial neural networks using disjoint sets of weights that are
uniquely sensitive to specific tasks or inputs. This has not yet enabled
continual learning over long sequences of previously unseen data without
corrupting existing knowledge: a problem known as catastrophic forgetting. In
this paper, we introduce a system that can learn sequentially over previously
unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is
done by controlling the activity of weights in a convolutional neural network
on the basis of inputs using top-down regulation generated by a second
feed-forward neural network. We find that our method learns continually under
domain transfer with sparse bursts of activity in weights that are recycled
across tasks, rather than by maintaining task-specific modules. Sparse synaptic
bursting is found to balance activity and suppression such that new functions
can be learned without corrupting extant knowledge, thus mirroring the balance
of order and disorder in systems at the edge of chaos. This behavior emerges
during a prior pre-training (or 'meta-learning') phase in which regulated
synapses are selectively disinhibited, or grown, from an initial state of
uniform suppression through prediction error minimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.09824">Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1">Geri Skenderi</a>, <a href="http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1">Christian Joppi</a>, <a href="http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1">Matteo Denitto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1">Marco Cristani</a></p>
<p>New fashion product sales forecasting is a challenging problem that involves
many business dynamics and cannot be solved by classical forecasting
approaches. In this paper, we investigate the effectiveness of systematically
probing exogenous knowledge in the form of Google Trends time series and
combining it with multi-modal information related to a brand-new fashion item,
in order to effectively forecast its sales despite the lack of past data. In
particular, we propose a neural network-based approach, where an encoder learns
a representation of the exogenous time series, while the decoder forecasts the
sales based on the Google Trends encoding and the available visual and metadata
information. Our model works in a non-autoregressive manner, avoiding the
compounding effect of large first-step errors. As a second contribution, we
present VISUELLE, a publicly available dataset for the task of new fashion
product sales forecasting, containing multimodal information for 5577 real, new
products sold between 2016-2019 from Nunalie, an Italian fast-fashion company.
The dataset is equipped with images of products, metadata, related sales, and
associated Google Trends. We use VISUELLE to compare our approach against
state-of-the-art alternatives and several baselines, showing that our neural
network-based approach is the most accurate in terms of both percentage and
absolute error. It is worth noting that the addition of exogenous knowledge
boosts the forecasting accuracy by 1.5% in terms of Weighted Absolute
Percentage Error (WAPE), revealing the importance of exploiting informative
external information. The code and dataset are both available at
https://github.com/HumaticsLAB/GTM-Transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.13004">Optimising for Interpretability: Convolutional Dynamic Alignment Networks. (arXiv:2109.13004v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bohle_M/0/1/0/all/0/1">Moritz B&#xf6;hle</a>, <a href="http://arxiv.org/find/stat/1/au:+Fritz_M/0/1/0/all/0/1">Mario Fritz</a>, <a href="http://arxiv.org/find/stat/1/au:+Schiele_B/0/1/0/all/0/1">Bernt Schiele</a></p>
<p>We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which are optimised to transform their inputs
with dynamically computed weight vectors that align with task-relevant
patterns. As a result, CoDA Nets model the classification prediction through a
series of input-dependent linear transformations, allowing for linear
decomposition of the output into individual input contributions. Given the
alignment of the DAUs, the resulting contribution maps align with
discriminative input patterns. These model-inherent decompositions are of high
visual quality and outperform existing attribution methods under quantitative
metrics. Further, CoDA Nets constitute performant classifiers, achieving on par
results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,
CoDA Nets can be combined with conventional neural network models to yield
powerful classifiers that more easily scale to complex datasets such as
Imagenet whilst exhibiting an increased interpretable depth, i.e., the output
can be explained well in terms of contributions from intermediate layers within
the network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.11802">Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jianfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zizheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Vision Transformers (ViTs) have achieved impressive performance over various
computer vision tasks. However, modeling global correlations with multi-head
self-attention (MSA) layers leads to two widely recognized issues: the massive
computational resource consumption and the lack of intrinsic inductive bias for
modeling local visual patterns. To solve both issues, we devise a simple yet
effective method named Single-Path Vision Transformer pruning (SPViT), to
efficiently and automatically compress the pre-trained ViTs into compact models
with proper locality added. Specifically, we first propose a novel
weight-sharing scheme between MSA and convolutional operations, delivering a
single-path space to encode all candidate operations. In this way, we cast the
operation search problem as finding which subset of parameters to use in each
MSA layer, which significantly reduces the computational cost and optimization
difficulty, and the convolution kernels can be well initialized using
pre-trained MSA parameters. Relying on the single-path space, we introduce
learnable binary gates to encode the operation choices in MSA layers.
Similarly, we further employ learnable gates to encode the fine-grained MLP
expansion ratios of FFN layers. In this way, our SPViT optimizes the learnable
gates to automatically explore from a vast and unified search space and
flexibly adjust the MSA-FFN pruning proportions for each individual dense
model. We conduct extensive experiments on two representative ViTs showing that
our SPViT achieves a new SOTA for pruning on ImageNet-1k. For example, our
SPViT can trim 52.0% FLOPs for DeiT-B and get an impressive 0.6% top-1 accuracy
gain simultaneously. The source code is available at
https://github.com/ziplab/SPViT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.04049">Graph Attention Transformer Network for Multi-Label Image Classification. (arXiv:2203.04049v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shikai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhongchao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jianping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rui_Y/0/1/0/all/0/1">Yong Rui</a></p>
<p>Multi-label classification aims to recognize multiple objects or attributes
from images. However, it is challenging to learn from proper label graphs to
effectively characterize such inter-label correlations or dependencies. Current
methods often use the co-occurrence probability of labels based on the training
set as the adjacency matrix to model this correlation, which is greatly limited
by the dataset and affects the model's generalization ability. In this paper,
we propose a Graph Attention Transformer Network (GATN), a general framework
for multi-label image classification that can effectively mine complex
inter-label relationships. First, we use the cosine similarity based on the
label word embedding as the initial correlation matrix, which can represent
rich semantic information. Subsequently, we design the graph attention
transformer layer to transfer this adjacency matrix to adapt to the current
domain. Our extensive experiments have demonstrated that our proposed methods
can achieve state-of-the-art performance on three datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.04962">Learning the Degradation Distribution for Blind Image Super-Resolution. (arXiv:2203.04962v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1">Zhengxiong Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a></p>
<p>Synthetic high-resolution (HR) \&amp; low-resolution (LR) pairs are widely used
in existing super-resolution (SR) methods. To avoid the domain gap between
synthetic and test images, most previous methods try to adaptively learn the
synthesizing (degrading) process via a deterministic model. However, some
degradations in real scenarios are stochastic and cannot be determined by the
content of the image. These deterministic models may fail to model the random
factors and content-independent parts of degradations, which will limit the
performance of the following SR models. In this paper, we propose a
probabilistic degradation model (PDM), which studies the degradation
$\mathbf{D}$ as a random variable, and learns its distribution by modeling the
mapping from a priori random variable $\mathbf{z}$ to $\mathbf{D}$. Compared
with previous deterministic degradation models, PDM could model more diverse
degradations and generate HR-LR pairs that may better cover the various
degradations of test images, and thus prevent the SR model from over-fitting to
specific ones. Extensive experiments have demonstrated that our degradation
model can help the SR model achieve better performance on different datasets.
The source codes are released at \url{git@github.com:greatlog/UnpairedSR.git}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.02791">Implicit Motion-Compensated Network for Unsupervised Video Object Segmentation. (arXiv:2204.02791v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xi_L/0/1/0/all/0/1">Lin Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weihai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengguo Li</a></p>
<p>Unsupervised video object segmentation (UVOS) aims at automatically
separating the primary foreground object(s) from the background in a video
sequence. Existing UVOS methods either lack robustness when there are visually
similar surroundings (appearance-based) or suffer from deterioration in the
quality of their predictions because of dynamic background and inaccurate flow
(flow-based). To overcome the limitations, we propose an implicit
motion-compensated network (IMCNet) combining complementary cues
($\textit{i.e.}$, appearance and motion) with aligned motion information from
the adjacent frames to the current frame at the feature level without
estimating optical flows. The proposed IMCNet consists of an affinity computing
module (ACM), an attention propagation module (APM), and a motion compensation
module (MCM). The light-weight ACM extracts commonality between neighboring
input frames based on appearance features. The APM then transmits global
correlation in a top-down manner. Through coarse-to-fine iterative inspiring,
the APM will refine object regions from multiple resolutions so as to
efficiently avoid losing details. Finally, the MCM aligns motion information
from temporally adjacent frames to the current frame which achieves implicit
motion compensation at the feature level. We perform extensive experiments on
$\textit{DAVIS}_{\textit{16}}$ and $\textit{YouTube-Objects}$. Our network
achieves favorable performance while running at a faster speed compared to the
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.07636">Lagrangian Motion Magnification with Double Sparse Optical Flow Decomposition. (arXiv:2204.07636v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Flotho_P/0/1/0/all/0/1">Philipp Flotho</a>, <a href="http://arxiv.org/find/cs/1/au:+Heiss_C/0/1/0/all/0/1">Cosmas Heiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Steidl_G/0/1/0/all/0/1">Gabriele Steidl</a>, <a href="http://arxiv.org/find/cs/1/au:+Strauss_D/0/1/0/all/0/1">Daniel J. Strauss</a></p>
<p>Microexpressions are fast and spatially small facial expressions that are
difficult to detect. Therefore motion magnification techniques, which aim at
amplifying and hence revealing subtle motion in videos, appear useful for
handling such expressions. There are basically two main approaches, namely via
Eulerian or Lagrangian techniques. While the first one magnifies motion
implicitly by operating directly on image pixels, the Lagrangian approach uses
optical flow (OF) techniques to extract and magnify pixel trajectories. In this
paper, we propose a novel approach for local Lagrangian motion magnification of
facial micro-motions. Our contribution is three-fold: first, we fine tune the
recurrent all-pairs field transforms (RAFT) for OFs deep learning approach for
faces by adding ground truth obtained from the variational dense inverse search
(DIS) for OF algorithm applied to the CASME II video set of facial micro
expressions. This enables us to produce OFs of facial videos in an efficient
and sufficiently accurate way. Second, since facial micro-motions are both
local in space and time, we propose to approximate the OF field by sparse
components both in space and time leading to a double sparse decomposition.
Third, we use this decomposition to magnify micro-motions in specific areas of
the face, where we introduce a new forward warping strategy using a triangular
splitting of the image grid and barycentric interpolation of the RGB vectors at
the corners of the transformed triangles. We demonstrate the feasibility of our
approach by various examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.00932">Understanding CNNs from excitations. (arXiv:2205.00932v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1">Zijian Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qianmu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zhichao Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jun Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a></p>
<p>Saliency maps have proven to be a highly efficacious approach for explicating
the decisions of Convolutional Neural Networks. However, extant methodologies
predominantly rely on gradients, which constrain their ability to explicate
complex models. Furthermore, such approaches are not fully adept at leveraging
negative gradient information to improve interpretive veracity. In this study,
we present a novel concept, termed positive and negative excitation, which
enables the direct extraction of positive and negative excitation for each
layer, thus enabling complete layer-by-layer information utilization sans
gradients. To organize these excitations into final saliency maps, we introduce
a double-chain backpropagation procedure. A comprehensive experimental
evaluation, encompassing both binary classification and multi-classification
tasks, was conducted to gauge the effectiveness of our proposed method.
Encouragingly, the results evince that our approach offers a significant
improvement over the state-of-the-art methods in terms of salient pixel
removal, minor pixel removal, and inconspicuous adversarial perturbation
generation guidance. Additionally, we verify the correlation between positive
and negative excitations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.11230">You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clerice_T/0/1/0/all/0/1">Thibault Cl&#xe9;rice</a> (ENC, CJM, HiSoMA, UJML, ALMAnaCH)</p>
<p>Layout Analysis (the identification of zones and their classification) is the
first step along line segmentation in Optical Character Recognition and similar
tasks. The ability of identifying main body of text from marginal text or
running titles makes the difference between extracting the work full text of a
digitized book and noisy outputs. We show that most segmenters focus on pixel
classification and that polygonization of this output has not been used as a
target for the latest competition on historical document (ICDAR 2017 and
onwards), despite being the focus in the early 2010s. We propose to shift, for
efficiency, the task from a pixel classification-based polygonization to an
object detection using isothetic rectangles. We compare the output of Kraken
and YOLOv5 in terms of segmentation and show that the later severely
outperforms the first on small datasets (1110 samples and below). We release
two datasets for training and evaluation on historical documents as well as a
new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of
Kraken 4.1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10100">Lirot.ai: A Novel Platform for Crowd-Sourcing Retinal Image Segmentations. (arXiv:2208.10100v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fhima_J/0/1/0/all/0/1">Jonathan Fhima</a>, <a href="http://arxiv.org/find/cs/1/au:+Eijgen_J/0/1/0/all/0/1">Jan Van Eijgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1">Moti Freiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Stalmans_I/0/1/0/all/0/1">Ingeborg Stalmans</a>, <a href="http://arxiv.org/find/cs/1/au:+Behar_J/0/1/0/all/0/1">Joachim A. Behar</a></p>
<p>Introduction: For supervised deep learning (DL) tasks, researchers need a
large annotated dataset. In medical data science, one of the major limitations
to develop DL models is the lack of annotated examples in large quantity. This
is most often due to the time and expertise required to annotate. We introduce
Lirot. ai, a novel platform for facilitating and crowd-sourcing image
segmentations. Methods: Lirot. ai is composed of three components; an iPadOS
client application named Lirot. ai-app, a backend server named Lirot. ai-server
and a python API name Lirot. ai-API. Lirot. ai-app was developed in Swift 5.6
and Lirot. ai-server is a firebase backend. Lirot. ai-API allows the management
of the database. Lirot. ai-app can be installed on as many iPadOS devices as
needed so that annotators may be able to perform their segmentation
simultaneously and remotely. We incorporate Apple Pencil compatibility, making
the segmentation faster, more accurate, and more intuitive for the expert than
any other computer-based alternative. Results: We demonstrate the usage of
Lirot. ai for the creation of a retinal fundus dataset with reference
vasculature segmentations. Discussion and future work: We will use active
learning strategies to continue enlarging our retinal fundus dataset by
including a more efficient process to select the images to be annotated and
distribute them to annotators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.14408">RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1">Eddy Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_A/0/1/0/all/0/1">Alex Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Budhwani_A/0/1/0/all/0/1">Alikasim Budhwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Leather_O/0/1/0/all/0/1">Owen Leather</a>, <a href="http://arxiv.org/find/cs/1/au:+Dempster_R/0/1/0/all/0/1">Rowan Dempster</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanquan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Sharman_M/0/1/0/all/0/1">Mohammad Al-Sharman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rayside_D/0/1/0/all/0/1">Derek Rayside</a>, <a href="http://arxiv.org/find/cs/1/au:+Melek_W/0/1/0/all/0/1">William Melek</a></p>
<p>When applied to autonomous vehicle (AV) settings, action recognition can
enhance an environment model's situational awareness. This is especially
prevalent in scenarios where traditional geometric descriptions and heuristics
in AVs are insufficient. However, action recognition has traditionally been
studied for humans, and its limited adaptability to noisy, un-clipped,
un-pampered, raw RGB data has limited its application in other fields. To push
for the advancement and adoption of action recognition into AVs, this work
proposes a novel two-stage action recognition system, termed RALACs. RALACs
formulates the problem of action recognition for road scenes, and bridges the
gap between it and the established field of human action recognition. This work
shows how attention layers can be useful for encoding the relations across
agents, and stresses how such a scheme can be class-agnostic. Furthermore, to
address the dynamic nature of agents on the road, RALACs constructs a novel
approach to adapting Region of Interest (ROI) Alignment to agent tracks for
downstream action classification. Finally, our scheme also considers the
problem of active agent detection, and utilizes a novel application of fusing
optical flow maps to discern relevant agents in a road scene. We show that our
proposed scheme can outperform the baseline on the ICCV2021 Road Challenge
dataset and by deploying it on a real vehicle platform, we provide preliminary
insight to the usefulness of action recognition in decision making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10537">Online LiDAR-Camera Extrinsic Parameters Self-checking. (arXiv:2210.10537v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1">Pengjin Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1">Guohang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yikang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1">Kun Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>With the development of neural networks and the increasing popularity of
automatic driving, the calibration of the LiDAR and the camera has attracted
more and more attention. This calibration task is multi-modal, where the rich
color and texture information captured by the camera and the accurate
three-dimensional spatial information from the LiDAR is incredibly significant
for downstream tasks. Current research interests mainly focus on obtaining
accurate calibration results through information fusion. However, they seldom
analyze whether the calibrated results are correct or not, which could be of
significant importance in real-world applications. For example, in large-scale
production, the LiDARs and the cameras of each smart car have to get
well-calibrated as the car leaves the production line, while in the rest of the
car life period, the poses of the LiDARs and cameras should also get
continually supervised to ensure the security. To this end, this paper proposes
a self-checking algorithm to judge whether the extrinsic parameters are
well-calibrated by introducing a binary classification network based on the
fused information from the camera and the LiDAR. Moreover, since there is no
such dataset for the task in this work, we further generate a new dataset
branch from the KITTI dataset tailored for the task. Our experiments on the
proposed dataset branch demonstrate the performance of our method. To the best
of our knowledge, this is the first work to address the significance of
continually checking the calibrated extrinsic parameters for autonomous
driving. The code is open-sourced on the Github website at
https://github.com/OpenCalib/LiDAR2camera_self-check.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16584">Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition of Pneumonia from Chest X-Ray Images. (arXiv:2210.16584v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Shengchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1">Sufen Ren</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Guanjun Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1">Mengxing Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xue_C/0/1/0/all/0/1">Chenyang Xue</a></p>
<p>Chest imaging plays an essential role in diagnosing and predicting patients
with COVID-19 with evidence of worsening respiratory status. Many deep
learning-based approaches for pneumonia recognition have been developed to
enable computer-aided diagnosis. However, the long training and inference time
makes them inflexible, and the lack of interpretability reduces their
credibility in clinical medical practice. This paper aims to develop a
pneumonia recognition framework with interpretability, which can understand the
complex relationship between lung features and related diseases in chest X-ray
(CXR) images to provide high-speed analytics support for medical practice. To
reduce the computational complexity to accelerate the recognition process, a
novel multi-level self-attention mechanism within Transformer has been proposed
to accelerate convergence and emphasize the task-related feature regions.
Moreover, a practical CXR image data augmentation has been adopted to address
the scarcity of medical image data problems to boost the model's performance.
The effectiveness of the proposed method has been demonstrated on the classic
COVID-19 recognition task using the widespread pneumonia CXR image dataset. In
addition, abundant ablation experiments validate the effectiveness and
necessity of all of the components of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07273">MLIC: Multi-Reference Entropy Model for Learned Image Compression. (arXiv:2211.07273v9 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1">Jiayu Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1">Yongqi Zhai</a>, <a href="http://arxiv.org/find/eess/1/au:+Ning_P/0/1/0/all/0/1">Peirong Ning</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Ronggang Wang</a></p>
<p>Recently, learned image compression has achieved remarkable performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in boosting rate-distortion performance. However, most
entropy models only capture correlations in one dimension, while the latent
representation contain channel-wise, local spatial, and global spatial
correlations. To tackle this issue, we propose the Multi-Reference Entropy
Model (MEM) and the advanced version, MEM$^+$. These models capture the
different types of correlations present in latent representation. Specifically,
We first divide the latent representation into slices. When decoding the
current slice, we use previously decoded slices as context and employ the
attention map of the previously decoded slice to predict global correlations in
the current slice. To capture local contexts, we introduce two enhanced
checkerboard context capturing techniques that avoids performance degradation.
Based on MEM and MEM$^+$, we propose image compression models MLIC and
MLIC$^+$. Extensive experimental evaluations demonstrate that our MLIC and
MLIC$^+$ models achieve state-of-the-art performance, reducing BD-rate by
$8.05\%$ and $11.39\%$ on the Kodak dataset compared to VTM-17.0 when measured
in PSNR. Our code is available at https://github.com/JiangWeibeta/MLIC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10526">Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference. (arXiv:2211.10526v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1">Haoran You</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yunyang Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xiaoliang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Haoqi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yingyan Lin</a></p>
<p>Vision Transformers (ViTs) have shown impressive performance but still
require a high computation cost as compared to convolutional neural networks
(CNNs), one reason is that ViTs' attention measures global similarities and
thus has a quadratic complexity with the number of input tokens. Existing
efficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g.,
Performer), which sacrifice ViTs' capabilities of capturing either global or
local context. In this work, we ask an important research question: Can ViTs
learn both global and local context while being more efficient during
inference? To this end, we propose a framework called Castling-ViT, which
trains ViTs using both linear-angular attention and masked softmax-based
quadratic attention, but then switches to having only linear angular attention
during ViT inference. Our Castling-ViT leverages angular kernels to measure the
similarities between queries and keys via spectral angles. And we further
simplify it with two techniques: (1) a novel linear-angular attention
mechanism: we decompose the angular kernels into linear terms and high-order
residuals, and only keep the linear terms; and (2) we adopt two parameterized
modules to approximate high-order residuals: a depthwise convolution and an
auxiliary masked softmax attention to help learn both global and local
information, where the masks for softmax attention are regularized to gradually
become zeros and thus incur no overhead during ViT inference. Extensive
experiments and ablation studies on three tasks consistently validate the
effectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higher
accuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP on
COCO detection under comparable FLOPs, as compared to ViTs with vanilla
softmax-based attentions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06244">PathFusion: Path-consistent Lidar-Camera Deep Feature Fusion. (arXiv:2212.06244v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lemeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Meng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yunyang Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1">Raghuraman Krishnamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a></p>
<p>Fusing 3D LiDAR features with 2D camera features is a promising technique for
enhancing the accuracy of 3D detection, thanks to their complementary physical
properties. While most of the existing methods focus on directly fusing camera
features with raw LiDAR point clouds or shallow-level 3D features, it is
observed that directly combining 2D and 3D features in deeper layers actually
leads to a decrease in accuracy due to feature misalignment. The misalignment,
which stems from the aggregation of features learned from large receptive
fields, becomes increasingly more severe as we delve into deeper layers. In
this paper, we propose PathFusion as a solution to enable the alignment of
semantically coherent LiDAR-camera deep feature fusion. PathFusion introduces a
path consistency loss at multiple stages within the network, encouraging the 2D
backbone and its fusion path to transform 2D features in a way that aligns
semantically with the transformation of the 3D backbone. This ensures semantic
consistency between 2D and 3D features, even in deeper layers, and amplifies
the usage of the network's learning capacity. We apply PathFusion to improve a
prior-art fusion baseline, Focals Conv, and observe an improvement of over 1.6%
in mAP on the nuScenes test split consistently with and without testing-time
data augmentations, and moreover, PathFusion also improves KITTI
$\text{AP}_{\text{3D}}$ (R11) by about 0.6% on the moderate level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.11542">Mask Focal Loss: A unifying framework for dense crowd counting with canonical object detection networks. (arXiv:2212.11542v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1">Xiaopin Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guankun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weixiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zongze Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yuanlong Deng</a></p>
<p>As a fundamental computer vision task, crowd counting plays an important role
in public safety. Currently, deep learning based head detection is a promising
method for crowd counting. However, the highly concerned object detection
networks cannot be well applied to this problem for three reasons: (1) Existing
loss functions fail to address sample imbalance in highly dense and complex
scenes; (2) Canonical object detectors lack spatial coherence in loss
calculation, disregarding the relationship between object location and
background region; (3) Most of the head detection datasets are only annotated
with the center points, i.e. without bounding boxes. To overcome these issues,
we propose a novel Mask Focal Loss (MFL) based on heatmap via the Gaussian
kernel. MFL provides a unifying framework for the loss functions based on both
heatmap and binary feature map ground truths. Additionally, we introduce
GTA_Head, a synthetic dataset with comprehensive annotations, for evaluation
and comparison. Extensive experimental results demonstrate the superior
performance of our MFL across various detectors and datasets, and it can reduce
MAE and RMSE by up to 47.03% and 61.99%, respectively. Therefore, our work
presents a strong foundation for advancing crowd counting methods based on
density estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05246">Online Class-Incremental Learning For Real-World Food Image Classification. (arXiv:2301.05246v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raghavan_S/0/1/0/all/0/1">Siddeshwar Raghavan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiangpeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fengqing Zhu</a></p>
<p>Food image classification is essential for monitoring health and tracking
dietary in image-based dietary assessment methods. However, conventional
systems often rely on static datasets with fixed classes and uniform
distribution. In contrast, real-world food consumption patterns, shaped by
cultural, economic, and personal influences, involve dynamic and evolving data.
Thus, require the classification system to cope with continuously evolving
data. Online Class Incremental Learning (OCIL) addresses the challenge of
learning continuously from a single-pass data stream while adapting to the new
knowledge and reducing catastrophic forgetting. Experience Replay (ER) based
OCIL methods store a small portion of previous data and have shown encouraging
performance. However, most existing OCIL works assume that the distribution of
encountered data is perfectly balanced, which rarely happens in real-world
scenarios. In this work, we explore OCIL for real-world food image
classification by first introducing a probabilistic framework to simulate
realistic food consumption scenarios. Subsequently, we present an attachable
Dynamic Model Update (DMU) module designed for existing ER methods, which
enables the selection of relevant images for model training, addressing
challenges arising from data repetition and imbalanced sample occurrences
inherent in realistic food consumption patterns within the OCIL framework. Our
performance evaluation demonstrates significant enhancements compared to
established ER methods, showing great potential for lifelong learning in
real-world food image classification scenarios. The code of our method is
publicly accessible at
https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06230">Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems. (arXiv:2301.06230v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lajoie_P/0/1/0/all/0/1">Pierre-Yves Lajoie</a>, <a href="http://arxiv.org/find/cs/1/au:+Beltrame_G/0/1/0/all/0/1">Giovanni Beltrame</a></p>
<p>Collaborative Simultaneous Localization And Mapping (C-SLAM) is a vital
component for successful multi-robot operations in environments without an
external positioning system, such as indoors, underground or underwater. In
this paper, we introduce Swarm-SLAM, an open-source C-SLAM system that is
designed to be scalable, flexible, decentralized, and sparse, which are all key
properties in swarm robotics. Our system supports inertial, lidar, stereo, and
RGB-D sensing, and it includes a novel inter-robot loop closure prioritization
technique that reduces communication and accelerates convergence. We evaluated
our ROS-2 implementation on five different datasets, and in a real-world
experiment with three robots communicating through an ad-hoc network. Our code
is publicly available: https://github.com/MISTLab/Swarm-SLAM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06268">Trust your neighbours: Penalty-based constraints for model calibration. (arXiv:2303.06268v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murugesan_B/0/1/0/all/0/1">Balamurali Murugesan</a>, <a href="http://arxiv.org/find/cs/1/au:+V_S/0/1/0/all/0/1">Sukesh Adiga V</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1">Herv&#xe9; Lombaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1">Jose Dolz</a></p>
<p>Ensuring reliable confidence scores from deep networks is of pivotal
importance in critical decision-making systems, notably in the medical domain.
While recent literature on calibrating deep segmentation networks has led to
significant progress, their uncertainty is usually modeled by leveraging the
information of individual pixels, which disregards the local structure of the
object of interest. In particular, only the recent Spatially Varying Label
Smoothing (SVLS) approach addresses this issue by softening the pixel label
assignments with a discrete spatial Gaussian kernel. In this work, we first
present a constrained optimization perspective of SVLS and demonstrate that it
enforces an implicit constraint on soft class proportions of surrounding
pixels. Furthermore, our analysis shows that SVLS lacks a mechanism to balance
the contribution of the constraint with the primary objective, potentially
hindering the optimization process. Based on these observations, we propose a
principled and simple solution based on equality constraints on the logit
values, which enables to control explicitly both the enforced constraint and
the weight of the penalty, offering more flexibility. Comprehensive experiments
on a variety of well-known segmentation benchmarks demonstrate the superior
performance of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03930">Photometric Correction for Infrared Sensors. (arXiv:2304.03930v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jincheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Brink_K/0/1/0/all/0/1">Kevin Brink</a>, <a href="http://arxiv.org/find/cs/1/au:+Willis_A/0/1/0/all/0/1">Andrew R Willis</a></p>
<p>Infrared thermography has been widely used in several domains to capture and
measure temperature distributions across surfaces and objects. This methodology
can be further expanded to 3D applications if the spatial distribution of the
temperature distribution is available. Structure from Motion (SfM) is a
photometric range imaging technique that makes it possible to obtain 3D
renderings from a cloud of 2D images. To explore the possibility of 3D
reconstruction via SfM from infrared images, this article proposes a
photometric correction model for infrared sensors based on temperature
constancy. Photometric correction is accomplished by estimating the scene
irradiance as the values from the solution to a differential equation for
microbolometer pixel excitation with unknown coefficients and initial
conditions. The model was integrated into an SfM framework and experimental
evaluations demonstrate the contribution of the photometric correction for
improving the estimates of both the camera motion and the scene structure.
Further, experiments show that the reconstruction quality from the corrected
infrared imagery achieves performance on par with state-of-the-art
reconstruction using RGB sensors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04978">StageInteractor: Query-based Object Detector with Cross-stage Interaction. (arXiv:2304.04978v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yao Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haisong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Sheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a></p>
<p>Previous object detectors make predictions based on dense grid points or
numerous preset anchors. Most of these detectors are trained with one-to-many
label assignment strategies. On the contrary, recent query-based object
detectors depend on a sparse set of learnable queries and a series of decoder
layers. The one-to-one label assignment is independently applied on each layer
for the deep supervision during training. Despite the great success of
query-based object detection, however, this one-to-one label assignment
strategy demands the detectors to have strong fine-grained discrimination and
modeling capacity. To solve the above problems, in this paper, we propose a new
query-based object detector with cross-stage interaction, coined as
StageInteractor. During the forward propagation, we come up with an efficient
way to improve this modeling ability by reusing dynamic operators with
lightweight adapters. As for the label assignment, a cross-stage label assigner
is applied subsequent to the one-to-one label assignment. With this assigner,
the training target class labels are gathered across stages and then
reallocated to proper predictions at each decoder layer. On MS COCO benchmark,
our model improves the baseline by 2.2 AP, and achieves 44.8 AP with ResNet-50
as backbone, 100 queries and 12 training epochs. With longer training time and
300 queries, StageInteractor achieves 51.1 AP and 52.2 AP with ResNeXt-101-DCN
and Swin-S, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07967">360$^\circ$ High-Resolution Depth Estimation via Uncertainty-aware Structural Knowledge Transfer. (arXiv:2304.07967v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zidong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1">Hao Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilakos_A/0/1/0/all/0/1">Athanasios V. Vasilakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lin Wang</a></p>
<p>To predict high-resolution (HR) omnidirectional depth map, existing methods
typically leverage HR omnidirectional image (ODI) as the input via
fully-supervised learning. However, in practice, taking HR ODI as input is
undesired due to resource-constrained devices. In addition, depth maps are
often with lower resolution than color images. Therefore, in this paper, we
explore for the first time to estimate the HR omnidirectional depth directly
from a low-resolution (LR) ODI, when no HR depth GT map is available. Our key
idea is to transfer the scene structural knowledge from the HR image modality
and the corresponding LR depth maps to achieve the goal of HR depth estimation
without any extra inference cost. Specifically, we introduce ODI
super-resolution (SR) as an auxiliary task and train both tasks collaboratively
in a weakly supervised manner to boost the performance of HR depth estimation.
The ODI SR task extracts the scene structural knowledge via uncertainty
estimation. Buttressed by this, a scene structural knowledge transfer (SSKT)
module is proposed with two key components. First, we employ a cylindrical
implicit interpolation function (CIIF) to learn cylindrical neural
interpolation weights for feature up-sampling and share the parameters of CIIFs
between the two tasks. Then, we propose a feature distillation (FD) loss that
provides extra structural regularization to help the HR depth estimation task
learn more scene structural knowledge. Extensive experiments demonstrate that
our weakly-supervised method outperforms baseline methods, and even achieves
comparable performance with the fully-supervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10050">Neural Radiance Fields: Past, Present, and Future. (arXiv:2304.10050v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Ansh Mittal</a></p>
<p>The various aspects like modeling and interpreting 3D environments and
surroundings have enticed humans to progress their research in 3D Computer
Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall
et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in
Computer Graphics, Robotics, Computer Vision, and the possible scope of
High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D
models have gained traction from res with more than 1000 preprints related to
NeRFs published. This paper serves as a bridge for people starting to study
these fields by building on the basics of Mathematics, Geometry, Computer
Vision, and Computer Graphics to the difficulties encountered in Implicit
Representations at the intersection of all these disciplines. This survey
provides the history of rendering, Implicit Learning, and NeRFs, the
progression of research on NeRFs, and the potential applications and
implications of NeRFs in today's world. In doing so, this survey categorizes
all the NeRF-related research in terms of the datasets used, objective
functions, applications solved, and evaluation criteria for these applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10765">BPJDet: Extended Object Representation for Generic Body-Part Joint Detection. (arXiv:2304.10765v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Huayi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Fei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1">Jiaxin Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yue Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongtao Lu</a></p>
<p>Detection of human body and its parts has been intensively studied. However,
most of CNNs-based detectors are trained independently, making it difficult to
associate detected parts with body. In this paper, we focus on the joint
detection of human body and its parts. Specifically, we propose a novel
extended object representation integrating center-offsets of body parts, and
construct an end-to-end generic Body-Part Joint Detector (BPJDet). In this way,
body-part associations are neatly embedded in a unified representation
containing both semantic and geometric contents. Therefore, we can optimize
multi-loss to tackle multi-tasks synergistically. Moreover, this representation
is suitable for anchor-based and anchor-free detectors. BPJDet does not suffer
from error-prone post matching, and keeps a better trade-off between speed and
accuracy. Furthermore, BPJDet can be generalized to detect body-part or
body-parts of either human or quadruped animals. To verify the superiority of
BPJDet, we conduct experiments on datasets of body-part (CityPersons,
CrowdHuman and BodyHands) and body-parts (COCOHumanParts and Animals5C). While
keeping high detection accuracy, BPJDet achieves state-of-the-art association
performance on all datasets. Besides, we show benefits of advanced body-part
association capability by improving performance of two representative
downstream applications: accurate crowd head detection and hand contact
estimation. Project is available in https://hnuzhy.github.io/projects/BPJDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10771">A Revisit of the Normalized Eight-Point Algorithm and A Self-Supervised Deep Solution. (arXiv:2304.10771v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1">Bin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuchao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1">Yongduek Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1">Mingyi He</a></p>
<p>The normalized eight-point algorithm has been widely viewed as the
cornerstone in two-view geometry computation, where the seminal Hartley's
normalization has greatly improved the performance of the direct linear
transformation algorithm. A natural question is, whether there exists and how
to find other normalization methods that may further improve the performance as
per each input sample. In this paper, we provide a novel perspective and
propose two contributions to this fundamental problem: 1) we revisit the
normalized eight-point algorithm and make a theoretical contribution by
presenting the existence of different and better normalization algorithms; 2)
we introduce a deep convolutional neural network with a self-supervised
learning strategy for normalization. Given eight pairs of correspondences, our
network directly predicts the normalization matrices, thus learning to
normalize each input sample. Our learning-based normalization module can be
integrated with both traditional (e.g., RANSAC) and deep learning frameworks
(affording good interpretability) with minimal effort. Extensive experiments on
both synthetic and real images demonstrate the effectiveness of our proposed
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11842">Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via Algorithm-Hardware Co-Design. (arXiv:2304.11842v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yonggan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhifan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiayi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shunyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sixu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1">Haoran You</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yingyan Lin</a></p>
<p>Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06671">WeditGAN: Few-Shot Image Generation via Latent Space Relocation. (arXiv:2305.06671v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yuxuan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Li Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liqing Zhang</a></p>
<p>In few-shot image generation, directly training GAN models on just a handful
of images faces the risk of overfitting. A popular solution is to transfer the
models pretrained on large source domains to small target ones. In this work,
we introduce WeditGAN, which realizes model transfer by editing the
intermediate latent codes $w$ in StyleGANs with learned constant offsets
($\Delta w$), discovering and constructing target latent spaces via simply
relocating the distribution of source latent spaces. The established one-to-one
mapping between latent spaces can naturally prevents mode collapse and
overfitting. Besides, we also propose variants of WeditGAN to further enhance
the relocation process by regularizing the direction or finetuning the
intensity of $\Delta w$. Experiments on a collection of widely used
source/target datasets manifest the capability of WeditGAN in generating
realistic and diverse images, which is simple yet highly effective in the
research area of few-shot image generation. Codes are available at
https://github.com/Ldhlwh/WeditGAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08946">Image Matching by Bare Homography. (arXiv:2305.08946v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1">Fabio Bellavia</a></p>
<p>This paper presents Slime, a novel non-deep image matching framework which
models the scene as rough local overlapping planes. This intermediate
representation sits in-between the local affine approximation of the keypoint
patches and the global matching based on both spatial and similarity
constraints, providing a progressive pruning of the correspondences, as planes
are easier to handle with respect to general scenes.
</p>
<p>Slime decomposes the images into overlapping regions at different scales and
computes loose planar homographies. Planes are mutually extended by compatible
matches and the images are split into fixed tiles, with only the best
homographies retained for each pair of tiles. Stable matches are identified
according to the consensus of the admissible stereo configurations provided by
pairwise homographies. Within tiles, the rough planes are then merged according
to their overlap in terms of matches and further consistent correspondences are
extracted.
</p>
<p>The whole process only involves homography constraints. As a result, both the
coverage and the stability of correct matches over the scene are amplified,
together with the ability to spot matches in challenging scenes, allowing
traditional hybrid matching pipelines to make up lost ground against recent
end-to-end deep matching methods.
</p>
<p>In addition, the paper gives a thorough comparative analysis of recent
state-of-the-art in image matching represented by end-to-end deep networks and
hybrid pipelines. The evaluation considers both planar and non-planar scenes,
taking into account critical and challenging scenarios including abrupt
temporal image changes and strong variations in relative image rotations.
According to this analysis, although the impressive progress done in this
field, there is still a wide room for improvements to be investigated in future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09373">Multi-task convolutional neural network for image aesthetic assessment. (arXiv:2305.09373v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Soydaner_D/0/1/0/all/0/1">Derya Soydaner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagemans_J/0/1/0/all/0/1">Johan Wagemans</a></p>
<p>As people's aesthetic preferences for images are far from understood, image
aesthetic assessment is a challenging artificial intelligence task. The range
of factors underlying this task is almost unlimited, but we know that some
aesthetic attributes affect those preferences. In this study, we present a
multi-task convolutional neural network that takes into account these
attributes. The proposed neural network jointly learns the attributes along
with the overall aesthetic scores of images. This multi-task learning framework
allows for effective generalization through the utilization of shared
representations. Our experiments demonstrate that the proposed method
outperforms the state-of-the-art approaches in predicting overall aesthetic
scores for images in one benchmark of image aesthetics. We achieve near-human
performance in terms of overall aesthetic scores when considering the
Spearman's rank correlations. Moreover, our model pioneers the application of
multi-tasking in another benchmark, serving as a new baseline for future
research. Notably, our approach achieves this performance while using fewer
parameters compared to existing multi-task neural networks in the literature,
and consequently makes our method more efficient in terms of computational
complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12825">Uncertainty-based Detection of Adversarial Attacks in Semantic Segmentation. (arXiv:2305.12825v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1">Kira Maag</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1">Asja Fischer</a></p>
<p>State-of-the-art deep neural networks have proven to be highly powerful in a
broad range of tasks, including semantic image segmentation. However, these
networks are vulnerable against adversarial attacks, i.e., non-perceptible
perturbations added to the input image causing incorrect predictions, which is
hazardous in safety-critical applications like automated driving. Adversarial
examples and defense strategies are well studied for the image classification
task, while there has been limited research in the context of semantic
segmentation. First works however show that the segmentation outcome can be
severely distorted by adversarial attacks. In this work, we introduce an
uncertainty-based approach for the detection of adversarial attacks in semantic
segmentation. We observe that uncertainty as for example captured by the
entropy of the output distribution behaves differently on clean and perturbed
images and leverage this property to distinguish between the two cases. Our
method works in a light-weight and post-processing manner, i.e., we do not
modify the model or need knowledge of the process used for generating
adversarial examples. In a thorough empirical analysis, we demonstrate the
ability of our approach to detect perturbed images across multiple types of
adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16494">Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Haotian Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1">Alexandre Araujo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Bin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yongxin Chen</a></p>
<p>Neural networks are known to be susceptible to adversarial samples: small
variations of natural examples crafted to deliberately mislead the models.
While they can be easily generated using gradient-based techniques in digital
and physical scenarios, they often differ greatly from the actual data
distribution of natural images, resulting in a trade-off between strength and
stealthiness. In this paper, we propose a novel framework dubbed
Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic
adversarial samples. By exploiting a gradient guided by a diffusion model,
Diff-PGD ensures that adversarial samples remain close to the original data
distribution while maintaining their effectiveness. Moreover, our framework can
be easily customized for specific tasks such as digital attacks, physical-world
attacks, and style-based attacks. Compared with existing methods for generating
natural-style adversarial samples, our framework enables the separation of
optimizing adversarial loss from other surrogate losses (e.g.,
content/smoothness/style loss), making it more stable and controllable.
Finally, we demonstrate that the samples generated using Diff-PGD have better
transferability and anti-purification power than traditional gradient-based
methods. Code will be released in https://github.com/xavihart/Diff-PGD
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16801">Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sartinas_E/0/1/0/all/0/1">Evangelos G. Sartinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Psarakis_E/0/1/0/all/0/1">Emmanouil Z. Psarakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1">Dimitrios I. Kosmopoulos</a></p>
<p>An interesting problem in many video-based applications is the generation of
short synopses by selecting the most informative frames, a procedure which is
known as video summarization. For sign language videos the benefits of using
the $t$-parameterized counterpart of the curvature of the 2-D signer's wrist
trajectory to identify keyframes, have been recently reported in the
literature. In this paper we extend these ideas by modeling the 3-D hand motion
that is extracted from each frame of the video. To this end we propose a new
informative function based on the $t$-parameterized curvature and torsion of
the 3-D trajectory. The method to characterize video frames as keyframes
depends on whether the motion occurs in 2-D or 3-D space. Specifically, in the
case of 3-D motion we look for the maxima of the harmonic mean of the curvature
and torsion of the target's trajectory; in the planar motion case we seek for
the maxima of the trajectory's curvature. The proposed 3-D feature is
experimentally evaluated in applications of sign language videos on (1)
objective measures using ground-truth keyframe annotations, (2) human-based
evaluation of understanding, and (3) gloss classification and the results
obtained are promising.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18455">Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weijian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Tianyang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shifeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiacheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihua Zhang</a></p>
<p>Due to the ease of training, ability to scale, and high sample quality,
diffusion models (DMs) have become the preferred option for generative
modeling, with numerous pre-trained models available for a wide variety of
datasets. Containing intricate information about data distributions,
pre-trained DMs are valuable assets for downstream applications. In this work,
we consider learning from pre-trained DMs and transferring their knowledge to
other generative models in a data-free fashion. Specifically, we propose a
general framework called Diff-Instruct to instruct the training of arbitrary
generative models as long as the generated samples are differentiable with
respect to the model parameters. Our proposed Diff-Instruct is built on a
rigorous mathematical foundation where the instruction process directly
corresponds to minimizing a novel divergence we call Integral Kullback-Leibler
(IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL
divergence along a diffusion process, which we show to be more robust in
comparing distributions with misaligned supports. We also reveal non-trivial
connections of our method to existing works such as DreamFusion, and generative
adversarial training. To demonstrate the effectiveness and universality of
Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion
models and refining existing GAN models. The experiments on distilling
pre-trained diffusion models show that Diff-Instruct results in
state-of-the-art single-step diffusion-based models. The experiments on
refining GAN models show that the Diff-Instruct can consistently improve the
pre-trained generators of GAN models across various settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19556">Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation. (arXiv:2305.19556v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Se Jin Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jeongsoo Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1">Yong Man Ro</a></p>
<p>Talking face generation is the challenging task of synthesizing a natural and
realistic face that requires accurate synchronization with a given audio. Due
to co-articulation, where an isolated phone is influenced by the preceding or
following phones, the articulation of a phone varies upon the phonetic context.
Therefore, modeling lip motion with the phonetic context can generate more
spatio-temporally aligned lip movement. In this respect, we investigate the
phonetic context in generating lip motion for talking face generation. We
propose Context-Aware Lip-Sync framework (CALS), which explicitly leverages
phonetic context to generate lip movement of the target face. CALS is comprised
of an Audio-to-Lip module and a Lip-to-Face module. The former is pretrained
based on masked learning to map each phone to a contextualized lip motion unit.
The contextualized lip motion unit then guides the latter in synthesizing a
target identity with context-aware lip motion. From extensive experiments, we
verify that simply exploiting the phonetic context in the proposed CALS
framework effectively enhances spatio-temporal alignment. We also demonstrate
the extent to which the phonetic context assists in lip synchronization and
find the effective window size for lip generation to be approximately 1.2
seconds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20089">Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images. (arXiv:2305.20089v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Junxing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zerui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengcheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunlong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhenan Sun</a></p>
<p>Reconstructing hand-held objects from monocular RGB images is an appealing
yet challenging task. In this task, contacts between hands and objects provide
important cues for recovering the 3D geometry of the hand-held objects. Though
recent works have employed implicit functions to achieve impressive progress,
they ignore formulating contacts in their frameworks, which results in
producing less realistic object meshes. In this work, we explore how to model
contacts in an explicit way to benefit the implicit reconstruction of hand-held
objects. Our method consists of two components: explicit contact prediction and
implicit shape reconstruction. In the first part, we propose a new subtask of
directly estimating 3D hand-object contacts from a single image. The part-level
and vertex-level graph-based transformers are cascaded and jointly learned in a
coarse-to-fine manner for more accurate contact probabilities. In the second
part, we introduce a novel method to diffuse estimated contact states from the
hand mesh surface to nearby 3D space and leverage diffused contact
probabilities to construct the implicit neural representation for the
manipulated object. Benefiting from estimating the interaction patterns between
the hand and the object, our method can reconstruct more realistic object
meshes, especially for object parts that are in contact with hands. Extensive
experiments on challenging benchmarks show that the proposed method outperforms
the current state of the arts by a great margin. Our code is publicly available
at https://junxinghu.github.io/projects/hoi.html.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00965">BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From A Single Image. (arXiv:2306.00965v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tao Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a></p>
<p>Understanding and modeling the 3D scene from a single image is a practical
problem. A recent advance proposes a panoptic 3D scene reconstruction task that
performs both 3D reconstruction and 3D panoptic segmentation from a single
image. Although having made substantial progress, recent works only focus on
top-down approaches that fill 2D instances into 3D voxels according to
estimated depth, which hinders their performance by two ambiguities. (1)
instance-channel ambiguity: The variable ids of instances in each scene lead to
ambiguity during filling voxel channels with 2D information, confusing the
following 3D refinement. (2) voxel-reconstruction ambiguity: 2D-to-3D lifting
with estimated single view depth only propagates 2D information onto the
surface of 3D regions, leading to ambiguity during the reconstruction of
regions behind the frontal view surface. In this paper, we propose BUOL, a
Bottom-Up framework with Occupancy-aware Lifting to address the two issues for
panoptic 3D scene reconstruction from a single image. For instance-channel
ambiguity, a bottom-up framework lifts 2D information to 3D voxels based on
deterministic semantic assignments rather than arbitrary instance id
assignments. The 3D voxels are then refined and grouped into 3D instances
according to the predicted 2D instance centers. For voxel-reconstruction
ambiguity, the estimated multi-plane occupancy is leveraged together with depth
to fill the whole regions of things and stuff. Our method shows a tremendous
performance advantage over state-of-the-art methods on synthetic dataset
3D-Front and real-world dataset Matterport3D. Code and models are available in
https://github.com/chtsy/buol.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01222">Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Shuvendu Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1">Ali Etemad</a></p>
<p>We propose UnMixMatch, a semi-supervised learning framework which can learn
effective representations from unconstrained unlabelled data in order to scale
up performance. Most existing semi-supervised methods rely on the assumption
that labelled and unlabelled samples are drawn from the same distribution,
which limits the potential for improvement through the use of free-living
unlabeled data. Consequently, the generalizability and scalability of
semi-supervised learning are often hindered by this assumption. Our method aims
to overcome these constraints and effectively utilize unconstrained unlabelled
data in semi-supervised learning. UnMixMatch consists of three main components:
a supervised learner with hard augmentations that provides strong
regularization, a contrastive consistency regularizer to learn underlying
representations from the unlabelled data, and a self-supervised loss to enhance
the representations that are learnt from the unlabelled data. We perform
extensive experiments on 4 commonly used datasets and demonstrate superior
performance over existing semi-supervised methods with a performance boost of
4.79%. Extensive ablation and sensitivity studies show the effectiveness and
impact of each of the proposed components of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03988">Learn the Force We Can: Enabling Sparse Motion Control in Multi-Object Video Generation. (arXiv:2306.03988v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1">Aram Davtyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1">Paolo Favaro</a></p>
<p>We propose a novel unsupervised method to autoregressively generate videos
from a single frame and a sparse motion input. Our trained model can generate
unseen realistic object-to-object interactions. Although our model has never
been given the explicit segmentation and motion of each object in the scene
during training, it is able to implicitly separate their dynamics and extents.
Key components in our method are the randomized conditioning scheme, the
encoding of the input motion control, and the randomized and sparse sampling to
enable generalization to out of distribution but realistic correlations. Our
model, which we call YODA, has therefore the ability to move objects without
physically touching them. Through extensive qualitative and quantitative
evaluations on several datasets, we show that YODA is on par with or better
than state of the art video generation prior work in terms of both
controllability and video quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08247">Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation. (arXiv:2306.08247v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1">Zhihao Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Ye Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yu Wu</a></p>
<p>Originating from the diffusion phenomenon in physics that describes particle
movement, the diffusion generative models inherit the characteristics of
stochastic random walk in the data space along the denoising trajectory.
However, the intrinsic mutual interference among image regions contradicts the
need for practical downstream application scenarios where the preservation of
low-level pixel information from given conditioning is desired (e.g.,
customization tasks like personalized generation and inpainting based on a
user-provided single image). In this work, we investigate the diffusion
(physics) in diffusion (machine learning) properties and propose our Cyclic
One-Way Diffusion (COW) method to control the direction of diffusion phenomenon
given a pre-trained frozen diffusion model for versatile customization
application scenarios, where the low-level pixel information from the
conditioning needs to be preserved. Notably, unlike most current methods that
incorporate additional conditions by fine-tuning the base text-to-image
diffusion model or learning auxiliary networks, our method provides a novel
perspective to understand the task needs and is applicable to a wider range of
customization scenarios in a learning-free manner. Extensive experiment results
show that our proposed COW can achieve more flexible customization based on
strict visual conditions in different application settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10898">B-cos Alignment for Inherently Interpretable CNNs and Vision Transformers. (arXiv:2306.10898v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bohle_M/0/1/0/all/0/1">Moritz B&#xf6;hle</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1">Navdeeppal Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1">Mario Fritz</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1">Bernt Schiele</a></p>
<p>We present a new direction for increasing the interpretability of deep neural
networks (DNNs) by promoting weight-input alignment during training. For this,
we propose to replace the linear transformations in DNNs by our novel B-cos
transformation. As we show, a sequence (network) of such transformations
induces a single linear transformation that faithfully summarises the full
model computations. Moreover, the B-cos transformation is designed such that
the weights align with relevant signals during optimisation. As a result, those
induced linear transformations become highly interpretable and highlight
task-relevant features. Importantly, the B-cos transformation is designed to be
compatible with existing architectures and we show that it can easily be
integrated into virtually all of the latest state of the art models for
computer vision - e.g. ResNets, DenseNets, ConvNext models, as well as Vision
Transformers - by combining the B-cos-based explanations with normalisation and
attention layers, all whilst maintaining similar accuracy on ImageNet. Finally,
we show that the resulting explanations are of high visual quality and perform
well under quantitative interpretability metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11238">CAMP-Net: Consistency-Aware Multi-Prior Network for Accelerated MRI Reconstruction. (arXiv:2306.11238v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Liping Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaobo Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1">Weitian Chen</a></p>
<p>Undersampling k-space data in MRI reduces scan time but pose challenges in
image reconstruction. Considerable progress has been made in reconstructing
accelerated MRI. However, restoration of high-frequency image details in highly
undersampled data remains challenging. To address this issue, we propose
CAMP-Net, an unrolling-based Consistency-Aware Multi-Prior Network for
accelerated MRI reconstruction. CAMP-Net leverages complementary multi-prior
knowledge and multi-slice information from various domains to enhance
reconstruction quality. Specifically, CAMP-Net comprises three interleaved
modules for image enhancement, k-space restoration, and calibration
consistency, respectively. These modules jointly learn priors from data in
image domain, k-domain, and calibration region, respectively, in data-driven
manner during each unrolled iteration. Notably, the encoded calibration prior
knowledge extracted from auto-calibrating signals implicitly guides the
learning of consistency-aware k-space correlation for reliable interpolation of
missing k-space data. To maximize the benefits of image domain and k-domain
prior knowledge, the reconstructions are aggregated in a frequency fusion
module, exploiting their complementary properties to optimize the trade-off
between artifact removal and fine detail preservation. Additionally, we
incorporate a surface data fidelity layer during the learning of k-domain and
calibration domain priors to prevent degradation of the reconstruction caused
by padding-induced data imperfections. We evaluate the generalizability and
robustness of our method on three large public datasets with varying
acceleration factors and sampling patterns. The experimental results
demonstrate that our method outperforms state-of-the-art approaches in terms of
both reconstruction quality and $T_2$ mapping estimation, particularly in
scenarios with high acceleration factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11990">Physics-constrained Attack against Convolution-based Human Motion Prediction. (arXiv:2306.11990v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1">Chengxu Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1">Yonghao Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jianqin Yin</a></p>
<p>Human motion prediction has achieved a brilliant performance with the help of
convolution-based neural networks. However, currently, there is no work
evaluating the potential risk in human motion prediction when facing
adversarial attacks. The adversarial attack will encounter problems against
human motion prediction in naturalness and data scale. To solve the problems
above, we propose a new adversarial attack method that generates the worst-case
perturbation by maximizing the human motion predictor's prediction error with
physical constraints. Specifically, we introduce a novel adaptable scheme that
facilitates the attack to suit the scale of the target pose and two physical
constraints to enhance the naturalness of the adversarial example. The
evaluating experiments on three datasets show that the prediction errors of all
target models are enlarged significantly, which means current convolution-based
human motion prediction models are vulnerable to the proposed attack. Based on
the experimental results, we provide insights on how to enhance the adversarial
robustness of the human motion predictor and how to improve the adversarial
attack against human motion prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12048">Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering. (arXiv:2306.12048v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xi_L/0/1/0/all/0/1">Lin Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weihai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengguo Li</a></p>
<p>Online unsupervised video object segmentation (UVOS) uses the previous frames
as its input to automatically separate the primary object(s) from a streaming
video without using any further manual annotation. A major challenge is that
the model has no access to the future and must rely solely on the history,
i.e., the segmentation mask is predicted from the current frame as soon as it
is captured. In this work, a novel contrastive motion clustering algorithm with
an optical flow as its input is proposed for the online UVOS by exploiting the
common fate principle that visual elements tend to be perceived as a group if
they possess the same motion pattern. We build a simple and effective
auto-encoder to iteratively summarize non-learnable prototypical bases for the
motion pattern, while the bases in turn help learn the representation of the
embedding network. Further, a contrastive learning strategy based on a boundary
prior is developed to improve foreground and background feature discrimination
in the representation learning stage. The proposed algorithm can be optimized
on arbitrarily-scale data i.e., frame, clip, dataset) and performed in an
online fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,
and $\textit{SegTrackV2}$ datasets show that the accuracy of our method
surpasses the previous state-of-the-art (SoTA) online UVOS method by a margin
of 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deep
subspace clustering to tackle the motion grouping, our method is able to
achieve higher accuracy at $3\times$ faster inference time compared to SoTA
online UVOS method, and making a good trade-off between effectiveness and
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14153">DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data. (arXiv:2306.14153v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jingyuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Huimin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiansheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jian Yuan</a></p>
<p>Denoising diffusion probabilistic models (DDPMs) have been proven capable of
synthesizing high-quality images with remarkable diversity when trained on
large amounts of data. Typical diffusion models and modern large-scale
conditional generative models like text-to-image generative models are
vulnerable to overfitting when fine-tuned on extremely limited data. Existing
works have explored subject-driven generation using a reference set containing
a few images. However, few prior works explore DDPM-based domain-driven
generation, which aims to learn the common features of target domains while
maintaining diversity. This paper proposes a novel DomainStudio approach to
adapt DDPMs pre-trained on large-scale source datasets to target domains using
limited data. It is designed to keep the diversity of subjects provided by
source domains and get high-quality and diverse adapted samples in target
domains. We propose to keep the relative distances between adapted samples to
achieve considerable generation diversity. In addition, we further enhance the
learning of high-frequency details for better generation quality. Our approach
is compatible with both unconditional and conditional diffusion models. This
work makes the first attempt to realize unconditional few-shot image generation
with diffusion models, achieving better quality and greater diversity than
current state-of-the-art GAN-based approaches. Moreover, this work also
significantly relieves overfitting for conditional generation and realizes
high-quality domain-driven generation, further expanding the applicable
scenarios of modern large-scale text-to-image models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14448">Progressive Energy-Based Cooperative Learning for Multi-Domain Image-to-Image Translation. (arXiv:2306.14448v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1">Weinan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yaxuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingnian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jianwen Xie</a></p>
<p>This paper studies a novel energy-based cooperative learning framework for
multi-domain image-to-image translation. The framework consists of four
components: descriptor, translator, style encoder, and style generator. The
descriptor is a multi-head energy-based model that represents a multi-domain
image distribution. The components of translator, style encoder, and style
generator constitute a diversified image generator. Specifically, given an
input image from a source domain, the translator turns it into a stylised
output image of the target domain according to a style code, which can be
inferred by the style encoder from a reference image or produced by the style
generator from a random noise. Since the style generator is represented as an
domain-specific distribution of style codes, the translator can provide a
one-to-many transformation (i.e., diversified generation) between source domain
and target domain. To train our framework, we propose a likelihood-based
multi-domain cooperative learning algorithm to jointly train the multi-domain
descriptor and the diversified image generator (including translator, style
encoder, and style generator modules) via multi-domain MCMC teaching, in which
the descriptor guides the diversified image generator to shift its probability
density toward the data distribution, while the diversified image generator
uses its randomly translated images to initialize the descriptor's Langevin
dynamics process for efficient sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14525">ParameterNet: Parameters Are All You Need. (arXiv:2306.14525v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1">Enhua Wu</a></p>
<p>The large-scale visual pretraining has significantly improve the performance
of large vision models. However, we observe the \emph{low FLOPs pitfall} that
the existing low-FLOPs models cannot benefit from large-scale pretraining. In
this paper, we introduce a novel design principle, termed ParameterNet, aimed
at augmenting the number of parameters in large-scale visual pretraining models
while minimizing the increase in FLOPs. We leverage dynamic convolutions to
incorporate additional parameters into the networks with only a marginal rise
in FLOPs. The ParameterNet approach allows low-FLOPs networks to take advantage
of large-scale visual pretraining. Furthermore, we extend the ParameterNet
concept to the language domain to enhance inference results while preserving
inference speed. Experiments on the large-scale ImageNet-22K have shown the
superiority of our ParameterNet scheme. For example, ParameterNet-600M can
achieve higher accuracy on ImageNet than the widely-used Swin Transformer
(81.6\% \emph{vs.} 80.9\%) and has much lower FLOPs (0.6G \emph{vs.} 4.5G). In
the language domain, LLaMA-1B enhanced with ParameterNet achieves 2\% higher
accuracy over vanilla LLaMA. The code will be released at
\url{https://parameternet.github.io/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14685">DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Ximing Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Haitao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a></p>
<p>Even though trained mainly on images, we discover that pretrained diffusion
models show impressive power in guiding sketch synthesis. In this paper, we
present DiffSketcher, an innovative algorithm that creates \textit{vectorized}
free-hand sketches using natural language input. DiffSketcher is developed
based on a pre-trained text-to-image diffusion model. It performs the task by
directly optimizing a set of B\'ezier curves with an extended version of the
score distillation sampling (SDS) loss, which allows us to use a raster-level
diffusion model as a prior for optimizing a parametric vectorized sketch
generator. Furthermore, we explore attention maps embedded in the diffusion
model for effective stroke initialization to speed up the generation process.
The generated sketches demonstrate multiple levels of abstraction while
maintaining recognizability, underlying structure, and essential visual details
of the subject drawn. Our experiments show that DiffSketcher achieves greater
quality than prior work. The code and demo of DiffSketcher can be found at
https://ximinng.github.io/DiffSketcher-project/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00097">Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation. (arXiv:2307.00097v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murugesan_B/0/1/0/all/0/1">Balamurali Murugesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1">Rukhshanda Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1">Rajarshi Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1">Jose Dolz</a></p>
<p>Recently, CLIP-based approaches have exhibited remarkable performance on
generalization and few-shot learning tasks, fueled by the power of contrastive
language-vision pre-training. In particular, prompt tuning has emerged as an
effective strategy to adapt the pre-trained language-vision models to
downstream tasks by employing task-related textual tokens. Motivated by this
progress, in this work we question whether other fundamental problems, such as
weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning.
Our findings reveal two interesting observations that shed light on the impact
of prompt tuning on WSSS. First, modifying only the class token of the text
prompt results in a greater impact on the Class Activation Map (CAM), compared
to arguably more complex strategies that optimize the context. And second, the
class token associated with the image ground truth does not necessarily
correspond to the category that yields the best CAM. Motivated by these
observations, we introduce a novel approach based on a PrOmpt cLass lEarning
(POLE) strategy. Through extensive experiments we demonstrate that our simple,
yet efficient approach achieves SOTA performance in a well-known WSSS
benchmark. These results highlight not only the benefits of language-vision
models in WSSS but also the potential of prompt learning for this problem. The
code is available at https://github.com/rB080/WSS_POLE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02251">RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McDonnell_M/0/1/0/all/0/1">Mark D. McDonnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1">Dong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Parveneh_A/0/1/0/all/0/1">Amin Parveneh</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1">Ehsan Abbasnejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a></p>
<p>Continual learning (CL) aims to incrementally learn different tasks (such as
classification) in a non-stationary data stream without forgetting old ones.
Most CL works focus on tackling catastrophic forgetting under a
learning-from-scratch paradigm. However, with the increasing prominence of
foundation models, pre-trained models equipped with informative representations
have become available for various downstream requirements. Several CL methods
based on pre-trained models have been explored, either utilizing pre-extracted
features directly (which makes bridging distribution gaps challenging) or
incorporating adaptors (which may be subject to forgetting). In this paper, we
propose a concise and effective approach for CL with pre-trained models. Given
that forgetting occurs during parameter updating, we contemplate an alternative
approach that exploits training-free random projectors and class-prototype
accumulation, which thus bypasses the issue. Specifically, we inject a frozen
Random Projection layer with nonlinear activation between the pre-trained
model's feature representations and output head, which captures interactions
between features with expanded dimensionality, providing enhanced linear
separability for class-prototype-based CL. We also demonstrate the importance
of decorrelating the class-prototypes to reduce the distribution disparity when
using pre-trained representations. These techniques prove to be effective and
circumvent the problem of forgetting for both class- and domain-incremental
continual learning. Compared to previous methods applied to pre-trained
ViT-B/16 models, we reduce final error rates by between 20% and 62% on seven
class-incremental benchmarks, despite not using any rehearsal memory. We
conclude that the full potential of pre-trained models for simple, effective,
and fast CL has not hitherto been fully tapped. Code is at
github.com/RanPAC/RanPAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04596">Distill-SODA: Distilling Self-Supervised Vision Transformer for Source-Free Open-Set Domain Adaptation in Computational Pathology. (arXiv:2307.04596v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vray_G/0/1/0/all/0/1">Guillaume Vray</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomar_D/0/1/0/all/0/1">Devavrat Tomar</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1">Jean-Philippe Thiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1">Behzad Bozorgtabar</a></p>
<p>Developing computational pathology models is essential for reducing manual
tissue typing from whole slide images, transferring knowledge from the source
domain to an unlabeled, shifted target domain, and identifying unseen
categories. We propose a practical setting by addressing the above-mentioned
challenges in one fell swoop, i.e., source-free open-set domain adaptation. Our
methodology focuses on adapting a pre-trained source model to an unlabeled
target dataset and encompasses both closed-set and open-set classes. Beyond
addressing the semantic shift of unknown classes, our framework also deals with
a covariate shift, which manifests as variations in color appearance between
source and target tissue samples. Our method hinges on distilling knowledge
from a self-supervised vision transformer (ViT), drawing guidance from either
robustly pre-trained transformer models or histopathology datasets, including
those from the target domain. In pursuit of this, we introduce a novel
style-based adversarial data augmentation, serving as hard positives for
self-training a ViT, resulting in highly contextualized embeddings. Following
this, we cluster semantically akin target images, with the source model
offering weak pseudo-labels, albeit with uncertain confidence. To enhance this
process, we present the closed-set affinity score (CSAS), aiming to correct the
confidence levels of these pseudo-labels and to calculate weighted class
prototypes within the contextualized embedding space. Our approach establishes
itself as state-of-the-art across three public histopathological datasets for
colorectal cancer assessment. Notably, our self-training method seamlessly
integrates with open-set detection methods, resulting in enhanced performance
in both closed-set and open-set recognition tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12130">Estimating temperatures with low-cost infrared cameras using deep neural networks. (arXiv:2307.12130v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oz_N/0/1/0/all/0/1">Navot Oz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sochen_N/0/1/0/all/0/1">Nir Sochen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendelovich_D/0/1/0/all/0/1">David Mendelovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Klapp_I/0/1/0/all/0/1">Iftach Klapp</a></p>
<p>Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have
space-variant nonuniformity across their detector. Both inaccuracy and
nonuniformity are dependent on the ambient temperature of the camera. The goal
of this work was to estimate temperatures with low-cost infrared cameras, and
rectify the nonuniformity.
</p>
<p>A nonuniformity simulator that accounts for the ambient temperature was
developed. An end-to-end neural network that incorporates both the physical
model of the camera and the ambient camera temperature was introduced. The
neural network was trained with the simulated nonuniformity data to estimate
the object's temperature and correct the nonuniformity, using only a single
image and the ambient temperature measured by the camera itself. Results of the
proposed method significantly improved the mean temperature error compared to
previous works by up to $0.5^\circ C$. In addition, constraining the physical
model of the camera with the network lowered the error by an additional
$0.1^\circ C$.
</p>
<p>The mean temperature error over an extensive validation dataset was
$0.37^\circ C$. The method was verified on real data in the field and produced
equivalent results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12542">Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging. (arXiv:2307.12542v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Meirui Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yuan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1">Anjie Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1">Qi Dou</a></p>
<p>Despite recent progress in enhancing the privacy of federated learning (FL)
via differential privacy (DP), the trade-off of DP between privacy protection
and performance is still underexplored for real-world medical scenario. In this
paper, we propose to optimize the trade-off under the context of client-level
DP, which focuses on privacy during communications. However, FL for medical
imaging involves typically much fewer participants (hospitals) than other
domains (e.g., mobile devices), thus ensuring clients be differentially private
is much more challenging. To tackle this problem, we propose an adaptive
intermediary strategy to improve performance without harming privacy.
Specifically, we theoretically find splitting clients into sub-clients, which
serve as intermediaries between hospitals and the server, can mitigate the
noises introduced by DP without harming privacy. Our proposed approach is
empirically evaluated on both classification and segmentation tasks using two
public datasets, and its effectiveness is demonstrated with significant
performance improvements and comprehensive analytical studies. Code is
available at: https://github.com/med-air/Client-DP-FL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14288">US \&amp; MRI Image Fusion Based on Markerless Skin Registration. (arXiv:2307.14288v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paccini_M/0/1/0/all/0/1">Martina Paccini</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschina_G/0/1/0/all/0/1">Giacomo Paschina</a>, <a href="http://arxiv.org/find/cs/1/au:+Beni_S/0/1/0/all/0/1">Stefano De Beni</a>, <a href="http://arxiv.org/find/cs/1/au:+Patane_G/0/1/0/all/0/1">Giuseppe Patan&#xe8;</a></p>
<p>This paper presents an innovative automatic fusion imaging system that
combines 3D CT/MR images with real-time ultrasound (US) acquisition. The system
eliminates the need for external physical markers and complex training, making
image fusion feasible for physicians with different experience levels. The
integrated system involves a portable 3D camera for patient-specific surface
acquisition, an electromagnetic tracking system, and US components. The fusion
algorithm comprises two main parts: skin segmentation and rigid
co-registration, both integrated into the US machine. The co-registration
software aligns the surface extracted from CT/MR images with patient-specific
coordinates, facilitating rapid and effective fusion. Experimental testing in
different settings, including the clinical environment, validates the system's
accuracy, computational efficiency, noise robustness, and operator
independence. The co-registration error remains under the acceptable range
of~$1$ cm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15220">Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Kun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1">Vinkle Srivastav</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1">Joel L. Lavanchy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1">Pietro Mascagni</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1">Nicolas Padoy</a></p>
<p>Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15421">MLIC++: Linear Complexity Attention-based Multi-Reference Entropy Modeling for Learned Image Compression. (arXiv:2307.15421v6 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1">Jiayu Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1">Yongqi Zhai</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Ronggang Wang</a></p>
<p>Recently, learned image compression has achieved impressive performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in enhancing rate-distortion performance. However,
existing global context modules rely on computationally intensive quadratic
complexity computations to capture global correlations. This quadratic
complexity imposes limitations on the potential of high-resolution image
coding. Moreover, effectively capturing local, global, and channel-wise
contexts with acceptable even linear complexity within a single entropy model
remains a challenge. To address these limitations, we propose the Linear
Complexity Attention-based Multi-Reference Entropy Model (MEM++). MEM++
effectively captures the diverse range of correlations inherent in the latent
representation. Specifically, the latent representation is first divided into
multiple slices. When compressing a particular slice, the previously compressed
slices serve as its channel-wise contexts. To capture local contexts without
sacrificing performance, we introduce a novel checkerboard attention module.
Additionally, to capture global contexts, we propose the linear complexity
attention-based global correlations capturing by leveraging the decomposition
of the softmax operation. The attention map of the previously decoded slice is
implicitly computed and employed to predict global correlations in the current
slice. Based on MEM++, we propose image compression model MLIC++. Extensive
experimental evaluations demonstrate that our MLIC++ achieves state-of-the-art
performance, reducing BD-rate by 13.39% on the Kodak dataset compared to
VTM-17.0 in PSNR. Furthermore, MLIC++ exhibits linear GPU memory consumption
with resolution, making it highly suitable for high-resolution image coding.
Code and pre-trained models are available at
https://github.com/JiangWeibeta/MLIC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05404">Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network. (arXiv:2308.05404v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xianqiang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a></p>
<p>This paper presents a novel and interpretable end-to-end learning framework,
called the deep compensation unfolding network (DCUNet), for restoring light
field (LF) images captured under low-light conditions. DCUNet is designed with
a multi-stage architecture that mimics the optimization process of solving an
inverse imaging problem in a data-driven fashion. The framework uses the
intermediate enhanced result to estimate the illumination map, which is then
employed in the unfolding process to produce a new enhanced result.
Additionally, DCUNet includes a content-associated deep compensation module at
each optimization stage to suppress noise and illumination map estimation
errors. To properly mine and leverage the unique characteristics of LF images,
this paper proposes a pseudo-explicit feature interaction module that
comprehensively exploits redundant information in LF images. The experimental
results on both simulated and real datasets demonstrate the superiority of our
DCUNet over state-of-the-art methods, both qualitatively and quantitatively.
Moreover, DCUNet preserves the essential geometric structure of enhanced LF
images much better. The code will be publicly available at
https://github.com/lyuxianqiang/LFLL-DCU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09460">Accelerated Bayesian imaging by relaxed proximal-point Langevin sampling. (arXiv:2308.09460v2 [stat.CO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Klatzer_T/0/1/0/all/0/1">Teresa Klatzer</a>, <a href="http://arxiv.org/find/stat/1/au:+Dobson_P/0/1/0/all/0/1">Paul Dobson</a>, <a href="http://arxiv.org/find/stat/1/au:+Altmann_Y/0/1/0/all/0/1">Yoann Altmann</a>, <a href="http://arxiv.org/find/stat/1/au:+Pereyra_M/0/1/0/all/0/1">Marcelo Pereyra</a>, <a href="http://arxiv.org/find/stat/1/au:+Sanz_Serna_J/0/1/0/all/0/1">Jes&#xfa;s Mar&#xed;a Sanz-Serna</a>, <a href="http://arxiv.org/find/stat/1/au:+Zygalakis_K/0/1/0/all/0/1">Konstantinos C. Zygalakis</a></p>
<p>This paper presents a new accelerated proximal Markov chain Monte Carlo
methodology to perform Bayesian inference in imaging inverse problems with an
underlying convex geometry. The proposed strategy takes the form of a
stochastic relaxed proximal-point iteration that admits two complementary
interpretations. For models that are smooth or regularised by Moreau-Yosida
smoothing, the algorithm is equivalent to an implicit midpoint discretisation
of an overdamped Langevin diffusion targeting the posterior distribution of
interest. This discretisation is asymptotically unbiased for Gaussian targets
and shown to converge in an accelerated manner for any target that is
$\kappa$-strongly log-concave (i.e., requiring in the order of $\sqrt{\kappa}$
iterations to converge, similarly to accelerated optimisation schemes),
comparing favorably to [M. Pereyra, L. Vargas Mieles, K.C. Zygalakis, SIAM J.
Imaging Sciences, 13,2 (2020), pp. 905-935] which is only provably accelerated
for Gaussian targets and has bias. For models that are not smooth, the
algorithm is equivalent to a Leimkuhler-Matthews discretisation of a Langevin
diffusion targeting a Moreau-Yosida approximation of the posterior distribution
of interest, and hence achieves a significantly lower bias than conventional
unadjusted Langevin strategies based on the Euler-Maruyama discretisation. For
targets that are $\kappa$-strongly log-concave, the provided non-asymptotic
convergence analysis also identifies the optimal time step which maximizes the
convergence speed. The proposed methodology is demonstrated through a range of
experiments related to image deconvolution with Gaussian and Poisson noise,
with assumption-driven and data-driven convex priors. Source codes for the
numerical experiments of this paper are available from
https://github.com/MI2G/accelerated-langevin-imla.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13363">CS-Mixer: A Cross-Scale Vision MLP Model with Spatial-Channel Mixing. (arXiv:2308.13363v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jonathan Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Araujo_D/0/1/0/all/0/1">David A. Araujo</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Suman Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1">Md. Faisal Kabir</a></p>
<p>Despite their simpler information fusion designs compared with Vision
Transformers and Convolutional Neural Networks, Vision MLP architectures have
demonstrated strong performance and high data efficiency in recent research.
However, existing works such as CycleMLP and Vision Permutator typically model
spatial information in equal-size spatial regions and do not consider
cross-scale spatial interactions. Further, their token mixers only model 1- or
2-axis correlations, avoiding 3-axis spatial-channel mixing due to its
computational demands. We therefore propose CS-Mixer, a hierarchical Vision MLP
that learns dynamic low-rank transformations for spatial-channel mixing through
cross-scale local and global aggregation. The proposed methodology achieves
competitive results on popular image recognition benchmarks without incurring
substantially more compute. Our largest model, CS-Mixer-L, reaches 83.2% top-1
accuracy on ImageNet-1k with 13.7 GFLOPs and 94 M parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03744">Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images. (arXiv:2309.03744v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Moradinasab_N/0/1/0/all/0/1">Nazanin Moradinasab</a>, <a href="http://arxiv.org/find/eess/1/au:+Deaton_R/0/1/0/all/0/1">Rebecca A. Deaton</a>, <a href="http://arxiv.org/find/eess/1/au:+Shankman_L/0/1/0/all/0/1">Laura S. Shankman</a>, <a href="http://arxiv.org/find/eess/1/au:+Owens_G/0/1/0/all/0/1">Gary K. Owens</a>, <a href="http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1">Donald E. Brown</a></p>
<p>Recently, deep learning-based methods achieved promising performance in
nuclei detection and classification applications. However, training deep
learning-based methods requires a large amount of pixel-wise annotated data,
which is time-consuming and labor-intensive, especially in 3D images. An
alternative approach is to adapt weak-annotation methods, such as labeling each
nucleus with a point, but this method does not extend from 2D histopathology
images (for which it was originally developed) to 3D immunofluorescent images.
The reason is that 3D images contain multiple channels (z-axis) for nuclei and
different markers separately, which makes training using point annotations
difficult. To address this challenge, we propose the Label-efficient
Contrastive learning-based (LECL) model to detect and classify various types of
nuclei in 3D immunofluorescent images. Previous methods use Maximum Intensity
Projection (MIP) to convert immunofluorescent images with multiple slices to 2D
images, which can cause signals from different z-stacks to falsely appear
associated with each other. To overcome this, we devised an Extended Maximum
Intensity Projection (EMIP) approach that addresses issues using MIP.
Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for
weakly supervised settings. We conducted experiments on cardiovascular datasets
and found that our proposed framework is effective and efficient in detecting
and classifying various types of nuclei in 3D immunofluorescent images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04041">Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models. (arXiv:2309.04041v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiaying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1">Jinmeng Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kezhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaoyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baochen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Carl Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety
of vision-language tasks. However, a challenge hindering their application in
real-world scenarios, particularly regarding safety, robustness, and
reliability, is their constrained semantic grounding ability, which pertains to
connecting language to the physical-world entities or concepts referenced in
images. Therefore, a crucial need arises for a comprehensive study to assess
the semantic grounding ability of widely used LVLMs. Despite the significance,
sufficient investigation in this direction is currently lacking. Our work
bridges this gap by designing a pipeline for generating large-scale evaluation
datasets covering fine-grained semantic information, such as color, number,
material, etc., along with a thorough assessment of seven popular LVLMs'
semantic grounding ability. Results highlight prevalent misgrounding across
various aspects and degrees. To address this issue, we propose a data-centric
enhancement method that aims to improve LVLMs' semantic grounding ability
through multimodal instruction tuning on fine-grained conversations.
Experiments on enhanced LVLMs demonstrate notable improvements in addressing
misgrounding issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05527">ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation. (arXiv:2309.05527v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiakang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Donglin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianfei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xiangchao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1">Renqiu Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Botian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1">Min Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Si Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Domain shifts such as sensor type changes and geographical situation
variations are prevalent in Autonomous Driving (AD), which poses a challenge
since AD model relying on the previous domain knowledge can be hardly directly
deployed to a new domain without additional costs. In this paper, we provide a
new perspective and approach of alleviating the domain shifts, by proposing a
Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the
implicit reconstruction process is based on the knowledge from the previous old
domain, aiming to convert the domain-related knowledge into domain-invariant
representations, e.g., 3D scene-level meshes. Besides, the point clouds
simulation process of multiple new domains is conditioned on the above
reconstructed 3D meshes, where the target-domain-like simulation samples can be
obtained, thus reducing the cost of collecting and annotating new-domain data
for the subsequent perception process. For experiments, we consider different
cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes,
Waymo-to-ONCE, etc, to verify the zero-shot target-domain perception using
ReSimAD. Results demonstrate that our method is beneficial to boost the domain
generalization ability, even promising for 3D pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12657">Exploiting Modality-Specific Features For Multi-Modal Manipulation Detection And Grounding. (arXiv:2309.12657v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiazhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Changtao Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhiwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1">Wanyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1">Qi Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1">Nenghai Yu</a></p>
<p>AI-synthesized text and images have gained significant attention,
particularly due to the widespread dissemination of multi-modal manipulations
on the internet, which has resulted in numerous negative impacts on society.
Existing methods for multi-modal manipulation detection and grounding primarily
focus on fusing vision-language features to make predictions, while overlooking
the importance of modality-specific features, leading to sub-optimal results.
In this paper, we construct a simple and novel transformer-based framework for
multi-modal manipulation detection and grounding tasks. Our framework
simultaneously explores modality-specific features while preserving the
capability for multi-modal alignment. To achieve this, we introduce
visual/language pre-trained encoders and dual-branch cross-attention (DCA) to
extract and fuse modality-unique features. Furthermore, we design decoupled
fine-grained classifiers (DFC) to enhance modality-specific feature mining and
mitigate modality competition. Moreover, we propose an implicit manipulation
query (IMQ) that adaptively aggregates global contextual cues within each
modality using learnable queries, thereby improving the discovery of forged
details. Extensive experiments on the $\rm DGM^4$ dataset demonstrate the
superior performance of our proposed model compared to state-of-the-art
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16672">Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singhal_U/0/1/0/all/0/1">Utkarsh Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1">Carlos Esteves</a>, <a href="http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1">Ameesh Makadia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Stella X. Yu</a></p>
<p>Computer vision research has long aimed to build systems that are robust to
spatial transformations found in natural data. Traditionally, this is done
using data augmentation or hard-coding invariances into the architecture.
However, too much or too little invariance can hurt, and the correct amount is
unknown a priori and dependent on the instance. Ideally, the appropriate
invariance would be learned from data and inferred at test-time.
</p>
<p>We treat invariance as a prediction problem. Given any image, we use a
normalizing flow to predict a distribution over transformations and average the
predictions over them. Since this distribution only depends on the instance, we
can align instances before classifying them and generalize invariance across
classes. The same distribution can also be used to adapt to out-of-distribution
poses. This normalizing flow is trained end-to-end and can learn a much larger
range of transformations than Augerino and InstaAug. When used as data
augmentation, our method shows accuracy and robustness gains on CIFAR 10,
CIFAR10-LT, and TinyImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17093">Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval. (arXiv:2309.17093v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lianli Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaosu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a></p>
<p>Cross-modal Retrieval methods build similarity relations between vision and
language modalities by jointly learning a common representation space. However,
the predictions are often unreliable due to the Aleatoric uncertainty, which is
induced by low-quality data, e.g., corrupt images, fast-paced videos, and
non-detailed texts. In this paper, we propose a novel Prototype-based Aleatoric
Uncertainty Quantification (PAU) framework to provide trustworthy predictions
by quantifying the uncertainty arisen from the inherent data ambiguity.
Concretely, we first construct a set of various learnable prototypes for each
modality to represent the entire semantics subspace. Then Dempster-Shafer
Theory and Subjective Logic Theory are utilized to build an evidential
theoretical framework by associating evidence with Dirichlet Distribution
parameters. The PAU model induces accurate uncertainty and reliable predictions
for cross-modal retrieval. Extensive experiments are performed on four major
benchmark datasets of MSR-VTT, MSVD, DiDeMo, and MS-COCO, demonstrating the
effectiveness of our method. The code is accessible at
https://github.com/leolee99/PAU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17334">Multi-Depth Branch Network for Efficient Image Super-Resolution. (arXiv:2309.17334v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tian_H/0/1/0/all/0/1">Huiyuan Tian</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shijian Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Yao_M/0/1/0/all/0/1">Min Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_G/0/1/0/all/0/1">Gang Pan</a></p>
<p>A longstanding challenge in Super-Resolution (SR) is how to efficiently
enhance high-frequency details in Low-Resolution (LR) images while maintaining
semantic coherence. This is particularly crucial in practical applications
where SR models are often deployed on low-power devices. To address this issue,
we propose an innovative asymmetric SR architecture featuring Multi-Depth
Branch Module (MDBM). These MDBMs contain branches of different depths,
designed to capture high- and low-frequency information simultaneously and
efficiently. The hierarchical structure of MDBM allows the deeper branch to
gradually accumulate fine-grained local details under the contextual guidance
of the shallower branch. We visualize this process using feature maps, and
further demonstrate the rationality and effectiveness of this design using
proposed novel Fourier spectral analysis methods. Moreover, our model exhibits
more significant spectral differentiation between branches than existing branch
networks. This suggests that MDBM reduces feature redundancy and offers a more
effective method for integrating high- and low-frequency information. Extensive
qualitative and quantitative evaluations on various datasets show that our
model can generate structurally consistent and visually realistic HR images. It
achieves state-of-the-art (SOTA) results at a very fast inference speed. Our
code is available at https://github.com/thy960112/MDBN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02901">Efficient Vectorized Backpropagation Algorithms for Training Feedforward Networks Composed of Quadratic Neurons. (arXiv:2310.02901v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Noel_M/0/1/0/all/0/1">Mathew Mithra Noel</a>, <a href="http://arxiv.org/find/cs/1/au:+Muthiah_Nakarajan_V/0/1/0/all/0/1">Venkataraman Muthiah-Nakarajan</a></p>
<p>Higher order artificial neurons whose outputs are computed by applying an
activation function to a higher order multinomial function of the inputs have
been considered in the past, but did not gain acceptance due to the extra
parameters and computational cost. However, higher order neurons have
significantly greater learning capabilities since the decision boundaries of
higher order neurons can be complex surfaces instead of just hyperplanes. The
boundary of a single quadratic neuron can be a general hyper-quadric surface
allowing it to learn many nonlinearly separable datasets. Since quadratic forms
can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional
parameters are needed instead of $n^2$. A quadratic Logistic regression model
is first presented. Solutions to the XOR problem with a single quadratic neuron
are considered. The complete vectorized equations for both forward and backward
propagation in feedforward networks composed of quadratic neurons are derived.
A reduced parameter quadratic neural network model with just $ n $ additional
parameters per neuron that provides a compromise between learning ability and
computational cost is presented. Comparison on benchmark classification
datasets are used to demonstrate that a final layer of quadratic neurons
enables networks to achieve higher accuracy with significantly fewer hidden
layer neurons. In particular this paper shows that any dataset composed of
$\mathcal{C}$ bounded clusters can be separated with only a single layer of
$\mathcal{C}$ quadratic neurons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07932">What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1">Ran Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenfeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1">Masayoshi Tomizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1">Jitendra Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1">Andrea Bajcsy</a></p>
<p>When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human's input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL's reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot's.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11217">Innovative Methods for Non-Destructive Inspection of Handwritten Documents. (arXiv:2310.11217v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Breci_E/0/1/0/all/0/1">Eleonora Breci</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1">Luca Guarnera</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1">Sebastiano Battiato</a> (1) ((1) University of Catania)</p>
<p>Handwritten document analysis is an area of forensic science, with the goal
of establishing authorship of documents through examination of inherent
characteristics. Law enforcement agencies use standard protocols based on
manual processing of handwritten documents. This method is time-consuming, is
often subjective in its evaluation, and is not replicable. To overcome these
limitations, in this paper we present a framework capable of extracting and
analyzing intrinsic measures of manuscript documents related to text line
heights, space between words, and character sizes using image processing and
deep learning techniques. The final feature vector for each document involved
consists of the mean and standard deviation for every type of measure
collected. By quantifying the Euclidean distance between the feature vectors of
the documents to be compared, authorship can be discerned. Our study pioneered
the comparison between traditionally handwritten documents and those produced
with digital tools (e.g., tablets). Experimental results demonstrate the
ability of our method to objectively determine authorship in different writing
media, outperforming the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12474">Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping. (arXiv:2310.12474v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zijie Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiachen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a></p>
<p>High-resolution 3D object generation remains a challenging task primarily due
to the limited availability of comprehensive annotated training data. Recent
advancements have aimed to overcome this constraint by harnessing image
generative models, pretrained on extensive curated web datasets, using
knowledge transfer techniques like Score Distillation Sampling (SDS).
Efficiently addressing the requirements of high-resolution rendering often
necessitates the adoption of latent representation-based models, such as the
Latent Diffusion Model (LDM). In this framework, a significant challenge
arises: To compute gradients for individual image pixels, it is necessary to
backpropagate gradients from the designated latent space through the frozen
components of the image model, such as the VAE encoder used within LDM.
However, this gradient propagation pathway has never been optimized, remaining
uncontrolled during training. We find that the unregulated gradients adversely
affect the 3D model's capacity in acquiring texture-related information from
the image generative model, leading to poor quality appearance synthesis. To
address this overarching challenge, we propose an innovative operation termed
Pixel-wise Gradient Clipping (PGC) designed for seamless integration into
existing 3D generative models, thereby enhancing their synthesis quality.
Specifically, we control the magnitude of stochastic gradients by clipping the
pixel-wise gradients efficiently, while preserving crucial texture-related
gradient directions. Despite this simplicity and minimal extra cost, extensive
experiments demonstrate the efficacy of our PGC in enhancing the performance of
existing 3D generative models for high-resolution object rendering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12600">FUSC: Fetal Ultrasound Semantic Clustering of Second Trimester Scans Using Deep Self-supervised Learning. (arXiv:2310.12600v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alasmawi_H/0/1/0/all/0/1">Hussain Alasmawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bricker_L/0/1/0/all/0/1">Leanne Bricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1">Mohammad Yaqub</a></p>
<p>Ultrasound is the primary imaging modality in clinical practice during
pregnancy. More than 140M fetuses are born yearly, resulting in numerous scans.
The availability of a large volume of fetal ultrasound scans presents the
opportunity to train robust machine learning models. However, the abundance of
scans also has its challenges, as manual labeling of each image is needed for
supervised methods. Labeling is typically labor-intensive and requires
expertise to annotate the images accurately. This study presents an
unsupervised approach for automatically clustering ultrasound images into a
large range of fetal views, reducing or eliminating the need for manual
labeling. Our Fetal Ultrasound Semantic Clustering (FUSC) method is developed
using a large dataset of 88,063 images and further evaluated on an additional
unseen dataset of 8,187 images achieving over 92% clustering purity. The result
of our investigation hold the potential to significantly impact the field of
fetal ultrasound imaging and pave the way for more advanced automated labeling
solutions. Finally, we make the code and the experimental setup publicly
available to help advance the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13349">DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kim_T/0/1/0/all/0/1">Taehyo Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Shu_H/0/1/0/all/0/1">Hai Shu</a>, <a href="http://arxiv.org/find/stat/1/au:+Jia_Q/0/1/0/all/0/1">Qiran Jia</a>, <a href="http://arxiv.org/find/stat/1/au:+Leon_M/0/1/0/all/0/1">Mony de Leon</a></p>
<p>Voxel-based multiple testing is widely used in neuroimaging data analysis.
Traditional false discovery rate (FDR) control methods often ignore the spatial
dependence among the voxel-based tests and thus suffer from substantial loss of
testing power. While recent spatial FDR control methods have emerged, their
validity and optimality remain questionable when handling the complex spatial
dependencies of the brain. Concurrently, deep learning methods have
revolutionized image segmentation, a task closely related to voxel-based
multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR
control method that leverages unsupervised deep learning-based image
segmentation to address the voxel-based multiple testing problem. Numerical
studies, including comprehensive simulations and Alzheimer's disease FDG-PET
image analysis, demonstrate DeepFDR's superiority over existing methods.
DeepFDR not only excels in FDR control and effectively diminishes the false
nondiscovery rate, but also boasts exceptional computational efficiency highly
suited for tackling large-scale neuroimaging data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16221">Hierarchical Randomized Smoothing. (arXiv:2310.16221v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1">Yan Scholten</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1">Jan Schuchardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1">Aleksandar Bojchevski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19224">CHAMMI: A benchmark for channel-adaptive models in microscopy imaging. (arXiv:2310.19224v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zitong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1">Chau Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Doron_M/0/1/0/all/0/1">Michael Doron</a>, <a href="http://arxiv.org/find/cs/1/au:+Moshkov_N/0/1/0/all/0/1">Nikita Moshkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1">Bryan A. Plummer</a>, <a href="http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1">Juan C. Caicedo</a></p>
<p>Most neural networks assume that input images have a fixed number of channels
(three for RGB images). However, there are many settings where the number of
channels may vary, such as microscopy images where the number of channels
changes depending on instruments and experimental goals. Yet, there has not
been a systemic attempt to create and evaluate neural networks that are
invariant to the number and type of channels. As a result, trained models
remain specific to individual studies and are hardly reusable for other
microscopy settings. In this paper, we present a benchmark for investigating
channel-adaptive models in microscopy imaging, which consists of 1) a dataset
of varied-channel single-cell images, and 2) a biologically relevant evaluation
framework. In addition, we adapted several existing techniques to create
channel-adaptive models and compared their performance on this benchmark to
fixed-channel, baseline models. We find that channel-adaptive models can
generalize better to out-of-domain tasks and can be computationally efficient.
We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an
evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate
objective comparisons in future research and applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01017">Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lunjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yuwen Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ze Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1">Sergio Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1">Raquel Urtasun</a></p>
<p>Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01352">Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yu Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Sollmann_J/0/1/0/all/0/1">Jan Sollmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jianxu Chen</a></p>
<p>With the fast development of modern microscopes and bioimaging techniques, an
unprecedentedly large amount of imaging data are being generated, stored,
analyzed, and even shared through networks. The size of the data poses great
challenges for current data infrastructure. One common way to reduce the data
size is by image compression. This present study analyzes classic and deep
learning based image compression methods, and their impact on deep learning
based image processing models. Deep learning based label-free prediction models
(i.e., predicting fluorescent images from bright field images) are used as an
example application for comparison and analysis. Effective image compression
methods could help reduce the data size significantly without losing necessary
information, and therefore reduce the burden on data management infrastructure
and permit fast transmission through the network for data sharing or cloud
computing. To compress images in such a wanted way, multiple classical lossy
image compression techniques are compared to several AI-based compression
models provided by and trained with the CompressAI toolbox using python. These
different compression techniques are compared in compression ratio, multiple
image similarity measures and, most importantly, the prediction accuracy from
label-free models on compressed images. We found that AI-based compression
techniques largely outperform the classic ones and will minimally affect the
downstream label-free task in 2D cases. In the end, we hope the present study
could shed light on the potential of deep learning based image compression and
the impact of image compression on downstream deep learning based image
analysis models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02332">Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1">Elisa Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonsang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">William Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1">Tanveer Syeda-Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1">Charles Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Arvind Rao</a></p>
<p>Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of "big data" in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02700">A Generative Multi-Resolution Pyramid and Normal-Conditioning 3D Cloth Draping. (arXiv:2311.02700v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laczko_H/0/1/0/all/0/1">Hunor Laczk&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1">Meysam Madadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1">Sergio Escalera</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Jordi Gonzalez</a></p>
<p>RGB cloth generation has been deeply studied in the related literature,
however, 3D garment generation remains an open problem. In this paper, we build
a conditional variational autoencoder for 3D garment generation and draping. We
propose a pyramid network to add garment details progressively in a canonical
space, i.e. unposing and unshaping the garments w.r.t. the body. We study
conditioning the network on surface normal UV maps, as an intermediate
representation, which is an easier problem to optimize than 3D coordinates. Our
results on two public datasets, CLOTH3D and CAPE, show that our model is
robust, controllable in terms of detail generation by the use of
multi-resolution pyramids, and achieves state-of-the-art results that can
highly generalize to unseen garments, poses, and shapes even when training with
small amounts of data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02791">MirrorCalib: Utilizing Human Pose Information for Mirror-based Virtual Camera Calibration. (arXiv:2311.02791v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1">Longyun Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_A/0/1/0/all/0/1">Andrew Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Rong Zheng</a></p>
<p>In this paper, we present the novel task of estimating the extrinsic
parameters of a virtual camera relative to a real camera in exercise videos
with a mirror. This task poses a significant challenge in scenarios where the
views from the real and mirrored cameras have no overlap or share salient
features. To address this issue, prior knowledge of a human body and 2D joint
locations are utilized to estimate the camera extrinsic parameters when a
person is in front of a mirror. We devise a modified eight-point algorithm to
obtain an initial estimation from 2D joint locations. The 2D joint locations
are then refined subject to human body constraints. Finally, a RANSAC algorithm
is employed to remove outliers by comparing their epipolar distances to a
predetermined threshold. MirrorCalib is evaluated on both synthetic and real
datasets and achieves a rotation error of 0.62{\deg}/1.82{\deg} and a
translation error of 37.33/69.51 mm on the synthetic/real dataset, which
outperforms the state-of-art method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03517">SoundCam: A Dataset for Finding Humans Using Room Acoustics. (arXiv:2311.03517v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mason Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Clarke_S/0/1/0/all/0/1">Samuel Clarke</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jui-Hsien Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruohan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>A room's acoustic properties are a product of the room's geometry, the
objects within the room, and their specific positions. A room's acoustic
properties can be characterized by its impulse response (RIR) between a source
and listener location, or roughly inferred from recordings of natural signals
present in the room. Variations in the positions of objects in a room can
effect measurable changes in the room's acoustic properties, as characterized
by the RIR. Existing datasets of RIRs either do not systematically vary
positions of objects in an environment, or they consist of only simulated RIRs.
We present SoundCam, the largest dataset of unique RIRs from in-the-wild rooms
publicly released to date. It includes 5,000 10-channel real-world measurements
of room impulse responses and 2,000 10-channel recordings of music in three
different rooms, including a controlled acoustic lab, an in-the-wild living
room, and a conference room, with different humans in positions throughout each
room. We show that these measurements can be used for interesting tasks, such
as detecting and identifying humans, and tracking their positions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04071">Energy-Calibrated VAE with Test Time Free Lunch. (arXiv:2311.04071v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yihong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Siya Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xingjian Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yujun Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jing Tang</a></p>
<p>In this paper, we propose a novel generative model that utilizes a
conditional Energy-Based Model (EBM) for enhancing Variational Autoencoder
(VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often suffer
from blurry generated samples due to the lack of a tailored training on the
samples generated in the generative direction. On the other hand, EBMs can
generate high-quality samples but require expensive Markov Chain Monte Carlo
(MCMC) sampling. To address these issues, we introduce a conditional EBM for
calibrating the generative direction of VAE during training, without requiring
it for the generation at test time. In particular, we train EC-VAE upon both
the input data and the calibrated samples with adaptive weight to enhance
efficacy while avoiding MCMC sampling at test time. Furthermore, we extend the
calibration idea of EC-VAE to variational learning and normalizing flows, and
apply EC-VAE to an additional application of zero-shot image restoration via
neural transport prior and range-null theory. We evaluate the proposed method
with two applications, including image generation and zero-shot image
restoration, and the experimental results show that our method achieves the
state-of-the-art performance over single-step non-adversarial generation. Our
code is available at https://github.com/DJ-LYH/EC-VAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04207">Deep Hashing via Householder Quantization. (arXiv:2311.04207v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schwengber_L/0/1/0/all/0/1">Lucas R. Schwengber</a>, <a href="http://arxiv.org/find/cs/1/au:+Resende_L/0/1/0/all/0/1">Lucas Resende</a>, <a href="http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1">Paulo Orenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1">Roberto I. Oliveira</a></p>
<p>Hashing is at the heart of large-scale image similarity search, and recent
methods have been substantially improved through deep learning techniques. Such
algorithms typically learn continuous embeddings of the data. To avoid a
subsequent costly binarization step, a common solution is to employ loss
functions that combine a similarity learning term (to ensure similar images are
grouped to nearby embeddings) and a quantization penalty term (to ensure that
the embedding entries are close to binarized entries, e.g., -1 or 1). Still,
the interaction between these two terms can make learning harder and the
embeddings worse. We propose an alternative quantization strategy that
decomposes the learning problem in two stages: first, perform similarity
learning over the embedding space with no quantization; second, find an optimal
orthogonal transformation of the embeddings so each coordinate of the embedding
is close to its sign, and then quantize the transformed embedding through the
sign function. In the second step, we parametrize orthogonal transformations
using Householder matrices to efficiently leverage stochastic gradient descent.
Since similarity measures are usually invariant under orthogonal
transformations, this quantization strategy comes at no cost in terms of
performance. The resulting algorithm is unsupervised, fast, hyperparameter-free
and can be run on top of any existing deep hashing or metric learning
algorithm. We provide extensive experimental results showing that this approach
leads to state-of-the-art performance on widely used image datasets, and,
unlike other quantization strategies, brings consistent improvements in
performance to existing deep hashing algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05559">Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Kolle_M/0/1/0/all/0/1">Michael K&#xf6;lle</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Maurer_J/0/1/0/all/0/1">Jonas Maurer</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Altmann_P/0/1/0/all/0/1">Philipp Altmann</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sunkel_L/0/1/0/all/0/1">Leo S&#xfc;nkel</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Stein_J/0/1/0/all/0/1">Jonas Stein</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1">Claudia Linnhoff-Popien</a></p>
<p>Quantum computing offers the potential for superior computational
capabilities, particularly for data-intensive tasks. However, the current state
of quantum hardware puts heavy restrictions on input size. To address this,
hybrid transfer learning solutions have been developed, merging pre-trained
classical models, capable of handling extensive inputs, with variational
quantum circuits. Yet, it remains unclear how much each component -- classical
and quantum -- contributes to the model's results. We propose a novel hybrid
architecture: instead of utilizing a pre-trained network for compression, we
employ an autoencoder to derive a compressed version of the input data. This
compressed data is then channeled through the encoder part of the autoencoder
to the quantum component. We assess our model's classification capabilities
against two state-of-the-art hybrid transfer learning architectures, two purely
classical architectures and one quantum architecture. Their accuracy is
compared across four datasets: Banknote Authentication, Breast Cancer
Wisconsin, MNIST digits, and AudioMNIST. Our research suggests that classical
components significantly influence classification in hybrid transfer learning,
a contribution often mistakenly ascribed to the quantum element. The
performance of our model aligns with that of a variational quantum circuit
using amplitude embedding, positioning it as a feasible alternative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08747">Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection. (arXiv:2311.08747v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1">Chun Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jie Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1">Yaqian Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianhua Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zechen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1">Qun Hao</a></p>
<p>Infrared small target detection based on deep learning offers unique
advantages in separating small targets from complex and dynamic backgrounds.
However, the features of infrared small targets gradually weaken as the depth
of convolutional neural network (CNN) increases. To address this issue, we
propose a novel method for detecting infrared small targets called improved
dense nested attention network (IDNANet), which is based on the transformer
architecture. We preserve the dense nested structure of dense nested attention
network (DNANet) and introduce the Swin-transformer during feature extraction
stage to enhance the continuity of features. Furthermore, we integrate the
ACmix attention structure into the dense nested structure to enhance the
features of intermediate layers. Additionally, we design a weighted dice binary
cross-entropy (WD-BCE) loss function to mitigate the negative impact of
foreground-background imbalance in the samples. Moreover, we develop a dataset
specifically for infrared small targets, called BIT-SIRST. The dataset
comprises a significant amount of real-world targets and manually annotated
labels, as well as synthetic data and corresponding labels. We have evaluated
the effectiveness of our method through experiments conducted on public
datasets. In comparison to other state-of-the-art methods, our approach
outperforms in terms of probability of detection ($P_d$), false-alarm rate
($F_a$), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89\% on
the NUDT-SIRST dataset and 79.72\% on the SIRST dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12401">CASR: Refining Action Segmentation via Marginalizing Frame-levle Causal Relationships. (arXiv:2311.12401v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1">Keqing Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hang Chen</a></p>
<p>Integrating deep learning and causal discovery has increased the
interpretability of Temporal Action Segmentation (TAS) tasks. However,
frame-level causal relationships exist many complicated noises outside the
segment-level, making it infeasible to directly express macro action semantics.
Thus, we propose Causal Abstraction Segmentation Refiner (CASR), which can
refine TAS results from various models by enhancing video causality in
marginalizing frame-level casual relationships. Specifically, we define the
equivalent frame-level casual model and segment-level causal model, so that the
causal adjacency matrix constructed from marginalized frame-level causal
relationships has the ability to represent the segmnet-level causal
relationships. CASR works out by reducing the difference in the causal
adjacency matrix between we constructed and pre-segmentation results of
backbone models. In addition, we propose a novel evaluation metric Causal Edit
Distance (CED) to evaluate the causal interpretability. Extensive experimental
results on mainstream datasets indicate that CASR significantly surpasses
existing various methods in action segmentation performance, as well as in
causal explainability and generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13629">Diffusion models meet image counter-forensics. (arXiv:2311.13629v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tailanian_M/0/1/0/all/0/1">Mat&#xed;as Tailanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardella_M/0/1/0/all/0/1">Marina Gardella</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1">&#xc1;lvaro Pardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Muse_P/0/1/0/all/0/1">Pablo Mus&#xe9;</a></p>
<p>From its acquisition in the camera sensors to its storage, different
operations are performed to generate the final image. This pipeline imprints
specific traces into the image to form a natural watermark. Tampering with an
image disturbs these traces; these disruptions are clues that are used by most
methods to detect and locate forgeries. In this article, we assess the
capabilities of diffusion models to erase the traces left by forgers and,
therefore, deceive forensics methods. Such an approach has been recently
introduced for adversarial purification, achieving significant performance. We
show that diffusion purification methods are well suited for counter-forensics
tasks. Such approaches outperform already existing counter-forensics techniques
both in deceiving forensics methods and in preserving the natural look of the
purified images. The source code is publicly available at
https://github.com/mtailanian/diff-cf.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14084">AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shicheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1">Danyang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jingcheng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a></p>
<p>With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon causes source bias in text retrieval for web search.
Specifically, neural retrieval models tend to rank generated texts higher than
human-written texts. In this paper, we extend the study of this bias to
cross-modal retrieval. Firstly, we successfully construct a suitable benchmark
to explore the existence of the bias. Subsequent extensive experiments on this
benchmark reveal that AI-generated images introduce an invisible relevance bias
to text-image retrieval models. Specifically, our experiments show that
text-image retrieval models tend to rank the AI-generated images higher than
the real images, even though the AI-generated images do not exhibit more
visually relevant features to the query than real images. This invisible
relevance bias is prevalent across retrieval models with varying training data
and architectures. Furthermore, our subsequent exploration reveals that the
inclusion of AI-generated images in the training data of the retrieval models
exacerbates the invisible relevance bias. The above phenomenon triggers a
vicious cycle, which makes the invisible relevance bias become more and more
serious. To elucidate the potential causes of invisible relevance and address
the aforementioned issues, we introduce an effective training method aimed at
alleviating the invisible relevance bias. Subsequently, we apply our proposed
debiasing method to retroactively identify the causes of invisible relevance,
revealing that the AI-generated images induce the image encoder to embed
additional information into their representation. This information exhibits a
certain consistency across generated images with different semantics and can
make the retriever estimate a higher relevance score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14656">Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. (arXiv:2311.14656v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1">Jonathan Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Luddecke_T/0/1/0/all/0/1">Timo L&#xfc;ddecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1">Rehan Sheikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1">Samuel Albanie</a></p>
<p>Multimodal large language models (MLLMs) have shown remarkable capabilities
across a broad range of tasks but their knowledge and abilities in the
geographic and geospatial domains are yet to be explored, despite potential
wide-ranging benefits to navigation, environmental research, urban development,
and disaster response. We conduct a series of experiments exploring various
vision capabilities of MLLMs within these domains, particularly focusing on the
frontier model GPT-4V, and benchmark its performance against open-source
counterparts. Our methodology involves challenging these models with a
small-scale geographic benchmark consisting of a suite of visual tasks, testing
their abilities across a spectrum of complexity. The analysis uncovers not only
where such models excel, including instances where they outperform humans, but
also where they falter, providing a balanced view of their capabilities in the
geographic domain. To enable the comparison and evaluation of future models,
our benchmark will be publicly released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18193">Persistent Test-time Adaptation in Episodic Testing Scenarios. (arXiv:2311.18193v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Trung-Hieu Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1">Duc Minh Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh N. Do</a></p>
<p>Current test-time adaptation (TTA) approaches aim to adapt to environments
that change continuously. Yet, when the environments not only change but also
recur in a correlated manner over time, such as in the case of day-night
surveillance cameras, it is unclear whether the adaptability of these methods
is sustained after a long run. This study aims to examine the error
accumulation of TTA models when they are repeatedly exposed to previous testing
environments, proposing a novel testing setting called episodic TTA. To study
this phenomenon, we design a simulation of TTA process on a simple yet
representative $\epsilon$-perturbed Gaussian Mixture Model Classifier and
derive the theoretical findings revealing the dataset- and algorithm-dependent
factors that contribute to the gradual degeneration of TTA methods through
time. Our investigation has led us to propose a method, named persistent TTA
(PeTTA). PeTTA senses the model divergence towards a collapsing and adjusts the
adaptation strategy of TTA, striking a balance between two primary objectives:
adaptation and preventing model collapse. The stability of PeTTA in the face of
episodic TTA scenarios has been demonstrated through a set of comprehensive
experiments on various benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18815">IMMA: Immunizing text-to-image Models against Malicious Adaptation. (arXiv:2311.18815v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1">Amber Yijia Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1">Raymond A. Yeh</a></p>
<p>Advancements in text-to-image models and fine-tuning methods have led to the
increasing risk of malicious adaptation, i.e., fine-tuning to generate harmful
unauthorized content. Recent works, e.g., Glaze or MIST, have developed
data-poisoning techniques which protect the data against adaptation methods. In
this work, we consider an alternative paradigm for protection. We propose to
``immunize'' the model by learning model parameters that are difficult for the
adaptation methods when fine-tuning malicious content; in short IMMA. Empirical
results show IMMA's effectiveness against malicious adaptations, including
mimicking the artistic style and learning of inappropriate/unauthorized
content, over three adaptation methods: LoRA, Textual-Inversion, and
DreamBooth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07327">Adaptive Confidence Multi-View Hashing for Multimedia Retrieval. (arXiv:2312.07327v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhangmin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Lingfang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1">Li-Rong Dai</a></p>
<p>The multi-view hash method converts heterogeneous data from multiple views
into binary hash codes, which is one of the critical technologies in multimedia
retrieval. However, the current methods mainly explore the complementarity
among multiple views while lacking confidence learning and fusion. Moreover, in
practical application scenarios, the single-view data contain redundant noise.
To conduct the confidence learning and eliminate unnecessary noise, we propose
a novel Adaptive Confidence Multi-View Hashing (ACMVH) method. First, a
confidence network is developed to extract useful information from various
single-view features and remove noise information. Furthermore, an adaptive
confidence multi-view network is employed to measure the confidence of each
view and then fuse multi-view features through a weighted summation. Lastly, a
dilation network is designed to further enhance the feature representation of
the fused features. To the best of our knowledge, we pioneer the application of
confidence learning into the field of multimedia retrieval. Extensive
experiments on two public datasets show that the proposed ACMVH performs better
than state-of-the-art methods (maximum increase of 3.24%). The source code is
available at https://github.com/HackerHyper/ACMVH.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07586">Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Candi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yuan Lan</a></p>
<p>Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07899">Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v2 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Tang_Q/0/1/0/all/0/1">Qiaosi Tang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ratnayake_R/0/1/0/all/0/1">Ranjala Ratnayake</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Seabra_G/0/1/0/all/0/1">Gustavo Seabra</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jiang_Z/0/1/0/all/0/1">Zhe Jiang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fang_R/0/1/0/all/0/1">Ruogu Fang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cui_L/0/1/0/all/0/1">Lina Cui</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1">Yousong Ding</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kahveci_T/0/1/0/all/0/1">Tamer Kahveci</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1">Chenglong Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Luesch_H/0/1/0/all/0/1">Hendrik Luesch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1">Yanjun Li</a></p>
<p>Morphological profiling is a valuable tool in phenotypic drug discovery. The
advent of high-throughput automated imaging has enabled the capturing of a wide
range of morphological features of cells or organisms in response to
perturbations at the single-cell resolution. Concurrently, significant advances
in machine learning and deep learning, especially in computer vision, have led
to substantial improvements in analyzing large-scale high-content images at
high-throughput. These efforts have facilitated understanding of compound
mechanism-of-action (MOA), drug repurposing, characterization of cell
morphodynamics under perturbation, and ultimately contributing to the
development of novel therapeutics. In this review, we provide a comprehensive
overview of the recent advances in the field of morphological profiling. We
summarize the image profiling analysis workflow, survey a broad spectrum of
analysis strategies encompassing feature engineering- and deep learning-based
approaches, and introduce publicly available benchmark datasets. We place a
particular emphasis on the application of deep learning in this pipeline,
covering cell segmentation, image representation learning, and multimodal
learning. Additionally, we illuminate the application of morphological
profiling in phenotypic drug discovery and highlight potential challenges and
opportunities in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10251">Advancing Surgical VQA with Scene Graph Knowledge. (arXiv:2312.10251v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Kun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kattel_M/0/1/0/all/0/1">Manasi Kattel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1">Joel L. Lavanchy</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1">Vinkle Srivastav</a>, <a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1">Nicolas Padoy</a></p>
<p>Modern operating room is becoming increasingly complex, requiring innovative
intra-operative support systems. While the focus of surgical data science has
largely been on video analysis, integrating surgical computer vision with
language capabilities is emerging as a necessity. Our work aims to advance
Visual Question Answering (VQA) in the surgical context with scene graph
knowledge, addressing two main challenges in the current surgical VQA systems:
removing question-condition bias in the surgical VQA dataset and incorporating
scene-aware reasoning in the surgical VQA model design. First, we propose a
Surgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation
and detection models on publicly available datasets. We build surgical scene
graphs using spatial and action information of instruments and anatomies. These
graphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA
dataset provides a more complex, diverse, geometrically grounded, unbiased, and
surgical action-oriented dataset compared to existing surgical VQA datasets. We
then propose SSG-QA-Net, a novel surgical VQA model incorporating a lightweight
Scene-embedded Interaction Module (SIM), which integrates geometric scene
knowledge in the VQA model design by employing cross-attention between the
textual and the scene features. Our comprehensive analysis of the SSG-QA
dataset shows that SSG-QA-Net outperforms existing methods across different
question types and complexities. We highlight that the primary limitation in
the current surgical VQA systems is the lack of scene knowledge to answer
complex queries. We present a novel surgical VQA dataset and model and show
that results can be significantly improved by incorporating geometric scene
features in the VQA model design. The source code and the dataset will be made
publicly available at: https://github.com/CAMMA-public/SSG-QA
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10282">RetailKLIP : Finetuning OpenCLIP backbone using metric learning on a single GPU for Zero-shot retail product image classification. (arXiv:2312.10282v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1">Muktabh Mayank Srivastava</a></p>
<p>Retail product or packaged grocery goods images need to classified in various
computer vision applications like self checkout stores, supply chain automation
and retail execution evaluation. Previous works explore ways to finetune deep
models for this purpose. But because of the fact that finetuning a large model
or even linear layer for a pretrained backbone requires to run at least a few
epochs of gradient descent for every new retail product added in classification
range, frequent retrainings are needed in a real world scenario. In this work,
we propose finetuning the vision encoder of a CLIP model in a way that its
embeddings can be easily used for nearest neighbor based classification, while
also getting accuracy close to or exceeding full finetuning. A nearest neighbor
based classifier needs no incremental training for new products, thus saving
resources and wait time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11841">MixRT: Mixed Neural Representations For Real-Time NeRF Rendering. (arXiv:2312.11841v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaojian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Yingyan/0/1/0/all/0/1">Yingyan</a> (Celine)Lin</p>
<p>Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12340">Scalable Geometric Fracture Assembly via Co-creation Space among Assemblers. (arXiv:2312.12340v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaxiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zexi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chao Wu</a></p>
<p>Geometric fracture assembly presents a challenging practical task in
archaeology and 3D computer vision. Previous methods have focused solely on
assembling fragments based on semantic information, which has limited the
quantity of objects that can be effectively assembled. Therefore, there is a
need to develop a scalable framework for geometric fracture assembly without
relying on semantic information. To improve the effectiveness of assembling
geometric fractures without semantic information, we propose a co-creation
space comprising several assemblers capable of gradually and unambiguously
assembling fractures. Additionally, we introduce a novel loss function, i.e.,
the geometric-based collision loss, to address collision issues during the
fracture assembly process and enhance the results. Our framework exhibits
better performance on both PartNet and Breaking Bad datasets compared to
existing state-of-the-art frameworks. Extensive experiments and quantitative
comparisons demonstrate the effectiveness of our proposed framework, which
features linear computational complexity, enhanced abstraction, and improved
generalization. Our code is publicly available at
https://github.com/Ruiyuan-Zhang/CCS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13845">Image Clustering using Restricted Boltzman Machine. (arXiv:2312.13845v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1">Abraham Woubie</a>, <a href="http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1">Enoch Solomon</a>, <a href="http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1">Eyael Solomon Emiru</a></p>
<p>In various verification systems, Restricted Boltzmann Machines (RBMs) have
demonstrated their efficacy in both front-end and back-end processes. In this
work, we propose the use of RBMs to the image clustering tasks. RBMs are
trained to convert images into image embeddings. We employ the conventional
bottom-up Agglomerative Hierarchical Clustering (AHC) technique. To address the
challenge of limited test face image data, we introduce Agglomerative
Hierarchical Clustering based Method for Image Clustering using Restricted
Boltzmann Machine (AHC-RBM) with two major steps. Initially, a universal RBM
model is trained using all available training dataset. Subsequently, we train
an adapted RBM model using the data from each test image. Finally, RBM vectors
which is the embedding vector is generated by concatenating the
visible-to-hidden weight matrices of these adapted models, and the bias
vectors. These vectors effectively preserve class-specific information and are
utilized in image clustering tasks. Our experimental results, conducted on two
benchmark image datasets (MS-Celeb-1M and DeepFashion), demonstrate that our
proposed approach surpasses well-known clustering algorithms such as k-means,
spectral clustering, and approximate Rank-order.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14198">ZeroShape: Regression-based Zero-shot Shape Reconstruction. (arXiv:2312.14198v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zixuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1">Stefan Stojanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1">Anh Thai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1">Varun Jampani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1">James M. Rehg</a></p>
<p>We study the problem of single-image zero-shot 3D shape reconstruction.
Recent works learn zero-shot shape reconstruction through generative modeling
of 3D assets, but these models are computationally expensive at train and
inference time. In contrast, the traditional approach to this problem is
regression-based, where deterministic models are trained to directly regress
the object shape. Such regression methods possess much higher computational
efficiency than generative methods. This raises a natural question: is
generative modeling necessary for high performance, or conversely, are
regression-based approaches still competitive? To answer this, we design a
strong regression-based model, called ZeroShape, based on the converging
findings in this field and a novel insight. We also curate a large real-world
evaluation benchmark, with objects from three different real-world 3D datasets.
This evaluation benchmark is more diverse and an order of magnitude larger than
what prior works use to quantitatively evaluate their models, aiming at
reducing the evaluation variance in our field. We show that ZeroShape not only
achieves superior performance over state-of-the-art methods, but also
demonstrates significantly higher computational and data efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16904">Block Pruning for Enhanced Efficiency in Convolutional Neural Networks. (arXiv:2312.16904v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Cheng-En Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Davoodi_A/0/1/0/all/0/1">Azadeh Davoodi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yu Hen Hu</a></p>
<p>This paper presents a novel approach to network pruning, targeting block
pruning in deep neural networks for edge computing environments. Our method
diverges from traditional techniques that utilize proxy metrics, instead
employing a direct block removal strategy to assess the impact on
classification accuracy. This hands-on approach allows for an accurate
evaluation of each block's importance. We conducted extensive experiments on
CIFAR-10, CIFAR-100, and ImageNet datasets using ResNet architectures. Our
results demonstrate the efficacy of our method, particularly on large-scale
datasets like ImageNet with ResNet50, where it excelled in reducing model size
while retaining high accuracy, even when pruning a significant portion of the
network. The findings underscore our method's capability in maintaining an
optimal balance between model size and performance, especially in
resource-constrained edge computing scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17071">SCTNet: Single-Branch CNN with Transformer Semantic Information for Real-Time Segmentation. (arXiv:2312.17071v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhengze Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dongyue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Changqian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Changxin Gao</a></p>
<p>Recent real-time semantic segmentation methods usually adopt an additional
semantic branch to pursue rich long-range context. However, the additional
branch incurs undesirable computational overhead and slows inference speed. To
eliminate this dilemma, we propose SCTNet, a single branch CNN with transformer
semantic information for real-time segmentation. SCTNet enjoys the rich
semantic representations of an inference-free semantic branch while retaining
the high efficiency of lightweight single branch CNN. SCTNet utilizes a
transformer as the training-only semantic branch considering its superb ability
to extract long-range context. With the help of the proposed transformer-like
CNN block CFBlock and the semantic information alignment module, SCTNet could
capture the rich semantic information from the transformer branch in training.
During the inference, only the single branch CNN needs to be deployed. We
conduct extensive experiments on Cityscapes, ADE20K, and COCO-Stuff-10K, and
the results show that our method achieves the new state-of-the-art performance.
The code and model is available at https://github.com/xzz777/SCTNet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00241">Image Super-resolution Reconstruction Network based on Enhanced Swin Transformer via Alternating Aggregation of Local-Global Features. (arXiv:2401.00241v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingpin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Changhui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hanrong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1">Binhui Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hui Wang</a></p>
<p>The Swin Transformer image super-resolution reconstruction network only
relies on the long-range relationship of window attention and shifted window
attention to explore features. This mechanism has two limitations. On the one
hand, it only focuses on global features while ignoring local features. On the
other hand, it is only concerned with spatial feature interactions while
ignoring channel features and channel interactions, thus limiting its
non-linear mapping ability. To address the above limitations, this paper
proposes enhanced Swin Transformer modules via alternating aggregation of
local-global features. In the local feature aggregation stage, we introduce a
shift convolution to realize the interaction between local spatial information
and channel information. Then, a block sparse global perception module is
introduced in the global feature aggregation stage. In this module, we
reorganize the spatial information first, then send the recombination
information into a multi-layer perceptron unit to implement the global
perception. After that, a multi-scale self-attention module and a low-parameter
residual channel attention module are introduced to realize information
aggregation at different scales. Finally, the proposed network is validated on
five publicly available datasets. The experimental results show that the
proposed network outperforms the other state-of-the-art super-resolution
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00436">Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic Matrix Space for Point Cloud Registration. (arXiv:2401.00436v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qianliang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haobo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yaqing Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Lei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a></p>
<p>Efficiently finding optimal correspondences between point clouds is crucial
for solving both rigid and non-rigid point cloud registration problems.
Existing methods often rely on geometric or semantic feature embedding to
establish correspondences and estimate transformations or flow fields.
Recently, state-of-the-art methods have employed RAFT-like iterative updates to
refine the solution. However, these methods have certain limitations. Firstly,
their iterative refinement design lacks transparency, and their iterative
updates follow a fixed path during the refinement process, which can lead to
suboptimal results. Secondly, these methods overlook the importance of refining
or optimizing correspondences (or matching matrices) as a precursor to solving
transformations or flow fields. They typically compute candidate
correspondences based on distances in the point feature space. However, they
only project the candidate matching matrix into some matrix space once with
Sinkhorn or dual softmax operations to obtain final correspondences. This
one-shot projected matching matrix may be far from the globally optimal one,
and these approaches do not consider the distribution of the target matching
matrix. In this paper, we propose a novel approach that exploits the Denoising
Diffusion Model to predict a searching gradient for the optimal matching matrix
within the Doubly Stochastic Matrix Space. During the reverse denoising
process, our method iteratively searches for better solutions along this
denoising gradient, which points towards the maximum likelihood direction of
the target matching matrix. Our method offers flexibility by allowing the
search to start from any initial matching matrix provided by the online
backbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and
4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00910">WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1">Saravanabalagi Ramachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Cibik_N/0/1/0/all/0/1">Nathaniel Cibik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1">Ganesh Sistu</a>, <a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1">John McDonald</a></p>
<p>Motion segmentation is a complex yet indispensable task in autonomous
driving. The challenges introduced by the ego-motion of the cameras, radial
distortion in fisheye lenses, and the need for temporal consistency make the
task more complicated, rendering traditional and standard Convolutional Neural
Network (CNN) approaches less effective. The consequent laborious data
labeling, representation of diverse and uncommon scenarios, and extensive data
capture requirements underscore the imperative of synthetic data for improving
machine learning model performance. To this end, we employ the PD-WoodScape
synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye
dataset. Thus, we present the WoodScape fisheye motion segmentation challenge
for autonomous driving, held as part of the CVPR 2023 Workshop on
Omnidirectional Computer Vision (OmniCV). As one of the first competitions
focused on fisheye motion segmentation, we aim to explore and evaluate the
potential and impact of utilizing synthetic data in this domain. In this paper,
we provide a detailed analysis on the competition which attracted the
participation of 112 global teams and a total of 234 submissions. This study
delineates the complexities inherent in the task of motion segmentation,
emphasizes the significance of fisheye datasets, articulate the necessity for
synthetic datasets and the resultant domain gap they engender, outlining the
foundational blueprint for devising successful solutions. Subsequently, we
delve into the details of the baseline experiments and winning methods
evaluating their qualitative and quantitative results, providing with useful
insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01699">WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope. (arXiv:2401.01699v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jun-Yan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhi-Qi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingdong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wangmeng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yusen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xianhui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1">Xiaoyang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zengke Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yifeng Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a></p>
<p>This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02099">CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification. (arXiv:2401.02099v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zeyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jingsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1">Suncheng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jiacheng Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yuzhuo Fu</a></p>
<p>Existing research on audio classification faces challenges in recognizing
attributes of passive underwater vessel scenarios and lacks well-annotated
datasets due to data privacy concerns. In this study, we introduce CLAPP
(Contrastive Language-Audio Pre-training in Passive Underwater Vessel
Classification), a novel model. Our aim is to train a neural network using a
wide range of vessel audio and vessel state text pairs obtained from an
oceanship dataset. CLAPP is capable of directly learning from raw vessel audio
data and, when available, from carefully curated labels, enabling improved
recognition of vessel attributes in passive underwater vessel scenarios.
Model's zero-shot capability allows predicting the most relevant vessel state
description for a given vessel audio, without directly optimizing for the task.
Our approach aims to solve 2 challenges: vessel audio-text classification and
passive underwater vessel audio attribute recognition. The proposed method
achieves new state-of-the-art results on both Deepship and Shipsear public
datasets, with a notable margin of about 7%-13% for accuracy compared to prior
methods on zero-shot task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02330">LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1">Zhicai Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1">Xiaofeng Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a></p>
<p>In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient
multi-modal assistant that harnesses the power of the recently advanced small
language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a
notable advancement in the realm of compact multi-modal models. It demonstrates
that even smaller language models, with as few as 2.7B parameters, can
effectively engage in intricate dialogues that integrate both textual and
visual elements, provided they are trained with high-quality corpora. Our model
delivers commendable performance on publicly available benchmarks that
encompass visual comprehension, reasoning, and knowledge-based perception.
Beyond its remarkable performance in multi-modal dialogue tasks, our model
opens new avenues for applications in time-sensitive environments and systems
that require real-time interaction, such as embodied agents. It highlights the
potential of smaller language models to achieve sophisticated levels of
understanding and interaction, while maintaining greater resource
efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02727">Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1">Hui Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Biwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Anjie Peng</a></p>
<p>Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. Yet,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features conducive to the target class and discourage the
features to the original class in a middle layer of the source model. Extensive
experiments demonstrate that only a few iterations of fine-tuning can boost
existing attacks' targeted transferability nontrivially and universally. Our
results also verify that the simple iterative attacks can yield comparable or
even better transferability than the resource-intensive methods, which rest on
training target-specific classifiers or generators with additional data. The
code is available at: github.com/zengh5/TA_feature_FT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02941">Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nananukul_N/0/1/0/all/0/1">Navapat Nananukul</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanian_zadeh_H/0/1/0/all/0/1">Hamid Soltanian-zadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a></p>
<p>Automatic semantic segmentation of magnetic resonance imaging (MRI) images
using deep neural networks greatly assists in evaluating and planning
treatments for various clinical applications. However, training these models is
conditioned on the availability of abundant annotated data to implement the
end-to-end supervised learning procedure. Even if we annotate enough data, MRI
images display considerable variability due to factors such as differences in
patients, MRI scanners, and imaging protocols. This variability necessitates
retraining neural networks for each specific application domain, which, in
turn, requires manual annotation by expert radiologists for all new domains. To
relax the need for persistent data annotation, we develop a method for
unsupervised federated domain adaptation using multiple annotated source
domains. Our approach enables the transfer of knowledge from several annotated
source domains to adapt a model for effective use in an unannotated target
domain. Initially, we ensure that the target domain data shares similar
representations with each source domain in a latent embedding space, modeled as
the output of a deep encoder, by minimizing the pair-wise distances of the
distributions for the target domain and the source domains. We then employ an
ensemble approach to leverage the knowledge obtained from all domains. We
provide theoretical analysis and perform experiments on the MICCAI 2016
multi-site dataset to demonstrate our method is effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03105">Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models. (arXiv:2401.03105v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Longhui Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lingxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a></p>
<p>Multimodal Large Language Models (MLLMs) are experiencing rapid growth,
yielding a plethora of noteworthy contributions in recent months. The
prevailing trend involves adopting data-driven methodologies, wherein diverse
instruction-following datasets are collected. However, a prevailing challenge
persists in these approaches, specifically in relation to the limited visual
perception ability, as CLIP-like encoders employed for extracting visual
information from inputs. Though these encoders are pre-trained on billions of
image-text pairs, they still grapple with the information loss dilemma, given
that textual captions only partially capture the contents depicted in images.
To address this limitation, this paper proposes to improve the visual
perception ability of MLLMs through a mixture-of-experts knowledge enhancement
mechanism. Specifically, we introduce a novel method that incorporates
multi-task encoders and visual tools into the existing MLLMs training and
inference pipeline, aiming to provide a more comprehensive and accurate
summarization of visual inputs. Extensive experiments have evaluated its
effectiveness of advancing MLLMs, showcasing improved visual perception
achieved through the integration of visual experts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03201">3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding. (arXiv:2401.03201v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zeju Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1">Ruilong Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruifei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangde Liu</a></p>
<p>The remarkable potential of multi-modal large language models (MLLMs) in
comprehending both vision and language information has been widely
acknowledged. However, the scarcity of 3D scenes-language pairs in comparison
to their 2D counterparts, coupled with the inadequacy of existing approaches in
understanding of 3D scenes by LLMs, poses a significant challenge. In response,
we collect and construct an extensive dataset comprising 75K
instruction-response pairs tailored for 3D scenes. This dataset addresses tasks
related to 3D VQA, 3D grounding, and 3D conversation. To further enhance the
integration of 3D spatial information into LLMs, we introduce a novel and
efficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment
stage between 3D scenes and language and extends the instruction prompt with
the 3D modality information including the entire scene and segmented objects.
We evaluate the effectiveness of our method across diverse tasks in the 3D
scene domain and find that our approach serves as a strategic means to enrich
LLMs' comprehension of the 3D world. Our code is available at
https://github.com/staymylove/3DMIT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03836">WidthFormer: Toward Efficient Transformer-based BEV View Transformation. (arXiv:2401.03836v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chenhongyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tianwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lichao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1">Elliot J. Crowley</a></p>
<p>In this work, we present WidthFormer, a novel transformer-based
Bird's-Eye-View (BEV) 3D detection method tailored for real-time
autonomous-driving applications. WidthFormer is computationally efficient,
robust and does not require any special engineering effort to deploy. In this
work, we propose a novel 3D positional encoding mechanism capable of accurately
encapsulating 3D geometric information, which enables our model to generate
high-quality BEV representations with only a single transformer decoder layer.
This mechanism is also beneficial for existing sparse 3D object detectors.
Inspired by the recently-proposed works, we further improve our model's
efficiency by vertically compressing the image features when serving as
attention keys and values. We also introduce two modules to compensate for
potential information loss due to feature compression. Experimental evaluation
on the widely-used nuScenes 3D object detection benchmark demonstrates that our
method outperforms previous approaches across different 3D detection
architectures. More importantly, our model is highly efficient. For example,
when using $256\times 704$ input images, it achieves 1.5 ms and 2.8 ms latency
on NVIDIA 3090 GPU and Horizon Journey-5 computation solutions, respectively.
Furthermore, WidthFormer also exhibits strong robustness to different degrees
of camera perturbations. Our study offers valuable insights into the deployment
of BEV transformation methods in real-world, complex road environments. Code is
available at https://github.com/ChenhongyiYang/WidthFormer .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04377">Towards Real-World Aerial Vision Guidance with Categorical 6D Pose Tracker. (arXiv:2401.04377v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingtao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danwei Wang</a></p>
<p>Tracking the object 6-DoF pose is crucial for various downstream robot tasks
and real-world applications. In this paper, we investigate the real-world robot
task of aerial vision guidance for aerial robotics manipulation, utilizing
category-level 6-DoF pose tracking. Aerial conditions inevitably introduce
special challenges, such as rapid viewpoint changes in pitch and roll and
inter-frame differences. To support these challenges in task, we firstly
introduce a robust category-level 6-DoF pose tracker (Robust6DoF). This tracker
leverages shape and temporal prior knowledge to explore optimal inter-frame
keypoint pairs, generated under a priori structural adaptive supervision in a
coarse-to-fine manner. Notably, our Robust6DoF employs a Spatial-Temporal
Augmentation module to deal with the problems of the inter-frame differences
and intra-class shape variations through both temporal dynamic filtering and
shape-similarity filtering. We further present a Pose-Aware Discrete Servo
strategy (PAD-Servo), serving as a decoupling approach to implement the final
aerial vision guidance task. It contains two servo action policies to better
accommodate the structural properties of aerial robotics manipulation.
Exhaustive experiments on four well-known public benchmarks demonstrate the
superiority of our Robust6DoF. Real-world tests directly verify that our
Robust6DoF along with PAD-Servo can be readily used in real-world aerial
robotic applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04464">PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fibaek_C/0/1/0/all/0/1">Casper Fibaek</a>, <a href="http://arxiv.org/find/cs/1/au:+Camilleri_L/0/1/0/all/0/1">Luke Camilleri</a>, <a href="http://arxiv.org/find/cs/1/au:+Luyts_A/0/1/0/all/0/1">Andreas Luyts</a>, <a href="http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1">Nikolaos Dionelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1">Bertrand Le Saux</a></p>
<p>Massive amounts of unlabelled data are captured by Earth Observation (EO)
satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.
This makes Remote Sensing a data-rich domain well suited to Machine Learning
(ML) solutions. However, a bottleneck in applying ML models to EO is the lack
of annotated data as annotation is a labour-intensive and costly process. As a
result, research in this domain has focused on Self-Supervised Learning and
Foundation Model approaches. This paper addresses the need to evaluate
different Foundation Models on a fair and uniform benchmark by introducing the
PhilEO Bench, a novel evaluation framework for EO Foundation Models. The
framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset
containing labels for three downstream tasks, building density estimation, road
segmentation, and land cover classification. We present experiments using our
framework evaluating different Foundation Models, including Prithvi and SatMAE,
at multiple n-shots and convergence rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05153">CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model. (arXiv:2401.05153v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1">Yinghui Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Litao Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shizhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanning Zhang</a></p>
<p>Fusion of a panchromatic (PAN) image and corresponding multispectral (MS)
image is also known as pansharpening, which aims to combine abundant spatial
details of PAN and spectral information of MS. Due to the absence of
high-resolution MS images, available deep-learning-based methods usually follow
the paradigm of training at reduced resolution and testing at both reduced and
full resolution. When taking original MS and PAN images as inputs, they always
obtain sub-optimal results due to the scale variation. In this paper, we
propose to explore the self-supervised representation of pansharpening by
designing a cross-predictive diffusion model, named CrossDiff. It has two-stage
training. In the first stage, we introduce a cross-predictive pretext task to
pre-train the UNet structure based on conditional DDPM, while in the second
stage, the encoders of the UNets are frozen to directly extract spatial and
spectral features from PAN and MS, and only the fusion head is trained to adapt
for pansharpening task. Extensive experiments show the effectiveness and
superiority of the proposed model compared with state-of-the-art supervised and
unsupervised methods. Besides, the cross-sensor experiments also verify the
generalization ability of proposed self-supervised representation learners for
other satellite's datasets. We will release our code for reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05577">VLP: Vision Language Planning for Autonomous Driving. (arXiv:2401.05577v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chenbin Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaman_B/0/1/0/all/0/1">Burhaneddin Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Nesti_T/0/1/0/all/0/1">Tommaso Nesti</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallik_A/0/1/0/all/0/1">Abhirup Mallik</a>, <a href="http://arxiv.org/find/cs/1/au:+Allievi_A/0/1/0/all/0/1">Alessandro G Allievi</a>, <a href="http://arxiv.org/find/cs/1/au:+Velipasalar_S/0/1/0/all/0/1">Senem Velipasalar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1">Liu Ren</a></p>
<p>Autonomous driving is a complex and challenging task that aims at safe motion
planning through scene understanding and reasoning. While vision-only
autonomous driving methods have recently achieved notable performance, through
enhanced scene understanding, several key issues, including lack of reasoning,
low generalization performance and long-tail scenarios, still need to be
addressed. In this paper, we present VLP, a novel Vision-Language-Planning
framework that exploits language models to bridge the gap between linguistic
understanding and autonomous driving. VLP enhances autonomous driving systems
by strengthening both the source memory foundation and the self-driving car's
contextual understanding. VLP achieves state-of-the-art end-to-end planning
performance on the challenging NuScenes dataset by achieving 35.9\% and 60.5\%
reduction in terms of average L2 error and collision rates, respectively,
compared to the previous best method. Moreover, VLP shows improved performance
in challenging long-tail scenarios and strong generalization capabilities when
faced with new urban environments.
</p>
</p>
</div>

    </div>
    </body>
    