<!DOCTYPE html>
<html>
<head>
<title>2023-12-30-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.16180">Investigating salient representations and label Variance in Dimensional Speech Emotion Analysis. (arXiv:2312.16180v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1">Vikramjit Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jingping Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Azemi_E/0/1/0/all/0/1">Erdrin Azemi</a></p>
<p>Representations derived from models such as BERT (Bidirectional Encoder
Representations from Transformers) and HuBERT (Hidden units BERT), have helped
to achieve state-of-the-art performance in dimensional speech emotion
recognition. Despite their large dimensionality, and even though these
representations are not tailored for emotion recognition tasks, they are
frequently used to train large speech emotion models with high memory and
computational costs. In this work, we show that there exist lower-dimensional
subspaces within the these pre-trained representational spaces that offer a
reduction in downstream model complexity without sacrificing performance on
emotion estimation. In addition, we model label uncertainty in the form of
grader opinion variance, and demonstrate that such information can improve the
models generalization capacity and robustness. Finally, we compare the
robustness of the emotion models against acoustic degradations and observed
that the reduced dimensional representations were able to retain the
performance similar to the full-dimensional representations without significant
regression in dimensional emotion performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16233">Chatbot is Not All You Need: Information-rich Prompting for More Realistic Responses. (arXiv:2312.16233v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Seokhoon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Makhmud_A/0/1/0/all/0/1">Assentay Makhmud</a></p>
<p>Recent Large Language Models (LLMs) have shown remarkable capabilities in
mimicking fictional characters or real humans in conversational settings.
However, the realism and consistency of these responses can be further enhanced
by providing richer information of the agent being mimicked. In this paper, we
propose a novel approach to generate more realistic and consistent responses
from LLMs, leveraging five senses, attributes, emotional states, relationship
with the interlocutor, and memories. By incorporating these factors, we aim to
increase the LLM's capacity for generating natural and realistic reactions in
conversational exchanges. Through our research, we expect to contribute to the
development of LLMs that demonstrate improved capabilities in mimicking
fictional characters. We release a new benchmark dataset and all our codes,
prompts, and sample results on our Github:
https://github.com/srafsasm/InfoRichBot
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16257">More than Correlation: Do Large Language Models Learn Causal Representations of Space?. (arXiv:2312.16257v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yida Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yixian Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Li Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiaohan Zhao</a></p>
<p>Recent work found high mutual information between the learned representations
of large language models (LLMs) and the geospatial property of its input,
hinting an emergent internal model of space. However, whether this internal
space model has any causal effects on the LLMs' behaviors was not answered by
that work, led to criticism of these findings as mere statistical correlation.
Our study focused on uncovering the causality of the spatial representations in
LLMs. In particular, we discovered the potential spatial representations in
DeBERTa, GPT-Neo using representational similarity analysis and linear and
non-linear probing. Our casual intervention experiments showed that the spatial
representations influenced the model's performance on next word prediction and
a downstream task that relies on geospatial information. Our experiments
suggested that the LLMs learn and use an internal model of space in solving
geospatial related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16291">Observable Propagation: A Data-Efficient Approach to Uncover Feature Vectors in Transformers. (arXiv:2312.16291v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dunefsky_J/0/1/0/all/0/1">Jacob Dunefsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>A key goal of current mechanistic interpretability research in NLP is to find
linear features (also called "feature vectors") for transformers: directions in
activation space corresponding to concepts that are used by a given model in
its computation. Present state-of-the-art methods for finding linear features
require large amounts of labelled data -- both laborious to acquire and
computationally expensive to utilize. In this work, we introduce a novel
method, called "observable propagation" (in short: ObsProp), for finding linear
features used by transformer language models in computing a given task -- using
almost no data. Our paradigm centers on the concept of observables, linear
functionals corresponding to given tasks. We then introduce a mathematical
theory for the analysis of feature vectors: we provide theoretical motivation
for why LayerNorm nonlinearities do not affect the direction of feature
vectors; we also introduce a similarity metric between feature vectors called
the coupling coefficient which estimates the degree to which one feature's
output correlates with another's. We use ObsProp to perform extensive
qualitative investigations into several tasks, including gendered occupational
bias, political party prediction, and programming language detection. Our
results suggest that ObsProp surpasses traditional approaches for finding
feature vectors in the low-data regime, and that ObsProp can be used to better
understand the mechanisms responsible for bias in large language models. Code
for experiments can be found at github.com/jacobdunefsky/ObservablePropagation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16309">Contribuci\&#x27;on de la sem\&#x27;antica combinatoria al desarrollo de herramientas digitales multiling\&quot;ues. (arXiv:2312.16309v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1">Mar&#xed;a Jos&#xe9; Dom&#xed;nguez V&#xe1;zquez</a></p>
<p>This paper describes how the field of Combinatorial Semantics has contributed
to the design of three prototypes for the automatic generation of argument
patterns in nominal phrases in Spanish, French and German (Xera, Combinatoria
and CombiContext). It also shows the importance of knowing about the argument
syntactic-semantic interface in a production situation in the context of
foreign languages. After a descriptive section on the design, typologie and
information levels of the resources, there follows an explanation of the
central role of the combinatorial meaning (roles and ontological features). The
study deals with different semantic f ilters applied in the selection,
organization and expansion of the lexicon, being these key pieces for the
generation of grammatically correct and semantically acceptable mono- and
biargumental nominal phrases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16311">Zur Darstellung eines mehrstufigen Prototypbegriffs in der multilingualen automatischen Sprachgenerierung: vom Korpus \&quot;uber word embeddings bis hin zum automatischen W\&quot;orterbuch. (arXiv:2312.16311v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1">Mar&#xed;a Jos&#xe9; Dom&#xed;nguez V&#xe1;zquez</a></p>
<p>The multilingual dictionary of noun valency Portlex is considered to be the
trigger for the creation of the automatic language generators Xera and
Combinatoria, whose development and use is presented in this paper. Both
prototypes are used for the automatic generation of nominal phrases with their
mono- and bi-argumental valence slots, which could be used, among others, as
dictionary examples or as integrated components of future autonomous
E-Learning-Tools. As samples for new types of automatic valency dictionaries
including user interaction, we consider the language generators as we know them
today. In the specific methodological procedure for the development of the
language generators, the syntactic-semantic description of the noun slots turns
out to be the main focus from a syntagmatic and paradigmatic point of view.
Along with factors such as representativeness, grammatical correctness,
semantic coherence, frequency and the variety of lexical candidates, as well as
semantic classes and argument structures, which are fixed components of both
resources, a concept of a multi-sided prototype stands out. The combined
application of this prototype concept as well as of word embeddings together
with techniques from the field of automatic natural language processing and
generation (NLP and NLG) opens up a new way for the future development of
automatically generated plurilingual valency dictionaries. All things
considered, the paper depicts the language generators both from the point of
view of their development as well as from that of the users. The focus lies on
the role of the prototype concept within the development of the resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16337">Task Contamination: Language Models May Not Be Few-Shot Anymore. (arXiv:2312.16337v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changmao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1">Jeffrey Flanigan</a></p>
<p>Large language models (LLMs) offer impressive performance in various
zero-shot and few-shot tasks. However, their success in zero-shot and few-shot
settings may be affected by task contamination, a potential limitation that has
not been thoroughly examined. This paper investigates how zero-shot and
few-shot performance of LLMs has changed chronologically over time. Utilizing
GPT-3 series models and several other recent open-sourced LLMs, and controlling
for dataset difficulty, we find that on datasets released before the LLM
training data creation date, LLMs perform surprisingly better than on datasets
released after. This strongly indicates that, for many LLMs, there exists task
contamination on zero-shot and few-shot evaluation for datasets released prior
to the LLMs' training data creation date. Additionally, we utilize training
data inspection, task example extraction, and a membership inference attack,
which reveal further evidence of task contamination. Importantly, we find that
for classification tasks with no possibility of task contamination, LLMs rarely
demonstrate statistically significant improvements over simple majority
baselines, in both zero and few-shot settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16374">LLM Polygraph: Uncovering LLMs&#x27; Factual Discernment through Intermediate Data Analysis. (arXiv:2312.16374v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinwen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yujia Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zijin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chengan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yue Zhao</a></p>
<p>Large Language Models (LLMs) have revolutionized various domains with
extensive knowledge and creative capabilities. However, a critical issue with
LLMs is their tendency to produce outputs that diverge from factual reality.
This phenomenon is particularly concerning in sensitive applications such as
medical consultation and legal advice, where accuracy is paramount. In this
paper, we introduce the LLM factoscope, a novel Siamese network-based model
that leverages the inner states of LLMs for factual detection. Our
investigation reveals distinguishable patterns in LLMs' inner states when
generating factual versus non-factual content. We demonstrate the LLM
factoscope's effectiveness across various architectures, achieving over 96%
accuracy in factual detection. Our work opens a new avenue for utilizing LLMs'
inner states for factual detection and encourages further exploration into
LLMs' inner workings for enhanced reliability and transparency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16378">Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs. (arXiv:2312.16378v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oruganti_S/0/1/0/all/0/1">Sanjay Oruganti</a>, <a href="http://arxiv.org/find/cs/1/au:+Nirenburg_S/0/1/0/all/0/1">Sergei Nirenburg</a>, <a href="http://arxiv.org/find/cs/1/au:+English_J/0/1/0/all/0/1">Jesse English</a>, <a href="http://arxiv.org/find/cs/1/au:+McShane_M/0/1/0/all/0/1">Marjorie McShane</a></p>
<p>The paper describes a system that uses large language model (LLM) technology
to support the automatic learning of new entries in an intelligent agent's
semantic lexicon. The process is bootstrapped by an existing non-toy lexicon
and a natural language generator that converts formal, ontologically-grounded
representations of meaning into natural language sentences. The learning method
involves a sequence of LLM requests and includes an automatic quality control
step. To date, this learning method has been applied to learning multiword
expressions whose meanings are equivalent to those of transitive verbs in the
agent's lexicon. The experiment demonstrates the benefits of a hybrid learning
architecture that integrates knowledge-based methods and resources with both
traditional data analytics and LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16467">Transfer and Alignment Network for Generalized Category Discovery. (arXiv:2312.16467v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1">Wenbin An</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1">Feng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Wenkai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yaqiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Ping Chen</a></p>
<p>Generalized Category Discovery is a crucial real-world task. Despite the
improved performance on known categories, current methods perform poorly on
novel categories. We attribute the poor performance to two reasons: biased
knowledge transfer between labeled and unlabeled data and noisy representation
learning on the unlabeled data. To mitigate these two issues, we propose a
Transfer and Alignment Network (TAN), which incorporates two knowledge transfer
mechanisms to calibrate the biased knowledge and two feature alignment
mechanisms to learn discriminative features. Specifically, we model different
categories with prototypes and transfer the prototypes in labeled data to
correct model bias towards known categories. On the one hand, we pull instances
with known categories in unlabeled data closer to these prototypes to form more
compact clusters and avoid boundary overlap between known and novel categories.
On the other hand, we use these prototypes to calibrate noisy prototypes
estimated from unlabeled data based on category similarities, which allows for
more accurate estimation of prototypes for novel categories that can be used as
reliable learning targets later. After knowledge transfer, we further propose
two feature alignment mechanisms to acquire both instance- and category-level
knowledge from unlabeled data by aligning instance features with both augmented
features and the calibrated prototypes, which can boost model performance on
both known and novel categories with less noise. Experiments on three benchmark
datasets show that our model outperforms SOTA methods, especially on novel
categories. Theoretical analysis is provided for an in-depth understanding of
our model in general. Our code and data are available at
https://github.com/Lackel/TAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16488">Source Code is a Graph, Not a Sequence: A Cross-Lingual Perspective on Code Clone Detection. (arXiv:2312.16488v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1">Mohammed Ataaur Rahaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1">Julia Ive</a></p>
<p>Source code clone detection is the task of finding code fragments that have
the same or similar functionality, but may differ in syntax or structure. This
task is important for software maintenance, reuse, and quality assurance (Roy
et al. 2009). However, code clone detection is challenging, as source code can
be written in different languages, domains, and styles. In this paper, we argue
that source code is inherently a graph, not a sequence, and that graph-based
methods are more suitable for code clone detection than sequence-based methods.
We compare the performance of two state-of-the-art models: CodeBERT (Feng et
al. 2020), a sequence-based model, and CodeGraph (Yu et al. 2023), a
graph-based model, on two benchmark data-sets: BCB (Svajlenko et al. 2014) and
PoolC (PoolC no date). We show that CodeGraph outperforms CodeBERT on both
data-sets, especially on cross-lingual code clones. To the best of our
knowledge, this is the first work to demonstrate the superiority of graph-based
methods over sequence-based methods on cross-lingual code clone detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16490">Understanding News Creation Intents: Frame, Dataset, and Method. (arXiv:2312.16490v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengjia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1">Qiang Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Juan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Silong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Beizhe Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Siyuan Ma</a></p>
<p>As the disruptive changes in the media economy and the proliferation of
alternative news media outlets, news intent has progressively deviated from
ethical standards that serve the public interest. News intent refers to the
purpose or intention behind the creation of a news article. While the
significance of research on news intent has been widely acknowledged, the
absence of a systematic news intent understanding framework hinders further
exploration of news intent and its downstream applications. To bridge this gap,
we propose News INTent (NINT) frame, the first component-aware formalism for
understanding the news creation intent based on research in philosophy,
psychology, and cognitive science. Within this frame, we define the news intent
identification task and provide a benchmark dataset with fine-grained labels
along with an efficient benchmark method. Experiments demonstrate that NINT is
beneficial in both the intent identification task and downstream tasks that
demand a profound understanding of news. This work marks a foundational step
towards a more systematic exploration of news creation intents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16511">S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering. (arXiv:2312.16511v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baokui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wangshu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yicheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Changlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Sen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Teng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+liu_S/0/1/0/all/0/1">Siye liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a></p>
<p>Supplying data augmentation to conversational question answering (CQA) can
effectively improve model performance. However, there is less improvement from
single-turn datasets in CQA due to the distribution gap between single-turn and
multi-turn datasets. On the other hand, while numerous single-turn datasets are
available, we have not utilized them effectively. To solve this problem, we
propose a novel method to convert single-turn datasets to multi-turn datasets.
The proposed method consists of three parts, namely, a QA pair Generator, a QA
pair Reassembler, and a question Rewriter. Given a sample consisting of context
and single-turn QA pairs, the Generator obtains candidate QA pairs and a
knowledge graph based on the context. The Reassembler utilizes the knowledge
graph to get sequential QA pairs, and the Rewriter rewrites questions from a
conversational perspective to obtain a multi-turn dataset S2M. Our experiments
show that our method can synthesize effective training resources for CQA.
Notably, S2M ranks 1st place on the QuAC leaderboard at the time of submission
(Aug 24th, 2022).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16548">A proposed new metric for the conceptual diversity of a text. (arXiv:2312.16548v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phd_I/0/1/0/all/0/1">&#x130;lknur D&#xf6;nmez Phd</a>, <a href="http://arxiv.org/find/cs/1/au:+Phd_M/0/1/0/all/0/1">Mehmet Hakl&#x131;d&#x131;r Phd</a></p>
<p>A word may contain one or more hidden concepts. While the "animal" word
evokes many images in our minds and encapsulates many concepts (birds, dogs,
cats, crocodiles, etc.), the `parrot' word evokes a single image (a colored
bird with a short, hooked beak and the ability to mimic sounds). In spoken or
written texts, we use some words in a general sense and some in a detailed way
to point to a specific object. Until now, a text's conceptual diversity value
cannot be determined using a standard and precise technique. This research
contributes to the natural language processing field of AI by offering a
standardized method and a generic metric for evaluating and comparing concept
diversity in different texts and domains. It also contributes to the field of
semantic research of languages. If we give examples for the diversity score of
two sentences, "He discovered an unknown entity." has a high conceptual
diversity score (16.6801), and "The endoplasmic reticulum forms a series of
flattened sacs within the cytoplasm of eukaryotic cells." sentence has a low
conceptual diversity score which is 3.9068.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16549">How Robust are LLMs to In-Context Majority Label Bias?. (arXiv:2312.16549v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1">Karan Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1">Sumegh Roychowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1">Siva Rajesh Kasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1">Santhosh Kumar Kasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhanushali_A/0/1/0/all/0/1">Anish Bhanushali</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattisapu_N/0/1/0/all/0/1">Nikhil Pattisapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Murthy_P/0/1/0/all/0/1">Prasanna Srinivasa Murthy</a></p>
<p>In the In-Context Learning (ICL) setup, various forms of label biases can
manifest. One such manifestation is majority label bias, which arises when the
distribution of labeled examples in the in-context samples is skewed towards
one or more specific classes making Large Language Models (LLMs) more prone to
predict those labels. Such discrepancies can arise from various factors,
including logistical constraints, inherent biases in data collection methods,
limited access to diverse data sources, etc. which are unavoidable in a
real-world industry setup. In this work, we study the robustness of in-context
learning in LLMs to shifts that occur due to majority label bias within the
purview of text classification tasks. Prior works have shown that in-context
learning with LLMs is susceptible to such biases. In our study, we go one level
deeper and show that the robustness boundary varies widely for different models
and tasks, with certain LLMs being highly robust (~90%) to majority label bias.
Additionally, our findings also highlight the impact of model size and the
richness of instructional prompts contributing towards model robustness. We
restrict our study to only publicly available open-source models to ensure
transparency and reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16599">Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN). (arXiv:2312.16599v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kejriwal_J/0/1/0/all/0/1">Jay Kejriwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Benus_S/0/1/0/all/0/1">&#x160;tefan Be&#x148;u&#x161;</a></p>
<p>The tendency of people to engage in similar, matching, or synchronized
behaviour when interacting is known as entrainment. Many studies examined
linguistic (syntactic and lexical structures) and paralinguistic (pitch,
intensity) entrainment, but less attention was given to finding the
relationship between them. In this study, we utilized state-of-the-art DNN
embeddings such as BERT and TRIpLet Loss network (TRILL) vectors to extract
features for measuring semantic and auditory similarities of turns within
dialogues in two comparable spoken corpora of two different languages. We found
people's tendency to entrain on semantic features more when compared to
auditory features. Additionally, we found that entrainment in semantic and
auditory linguistic features are positively correlated. The findings of this
study might assist in implementing the mechanism of entrainment in
human-machine interaction (HMI).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16623">Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model. (arXiv:2312.16623v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yongchang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xinyu Dai</a></p>
<p>BERT-based models have shown a remarkable ability in the Chinese Spelling
Check (CSC) task recently. However, traditional BERT-based methods still suffer
from two limitations. First, although previous works have identified that
explicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the
CSC task, they neglected the fact that spelling errors inherent in CSC data can
lead to incorrect tags and therefore mislead models. Additionally, they ignored
the correlation between the implicit hierarchical information encoded by BERT's
intermediate layers and different linguistic phenomena. This results in
sub-optimal accuracy. To alleviate the above two issues, we design a
heterogeneous knowledge-infused framework to strengthen BERT-based CSC models.
To incorporate explicit POS knowledge, we utilize an auxiliary task strategy
driven by Gaussian mixture model. Meanwhile, to incorporate implicit
hierarchical linguistic knowledge within the encoder, we propose a novel form
of n-gram-based layerwise self-attention to generate a multilayer
representation. Experimental results show that our proposed framework yields a
stable performance boost over four strong baseline models and outperforms the
previous state-of-the-art methods on two datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16659">A Large Language Model-based Computational Approach to Improve Identity-Related Write-Ups. (arXiv:2312.16659v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doboli_A/0/1/0/all/0/1">Alex Doboli</a></p>
<p>Creating written products is essential to modern life, including writings
about one's identity and personal experiences. However, writing is often a
difficult activity that requires extensive effort to frame the central ideas,
the pursued approach to communicate the central ideas, e.g., using analogies,
metaphors, or other possible means, the needed presentation structure, and the
actual verbal expression. Large Language Models, a recently emerged approach in
Machine Learning, can offer a significant help in reducing the effort and
improving the quality of written products. This paper proposes a new
computational approach to explore prompts that given as inputs to a Large
Language Models can generate cues to improve the considered written products.
Two case studies on improving write-ups, one based on an analogy and one on a
metaphor, are also presented in the paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16682">Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss. (arXiv:2312.16682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1">Andrew Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1">Sainbayar Sukhbaatar</a>, <a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1">Jason Weston</a></p>
<p>Practitioners commonly align large language models using pairwise
preferences, i.e., given labels of the type response A is preferred to response
B for a given input. Perhaps less commonly, methods have also been developed
for binary feedback, i.e. training models given labels of type response A is
good or bad. We show how an existing performant binary feedback method, the
Cringe Loss (Adolphs et al., 2022), can be generalized to the pairwise
preference setting using a simple soft margin extension. Pairwise Cringe Loss
is straightforward to implement and efficient to train, and we find it
outperforms state-of-the-art preference optimization algorithms such as PPO and
DPO on the AlpacaFarm benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16702">Rethinking Tabular Data Understanding with Large Language Models. (arXiv:2312.16702v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>Large Language Models (LLMs) have shown to be capable of various tasks, yet
their capability in interpreting and reasoning over tabular data remains an
underexplored area. In this context, this study investigates from three core
perspectives: the robustness of LLMs to structural perturbations in tables, the
comparative analysis of textual and symbolic reasoning on tables, and the
potential of boosting model performance through the aggregation of multiple
reasoning pathways. We discover that structural variance of tables presenting
the same content reveals a notable performance decline, particularly in
symbolic reasoning tasks. This prompts the proposal of a method for table
structure normalization. Moreover, textual reasoning slightly edges out
symbolic reasoning, and a detailed error analysis reveals that each exhibits
different strengths depending on the specific tasks. Notably, the aggregation
of textual and symbolic reasoning pathways, bolstered by a mix self-consistency
mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on
WIKITABLEQUESTIONS, representing a substantial advancement over previous
existing table processing paradigms of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16714">A Reversible Perspective on Petri Nets and Event Structures. (arXiv:2312.16714v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Melgratti_H/0/1/0/all/0/1">Hern&#xe1;n Melgratti</a>, <a href="http://arxiv.org/find/cs/1/au:+Mezzina_C/0/1/0/all/0/1">Claudio Antares Mezzina</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinna_G/0/1/0/all/0/1">G. Michele Pinna</a></p>
<p>Event structures have emerged as a foundational model for concurrent
computation, explaining computational processes by outlining the events and the
relationships that dictate their execution. They play a pivotal role in the
study of key aspects of concurrent computation models, such as causality and
independence, and have found applications across a broad range of languages and
models, spanning realms like persistence, probabilities, and quantum computing.
Recently, event structures have been extended to address reversibility, where
computational processes can undo previous computations. In this context,
reversible event structures provide abstract representations of processes
capable of both forward and backward steps in a computation. Since their
introduction, event structures have played a crucial role in bridging
operational models, traditionally exemplified by Petri nets and process
calculi, with denotational ones, i.e., algebraic domains. In this context, we
revisit the standard connection between Petri nets and event structures under
the lenses of reversibility. Specifically, we introduce a subset of contextual
Petri nets, dubbed reversible causal nets, that precisely correspond to
reversible prime event structures. The distinctive feature of reversible causal
nets lies in deriving causality from inhibitor arcs, departing from the
conventional dependence on the overlap between the post and preset of
transitions. In this way, we are able to operationally explain the full model
of reversible prime event structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16755">Graph Neural Networks for Antisocial Behavior Detection on Twitter. (arXiv:2312.16755v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Toshevska_M/0/1/0/all/0/1">Martina Toshevska</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalajdziski_S/0/1/0/all/0/1">Slobodan Kalajdziski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gievska_S/0/1/0/all/0/1">Sonja Gievska</a></p>
<p>Social media resurgence of antisocial behavior has exerted a downward spiral
on stereotypical beliefs, and hateful comments towards individuals and social
groups, as well as false or distorted news. The advances in graph neural
networks employed on massive quantities of graph-structured data raise high
hopes for the future of mediating communication on social media platforms. An
approach based on graph convolutional data was employed to better capture the
dependencies between the heterogeneous types of data.
</p>
<p>Utilizing past and present experiences on the topic, we proposed and
evaluated a graph-based approach for antisocial behavior detection, with
general applicability that is both language- and context-independent. In this
research, we carried out an experimental validation of our graph-based approach
on several PAN datasets provided as part of their shared tasks, that enable the
discussion of the results obtained by the proposed solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16778">Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition. (arXiv:2312.16778v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shou_Y/0/1/0/all/0/1">Yuntao Shou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1">Tao Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1">Wei Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Keqin Li</a></p>
<p>With the release of increasing open-source emotion recognition datasets on
social media platforms and the rapid development of computing resources,
multimodal emotion recognition tasks (MER) have begun to receive widespread
research attention. The MER task extracts and fuses complementary semantic
information from different modalities, which can classify the speaker's
emotions. However, the existing feature fusion methods have usually mapped the
features of different modalities into the same feature space for information
fusion, which can not eliminate the heterogeneity between different modalities.
Therefore, it is challenging to make the subsequent emotion class boundary
learning. To tackle the above problems, we have proposed a novel Adversarial
Representation with Intra-Modal and Inter-Modal Graph Contrastive for
Multimodal Emotion Recognition (AR-IIGCN) method. Firstly, we input video,
audio, and text features into a multi-layer perceptron (MLP) to map them into
separate feature spaces. Secondly, we build a generator and a discriminator for
the three modal features through adversarial representation, which can achieve
information interaction between modalities and eliminate heterogeneity among
modalities. Thirdly, we introduce contrastive graph representation learning to
capture intra-modal and inter-modal complementary semantic information and
learn intra-class and inter-class boundary information of emotion categories.
Specifically, we construct a graph structure for three modal features and
perform contrastive representation learning on nodes with different emotions in
the same modality and the same emotion in different modalities, which can
improve the feature representation ability of nodes. Extensive experimental
works show that the ARL-IIGCN method can significantly improve emotion
recognition accuracy on IEMOCAP and MELD datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.08303">Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jinpeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiang Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1">Tsung-Hui Chang</a></p>
<p>The impression is crucial for the referring physicians to grasp key
information since it is concluded from the findings and reasoning of
radiologists. To alleviate the workload of radiologists and reduce repetitive
human labor in impression writing, many researchers have focused on automatic
impression generation. However, recent works on this task mainly summarize the
corresponding findings and pay less attention to the radiology images. In
clinical, radiographs can provide more detailed valuable observations to
enhance radiologists' impression writing, especially for complicated cases.
Besides, each sentence in findings usually focuses on single anatomy, so they
only need to be matched to corresponding anatomical regions instead of the
whole image, which is beneficial for textual and visual features alignment.
Therefore, we propose a novel anatomy-enhanced multimodal model to promote
impression generation. In detail, we first construct a set of rules to extract
anatomies and put these prompts into each sentence to highlight anatomy
characteristics. Then, two separate encoders are applied to extract features
from the radiograph and findings. Afterward, we utilize a contrastive learning
module to align these two representations at the overall level and use a
co-attention to fuse them at the sentence level with the help of
anatomy-enhanced sentence representation. Finally, the decoder takes the fused
information as the input to generate impressions. The experimental results on
two benchmark datasets confirm the effectiveness of the proposed method, which
achieves state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04312">Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk technique. (arXiv:2301.04312v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jiahong Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huacan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Feijuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuanzhe Cai</a></p>
<p>Word embedding has become ubiquitous and is widely used in various natural
language processing (NLP) tasks, such as web retrieval, web semantic analysis,
and machine translation, and so on. Unfortunately, training the word embedding
in a relatively large corpus is prohibitively expensive. We propose a
graph-based word embedding algorithm, called Word-Graph2vec, which converts the
large corpus into a word co-occurrence graph, then takes the word sequence
samples from this graph by randomly traveling and trains the word embedding on
this sampling corpus in the end. We posit that because of the limited
vocabulary, huge idioms, and fixed expressions in English, the size and density
of the word co-occurrence graph change slightly with the increase in the
training corpus. So that Word-Graph2vec has stable runtime on the large-scale
data set, and its performance advantage becomes more and more obvious with the
growth of the training corpus. Extensive experiments conducted on real-world
datasets show that the proposed algorithm outperforms traditional Word2vec four
to five times in terms of efficiency and two to three times than FastText,
while the error generated by the random walk technique is small.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10405">Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v8 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hypernetwork to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01234">Frauds Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process. (arXiv:2303.01234v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1">Mingze Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhensu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>Recent research has revealed that natural language processing (NLP) models
are vulnerable to adversarial examples. However, the current techniques for
generating such examples rely on deterministic heuristic rules, which fail to
produce optimal adversarial examples. In response, this study proposes a new
method called the Fraud's Bargain Attack (FBA), which uses a randomization
mechanism to expand the search space and produce high-quality adversarial
examples with a higher probability of success. FBA uses the Metropolis-Hasting
sampler, a type of Markov Chain Monte Carlo sampler, to improve the selection
of adversarial examples from all candidates generated by a customized
stochastic process called the Word Manipulation Process (WMP). The WMP method
modifies individual words in a contextually-aware manner through insertion,
removal, or substitution. Through extensive experiments, this study
demonstrates that FBA outperforms other methods in terms of attack success
rate, imperceptibility and sentence quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14582">Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyang R. Zhang</a></p>
<p>Multitask learning is widely used in practice to train a low-resource target
task by augmenting it with multiple related source tasks. Yet, naively
combining all the source tasks with a target task does not always improve the
prediction performance for the target task due to negative transfers. Thus, a
critical problem in multitask learning is identifying subsets of source tasks
that would benefit the target task. This problem is computationally challenging
since the number of subsets grows exponentially with the number of source
tasks; efficient heuristics for subset selection do not always capture the
relationship between task subsets and multitask learning performances. In this
paper, we introduce an efficient procedure to address this problem via
surrogate modeling. In surrogate modeling, we sample (random) subsets of source
tasks and precompute their multitask learning performances. Then, we
approximate the precomputed performances with a linear regression model that
can also predict the multitask performance of unseen task subsets. We show
theoretically and empirically that fitting this model only requires sampling
linearly many subsets in the number of source tasks. The fitted model provides
a relevance score between each source and target task. We use the relevance
scores to perform subset selection for multitask learning by thresholding.
Through extensive experiments, we show that our approach predicts negative
transfers from multiple source tasks to target tasks much more accurately than
existing task affinity measures. Additionally, we demonstrate that for several
weak supervision datasets, our approach consistently improves upon existing
optimization methods for multitask learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03236">A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1">Hao Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yinhe Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a></p>
<p>Out-of-distribution (OOD) detection is essential for the reliable and safe
deployment of machine learning systems in the real world. Great progress has
been made over the past years. This paper presents the first review of recent
advances in OOD detection with a particular focus on natural language
processing approaches. First, we provide a formal definition of OOD detection
and discuss several related fields. We then categorize recent algorithms into
three classes according to the data they used: (1) OOD data available, (2) OOD
data unavailable + in-distribution (ID) label available, and (3) OOD data
unavailable + ID label unavailable. Third, we introduce datasets, applications,
and metrics. Finally, we summarize existing work and present potential future
research topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08777">Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes. (arXiv:2305.08777v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1">Maria Mahbub</a>, <a href="http://arxiv.org/find/cs/1/au:+Goethert_I/0/1/0/all/0/1">Ian Goethert</a>, <a href="http://arxiv.org/find/cs/1/au:+Danciu_I/0/1/0/all/0/1">Ioana Danciu</a>, <a href="http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1">Kathryn Knight</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Sudarshan Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamang_S/0/1/0/all/0/1">Suzanne Tamang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rozenberg_Ben_Dror_K/0/1/0/all/0/1">Karine Rozenberg-Ben-Dror</a>, <a href="http://arxiv.org/find/cs/1/au:+Solares_H/0/1/0/all/0/1">Hugo Solares</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_S/0/1/0/all/0/1">Susana Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Trafton_J/0/1/0/all/0/1">Jodie Trafton</a>, <a href="http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1">Edmon Begoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1">Gregory Peterson</a></p>
<p>Background: Injection drug use (IDU) is a dangerous health behavior that
increases mortality and morbidity. Identifying IDU early and initiating harm
reduction interventions can benefit individuals at risk. However, extracting
IDU behaviors from patients' electronic health records (EHR) is difficult
because there is no International Classification of Disease (ICD) code and the
only place IDU information can be indicated is unstructured free-text clinical
notes. Although natural language processing can efficiently extract this
information from unstructured data, there are no validated tools. Methods: To
address this gap in clinical information, we design and demonstrate a
question-answering (QA) framework to extract information on IDU from clinical
notes. Our framework involves two main steps: (1) generating a gold-standard QA
dataset and (2) developing and testing the QA model. We utilize 2323 clinical
notes of 1145 patients sourced from the VA Corporate Data Warehouse to
construct the gold-standard dataset for developing and evaluating the QA model.
We also demonstrate the QA model's ability to extract IDU-related information
on temporally out-of-distribution data. Results: Here we show that for a strict
match between gold-standard and predicted answers, the QA model achieves 51.65%
F1 score. For a relaxed match between the gold-standard and predicted answers,
the QA model obtains 78.03% F1 score, along with 85.38% Precision and 79.02%
Recall scores. Moreover, the QA model demonstrates consistent performance when
subjected to temporally out-of-distribution data. Conclusions: Our study
introduces a QA framework designed to extract IDU information from clinical
notes, aiming to enhance the accurate and efficient detection of people who
inject drugs, extract relevant information, and ultimately facilitate informed
patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18365">What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Taicheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Kehan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_B/0/1/0/all/0/1">Bozhao Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhenwen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhichun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1">Nitesh V. Chawla</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiest_O/0/1/0/all/0/1">Olaf Wiest</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangliang Zhang</a></p>
<p>Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been applied in various kinds of areas
such as science, finance and software engineering. However, the capability of
LLMs to advance the field of chemistry remains unclear. In this paper, rather
than pursuing state-of-the-art performance, we aim to evaluate capabilities of
LLMs in a wide range of tasks across the chemistry domain. We identify three
key chemistry-related capabilities including understanding, reasoning and
explaining to explore in LLMs and establish a benchmark containing eight
chemistry tasks. Our analysis draws on widely recognized datasets facilitating
a broad exploration of the capacities of LLMs within the context of practical
chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are
evaluated for each chemistry task in zero-shot and few-shot in-context learning
settings with carefully selected demonstration examples and specially crafted
prompts. Our investigation found that GPT-4 outperformed other models and LLMs
exhibit different competitive levels in eight chemistry tasks. In addition to
the key findings from the comprehensive benchmark analysis, our work provides
insights into the limitation of current LLMs and the impact of in-context
learning settings on LLMs' performance across various chemistry tasks. The code
and datasets used in this study are available at
https://github.com/ChemFoundationModels/ChemLLMBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19926">Revisiting the Reliability of Psychological Scales on Large Language Models. (arXiv:2305.19926v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jen-tse Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1">Man Ho Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1">Eric John Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenxiang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Michael R. Lyu</a></p>
<p>Recent research has extended beyond assessing the performance of Large
Language Models (LLMs) to examining their characteristics from a psychological
standpoint, acknowledging the necessity of understanding their behavioral
characteristics. The administration of personality tests to LLMs has emerged as
a noteworthy area in this context. However, the suitability of employing
psychological scales, initially devised for humans, on LLMs is a matter of
ongoing debate. Our study aims to determine the reliability of applying
personality assessments to LLMs, explicitly investigating whether LLMs
demonstrate consistent personality traits. Analyzing responses under 2,500
settings reveals that gpt-3.5-turbo shows consistency in responses to the Big
Five Inventory, indicating a high degree of reliability. Furthermore, our
research explores the potential of gpt-3.5-turbo to emulate diverse
personalities and represent various groups, which is a capability increasingly
sought after in social sciences for substituting human participants with LLMs
to reduce costs. Our findings reveal that LLMs have the potential to represent
different personalities with specific prompt instructions. By shedding light on
the personalization of LLMs, our study endeavors to pave the way for future
explorations in this field. We have made our experimental results and the
corresponding code openly accessible via
https://github.com/CUHK-ARISE/LLMPersonality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00789">Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1">Sameer Khurana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dawalatabad_N/0/1/0/all/0/1">Nauman Dawalatabad</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1">Antoine Laurent</a>, <a href="http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1">Luis Vicente</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1">Pablo Gimeno</a>, <a href="http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1">Victoria Mingote</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1">Jonathan Le Roux</a>, <a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1">James Glass</a></p>
<p>The paper presents a novel three-step transfer learning framework for
enhancing cross-lingual transfer from high- to low-resource languages in the
downstream application of Automatic Speech Translation. The approach integrates
a semantic knowledge-distillation step into the existing two-step cross-lingual
transfer learning framework XLS-R. This extra step aims to encode semantic
knowledge in the multilingual speech encoder pre-trained via Self-Supervised
Learning using unlabeled speech. Our proposed three-step cross-lingual transfer
learning framework addresses the large cross-lingual transfer gap (TRFGap)
observed in the XLS-R framework between high-resource and low-resource
languages. We validate our proposal through extensive experiments and
comparisons on the CoVoST-2 benchmark, showing significant improvements in
translation performance, especially for low-resource languages, and a notable
reduction in the TRFGap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04047">CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments. (arXiv:2306.04047v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiulong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1">Sudipta Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1">Moitreya Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1">Anoop Cherian</a></p>
<p>Audio-visual navigation of an agent towards locating an audio goal is a
challenging task especially when the audio is sporadic or the environment is
noisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual
Embodied Navigation framework in which the agent may interact with a
human/oracle for solving the task of navigating to an audio goal. Specifically,
CAVEN is modeled as a budget-aware partially observable semi-Markov decision
process that implicitly learns the uncertainty in the audio-based navigation
policy to decide when and how the agent may interact with the oracle. Our CAVEN
agent can engage in fully-bidirectional natural language conversations by
producing relevant questions and interpret free-form, potentially noisy
responses from the oracle based on the audio-visual context. To enable such a
capability, CAVEN is equipped with: (i) a trajectory forecasting network that
is grounded in audio-visual cues to produce a potential trajectory to the
estimated goal, and (ii) a natural language based question generation and
reasoning network to pose an interactive question to the oracle or interpret
the oracle's response to produce navigation instructions. To train the
interactive modules, we present a large scale dataset: AVN-Instruct, based on
the Landmark-RxR dataset. To substantiate the usefulness of conversations, we
present experiments on the benchmark audio-goal task using the SoundSpaces
simulator under various noisy settings. Our results reveal that our
fully-conversational approach leads to nearly an order-of-magnitude improvement
in success rate, especially in localizing new sound sources and against methods
that only use uni-directional interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03952">Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yu Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a></p>
<p>In recent years, personality has been regarded as a valuable personal factor
being incorporated into numerous tasks such as sentiment analysis and product
recommendation. This has led to widespread attention to text-based personality
recognition task, which aims to identify an individual's personality based on
given text. Considering that ChatGPT has recently exhibited remarkable
abilities on various natural language processing tasks, we provide a
preliminary evaluation of ChatGPT on text-based personality recognition task
for generating effective personality data. Concretely, we employ a variety of
prompting strategies to explore ChatGPT's ability in recognizing personality
from given text, especially the level-oriented prompting strategy we designed
for guiding ChatGPT in analyzing given text at a specified level. The
experimental results on two representative real-world datasets reveal that
ChatGPT with zero-shot chain-of-thought prompting exhibits impressive
personality recognition ability and is capable to provide natural language
explanations through text-based logical reasoning. Furthermore, by employing
the level-oriented prompting strategy to optimize zero-shot chain-of-thought
prompting, the performance gap between ChatGPT and corresponding
state-of-the-art model has been narrowed even more. However, we observe that
ChatGPT shows unfairness towards certain sensitive demographic attributes such
as gender and age. Additionally, we discover that eliciting the personality
recognition ability of ChatGPT helps improve its performance on
personality-related downstream tasks such as sentiment classification and
stress prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06435">A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1">Humza Naveed</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Asad Ullah Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Shi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1">Muhammad Saqib</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1">Saeed Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1">Muhammad Usman</a>, <a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1">Naveed Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1">Nick Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1">Ajmal Mian</a></p>
<p>Large Language Models (LLMs) have recently demonstrated remarkable
capabilities in natural language processing tasks and beyond. This success of
LLMs has led to a large influx of research contributions in this direction.
These works encompass diverse topics such as architectural innovations, better
training strategies, context length improvements, fine-tuning, multi-modal
LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid
development of techniques and regular breakthroughs in LLM research, it has
become considerably challenging to perceive the bigger picture of the advances
in this direction. Considering the rapidly emerging plethora of literature on
LLMs, it is imperative that the research community is able to benefit from a
concise yet comprehensive overview of the recent developments in this field.
This article provides an overview of the existing literature on a broad range
of LLM-related concepts. Our self-contained comprehensive overview of LLMs
discusses relevant background concepts along with covering the advanced topics
at the frontier of research in LLMs. This review article is intended to not
only provide a systematic survey but also a quick comprehensive reference for
the researchers and practitioners to draw insights from extensive informative
summaries of the existing works to advance the LLM research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16082">EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1">Mohammadali Sefidi Esfahani</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1">Mohammad Akbari</a></p>
<p>Social platforms have emerged as crucial platforms for disseminating
information and discussing real-life social events, offering researchers an
excellent opportunity to design and implement novel event detection frameworks.
However, most existing approaches only exploit keyword burstiness or network
structures to detect unspecified events. Thus, they often need help identifying
unknown events regarding the challenging nature of events and social data.
Social data, e.g., tweets, is characterized by misspellings, incompleteness,
word sense ambiguation, irregular language, and variation in aspects of
opinions. Moreover, extracting discriminative features and patterns for
evolving events by exploiting the limited structural knowledge is almost
infeasible. To address these challenges, in this paper, we propose a novel
framework, namely EnrichEvent, that leverages the linguistic and contextual
representations of streaming social data. In particular, we leverage contextual
and linguistic knowledge to detect semantically related tweets and enhance the
effectiveness of the event detection approaches. Eventually, our proposed
framework produces cluster chains for each event to show the evolving variation
of the event through time. We conducted extensive experiments to evaluate our
framework, validating its high performance and effectiveness in detecting and
distinguishing unspecified social events.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03549">Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songhua Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hanjie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Senbin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guangyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongfei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yuxiang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1">Hongying Zan</a></p>
<p>Recent advances in Large Language Models (LLMs) have achieved remarkable
breakthroughs in understanding and responding to user intents. However, their
performance lag behind general use cases in some expertise domains, such as
Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs
rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue
data. These models lack the ability for doctor-like proactive inquiry and
multi-turn comprehension and cannot align responses with experts' intentions.
In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM
that implements an entire training pipeline from continuous pre-training, SFT,
to Reinforcement Learning from Human Feedback (RLHF). Additionally, we
construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic
doctor-patient dialogues, CMtMedQA, which significantly enhances the model's
capability for complex dialogue and proactive inquiry initiation. We also
define a refined annotation rule and evaluation criteria given the unique
characteristics of the biomedical domain. Extensive experimental results show
that Zhongjing outperforms baselines in various capacities and matches the
performance of ChatGPT in some abilities, despite the 100x parameters. Ablation
studies also demonstrate the contributions of each component: pre-training
enhances medical knowledge, and RLHF further improves instruction-following
ability and safety. Our code, datasets, and models are available at
https://github.com/SupritYoung/Zhongjing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10253">StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data. (arXiv:2308.10253v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1">Bin Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a></p>
<p>The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have
sparked significant interest in the development of multimodal Large Language
Models (LLMs). A primary research objective of such models is to align visual
and textual modalities effectively while comprehending human instructions.
Current methodologies often rely on annotations derived from benchmark datasets
to construct image-dialogue datasets for training purposes, akin to instruction
tuning in LLMs. However, these datasets often exhibit domain bias, potentially
constraining the generative capabilities of the models. In an effort to
mitigate these limitations, we propose a novel data collection methodology that
synchronously synthesizes images and dialogues for visual instruction tuning.
This approach harnesses the power of generative models, marrying the abilities
of ChatGPT and text-to-image generative models to yield a diverse and
controllable dataset with varied image content. Additionally, datasets can be
arbitrarily scaled. This not only provides greater flexibility compared to
existing methodologies but also significantly enhances several model
capabilities. Our research includes comprehensive experiments conducted on
various datasets. The results emphasize substantial enhancements in more than
ten commonly assessed capabilities. Additionally, our model achieves
state-of-the-art results across multiple widely recognized multimodal
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11940">Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhifang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jianguo Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Rui Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1">Long Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouchi_K/0/1/0/all/0/1">Kazushige Ouchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangdong Wang</a></p>
<p>Text-based audio generation models have limitations as they cannot encompass
all the information in audio, leading to restricted controllability when
relying solely on text. To address this issue, we propose a novel model that
enhances the controllability of existing pre-trained text-to-audio models by
incorporating additional conditions including content (timestamp) and style
(pitch contour and energy contour) as supplements to the text. This approach
achieves fine-grained control over the temporal order, pitch, and energy of
generated audio. To preserve the diversity of generation, we employ a trainable
control condition encoder that is enhanced by a large language model and a
trainable Fusion-Net to encode and fuse the additional conditions while keeping
the weights of the pre-trained text-to-audio model frozen. Due to the lack of
suitable datasets and evaluation metrics, we consolidate existing datasets into
a new dataset comprising the audio and corresponding conditions and use a
series of evaluation metrics to evaluate the controllability performance.
Experimental results demonstrate that our model successfully achieves
fine-grained control to accomplish controllable audio generation. Audio samples
and our dataset are publicly available at
https://conditionaudiogen.github.io/conditionaudiogen/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02772">Hot or Cold? Adaptive Temperature Sampling for Code Generation with Large Language Models. (arXiv:2309.02772v3 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuqi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Ge Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">YunFei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hong Mei</a></p>
<p>Recently, Large Language Models (LLMs) have shown impressive abilities in
code generation. However, existing LLMs' decoding strategies are designed for
Natural Language (NL) generation, overlooking the differences between NL and
programming languages (PL). Due to this oversight, a better decoding strategy
for code generation remains an open question. In this paper, we conduct the
first systematic study to explore a decoding strategy specialized in code
generation. With an analysis of loss distributions of code tokens, we find that
code tokens can be divided into two categories: challenging tokens that are
difficult to predict and confident tokens that can be easily inferred. Among
them, the challenging tokens mainly appear at the beginning of a code block.
Inspired by the above findings, we propose a simple yet effective method:
Adaptive Temperature (AdapT) sampling, which dynamically adjusts the
temperature coefficient when decoding different tokens. We apply a larger
temperature when sampling for challenging tokens, allowing LLMs to explore
diverse choices. We employ a smaller temperature for confident tokens avoiding
the influence of tail randomness noises. We apply AdapT sampling to LLMs with
different sizes and conduct evaluations on two popular datasets. Results show
that AdapT sampling significantly outperforms state-of-the-art decoding
strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07707">CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Heng-Jui Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1">Ning Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mavlyutov_R/0/1/0/all/0/1">Ruslan Mavlyutov</a>, <a href="http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1">Sravya Popuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1">Yu-An Chung</a></p>
<p>Large-scale self-supervised pre-trained speech encoders outperform
conventional approaches in speech recognition and translation tasks. Due to the
high cost of developing these large models, building new encoders for new tasks
and deploying them to on-device applications are infeasible. Prior studies
propose model compression methods to address this issue, but those works focus
on smaller models and less realistic tasks. Thus, we propose Contrastive
Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to
compress pre-trained speech encoders by leveraging masked prediction and
contrastive learning to train student models to copy the behavior of a large
teacher model. CoLLD outperforms prior methods and closes the gap between small
and large models on multilingual speech-to-text translation and recognition
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08140">PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shimizu_R/0/1/0/all/0/1">Reo Shimizu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yamamoto_R/0/1/0/all/0/1">Ryuichi Yamamoto</a>, <a href="http://arxiv.org/find/eess/1/au:+Kawamura_M/0/1/0/all/0/1">Masaya Kawamura</a>, <a href="http://arxiv.org/find/eess/1/au:+Shirahata_Y/0/1/0/all/0/1">Yuma Shirahata</a>, <a href="http://arxiv.org/find/eess/1/au:+Doi_H/0/1/0/all/0/1">Hironori Doi</a>, <a href="http://arxiv.org/find/eess/1/au:+Komatsu_T/0/1/0/all/0/1">Tatsuya Komatsu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tachibana_K/0/1/0/all/0/1">Kentaro Tachibana</a></p>
<p>We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system
that allows control over speaker identity using natural language descriptions.
To control speaker identity within the prompt-based TTS framework, we introduce
the concept of speaker prompt, which describes voice characteristics (e.g.,
gender-neutral, young, old, and muffled) designed to be approximately
independent of speaking style. Since there is no large-scale dataset containing
speaker prompts, we first construct a dataset based on the LibriTTS-R corpus
with manually annotated speaker prompts. We then employ a diffusion-based
acoustic model with mixture density networks to model diverse speaker factors
in the training data. Unlike previous studies that rely on style prompts
describing only a limited aspect of speaker individuality, such as pitch,
speaking speed, and energy, our method utilizes an additional speaker prompt to
effectively learn the mapping from natural language descriptions to the
acoustic features of diverse speakers. Our subjective evaluation results show
that the proposed method can better control speaker characteristics than the
methods without the speaker prompt. Audio samples are available at
https://reppy4620.github.io/demo.promptttspp/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08551">Augmenting conformers with structured state-space sequence models for online speech recognition. (arXiv:2309.08551v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1">Haozhe Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1">Albert Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1">Zhong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1">Krzysztof Choromanski</a>, <a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1">Tara Sainath</a></p>
<p>Online speech recognition, where the model only accesses context to the left,
is an important and challenging use case for ASR systems. In this work, we
investigate augmenting neural encoders for online ASR by incorporating
structured state-space sequence models (S4), a family of models that provide a
parameter-efficient way of accessing arbitrarily long left context. We
performed systematic ablation studies to compare variants of S4 models and
propose two novel approaches that combine them with convolutions. We found that
the most effective design is to stack a small S4 using real-valued recurrent
weights with a local convolution, allowing them to work complementarily. Our
best model achieves WERs of 4.01%/8.53% on test sets from Librispeech,
outperforming Conformers with extensively tuned convolution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08475">Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09772">Rethinking Relation Classification with Graph Meaning Representations. (arXiv:2310.09772v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Li Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Dingyi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Malu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1">Daniel Hershcovich</a></p>
<p>In the field of natural language understanding, the intersection of neural
models and graph meaning representations (GMRs) remains a compelling area of
research. Despite the growing interest, a critical gap persists in
understanding the exact influence of GMRs, particularly concerning relation
extraction tasks. Addressing this, we introduce DAGNN-plus, a simple and
parameter-efficient neural architecture designed to decouple contextual
representation learning from structural information propagation. Coupled with
various sequence encoders and GMRs, this architecture provides a foundation for
systematic experimentation on two English and two Chinese datasets. Our
empirical analysis utilizes four different graph formalisms and nine parsers.
The results yield a nuanced understanding of GMRs, showing improvements in
three out of the four datasets, particularly favoring English over Chinese due
to highly accurate parsers. Interestingly, GMRs appear less effective in
literary-domain datasets compared to general-domain datasets. These findings
lay the groundwork for better-informed design of GMRs and parsers to improve
relation classification, which is expected to tangibly impact the future
trajectory of natural language understanding research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10062">A Comprehensive Evaluation of Tool-Assisted Generation Strategies. (arXiv:2310.10062v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1">Alon Jacovi</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1">Jonathan Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1">Roee Aharoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a></p>
<p>A growing area of research investigates augmenting language models with tools
(e.g., search engines, calculators) to overcome their shortcomings (e.g.,
missing or incorrect knowledge, incorrect logical inferences). Various few-shot
tool-usage strategies have been proposed. However, there is no systematic and
fair comparison across different strategies, or between these strategies and
strong baselines that do not leverage tools. We conduct an extensive empirical
analysis, finding that (1) across various datasets, example difficulty levels,
and models, strong no-tool baselines are competitive to tool-assisted
strategies, implying that effectively using tools with in-context
demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval
tasks, strategies that *refine* incorrect outputs with tools outperform
strategies that retrieve relevant information *ahead of* or *during
generation*; (3) tool-assisted strategies are expensive in the number of tokens
they require to work -- incurring additional costs by orders of magnitude --
which does not translate into significant improvement in performance. Overall,
our findings suggest that few-shot tool integration is still an open challenge,
emphasizing the need for comprehensive evaluations of future strategies to
accurately assess their *benefits* and *costs*.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10477">Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jianhua Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lanqing Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenyong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1">Dit-Yan Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1">Lifeng Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a></p>
<p>The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model's intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14628">Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts. (arXiv:2310.14628v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tengxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qipeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuqing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiangkun Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a></p>
<p>As large language models (LLMs) have shown effectiveness with different
prompting methods, such as Chain of Thought, Program of Thought, we find that
these methods have formed a great complementarity to each other on math
reasoning tasks. In this work, we propose XoT, an integrated problem solving
framework by prompting LLMs with diverse reasoning thoughts. For each question,
XoT always begins with selecting the most suitable method then executes each
method iteratively. Within each iteration, XoT actively checks the validity of
the generated answer and incorporates the feedback from external executors,
allowing it to dynamically switch among different prompting methods. Through
extensive experiments on 10 popular math reasoning datasets, we demonstrate the
effectiveness of our proposed approach and thoroughly analyze the strengths of
each module. Moreover, empirical results suggest that our framework is
orthogonal to recent work that makes improvements on single reasoning methods
and can further generalise to logical reasoning domain. By allowing method
switching, XoT provides a fresh perspective on the collaborative integration of
diverse reasoning thoughts in a unified framework. The code is available at
https://github.com/tengxiaoliu/XoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anthony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM's. These reward models are additionally used at
inference-time to estimate LLM responses' adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16509">StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models. (arXiv:2311.16509v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamauchi_K/0/1/0/all/0/1">Kazuki Yamauchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ijima_Y/0/1/0/all/0/1">Yusuke Ijima</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuki Saito</a></p>
<p>We propose StyleCap, a method to generate natural language descriptions of
speaking styles appearing in speech. Although most of conventional techniques
for para-/non-linguistic information recognition focus on the category
classification or the intensity estimation of pre-defined labels, they cannot
provide the reasoning of the recognition result in an interpretable manner.
StyleCap is a first step towards an end-to-end method for generating
speaking-style prompts from speech, i.e., automatic speaking-style captioning.
StyleCap is trained with paired data of speech and natural language
descriptions. We train neural networks that convert a speech representation
vector into prefix vectors that are fed into a large language model (LLM)-based
text decoder. We explore an appropriate text decoder and speech feature
representation suitable for this new task. The experimental results demonstrate
that our StyleCap leveraging richer LLMs for the text decoder, speech
self-supervised learning (SSL) features, and sentence rephrasing augmentation
improves the accuracy and diversity of generated speaking-style captions.
Samples of speaking-style captions generated by our StyleCap are publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03719">Assessing AI Chatbots Performance in Comprehensive Standardized Test Preparation; A Case Study with GRE. (arXiv:2312.03719v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abu_Haifa_M/0/1/0/all/0/1">Mohammad Abu-Haifa</a>, <a href="http://arxiv.org/find/cs/1/au:+Etawi_B/0/1/0/all/0/1">Bara&#x27;a Etawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkhatatbeh_H/0/1/0/all/0/1">Huthaifa Alkhatatbeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ababneh_A/0/1/0/all/0/1">Ayman Ababneh</a></p>
<p>This research paper presents an analysis of how well three artificial
intelligence chatbots, Bing, ChatGPT, and GPT-4, perform when answering
questions from standardized tests. The Graduate Record Examination (GRE) is
used in this paper as a case study. A total of 137 questions with different
forms of quantitative reasoning and 157 questions with verbal categories were
used to assess their capabilities. This paper presents the performance of each
chatbot across various skills and styles tested in the exam. This paper also
explores the proficiency of these chatbots in addressing image-based questions
and illustrates the uncertainty level of each chatbot. The results show varying
degrees of success across the chatbots, where GPT-4 served as the most
proficient, especially in complex language understanding tasks and image-based
questions. Results highlight the ability of these chatbots to pass the GRE with
a high score, which encourages the use of these chatbots in test preparation.
The results also show how important it is to ensure that, if the test is
administered online, as it was during COVID, the test taker is segregated from
these resources for a fair competition on higher education opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06681">Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1">Nina Rimsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1">Nick Gabrieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1">Julian Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1">Alexander Matt Turner</a></p>
<p>We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors'' by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user's prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA's
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA's mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07492">SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1">Manish Nagireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1">Lamogha Chiazor</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Moninder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1">Ioana Baldini</a></p>
<p>Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. Taking inspiration
from social science research, we start with a documented list of 93 US-centric
stigmas and curate a question-answering (QA) dataset which involves simple
social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,
with a variety of prompt styles, carefully constructed to systematically test
for both social bias and model robustness. We present results for
SocialStigmaQA with two open source generative language models and we find that
the proportion of socially biased output ranges from 45% to 59% across a
variety of decoding strategies and prompting styles. We demonstrate that the
deliberate design of the templates in our benchmark (e.g., adding biasing text
to the prompt or using different verbs that change the answer that indicates
bias) impacts the model tendencies to generate socially biased output.
Additionally, through manual evaluation, we discover problematic patterns in
the generated chain-of-thought output that range from subtle bias to lack of
reasoning.
</p>
<p>Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08078">Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Linlin Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yixuan Yuan</a></p>
<p>To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches' for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08935">Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. (arXiv:2312.08935v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhihong Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">R.X. Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Damai Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Deli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Y.Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1">Zhifang Sui</a></p>
<p>In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09785">RJUA-QA: A Comprehensive QA Dataset for Urology. (arXiv:2312.09785v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shiwei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1">Chenfei Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hongbo Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Lei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoyan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Deng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xianguo Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fangzhou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaowei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiran Huang</a></p>
<p>We introduce RJUA-QA, a novel medical dataset for question answering (QA) and
reasoning with clinical evidence, contributing to bridge the gap between
general large language models (LLMs) and medical-specific LLM applications.
RJUA-QA is derived from realistic clinical scenarios and aims to facilitate
LLMs in generating reliable diagnostic and advice. The dataset contains 2,132
curated Question-Context-Answer pairs, corresponding about 25,000 diagnostic
records and clinical cases. The dataset covers 67 common urological disease
categories, where the disease coverage exceeds 97.6\% of the population seeking
medical services in urology. Each data instance in RJUA-QA comprises: (1) a
question mirroring real patient to inquiry about clinical symptoms and medical
conditions, (2) a context including comprehensive expert knowledge, serving as
a reference for medical examination and diagnosis, (3) a doctor response
offering the diagnostic conclusion and suggested examination guidance, (4) a
diagnosed clinical disease as the recommended diagnostic outcome, and (5)
clinical advice providing recommendations for medical examination. RJUA-QA is
the first medical QA dataset for clinical reasoning over the patient inquiries,
where expert-level knowledge and experience are required for yielding
diagnostic conclusions and medical examination advice. A comprehensive
evaluation is conducted to evaluate the performance of both medical-specific
and general LLMs on the RJUA-QA dataset. Our data is are publicly available at
\url{https://github.com/alipay/RJU_Ant_QA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10967">Knowledge Graphs and Pre-trained Language Models enhanced Representation Learning for Conversational Recommender Systems. (arXiv:2312.10967v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zhangchi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1">Ye Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liew_A/0/1/0/all/0/1">Alan Wee-Chung Liew</a></p>
<p>Conversational recommender systems (CRS) utilize natural language
interactions and dialogue history to infer user preferences and provide
accurate recommendations. Due to the limited conversation context and
background knowledge, existing CRSs rely on external sources such as knowledge
graphs to enrich the context and model entities based on their inter-relations.
However, these methods ignore the rich intrinsic information within entities.
To address this, we introduce the Knowledge-Enhanced Entity Representation
Learning (KERL) framework, which leverages both the knowledge graph and a
pre-trained language model to improve the semantic understanding of entities
for CRS. In our KERL framework, entity textual descriptions are encoded via a
pre-trained language model, while a knowledge graph helps reinforce the
representation of these entities. We also employ positional encoding to
effectively capture the temporal information of entities in a conversation. The
enhanced entity representation is then used to develop a recommender component
that fuses both entity and contextual representations for more informed
recommendations, as well as a dialogue component that generates informative
entity-related information in the response text. A high-quality knowledge graph
with aligned entity descriptions is constructed to facilitate our study, namely
the Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that
KERL achieves state-of-the-art results in both recommendation and response
generation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11193">&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yijiong Yu</a></p>
<p>Most open-source LLMs still have a context window of no more than 4k,
limiting their ability to handle long-context problems. Meanwhile, even those
with a long context window still lack satisfactory accuracy. To address this
issue, we explore from the perspective of training data and theoretically prove
training the capability to handle long contexts requires "effective" rather
than "long" data. Based on this, we propose using the "original text
paraphrase" task, and successfully extend the context window of the existing
model to 32k by a low-cost and effective method, achieving the SOTA accuracy in
multi-document-QA among models of the same scale. The model and training data
have been open-sourced on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11508">Rethinking the Instruction Quality: LIFT is What You Need. (arXiv:2312.11508v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yongqiang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yufan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1">Mengnan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Maoquan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Bin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1">Neel Sundaresan</a></p>
<p>Instruction tuning, a specialized technique to enhance large language model
(LLM) performance via instruction datasets, relies heavily on the quality of
employed data. Existing quality improvement methods alter instruction data
through dataset expansion or curation. However, the expansion method risks data
redundancy, potentially compromising LLM performance, while the curation
approach confines the LLM's potential to the original dataset. Our aim is to
surpass the original data quality without encountering these shortcomings. To
achieve this, we propose LIFT (LLM Instruction Fusion Transfer), a novel and
versatile paradigm designed to elevate the instruction quality to new heights.
LIFT strategically broadens data distribution to encompass more high-quality
subspaces and eliminates redundancy, concentrating on high-quality segments
across overall data subspaces. Experimental results demonstrate that, even with
a limited quantity of high-quality instruction data selected by our paradigm,
LLMs not only consistently uphold robust performance across various tasks but
also surpass some state-of-the-art results, highlighting the significant
improvement in instruction quality achieved by our paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12343">LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction. (arXiv:2312.12343v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yucheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1">Frank Guerin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Data contamination in evaluation is getting increasingly prevalent with the
emergence of language models pre-trained on super large, automatically crawled
corpora. This problem leads to significant challenges in the accurate
assessment of model capabilities and generalisations. In this paper, we propose
LatestEval, an automatic method that leverages the most recent texts to create
uncontaminated reading comprehension evaluations. LatestEval avoids data
contamination by only using texts published within a recent time window,
ensuring no overlap with the training corpora of pre-trained language models.
We develop the LatestEval automated pipeline to 1) gather the latest texts; 2)
identify key information, and 3) construct questions targeting the information
while removing the existing answers from the context. This encourages models to
infer the answers themselves based on the remaining context, rather than just
copy-paste. Our experiments demonstrate that language models exhibit negligible
memorisation behaviours on LatestEval as opposed to previous benchmarks,
suggesting a significantly reduced risk of data contamination and leading to a
more robust evaluation. Data and code are publicly available at:
https://github.com/liyucheng09/LatestEval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14183">On Early Detection of Hallucinations in Factual Question Answering. (arXiv:2312.14183v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Snyder_B/0/1/0/all/0/1">Ben Snyder</a>, <a href="http://arxiv.org/find/cs/1/au:+Moisescu_M/0/1/0/all/0/1">Marius Moisescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1">Muhammad Bilal Zafar</a></p>
<p>While large language models (LLMs) have taken great strides towards helping
humans with a plethora of tasks like search and summarization, hallucinations
remain a major impediment towards gaining user trust. The fluency and coherence
of model generations even when hallucinating makes it difficult to detect
whether or not a model is hallucinating. In this work, we explore if the
artifacts associated with the model generations can provide hints that the
generation will contain hallucinations. Specifically, we probe LLMs at 1) the
inputs via Integrated Gradients based token attribution, 2) the outputs via the
Softmax probabilities, and 3) the internal state via self-attention and
fully-connected layer activations for signs of hallucinations on open-ended
question answering tasks. Our results show that the distributions of these
artifacts differ between hallucinated and non-hallucinated generations.
Building on this insight, we train binary classifiers that use these artifacts
as input features to classify model generations into hallucinations and
non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC.
We further show that tokens preceding a hallucination can predict the
subsequent hallucination before it occurs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15291">Reverse Multi-Choice Dialogue Commonsense Inference with Graph-of-Thought. (arXiv:2312.15291v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Li Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1">Hao Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bobo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1">Lizi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1">Donghong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1">Chong Teng</a></p>
<p>With the proliferation of dialogic data across the Internet, the Dialogue
Commonsense Multi-choice Question Answering (DC-MCQ) task has emerged as a
response to the challenge of comprehending user queries and intentions.
Although prevailing methodologies exhibit effectiveness in addressing
single-choice questions, they encounter difficulties in handling multi-choice
queries due to the heightened intricacy and informational density. In this
paper, inspired by the human cognitive process of progressively excluding
options, we propose a three-step Reverse Exclusion Graph-of-Thought (ReX-GoT)
framework, including Option Exclusion, Error Analysis, and Combine Information.
Specifically, our ReX-GoT mimics human reasoning by gradually excluding
irrelevant options and learning the reasons for option errors to choose the
optimal path of the GoT and ultimately infer the correct answer. By
progressively integrating intricate clues, our method effectively reduces the
difficulty of multi-choice reasoning and provides a novel solution for DC-MCQ.
Extensive experiments on the CICERO and CICERO$_{v2}$ datasets validate the
significant improvement of our approach on DC-MCQ task. On zero-shot setting,
our model outperform the best baseline by 17.67% in terms of F1 score for the
multi-choice task. Most strikingly, our GPT3.5-based ReX-GoT framework achieves
a remarkable 39.44% increase in F1 score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16148">The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias. (arXiv:2312.16148v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1">Timo Spinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinterreiter_S/0/1/0/all/0/1">Smilla Hinterreiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Haak_F/0/1/0/all/0/1">Fabian Haak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1">Terry Ruas</a>, <a href="http://arxiv.org/find/cs/1/au:+Giese_H/0/1/0/all/0/1">Helge Giese</a>, <a href="http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1">Norman Meuschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1">Bela Gipp</a></p>
<p>The way the media presents events can significantly affect public perception,
which in turn can alter people's beliefs and views. Media bias describes a
one-sided or polarizing perspective on a topic. This article summarizes the
research on computational methods to detect media bias by systematically
reviewing 3140 research papers published between 2019 and 2022. To structure
our review and support a mutual understanding of bias across research domains,
we introduce the Media Bias Taxonomy, which provides a coherent overview of the
current state of research on media bias from different perspectives. We show
that media bias detection is a highly active research field, in which
transformer-based classification approaches have led to significant
improvements in recent years. These improvements include higher classification
accuracy and the ability to detect more fine-granular types of bias. However,
we have identified a lack of interdisciplinarity in existing projects, and a
need for more awareness of the various types of media bias to support
methodologically thorough performance evaluations of media bias detection
systems. Concluding from our analysis, we see the integration of recent machine
learning advancements with reliable and diverse bias assessment strategies from
other research areas as the most promising area for future research
contributions in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14654">Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation. (arXiv:2308.14654v1 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_N/0/1/0/all/0/1">Nguyen Anh Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Uyen_H/0/1/0/all/0/1">Hoang Thi Thu Uyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Phuong_T/0/1/0/all/0/1">Tu Minh Phuong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1">Ngo Xuan Bach</a></p>
<p>Multiple intent detection and slot filling are two fundamental and crucial
tasks in spoken language understanding. Motivated by the fact that the two
tasks are closely related, joint models that can detect intents and extract
slots simultaneously are preferred to individual models that perform each task
independently. The accuracy of a joint model depends heavily on the ability of
the model to transfer information between the two tasks so that the result of
one task can correct the result of the other. In addition, since a joint model
has multiple outputs, how to train the model effectively is also challenging.
In this paper, we present a method for multiple intent detection and slot
filling by addressing these challenges. First, we propose a bidirectional joint
model that explicitly employs intent information to recognize slots and slot
features to detect intents. Second, we introduce a novel method for training
the proposed joint model using supervised contrastive learning and
self-distillation. Experimental results on two benchmark datasets MixATIS and
MixSNIPS show that our method outperforms state-of-the-art models in both
tasks. The results also demonstrate the contributions of both bidirectional
design and the training method to the accuracy improvement. Our source code is
available at https://github.com/anhtunguyen98/BiSLU
</p>
</p>
</div>

    </div>
    </body>
    