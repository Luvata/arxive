<!DOCTYPE html>
<html>
<head>
<title>2023-12-01-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.17072">IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers. (arXiv:2311.17072v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chenglin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1">Siyuan Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1">Tao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiahui Yu</a></p>
<p>Generative training has been demonstrated to be powerful for building
visual-language models. However, on zero-shot discriminative benchmarks, there
is still a performance gap between models trained with generative and
discriminative objectives. In this paper, we aim to narrow this gap by
improving the efficacy of generative training on classification tasks, without
any finetuning processes or additional modules.
</p>
<p>Specifically, we focus on narrowing the gap between the generative captioner
and the CLIP classifier. We begin by analysing the predictions made by the
captioner and classifier and observe that the caption generation inherits the
distribution bias from the language model trained with pure text modality,
making it less grounded on the visual signal. To tackle this problem, we
redesign the scoring objective for the captioner to alleviate the
distributional bias and focus on measuring the gain of information brought by
the visual inputs. We further design a generative training objective to match
the evaluation objective. We name our model trained and evaluated from the
novel procedures as Information Gain (IG) captioner. We pretrain the models on
the public Laion-5B dataset and perform a series of discriminative evaluations.
For the zero-shot classification on ImageNet, IG captioner achieves $&gt; 18\%$
improvements over the standard captioner, achieving comparable performances
with the CLIP classifier. IG captioner also demonstrated strong performance on
zero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this
paper inspires further research towards unifying generative and discriminative
training procedures for visual-language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17074">Self-Supervised Learning of Whole and Component-Based Semantic Representations for Person Re-Identification. (arXiv:2311.17074v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yifan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kathirvel_R/0/1/0/all/0/1">Ram Prabhakar Kathirvel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1">Rama Chellappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1">Chun Pong Lau</a></p>
<p>Interactive Segmentation Models (ISMs) like the Segment Anything Model have
significantly improved various computer vision tasks, yet their application to
Person Re-identification (ReID) remains limited. On the other hand, existing
semantic pre-training models for ReID often have limitations like predefined
parsing ranges or coarse semantics. Additionally, ReID and Clothes-Changing
ReID (CC-ReID) are usually treated separately due to their different domains.
This paper investigates whether utilizing precise human-centric semantic
representation can boost the ReID performance and improve the generalization
among various ReID tasks. We propose SemReID, a self-supervised ReID model that
leverages ISMs for adaptive part-based semantic extraction, contributing to the
improvement of ReID performance. SemReID additionally refines its semantic
representation through techniques such as image masking and KoLeo
regularization. Evaluation across three types of ReID datasets -- standard
ReID, CC-ReID, and unconstrained ReID -- demonstrates superior performance
compared to state-of-the-art methods. In addition, recognizing the scarcity of
large person datasets with fine-grained semantics, we introduce the novel
LUPerson-Part dataset to assist ReID methods in acquiring the fine-grained part
semantics for robust performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17076">Compositional Chain-of-Thought Prompting for Large Multimodal Models. (arXiv:2311.17076v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1">Chancharik Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Brandon Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1">Roei Herzig</a></p>
<p>The combination of strong visual backbones and Large Language Model (LLM)
reasoning has led to Large Multimodal Models (LMMs) becoming the current
standard for a wide range of vision and language (VL) tasks. However, recent
research has shown that even the most advanced LMMs still struggle to capture
aspects of compositional visual reasoning, such as attributes and relationships
between objects. One solution is to utilize scene graphs (SGs)--a formalization
of objects and their relations and attributes that has been extensively used as
a bridge between the visual and textual domains. Yet, scene graph data requires
scene graph annotations, which are expensive to collect and thus not easily
scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic
forgetting of the pretraining objective. To overcome this, inspired by
chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a
novel zero-shot Chain-of-Thought prompting method that utilizes SG
representations in order to extract compositional knowledge from an LMM.
Specifically, we first generate an SG using the LMM, and then use that SG in
the prompt to produce a response. Through extensive experiments, we find that
the proposed CCoT approach not only improves LMM performance on several vision
and language VL compositional benchmarks but also improves the performance of
several popular LMMs on general multimodal benchmarks, without the need for
fine-tuning or annotated ground-truth SGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17080">Combating the &quot;Sameness&quot; in AI Art: Reflections on the Interactive AI Installation Fencing Hallucination. (arXiv:2311.17080v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weihao Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Legrady_G/0/1/0/all/0/1">George Legrady</a></p>
<p>The article summarizes three types of "sameness" issues in Artificial
Intelligence(AI) art, each occurring at different stages of development in AI
image creation tools. Through the Fencing Hallucination project, the article
reflects on the design of AI art production in alleviating the sense of
uniformity, maintaining the uniqueness of images from an AI image synthesizer,
and enhancing the connection between the artworks and the audience. This paper
endeavors to stimulate the creation of distinctive AI art by recounting the
efforts and insights derived from the Fencing Hallucination project, all
dedicated to addressing the issue of "sameness".
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17081">I-MedSAM: Implicit Medical Image Segmentation with Segment Anything. (arXiv:2311.17081v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaobao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiajun Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yizhu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Ming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a></p>
<p>With the development of Deep Neural Networks (DNNs), many efforts have been
made to handle medical image segmentation. Traditional methods such as nnUNet
train specific segmentation models on the individual datasets. Plenty of recent
methods have been proposed to adapt the foundational Segment Anything Model
(SAM) to medical image segmentation. However, they still focus on discrete
representations to generate pixel-wise predictions, which are spatially
inflexible and scale poorly to higher resolution. In contrast, implicit methods
learn continuous representations for segmentation, which is crucial for medical
image segmentation. In this paper, we propose I-MedSAM, which leverages the
benefits of both continuous representations and SAM, to obtain better
cross-domain ability and accurate boundary delineation. Since medical image
segmentation needs to predict detailed segmentation boundaries, we designed a
novel adapter to enhance the SAM features with high-frequency information
during Parameter Efficient Fine Tuning (PEFT). To convert the SAM features and
coordinates into continuous segmentation output, we utilize Implicit Neural
Representation (INR) to learn an implicit segmentation decoder. We also propose
an uncertainty-guided sampling strategy for efficient learning of INR.
Extensive evaluations on 2D medical image segmentation tasks have shown that
our proposed method with only 1.6M trainable parameters outperforms existing
methods including discrete and continuous methods. The code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17082">DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling. (arXiv:2311.17082v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Linqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shih_A/0/1/0/all/0/1">Andy Shih</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a></p>
<p>Recent methods such as Score Distillation Sampling (SDS) and Variational
Score Distillation (VSD) using 2D diffusion models for text-to-3D generation
have demonstrated impressive generation quality. However, the long generation
time of such algorithms significantly degrades the user experience. To tackle
this problem, we propose DreamPropeller, a drop-in acceleration algorithm that
can be wrapped around any existing text-to-3D generation pipeline based on
score distillation. Our framework generalizes Picard iterations, a classical
algorithm for parallel sampling an ODE path, and can account for non-ODE paths
such as momentum-based gradient updates and changes in dimensions during the
optimization process as in many cases of 3D generation. We show that our
algorithm trades parallel compute for wallclock time and empirically achieves
up to 4.7x speedup with a negligible drop in generation quality for all tested
frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17083">CLiC: Concept Learning in Context. (arXiv:2311.17083v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Safaee_M/0/1/0/all/0/1">Mehdi Safaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikaeili_A/0/1/0/all/0/1">Aryan Mikaeili</a>, <a href="http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1">Or Patashnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1">Ali Mahdavi-Amiri</a></p>
<p>This paper addresses the challenge of learning a local visual pattern of an
object from one image, and generating images depicting objects with that
pattern. Learning a localized concept and placing it on an object in a target
image is a nontrivial task, as the objects may have different orientations and
shapes. Our approach builds upon recent advancements in visual concept
learning. It involves acquiring a visual concept (e.g., an ornament) from a
source image and subsequently applying it to an object (e.g., a chair) in a
target image. Our key idea is to perform in-context concept learning, acquiring
the local visual concept within the broader context of the objects they belong
to. To localize the concept learning, we employ soft masks that contain both
the concept within the mask and the surrounding image area. We demonstrate our
approach through object generation within an image, showcasing plausible
embedding of in-context learned concepts. We also introduce methods for
directing acquired concepts to specific locations within target images,
employing cross-attention mechanisms, and establishing correspondences between
source and target objects. The effectiveness of our method is demonstrated
through quantitative and qualitative experiments, along with comparisons
against baseline techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17084">DepthSSC: Depth-Spatial Alignment and Dynamic Voxel Resolution for Monocular 3D Semantic Scene Completion. (arXiv:2311.17084v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiawei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jusheng Zhang</a></p>
<p>The task of 3D semantic scene completion with monocular cameras is gaining
increasing attention in the field of autonomous driving. Its objective is to
predict the occupancy status of each voxel in the 3D scene from partial image
inputs. Despite the existence of numerous methods, many of them overlook the
issue of accurate alignment between spatial and depth information. To address
this, we propose DepthSSC, an advanced method for semantic scene completion
solely based on monocular cameras. DepthSSC combines the ST-GF (Spatial
Transformation Graph Fusion) module with geometric-aware voxelization, enabling
dynamic adjustment of voxel resolution and considering the geometric complexity
of 3D space to ensure precise alignment between spatial and depth information.
This approach successfully mitigates spatial misalignment and distortion issues
observed in prior methods. Through evaluation on the SemanticKITTI dataset,
DepthSSC not only demonstrates its effectiveness in capturing intricate 3D
structural details but also achieves state-of-the-art performance. We believe
DepthSSC provides a fresh perspective on monocular camera-based 3D semantic
scene completion research and anticipate it will inspire further related
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17085">Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking. (arXiv:2311.17085v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiawei Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangmei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiuxin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xuelin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a></p>
<p>Single object tracking aims to locate one specific target in video sequences,
given its initial state. Classical trackers rely solely on visual cues,
restricting their ability to handle challenges such as appearance variations,
ambiguity, and distractions. Hence, Vision-Language (VL) tracking has emerged
as a promising approach, incorporating language descriptions to directly
provide high-level semantics and enhance tracking performance. However, current
VL trackers have not fully exploited the power of VL learning, as they suffer
from limitations such as heavily relying on off-the-shelf backbones for feature
extraction, ineffective VL fusion designs, and the absence of VL-related loss
functions. Consequently, we present a novel tracker that progressively explores
target-centric semantics for VL tracking. Specifically, we propose the first
Synchronous Learning Backbone (SLB) for VL tracking, which consists of two
novel modules: the Target Enhance Module (TEM) and the Semantic Aware Module
(SAM). These modules enable the tracker to perceive target-related semantics
and comprehend the context of both visual and textual modalities at the same
pace, facilitating VL feature extraction and fusion at different semantic
levels. Moreover, we devise the dense matching loss to further strengthen
multi-modal representation learning. Extensive experiments on VL tracking
datasets demonstrate the superiority and effectiveness of our methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17086">PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation. (arXiv:2311.17086v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qingsong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Haonan Lu</a></p>
<p>Text-to-image diffusion models are well-known for their ability to generate
realistic images based on textual prompts. However, the existing works have
predominantly focused on English, lacking support for non-English text-to-image
models. The most commonly used translation methods cannot solve the generation
problem related to language culture, while training from scratch on a specific
language dataset is prohibitively expensive. In this paper, we are inspired to
propose a simple plug-and-play language transfer method based on knowledge
distillation. All we need to do is train a lightweight MLP-like
parameter-efficient adapter (PEA) with only 6M parameters under teacher
knowledge distillation along with a small parallel data corpus. We are
surprised to find that freezing the parameters of UNet can still achieve
remarkable performance on the language-specific prompt evaluation set,
demonstrating that PEA can stimulate the potential generation ability of the
original UNet. Additionally, it closely approaches the performance of the
English text-to-image model on a general prompt evaluation set. Furthermore,
our adapter can be used as a plugin to achieve significant results in
downstream tasks in cross-lingual text-to-image generation. Code will be
available at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17087">Rethinking Mixup for Improving the Adversarial Transferability. (arXiv:2311.17087v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaosen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zeyuan Yin</a></p>
<p>Mixup augmentation has been widely integrated to generate adversarial
examples with superior adversarial transferability when immigrating from a
surrogate model to other models. However, the underlying mechanism influencing
the mixup's effect on transferability remains unexplored. In this work, we
posit that the adversarial examples located at the convergence of decision
boundaries across various categories exhibit better transferability and
identify that Admix tends to steer the adversarial examples towards such
regions. However, we find the constraint on the added image in Admix decays its
capability, resulting in limited transferability. To address such an issue, we
propose a new input transformation-based attack called Mixing the Image but
Separating the gradienT (MIST). Specifically, MIST randomly mixes the input
image with a randomly shifted image and separates the gradient of each loss
item for each mixed image. To counteract the imprecise gradient, MIST
calculates the gradient on several mixed images for each input sample.
Extensive experimental results on the ImageNet dataset demonstrate that MIST
outperforms existing SOTA input transformation-based attacks with a clear
margin on both Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) w/wo defense mechanisms, supporting MIST's high effectiveness and
generality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17088">Unsupervised Multimodal Deepfake Detection Using Intra- and Cross-Modal Inconsistencies. (arXiv:2311.17088v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1">Mulin Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1">Mahyar Khayatkhoei</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathai_J/0/1/0/all/0/1">Joe Mathai</a>, <a href="http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1">Wael AbdAlmageed</a></p>
<p>Deepfake videos present an increasing threat to society with potentially
negative impact on criminal justice, democracy, and personal safety and
privacy. Meanwhile, detecting deepfakes, at scale, remains a very challenging
tasks that often requires labeled training data from existing deepfake
generation methods. Further, even the most accurate supervised learning,
deepfake detection methods do not generalize to deepfakes generated using new
generation methods. In this paper, we introduce a novel unsupervised approach
for detecting deepfake videos by measuring of intra- and cross-modal
consistency among multimodal features; specifically visual, audio, and identity
features. The fundamental hypothesis behind the proposed detection method is
that since deepfake generation attempts to transfer the facial motion of one
identity to another, these methods will eventually encounter a trade-off
between motion and identity that enviably leads to detectable inconsistencies.
We validate our method through extensive experimentation, demonstrating the
existence of significant intra- and cross- modal inconsistencies in deepfake
videos, which can be effectively utilized to detect them with high accuracy.
Our proposed method is scalable because it does not require pristine samples at
inference, generalizable because it is trained only on real data, and is
explainable since it can pinpoint the exact location of modality
inconsistencies which are then verifiable by a human expert.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17089">Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering. (arXiv:2311.17089v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhiwen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Low_W/0/1/0/all/0/1">Weng Fei Low</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gim Hee Lee</a></p>
<p>3D Gaussians have recently emerged as a highly efficient representation for
3D reconstruction and rendering. Despite its high rendering quality and speed
at high resolutions, they both deteriorate drastically when rendered at lower
resolutions or from far away camera position. During low resolution or far away
rendering, the pixel size of the image can fall below the Nyquist frequency
compared to the screen size of each splatted 3D Gaussian and leads to aliasing
effect. The rendering is also drastically slowed down by the sequential alpha
blending of more splatted Gaussians per pixel. To address these issues, we
propose a multi-scale 3D Gaussian splatting algorithm, which maintains
Gaussians at different scales to represent the same scene. Higher-resolution
images are rendered with more small Gaussians, and lower-resolution images are
rendered with fewer larger Gaussians. With similar training time, our algorithm
can achieve 13\%-66\% PSNR and 160\%-2400\% rendering speed improvement at
4$\times$-128$\times$ scale rendering on Mip-NeRF360 dataset compared to the
single scale 3D Gaussian splatting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17091">Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models. (arXiv:2311.17091v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhihe Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jiawang Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zeyu Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>Fine-tuning pre-trained vision-language models (VLMs), e.g., CLIP, for the
open-world generalization has gained increasing popularity due to its practical
value. However, performance advancements are limited when relying solely on
intricate algorithmic designs for a single model, even one exhibiting strong
performance, e.g., CLIP-ViT-B/16. This paper, for the first time, explores the
collaborative potential of leveraging much weaker VLMs to enhance the
generalization of a robust single model. The affirmative findings motivate us
to address the generalization problem from a novel perspective, i.e., ensemble
of pre-trained VLMs. We introduce three customized ensemble strategies, each
tailored to one specific scenario. Firstly, we introduce the zero-shot
ensemble, automatically adjusting the logits of different models based on their
confidence when only pre-trained VLMs are available. Furthermore, for scenarios
with extra few-shot samples, we propose the training-free and tuning ensemble,
offering flexibility based on the availability of computing resources. The
proposed ensemble strategies are evaluated on zero-shot, base-to-new, and
cross-dataset generalization, achieving new state-of-the-art performance.
Notably, this work represents an initial stride toward enhancing the
generalization performance of VLMs via ensemble. The code is available at
https://github.com/zhiheLu/Ensemble_VLM.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17092">SEED-Bench-2: Benchmarking Multimodal Large Language Models. (arXiv:2311.17092v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yuying Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yixiao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruimao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Multimodal large language models (MLLMs), building upon the foundation of
powerful large language models (LLMs), have recently demonstrated exceptional
capabilities in generating not only texts but also images given interleaved
multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However,
existing MLLM benchmarks remain limited to assessing only models' comprehension
ability of single image-text inputs, failing to keep up with the strides made
in MLLMs. A comprehensive benchmark is imperative for investigating the
progress and uncovering the limitations of current MLLMs. In this work, we
categorize the capabilities of MLLMs into hierarchical levels from $L_0$ to
$L_4$ based on the modalities they can accept and generate, and propose
SEED-Bench-2, a comprehensive benchmark that evaluates the
\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2
comprises 24K multiple-choice questions with accurate human annotations, which
spans 27 dimensions, including the evaluation of both text and image
generation. Multiple-choice questions with groundtruth options derived from
human annotation enables an objective and efficient assessment of model
performance, eliminating the need for human or GPT intervention during
evaluation. We further evaluate the performance of 23 prominent open-source
MLLMs and summarize valuable observations. By revealing the limitations of
existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to
provide insights that will motivate future research towards the goal of General
Artificial Intelligence. Dataset and evaluation code are available at
\href{https://github.com/AILab-CVC/SEED-Bench}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17093">Improved Prototypical Semi-Supervised Learning with Foundation Models: Prototype Selection, Parametric vMF-SNE Pretraining and Multi-view Pseudolabelling. (arXiv:2311.17093v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mannix_E/0/1/0/all/0/1">Evelyn Mannix</a>, <a href="http://arxiv.org/find/cs/1/au:+Bondell_H/0/1/0/all/0/1">Howard Bondell</a></p>
<p>In this paper we present an improved approach to prototypical semi-supervised
learning for computer vision, in the context of leveraging a frozen foundation
model as the backbone of our neural network. As a general tool, we propose
parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to create
mappings with neural networks between high-dimensional latent spaces that
preserve local structure. This enables us to pretrain the projection head of
our network using the high-quality embeddings of the foundation model with
vMF-SNE. We also propose soft multi-view pseudolabels, where predictions across
multiple views are combined to provide a more reliable supervision signal
compared to a consistency or swapped assignment approach. We demonstrate that
these ideas improve upon P}redicting View-Assignments with Support Samples
(PAWS), a current state-of-the-art semi-supervised learning method, as well as
Robust PAWS (RoPAWS), over a range of benchmarking datasets. We also introduce
simple $k$-means prototype selection, a technique that provides superior
performance to other unsupervised label selection approaches in this context.
These changes improve upon PAWS by an average of +2.9% for CIFAR-10 and +5.7%
for CIFAR-100 with four labels per class, and by +15.2% for DeepWeeds, a
particularly challenging dataset for semi-supervised learning. We also achieve
new state-of-the-art results in semi-supervised learning in this small label
regime for CIFAR-10 - 95.8% (+0.7%) and CIFAR-100 - 76.6% (+12.0%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17094">In Search of a Data Transformation That Accelerates Neural Field Training. (arXiv:2311.17094v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Junwon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangyoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kwang In Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaeho Lee</a></p>
<p>Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17095">Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models. (arXiv:2311.17095v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiayun_L/0/1/0/all/0/1">Luo Jiayun</a>, <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_S/0/1/0/all/0/1">Siddhesh Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1">Leonid Sigal</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a></p>
<p>From an enormous amount of image-text pairs, large-scale vision-language
models (VLMs) learn to implicitly associate image regions with words, which is
vital for tasks such as image captioning and visual question answering.
However, leveraging such pre-trained models for open-vocabulary semantic
segmentation remains a challenge. In this paper, we propose a simple, yet
extremely effective, training-free technique, Plug-and-Play Open-Vocabulary
Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with
direct text-to-image cross-attention and an image-text matching loss to produce
semantic segmentation. However, cross-attention alone tends to over-segment,
whereas cross-attention plus GradCAM tend to under-segment. To alleviate this
issue, we introduce Salience Dropout; by iteratively dropping patches that the
model is most attentive to, we are able to better resolve the entire extent of
the segmentation mask. Compared to existing techniques, the proposed method
does not require any neural network training and performs hyperparameter tuning
without the need for any segmentation annotations, even for a validation set.
PnP-OVSS demonstrates substantial improvements over a comparable baseline
(+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS
COCO, +2.4% mIoU on COCO Stuff) and even outperforms most baselines that
conduct additional network training on top of pretrained VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17096">Robust Transductive Few-shot Learning via Joint Message Passing and Prototype-based Soft-label Propagation. (arXiv:2311.17096v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Few-shot learning (FSL) aims to develop a learning model with the ability to
generalize to new classes using a few support samples. For transductive FSL
tasks, prototype learning and label propagation methods are commonly employed.
Prototype methods generally first learn the representative prototypes from the
support set and then determine the labels of queries based on the metric
between query samples and prototypes. Label propagation methods try to
propagate the labels of support samples on the constructed graph encoding the
relationships between both support and query samples. This paper aims to
integrate these two principles together and develop an efficient and robust
transductive FSL approach, termed Prototype-based Soft-label Propagation
(PSLP). Specifically, we first estimate the soft-label presentation for each
query sample by leveraging prototypes. Then, we conduct soft-label propagation
on our learned query-support graph. Both steps are conducted progressively to
boost their respective performance. Moreover, to learn effective prototypes for
soft-label estimation as well as the desirable query-support graph for
soft-label propagation, we design a new joint message passing scheme to learn
sample presentation and relational graph jointly. Our PSLP method is
parameter-free and can be implemented very efficiently. On four popular
datasets, our method achieves competitive results on both balanced and
imbalanced settings compared to the state-of-the-art methods. The code will be
released upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17098">DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection. (arXiv:2311.17098v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1">Daeun Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hoeseok Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyungshin Kim</a></p>
<p>In object detection, achieving constant accuracy is challenging due to the
variability of object sizes. One possible solution to this problem is to
optimize the input resolution, known as a multi-resolution strategy. Previous
approaches for optimizing resolution are often based on pre-defined resolutions
or a dynamic neural network, but there is a lack of study for run-time
resolution optimization for existing architecture. In this paper, we propose an
adaptive resolution scaling network called DyRA, which comprises convolutions
and transformer encoder blocks, for existing detectors. Our DyRA returns a
scale factor from an input image, which enables instance-specific scaling. This
network is jointly trained with detectors with specially designed loss
functions, namely ParetoScaleLoss and BalanceLoss. The ParetoScaleLoss produces
an adaptive scale factor from the image, while the BalanceLoss optimizes the
scale factor according to localization power for the dataset. The loss function
is designed to minimize accuracy drop about the contrasting objective of small
and large objects. Our experiments on COCO, RetinaNet, Faster-RCNN, FCOS, and
Mask-RCNN achieved 1.3%, 1.1%, 1.3%, and 0.8% accuracy improvement than a
multi-resolution baseline with solely resolution adjustment. The code is
available at https://github.com/DaEunFullGrace/DyRA.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17099">StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences. (arXiv:2311.17099v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shangkun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Thomas H. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huaxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guoqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wei Gao</a></p>
<p>Occlusions between consecutive frames have long posed a significant challenge
in optical flow estimation. The inherent ambiguity introduced by occlusions
directly violates the brightness constancy constraint and considerably hinders
pixel-to-pixel matching. To address this issue, multi-frame optical flow
methods leverage adjacent frames to mitigate the local ambiguity. Nevertheless,
prior multi-frame methods predominantly adopt recursive flow estimation,
resulting in a considerable computational overlap. In contrast, we propose a
streamlined in-batch framework that eliminates the need for extensive redundant
recursive computations while concurrently developing effective spatio-temporal
modeling approaches under in-batch estimation constraints. Specifically, we
present a Streamlined In-batch Multi-frame (SIM) pipeline tailored to video
input, attaining a similar level of time efficiency to two-frame networks.
Furthermore, we introduce an efficient Integrative Spatio-temporal Coherence
(ISC) modeling method for effective spatio-temporal modeling during the
encoding phase, which introduces no additional parameter overhead.
Additionally, we devise a Global Temporal Regressor (GTR) that effectively
explores temporal relations during decoding. Benefiting from the efficient SIM
pipeline and effective modules, StreamFlow not only excels in terms of
performance on the challenging KITTI and Sintel datasets, with particular
improvement in occluded areas but also attains a remarkable $63.82\%$
enhancement in speed compared with previous multi-frame methods. The code will
be available soon at https://github.com/littlespray/StreamFlow.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17101">Robust Diffusion GAN using Semi-Unbalanced Optimal Transport. (arXiv:2311.17101v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dao_Q/0/1/0/all/0/1">Quan Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ta_B/0/1/0/all/0/1">Binh Ta</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Tung Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1">Anh Tran</a></p>
<p>Diffusion models, a type of generative model, have demonstrated great
potential for synthesizing highly detailed images. By integrating with GAN,
advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach
real-time performance for expansive practical applications. While DDGAN has
effectively addressed the challenges of generative modeling, namely producing
high-quality samples, covering different data modes, and achieving faster
sampling, it remains susceptible to performance drops caused by datasets that
are corrupted with outlier samples. This work introduces a robust training
technique based on semi-unbalanced optimal transport to mitigate the impact of
outliers effectively. Through comprehensive evaluations, we demonstrate that
our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the
aforementioned generative modeling criteria, i.e., image quality, mode coverage
of distribution, and inference speed, and exhibits improved robustness when
dealing with both clean and corrupted datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17105">On the Calibration of Human Pose Estimation. (arXiv:2311.17105v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1">Kerui Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Rongyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Angela Yao</a></p>
<p>Most 2D human pose estimation frameworks estimate keypoint confidence in an
ad-hoc manner, using heuristics such as the maximum value of heatmaps. The
confidence is part of the evaluation scheme, e.g., AP for the MSCOCO dataset,
yet has been largely overlooked in the development of state-of-the-art methods.
This paper takes the first steps in addressing miscalibration in pose
estimation. From a calibration point of view, the confidence should be aligned
with the pose accuracy. In practice, existing methods are poorly calibrated. We
show, through theoretical analysis, why a miscalibration gap exists and how to
narrow the gap. Simply predicting the instance size and adjusting the
confidence function gives considerable AP improvements. Given the black-box
nature of deep neural networks, however, it is not possible to fully close this
gap with only closed-form adjustments. As such, we go one step further and
learn network-specific adjustments by enforcing consistency between confidence
and pose accuracy. Our proposed Calibrated ConfidenceNet (CCNet) is a
light-weight post-hoc addition that improves AP by up to 1.4% on off-the-shelf
pose estimation frameworks. Applied to the downstream task of mesh recovery,
CCNet facilitates an additional 1.0mm decrease in 3D keypoint error.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17109">Neural Texture Puppeteer: A Framework for Neural Geometry and Texture Rendering of Articulated Shapes, Enabling Re-Identification at Interactive Speed. (arXiv:2311.17109v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Waldmann_U/0/1/0/all/0/1">Urs Waldmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Johannsen_O/0/1/0/all/0/1">Ole Johannsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldluecke_B/0/1/0/all/0/1">Bastian Goldluecke</a></p>
<p>In this paper, we present a neural rendering pipeline for textured
articulated shapes that we call Neural Texture Puppeteer. Our method separates
geometry and texture encoding. The geometry pipeline learns to capture spatial
relationships on the surface of the articulated shape from ground truth data
that provides this geometric information. A texture auto-encoder makes use of
this information to encode textured images into a global latent code. This
global texture embedding can be efficiently trained separately from the
geometry, and used in a downstream task to identify individuals. The neural
texture rendering and the identification of individuals run at interactive
speeds. To the best of our knowledge, we are the first to offer a promising
alternative to CNN- or transformer-based approaches for re-identification of
articulated individuals based on neural rendering. Realistic looking novel view
and pose synthesis for different synthetic cow textures further demonstrate the
quality of our method. Restricted by the availability of ground truth data for
the articulated shape's geometry, the quality for real-world data synthesis is
reduced. We further demonstrate the flexibility of our model for real-world
data by applying a synthetic to real-world texture domain shift where we
reconstruct the texture from a real-world 2D RGB image. Thus, our method can be
applied to endangered species where data is limited. Our novel synthetic
texture dataset NePuMoo is publicly available to inspire further development in
the field of neural rendering-based re-identification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17112">Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model. (arXiv:2311.17112v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zelin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhengqin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhilin Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lingxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a></p>
<p>Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash
the potential of large foundation models in novel scenarios with limited
training data. In the computer vision community, PEFT has shown effectiveness
in image classification, but little research has studied its ability for image
segmentation. Fine-tuning segmentation models usually require a heavier
adjustment of parameters to align the proper projection directions in the
parameter space for new scenarios. This raises a challenge to existing PEFT
algorithms, as they often inject a limited number of individual parameters into
each block, which prevents substantial adjustment of the projection direction
of the parameter space due to the limitation of Hidden Markov Chain along
blocks. In this paper, we equip PEFT with a cross-block orchestration mechanism
to enable the adaptation of the Segment Anything Model (SAM) to various
downstream scenarios. We introduce a novel inter-block communication module,
which integrates a learnable relation matrix to facilitate communication among
different coefficient sets of each PEFT block's parameter space. Moreover, we
propose an intra-block enhancement module, which introduces a linear projection
head whose weights are generated from a hyper-complex layer, further enhancing
the impact of the adjustment of projection directions on the entire parameter
space. Extensive experiments on diverse benchmarks demonstrate that our
proposed approach consistently improves the segmentation performance
significantly on novel scenarios with only around 1K additional parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17113">Human Gaussian Splatting: Real-time Rendering of Animatable Avatars. (arXiv:2311.17113v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moreau_A/0/1/0/all/0/1">Arthur Moreau</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jifei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1">Helisa Dhamo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1">Richard Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1">Eduardo P&#xe9;rez-Pellitero</a></p>
<p>This work addresses the problem of real-time rendering of photorealistic
human body avatars learned from multi-view videos. While the classical
approaches to model and render virtual humans generally use a textured mesh,
recent research has developed neural body representations that achieve
impressive visual quality. However, these models are difficult to render in
real-time and their quality degrades when the character is animated with body
poses different than the training observations. We propose the first animatable
human model based on 3D Gaussian Splatting, that has recently emerged as a very
efficient alternative to neural radiance fields. Our body is represented by a
set of gaussian primitives in a canonical space which are deformed in a coarse
to fine approach that combines forward skinning and local non-rigid refinement.
We describe how to learn our Human Gaussian Splatting (\OURS) model in an
end-to-end fashion from multi-view observations, and evaluate it against the
state-of-the-art approaches for novel pose synthesis of clothed body. Our
method presents a PSNR 1.5dbB better than the state-of-the-art on THuman4
dataset while being able to render at 20fps or more.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17116">REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field. (arXiv:2311.17116v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Wooseok Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fukiage_T/0/1/0/all/0/1">Taiki Fukiage</a>, <a href="http://arxiv.org/find/cs/1/au:+Oishi_T/0/1/0/all/0/1">Takeshi Oishi</a></p>
<p>Recently, significant progress has been made in the study of methods for 3D
reconstruction from multiple images using implicit neural representations,
exemplified by the neural radiance field (NeRF) method. Such methods, which are
based on volume rendering, can model various light phenomena, and various
extended methods have been proposed to accommodate different scenes and
situations. However, when handling scenes with multiple glass objects, e.g.,
objects in a glass showcase, modeling the target scene accurately has been
challenging due to the presence of multiple reflection and refraction effects.
Thus, this paper proposes a NeRF-based modeling method for scenes containing a
glass case. In the proposed method, refraction and reflection are modeled using
elements that are dependent and independent of the viewer's perspective. This
approach allows us to estimate the surfaces where refraction occurs, i.e.,
glass surfaces, and enables the separation and modeling of both direct and
reflected light components. Compared to existing methods, the proposed method
enables more accurate modeling of both glass refraction and the overall scene.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17117">Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation. (arXiv:2311.17117v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Li Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Ke Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1">Liefeng Bo</a></p>
<p>Character Animation aims to generating character videos from still images
through driving signals. Currently, diffusion models have become the mainstream
in visual generation research, owing to their robust generative capabilities.
However, challenges persist in the realm of image-to-video, especially in
character animation, where temporally maintaining consistency with detailed
information from character remains a formidable problem. In this paper, we
leverage the power of diffusion models and propose a novel framework tailored
for character animation. To preserve consistency of intricate appearance
features from reference image, we design ReferenceNet to merge detail features
via spatial attention. To ensure controllability and continuity, we introduce
an efficient pose guider to direct character's movements and employ an
effective temporal modeling approach to ensure smooth inter-frame transitions
between video frames. By expanding the training data, our approach can animate
arbitrary characters, yielding superior results in character animation compared
to other image-to-video methods. Furthermore, we evaluate our method on
benchmarks for fashion video and human dance synthesis, achieving
state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17118">AdaFocus: Towards End-to-end Weakly Supervised Learning for Long-Video Action Understanding. (arXiv:2311.17118v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiaming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hanjun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1">Kun-Yu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Junwei Liang</a></p>
<p>Developing end-to-end models for long-video action understanding tasks
presents significant computational and memory challenges. Existing works
generally build models on long-video features extracted by off-the-shelf action
recognition models, which are trained on short-video datasets in different
domains, making the extracted features suffer domain discrepancy. To avoid
this, action recognition models can be end-to-end trained on clips, which are
trimmed from long videos and labeled using action interval annotations. Such
fully supervised annotations are expensive to collect. Thus, a weakly
supervised method is needed for long-video action understanding at scale. Under
the weak supervision setting, action labels are provided for the whole video
without precise start and end times of the action clip. To this end, we propose
an AdaFocus framework. AdaFocus estimates the spike-actionness and temporal
positions of actions, enabling it to adaptively focus on action clips that
facilitate better training without the need for precise annotations.
Experiments on three long-video datasets show its effectiveness. Remarkably, on
two of datasets, models trained with AdaFocus under weak supervision outperform
those trained under full supervision. Furthermore, we form a weakly supervised
feature extraction pipeline with our AdaFocus, which enables significant
improvements on three long-video action understanding tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17119">Continuous Pose for Monocular Cameras in Neural Implicit Representation. (arXiv:2311.17119v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1">Danda Pani Paudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1">Ajad Chhatkuli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>In this paper, we showcase the effectiveness of optimizing monocular camera
poses as a continuous function of time. The camera poses are represented using
an implicit neural function which maps the given time to the corresponding
camera pose. The mapped camera poses are then used for the downstream tasks
where joint camera pose optimization is also required. While doing so, the
network parameters -- that implicitly represent camera poses -- are optimized.
We exploit the proposed method in four diverse experimental settings, namely,
(1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual
Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all
four settings, the proposed method performs significantly better than the
compared baselines and the state-of-the-art methods. Additionally, using the
assumption of continuous motion, changes in pose may actually live in a
manifold that has lower than 6 degrees of freedom (DOF) is also realized. We
call this low DOF motion representation as the \emph{intrinsic motion} and use
the approach in vSLAM settings, showing impressive camera tracking performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17121">Generative Data Augmentation Improves Scribble-supervised Semantic Segmentation. (arXiv:2311.17121v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schnell_J/0/1/0/all/0/1">Jacob Schnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jieke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1">Vincent Tao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Meng Tang</a></p>
<p>Recent advances in generative models, such as diffusion models, have made
generating high-quality synthetic images widely accessible. Prior works have
shown that training on synthetic images improves many perception tasks, such as
image classification, object detection, and semantic segmentation. We are the
first to explore generative data augmentations for scribble-supervised semantic
segmentation. We propose a generative data augmentation method that leverages a
ControlNet diffusion model conditioned on semantic scribbles to produce
high-quality training data. However, naive implementations of generative data
augmentations may inadvertently harm the performance of the downstream
segmentor rather than improve it. We leverage classifier-free diffusion
guidance to enforce class consistency and introduce encode ratios to trade off
data diversity for data realism. Using the guidance scale and encode ratio, we
are able to generate a spectrum of high-quality training images. We propose
multiple augmentation schemes and find that these schemes significantly impact
model performance, especially in the low-data regime. Our framework further
reduces the gap between the performance of scribble-supervised segmentation and
that of fully-supervised segmentation. We also show that our framework
significantly improves segmentation performance on small datasets, even
surpassing fully-supervised segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17122">Large Model Based Referring Camouflaged Object Detection. (arXiv:2311.17122v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Shupeng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Ge-Peng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1">Pengda Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">Deng-Ping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bowen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Peng Xu</a></p>
<p>Referring camouflaged object detection (Ref-COD) is a recently-proposed
problem aiming to segment out specified camouflaged objects matched with a
textual or visual reference. This task involves two major challenges: the COD
domain-specific perception and multimodal reference-image alignment. Our
motivation is to make full use of the semantic intelligence and intrinsic
knowledge of recent Multimodal Large Language Models (MLLMs) to decompose this
complex task in a human-like way. As language is highly condensed and
inductive, linguistic expression is the main media of human knowledge learning,
and the transmission of knowledge information follows a multi-level progression
from simplicity to complexity. In this paper, we propose a large-model-based
Multi-Level Knowledge-Guided multimodal method for Ref-COD termed MLKG, where
multi-level knowledge descriptions from MLLM are organized to guide the large
vision model of segmentation to perceive the camouflage-targets and
camouflage-scene progressively and meanwhile deeply align the textual
references with camouflaged photos. To our knowledge, our contributions mainly
include: (1) This is the first time that the MLLM knowledge is studied for
Ref-COD and COD. (2) We, for the first time, propose decomposing Ref-COD into
two main perspectives of perceiving the target and scene by integrating MLLM
knowledge, and contribute a multi-level knowledge-guided method. (3) Our method
achieves the state-of-the-art on the Ref-COD benchmark outperforming numerous
strong competitors. Moreover, thanks to the injected rich knowledge, it
demonstrates zero-shot generalization ability on uni-modal COD datasets. We
will release our code soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17123">ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis. (arXiv:2311.17123v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiangjun Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaopeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yanpei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1">Long Quan</a></p>
<p>In this work, we propose a method to address the challenge of rendering a 3D
human from a single image in a free-view manner. Some existing approaches could
achieve this by using generalizable pixel-aligned implicit fields to
reconstruct a textured mesh of a human or by employing a 2D diffusion model as
guidance with the Score Distillation Sampling (SDS) method, to lift the 2D
image into 3D space. However, a generalizable implicit field often results in
an over-smooth texture field, while the SDS method tends to lead to a
texture-inconsistent novel view with the input image. In this paper, we
introduce a texture-consistent back view synthesis module that could transfer
the reference image content to the back view through depth and text-guided
attention injection. Moreover, to alleviate the color distortion that occurs in
the side region, we propose a visibility-aware patch consistency regularization
for texture mapping and refinement combined with the synthesized back view
texture. With the above techniques, we could achieve high-fidelity and
texture-consistent human rendering from a single image. Experiments conducted
on both real and synthetic data demonstrate the effectiveness of our method and
show that our approach outperforms previous baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17126">Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis. (arXiv:2311.17126v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaohui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yongfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yingxiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jianbo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1">Quanzeng You</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Li-Ping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a></p>
<p>Recent advancements in text-to-image (T2I) generative models have shown
remarkable capabilities in producing diverse and imaginative visuals based on
text prompts. Despite the advancement, these diffusion models sometimes
struggle to translate the semantic content from the text into images entirely.
While conditioning on the layout has shown to be effective in improving the
compositional ability of T2I diffusion models, they typically require manual
layout input. In this work, we introduce a novel approach to improving T2I
diffusion models using Large Language Models (LLMs) as layout generators. Our
method leverages the Chain-of-Thought prompting of LLMs to interpret text and
generate spatially reasonable object layouts. The generated layout is then used
to enhance the generated images' composition and spatial accuracy. Moreover, we
propose an efficient adapter based on a cross-attention mechanism, which
explicitly integrates the layout information into the stable diffusion models.
Our experiments demonstrate significant improvements in image quality and
layout accuracy, showcasing the potential of LLMs in augmenting generative
image models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17128">Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks. (arXiv:2311.17128v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beerens_L/0/1/0/all/0/1">Lucas Beerens</a>, <a href="http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1">Desmond J. Higham</a></p>
<p>Recent advancements in Optical Character Recognition (OCR) have been driven
by transformer-based models. OCR systems are critical in numerous high-stakes
domains, yet their vulnerability to adversarial attack remains largely
uncharted territory, raising concerns about security and compliance with
emerging AI regulations. In this work we present a novel framework to assess
the resilience of Transformer-based OCR (TrOCR) models. We develop and assess
algorithms for both targeted and untargeted attacks. For the untargeted case,
we measure the Character Error Rate (CER), while for the targeted case we use
the success ratio. We find that TrOCR is highly vulnerable to untargeted
attacks and somewhat less vulnerable to targeted attacks. On a benchmark
handwriting data set, untargeted attacks can cause a CER of more than 1 without
being noticeable to the eye. With a similar perturbation size, targeted attacks
can lead to success rates of around $25\%$ -- here we attacked single tokens,
requiring TrOCR to output the tenth most likely token from a large vocabulary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17129">Feedback RoI Features Improve Aerial Object Detection. (arXiv:2311.17129v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1">Botao Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Botian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tengyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhidong Deng</a></p>
<p>Neuroscience studies have shown that the human visual system utilizes
high-level feedback information to guide lower-level perception, enabling
adaptation to signals of different characteristics. In light of this, we
propose Feedback multi-Level feature Extractor (Flex) to incorporate a similar
mechanism for object detection. Flex refines feature selection based on
image-wise and instance-level feedback information in response to image quality
variation and classification uncertainty. Experimental results show that Flex
offers consistent improvement to a range of existing SOTA methods on the
challenging aerial object detection datasets including DOTA-v1.0, DOTA-v1.5,
and HRSC2016. Although the design originates in aerial image detection, further
experiments on MS COCO also reveal our module's efficacy in general detection
models. Quantitative and qualitative analyses indicate that the improvements
are closely related to image qualities, which match our motivation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17132">TransNeXt: Robust Foveal Visual Perception for Vision Transformers. (arXiv:2311.17132v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Dai Shi</a></p>
<p>Due to the depth degradation effect in residual connections, many efficient
Vision Transformers models that rely on stacking layers for information
exchange often fail to form sufficient information mixing, leading to unnatural
visual perception. To address this issue, in this paper, we propose Aggregated
Attention, a biomimetic design-based token mixer that simulates biological
foveal vision and continuous eye movement while enabling each token on the
feature map to have a global perception. Furthermore, we incorporate learnable
tokens that interact with conventional queries and keys, which further
diversifies the generation of affinity matrices beyond merely relying on the
similarity between queries and keys. Our approach does not rely on stacking for
information exchange, thus effectively avoiding depth degradation and achieving
natural visual perception. Additionally, we propose Convolutional GLU, a
channel mixer that bridges the gap between GLU and SE mechanism, which empowers
each token to have channel attention based on its nearest neighbor image
features, enhancing local modeling capability and model robustness. We combine
aggregated attention and convolutional GLU to create a new visual backbone
called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves
state-of-the-art performance across multiple model sizes. At a resolution of
$224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing
ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet
accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of
$384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic
segmentation mIoU of 54.7.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17135">TLControl: Trajectory and Language Control for Human Motion Synthesis. (arXiv:2311.17135v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Weilin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhiyang Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1">Dinesh Jayaraman</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingjie Liu</a></p>
<p>Controllable human motion synthesis is essential for applications in AR/VR,
gaming, movies, and embodied AI. Existing methods often focus solely on either
language or full trajectory control, lacking precision in synthesizing motions
aligned with user-specified trajectories, especially for multi-joint control.
To address these issues, we present TLControl, a new method for realistic human
motion synthesis, incorporating both low-level trajectory and high-level
language semantics controls. Specifically, we first train a VQ-VAE to learn a
compact latent motion space organized by body parts. We then propose a Masked
Trajectories Transformer to make coarse initial predictions of full
trajectories of joints based on the learned latent motion space, with
user-specified partial trajectories and text descriptions as conditioning.
Finally, we introduce an efficient test-time optimization to refine these
coarse predictions for accurate trajectory control. Experiments demonstrate
that TLControl outperforms the state-of-the-art in trajectory accuracy and time
efficiency, making it practical for interactive and high-quality animation
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17136">UniIR: Training and Benchmarking Universal Multimodal Information Retrievers. (arXiv:2311.17136v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Cong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haonan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1">Alan Ritter</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a></p>
<p>Existing information retrieval (IR) models often assume a homogeneous format,
limiting their applicability to diverse user needs, such as searching for
images with text descriptions, searching for a news article with a headline
image, or finding a similar photo with a query image. To approach such
different information-seeking demands, we introduce UniIR, a unified
instruction-guided multimodal retriever capable of handling eight distinct
retrieval tasks across modalities. UniIR, a single retrieval system jointly
trained on ten diverse multimodal-IR datasets, interprets user instructions to
execute various retrieval tasks, demonstrating robust performance across
existing datasets and zero-shot generalization to new tasks. Our experiments
highlight that multi-task training and instruction tuning are keys to UniIR's
generalization ability. Additionally, we construct the M-BEIR, a multimodal
retrieval benchmark with comprehensive results, to standardize the evaluation
of universal multimodal information retrieval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17137">Generative Models: What do they know? Do they know things? Let&#x27;s find out!. (arXiv:2311.17137v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaodan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1">Nicholas Kolkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1">Greg Shakhnarovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1">Anand Bhattad</a></p>
<p>Generative models have been shown to be capable of synthesizing highly
detailed and realistic images. It is natural to suspect that they implicitly
learn to model some image intrinsics such as surface normals, depth, or
shadows. In this paper, we present compelling evidence that generative models
indeed internally produce high-quality scene intrinsic maps. We introduce
Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms
any generative model into a scene intrinsic predictor, capable of extracting
intrinsic scene maps directly from the original generator network without
needing additional decoders or fully fine-tuning the original network. Our
method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly
learned parameters that make up less than 0.6% of the total parameters in the
generative model. Optimized with a small set of labeled images, our
model-agnostic approach adapts to various generative architectures, including
Diffusion models, GANs, and Autoregressive models. We show that the scene
intrinsic maps produced by our method compare well with, and in some cases
surpass those generated by leading supervised techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17138">Shadows Don&#x27;t Lie and Lines Can&#x27;t Bend! Generative Models don&#x27;t know Projective Geometry...for now. (arXiv:2311.17138v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1">Ayush Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1">Hanlin Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1">Amitabh Mahapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1">Svetlana Lazebnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1">D.A. Forsyth</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1">Anand Bhattad</a></p>
<p>Generative models can produce impressively realistic images. This paper
demonstrates that generated images have geometric features different from those
of real images. We build a set of collections of generated images, prequalified
to fool simple, signal-based classifiers into believing they are real. We then
show that prequalified generated images can be identified reliably by
classifiers that only look at geometric properties. We use three such
classifiers. All three classifiers are denied access to image pixels, and look
only at derived geometric features. The first classifier looks at the
perspective field of the image, the second looks at lines detected in the
image, and the third looks at relations between detected objects and shadows.
Our procedure detects generated images more reliably than SOTA local signal
based detectors, for images from a number of distinct generators. Saliency maps
suggest that the classifiers can identify geometric problems reliably. We
conclude that current generators cannot reliably reproduce geometric properties
of real images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17177">THInImg: Cross-modal Steganography for Presenting Talking Heads in Images. (arXiv:2311.17177v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1">Xuefei Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xinru Jiang</a></p>
<p>Cross-modal Steganography is the practice of concealing secret signals in
publicly available cover signals (distinct from the modality of the secret
signals) unobtrusively. While previous approaches primarily concentrated on
concealing a relatively small amount of information, we propose THInImg, which
manages to hide lengthy audio data (and subsequently decode talking head video)
inside an identity image by leveraging the properties of human face, which can
be effectively utilized for covert communication, transmission and copyright
protection. THInImg consists of two parts: the encoder and decoder. Inside the
encoder-decoder pipeline, we introduce a novel architecture that substantially
increase the capacity of hiding audio in images. Moreover, our framework can be
extended to iteratively hide multiple audio clips into an identity image,
offering multiple levels of control over permissions. We conduct extensive
experiments to prove the effectiveness of our method, demonstrating that
THInImg can present up to 80 seconds of high quality talking-head video
(including audio) in an identity image with 160x160 resolution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17179">SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery. (arXiv:2311.17179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1">Konstantin Klemmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1">Esther Rolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1">Caleb Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1">Lester Mackey</a>, <a href="http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1">Marc Ru&#xdf;wurm</a></p>
<p>Geographic location is essential for modeling tasks in fields ranging from
ecology to epidemiology to the Earth system sciences. However, extracting
relevant and meaningful characteristics of a location can be challenging, often
entailing expensive data fusion or data distillation from global imagery
datasets. To address this challenge, we introduce Satellite Contrastive
Location-Image Pretraining (SatCLIP), a global, general-purpose geographic
location encoder that learns an implicit representation of locations from
openly available satellite imagery. Trained location encoders provide vector
embeddings summarizing the characteristics of any given location for convenient
usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained
on globally sampled multi-spectral Sentinel-2 satellite data, can be used in
various predictive tasks that depend on location information but not
necessarily satellite imagery, including temperature prediction, animal
recognition in imagery, and population density estimation. Across tasks,
SatCLIP embeddings consistently outperform embeddings from existing pretrained
location encoders, ranging from models trained on natural images to models
trained on semantic context. SatCLIP embeddings also help to improve geographic
generalization. This demonstrates the potential of general-purpose location
encoders and opens the door to learning meaningful representations of our
planet from the vast, varied, and largely untapped modalities of geospatial
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17216">Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation. (arXiv:2311.17216v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chengzhi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a></p>
<p>Diffusion-based models have gained significant popularity for text-to-image
generation due to their exceptional image-generation capabilities. A risk with
these models is the potential generation of inappropriate content, such as
biased or harmful images. However, the underlying reasons for generating such
undesired content from the perspective of the diffusion model's internal
representation remain unclear. Previous work interprets vectors in an
interpretable latent space of diffusion models as semantic concepts. However,
existing approaches cannot discover directions for arbitrary concepts, such as
those related to inappropriate concepts. In this work, we propose a novel
self-supervised approach to find interpretable latent directions for a given
concept. With the discovered vectors, we further propose a simple approach to
mitigate inappropriate generation. Extensive experiments have been conducted to
verify the effectiveness of our mitigation approach, namely, for fair
generation, safe generation, and responsible text-enhancing generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17218">BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling. (arXiv:2311.17218v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yixuan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengye Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sai Qian Zhang</a></p>
<p>Like masked language modeling (MLM) in natural language processing, masked
image modeling (MIM) aims to extract valuable insights from image patches to
enhance the feature extraction capabilities of the underlying deep neural
network (DNN). Contrasted with other training paradigms like supervised
learning and unsupervised contrastive learning, masked image modeling (MIM)
pretraining typically demands significant computational resources in order to
manage large training data batches (e.g., 4096). The significant memory and
computation requirements pose a considerable challenge to its broad adoption.
To mitigate this, we introduce a novel learning framework,
termed~\textit{Block-Wise Masked Image Modeling} (BIM). This framework involves
decomposing the MIM tasks into several sub-tasks with independent computation
patterns, resulting in block-wise back-propagation operations instead of the
traditional end-to-end approach. Our proposed BIM maintains superior
performance compared to conventional MIM while greatly reducing peak memory
consumption. Moreover, BIM naturally enables the concurrent training of
numerous DNN backbones of varying depths. This leads to the creation of
multiple trained DNN backbones, each tailored to different hardware platforms
with distinct computing capabilities. This approach significantly reduces
computational costs in comparison with training each DNN backbone individually.
Our framework offers a promising solution for resource constrained training of
MIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17232">ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development. (arXiv:2311.17232v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cappell_B/0/1/0/all/0/1">Benjamin Cappell</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoll_A/0/1/0/all/0/1">Andreas Stoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Umah_W/0/1/0/all/0/1">Williams Chukwudi Umah</a>, <a href="http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1">Bernhard Egger</a></p>
<p>Computational models trained on a large amount of natural images are the
state-of-the-art to study human vision - usually adult vision. Computational
models of infant vision and its further development are gaining more and more
attention in the community. In this work we aim at the very beginning of our
visual experience - pre- and post-natal retinal waves which suggest to be a
pre-training mechanism for the primate visual system at a very early stage of
development. We see this approach as an instance of biologically plausible data
driven inductive bias through pre-training. We built a computational model that
mimics this development mechanism by pre-training different artificial
convolutional neural networks with simulated retinal wave images. The resulting
features of this biologically plausible pre-training closely match the V1
features of the primate visual system. We show that the performance gain by
pre-training with retinal waves is similar to a state-of-the art pre-training
pipeline. Our framework contains the retinal wave generator, as well as a
training strategy, which can be a first step in a curriculum learning based
training diet for various models of development. We release code, data and
trained networks to build the basis for future work on visual development and
based on a curriculum learning approach including prenatal development to
support studies of innate vs. learned properties of the primate visual system.
An additional benefit of our pre-trained networks for neuroscience or computer
vision applications is the absence of biases inherited from datasets like
ImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17241">End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames. (arXiv:2311.17241v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shuming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen-Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a></p>
<p>Recently, temporal action detection (TAD) has seen significant performance
improvement with end-to-end training. However, due to the memory bottleneck,
only models with limited scales and limited data volumes can afford end-to-end
training, which inevitably restricts TAD performance. In this paper, we reduce
the memory consumption for end-to-end training, and manage to scale up the TAD
backbone to 1 billion parameters and the input video to 1,536 frames, leading
to significant detection performance. The key to our approach lies in our
proposed temporal-informative adapter (TIA), which is a novel lightweight
module that reduces training memory. Using TIA, we free the humongous backbone
from learning to adapt to the TAD task by only updating the parameters in TIA.
TIA also leads to better TAD representation by temporally aggregating context
from adjacent frames throughout the backbone. We evaluate our model across four
representative datasets. Owing to our efficient design, we are able to train
end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the
first end-to-end model to outperform the best feature-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17243">PHG-Net: Persistent Homology Guided Medical Image Classification. (arXiv:2311.17243v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yaopeng Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1">Milan Sonka</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danny Z. Chen</a></p>
<p>Modern deep neural networks have achieved great successes in medical image
analysis. However, the features captured by convolutional neural networks
(CNNs) or Transformers tend to be optimized for pixel intensities and neglect
key anatomical structures such as connected components and loops. In this
paper, we propose a persistent homology guided approach (PHG-Net) that explores
topological features of objects for medical image classification. For an input
image, we first compute its cubical persistence diagram and extract topological
features into a vector representation using a small neural network (called the
PH module). The extracted topological features are then incorporated into the
feature map generated by CNN or Transformer for feature fusion. The PH module
is lightweight and capable of integrating topological features into any CNN or
Transformer architectures in an end-to-end fashion. We evaluate our PHG-Net on
three public datasets and demonstrate its considerable improvements on the
target classification tasks over state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17245">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS. (arXiv:2311.17245v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kevin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1">Kairun Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zehao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>Recent advancements in real-time neural rendering using point-based
techniques have paved the way for the widespread adoption of 3D
representations. However, foundational approaches like 3D Gaussian Splatting
come with a substantial storage overhead caused by growing the SfM points to
millions, often demanding gigabyte-level disk space for a single unbounded
scene, posing significant scalability challenges and hindering the splatting
efficiency.
</p>
<p>To address this challenge, we introduce LightGaussian, a novel method
designed to transform 3D Gaussians into a more efficient and compact format.
Drawing inspiration from the concept of Network Pruning, LightGaussian
identifies Gaussians that are insignificant in contributing to the scene
reconstruction and adopts a pruning and recovery process, effectively reducing
redundancy in Gaussian counts while preserving visual effects. Additionally,
LightGaussian employs distillation and pseudo-view augmentation to distill
spherical harmonics to a lower degree, allowing knowledge transfer to more
compact representations while maintaining reflectance. Furthermore, we propose
a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in
lower bitwidth representations with minimal accuracy losses.
</p>
<p>In summary, LightGaussian achieves an averaged compression rate over 15x
while boosting the FPS from 139 to 215, enabling an efficient representation of
complex scenes on Mip-NeRF 360, Tank and Temple datasets.
</p>
<p>Project website: https://lightgaussian.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17251">SubZero: Subspace Zero-Shot MRI Reconstruction. (arXiv:2311.17251v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1">Heng Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Arefeen_Y/0/1/0/all/0/1">Yamin Arefeen</a>, <a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1">Berkin Bilgic</a></p>
<p>Recently introduced zero-shot self-supervised learning (ZS-SSL) has shown
potential in accelerated MRI in a scan-specific scenario, which enabled
high-quality reconstructions without access to a large training dataset. ZS-SSL
has been further combined with the subspace model to accelerate 2D T2-shuffling
acquisitions. In this work, we propose a parallel network framework and
introduce an attention mechanism to improve subspace-based zero-shot
self-supervised learning and enable higher acceleration factors. We name our
method SubZero and demonstrate that it can achieve improved performance
compared with current methods in T1 and T2 mapping acquisitions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17256">Pattern retrieval of traffic congestion using graph-based associations of traffic domain-specific features. (arXiv:2311.17256v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tin T. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Calvert_S/0/1/0/all/0/1">Simeon C. Calvert</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lint_H/0/1/0/all/0/1">Hans van Lint</a></p>
<p>The fast-growing amount of traffic data brings many opportunities for
revealing more insightful information about traffic dynamics. However, it also
demands an effective database management system in which information retrieval
is arguably an important feature. The ability to locate similar patterns in big
datasets potentially paves the way for further valuable analyses in traffic
management. This paper proposes a content-based retrieval system for
spatiotemporal patterns of highway traffic congestion. There are two main
components in our framework, namely pattern representation and similarity
measurement. To effectively interpret retrieval outcomes, the paper proposes a
graph-based approach (relation-graph) for the former component, in which
fundamental traffic phenomena are encoded as nodes and their spatiotemporal
relationships as edges. In the latter component, the similarities between
congestion patterns are customizable with various aspects according to user
expectations. We evaluated the proposed framework by applying it to a dataset
of hundreds of patterns with various complexities (temporally and spatially).
The example queries indicate the effectiveness of the proposed method, i.e. the
obtained patterns present similar traffic phenomena as in the given examples.
In addition, the success of the proposed approach directly derives a new
opportunity for semantic retrieval, in which expected patterns are described by
adopting the relation-graph notion to associate fundamental traffic phenomena.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17261">SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors. (arXiv:2311.17261v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dave Zhenyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hsin-Ying Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1">Matthias Nie&#xdf;ner</a></p>
<p>We propose SceneTex, a novel method for effectively generating high-quality
and style-consistent textures for indoor scenes using depth-to-image diffusion
priors. Unlike previous methods that either iteratively warp 2D views onto a
mesh surface or distillate diffusion latent features without accurate geometric
and style cues, SceneTex formulates the texture synthesis task as an
optimization problem in the RGB space where style and geometry consistency are
properly reflected. At its core, SceneTex proposes a multiresolution texture
field to implicitly encode the mesh appearance. We optimize the target texture
via a score-distillation-based objective function in respective RGB renderings.
To further secure the style consistency across views, we introduce a
cross-attention decoder to predict the RGB values by cross-attending to the
pre-sampled reference locations in each instance. SceneTex enables various and
accurate texture synthesis for 3D-FRONT scenes, demonstrating significant
improvements in visual quality and prompt fidelity over the prior texture
generation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17267">E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer. (arXiv:2311.17267v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jacob Zhiyuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Skyler Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1">Vasu Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1">Robinson Piramuthu</a></p>
<p>To build scalable models for challenging real-world tasks, it is important to
learn from diverse, multi-modal data in various forms (e.g., videos, text, and
images). Among the existing works, a plethora of them have focused on
leveraging large but cumbersome cross-modal architectures. Regardless of their
effectiveness, larger architectures unavoidably prevent the models from being
extended to real-world applications, so building a lightweight VL architecture
and an efficient learning schema is of great practical value. In this paper, we
propose an Efficient Video-Language Model (dubbed as E-ViLM) and a masked video
modeling (MVM) schema, assisted with a semantic vector-quantized tokenizer. In
particular, our E-ViLM learns to reconstruct the semantic labels of masked
video regions, produced by the pre-trained vector-quantized tokenizer, which
discretizes the continuous visual signals into labels. We show that with our
simple MVM task and regular VL pre-training modelings, our E-ViLM, despite its
compactness, is able to learn expressive representations from Video-Language
corpus and generalize well to extensive Video-Language tasks including video
question answering, text-to-video retrieval, etc. In particular, our E-ViLM
obtains obvious efficiency improvements by reaching competing performances with
faster inference speed, i.e., our model reaches $39.3$% Top-$1$ accuracy on the
MSRVTT benchmark, retaining $91.4$% of the accuracy of state-of-the-art larger
VL architecture with only $15%$ parameters and $94.8%$ fewer GFLOPs. We also
provide extensive ablative studies that validate the effectiveness of our
proposed learning schema for E-ViLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17280">Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1">Ishika Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>Data augmentation via back-translation is common when pretraining
Vision-and-Language Navigation (VLN) models, even though the generated
instructions are noisy. But: does that noise matter? We find that nonsensical
or irrelevant language instructions during pretraining can have little effect
on downstream performance for both HAMT and VLN-BERT on R2R, and is still
better than only using clean, human data. To underscore these results, we
concoct an efficient augmentation method, Unigram + Object, which generates
nonsensical instructions that nonetheless improve downstream performance. Our
findings suggest that what matters for VLN R2R pretraining is the quantity of
visual trajectories, not the quality of instructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17286">LEOD: Label-Efficient Object Detection for Event Cameras. (arXiv:2311.17286v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1">Mathias Gehrig</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1">Qing Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xudong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1">Igor Gilitschenski</a></p>
<p>Object detection with event cameras enjoys the property of low latency and
high dynamic range, making it suitable for safety-critical scenarios such as
self-driving. However, labeling event streams with high temporal resolutions
for supervised training is costly. We address this issue with LEOD, the first
framework for label-efficient event-based detection. Our method unifies weakly-
and semi-supervised object detection with a self-training mechanism. We first
utilize a detector pre-trained on limited labels to produce pseudo ground truth
on unlabeled events, and then re-train the detector with both real and
generated labels. Leveraging the temporal consistency of events, we run
bi-directional inference and apply tracking-based post-processing to enhance
the quality of pseudo labels. To stabilize training, we further design a soft
anchor assignment strategy to mitigate the noise in labels. We introduce new
experimental protocols to evaluate the task of label-efficient event-based
detection on Gen1 and 1Mpx datasets. LEOD consistently outperforms supervised
baselines across various labeling ratios. For example, on Gen1, it improves mAP
by 8.6% and 7.8% for RVT-S trained with 1% and 2% labels. On 1Mpx, RVT-S with
10% labels even surpasses its fully-supervised counterpart using 100% labels.
LEOD maintains its effectiveness even when all labeled data are available,
reaching new state-of-the-art results. Finally, we show that our method readily
scales to improve larger detectors as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17299">Federated Fine-Tuning of Foundation Models via Probabilistic Masking. (arXiv:2311.17299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1">Vasileios Tsouvalas</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki Asano</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1">Aaqib Saeed</a></p>
<p>Foundation Models (FMs) have revolutionized machine learning with their
adaptability and high performance across tasks; yet, their integration into
Federated Learning (FL) is challenging due to substantial communication
overhead from their extensive parameterization. Current communication-efficient
FL strategies, such as gradient compression, reduce bitrates to around $1$
bit-per-parameter (bpp). However, these approaches fail to harness the
characteristics of FMs, with their large number of parameters still posing a
challenge to communication efficiency, even at these bitrate regimes. In this
work, we present DeltaMask, a novel method that efficiently fine-tunes FMs in
FL at an ultra-low bitrate, well below 1 bpp. DeltaMask employs stochastic
masking to detect highly effective subnetworks within FMs and leverage
stochasticity and sparsity in client masks to compress updates into a compact
grayscale image using probabilistic filters, deviating from traditional weight
training approaches. Our comprehensive evaluations across various datasets and
architectures demonstrate DeltaMask efficiently achieves bitrates as low as
0.09 bpp, enhancing communication efficiency while maintaining FMs performance,
as measured on 8 datasets and 5 pre-trained models of various network
architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17315">Explaining CLIP&#x27;s performance disparities on data from blind/low vision users. (arXiv:2311.17315v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1">Daniela Massiceti</a>, <a href="http://arxiv.org/find/cs/1/au:+Longden_C/0/1/0/all/0/1">Camilla Longden</a>, <a href="http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1">Agnieszka Slowik</a>, <a href="http://arxiv.org/find/cs/1/au:+Wills_S/0/1/0/all/0/1">Samuel Wills</a>, <a href="http://arxiv.org/find/cs/1/au:+Grayson_M/0/1/0/all/0/1">Martin Grayson</a>, <a href="http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1">Cecily Morrison</a></p>
<p>Large multi-modal models (LMMs) hold the potential to usher in a new era of
automated visual assistance for people who are blind or low vision (BLV). Yet,
these models have not been systematically evaluated on data captured by BLV
users. We address this by empirically assessing CLIP, a widely-used LMM likely
to underpin many assistive technologies. Testing 25 CLIP variants in a
zero-shot classification task, we find that their accuracy is 15 percentage
points lower on average for images captured by BLV users than web-crawled
images. This disparity stems from CLIP's sensitivities to 1) image content
(e.g. not recognizing disability objects as well as other objects); 2) image
quality (e.g. not being robust to lighting variation); and 3) text content
(e.g. not recognizing objects described by tactile adjectives as well as visual
ones). We delve deeper with a textual analysis of three common pre-training
datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content
is rarely mentioned. We then provide three examples that illustrate how the
performance disparities extend to three downstream models underpinned by CLIP:
OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5
images can mitigate CLIP's quality-of-service disparities for BLV users in some
scenarios, which we discuss alongside a set of other possible mitigations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17320">Revisiting Single Image Reflection Removal In the Wild. (arXiv:2311.17320v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yurui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xueyang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1">Peng-Tao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qibin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1">Zheng-Jun Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>This research focuses on the issue of single-image reflection removal (SIRR)
in real-world conditions, examining it from two angles: the collection pipeline
of real reflection pairs and the perception of real reflection locations. We
devise an advanced reflection collection pipeline that is highly adaptable to a
wide range of real-world reflection scenarios and incurs reduced costs in
collecting large-scale aligned reflection pairs. In the process, we develop a
large-scale, high-quality reflection dataset named Reflection Removal in the
Wild (RRW). RRW contains over 14,950 high-resolution real-world reflection
pairs, a dataset forty-five times larger than its predecessors. Regarding
perception of reflection locations, we identify that numerous virtual
reflection objects visible in reflection images are not present in the
corresponding ground-truth images. This observation, drawn from the aligned
pairs, leads us to conceive the Maximum Reflection Filter (MaxRF). The MaxRF
could accurately and explicitly characterize reflection locations from pairs of
images. Building upon this, we design a reflection location-aware cascaded
framework, specifically tailored for SIRR. Powered by these innovative
techniques, our solution achieves superior performance than current leading
methods across multiple real-world benchmarks. Codes and datasets will be
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17325">Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation. (arXiv:2311.17325v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zicheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yixuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luping Zhou</a></p>
<p>Semi-supervised medical image segmentation studies have shown promise in
training models with limited labeled data. However, current dominant
teacher-student based approaches can suffer from the confirmation bias. To
address this challenge, we propose AD-MT, an alternate diverse teaching
approach in a teacher-student framework. It involves a single student model and
two non-trainable teacher models that are momentum-updated periodically and
randomly in an alternate fashion. To mitigate the confirmation bias from the
diverse supervision, the core of AD-MT lies in two proposed modules: the Random
Periodic Alternate (RPA) Updating Module and the Conflict-Combating Module
(CCM). The RPA schedules the alternating diverse updating process with
complementary data batches, distinct data augmentation, and random switching
periods to encourage diverse reasoning from different teaching perspectives.
The CCM employs an entropy-based ensembling strategy to encourage the model to
learn from both the consistent and conflicting predictions between the
teachers. Experimental results demonstrate the effectiveness and superiority of
our AD-MT on the 2D and 3D medical segmentation benchmarks across various
semi-supervised settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17331">Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering. (arXiv:2311.17331v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zeqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Wentao Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Runmeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1">Qiqing Lao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_M/0/1/0/all/0/1">Minjie Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Keze Wang</a></p>
<p>Recently, Vision Language Models (VLMs) have gained significant attention,
exhibiting notable advancements across various tasks by leveraging extensive
image-text paired data. However, prevailing VLMs often treat Visual Question
Answering (VQA) as perception tasks, employing black-box models that overlook
explicit modeling of relationships between different questions within the same
visual scene. Moreover, the existing VQA methods that rely on Knowledge Bases
(KBs) might frequently encounter biases from limited data and face challenges
in relevant information indexing. Attempt to overcome these limitations, this
paper introduces an explainable multi-agent collaboration framework by tapping
into knowledge embedded in Large Language Models (LLMs) trained on extensive
corpora. Inspired by human cognition, our framework uncovers latent information
within the given question by employing three agents, i.e., Seeker, Responder,
and Integrator, to perform a top-down reasoning process. The Seeker agent
generates relevant issues related to the original question. The Responder
agent, based on VLM, handles simple VQA tasks and provides candidate answers.
The Integrator agent combines information from the Seeker agent and the
Responder agent to produce the final VQA answer. Through the above
collaboration mechanism, our framework explicitly constructs a multi-view
knowledge base for a specific image scene, reasoning answers in a top-down
processing manner. We extensively evaluate our method on diverse VQA datasets
and VLMs, demonstrating its broad applicability and interpretability with
comprehensive experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17332">NeRFTAP: Enhancing Transferability of Adversarial Patches on Face Recognition using Neural Radiance Fields. (arXiv:2311.17332v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Furao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Feng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1">Changhai Nie</a></p>
<p>Face recognition (FR) technology plays a crucial role in various
applications, but its vulnerability to adversarial attacks poses significant
security concerns. Existing research primarily focuses on transferability to
different FR models, overlooking the direct transferability to victim's face
images, which is a practical threat in real-world scenarios. In this study, we
propose a novel adversarial attack method that considers both the
transferability to the FR model and the victim's face image, called NeRFTAP.
Leveraging NeRF-based 3D-GAN, we generate new view face images for the source
and target subjects to enhance transferability of adversarial patches. We
introduce a style consistency loss to ensure the visual similarity between the
adversarial UV map and the target UV map under a 0-1 mask, enhancing the
effectiveness and naturalness of the generated adversarial face images.
Extensive experiments and evaluations on various FR models demonstrate the
superiority of our approach over existing attack techniques. Our work provides
valuable insights for enhancing the robustness of FR systems in practical
adversarial settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17334">Long-tailed multi-label classification with noisy label of thoracic diseases from chest X-ray. (arXiv:2311.17334v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1">Haoran Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1">Qingsong Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiyang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xiaodong Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">S Kevin Zhou</a></p>
<p>Chest X-rays (CXR) often reveal rare diseases, demanding precise diagnosis.
However, current computer-aided diagnosis (CAD) methods focus on common
diseases, leading to inadequate detection of rare conditions due to the absence
of comprehensive datasets. To overcome this, we present a novel benchmark for
long-tailed multi-label classification in CXRs, encapsulating both common and
rare thoracic diseases. Our approach includes developing the "LTML-MIMIC-CXR"
dataset, an augmentation of MIMIC-CXR with 26 additional rare diseases. We
propose a baseline method for this classification challenge, integrating
adaptive negative regularization to address negative logits' over-suppression
in tail classes, and a large loss reconsideration strategy for correcting noisy
labels from automated annotations. Our evaluation on LTML-MIMIC-CXR
demonstrates significant advancements in rare disease detection. This work
establishes a foundation for robust CAD methods, achieving a balance in
identifying a spectrum of thoracic diseases in CXRs. Access to our code and
dataset is provided at:https://github.com/laihaoran/LTML-MIMIC-CXR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17335">eMotions: A Large-Scale Dataset for Emotion Recognition in Short Videos. (arXiv:2311.17335v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuecheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Heli Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Junxiao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_R/0/1/0/all/0/1">Ruofan Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xiangyan Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jiayu Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a></p>
<p>Nowadays, short videos (SVs) are essential to information acquisition and
sharing in our life. The prevailing use of SVs to spread emotions leads to the
necessity of emotion recognition in SVs. Considering the lack of SVs emotion
data, we introduce a large-scale dataset named eMotions, comprising 27,996
videos. Meanwhile, we alleviate the impact of subjectivities on labeling
quality by emphasizing better personnel allocations and multi-stage
annotations. In addition, we provide the category-balanced and test-oriented
variants through targeted data sampling. Some commonly used videos (e.g.,
facial expressions and postures) have been well studied. However, it is still
challenging to understand the emotions in SVs. Since the enhanced content
diversity brings more distinct semantic gaps and difficulties in learning
emotion-related features, and there exists information gaps caused by the
emotion incompleteness under the prevalently audio-visual co-expressions. To
tackle these problems, we present an end-to-end baseline method AV-CPNet that
employs the video transformer to better learn semantically relevant
representations. We further design the two-stage cross-modal fusion module to
complementarily model the correlations of audio-visual features. The EP-CE
Loss, incorporating three emotion polarities, is then applied to guide model
optimization. Extensive experimental results on nine datasets verify the
effectiveness of AV-CPNet. Datasets and code will be open on
https://github.com/XuecWu/eMotions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17338">VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model. (arXiv:2311.17338v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haoyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tianyi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiaxi Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zuxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Identity-consistent video generation seeks to synthesize videos that are
guided by both textual prompts and reference images of entities. Current
approaches typically utilize cross-attention layers to integrate the appearance
of the entity, which predominantly captures semantic attributes, resulting in
compromised fidelity of entities. Moreover, these methods necessitate iterative
fine-tuning for each new entity encountered, thereby limiting their
applicability. To address these challenges, we introduce VideoAssembler, a
novel end-to-end framework for identity-consistent video generation that can
conduct inference directly when encountering new entities. VideoAssembler is
adept at producing videos that are not only flexible with respect to the input
reference entities but also responsive to textual conditions. Additionally, by
modulating the quantity of input images for the entity, VideoAssembler enables
the execution of tasks ranging from image-to-video generation to sophisticated
video editing. VideoAssembler comprises two principal components: the Reference
Entity Pyramid (REP) encoder and the Entity-Prompt Attention Fusion (EPAF)
module. The REP encoder is designed to infuse comprehensive appearance details
into the denoising stages of the stable diffusion model. Concurrently, the EPAF
module is utilized to integrate text-aligned features effectively. Furthermore,
to mitigate the challenge of scarce data, we present a methodology for the
preprocessing of training data. Our evaluation of the VideoAssembler framework
on the UCF-101, MSR-VTT, and DAVIS datasets indicates that it achieves good
performances in both quantitative and qualitative analyses (346.84 in FVD and
48.01 in IS on UCF-101). Our project page is at
https://videoassembler.github.io/videoassembler.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17339">RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on Face Recognition. (arXiv:2311.17339v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Furao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1">Changhai Nie</a></p>
<p>Face recognition (FR) systems powered by deep learning have become widely
used in various applications. However, they are vulnerable to adversarial
attacks, especially those based on local adversarial patches that can be
physically applied to real-world objects. In this paper, we propose RADAP, a
robust and adaptive defense mechanism against diverse adversarial patches in
both closed-set and open-set FR systems. RADAP employs innovative techniques,
such as FCutout and F-patch, which use Fourier space sampling masks to improve
the occlusion robustness of the FR model and the performance of the patch
segmenter. Moreover, we introduce an edge-aware binary cross-entropy (EBCE)
loss function to enhance the accuracy of patch detection. We also present the
split and fill (SAF) strategy, which is designed to counter the vulnerability
of the patch segmenter to complete white-box adaptive attacks. We conduct
comprehensive experiments to validate the effectiveness of RADAP, which shows
significant improvements in defense performance against various adversarial
patches, while maintaining clean accuracy higher than that of the undefended
Vanilla model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17340">Cross-Scope Spatial-Spectral Information Aggregation for Hyperspectral Image Super-Resolution. (arXiv:2311.17340v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Shi Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lefei Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Liangpei Zhang</a></p>
<p>Hyperspectral image super-resolution has attained widespread prominence to
enhance the spatial resolution of hyperspectral images. However,
convolution-based methods have encountered challenges in harnessing the global
spatial-spectral information. The prevailing transformer-based methods have not
adequately captured the long-range dependencies in both spectral and spatial
dimensions. To alleviate this issue, we propose a novel cross-scope
spatial-spectral Transformer (CST) to efficiently investigate long-range
spatial and spectral similarities for single hyperspectral image
super-resolution. Specifically, we devise cross-attention mechanisms in spatial
and spectral dimensions to comprehensively model the long-range
spatial-spectral characteristics. By integrating global information into the
rectangle-window self-attention, we first design a cross-scope spatial
self-attention to facilitate long-range spatial interactions. Then, by
leveraging appropriately characteristic spatial-spectral features, we construct
a cross-scope spectral self-attention to effectively capture the intrinsic
correlations among global spectral bands. Finally, we elaborate a concise
feed-forward neural network to enhance the feature representation capacity in
the Transformer structure. Extensive experiments over three hyperspectral
datasets demonstrate that the proposed CST is superior to other
state-of-the-art methods both quantitatively and visually. The code is
available at \url{https://github.com/Tomchenshi/CST.git}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17350">Implicit-explicit Integrated Representations for Multi-view Video Compression. (arXiv:2311.17350v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Rong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Li Song</a></p>
<p>With the increasing consumption of 3D displays and virtual reality,
multi-view video has become a promising format. However, its high resolution
and multi-camera shooting result in a substantial increase in data volume,
making storage and transmission a challenging task. To tackle these
difficulties, we propose an implicit-explicit integrated representation for
multi-view video compression. Specifically, we first use the explicit
representation-based 2D video codec to encode one of the source views.
Subsequently, we propose employing the implicit neural representation
(INR)-based codec to encode the remaining views. The implicit codec takes the
time and view index of multi-view video as coordinate inputs and generates the
corresponding implicit reconstruction frames.To enhance the compressibility, we
introduce a multi-level feature grid embedding and a fully convolutional
architecture into the implicit codec. These components facilitate
coordinate-feature and feature-RGB mapping, respectively. To further enhance
the reconstruction quality from the INR codec, we leverage the high-quality
reconstructed frames from the explicit codec to achieve inter-view
compensation. Finally, the compensated results are fused with the implicit
reconstructions from the INR to obtain the final reconstructed frames. Our
proposed framework combines the strengths of both implicit neural
representation and explicit 2D codec. Extensive experiments conducted on public
datasets demonstrate that the proposed framework can achieve comparable or even
superior performance to the latest multi-view video compression standard MIV
and other INR-based schemes in terms of view compression and scene modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17352">Efficient Stitchable Task Adaptation. (arXiv:2311.17352v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zizheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jianfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>The paradigm of pre-training and fine-tuning has laid the foundation for
deploying deep learning models. However, most fine-tuning methods are designed
to meet a specific resource budget. Recently, considering diverse deployment
scenarios with various resource budgets, stitchable neural network (SN-Net) is
introduced to quickly obtain numerous new networks (stitches) from the
pre-trained models (anchors) in a model family via model stitching. Although
promising, SN-Net confronts new challenges when adapting it to new target
domains, including huge memory and storage requirements and a long and
sub-optimal multistage adaptation process. In this work, we present a novel
framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce
a palette of fine-tuned models that adhere to diverse resource constraints.
Specifically, we first tailor parameter-efficient fine-tuning to share low-rank
updates among the stitches while maintaining independent bias terms. In this
way, we largely reduce fine-tuning memory burdens and mitigate the interference
among stitches that arises in task adaptation. Furthermore, we streamline a
simple yet effective one-stage deployment pipeline, which estimates the
important stitches to deploy with training-time gradient statistics. By
assigning higher sampling probabilities to important stitches, we also get a
boosted Pareto frontier. Extensive experiments on 25 downstream visual
recognition tasks demonstrate that our ESTA is capable of generating stitches
with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net
adaptation by remarkable margins with significantly lower training time and
fewer trainable parameters. Furthermore, we demonstrate the flexibility and
scalability of our ESTA framework by stitching LLMs from LLaMA family,
obtaining chatbot stitches of assorted sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17354">A natural language processing-based approach: mapping human perception by understanding deep semantic features in street view images. (arXiv:2311.17354v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoran Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dongdong Wu</a></p>
<p>In the past decade, using Street View images and machine learning to measure
human perception has become a mainstream research approach in urban science.
However, this approach using only image-shallow information makes it difficult
to comprehensively understand the deep semantic features of human perception of
a scene. In this study, we proposed a new framework based on a pre-train
natural language model to understand the relationship between human perception
and the sense of a scene. Firstly, Place Pulse 2.0 was used as our base
dataset, which contains a variety of human-perceived labels, namely, beautiful,
safe, wealthy, depressing, boring, and lively. An image captioning network was
used to extract the description information of each street view image.
Secondly, a pre-trained BERT model was finetuning and added a regression
function for six human perceptual dimensions. Furthermore, we compared the
performance of five traditional regression methods with our approach and
conducted a migration experiment in Hong Kong. Our results show that human
perception scoring by deep semantic features performed better than previous
studies by machine learning methods with shallow features. The use of deep
scene semantic features provides new ideas for subsequent human perception
research, as well as better explanatory power in the face of spatial
heterogeneity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17361">How does spatial structure affect psychological restoration? A method based on Graph Neural Networks and Street View Imagery. (arXiv:2311.17361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoran Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhua_P/0/1/0/all/0/1">Pengyu Zhua</a></p>
<p>The Attention Restoration Theory (ART) presents a theoretical framework with
four essential indicators (being away, extent, fascinating, and compatibility)
for comprehending urban and natural restoration quality. However, previous
studies relied on non-sequential data and non-spatial dependent methods, which
overlooks the impact of spatial structure defined here as the positional
relationships between scene entities on restoration quality. The past methods
also make it challenging to measure restoration quality on an urban scale. In
this work, a spatial-dependent graph neural networks (GNNs) approach is
proposed to reveal the relation between spatial structure and restoration
quality on an urban scale. Specifically, we constructed two different types of
graphs at the street and city levels. The street-level graphs, using sequential
street view images (SVIs) of road segments to capture position relationships
between entities, were used to represent spatial structure. The city-level
graph, modeling the topological relationships of roads as non-Euclidean data
structures and embedding urban features (including Perception-features,
Spatial-features, and Socioeconomic-features), was used to measure restoration
quality. The results demonstrate that: 1) spatial-dependent GNNs model
outperforms traditional methods (Acc = 0.735, F1 = 0.732); 2) spatial structure
portrayed through sequential SVIs data significantly influences restoration
quality; 3) spaces with the same restoration quality exhibited distinct spatial
structures patterns. This study clarifies the association between spatial
structure and restoration quality, providing a new perspective to improve urban
well-being in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17365">Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning. (arXiv:2311.17365v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong-Lu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jianhua Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cewu Lu</a></p>
<p>Human reasoning can be understood as a cooperation between the intuitive,
associative "System-1" and the deliberative, logical "System-2". For existing
System-1-like methods in visual activity understanding, it is crucial to
integrate System-2 processing to improve explainability, generalization, and
data efficiency. One possible path of activity reasoning is building a symbolic
system composed of symbols and rules, where one rule connects multiple symbols,
implying human knowledge and reasoning abilities. Previous methods have made
progress, but are defective with limited symbols from handcraft and limited
rules from visual-based annotations, failing to cover the complex patterns of
activities and lacking compositional generalization. To overcome the defects,
we propose a new symbolic system with two ideal important properties:
broad-coverage symbols and rational rules. Collecting massive human knowledge
via manual annotations is expensive to instantiate this symbolic system.
Instead, we leverage the recent advancement of LLMs (Large Language Models) as
an approximation of the two ideal properties, i.e., Symbols from Large Language
Models (Symbol-LLM). Then, given an image, visual contents from the images are
extracted and checked as symbols and activity semantics are reasoned out based
on rules via fuzzy logic calculation. Our method shows superiority in extensive
activity understanding tasks. Code and data are available at
https://mvig-rhos.com/symbol_llm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17366">Generative Hierarchical Temporal Transformer for Hand Action Recognition and Motion Prediction. (arXiv:2311.17366v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yilin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Hao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1">Takehiko Ohkawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jia Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1">Yoichi Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>We present a novel framework that concurrently tackles hand action
recognition and 3D future hand motion prediction. While previous works focus on
either recognition or prediction, we propose a generative Transformer VAE
architecture to jointly capture both aspects, facilitating realistic motion
prediction by leveraging the short-term hand motion and long-term action
consistency observed across timestamps.To ensure faithful representation of the
semantic dependency and different temporal granularity of hand pose and action,
our framework is decomposed into two cascaded VAE blocks. The lower pose block
models short-span poses, while the upper action block models long-span action.
These are connected by a mid-level feature that represents sub-second series of
hand poses.Our framework is trained across multiple datasets, where pose and
action blocks are trained separately to fully utilize pose-action annotations
of different qualities. Evaluations show that on multiple datasets, the joint
modeling of recognition and prediction improves over separate solutions, and
the semantic and temporal hierarchy enables long-term pose and action modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17368">Two Scalable Approaches for Burned-Area Mapping Using U-Net and Landsat Imagery. (arXiv:2311.17368v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mancilla_Wulff_I/0/1/0/all/0/1">Ian Mancilla-Wulff</a>, <a href="http://arxiv.org/find/cs/1/au:+Carrasco_J/0/1/0/all/0/1">Jaime Carrasco</a>, <a href="http://arxiv.org/find/cs/1/au:+Pais_C/0/1/0/all/0/1">Cristobal Pais</a>, <a href="http://arxiv.org/find/cs/1/au:+Miranda_A/0/1/0/all/0/1">Alejandro Miranda</a>, <a href="http://arxiv.org/find/cs/1/au:+Weintraub_A/0/1/0/all/0/1">Andres Weintraub</a></p>
<p>Monitoring wildfires is an essential step in minimizing their impact on the
planet, understanding the many negative environmental, economic, and social
consequences. Recent advances in remote sensing technology combined with the
increasing application of artificial intelligence methods have improved
real-time, high-resolution fire monitoring. This study explores two proposed
approaches based on the U-Net model for automating and optimizing the
burned-area mapping process. Denoted 128 and AllSizes (AS), they are trained on
datasets with a different class balance by cropping input images to different
sizes. They are then applied to Landsat imagery and time-series data from two
fire-prone regions in Chile. The results obtained after enhancement of model
performance by hyperparameter optimization demonstrate the effectiveness of
both approaches. Tests based on 195 representative images of the study area
show that increasing dataset balance using the AS model yields better
performance. More specifically, AS exhibited a Dice Coefficient (DC) of 0.93,
an Omission Error (OE) of 0.086, and a Commission Error (CE) of 0.045, while
the 128 model achieved a DC of 0.86, an OE of 0.12, and a CE of 0.12. These
findings should provide a basis for further development of scalable automatic
burned-area mapping tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17389">360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries. (arXiv:2311.17389v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huajian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changkun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yipeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hui Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Braud_T/0/1/0/all/0/1">Tristan Braud</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1">Sai-Kit Yeung</a></p>
<p>Portable 360$^\circ$ cameras are becoming a cheap and efficient tool to
establish large visual databases. By capturing omnidirectional views of a
scene, these cameras could expedite building environment models that are
essential for visual localization. However, such an advantage is often
overlooked due to the lack of valuable datasets. This paper introduces a new
benchmark dataset, 360Loc, composed of 360$^\circ$ images with ground truth
poses for visual localization. We present a practical implementation of
360$^\circ$ mapping combining 360$^\circ$ images with lidar data to generate
the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that
explores the challenge of cross-device visual positioning, involving
360$^\circ$ reference frames, and query frames from pinhole, ultra-wide FoV
fisheye, and 360$^\circ$ cameras. We propose a virtual camera approach to
generate lower-FoV query frames from 360$^\circ$ images, which ensures a fair
comparison of performance among different query types in visual localization
tasks. We also extend this virtual camera approach to feature matching-based
and pose regression-based methods to alleviate the performance loss caused by
the cross-device domain gap, and evaluate its effectiveness against
state-of-the-art baselines. We demonstrate that omnidirectional visual
localization is more robust in challenging large-scale scenes with symmetries
and repetitive structures. These results provide new insights into 360-camera
mapping and omnidirectional visual localization with cross-device queries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17396">Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset. (arXiv:2311.17396v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1">Yujin Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Eunsue Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngchan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1">Yunseong Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Omer_K/0/1/0/all/0/1">Khalid Omer</a>, <a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1">Felix Heide</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Seung-Hwan Baek</a></p>
<p>Image datasets are essential not only in validating existing methods in
computer vision but also in developing new methods. Most existing image
datasets focus on trichromatic intensity images to mimic human vision. However,
polarization and spectrum, the wave properties of light that animals in harsh
environments and with limited brain capacity often rely on, remain
underrepresented in existing datasets. Although spectro-polarimetric datasets
exist, these datasets have insufficient object diversity, limited illumination
conditions, linear-only polarization data, and inadequate image count. Here, we
introduce two spectro-polarimetric datasets: trichromatic Stokes images and
hyperspectral Stokes images. These novel datasets encompass both linear and
circular polarization; they introduce multiple spectral channels; and they
feature a broad selection of real-world scenes. With our dataset in hand, we
analyze the spectro-polarimetric image statistics, develop efficient
representations of such high-dimensional data, and evaluate spectral dependency
of shape-from-polarization methods. As such, the proposed dataset promises a
foundation for data-driven spectro-polarimetric imaging and vision research.
Dataset and code will be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17404">VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models. (arXiv:2311.17404v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shicheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shuhuai Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Rundong Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lu Hou</a></p>
<p>The ability to perceive how objects change over time is a crucial ingredient
in human intelligence. However, current benchmarks cannot faithfully reflect
the temporal understanding abilities of video-language models (VidLMs) due to
the existence of static visual shortcuts. To remedy this issue, we present
VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal
Concept underStanding. Specifically, we first introduce a fine-grained taxonomy
of temporal concepts in natural language in order to diagnose the capability of
VidLMs to comprehend different temporal aspects. Furthermore, to disentangle
the correlation between static and temporal information, we generate
counterfactual video descriptions that differ from the original one only in the
specified temporal aspect. We employ a semi-automatic data collection framework
using large language models and human-in-the-loop annotation to obtain
high-quality counterfactual descriptions efficiently. Evaluation of
representative video-language understanding models confirms their deficiency in
temporal understanding, revealing the need for greater emphasis on the temporal
elements in video-language research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17408">Dynamic Dense Graph Convolutional Network for Skeleton-based Human Motion Prediction. (arXiv:2311.17408v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinshun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wanying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>Graph Convolutional Networks (GCN) which typically follows a neural message
passing framework to model dependencies among skeletal joints has achieved high
success in skeleton-based human motion prediction task. Nevertheless, how to
construct a graph from a skeleton sequence and how to perform message passing
on the graph are still open problems, which severely affect the performance of
GCN. To solve both problems, this paper presents a Dynamic Dense Graph
Convolutional Network (DD-GCN), which constructs a dense graph and implements
an integrated dynamic message passing. More specifically, we construct a dense
graph with 4D adjacency modeling as a comprehensive representation of motion
sequence at different levels of abstraction. Based on the dense graph, we
propose a dynamic message passing framework that learns dynamically from data
to generate distinctive messages reflecting sample-specific relevance among
nodes in the graph. Extensive experiments on benchmark Human 3.6M and CMU Mocap
datasets verify the effectiveness of our DD-GCN which obviously outperforms
state-of-the-art GCN-based methods, especially when using long-term and our
proposed extremely long-term protocol.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17409">Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation. (arXiv:2311.17409v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khungurn_P/0/1/0/all/0/1">Pramook Khungurn</a></p>
<p>We study the problem of creating a character model that can be controlled in
real time from a single image of an anime character. A solution to this problem
would greatly reduce the cost of creating avatars, computer games, and other
interactive applications.
</p>
<p>Talking Head Anime 3 (THA3) is an open source project that attempts to
directly addresses the problem. It takes as input (1) an image of an anime
character's upper body and (2) a 45-dimensional pose vector and outputs a new
image of the same character taking the specified pose. The range of possible
movements is expressive enough for personal avatars and certain types of game
characters. However, the system is too slow to generate animations in real time
on common PCs, and its image quality can be improved.
</p>
<p>In this paper, we improve THA3 in two ways. First, we propose new
architectures for constituent networks that rotate the character's head and
body based on U-Nets with attention that are widely used in modern generative
models. The new architectures consistently yield better image quality than the
THA3 baseline. Nevertheless, they also make the whole system much slower: it
takes up to 150 milliseconds to generate a frame. Second, we propose a
technique to distill the system into a small network (less than 2 MB) that can
generate 512x512 animation frames in real time (under 30 FPS) using consumer
gaming GPUs while keeping the image quality close to that of the full system.
This improvement makes the whole system practical for real-time applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17425">SpeechAct: Towards Generating Whole-body Motion from Speech. (arXiv:2311.17425v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinsong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a></p>
<p>This paper addresses the problem of generating whole-body motion from speech.
Despite great successes, prior methods still struggle to produce reasonable and
diverse whole-body motions from speech. This is due to their reliance on
suboptimal representations and a lack of strategies for generating diverse
results. To address these challenges, we present a novel hybrid point
representation to achieve accurate and continuous motion generation, e.g.,
avoiding foot skating, and this representation can be transformed into an
easy-to-use representation, i.e., SMPL-X body mesh, for many applications. To
generate whole-body motion from speech, for facial motion, closely tied to the
audio signal, we introduce an encoder-decoder architecture to achieve
deterministic outcomes. However, for the body and hands, which have weaker
connections to the audio signal, we aim to generate diverse yet reasonable
motions. To boost diversity in motion generation, we propose a contrastive
motion learning method to encourage the model to produce more distinctive
representations. Specifically, we design a robust VQ-VAE to learn a quantized
motion codebook using our hybrid representation. Then, we regress the motion
representation from the audio signal by a translation model employing our
contrastive motion learning method. Experimental results validate the superior
performance and the correctness of our model. The project page is available for
research purposes at <a href="http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17428">SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation. (arXiv:2311.17428v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiaoyan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wu Liu</a></p>
<p>Multi-modal human action segmentation is a critical and challenging task with
a wide range of applications. Nowadays, the majority of approaches concentrate
on the fusion of dense signals (i.e., RGB, optical flow, and depth maps).
However, the potential contributions of sparse IoT sensor signals, which can be
crucial for achieving accurate recognition, have not been fully explored. To
make up for this, we introduce a Sparse signalguided Transformer (SigFormer) to
combine both dense and sparse signals. We employ mask attention to fuse
localized features by constraining cross-attention within the regions where
sparse signals are valid. However, since sparse signals are discrete, they lack
sufficient information about the temporal action boundaries. Therefore, in
SigFormer, we propose to emphasize the boundary information at two stages to
alleviate this problem. In the first feature extraction stage, we introduce an
intermediate bottleneck module to jointly learn both category and boundary
features of each dense modality through the inner loss functions. After the
fusion of dense modalities and sparse signals, we then devise a two-branch
architecture that explicitly models the interrelationship between action
category and temporal boundary. Experimental results demonstrate that SigFormer
outperforms the state-of-the-art approaches on a multi-modal action
segmentation dataset from real industrial environments, reaching an outstanding
F1 score of 0.958. The codes and pre-trained models have been available at
https://github.com/LIUQI-creat/SigFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17434">Group-wise Sparse and Explainable Adversarial Attacks. (arXiv:2311.17434v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadiku_S/0/1/0/all/0/1">Shpresim Sadiku</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1">Moritz Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1">Sebastian Pokutta</a></p>
<p>Sparse adversarial attacks fool deep neural networks (DNNs) through minimal
pixel perturbations, typically regularized by the $\ell_0$ norm. Recent efforts
have replaced this norm with a structural sparsity regularizer, such as the
nuclear group norm, to craft group-wise sparse adversarial attacks. The
resulting perturbations are thus explainable and hold significant practical
relevance, shedding light on an even greater vulnerability of DNNs than
previously anticipated. However, crafting such attacks poses an optimization
challenge, as it involves computing norms for groups of pixels within a
non-convex objective. In this paper, we tackle this challenge by presenting an
algorithm that simultaneously generates group-wise sparse attacks within
semantically meaningful areas of an image. In each iteration, the core
operation of our algorithm involves the optimization of a quasinorm adversarial
loss. This optimization is achieved by employing the $1/2$-quasinorm proximal
operator for some iterations, a method tailored for nonconvex programming.
Subsequently, the algorithm transitions to a projected Nesterov's accelerated
gradient descent with $2$-norm regularization applied to perturbation
magnitudes. We rigorously evaluate the efficacy of our novel attack in both
targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets.
When compared to state-of-the-art methods, our attack consistently results in a
remarkable increase in group-wise sparsity, e.g., an increase of $48.12\%$ on
CIFAR-10 and $40.78\%$ on ImageNet (average case, targeted attack), all while
maintaining lower perturbation magnitudes. Notably, this performance is
complemented by a significantly faster computation time and a $100\%$ attack
success rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17435">MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning. (arXiv:2311.17435v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1">Kevin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chung-Ching Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a></p>
<p>We present MM-Narrator, a novel system leveraging GPT-4 with multimodal
in-context learning for the generation of audio descriptions (AD). Unlike
previous methods that primarily focused on downstream fine-tuning with short
video clips, MM-Narrator excels in generating precise audio descriptions for
videos of extensive lengths, even beyond hours, in an autoregressive manner.
This capability is made possible by the proposed memory-augmented generation
process, which effectively utilizes both the short-term textual context and
long-term visual memory through an efficient register-and-recall mechanism.
These contextual memories compile pertinent past information, including
storylines and character identities, ensuring an accurate tracking and
depicting of story-coherent and character-centric audio descriptions.
Maintaining the training-free design of MM-Narrator, we further propose a
complexity-based demonstration selection strategy to largely enhance its
multi-step reasoning capability via few-shot multimodal in-context learning
(MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator
consistently outperforms both the existing fine-tuning-based approaches and
LLM-based approaches in most scenarios, as measured by standard evaluation
metrics. Additionally, we introduce the first segment-based evaluator for
recurrent text generation. Empowered by GPT-4, this evaluator comprehensively
reasons and marks AD generation performance in various extendable dimensions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17449">Weakly-semi-supervised object detection in remotely sensed imagery. (arXiv:2311.17449v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Ji Hun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1">Jeremy Irvin</a>, <a href="http://arxiv.org/find/cs/1/au:+Behar_B/0/1/0/all/0/1">Beri Kohen Behar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1">Ha Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Samavedam_R/0/1/0/all/0/1">Raghav Samavedam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_Q/0/1/0/all/0/1">Quentin Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1">Andrew Y. Ng</a></p>
<p>Deep learning for detecting objects in remotely sensed imagery can enable new
technologies for important applications including mitigating climate change.
However, these models often require large datasets labeled with bounding box
annotations which are expensive to curate, prohibiting the development of
models for new tasks and geographies. To address this challenge, we develop
weakly-semi-supervised object detection (WSSOD) models on remotely sensed
imagery which can leverage a small amount of bounding boxes together with a
large amount of point labels that are easy to acquire at scale in geospatial
data. We train WSSOD models which use large amounts of point-labeled images
with varying fractions of bounding box labeled images in FAIR1M and a wind
turbine detection dataset, and demonstrate that they substantially outperform
fully supervised models trained with the same amount of bounding box labeled
images on both datasets. Furthermore, we find that the WSSOD models trained
with 2-10x fewer bounding box labeled images can perform similarly to or
outperform fully supervised models trained on the full set of bounding-box
labeled images. We believe that the approach can be extended to other remote
sensing tasks to reduce reliance on bounding box labels and increase
development of models for impactful applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17450">Continual Learning for Image Segmentation with Dynamic Query. (arXiv:2311.17450v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1">Lianlei Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>Image segmentation based on continual learning exhibits a critical drop of
performance, mainly due to catastrophic forgetting and background shift, as
they are required to incorporate new classes continually. In this paper, we
propose a simple, yet effective Continual Image Segmentation method with
incremental Dynamic Query (CISDQ), which decouples the representation learning
of both old and new knowledge with lightweight query embedding. CISDQ mainly
includes three contributions: 1) We define dynamic queries with adaptive
background class to exploit past knowledge and learn future classes naturally.
2) CISDQ proposes a class/instance-aware Query Guided Knowledge Distillation
strategy to overcome catastrophic forgetting by capturing the inter-class
diversity and intra-class identity. 3) Apart from semantic segmentation, CISDQ
introduce the continual learning for instance segmentation in which
instance-wise labeling and supervision are considered. Extensive experiments on
three datasets for two tasks (i.e., continual semantic and instance
segmentation are conducted to demonstrate that CISDQ achieves the
state-of-the-art performance, specifically, obtaining 4.4% and 2.9% mIoU
improvements for the ADE 100-10 (6 steps) setting and ADE 100-5 (11 steps)
setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17456">DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Diffusion Model. (arXiv:2311.17456v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiuming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Weicai Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chaokang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jinru Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guofeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1">Dalong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>Scene flow estimation, which aims to predict per-point 3D displacements of
dynamic scenes, is a fundamental task in the computer vision field. However,
previous works commonly suffer from unreliable correlation caused by locally
constrained searching ranges, and struggle with accumulated inaccuracy arising
from the coarse-to-fine structure. To alleviate these problems, we propose a
novel uncertainty-aware scene flow estimation network (DifFlow3D) with the
diffusion probabilistic model. Iterative diffusion-based refinement is designed
to enhance the correlation robustness and resilience to challenging cases,
e.g., dynamics, noisy inputs, repetitive patterns, etc. To restrain the
generation diversity, three key flow-related features are leveraged as
conditions in our diffusion model. Furthermore, we also develop an uncertainty
estimation module within diffusion to evaluate the reliability of estimated
scene flow. Our DifFlow3D achieves state-of-the-art performance, with 6.7\% and
19.1\% EPE3D reduction respectively on FlyingThings3D and KITTI 2015 datasets.
Notably, our method achieves an unprecedented millimeter-level accuracy
(0.0089m in EPE3D) on the KITTI dataset. Additionally, our diffusion-based
refinement paradigm can be readily integrated as a plug-and-play module into
existing scene flow networks, significantly increasing their estimation
accuracy. Codes will be released later.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17460">W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera Calibration and Orientation Correction. (arXiv:2311.17460v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yunlian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jinhui Tang</a></p>
<p>For a long time, in the field of reconstructing 3D human bodies from
monocular images, most methods opted to simplify the task by minimizing the
influence of the camera. Using a coarse focal length setting results in the
reconstructed bodies not aligning well with distorted images. Ignoring camera
rotation leads to an unrealistic reconstructed body pose in world space.
Consequently, existing methods' application scenarios are confined to
controlled environments. And they struggle to achieve accurate and reasonable
reconstruction in world space when confronted with complex and diverse
in-the-wild images. To address the above issues, we propose W-HMR, which
decouples global body recovery into camera calibration, local body recovery and
global body orientation correction. We design the first weak-supervised camera
calibration method for body distortion, eliminating dependence on focal length
labels and achieving finer mesh-image alignment. We propose a novel orientation
correction module to allow the reconstructed human body to remain normal in
world space. Decoupling body orientation and body pose enables our model to
consider the accuracy in camera coordinate and the reasonableness in world
coordinate simultaneously, expanding the range of applications. As a result,
W-HMR achieves high-quality reconstruction in dual coordinate systems,
particularly in challenging scenes. Codes will be released on
https://yw0208.github.io/ after publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17461">When StyleGAN Meets Stable Diffusion: a $\mathscr{W}_+$ Adapter for Personalized Image Generation. (arXiv:2311.17461v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xinyu Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>Text-to-image diffusion models have remarkably excelled in producing diverse,
high-quality, and photo-realistic images. This advancement has spurred a
growing interest in incorporating specific identities into generated content.
Most current methods employ an inversion approach to embed a target visual
concept into the text embedding space using a single reference image. However,
the newly synthesized faces either closely resemble the reference image in
terms of facial attributes, such as expression, or exhibit a reduced capacity
for identity preservation. Text descriptions intended to guide the facial
attributes of the synthesized face may fall short, owing to the intricate
entanglement of identity information with identity-irrelevant facial attributes
derived from the reference image. To address these issues, we present the novel
use of the extended StyleGAN embedding space $\mathcal{W}_+$, to achieve
enhanced identity preservation and disentanglement for diffusion models. By
aligning this semantically meaningful human face latent space with
text-to-image diffusion models, we succeed in maintaining high fidelity in
identity preservation, coupled with the capacity for semantic editing.
Additionally, we propose new training objectives to balance the influences of
both prompt and identity conditions, ensuring that the identity-irrelevant
background remains unaffected during facial attribute modifications. Extensive
experiments reveal that our method adeptly generates personalized text-to-image
outputs that are not only compatible with prompt descriptions but also amenable
to common StyleGAN editing directions in diverse settings. Our source code will
be available at \url{https://github.com/csxmli2016/w-plus-adapter}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17465">AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents. (arXiv:2311.17465v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Duomin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1">Bin Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yu Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baoyuan Wang</a></p>
<p>In this study, our goal is to create interactive avatar agents that can
autonomously plan and animate nuanced facial movements realistically, from both
visual and behavioral perspectives. Given high-level inputs about the
environment and agent profile, our framework harnesses LLMs to produce a series
of detailed text descriptions of the avatar agents' facial motions. These
descriptions are then processed by our task-agnostic driving engine into motion
token sequences, which are subsequently converted into continuous motion
embeddings that are further consumed by our standalone neural-based renderer to
generate the final photorealistic avatar animations. These streamlined
processes allow our framework to adapt to a variety of non-verbal avatar
interactions, both monadic and dyadic. Our extensive study, which includes
experiments on both newly compiled and existing datasets featuring two types of
agents -- one capable of monadic interaction with the environment, and the
other designed for dyadic conversation -- validates the effectiveness and
versatility of our approach. To our knowledge, we advanced a leap step by
combining LLMs and neural rendering for generalized non-verbal prediction and
photo-realistic rendering of avatar agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17466">Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification. (arXiv:2311.17466v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keum_S/0/1/0/all/0/1">Seongho Keum</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sanghyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Soojeong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a></p>
<p>Whole slide image (WSI) classification requires repetitive zoom-in and out
for pathologists, as only small portions of the slide may be relevant to
detecting cancer. Due to the lack of patch-level labels, multiple instance
learning (MIL) is a common practice for training a WSI classifier. One of the
challenges in MIL for WSIs is the weak supervision coming only from the
slide-level labels, often resulting in severe overfitting. In response,
researchers have considered adopting patch-level augmentation or applying mixup
augmentation, but their applicability remains unverified. Our approach augments
the training dataset by sampling a subset of patches in the WSI without
significantly altering the underlying semantics of the original slides.
Additionally, we introduce an efficient model (Slot-MIL) that organizes patches
into a fixed number of slots, the abstract representation of patches, using an
attention mechanism. We empirically demonstrate that the subsampling
augmentation helps to make more informative slots by restricting the
over-concentration of attention and to improve interpretability. Finally, we
illustrate that combining our attention-based aggregation model with
subsampling and mixup, which has shown limited compatibility in existing MIL
methods, can enhance both generalization and calibration. Our proposed methods
achieve the state-of-the-art performance across various benchmark datasets
including class imbalance and distribution shifts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17475">CLiSA: A Hierarchical Hybrid Transformer Model using Orthogonal Cross Attention for Satellite Image Cloud Segmentation. (arXiv:2311.17475v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1">Subhajit Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ashutosh Gupta</a></p>
<p>Clouds in optical satellite images are a major concern since their presence
hinders the ability to carry accurate analysis as well as processing. Presence
of clouds also affects the image tasking schedule and results in wastage of
valuable storage space on ground as well as space-based systems. Due to these
reasons, deriving accurate cloud masks from optical remote-sensing images is an
important task. Traditional methods such as threshold-based, spatial filtering
for cloud detection in satellite images suffer from lack of accuracy. In recent
years, deep learning algorithms have emerged as a promising approach to solve
image segmentation problems as it allows pixel-level classification and
semantic-level segmentation. In this paper, we introduce a deep-learning model
based on hybrid transformer architecture for effective cloud mask generation
named CLiSA - Cloud segmentation via Lipschitz Stable Attention network. In
this context, we propose an concept of orthogonal self-attention combined with
hierarchical cross attention model, and we validate its Lipschitz stability
theoretically and empirically. We design the whole setup under adversarial
setting in presence of Lov\'asz-Softmax loss. We demonstrate both qualitative
and quantitative outcomes for multiple satellite image datasets including
Landsat-8, Sentinel-2, and Cartosat-2s. Performing comparative study we show
that our model performs preferably against other state-of-the-art methods and
also provides better generalization in precise cloud extraction from satellite
multi-spectral (MX) images. We also showcase different ablation studies to
endorse our choices corresponding to different architectural elements and
objective functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17486">Non-Visible Light Data Synthesis and Application: A Case Study for Synthetic Aperture Radar Imagery. (arXiv:2311.17486v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zichen Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhaozheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qianru Sun</a></p>
<p>We explore the "hidden" ability of large-scale pre-trained image generation
models, such as Stable Diffusion and Imagen, in non-visible light domains,
taking Synthetic Aperture Radar (SAR) data for a case study. Due to the
inherent challenges in capturing satellite data, acquiring ample SAR training
samples is infeasible. For instance, for a particular category of ship in the
open sea, we can collect only few-shot SAR images which are too limited to
derive effective ship recognition models. If large-scale models pre-trained
with regular images can be adapted to generating novel SAR images, the problem
is solved. In preliminary study, we found that fine-tuning these models with
few-shot SAR images is not working, as the models can not capture the two
primary differences between SAR and regular images: structure and modality. To
address this, we propose a 2-stage low-rank adaptation method, and we call it
2LoRA. In the first stage, the model is adapted using aerial-view regular image
data (whose structure matches SAR), followed by the second stage where the base
model from the first stage is further adapted using SAR modality data.
Particularly in the second stage, we introduce a novel prototype LoRA (pLoRA),
as an improved version of 2LoRA, to resolve the class imbalance problem in SAR
datasets. For evaluation, we employ the resulting generation model to
synthesize additional SAR data. This augmentation, when integrated into the
training process of SAR classification as well as segmentation models, yields
notably improved performance for minor classes
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17491">Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation. (arXiv:2311.17491v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiuming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>LiDAR point cloud semantic segmentation enables the robots to obtain
fine-grained semantic information of the surrounding environment. Recently,
many works project the point cloud onto the 2D image and adopt the 2D
Convolutional Neural Networks (CNNs) or vision transformer for LiDAR point
cloud semantic segmentation. However, since more than one point can be
projected onto the same 2D position but only one point can be preserved, the
previous 2D image-based segmentation methods suffer from inevitable quantized
information loss. To avoid quantized information loss, in this paper, we
propose a novel spherical frustum structure. The points projected onto the same
2D position are preserved in the spherical frustums. Moreover, we propose a
memory-efficient hash-based representation of spherical frustums. Through the
hash-based representation, we propose the Spherical Frustum sparse Convolution
(SFC) and Frustum Fast Point Sampling (F2PS) to convolve and sample the points
stored in spherical frustums respectively. Finally, we present the Spherical
Frustum sparse Convolution Network (SFCNet) to adopt 2D CNNs for LiDAR point
cloud semantic segmentation without quantized information loss. Extensive
experiments on the SemanticKITTI and nuScenes datasets demonstrate that our
SFCNet outperforms the 2D image-based semantic segmentation methods based on
conventional spherical projection. The source code will be released later.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17493">Towards Higher Ranks via Adversarial Weight Pruning. (arXiv:2311.17493v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuchuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hanting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tianyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a></p>
<p>Convolutional Neural Networks (CNNs) are hard to deploy on edge devices due
to its high computation and storage complexities. As a common practice for
model compression, network pruning consists of two major categories:
unstructured and structured pruning, where unstructured pruning constantly
performs better. However, unstructured pruning presents a structured pattern at
high pruning rates, which limits its performance. To this end, we propose a
Rank-based PruninG (RPG) method to maintain the ranks of sparse weights in an
adversarial manner. In each step, we minimize the low-rank approximation error
for the weight matrices using singular value decomposition, and maximize their
distance by pushing the weight matrices away from its low rank approximation.
This rank-based optimization objective guides sparse weights towards a
high-rank topology. The proposed method is conducted in a gradual pruning
fashion to stabilize the change of rank during training. Experimental results
on various datasets and different tasks demonstrate the effectiveness of our
algorithm in high sparsity. The proposed RPG outperforms the state-of-the-art
performance by 1.13% top-1 accuracy on ImageNet in ResNet-50 with 98% sparsity.
The codes are available at
https://github.com/huawei-noah/Efficient-Computing/tree/master/Pruning/RPG and
https://gitee.com/mindspore/models/tree/master/research/cv/RPG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17504">PViT-6D: Overclocking Vision Transformers for 6D Pose Estimation with Confidence-Level Prediction and Pose Tokens. (arXiv:2311.17504v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stapf_S/0/1/0/all/0/1">Sebastian Stapf</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauernfeind_T/0/1/0/all/0/1">Tobias Bauernfeind</a>, <a href="http://arxiv.org/find/cs/1/au:+Riboldi_M/0/1/0/all/0/1">Marco Riboldi</a></p>
<p>In the current state of 6D pose estimation, top-performing techniques depend
on complex intermediate correspondences, specialized architectures, and
non-end-to-end algorithms. In contrast, our research reframes the problem as a
straightforward regression task by exploring the capabilities of Vision
Transformers for direct 6D pose estimation through a tailored use of
classification tokens. We also introduce a simple method for determining pose
confidence, which can be readily integrated into most 6D pose estimation
frameworks. This involves modifying the transformer architecture by decreasing
the number of query elements based on the network's assessment of the scene
complexity. Our method that we call Pose Vision Transformer or PViT-6D provides
the benefits of simple implementation and being end-to-end learnable while
outperforming current state-of-the-art methods by +0.3% ADD(-S) on
Linemod-Occlusion and +2.7% ADD(-S) on the YCB-V dataset. Moreover, our method
enhances both the model's interpretability and the reliability of its
performance during inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17510">StructRe: Rewriting for Structured Shape Modeling. (arXiv:2311.17510v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1">Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiepeng/0/1/0/all/0/1">Jiepeng</a>, Pan, Hao, Liu, <a href="http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1">Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong/0/1/0/all/0/1">Tong</a>, Xin, <a href="http://arxiv.org/find/cs/1/au:+Komura/0/1/0/all/0/1">Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Taku/0/1/0/all/0/1">Taku</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1">Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wenping/0/1/0/all/0/1">Wenping</a></p>
<p>Man-made 3D shapes are naturally organized in parts and hierarchies; such
structures provide important constraints for shape reconstruction and
generation. Modeling shape structures is difficult, because there can be
multiple hierarchies for a given shape, causing ambiguity, and across different
categories the shape structures are correlated with semantics, limiting
generalization. We present StructRe, a structure rewriting system, as a novel
approach to structured shape modeling. Given a 3D object represented by points
and components, StructRe can rewrite it upward into more concise structures, or
downward into more detailed structures; by iterating the rewriting process,
hierarchies are obtained. Such a localized rewriting process enables
probabilistic modeling of ambiguous structures and robust generalization across
object categories. We train StructRe on PartNet data and show its
generalization to cross-category and multiple object hierarchies, and test its
extension to ShapeNet. We also demonstrate the benefits of probabilistic and
generalizable structure modeling for shape reconstruction, generation and
editing tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17515">Fusion of Single and Integral Multispectral Aerial Images. (arXiv:2311.17515v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Youssef_M/0/1/0/all/0/1">Mohamed Youssef</a>, <a href="http://arxiv.org/find/eess/1/au:+Bimber_O/0/1/0/all/0/1">Oliver Bimber</a></p>
<p>We present a novel hybrid (model- and learning-based) architecture for fusing
the most significant features from conventional aerial images and integral
aerial images that result from synthetic aperture sensing for removing
occlusion caused by dense vegetation. It combines the environment's spatial
references with features of unoccluded targets. Our method out-beats the
state-of-the-art, does not require manually tuned parameters, can be extended
to an arbitrary number and combinations of spectral channels, and is
reconfigurable to address different use-cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17516">MMA-Diffusion: MultiModal Attack on Diffusion Models. (arXiv:2311.17516v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yijun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaosen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>In recent years, Text-to-Image (T2I) models have seen remarkable
advancements, gaining widespread adoption. However, this progress has
inadvertently opened avenues for potential misuse, particularly in generating
inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces
MMA-Diffusion, a framework that presents a significant and realistic threat to
the security of T2I models by effectively circumventing current defensive
measures in both open-source models and commercial online services. Unlike
previous approaches, MMA-Diffusion leverages both textual and visual modalities
to bypass safeguards like prompt filters and post-hoc safety checkers, thus
exposing and highlighting the vulnerabilities in existing defense mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17518">The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding. (arXiv:2311.17518v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bianchi_L/0/1/0/all/0/1">Lorenzo Bianchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Carrara_F/0/1/0/all/0/1">Fabio Carrara</a>, <a href="http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1">Nicola Messina</a>, <a href="http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1">Claudio Gennaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1">Fabrizio Falchi</a></p>
<p>Recent advancements in large vision-language models enabled visual object
detection in open-vocabulary scenarios, where object classes are defined in
free-text formats during inference. In this paper, we aim to probe the
state-of-the-art methods for open-vocabulary object detection to determine to
what extent they understand fine-grained properties of objects and their parts.
To this end, we introduce an evaluation protocol based on dynamic vocabulary
generation to test whether models detect, discern, and assign the correct
fine-grained description to objects in the presence of hard-negative classes.
We contribute with a benchmark suite of increasing difficulty and probing
different properties like color, pattern, and material. We further enhance our
investigation by evaluating several state-of-the-art open-vocabulary object
detectors using the proposed protocol and find that most existing solutions,
which shine in standard open-vocabulary benchmarks, struggle to accurately
capture and distinguish finer object details. We conclude the paper by
highlighting the limitations of current methodologies and exploring promising
research directions to overcome the discovered drawbacks. Data and code are
available at https://github.com/lorebianchi98/FG-OVD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17524">Improving Stability during Upsampling -- on the Importance of Spatial Context. (arXiv:2311.17524v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agnihotri_S/0/1/0/all/0/1">Shashank Agnihotri</a>, <a href="http://arxiv.org/find/cs/1/au:+Grabinski_J/0/1/0/all/0/1">Julia Grabinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1">Margret Keuper</a></p>
<p>State-of-the-art models for pixel-wise prediction tasks such as image
restoration, image segmentation, or disparity estimation, involve several
stages of data resampling, in which the resolution of feature maps is first
reduced to aggregate information and then sequentially increased to generate a
high-resolution output. Several previous works have investigated the effect of
artifacts that are invoked during downsampling and diverse cures have been
proposed that facilitate to improve prediction stability and even robustness
for image classification. However, equally relevant, artifacts that arise
during upsampling have been less discussed. This is significantly relevant as
upsampling and downsampling approaches face fundamentally different challenges.
While during downsampling, aliases and artifacts can be reduced by blurring
feature maps, the emergence of fine details is crucial during upsampling.
Blurring is therefore not an option and dedicated operations need to be
considered. In this work, we are the first to explore the relevance of context
during upsampling by employing convolutional upsampling operations with
increasing kernel size while keeping the encoder unchanged. We find that
increased kernel sizes can in general improve the prediction stability in tasks
such as image restoration or image segmentation, while a block that allows for
a combination of small-size kernels for fine details and large-size kernels for
artifact removal and increased context yields the best results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17525">A publicly available vessel segmentation algorithm for SLO images. (arXiv:2311.17525v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Threlfall_A/0/1/0/all/0/1">Adam Threlfall</a>, <a href="http://arxiv.org/find/eess/1/au:+Gibbon_S/0/1/0/all/0/1">Samuel Gibbon</a>, <a href="http://arxiv.org/find/eess/1/au:+Cameron_J/0/1/0/all/0/1">James Cameron</a>, <a href="http://arxiv.org/find/eess/1/au:+MacGillivray_T/0/1/0/all/0/1">Tom MacGillivray</a></p>
<p>Background and Objective: Infra-red scanning laser ophthalmoscope (IRSLO)
images are akin to colour fundus photographs in displaying the posterior pole
and retinal vasculature fine detail. While there are many trained networks
readily available for retinal vessel segmentation in colour fundus photographs,
none cater to IRSLO images. Accordingly, we aimed to develop (and release as
open source) a vessel segmentation algorithm tailored specifically to IRSLO
images. Materials and Methods: We used 23 expertly annotated IRSLO images from
the RAVIR dataset, combined with 7 additional images annotated in-house. We
trained a U-Net (convolutional neural network) to label pixels as 'vessel' or
'background'. Results: On an unseen test set (4 images), our model achieved an
AUC of 0.981, and an AUPRC of 0.815. Upon thresholding, it achieved a
sensitivity of 0.844, a specificity of 0.983, and an F1 score of 0.857.
Conclusion: We have made our automatic segmentation algorithm publicly
available and easy to use. Researchers can use the generated vessel maps to
compute metrics such as fractal dimension and vessel density.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17528">HiDiffusion: Unlocking High-Resolution Creativity and Efficiency in Low-Resolution Trained Diffusion Models. (arXiv:2311.17528v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhaowei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhenyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1">Wengang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiajun Liang</a></p>
<p>We introduce HiDiffusion, a tuning-free framework comprised of
Resolution-Aware U-Net (RAU-Net) and Modified Shifted Window Multi-head
Self-Attention (MSW-MSA) to enable pretrained large text-to-image diffusion
models to efficiently generate high-resolution images (e.g. 1024$\times$1024)
that surpass the training image resolution. Pretrained diffusion models
encounter unreasonable object duplication in generating images beyond the
training image resolution. We attribute it to the mismatch between the feature
map size of high-resolution images and the receptive field of U-Net's
convolution. To address this issue, we propose a simple yet scalable method
named RAU-Net. RAU-Net dynamically adjusts the feature map size to match the
convolution's receptive field in the deep block of U-Net. Another obstacle in
high-resolution synthesis is the slow inference speed of U-Net. Our
observations reveal that the global self-attention in the top block, which
exhibits locality, however, consumes the majority of computational resources.
To tackle this issue, we propose MSW-MSA. Unlike previous window attention
mechanisms, our method uses a much larger window size and dynamically shifts
windows to better accommodate diffusion models. Extensive experiments
demonstrate that our HiDiffusion can scale diffusion models to generate
1024$\times$1024, 2048$\times$2048, or even 4096$\times$4096 resolution images,
while simultaneously reducing inference time by 40\%-60\%, achieving
state-of-the-art performance on high-resolution image synthesis. The most
significant revelation of our work is that a pretrained diffusion model on
low-resolution images is scalable for high-resolution generation without
further tuning. We hope this revelation can provide insights for future
research on the scalability of diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17532">Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation. (arXiv:2311.17532v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xingqun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiahao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1">Xiaowei Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Wenhan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qifeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a></p>
<p>Generating vivid and emotional 3D co-speech gestures is crucial for virtual
avatar animation in human-machine interaction applications. While the existing
methods enable generating the gestures to follow a single emotion label, they
overlook that long gesture sequence modeling with emotion transition is more
practical in real scenes. In addition, the lack of large-scale available
datasets with emotional transition speech and corresponding 3D human gestures
also limits the addressing of this task. To fulfill this goal, we first
incorporate the ChatGPT-4 and an audio inpainting approach to construct the
high-fidelity emotion transition human speeches. Considering obtaining the
realistic 3D pose annotations corresponding to the dynamically inpainted
emotion transition audio is extremely difficult, we propose a novel weakly
supervised training strategy to encourage authority gesture transitions.
Specifically, to enhance the coordination of transition gestures w.r.t
different emotional ones, we model the temporal association representation
between two different emotional gesture sequences as style guidance and infuse
it into the transition generation. We further devise an emotion mixture
mechanism that provides weak supervision based on a learnable mixed emotion
label for transition gestures. Last, we present a keyframe sampler to supply
effective initial posture cues in long sequences, enabling us to generate
diverse gestures. Extensive experiments demonstrate that our method outperforms
the state-of-the-art models constructed by adapting single emotion-conditioned
counterparts on our newly defined emotion transition task and datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17536">Smooth Video Synthesis with Noise Constraints on Diffusion Models for One-shot Video Tuning. (arXiv:2311.17536v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Liang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Haoran Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Ruisi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1">Linxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chaotian Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinglin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Boxi Wu</a></p>
<p>Recent one-shot video tuning methods, which fine-tune the network on a
specific video based on pre-trained text-to-image models (e.g., Stable
Diffusion), are popular in the community because of the flexibility. However,
these methods often produce videos marred by incoherence and inconsistency. To
address these limitations, this paper introduces a simple yet effective noise
constraint across video frames. This constraint aims to regulate noise
predictions across their temporal neighbors, resulting in smooth latents. It
can be simply included as a loss term during the training phase. By applying
the loss to existing one-shot video tuning methods, we significantly improve
the overall consistency and smoothness of the generated videos. Furthermore, we
argue that current video evaluation metrics inadequately capture smoothness. To
address this, we introduce a novel metric that considers detailed features and
their temporal dynamics. Experimental results validate the effectiveness of our
approach in producing smoother videos on various one-shot video tuning
baselines. The source codes and video demos are available at
\href{https://github.com/SPengLiang/SmoothVideo}{https://github.com/SPengLiang/SmoothVideo}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17546">VINNA for Neonates -- Orientation Independence through Latent Augmentations. (arXiv:2311.17546v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Henschel_L/0/1/0/all/0/1">Leonie Henschel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugler_D/0/1/0/all/0/1">David K&#xfc;gler</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollei_L/0/1/0/all/0/1">Lilla Z&#xf6;llei</a>, <a href="http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1">Martin Reuter</a></p>
<p>Fast and accurate segmentation of neonatal brain images is highly desired to
better understand and detect changes during development and disease. Yet, the
limited availability of ground truth datasets, lack of standardized acquisition
protocols, and wide variations of head positioning pose challenges for method
development. A few automated image analysis pipelines exist for newborn brain
MRI segmentation, but they often rely on time-consuming procedures and require
resampling to a common resolution, subject to loss of information due to
interpolation and down-sampling. Without registration and image resampling,
variations with respect to head positions and voxel resolutions have to be
addressed differently. In deep-learning, external augmentations are
traditionally used to artificially expand the representation of spatial
variability, increasing the training dataset size and robustness. However,
these transformations in the image space still require resampling, reducing
accuracy specifically in the context of label interpolation. We recently
introduced the concept of resolution-independence with the Voxel-size
Independent Neural Network framework, VINN. Here, we extend this concept by
additionally shifting all rigid-transforms into the network architecture with a
four degree of freedom (4-DOF) transform module, enabling resolution-aware
internal augmentations (VINNA). In this work we show that VINNA (i)
significantly outperforms state-of-the-art external augmentation approaches,
(ii) effectively addresses the head variations present specifically in newborn
datasets, and (iii) retains high segmentation accuracy across a range of
resolutions (0.5-1.0 mm). The 4-DOF transform module is a powerful, general
approach to implement spatial augmentation without requiring image or label
interpolation. The specific network application to newborns will be made
publicly available as VINNA4neonates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17552">An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance. (arXiv:2311.17552v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pendharkar_G/0/1/0/all/0/1">Gaurav Pendharkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Micheal_A/0/1/0/all/0/1">A.Ancy Micheal</a>, <a href="http://arxiv.org/find/cs/1/au:+Misquitta_J/0/1/0/all/0/1">Jason Misquitta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaippada_R/0/1/0/all/0/1">Ranjeesh Kaippada</a></p>
<p>Tiger conservation necessitates the strategic deployment of multifaceted
initiatives encompassing the preservation of ecological habitats, anti-poaching
measures, and community involvement for sustainable growth in the tiger
population. With the advent of artificial intelligence, tiger surveillance can
be automated using object detection. In this paper, an accurate illumination
invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger
detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without
illumination enhancement. The illumination enhancement improves the mAP by
0.7%. The approaches elevate the state-of-the-art performance on the ATRW
dataset by approximately 6% to 7%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17571">LGFCTR: Local and Global Feature Convolutional Transformer for Image Matching. (arXiv:2311.17571v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Wenhao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jie Jiang</a></p>
<p>Image matching that finding robust and accurate correspondences across images
is a challenging task under extreme conditions. Capturing local and global
features simultaneously is an important way to mitigate such an issue but
recent transformer-based decoders were still stuck in the issues that CNN-based
encoders only extract local features and the transformers lack locality.
Inspired by the locality and implicit positional encoding of convolutions, a
novel convolutional transformer is proposed to capture both local contexts and
global structures more sufficiently for detector-free matching. Firstly, a
universal FPN-like framework captures global structures in self-encoder as well
as cross-decoder by transformers and compensates local contexts as well as
implicit positional encoding by convolutions. Secondly, a novel convolutional
transformer module explores multi-scale long range dependencies by a novel
multi-scale attention and further aggregates local information inside
dependencies for enhancing locality. Finally, a novel regression-based
sub-pixel refinement module exploits the whole fine-grained window features for
fine-level positional deviation regression. The proposed method achieves
superior performances on a wide range of benchmarks. The code will be available
on https://github.com/zwh0527/LGFCTR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17583">CLIPC8: Face liveness detection algorithm based on image-text pairs and contrastive learning. (arXiv:2311.17583v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yurong Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Wenzhe Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a></p>
<p>Face recognition technology is widely used in the financial field, and
various types of liveness attack behaviors need to be addressed. Existing
liveness detection algorithms are trained on specific training datasets and
tested on testing datasets, but their performance and robustness in
transferring to unseen datasets are relatively poor. To tackle this issue, we
propose a face liveness detection method based on image-text pairs and
contrastive learning, dividing liveness attack problems in the financial field
into eight categories and using text information to describe the images of
these eight types of attacks. The text encoder and image encoder are used to
extract feature vector representations for the classification description text
and face images, respectively. By maximizing the similarity of positive samples
and minimizing the similarity of negative samples, the model learns shared
representations between images and texts. The proposed method is capable of
effectively detecting specific liveness attack behaviors in certain scenarios,
such as those occurring in dark environments or involving the tampering of ID
card photos. Additionally, it is also effective in detecting traditional
liveness attack methods, such as printing photo attacks and screen remake
attacks. The zero-shot capabilities of face liveness detection on five public
datasets, including NUAA, CASIA-FASD, Replay-Attack, OULU-NPU and MSU-MFSD also
reaches the level of commercial algorithms. The detection capability of
proposed algorithm was verified on 5 types of testing datasets, and the results
show that the method outperformed commercial algorithms, and the detection
rates reached 100% on multiple datasets. Demonstrating the effectiveness and
robustness of introducing image-text pairs and contrastive learning into
liveness detection tasks as proposed in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17590">SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis. (arXiv:2311.17590v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Ziqiao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wentao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yue Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaomei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhaoxin Fan</a></p>
<p>Achieving high synchronization in the synthesis of realistic, speech-driven
talking head videos presents a significant challenge. Traditional Generative
Adversarial Networks (GAN) struggle to maintain consistent facial identity,
while Neural Radiance Fields (NeRF) methods, although they can address this
issue, often produce mismatched lip movements, inadequate facial expressions,
and unstable head poses. A lifelike talking head requires synchronized
coordination of subject identity, lip movements, facial expressions, and head
poses. The absence of these synchronizations is a fundamental flaw, leading to
unrealistic and artificial outcomes. To address the critical issue of
synchronization, identified as the "devil" in creating realistic talking heads,
we introduce SyncTalk. This NeRF-based method effectively maintains subject
identity, enhancing synchronization and realism in talking head synthesis.
SyncTalk employs a Face-Sync Controller to align lip movements with speech and
innovatively uses a 3D facial blendshape model to capture accurate facial
expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more
natural head movements. The Portrait-Sync Generator restores hair details and
blends the generated head with the torso for a seamless visual experience.
Extensive experiments and user studies demonstrate that SyncTalk outperforms
state-of-the-art methods in synchronization and realism. We recommend watching
the supplementary video: https://ziqiaopeng.github.io/synctalk
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17593">LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poudel_R/0/1/0/all/0/1">Rudra P.K. Poudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandya_H/0/1/0/all/0/1">Harit Pandya</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1">Roberto Cipolla</a></p>
<p>Recent advances in deep reinforcement learning have showcased its potential
in tackling complex tasks. However, experiments on visual control tasks have
revealed that state-of-the-art reinforcement learning models struggle with
out-of-distribution generalization. Conversely, expressing higher-level
concepts and global contexts is relatively easy using language.
</p>
<p>Building upon recent success of the large language models, our main objective
is to improve the state abstraction technique in reinforcement learning by
leveraging language for robust action selection. Specifically, we focus on
learning language-grounded visual features to enhance the world model learning,
a model-based reinforcement learning technique.
</p>
<p>To enforce our hypothesis explicitly, we mask out the bounding boxes of a few
objects in the image observation and provide the text prompt as descriptions
for these masked objects. Subsequently, we predict the masked objects along
with the surrounding regions as pixel reconstruction, similar to the
transformer-based masked autoencoder approach.
</p>
<p>Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art
performance in out-of-distribution test at the 100K interaction steps
benchmarks of iGibson point navigation tasks. Furthermore, our proposed
technique of explicit language-grounded visual representation learning has the
potential to improve models for human-robot interaction because our extracted
visual features are language grounded.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17597">Continual Self-supervised Learning: Towards Universal Multi-modal Medical Data Representation Learning. (arXiv:2311.17597v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yiwen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yutong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yong Xia</a></p>
<p>Self-supervised learning is an efficient pre-training method for medical
image analysis. However, current research is mostly confined to
specific-modality data pre-training, consuming considerable time and resources
without achieving universality across different modalities. A straightforward
solution is combining all modality data for joint self-supervised pre-training,
which poses practical challenges. Firstly, our experiments reveal conflicts in
representation learning as the number of modalities increases. Secondly,
multi-modal data collected in advance cannot cover all real-world scenarios. In
this paper, we reconsider versatile self-supervised learning from the
perspective of continual learning and propose MedCoSS, a continuous
self-supervised learning approach for multi-modal medical data. Unlike joint
self-supervised learning, MedCoSS assigns different modality data to different
training stages, forming a multi-stage pre-training process. To balance modal
conflicts and prevent catastrophic forgetting, we propose a rehearsal-based
continual learning method. We introduce the k-means sampling strategy to retain
data from previous modalities and rehearse it when learning new modalities.
Instead of executing the pretext task on buffer data, a feature distillation
strategy and an intra-modal mixup strategy are applied to these data for
knowledge retention. We conduct continuous self-supervised pre-training on a
large-scale multi-modal unlabeled dataset, including clinical reports, X-rays,
CT scans, MRI scans, and pathological images. Experimental results demonstrate
MedCoSS's exceptional generalization ability across nine downstream datasets
and its significant scalability in integrating new modality data. Code and
pre-trained weight are available at https://github.com/yeerwen/MedCoSS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17600">Query-Relevant Images Jailbreak Large Multi-Modal Models. (arXiv:2311.17600v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yunshi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Warning: This paper contains examples of harmful language and images, and
reader discretion is recommended. The security concerns surrounding Large
Language Models (LLMs) have been extensively explored, yet the safety of Large
Multi-Modal Models (LMMs) remains understudied. In our study, we present a
novel visual prompt attack that exploits query-relevant images to jailbreak the
open-source LMMs. Our method creates a composite image from one image generated
by diffusion models and another that displays the text as typography, based on
keywords extracted from a malicious query. We show LLMs can be easily attacked
by our approach, even if the employed Large Language Models are safely aligned.
To evaluate the extent of this vulnerability in open-source LMMs, we have
compiled a substantial dataset encompassing 13 scenarios with a total of 5,040
text-image pairs, using our presented attack technique. Our evaluation of 12
cutting-edge LMMs using this dataset shows the vulnerability of existing
multi-modal models on adversarial attacks. This finding underscores the need
for a concerted effort to strengthen and enhance the safety measures of
open-source LMMs against potential malicious exploits. The resource is
available at \href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17607">Topology-Preserving Adversarial Training. (arXiv:2311.17607v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1">Xiaoyue Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1">Fan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1">Yepeng Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Juan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Sheng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Despite the effectiveness in improving the robustness of neural networks,
adversarial training has suffered from the natural accuracy degradation
problem, i.e., accuracy on natural samples has reduced significantly. In this
study, we reveal that natural accuracy degradation is highly related to the
disruption of the natural sample topology in the representation space by
quantitative and qualitative experiments. Based on this observation, we propose
Topology-pReserving Adversarial traINing (TRAIN) to alleviate the problem by
preserving the topology structure of natural samples from a standard model
trained only on natural samples during adversarial training. As an additional
regularization, our method can easily be combined with various popular
adversarial training algorithms in a plug-and-play manner, taking advantage of
both sides. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet
show that our proposed method achieves consistent and significant improvements
over various strong baselines in most cases. Specifically, without additional
data, our proposed method achieves up to 8.78% improvement in natural accuracy
and 4.50% improvement in robust accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17608">Adversarial Robust Memory-Based Continual Learner. (arXiv:2311.17608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1">Xiaoyue Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1">Fan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zonghan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Juan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Despite the remarkable advances that have been made in continual learning,
the adversarial vulnerability of such methods has not been fully discussed. We
delve into the adversarial robustness of memory-based continual learning
algorithms and observe limited robustness improvement by directly applying
adversarial training techniques. Preliminary studies reveal the twin challenges
for building adversarial robust continual learners: accelerated forgetting in
continual learning and gradient obfuscation in adversarial robustness. In this
study, we put forward a novel adversarial robust memory-based continual learner
that adjusts data logits to mitigate the forgetting of pasts caused by
adversarial samples. Furthermore, we devise a gradient-based data selection
mechanism to overcome the gradient obfuscation caused by limited stored data.
The proposed approach can widely integrate with existing memory-based continual
learning as well as adversarial training algorithms in a plug-and-play way.
Extensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate
the effectiveness of our approach, achieving up to 8.13% higher accuracy for
adversarial data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17609">AnyLens: A Generative Diffusion Model with Any Rendering Lens. (arXiv:2311.17609v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1">Moab Arar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a></p>
<p>State-of-the-art diffusion models can generate highly realistic images based
on various conditioning like text, segmentation, and depth. However, an
essential aspect often overlooked is the specific camera geometry used during
image capture. The influence of different optical systems on the final scene
appearance is frequently overlooked. This study introduces a framework that
intimately integrates a text-to-image diffusion model with the particular lens
geometry used in image rendering. Our method is based on a per-pixel coordinate
conditioning method, enabling the control over the rendering geometry. Notably,
we demonstrate the manipulation of curvature properties, achieving diverse
visual effects, such as fish-eye, panoramic views, and spherical texturing
using a single diffusion model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17618">ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model. (arXiv:2311.17618v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Fukun Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Biao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zibo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiayuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Taihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a></p>
<p>The advent of large language models, enabling flexibility through
instruction-driven approaches, has revolutionized many traditional generative
tasks, but large models for 3D data, particularly in comprehensively handling
3D shapes with other modalities, are still under-explored. By achieving
instruction-based shape generations, versatile multimodal generative shape
models can significantly benefit various fields like 3D virtual construction
and network-aided design. In this work, we present ShapeGPT, a shape-included
multi-modal framework to leverage strong pre-trained language models to address
multiple shape-relevant tasks. Specifically, ShapeGPT employs a
word-sentence-paragraph framework to discretize continuous shapes into shape
words, further assembles these words for shape sentences, as well as integrates
shape with instructional text for multi-modal paragraphs. To learn this
shape-language model, we use a three-stage training scheme, including shape
representation, multimodal alignment, and instruction-based generation, to
align shape-language codebooks and learn the intricate correlations among these
modalities. Extensive experiments demonstrate that ShapeGPT achieves comparable
performance across shape-relevant tasks, including text-to-shape,
shape-to-text, shape completion, and shape editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17626">Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation. (arXiv:2311.17626v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_N/0/1/0/all/0/1">Naisong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianzhu Zhang</a></p>
<p>Few-shot segmentation (FSS) aims to segment objects of new categories given
only a handful of annotated samples. Previous works focus their efforts on
exploring the support information while paying less attention to the mining of
the critical query branch. In this paper, we rethink the importance of support
information and propose a new query-centric FSS model Adversarial Mining
Transformer (AMFormer), which achieves accurate query image segmentation with
only rough support guidance or even weak support labels. The proposed AMFormer
enjoys several merits. First, we design an object mining transformer (G) that
can achieve the expansion of incomplete region activated by support clue, and a
detail mining transformer (D) to discriminate the detailed local difference
between the expanded mask and the ground truth. Second, we propose to train G
and D via an adversarial process, where G is optimized to generate more
accurate masks approaching ground truth to fool D. We conduct extensive
experiments on commonly used Pascal-5i and COCO-20i benchmarks and achieve
state-of-the-art results across all settings. In addition, the decent
performance with weak support labels in our query-centric paradigm may inspire
the development of more general FSS models. Code will be available at
https://github.com/Wyxdm/AMNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17629">Efficient Decoder for End-to-End Oriented Object Detection in Remote Sensing Images. (arXiv:2311.17629v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiaqi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zeyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hancheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Wenliang Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1">Rui Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1">Abdulmotaleb El Saddik</a></p>
<p>Object instances in remote sensing images often distribute with
multi-orientations, varying scales, and dense distribution. These issues bring
challenges to end-to-end oriented object detectors including multi-scale
features alignment and a large number of queries. To address these limitations,
we propose an end-to-end oriented detector equipped with an efficient decoder,
which incorporates two technologies, Rotated RoI attention (RRoI attention) and
Selective Distinct Queries (SDQ). Specifically, RRoI attention effectively
focuses on oriented regions of interest through a cross-attention mechanism and
aligns multi-scale features. SDQ collects queries from intermediate decoder
layers and then filters similar queries to obtain distinct queries. The
proposed SDQ can facilitate the optimization of one-to-one label assignment,
without introducing redundant initial queries or extra auxiliary branches.
Extensive experiments on five datasets demonstrate the effectiveness of our
method. Notably, our method achieves state-of-the-art performance on DIOR-R
(67.31% mAP), DOTA-v1.5 (67.43% mAP), and DOTA-v2.0 (53.28% mAP) with the
ResNet50 backbone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17634">Erasing the Ephemeral: Joint Camera Refinement and Transient Object Removal for Street View Synthesis. (arXiv:2311.17634v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deka_M/0/1/0/all/0/1">Mreenav Shyam Deka</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_L/0/1/0/all/0/1">Lu Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>Synthesizing novel views for urban environments is crucial for tasks like
autonomous driving and virtual tours. Compared to object-level or indoor
situations, outdoor settings present unique challenges, such as inconsistency
across frames due to moving vehicles and camera pose drift over lengthy
sequences. In this paper, we introduce a method that tackles these challenges
on view synthesis for outdoor scenarios. We employ a neural point light field
scene representation and strategically detect and mask out dynamic objects to
reconstruct novel scenes without artifacts. Moreover, we simultaneously
optimize camera pose along with the view synthesis process, and thus, we
simultaneously refine both elements. Through validation on real-world urban
datasets, we demonstrate state-of-the-art results in synthesizing novel views
of urban scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17643">Neural Fields with Thermal Activations for Arbitrary-Scale Super-Resolution. (arXiv:2311.17643v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1">Alexander Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1">Rodrigo Caye Daudt</a>, <a href="http://arxiv.org/find/cs/1/au:+Metzger_N/0/1/0/all/0/1">Nando Metzger</a>, <a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1">Jan Dirk Wegner</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a></p>
<p>Recent approaches for arbitrary-scale single image super-resolution (ASSR)
have used local neural fields to represent continuous signals that can be
sampled at different rates. However, in such formulation, the point-wise query
of field values does not naturally match the point spread function (PSF) of a
given pixel. In this work we present a novel way to design neural fields such
that points can be queried with a Gaussian PSF, which serves as anti-aliasing
when moving across resolutions for ASSR. We achieve this using a novel
activation function derived from Fourier theory and the heat equation. This
comes at no additional cost: querying a point with a Gaussian PSF in our
framework does not affect computational cost, unlike filtering in the image
domain. Coupled with a hypernetwork, our method not only provides theoretically
guaranteed anti-aliasing, but also sets a new bar for ASSR while also being
more parameter-efficient than previous methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17647">VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following. (arXiv:2311.17647v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yujie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiujun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a></p>
<p>We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to
evaluate the visual instruction following capability of Multimodal Large
Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs
by embedding the instructions into the visual scenes, demanding strong visual
interpretative skills for instruction following. We adapt VIM to various
benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM
bench, and probe diverse MLLMs across three distinct in-context learning
settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a
significant performance disparity between the open-source MLLMs and GPT-4V,
implying that their proficiency in visual instruction comprehension is not up
to par. Our results highlight a promising direction for the enhancement of
MLLMs capabilities on instruction following. We aim VIM to serve as a useful
norm for advancing the state of the art and driving further progress in the
field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17655">Vulnerability of Automatic Identity Recognition to Audio-Visual Deepfakes. (arXiv:2311.17655v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Korshunov_P/0/1/0/all/0/1">Pavel Korshunov</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haolin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Garner_P/0/1/0/all/0/1">Philip N. Garner</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1">Sebastien Marcel</a></p>
<p>The task of deepfakes detection is far from being solved by speech or vision
researchers. Several publicly available databases of fake synthetic video and
speech were built to aid the development of detection methods. However,
existing databases typically focus on visual or voice modalities and provide no
proof that their deepfakes can in fact impersonate any real person. In this
paper, we present the first realistic audio-visual database of deepfakes
SWAN-DF, where lips and speech are well synchronized and video have high visual
and audio qualities. We took the publicly available SWAN dataset of real videos
with different identities to create audio-visual deepfakes using several models
from DeepFaceLab and blending techniques for face swapping and HiFiVC, DiffVC,
YourTTS, and FreeVC models for voice conversion. From the publicly available
speech dataset LibriTTS, we also created a separate database of only audio
deepfakes LibriTTS-DF using several latest text to speech methods: YourTTS,
Adaspeech, and TorToiSe. We demonstrate the vulnerability of a state of the art
speaker recognition system, such as ECAPA-TDNN-based model from SpeechBrain, to
the synthetic voices. Similarly, we tested face recognition system based on the
MobileFaceNet architecture to several variants of our visual deepfakes. The
vulnerability assessment show that by tuning the existing pretrained deepfake
models to specific identities, one can successfully spoof the face and speaker
recognition systems in more than 90% of the time and achieve a very realistic
looking and sounding fake video of a given person.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17656">Multiple Toddler Tracking in Indoor Videos. (arXiv:2311.17656v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amraee_S/0/1/0/all/0/1">Somaieh Amraee</a>, <a href="http://arxiv.org/find/cs/1/au:+Galoaa_B/0/1/0/all/0/1">Bishoy Galoaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1">Matthew Goodwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatamimajoumerd_E/0/1/0/all/0/1">Elaheh Hatamimajoumerd</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1">Sarah Ostadabbas</a></p>
<p>Multiple toddler tracking (MTT) involves identifying and differentiating
toddlers in video footage. While conventional multi-object tracking (MOT)
algorithms are adept at tracking diverse objects, toddlers pose unique
challenges due to their unpredictable movements, various poses, and similar
appearance. Tracking toddlers in indoor environments introduces additional
complexities such as occlusions and limited fields of view. In this paper, we
address the challenges of MTT and propose MTTSort, a customized method built
upon the DeepSort algorithm. MTTSort is designed to track multiple toddlers in
indoor videos accurately. Our contributions include discussing the primary
challenges in MTT, introducing a genetic algorithm to optimize hyperparameters,
proposing an accurate tracking algorithm, and curating the MTTrack dataset
using unbiased AI co-labeling techniques. We quantitatively compare MTTSort to
state-of-the-art MOT methods on MTTrack, DanceTrack, and MOT15 datasets. In our
evaluation, the proposed method outperformed other MOT methods, achieving 0.98,
0.68, and 0.98 in multiple object tracking accuracy (MOTA), higher order
tracking accuracy (HOTA), and iterative and discriminative framework 1 (IDF1)
metrics, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17657">Volumetric Cloud Field Reconstruction. (arXiv:2311.17657v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jacob Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Farinha_M/0/1/0/all/0/1">Miguel Farinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Gryspeerdt_E/0/1/0/all/0/1">Edward Gryspeerdt</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1">Ronald Clark</a></p>
<p>Volumetric phenomena, such as clouds and fog, present a significant challenge
for 3D reconstruction systems due to their translucent nature and their complex
interactions with light. Conventional techniques for reconstructing scattering
volumes rely on controlled setups, limiting practical applications. This paper
introduces an approach to reconstructing volumes from a few input stereo pairs.
We propose a novel deep learning framework that integrates a deep stereo model
with a 3D Convolutional Neural Network (3D CNN) and an advection module,
capable of capturing the shape and dynamics of volumes. The stereo depths are
used to carve empty space around volumes, providing the 3D CNN with a prior for
coping with the lack of input views. Refining our output, the advection module
leverages the temporal evolution of the medium, providing a mechanism to infer
motion and improve temporal consistency. The efficacy of our system is
demonstrated through its ability to estimate density and velocity fields of
large-scale volumes, in this case, clouds, from a sparse set of stereo image
pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17663">Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications. (arXiv:2311.17663v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Junyi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xieyuanli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiawei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingyi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jintao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1">Weihao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_R/0/1/0/all/0/1">Rui Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>Understanding how the surrounding environment changes is crucial for
performing downstream tasks safely and reliably in autonomous driving
applications. Recent occupancy estimation techniques using only camera images
as input can provide dense occupancy representations of large-scale scenes
based on the current observation. However, they are mostly limited to
representing the current 3D space and do not consider the future state of
surrounding objects along the time axis. To extend camera-only occupancy
estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark
for camera-only 4D occupancy forecasting, evaluating the surrounding scene
changes in a near future. We build our benchmark based on multiple publicly
available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5,
which provides sequential occupancy states of general movable and static
objects, as well as their 3D backward centripetal flow. To establish this
benchmark for future research with comprehensive comparisons, we introduce four
baseline types from diverse camera-based perception and prediction
implementations, including a static-world occupancy model, voxelization of
point cloud prediction, 2D-3D instance-based prediction, and our proposed novel
end-to-end 4D occupancy forecasting network. Furthermore, the standardized
evaluation protocol for preset multiple tasks is also provided to compare the
performance of all the proposed baselines on present and future occupancy
estimation with respect to objects of interest in autonomous driving scenarios.
The dataset and our implementation of all four baselines in the proposed
Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17677">COVIDx CXR-4: An Expanded Multi-Institutional Open-Source Benchmark Dataset for Chest X-ray Image-Based Computer-Aided COVID-19 Diagnostics. (arXiv:2311.17677v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1">Yifan Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1">Hayden Gunraj</a>, <a href="http://arxiv.org/find/eess/1/au:+Tai_C/0/1/0/all/0/1">Chi-en Amy Tai</a>, <a href="http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1">Alexander Wong</a></p>
<p>The global ramifications of the COVID-19 pandemic remain significant,
exerting persistent pressure on nations even three years after its initial
outbreak. Deep learning models have shown promise in improving COVID-19
diagnostics but require diverse and larger-scale datasets to improve
performance. In this paper, we introduce COVIDx CXR-4, an expanded
multi-institutional open-source benchmark dataset for chest X-ray image-based
computer-aided COVID-19 diagnostics. COVIDx CXR-4 expands significantly on the
previous COVIDx CXR-3 dataset by increasing the total patient cohort size by
greater than 2.66 times, resulting in 84,818 images from 45,342 patients across
multiple institutions. We provide extensive analysis on the diversity of the
patient demographic, imaging metadata, and disease distributions to highlight
potential dataset biases. To the best of the authors' knowledge, COVIDx CXR-4
is the largest and most diverse open-source COVID-19 CXR dataset and is made
publicly available as part of an open initiative to advance research to aid
clinicians against the COVID-19 disease.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17693">Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning. (arXiv:2311.17693v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gomaa_A/0/1/0/all/0/1">Amr Gomaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdy_B/0/1/0/all/0/1">Bilal Mahdy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleer_N/0/1/0/all/0/1">Niko Kleer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruger_A/0/1/0/all/0/1">Antonio Kr&#xfc;ger</a></p>
<p>Robotic-assisted surgical systems have demonstrated significant potential in
enhancing surgical precision and minimizing human errors. However, existing
systems lack the ability to accommodate the unique preferences and requirements
of individual surgeons. Additionally, they primarily focus on general surgeries
(e.g., laparoscopy) and are not suitable for highly precise microsurgeries,
such as ophthalmic procedures. Thus, we propose a simulation-based image-guided
approach for surgeon-centered autonomous agents that can adapt to the
individual surgeon's skill level and preferred surgical techniques during
ophthalmic cataract surgery. Our approach utilizes a simulated environment to
train reinforcement and imitation learning agents guided by image data to
perform all tasks of the incision phase of cataract surgery. By integrating the
surgeon's actions and preferences into the training process with the
surgeon-in-the-loop, our approach enables the robot to implicitly learn and
adapt to the individual surgeon's unique approach through demonstrations. This
results in a more intuitive and personalized surgical experience for the
surgeon. Simultaneously, it ensures consistent performance for the autonomous
robotic apprentice. We define and evaluate the effectiveness of our approach
using our proposed metrics; and highlight the trade-off between a generic agent
and a surgeon-centered adapted agent. Moreover, our approach has the potential
to extend to other ophthalmic surgical procedures, opening the door to a new
generation of surgeon-in-the-loop autonomous surgical robots. We provide an
open-source simulation framework for future development and reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17695">Fair Text-to-Image Diffusion via Fair Mapping. (arXiv:2311.17695v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lijie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianhang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a></p>
<p>In this paper, we address the limitations of existing text-to-image diffusion
models in generating demographically fair results when given human-related
descriptions. These models often struggle to disentangle the target language
context from sociocultural biases, resulting in biased image generation. To
overcome this challenge, we propose Fair Mapping, a general, model-agnostic,
and lightweight approach that modifies a pre-trained text-to-image model by
controlling the prompt to achieve fair image generation. One key advantage of
our approach is its high efficiency. The training process only requires
updating a small number of parameters in an additional linear mapping network.
This not only reduces the computational cost but also accelerates the
optimization process. We first demonstrate the issue of bias in generated
results caused by language biases in text-guided diffusion models. By
developing a mapping network that projects language embeddings into an unbiased
space, we enable the generation of relatively balanced demographic results
based on a keyword specified in the prompt. With comprehensive experiments on
face image generation, we show that our method significantly improves image
generation performance when prompted with descriptions related to human faces.
By effectively addressing the issue of bias, we produce more fair and diverse
image outputs. This work contributes to the field of text-to-image generation
by enhancing the ability to generate images that accurately reflect the
intended demographic characteristics specified in the text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17707">SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation. (arXiv:2311.17707v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mutian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xingyilang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Lingteng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1">Xin Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaoguang Han</a></p>
<p>We introduce SAMPro3D for zero-shot 3D indoor scene segmentation. Given the
3D point cloud and multiple posed 2D frames of 3D scenes, our approach segments
3D scenes by applying the pretrained Segment Anything Model (SAM) to 2D frames.
Our key idea involves locating 3D points in scenes as natural 3D prompts to
align their projected pixel prompts across frames, ensuring frame-consistency
in both pixel prompts and their SAM-predicted masks. Moreover, we suggest
filtering out low-quality 3D prompts based on feedback from all 2D frames, for
enhancing segmentation quality. We also propose to consolidate different 3D
prompts if they are segmenting the same object, bringing a more comprehensive
segmentation. Notably, our method does not require any additional training on
domain-specific data, enabling us to preserve the zero-shot power of SAM.
Extensive qualitative and quantitative results show that our method
consistently achieves higher quality and more diverse segmentation than
previous zero-shot or fully supervised approaches, and in many cases even
surpasses human-level annotations. The project page can be accessed at
https://mutianxu.github.io/sampro3d/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17717">Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers. (arXiv:2311.17717v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chi-Pin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Po Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1">Chung-Ting Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1">Yung-Hsuan Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Chiang Frank Wang</a></p>
<p>Concept erasure in text-to-image diffusion models aims to disable pre-trained
diffusion models from generating images related to a target concept. To perform
reliable concept erasure, the properties of robustness and locality are
desirable. The former refrains the model from producing images associated with
the target concept for any paraphrased or learned prompts, while the latter
preserves the model ability in generating images for non-target concepts. In
this paper, we propose Reliable Concept Erasing via Lightweight Erasers
(Receler), which learns a lightweight Eraser to perform concept erasing and
enhances locality and robustness with the proposed concept-localized
regularization and adversarial prompt learning, respectively. Comprehensive
quantitative and qualitative experiments with various concept prompts verify
the superiority of Receler over the previous erasing methods on the above two
desirable properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17737">GenZI: Zero-Shot 3D Human-Scene Interaction Generation. (arXiv:2311.17737v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Angela Dai</a></p>
<p>Can we synthesize 3D humans interacting with scenes without learning from any
3D human-scene interaction data? We propose GenZI, the first zero-shot approach
to generating 3D human-scene interactions. Key to GenZI is our distillation of
interaction priors from large vision-language models (VLMs), which have learned
a rich semantic space of 2D human-scene compositions. Given a natural language
description and a coarse point location of the desired interaction in a 3D
scene, we first leverage VLMs to imagine plausible 2D human interactions
inpainted into multiple rendered views of the scene. We then formulate a robust
iterative optimization to synthesize the pose and shape of a 3D human model in
the scene, guided by consistency with the 2D interaction hypotheses. In
contrast to existing learning-based approaches, GenZI circumvents the
conventional need for captured 3D interaction data, and allows for flexible
control of the 3D interaction synthesis with easy-to-use text prompts.
Extensive experiments show that our zero-shot approach has high flexibility and
generality, making it applicable to diverse scene types, including both indoor
and outdoor environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17744">Variational Bayes image restoration with compressive autoencoders. (arXiv:2311.17744v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biquard_M/0/1/0/all/0/1">Maud Biquard</a>, <a href="http://arxiv.org/find/cs/1/au:+Chabert_M/0/1/0/all/0/1">Marie Chabert</a>, <a href="http://arxiv.org/find/cs/1/au:+Oberlin_T/0/1/0/all/0/1">Thomas Oberlin</a></p>
<p>Regularization of inverse problems is of paramount importance in
computational imaging. The ability of neural networks to learn efficient image
representations has been recently exploited to design powerful data-driven
regularizers. While state-of-the-art plug-and-play methods rely on an implicit
regularization provided by neural denoisers, alternative Bayesian approaches
consider Maximum A Posteriori (MAP) estimation in the latent space of a
generative model, thus with an explicit regularization. However,
state-of-the-art deep generative models require a huge amount of training data
compared to denoisers. Besides, their complexity hampers the optimization of
the latent MAP. In this work, we propose to use compressive autoencoders for
latent estimation. These networks, which can be seen as variational
autoencoders with a flexible latent prior, are smaller and easier to train than
state-of-the-art generative models. We then introduce the Variational Bayes
Latent Estimation (VBLE) algorithm, which performs this estimation within the
framework of variational inference. This allows for fast and easy (approximate)
posterior sampling. Experimental results on image datasets BSD and FFHQ
demonstrate that VBLE reaches similar performance than state-of-the-art
plug-and-play methods, while being able to quantify uncertainties faster than
other existing posterior sampling techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17752">BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment. (arXiv:2311.17752v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1">Fangfang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Ru Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a></p>
<p>Banding, also known as staircase-like contours, frequently occurs in flat
areas of images/videos processed by the compression or quantization algorithms.
As undesirable artifacts, banding destroys the original image structure, thus
degrading users' quality of experience (QoE). In this paper, we systematically
investigate the banding image quality assessment (IQA) problem, aiming to
detect the image banding artifacts and evaluate their perceptual visual
quality. Considering that the existing image banding databases only contain
limited content sources and banding generation methods, and lack perceptual
quality labels (i.e. mean opinion scores), we first build the largest banding
IQA database so far, named Banding Artifact Noticeable Database (BAND-2k),
which consists of 2,000 banding images generated by 15 compression and
quantization schemes. A total of 23 workers participated in the subjective IQA
experiment, yielding over 214,000 patch-level banding class labels and 44,371
reliable image-level quality ratings. Subsequently, we develop an effective
no-reference (NR) banding evaluator for banding detection and quality
assessment by leveraging frequency characteristics of banding artifacts. A dual
convolutional neural network is employed to concurrently learn the feature
representation from the high-frequency and low-frequency maps, thereby
enhancing the ability to discern banding artifacts. The quality score of a
banding image is generated by pooling the banding detection maps masked by the
spatial frequency filters. Experiments demonstrate that our banding evaluator
achieves a remarkably high accuracy in banding detection and also exhibits high
SRCC and PLCC results with the perceptual quality labels. These findings unveil
the strong correlations between the intensity of banding artifacts and the
perceptual visual quality, thus validating the necessity of banding quality
assessment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17754">Cinematic Behavior Transfer via NeRF-based Differentiable Filming. (arXiv:2311.17754v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xuekun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Anyi Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1">Bo Dai</a></p>
<p>In the evolving landscape of digital media and video production, the precise
manipulation and reproduction of visual elements like camera movements and
character actions are highly desired. Existing SLAM methods face limitations in
dynamic scenes and human pose estimation often focuses on 2D projections,
neglecting 3D statuses. To address these issues, we first introduce a reverse
filming behavior estimation technique. It optimizes camera trajectories by
leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then
introduce a cinematic transfer pipeline that is able to transfer various shot
types to a new 2D video or a 3D virtual environment. The incorporation of 3D
engine workflow enables superior rendering and control abilities, which also
achieves a higher rating in the user study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17770">PillarNeSt: Embracing Backbone Scaling and Pretraining for Pillar-based 3D Object Detection. (arXiv:2311.17770v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Weixin Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tiancai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Diankun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junjie Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1">Osamu Yoshie</a></p>
<p>This paper shows the effectiveness of 2D backbone scaling and pretraining for
pillar-based 3D object detectors. Pillar-based methods mainly employ randomly
initialized 2D convolution neural network (ConvNet) for feature extraction and
fail to enjoy the benefits from the backbone scaling and pretraining in the
image domain. To show the scaling-up capacity in point clouds, we introduce the
dense ConvNet pretrained on large-scale image datasets (e.g., ImageNet) as the
2D backbone of pillar-based detectors. The ConvNets are adaptively designed
based on the model size according to the specific features of point clouds,
such as sparsity and irregularity. Equipped with the pretrained ConvNets, our
proposed pillar-based detector, termed PillarNeSt, outperforms the existing 3D
object detectors by a large margin on the nuScenes and Argoversev2 datasets.
Our code shall be released upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17776">One-Shot Open Affordance Learning with Foundation Models. (arXiv:2311.17776v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Deqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1">Laura Sevilla-Lara</a>, <a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1">Varun Jampani</a></p>
<p>We introduce One-shot Open Affordance Learning (OOAL), where a model is
trained with just one example per base object category, but is expected to
identify novel objects and affordances. While vision-language models excel at
recognizing novel objects and scenes, they often struggle to understand finer
levels of granularity such as affordances. To handle this issue, we conduct a
comprehensive analysis of existing foundation models, to explore their inherent
understanding of affordances and assess the potential for data-limited
affordance learning. We then propose a vision-language framework with simple
and effective designs that boost the alignment between visual features and
affordance text embeddings. Experiments on two affordance segmentation
benchmarks show that the proposed method outperforms state-of-the-art models
with less than 1% of the full training data, and exhibits reasonable
generalization capability on unseen objects and affordances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17791">U-Net v2: Rethinking the Skip Connections of U-Net for Medical Image Segmentation. (arXiv:2311.17791v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1">Yaopeng Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Sonka_M/0/1/0/all/0/1">Milan Sonka</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1">Danny Z. Chen</a></p>
<p>In this paper, we introduce U-Net v2, a new robust and efficient U-Net
variant for medical image segmentation. It aims to augment the infusion of
semantic information into low-level features while simultaneously refining
high-level features with finer details. For an input image, we begin by
extracting multi-level features with a deep neural network encoder. Next, we
enhance the feature map of each level by infusing semantic information from
higher-level features and integrating finer details from lower-level features
through Hadamard product. Our novel skip connections empower features of all
the levels with enriched semantic characteristics and intricate details. The
improved features are subsequently transmitted to the decoder for further
processing and segmentation. Our method can be seamlessly integrated into any
Encoder-Decoder network. We evaluate our method on several public medical image
segmentation datasets for skin lesion segmentation and polyp segmentation, and
the experimental results demonstrate the segmentation accuracy of our new
method over state-of-the-art methods, while preserving memory and computational
efficiency. Code is available at: https://github.com/yaoppeng/U-Net\_v2
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17804">Aggregation Model Hyperparameters Matter in Digital Pathology. (arXiv:2311.17804v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1">Gustav Bredell</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Marcel Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Szostak_P/0/1/0/all/0/1">Przemyslaw Szostak</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasi_Sureshjani_S/0/1/0/all/0/1">Samaneh Abbasi-Sureshjani</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1">Alvaro Gomariz</a></p>
<p>Digital pathology has significantly advanced disease detection and
pathologist efficiency through the analysis of gigapixel whole-slide images
(WSI). In this process, WSIs are first divided into patches, for which a
feature extractor model is applied to obtain feature vectors, which are
subsequently processed by an aggregation model to predict the respective WSI
label. With the rapid evolution of representation learning, numerous new
feature extractor models, often termed foundational models, have emerged.
Traditional evaluation methods, however, rely on fixed aggregation model
hyperparameters, a framework we identify as potentially biasing the results.
Our study uncovers a co-dependence between feature extractor models and
aggregation model hyperparameters, indicating that performance comparability
can be skewed based on the chosen hyperparameters. By accounting for this
co-dependency, we find that the performance of many current feature extractor
models is notably similar. We support this insight by evaluating seven feature
extractor models across three different datasets with 162 different aggregation
model configurations. This comprehensive approach provides a more nuanced
understanding of the relationship between feature extractors and aggregation
models, leading to a fairer and more accurate assessment of feature extractor
models in digital pathology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17810">Coloring the Past: Neural Historical Buildings Reconstruction from Archival Photography. (arXiv:2311.17810v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Komorowicz_D/0/1/0/all/0/1">David Komorowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_L/0/1/0/all/0/1">Lu Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Maiwald_F/0/1/0/all/0/1">Ferdinand Maiwald</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>Historical buildings are a treasure and milestone of human cultural heritage.
Reconstructing the 3D models of these building hold significant value. The
rapid development of neural rendering methods makes it possible to recover the
3D shape only based on archival photographs. However, this task presents
considerable challenges due to the limitations of such datasets. Historical
photographs are often limited in number and the scenes in these photos might
have altered over time. The radiometric quality of these images is also often
sub-optimal. To address these challenges, we introduce an approach to
reconstruct the geometry of historical buildings, employing volumetric
rendering techniques. We leverage dense point clouds as a geometric prior and
introduce a color appearance embedding loss to recover the color of the
building given limited available color images. We aim for our work to spark
increased interest and focus on preserving historical buildings. Thus, we also
introduce a new historical dataset of the Hungarian National Theater, providing
a new benchmark for the reconstruction method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17812">DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation. (arXiv:2311.17812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wansen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Youkai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Quanjun Yin</a></p>
<p>Following language instructions to navigate in unseen environments is a
challenging task for autonomous embodied agents. With strong representation
capabilities, pretrained vision-and-language models are widely used in VLN.
However, most of them are trained on web-crawled general-purpose datasets,
which incurs a considerable domain gap when used for VLN tasks. To address the
problem, we propose a novel and model-agnostic domain-aware prompt learning
(DAP) framework. For equipping the pretrained models with specific object-level
and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost
prompt tuning paradigm to learn soft visual prompts for extracting in-domain
image semantics. Specifically, we first generate a set of in-domain image-text
pairs with the help of the CLIP model. Then we introduce soft visual prompts in
the input space of the visual encoder in a pretrained model. DAP injects
in-domain visual knowledge into the visual encoder of the pretrained model in
an efficient way. Experimental results on both R2R and REVERIE show the
superiority of DAP compared to existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17833">Analyzing and Explaining Image Classifiers via Diffusion Guidance. (arXiv:2311.17833v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1">Maximilian Augustin</a>, <a href="http://arxiv.org/find/cs/1/au:+Neuhaus_Y/0/1/0/all/0/1">Yannic Neuhaus</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a></p>
<p>While deep learning has led to huge progress in complex image classification
tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call
into question how reliably these classifiers work in the wild. Furthermore, for
safety-critical tasks the black-box nature of their decisions is problematic,
and explanations or at least methods which make decisions plausible are needed
urgently. In this paper, we address these problems by generating images that
optimize a classifier-derived objective using a framework for guided image
generation. We analyze the behavior and decisions of image classifiers by
visual counterfactual explanations (VCEs), detection of systematic mistakes by
analyzing images where classifiers maximally disagree, and visualization of
neurons to verify potential spurious features. In this way, we validate
existing observations, e.g. the shape bias of adversarially robust models, as
well as novel failure modes, e.g. systematic errors of zero-shot CLIP
classifiers, or identify harmful spurious features. Moreover, our VCEs
outperform previous work while being more versatile.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17834">SPiC-E : Structural Priors in 3D Diffusion Models using Cross Entity Attention. (arXiv:2311.17834v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sella_E/0/1/0/all/0/1">Etai Sella</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiebelman_G/0/1/0/all/0/1">Gal Fiebelman</a>, <a href="http://arxiv.org/find/cs/1/au:+Atia_N/0/1/0/all/0/1">Noam Atia</a>, <a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1">Hadar Averbuch-Elor</a></p>
<p>We are witnessing rapid progress in automatically generating and manipulating
3D assets due to the availability of pretrained text-image diffusion models.
However, time-consuming optimization procedures are required for synthesizing
each sample, hindering their potential for democratizing 3D content creation.
Conversely, 3D diffusion models now train on million-scale 3D datasets,
yielding high-quality text-conditional 3D samples within seconds. In this work,
we present SPiC-E - a neural network that adds structural guidance to 3D
diffusion models, extending their usage beyond text-conditional generation. At
its core, our framework introduces a cross-entity attention mechanism that
allows for multiple entities (in particular, paired input and guidance 3D
shapes) to interact via their internal representations within the denoising
network. We utilize this mechanism for learning task-specific structural priors
in 3D diffusion models from auxiliary guidance shapes. We show that our
approach supports a variety of applications, including 3D stylization, semantic
shape editing and text-conditional abstraction-to-3D, which transforms
primitive-based abstractions into highly-expressive shapes. Extensive
experiments demonstrate that SPiC-E achieves SOTA performance over these tasks
while often being considerably faster than alternative methods. Importantly,
this is accomplished without tailoring our approach for any specific task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17842">Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yingdong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Fanqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1">Li Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a></p>
<p>In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa's superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17846">Towards Real-World Focus Stacking with Deep Learning. (arXiv:2311.17846v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1">Alexandre Araujo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1">Jean Ponce</a>, <a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1">Julien Mairal</a></p>
<p>Focus stacking is widely used in micro, macro, and landscape photography to
reconstruct all-in-focus images from multiple frames obtained with focus
bracketing, that is, with shallow depth of field and different focus planes.
Existing deep learning approaches to the underlying multi-focus image fusion
problem have limited applicability to real-world imagery since they are
designed for very short image sequences (two to four images), and are typically
trained on small, low-resolution datasets either acquired by light-field
cameras or generated synthetically. We introduce a new dataset consisting of 94
high-resolution bursts of raw images with focus bracketing, with pseudo ground
truth computed from the data using state-of-the-art commercial software. This
dataset is used to train the first deep learning algorithm for focus stacking
capable of handling bursts of sufficient length for real-world applications.
Qualitative experiments demonstrate that it is on par with existing commercial
solutions in the long-burst, realistic regime while being significantly more
tolerant to noise. The code and dataset are available at
https://github.com/araujoalexandre/FocusStackingDataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17851">Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects. (arXiv:2311.17851v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabra_R/0/1/0/all/0/1">Rishabh Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Matthey_L/0/1/0/all/0/1">Loic Matthey</a>, <a href="http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1">Alexander Lerchner</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy J. Mitra</a></p>
<p>Unlabeled 3D objects present an opportunity to leverage pretrained vision
language models (VLMs) on a range of annotation tasks -- from describing object
semantics to physical properties. An accurate response must take into account
the full appearance of the object in 3D, various ways of phrasing the
question/prompt, and changes in other factors that affect the response. We
present a method to marginalize over any factors varied across VLM queries,
utilizing the VLM's scores for sampled responses. We first show that this
probabilistic aggregation can outperform a language model (e.g., GPT4) for
summarization, for instance avoiding hallucinations when there are contrasting
details between responses. Secondly, we show that aggregated annotations are
useful for prompt-chaining; they help improve downstream VLM predictions (e.g.,
of object material when the object's type is specified as an auxiliary input in
the prompt). Such auxiliary inputs allow ablating and measuring the
contribution of visual reasoning over language-only reasoning. Using these
evaluations, we show how VLMs can approach, without additional training or
in-context learning, the quality of human-verified type and material
annotations on the large-scale Objaverse dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17857">Gaussian Shell Maps for Efficient 3D Human Generation. (arXiv:2311.17857v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1">Rameen Abdal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1">Wang Yifan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zifan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinghao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Po_R/0/1/0/all/0/1">Ryan Po</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1">Zhengfei Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1">Dit-Yan Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1">Gordon Wetzstein</a></p>
<p>Efficient generation of 3D digital humans is important in several industries,
including virtual reality, social media, and cinematic production. 3D
generative adversarial networks (GANs) have demonstrated state-of-the-art
(SOTA) quality and diversity for generated assets. Current 3D GAN
architectures, however, typically rely on volume representations, which are
slow to render, thereby hampering the GAN training and requiring
multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps
(GSMs) as a framework that connects SOTA generator network architectures with
emerging 3D Gaussian rendering primitives using an articulable multi
shell--based scaffold. In this setting, a CNN generates a 3D texture stack with
features that are mapped to the shells. The latter represent inflated and
deflated versions of a template surface of a digital human in a canonical body
pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the
shells whose attributes are encoded in the texture features. These Gaussians
are efficiently and differentiably rendered. The ability to articulate the
shells is important during GAN training and, at inference time, to deform a
body into arbitrary user-defined poses. Our efficient rendering scheme bypasses
the need for view-inconsistent upsamplers and achieves high-quality multi-view
consistent renderings at a native resolution of $512 \times 512$ pixels. We
demonstrate that GSMs successfully generate 3D humans when trained on
single-view datasets, including SHHQ and DeepFashion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.07737">Zero-Shot Self-Supervised Learning for MRI Reconstruction. (arXiv:2102.07737v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1">Burhaneddin Yaman</a>, <a href="http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1">Seyed Amir Hossein Hosseini</a>, <a href="http://arxiv.org/find/eess/1/au:+Akcakaya_M/0/1/0/all/0/1">Mehmet Ak&#xe7;akaya</a></p>
<p>Deep learning (DL) has emerged as a powerful tool for accelerated MRI
reconstruction, but often necessitates a database of fully-sampled measurements
for training. Recent self-supervised and unsupervised learning approaches
enable training without fully-sampled data. However, a database of undersampled
measurements may not be available in many scenarios, especially for scans
involving contrast or translational acquisitions in development. Moreover,
recent studies show that database-trained models may not generalize well when
the unseen measurements differ in terms of sampling pattern, acceleration rate,
SNR, image contrast, and anatomy. Such challenges necessitate a new methodology
to enable subject-specific DL MRI reconstruction without external training
datasets, since it is clinically imperative to provide high-quality
reconstructions that can be used to identify lesions/disease for \emph{every
individual}. In this work, we propose a zero-shot self-supervised learning
approach to perform subject-specific accelerated DL MRI reconstruction to
tackle these issues. The proposed approach partitions the available
measurements from a single scan into three disjoint sets. Two of these sets are
used to enforce data consistency and define loss during training for
self-supervision, while the last set serves to self-validate, establishing an
early stopping criterion. In the presence of models pre-trained on a database
with different image characteristics, we show that the proposed approach can be
combined with transfer learning for faster convergence time and reduced
computational complexity. The code is available at
\url{https://github.com/byaman14/ZS-SSL}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.12971">Natural &amp; Adversarial Bokeh Rendering via Circle-of-Confusion Predictive Network. (arXiv:2111.12971v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1">Felix Juefei-Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">Geguang Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Bokeh effect is a natural shallow depth-of-field phenomenon that blurs the
out-of-focus part in photography. In recent years, a series of works have
proposed automatic and realistic bokeh rendering methods for artistic and
aesthetic purposes. They usually employ cutting-edge data-driven deep
generative networks with complex training strategies and network architectures.
However, these works neglect that the bokeh effect, as a real phenomenon, can
inevitably affect the subsequent visual intelligent tasks like recognition, and
their data-driven nature prevents them from studying the influence of
bokeh-related physical parameters (i.e., depth-of-the-field) on the intelligent
tasks. To fill this gap, we study a totally new problem, i.e., natural &amp;
adversarial bokeh rendering, which consists of two objectives: rendering
realistic and natural bokeh and fooling the visual perception models (i.e.,
bokeh-based adversarial attack). To this end, beyond the pure data-driven
solution, we propose a hybrid alternative by taking the respective advantages
of data-driven and physical-aware methods. Specifically, we propose the
circle-of-confusion predictive network (CoCNet) by taking the all-in-focus
image and depth image as inputs to estimate circle-of-confusion parameters for
each pixel, which are employed to render the final image through a well-known
physical model of bokeh. With the hybrid solution, our method could achieve
more realistic rendering results with the naive training strategy and a much
lighter network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.00948">CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors. (arXiv:2203.00948v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jin-Ju Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Dobigeon_N/0/1/0/all/0/1">Nicolas Dobigeon</a>, <a href="http://arxiv.org/find/eess/1/au:+Chabert_M/0/1/0/all/0/1">Marie Chabert</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1">Ding-Cheng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1">Ting-Zhu Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1">Jie Huang</a></p>
<p>In the context of Earth observation, change detection boils down to comparing
images acquired at different times by sensors of possibly different spatial
and/or spectral resolutions or different modalities (e.g., optical or radar).
Even when considering only optical images, this task has proven to be
challenging as soon as the sensors differ by their spatial and/or spectral
resolutions. This paper proposes a novel unsupervised change detection method
dedicated to images acquired by such so-called heterogeneous optical sensors.
It capitalizes on recent advances which formulate the change detection task
into a robust fusion framework. Adopting this formulation, the work reported in
this paper shows that any off-the-shelf network trained beforehand to fuse
optical images of different spatial and/or spectral resolutions can be easily
complemented with a network of the same architecture and embedded into an
adversarial framework to perform change detection. A comparison with
state-of-the-art change detection methods demonstrates the versatility and the
effectiveness of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11952">3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual Architecture. (arXiv:2205.11952v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1">Jevgenija Rudzusika</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1">Buda Baji&#x107;</a>, <a href="http://arxiv.org/find/eess/1/au:+Koehler_T/0/1/0/all/0/1">Thomas Koehler</a>, <a href="http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1">Ozan &#xd6;ktem</a></p>
<p>Deep learning based computed tomography (CT) reconstruction has demonstrated
outstanding performance on simulated 2D low-dose CT data. This applies in
particular to domain adapted neural networks, which incorporate a handcrafted
physics model for CT imaging. Empirical evidence shows that employing such
architectures reduces the demand for training data and improves upon
generalisation. However, their training requires large computational resources
that quickly become prohibitive in 3D helical CT, which is the most common
acquisition geometry used for medical imaging. Furthermore, clinical data also
comes with other challenges not accounted for in simulations, like errors in
flux measurement, resolution mismatch and, most importantly, the absence of the
real ground truth. The necessity to have a computationally feasible training
combined with the need to address these issues has made it difficult to
evaluate deep learning based reconstruction on clinical 3D helical CT. This
paper modifies a domain adapted neural network architecture, the Learned
Primal-Dual (LPD), so that it can be trained and applied to reconstruction in
this setting. We achieve this by splitting the helical trajectory into sections
and applying the unrolled LPD iterations to those sections sequentially. To the
best of our knowledge, this work is the first to apply an unrolled deep
learning architecture for reconstruction on full-sized clinical data, like
those in the Low dose CT image and projection data set (LDCT). Moreover,
training and testing is done on a single GPU card with 24GB of memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04671">TCDM: Transformational Complexity Based Distortion Metric for Perceptual Point Cloud Quality Assessment. (arXiv:2210.04671v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yujie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yifei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaozhong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Le Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yiling Xu</a></p>
<p>The goal of objective point cloud quality assessment (PCQA) research is to
develop quantitative metrics that measure point cloud quality in a perceptually
consistent manner. Merging the research of cognitive science and intuition of
the human visual system (HVS), in this paper, we evaluate the point cloud
quality by measuring the complexity of transforming the distorted point cloud
back to its reference, which in practice can be approximated by the code length
of one point cloud when the other is given. For this purpose, we first make
space segmentation for the reference and distorted point clouds based on a 3D
Voronoi diagram to obtain a series of local patch pairs. Next, inspired by the
predictive coding theory, we utilize a space-aware vector autoregressive
(SA-VAR) model to encode the geometry and color channels of each reference
patch with and without the distorted patch, respectively. Assuming that the
residual errors follow the multi-variate Gaussian distributions, the
self-complexity of the reference and transformational complexity between the
reference and distorted samples are computed using covariance matrices.
Additionally, the prediction terms generated by SA-VAR are introduced as one
auxiliary feature to promote the final quality prediction. The effectiveness of
the proposed transformational complexity based distortion metric (TCDM) is
evaluated through extensive experiments conducted on five public point cloud
quality assessment databases. The results demonstrate that TCDM achieves
state-of-the-art (SOTA) performance, and further analysis confirms its
robustness in various scenarios. The code is publicly available at
https://github.com/zyj1318053/TCDM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.15646">Beyond Invariance: Test-Time Label-Shift Adaptation for Distributions with &quot;Spurious&quot; Correlations. (arXiv:2211.15646v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sun_Q/0/1/0/all/0/1">Qingyao Sun</a> (Cornell University), <a href="http://arxiv.org/find/stat/1/au:+Murphy_K/0/1/0/all/0/1">Kevin Murphy</a> (Google DeepMind), <a href="http://arxiv.org/find/stat/1/au:+Ebrahimi_S/0/1/0/all/0/1">Sayna Ebrahimi</a> (Google Cloud AI Research), <a href="http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1">Alexander D&#x27;Amour</a> (Google DeepMind)</p>
<p>Changes in the data distribution at test time can have deleterious effects on
the performance of predictive models $p(y|x)$. We consider situations where
there are additional meta-data labels (such as group labels), denoted by $z$,
that can account for such changes in the distribution. In particular, we assume
that the prior distribution $p(y, z)$, which models the dependence between the
class label $y$ and the "nuisance" factors $z$, may change across domains,
either due to a change in the correlation between these terms, or a change in
one of their marginals. However, we assume that the generative model for
features $p(x|y,z)$ is invariant across domains. We note that this corresponds
to an expanded version of the widely used "label shift" assumption, where the
labels now also include the nuisance factors $z$. Based on this observation, we
propose a test-time label shift correction that adapts to changes in the joint
distribution $p(y, z)$ using EM applied to unlabeled samples from the target
domain distribution, $p_t(x)$. Importantly, we are able to avoid fitting a
generative model $p(x|y, z)$, and merely need to reweight the outputs of a
discriminative model $p_s(y, z|x)$ trained on the source distribution. We
evaluate our method, which we call "Test-Time Label-Shift Adaptation" (TTLSA),
on several standard image and text datasets, as well as the CheXpert chest
X-ray dataset, and show that it improves performance over methods that target
invariance to changes in the distribution, as well as baseline empirical risk
minimization methods. Code for reproducing experiments is available at
https://github.com/nalzok/test-time-label-shift .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06872">Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods. (arXiv:2212.06872v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Mingqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1">Saeed Khorram</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuxin_L/0/1/0/all/0/1">Li Fuxin</a></p>
<p>In order to learn better about how different visual recognition backbones
make decisions, we propose a methodology that systematically applies deep
explanation algorithms on a dataset-wide basis, and compares the statistics
generated from the amount and nature of the explanations to gain insights about
the decision-making of different models. Specifically, we propose two
methodologies called sub-explanation counting and cross-testing. These
methodologies reveal the difference among networks in terms of two properties
called compositionality and disjunctivism. Transformers and ConvNeXt are found
to be more compositional, in the sense that they jointly consider multiple
parts of the image in building their decisions, whereas traditional CNNs and
distilled transformers are less compositional and more disjunctive, which means
that they use multiple diverse but smaller set of parts to achieve a confident
prediction. Through further experiments, we pinpointed the choice of
normalization to be especially important in the compositionality of a model, in
that batch normalization leads to less compositionality while group and layer
normalization lead to more. Finally, we also analyze the features shared by
different backbones and plot a landscape of different models based on their
feature-use similarity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00545">Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels. (arXiv:2301.00545v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yikai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yanwei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xinwei Sun</a></p>
<p>A noisy training set usually leads to the degradation of the generalization
and robustness of neural networks. In this paper, we propose a novel
theoretically guaranteed clean sample selection framework for learning with
noisy labels. Specifically, we first present a Scalable Penalized Regression
(SPR) method, to model the linear relation between network features and one-hot
labels. In SPR, the clean data are identified by the zero mean-shift parameters
solved in the regression model. We theoretically show that SPR can recover
clean data under some conditions. Under general scenarios, the conditions may
be no longer satisfied; and some noisy data are falsely selected as clean data.
To solve this problem, we propose a data-adaptive method for Scalable Penalized
Regression with Knockoff filters (Knockoffs-SPR), which is provable to control
the False-Selection-Rate (FSR) in the selected clean data. To improve the
efficiency, we further present a split algorithm that divides the whole
training set into small pieces that can be solved in parallel to make the
framework scalable to large datasets. While Knockoffs-SPR can be regarded as a
sample selection module for a standard supervised training pipeline, we further
combine it with a semi-supervised algorithm to exploit the support of noisy
data as unlabeled data. Experimental results on several benchmark datasets and
real-world noisy datasets show the effectiveness of our framework and validate
the theoretical results of Knockoffs-SPR. Our code and pre-trained models are
available at https://github.com/Yikai-Wang/Knockoffs-SPR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02124">Transform, Contrast and Tell: Coherent Entity-Aware Multi-Image Captioning. (arXiv:2302.02124v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingqiang Chen</a></p>
<p>Coherent entity-aware multi-image captioning aims to generate coherent
captions for neighboring images in a news document. There are coherence
relationships among neighboring images because they often describe same
entities or events. These relationships are important for entity-aware
multi-image captioning, but are neglected in entity-aware single-image
captioning. Most existing work focuses on single-image captioning, while
multi-image captioning has not been explored before. Hence, this paper proposes
a coherent entity-aware multi-image captioning model by making use of coherence
relationships. The model consists of a Transformer-based caption generation
model and two types of contrastive learning-based coherence mechanisms. The
generation model generates the caption by paying attention to the image and the
accompanying text. The caption-caption coherence mechanism aims to render
entities in the caption of the image be also in captions of neighboring images.
The caption-image-text coherence mechanism aims to render entities in the
caption of the image be also in the accompanying text. To evaluate coherence
between captions, two coherence evaluation metrics are proposed. The new
dataset DM800K is constructed that has more images per document than two
existing datasets GoodNews and NYT800K, and is more suitable for multi-image
captioning. Experiments on three datasets show the proposed captioning model
outperforms 7 baselines according to BLUE, Rouge, METEOR, and entity precision
and recall scores. Experiments also show that the generated captions are more
coherent than that of baselines according to caption entity scores, caption
Rouge scores, the two proposed coherence evaluation metrics, and human
evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09251">StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization. (arXiv:2302.09251v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bose_S/0/1/0/all/0/1">Shirsha Bose</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1">Ankit Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1">Enrico Fini</a>, <a href="http://arxiv.org/find/cs/1/au:+Singha_M/0/1/0/all/0/1">Mainak Singha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1">Elisa Ricci</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1">Biplab Banerjee</a></p>
<p>Large-scale foundation models, such as CLIP, have demonstrated impressive
zero-shot generalization performance on downstream tasks, leveraging
well-designed language prompts. However, these prompt learning techniques often
struggle with domain shift, limiting their generalization capabilities. In our
study, we tackle this issue by proposing StyLIP, a novel approach for Domain
Generalization (DG) that enhances CLIP's classification performance across
domains. Our method focuses on a domain-agnostic prompt learning strategy,
aiming to disentangle the visual style and content information embedded in
CLIP's pre-trained vision encoder, enabling effortless adaptation to novel
domains during inference. To achieve this, we introduce a set of style
projectors that directly learn the domain-specific prompt tokens from the
extracted multi-scale style features. These generated prompt embeddings are
subsequently combined with the multi-scale visual content features learned by a
content projector. The projectors are trained in a contrastive manner,
utilizing CLIP's fixed vision and text backbones. Through extensive experiments
conducted in five different DG settings on multiple benchmark datasets, we
consistently demonstrate that StyLIP outperforms the current state-of-the-art
(SOTA) methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06753">Modular Quantization-Aware Training: Increasing Accuracy by Decreasing Precision in 6D Object Pose Estimation. (arXiv:2303.06753v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1">Saqib Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengkun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_A/0/1/0/all/0/1">Andrew Price</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yinlin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a></p>
<p>Edge applications, such as collaborative robotics and spacecraft rendezvous,
demand efficient 6D object pose estimation on resource-constrained embedded
platforms. Existing 6D pose estimation networks are often too large for such
deployments, necessitating compression while maintaining reliable performance.
To address this challenge, we introduce Modular Quantization-Aware Training
(MQAT), an adaptive and mixed-precision quantization-aware training strategy
that exploits the modular structure of modern 6D pose estimation architectures.
MQAT guides a systematic gradated modular quantization sequence and determines
module-specific bit precisions, leading to quantized models that outperform
those produced by state-of-the-art uniform and mixed-precision quantization
techniques. Our experiments showcase the generality of MQAT across datasets,
architectures, and quantization algorithms. Remarkably, MQAT-trained quantized
models achieve a significant accuracy boost (&gt;7%) over the baseline
full-precision network while reducing model size by a factor of 4x or more.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06842">Hierarchical Relationships: A New Perspective to Enhance Scene Graph Generation. (arXiv:2303.06842v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bowen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1">Camillo J. Taylor</a></p>
<p>This paper presents a finding that leveraging the hierarchical structures
among labels for relationships and objects can substantially improve the
performance of scene graph generation systems. The focus of this work is to
create an informative hierarchical structure that can divide object and
relationship categories into disjoint super-categories in a systematic way.
Specifically, we introduce a Bayesian prediction head to jointly predict the
super-category of relationships between a pair of object instances, as well as
the detailed relationship within that super-category simultaneously,
facilitating more informative predictions. The resulting model exhibits the
capability to produce a more extensive set of predicates beyond the dataset
annotations, and to tackle the prevalent issue of low annotation quality. While
our paper presents preliminary findings, experiments on the Visual Genome
dataset show its strong performance, particularly in predicate classifications
and zero-shot settings, that demonstrates the promise of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07321">Collision Cross-entropy for Soft Class Labels and Deep Clustering. (arXiv:2303.07321v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boykov_Y/0/1/0/all/0/1">Yuri Boykov</a></p>
<p>We propose "collision cross-entropy" as a robust alternative to Shannon's
cross-entropy (CE) loss when class labels are represented by soft categorical
distributions y. In general, soft labels can naturally represent ambiguous
targets in classification. They are particularly relevant for self-labeled
clustering methods, where latent pseudo-labels are jointly estimated with the
model parameters and uncertainty is prevalent. In case of soft labels,
Shannon's CE teaches the model predictions to reproduce the uncertainty in each
training example, which inhibits the model's ability to learn and generalize
from these examples. As an alternative loss, we propose the negative log of
"collision probability" that maximizes the chance of equality between two
random variables, predicted class and unknown true class. We show that it has
the properties of a generalized CE. The proposed collision CE agrees with
Shannon's CE for one-hot labels, but the training from soft labels differs. For
example, unlike Shannon's CE, data points where y is a uniform distribution
have zero contribution to the training. Collision CE significantly improves
classification supervised by soft uncertain targets. Unlike Shannon's,
collision CE is symmetric for y and network predictions, which is particularly
relevant when both distributions are estimated in the context of self-labeled
clustering. Focusing on discriminative deep clustering where self-labeling and
entropy-based losses are dominant, we show that the use of collision CE
improves the state-of-the-art. We also derive an efficient EM algorithm that
significantly speeds up the pseudo-label estimation with collision CE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08646">High-level Feature Guided Decoding for Semantic Segmentation. (arXiv:2303.08646v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Ye Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Di Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shenghua Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1">Lixin Duan</a></p>
<p>Existing pyramid-based upsamplers (e.g. SemanticFPN), although efficient,
usually produce less accurate results compared to dilation-based models when
using the same backbone. This is partially caused by the contaminated
high-level features since they are fused and fine-tuned with noisy low-level
features on limited data. To address this issue, we propose to use powerful
pre-trained high-level features as guidance (HFG) so that the upsampler can
produce robust results. Specifically, \emph{only} the high-level features from
the backbone are used to train the class tokens, which are then reused by the
upsampler for classification, guiding the upsampler features to more
discriminative backbone features. One crucial design of the HFG is to protect
the high-level features from being contaminated by using proper stop-gradient
operations so that the backbone does not update according to the noisy gradient
from the upsampler. To push the upper limit of HFG, we introduce a context
augmentation encoder (CAE) that can efficiently and effectively operate on the
low-resolution high-level feature, resulting in improved representation and
thus better guidance. We named our complete solution as the High-Level Features
Guided Decoder (HFGD). We evaluate the proposed HFGD on three benchmarks:
Pascal Context, COCOStuff164k, and Cityscapes. HFGD achieves state-of-the-art
results among methods that do not use extra training data, demonstrating its
effectiveness and generalization ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11530">Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images. (arXiv:2303.11530v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1">Akshay Gadi Patil</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fenggen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a></p>
<p>We introduce the first active learning (AL) framework for high-accuracy
instance segmentation of moveable parts from RGB images of real indoor scenes.
As with most human-in-the-loop approaches, the key criterion for success in AL
is to minimize human effort while still attaining high performance. To this
end, we employ a transformer that utilizes a masked-attention mechanism to
supervise the active segmentation. To enhance the network tailored to moveable
parts, we introduce a coarse-to-fine AL approach which first uses an
object-aware masked attention and then a pose-aware one, leveraging the
hierarchical nature of the problem and a correlation between moveable parts and
object poses and interaction directions. Our method achieves close to fully
accurate (96% and higher) segmentation results, with semantic labels, on real
images, with 82% time saving over manual effort, where the training data
consists of only 11.45% annotated real photographs. At last, we contribute a
dataset of 2,550 real photographs with annotated moveable parts, demonstrating
its superior quality and diversity over the current best alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00450">Sketch-based Video Object Localization. (arXiv:2304.00450v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1">Sangmin Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1">So-Yeong Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_M/0/1/0/all/0/1">Minji Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sumin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changick Kim</a></p>
<p>We introduce Sketch-based Video Object Localization (SVOL), a new task aimed
at localizing spatio-temporal object boxes in video queried by the input
sketch. We first outline the challenges in the SVOL task and build the
Sketch-Video Attention Network (SVANet) with the following design principles:
(i) to consider temporal information of video and bridge the domain gap between
sketch and video; (ii) to accurately identify and localize multiple objects
simultaneously; (iii) to handle various styles of sketches; (iv) to be
classification-free. In particular, SVANet is equipped with a Cross-modal
Transformer that models the interaction between learnable object tokens, query
sketch, and video through attention operations, and learns upon a per-frame set
matching strategy that enables frame-wise prediction while utilizing global
video context. We evaluate SVANet on a newly curated SVOL dataset. By design,
SVANet successfully learns the mapping between the query sketches and video
objects, achieving state-of-the-art results on the SVOL benchmark. We further
confirm the effectiveness of SVANet via extensive ablation studies and
visualizations. Lastly, we demonstrate its transfer capability on unseen
datasets and novel categories, suggesting its high scalability in real-world
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01552">Meta-Learning with a Geometry-Adaptive Preconditioner. (arXiv:2304.01552v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Suhyun Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1">Duhun Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Eo_M/0/1/0/all/0/1">Moonjung Eo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taesup Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1">Wonjong Rhee</a></p>
<p>Model-agnostic meta-learning (MAML) is one of the most successful
meta-learning algorithms. It has a bi-level optimization structure where the
outer-loop process learns a shared initialization and the inner-loop process
optimizes task-specific weights. Although MAML relies on the standard gradient
descent in the inner-loop, recent studies have shown that controlling the
inner-loop's gradient descent with a meta-learned preconditioner can be
beneficial. Existing preconditioners, however, cannot simultaneously adapt in a
task-specific and path-dependent way. Additionally, they do not satisfy the
Riemannian metric condition, which can enable the steepest descent learning
with preconditioned gradient. In this study, we propose Geometry-Adaptive
Preconditioned gradient descent (GAP) that can overcome the limitations in
MAML; GAP can efficiently meta-learn a preconditioner that is dependent on
task-specific parameters, and its preconditioner can be shown to be a
Riemannian metric. Thanks to the two properties, the geometry-adaptive
preconditioner is effective for improving the inner-loop optimization.
Experiment results show that GAP outperforms the state-of-the-art MAML family
and preconditioned gradient descent-MAML (PGD-MAML) family in a variety of
few-shot learning tasks. Code is available at:
https://github.com/Suhyun777/CVPR23-GAP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03373">Training-Free Layout Control with Cross-Attention Guidance. (arXiv:2304.03373v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minghao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Laina_I/0/1/0/all/0/1">Iro Laina</a>, <a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1">Andrea Vedaldi</a></p>
<p>Recent diffusion-based generators can produce high-quality images from
textual prompts. However, they often disregard textual instructions that
specify the spatial layout of the composition. We propose a simple approach
that achieves robust layout control without the need for training or
fine-tuning of the image generator. Our technique manipulates the
cross-attention layers that the model uses to interface textual and visual
information and steers the generation in the desired direction given, e.g., a
user-specified layout. To determine how to best guide attention, we study the
role of attention maps and explore two alternative strategies, forward and
backward guidance. We thoroughly evaluate our approach on three benchmarks and
provide several qualitative examples and a comparative analysis of the two
strategies that demonstrate the superiority of backward guidance compared to
forward guidance, as well as prior work. We further demonstrate the versatility
of layout guidance by extending it to applications such as editing the layout
and context of real images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06255">SPColor: Semantic Prior Guided Exemplar-based Image Colorization. (arXiv:2304.06255v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Siqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xueming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xianlin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingdao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a></p>
<p>Exemplar-based image colorization aims to colorize a target grayscale image
based on a color reference image, and the key is to establish accurate
pixel-level semantic correspondence between these two images. Previous methods
search for correspondence across the entire reference image, and this type of
global matching is easy to get mismatch. We summarize the difficulties in two
aspects: (1) When the reference image only contains a part of objects related
to target image, improper correspondence will be established in unrelated
regions. (2) It is prone to get mismatch in regions where the shape or texture
of the object is easily confused. To overcome these issues, we propose SPColor,
a semantic prior guided exemplar-based image colorization framework. Different
from previous methods, SPColor first coarsely classifies pixels of the
reference and target images to several pseudo-classes under the guidance of
semantic prior, then the correspondences are only established locally between
the pixels in the same class via the newly designed semantic prior guided
correspondence network. In this way, improper correspondence between different
semantic classes is explicitly excluded, and the mismatch is obviously
alleviated. Besides, to better reserve the color from reference, a similarity
masked perceptual loss is designed. Noting that the carefully designed SPColor
utilizes the semantic prior provided by an unsupervised segmentation model,
which is free for additional manual semantic annotations. Experiments
demonstrate that our model outperforms recent state-of-the-art methods both
quantitatively and qualitatively on public dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05062">A Feasibility Study on Indoor Localization and Multi-person Tracking Using Sparsely Distributed Camera Network with Edge Computing. (arXiv:2305.05062v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1">Hyeokhyen Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">Chaitra Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiarashi_Y/0/1/0/all/0/1">Yashar Kiarashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Madala_V/0/1/0/all/0/1">Venkata Siva Krishna Madala</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Ratan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakum_A/0/1/0/all/0/1">ArjunSinh Nakum</a>, <a href="http://arxiv.org/find/cs/1/au:+Tweedy_R/0/1/0/all/0/1">Robert Tweedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonetto_L/0/1/0/all/0/1">Leandro Miletto Tonetto</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimring_C/0/1/0/all/0/1">Craig M. Zimring</a>, <a href="http://arxiv.org/find/cs/1/au:+Doiron_M/0/1/0/all/0/1">Matthew Doiron</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1">Amy D. Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Levey_A/0/1/0/all/0/1">Allan I. Levey</a>, <a href="http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1">Gari D. Clifford</a></p>
<p>Camera-based activity monitoring systems are becoming an attractive solution
for smart building applications with the advances in computer vision and edge
computing technologies. In this paper, we present a feasibility study and
systematic analysis of a camera-based indoor localization and multi-person
tracking system implemented on edge computing devices within a large indoor
space. To this end, we deployed an end-to-end edge computing pipeline that
utilizes multiple cameras to achieve localization, body orientation estimation
and tracking of multiple individuals within a large therapeutic space spanning
$1700m^2$, all while maintaining a strong focus on preserving privacy. Our
pipeline consists of 39 edge computing camera systems equipped with Tensor
Processing Units (TPUs) placed in the indoor space's ceiling. To ensure the
privacy of individuals, a real-time multi-person pose estimation algorithm runs
on the TPU of the computing camera system. This algorithm extracts poses and
bounding boxes, which are utilized for indoor localization, body orientation
estimation, and multi-person tracking. Our pipeline demonstrated an average
localization error of 1.41 meters, a multiple-object tracking accuracy score of
88.6\%, and a mean absolute body orientation error of 29\degree. These results
shows that localization and tracking of individuals in a large indoor space is
feasible even with the privacy constrains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05189">SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shanshan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhongzhan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wushao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jinghui Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Diffusion models, which have emerged to become popular text-to-image
generation models, can produce high-quality and content-rich images guided by
textual prompts. However, there are limitations to semantic understanding and
commonsense reasoning in existing models when the input prompts are concise
narrative, resulting in low-quality image generation. To improve the capacities
for narrative prompts, we propose a simple-yet-effective parameter-efficient
fine-tuning approach called the Semantic Understanding and Reasoning adapter
(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first
collect and annotate a new dataset SURD which consists of more than 57,000
semantically corrected multi-modal samples. Each sample contains a simple
narrative prompt, a complex keyword-based prompt, and a high-quality image.
Then, we align the semantic representation of narrative prompts to the complex
prompts and transfer knowledge of large language models (LLMs) to our
SUR-adapter via knowledge distillation so that it can acquire the powerful
semantic understanding and reasoning capabilities to build a high-quality
textual semantic representation for text-to-image generation. We conduct
experiments by integrating multiple LLMs and popular pre-trained diffusion
models to show the effectiveness of our approach in enabling diffusion models
to understand and reason concise natural language without image quality
degradation. Our approach can make text-to-image diffusion models easier to use
with better user experience, which demonstrates our approach has the potential
for further advancing the development of user-friendly text-to-image generation
models by bridging the semantic gap between simple narrative prompts and
complex keyword-based prompts. The code is released at
https://github.com/Qrange-group/SUR-adapter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07598">Hausdorff Distance Matching with Adaptive Query Denoising for Rotated Detection Transformer. (arXiv:2305.07598v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hakjin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Minki Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1">Jamyoung Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Junghoon Seo</a></p>
<p>The Detection Transformer (DETR) has emerged as a pivotal role in object
detection tasks, setting new performance benchmarks due to its end-to-end
design and scalability. Despite its advancements, the application of DETR in
detecting rotated objects has demonstrated suboptimal performance relative to
established oriented object detectors. Our analysis identifies a key
limitation: the L1 cost used in Hungarian Matching leads to duplicate
predictions due to the square-like problem in oriented object detection,
thereby obstructing the training process of the detector. We introduce a
Hausdorff distance-based cost for Hungarian matching, which more accurately
quantifies the discrepancy between predictions and ground truths. Moreover, we
note that a static denoising approach hampers the training of rotated DETR,
particularly when the detector's predictions surpass the quality of noised
ground truths. We propose an adaptive query denoising technique, employing
Hungarian matching to selectively filter out superfluous noised queries that no
longer contribute to model improvement. Our proposed modifications to DETR have
resulted in superior performance, surpassing previous rotated DETR models and
other alternatives. This is evidenced by our model's state-of-the-art
achievements in benchmarks such as DOTA-v1.0/v1.5/v2.0, and DIOR-R.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10579">MultiPlaneNeRF: Neural Radiance Field with Non-Trainable Representation. (arXiv:2305.10579v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zimny_D/0/1/0/all/0/1">Dominik Zimny</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasymov_A/0/1/0/all/0/1">Artur Kasymov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kania_A/0/1/0/all/0/1">Adam Kania</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a>, <a href="http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1">Maciej Zi&#x119;ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1">Przemys&#x142;aw Spurek</a></p>
<p>NeRF is a popular model that efficiently represents 3D objects from 2D
images. However, vanilla NeRF has some important limitations. NeRF must be
trained on each object separately. The training time is long since we encode
the object's shape and color in neural network weights. Moreover, NeRF does not
generalize well to unseen data. In this paper, we present MultiPlaneNeRF -- a
model that simultaneously solves the above problems. Our model works directly
on 2D images. We project 3D points on 2D images to produce non-trainable
representations. The projection step is not parametrized and a very shallow
decoder can efficiently process the representation. Furthermore, we can train
MultiPlaneNeRF on a large data set and force our implicit decoder to generalize
across many objects. Consequently, we can only replace the 2D images (without
additional training) to produce a NeRF representation of the new object. In the
experimental section, we demonstrate that MultiPlaneNeRF achieves results
comparable to state-of-the-art models for synthesizing new views and has
generalization properties. Additionally, MultiPlane decoder can be used as a
component in large generative models like GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10665">Content-based Unrestricted Adversarial Attack. (arXiv:2305.10665v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shuang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kaixun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Shouhong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqiang Zhang</a></p>
<p>Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16316">Making Vision Transformers Truly Shift-Equivariant. (arXiv:2305.16316v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1">Renan A. Rojas-Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_T/0/1/0/all/0/1">Teck-Yian Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh N. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1">Raymond A. Yeh</a></p>
<p>For computer vision, Vision Transformers (ViTs) have become one of the go-to
deep net architectures. Despite being inspired by Convolutional Neural Networks
(CNNs), ViTs' output remains sensitive to small spatial shifts in the input,
i.e., not shift invariant. To address this shortcoming, we introduce novel
data-adaptive designs for each of the modules in ViTs, such as tokenization,
self-attention, patch merging, and positional encoding. With our proposed
modules, we achieve true shift-equivariance on four well-established ViTs,
namely, Swin, SwinV2, CvT, and MViTv2. Empirically, we evaluate the proposed
adaptive models on image classification and semantic segmentation tasks. These
models achieve competitive performance across three different datasets while
maintaining 100% shift consistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18381">Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yue Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong-Lu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1">Kaitong Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cewu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1">Yu-Wing Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chi-Keung Tang</a></p>
<p>Data-efficient learning has garnered significant attention, especially given
the current trend of large multi-modal models. Recently, dataset distillation
becomes an effective approach for data-efficiency; however, the distillation
process itself can still be inefficient. In this work, we model the dataset
distillation task within the context of information transport. By observing the
substantial data redundancy inherent in the distillation, we argue to put more
emphasis on the samples' utility for the distillation task. We introduce and
validate a family of data utility estimators and optimal data selection methods
to exploit the most valuable samples. This strategy significantly reduces the
training costs and extends various existing distillation algorithms to larger
and more diversified datasets, e.g., in some cases only 0.04% training data is
sufficient for comparable distillation performance. Our method consistently
enhances the distillation algorithms, even on much larger-scale and more
heterogeneous datasets, e.g. ImageNet-1K and Kinetics-400. This paradigm opens
up new avenues in the dynamics of distillation and paves the way for efficient
dataset distillation. Our code is available on
https://github.com/silicx/GoldFromOres .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02027">Evolving Knowledge Mining for Class Incremental Segmentation. (arXiv:2306.02027v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhihe Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>Class Incremental Semantic Segmentation (CISS) has been a trend recently due
to its great significance in real-world applications. Although the existing
CISS methods demonstrate remarkable performance, they either leverage the
high-level knowledge (feature) only while neglecting the rich and diverse
knowledge in the low-level features, leading to poor old knowledge preservation
and weak new knowledge exploration; or use multi-level features for knowledge
distillation by retraining a heavy backbone, which is computationally
intensive. In this paper, we for the first time investigate the efficient
multi-grained knowledge reuse for CISS, and propose a novel method, Evolving
kNowleDge minING (ENDING), employing a frozen backbone. ENDING incorporates two
key modules: evolving fusion and semantic enhancement, for dynamic and
comprehensive exploration of multi-grained knowledge. Evolving fusion is
tailored to extract knowledge from individual low-level feature using a
personalized lightweight network, which is generated from a meta-net, taking
the high-level feature as input. This design enables the evolution of knowledge
mining and fusing when applied to incremental new classes. In contrast,
semantic enhancement is specifically crafted to aggregate prototype-based
semantics from multi-level features, contributing to an enhanced
representation. We evaluate our method on two widely used benchmarks and
consistently demonstrate new state-of-the-art performance. The code is
available at https://github.com/zhiheLu/ENDING_ISS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05382">Image Blending Algorithm with Automatic Mask Generation. (arXiv:2306.05382v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Haochen Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Mingyu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuxuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Q/0/1/0/all/0/1">Qian Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaobo Jin</a></p>
<p>In recent years, image blending has gained popularity for its ability to
create visually stunning content. However, the current image blending
algorithms mainly have the following problems: manually creating image blending
masks requires a lot of manpower and material resources; image blending
algorithms cannot effectively solve the problems of brightness distortion and
low resolution. To this end, we propose a new image blending method with
automatic mask generation: it combines semantic object detection and
segmentation with mask generation to achieve deep blended images based on our
proposed new saturation loss and two-stage iteration of the PAN algorithm to
fix brightness distortion and low-resolution issues. Results on publicly
available datasets show that our method outperforms other classical image
blending algorithms on various performance metrics, including PSNR and SSIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10720">Exploring the Relationship between Samples and Masks for Robust Defect Localization. (arXiv:2306.10720v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yaping Yan</a></p>
<p>Defect detection aims to detect and localize regions out of the normal
distribution.Previous approaches model normality and compare it with the input
to identify defective regions, potentially limiting their generalizability.This
paper proposes a one-stage framework that detects defective patterns directly
without the modeling process.This ability is adopted through the joint efforts
of three parties: a generative adversarial network (GAN), a newly proposed
scaled pattern loss, and a dynamic masked cycle-consistent auxiliary network.
Explicit information that could indicate the position of defects is
intentionally excluded to avoid learning any direct mapping.Experimental
results on the texture class of the challenging MVTec AD dataset show that the
proposed method is 2.9% higher than the SOTA methods in F1-Score, while
substantially outperforming SOTA methods in generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15832">Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deck_K/0/1/0/all/0/1">Katherine Deck</a>, <a href="http://arxiv.org/find/cs/1/au:+Bischoff_T/0/1/0/all/0/1">Tobias Bischoff</a></p>
<p>Generated images of score-based models can suffer from errors in their
spatial means, an effect, referred to as a color shift, which grows for larger
images. This paper investigates a previously-introduced approach to mitigate
color shifts in score-based diffusion models. We quantify the performance of a
nonlinear bypass connection in the score network, designed to process the
spatial mean of the input and to predict the mean of the score function. We
show that this network architecture substantially improves the resulting
quality of the generated images, and that this improvement is approximately
independent of the size of the generated images. As a result, this modified
architecture offers a simple solution for the color shift problem across image
sizes. We additionally discuss the origin of color shifts in an idealized
setting in order to motivate the approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16269">RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model. (arXiv:2306.16269v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haotian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zhengxia Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenwei Shi</a></p>
<p>Leveraging the extensive training data from SA-1B, the Segment Anything Model
(SAM) demonstrates remarkable generalization and zero-shot capabilities.
However, as a category-agnostic instance segmentation method, SAM heavily
relies on prior manual guidance, including points, boxes, and coarse-grained
masks. Furthermore, its performance in remote sensing image segmentation tasks
remains largely unexplored and unproven. In this paper, we aim to develop an
automated instance segmentation approach for remote sensing images, based on
the foundational SAM model and incorporating semantic category information.
Drawing inspiration from prompt learning, we propose a method to learn the
generation of appropriate prompts for SAM. This enables SAM to produce
semantically discernible segmentation results for remote sensing images, a
concept we have termed RSPrompter. We also propose several ongoing derivatives
for instance segmentation tasks, drawing on recent advancements within the SAM
community, and compare their performance with RSPrompter. Extensive
experimental results, derived from the WHU building, NWPU VHR-10, and SSDD
datasets, validate the effectiveness of our proposed method. The code for our
method is publicly available at kychen.me/RSPrompter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16741">Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train. (arXiv:2306.16741v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1">Qi Dou</a></p>
<p>Foundation models have exhibited remarkable success in various applications,
such as disease diagnosis and text report generation. To date, a foundation
model for endoscopic video analysis is still lacking. In this paper, we propose
Endo-FM, a foundation model specifically developed using massive endoscopic
video data. First, we build a video transformer, which captures both local and
global long-range dependencies across spatial and temporal dimensions. Second,
we pre-train our transformer model using global and local views via a
self-supervised manner, aiming to make it robust to spatial-temporal variations
and discriminative across different scenes. To develop the foundation model, we
construct a large-scale endoscopy video dataset by combining 9 publicly
available datasets and a privately collected dataset from Baoshan Branch of
Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K
video clips with up to 5 million frames, encompassing various protocols, target
organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a
given downstream task via fine-tuning by serving as the backbone. With
experiments on 3 different types of downstream tasks, including classification,
segmentation, and detection, our Endo-FM surpasses the current state-of-the-art
(SOTA) self-supervised pre-training and adapter-based transfer learning methods
by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for
classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6%
Dice, and 9.9% F1 for classification, segmentation, and detection). Code,
datasets, and models are released at https://github.com/med-air/Endo-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16772">Learning from Synthetic Human Group Activities. (arXiv:2306.16772v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Che-Jui Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Danrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1">Deep Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1">Parth Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Honglu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seonghyeon Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1">Samuel S. Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sejong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1">Vladimir Pavlovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1">Mubbasir Kapadia</a></p>
<p>The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by the Unity engine, M3Act
features multiple semantic groups, highly diverse and photorealistic images,
and a comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments using various input modalities. First, adding our synthetic data
significantly improves the performance of MOTRv2 on DanceTrack, leading to a
hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking
results on par with MOTRv2*, which is trained with 62.5% more real-world data.
Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on
group activity and atomic action accuracy respectively. Moreover, M3Act opens
new research for controllable 3D group activity generation. We define multiple
metrics and propose a competitive baseline for the novel task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04760">Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos. (arXiv:2307.04760v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1">Sagnik Majumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1">Ziad Al-Halah</a>, <a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1">Kristen Grauman</a></p>
<p>We propose a self-supervised method for learning representations based on
spatial audio-visual correspondences in egocentric videos. Our method uses a
masked auto-encoding framework to synthesize masked binaural (multi-channel)
audio through the synergy of audio and vision, thereby learning useful spatial
relationships between the two modalities. We use our pretrained features to
tackle two downstream video tasks requiring spatial understanding in social
scenarios: active speaker detection and spatial audio denoising. Through
extensive experiments, we show that our features are generic enough to improve
over multiple state-of-the-art baselines on both tasks on two challenging
egocentric video datasets that offer binaural audio, EgoCom and EasyCom.
Project: <a href="http://vision.cs.utexas.edu/projects/ego_av_corr.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05468">My3DGen: A Scalable Personalized 3D Generative Model. (arXiv:2307.05468v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Luchao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiaye Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Annie N. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_R/0/1/0/all/0/1">Roni Sengupta</a></p>
<p>In recent years, generative 3D face models (e.g., EG3D) have been developed
to tackle the problem of synthesizing photo-realistic faces. However, these
models are often unable to capture facial features unique to each individual,
highlighting the importance of personalization. Some prior works have shown
promise in personalizing generative face models, but these studies primarily
focus on 2D settings. Also, these methods require both fine-tuning and storing
a large number of parameters for each user, posing a hindrance to achieving
scalable personalization. Another challenge of personalization is the limited
number of training images available for each individual, which often leads to
overfitting when using full fine-tuning methods. Our proposed approach,
My3DGen, generates a personalized 3D prior of an individual using as few as 50
training images. My3DGen allows for novel view synthesis, semantic editing of a
given face (e.g. adding a smile), and synthesizing novel appearances, all while
preserving the original person's identity. We decouple the 3D facial features
into global features and personalized features by freezing the pre-trained EG3D
and training additional personalized weights through low-rank decomposition. As
a result, My3DGen introduces only $\textbf{240K}$ personalized parameters per
individual, leading to a $\textbf{127}\times$ reduction in trainable parameters
compared to the $\textbf{30.6M}$ required for fine-tuning the entire parameter
space. Despite this significant reduction in storage, our model preserves
identity features without compromising the quality of downstream applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00688">AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1">Nikhil Keetha</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Avneesh Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Karhade_J/0/1/0/all/0/1">Jay Karhade</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1">Krishna Murthy Jatavallabhula</a>, <a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1">Sebastian Scherer</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_M/0/1/0/all/0/1">Madhava Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Sourav Garg</a></p>
<p>Visual Place Recognition (VPR) is vital for robot localization. To date, the
most performant VPR approaches are environment- and task-specific: while they
exhibit strong performance in structured environments (predominantly urban
driving), their performance degrades severely in unstructured environments,
rendering most approaches brittle to robust real-world deployment. In this
work, we develop a universal solution to VPR -- a technique that works across a
broad range of structured and unstructured environments (urban, outdoors,
indoors, aerial, underwater, and subterranean environments) without any
re-training or fine-tuning. We demonstrate that general-purpose feature
representations derived from off-the-shelf self-supervised models with no
VPR-specific training are the right substrate upon which to build such a
universal VPR solution. Combining these derived features with unsupervised
feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X
significantly higher performance than existing approaches. We further obtain a
6% improvement in performance by characterizing the semantic properties of
these features, uncovering unique domains which encapsulate datasets from
similar environments. Our detailed experiments and analysis lay a foundation
for building VPR solutions that may be deployed anywhere, anytime, and across
anyview. We encourage the readers to explore our project page and interactive
demos: https://anyloc.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01766">Neural Poisson Surface Reconstruction: Resolution-Agnostic Shape Reconstruction from Point Clouds. (arXiv:2308.01766v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andrade_Loarca_H/0/1/0/all/0/1">Hector Andrade-Loarca</a>, <a href="http://arxiv.org/find/cs/1/au:+Hege_J/0/1/0/all/0/1">Julius Hege</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1">Gitta Kutyniok</a></p>
<p>We introduce Neural Poisson Surface Reconstruction (nPSR), an architecture
for shape reconstruction that addresses the challenge of recovering 3D shapes
from points. Traditional deep neural networks face challenges with common 3D
shape discretization techniques due to their computational complexity at higher
resolutions. To overcome this, we leverage Fourier Neural Operators to solve
the Poisson equation and reconstruct a mesh from oriented point cloud
measurements. nPSR exhibits two main advantages: First, it enables efficient
training on low-resolution data while achieving comparable performance at
high-resolution evaluation, thanks to the resolution-agnostic nature of FNOs.
This feature allows for one-shot super-resolution. Second, our method surpasses
existing approaches in reconstruction quality while being differentiable and
robust with respect to point sampling rates. Overall, the neural Poisson
surface reconstruction not only improves upon the limitations of classical deep
neural networks in shape reconstruction but also achieves superior results in
terms of reconstruction quality, running time, and resolution agnosticism.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04638">GeoAdapt: Self-Supervised Test-Time Adaptation in LiDAR Place Recognition Using Geometric Priors. (arXiv:2308.04638v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1">Joshua Knights</a>, <a href="http://arxiv.org/find/cs/1/au:+Hausler_S/0/1/0/all/0/1">Stephen Hausler</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1">Sridha Sridharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1">Clinton Fookes</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1">Peyman Moghadam</a></p>
<p>LiDAR place recognition approaches based on deep learning suffer from
significant performance degradation when there is a shift between the
distribution of training and test datasets, often requiring re-training the
networks to achieve peak performance. However, obtaining accurate ground truth
data for new training data can be prohibitively expensive, especially in
complex or GPS-deprived environments. To address this issue we propose
GeoAdapt, which introduces a novel auxiliary classification head to generate
pseudo-labels for re-training on unseen environments in a self-supervised
manner. GeoAdapt uses geometric consistency as a prior to improve the
robustness of our generated pseudo-labels against domain shift, improving the
performance and reliability of our Test-Time Adaptation approach. Comprehensive
experiments show that GeoAdapt significantly boosts place recognition
performance across moderate to severe domain shifts, and is competitive with
fully supervised test-time adaptation approaches. Our code is available at
https://github.com/csiro-robotics/GeoAdapt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08333">Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN. (arXiv:2308.08333v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiawei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofeng Zhang</a></p>
<p>Monocular depth estimation is an ongoing challenge in computer vision. Recent
progress with Transformer models has demonstrated notable advantages over
conventional CNNs in this area. However, there's still a gap in understanding
how these models prioritize different regions in 2D images and how these
regions affect depth estimation performance. To explore the differences between
Transformers and CNNs, we employ a sparse pixel approach to contrastively
analyze the distinctions between the two. Our findings suggest that while
Transformers excel in handling global context and intricate textures, they lag
behind CNNs in preserving depth gradient continuity. To further enhance the
performance of Transformer models in monocular depth estimation, we propose the
Depth Gradient Refinement (DGR) module that refines depth estimation through
high-order differentiation, feature fusion, and recalibration. Additionally, we
leverage optimal transport theory, treating depth maps as spatial probability
distributions, and employ the optimal transport distance as a loss function to
optimize our model. Experimental results demonstrate that models integrated
with the plug-and-play Depth Gradient Refinement (DGR) module and the proposed
loss function enhance performance without increasing complexity and
computational costs on both outdoor KITTI and indoor NYU-Depth-v2 datasets.
This research not only offers fresh insights into the distinctions between
Transformers and CNNs in depth estimation but also paves the way for novel
depth estimation methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10743">Enhancing Adversarial Attacks: The Similar Target Method. (arXiv:2308.10743v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziruo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zikai Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanran Chen</a></p>
<p>Deep neural networks are vulnerable to adversarial examples, posing a threat
to the models' applications and raising security concerns. An intriguing
property of adversarial examples is their strong transferability. Several
methods have been proposed to enhance transferability, including ensemble
attacks which have demonstrated their efficacy. However, prior approaches
simply average logits, probabilities, or losses for model ensembling, lacking a
comprehensive analysis of how and why model ensembling significantly improves
transferability. In this paper, we propose a similar targeted attack method
named Similar Target~(ST). By promoting cosine similarity between the gradients
of each model, our method regularizes the optimization direction to
simultaneously attack all surrogate models. This strategy has been proven to
enhance generalization ability. Experimental results on ImageNet validate the
effectiveness of our approach in improving adversarial transferability. Our
method outperforms state-of-the-art attackers on 18 discriminative classifiers
and adversarially trained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15068">A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yaping Yan</a></p>
<p>Data augmentation methods are commonly integrated into the training of
anomaly detection models. Previous approaches have primarily focused on
replicating real-world anomalies or enhancing diversity, without considering
that the standard of anomaly varies across different classes, potentially
leading to a biased training distribution.This paper analyzes crucial traits of
simulated anomalies that contribute to the training of reconstructive networks
and condenses them into several methods, thus creating a comprehensive
framework by selectively utilizing appropriate combinations.Furthermore, we
integrate this framework with a reconstruction-based approach and concurrently
propose a split training strategy that alleviates the issue of overfitting
while avoiding introducing interference to the reconstruction process. The
evaluations conducted on the MVTec anomaly detection dataset demonstrate that
our method outperforms the previous state-of-the-art approach, particularly in
terms of object classes. To evaluate generalizability, we generate a simulated
dataset comprising anomalies with diverse characteristics since the original
test samples only include specific types of anomalies and may lead to biased
evaluations. Experimental results demonstrate that our approach exhibits
promising potential for generalizing effectively to various unforeseen
anomalies encountered in real-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14327">DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Conglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Heyang Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1">Olatunji Ruwase</a>, <a href="http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1">Ammar Ahmad Awan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1">Samyam Rajbhandari</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuxiong He</a></p>
<p>Most of the existing multi-modal models, hindered by their incapacity to
adeptly manage interleaved image-and-text inputs in multi-image, multi-round
dialogues, face substantial constraints in resource allocation for training and
data accessibility, impacting their adaptability and scalability across varied
interaction realms. To address this, we present the DeepSpeed-VisualChat
framework, designed to optimize Large Language Models (LLMs) by incorporating
multi-modal capabilities, with a focus on enhancing the proficiency of Large
Vision and Language Models in handling interleaved inputs. Our framework is
notable for (1) its open-source support for multi-round and multi-image
dialogues, (2) introducing an innovative multi-modal causal attention
mechanism, and (3) utilizing data blending techniques on existing datasets to
assure seamless interactions in multi-round, multi-image conversations.
Compared to existing frameworks, DeepSpeed-VisualChat shows superior
scalability up to 70B parameter language model size, representing a significant
advancement in multi-modal language models and setting a solid foundation for
future explorations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01406">HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation. (arXiv:2310.01406v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1">Ruizhi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Ying Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qing Wang</a></p>
<p>Recent text-to-3D methods employing diffusion models have made significant
advancements in 3D human generation. However, these approaches face challenges
due to the limitations of text-to-image diffusion models, which lack an
understanding of 3D structures. Consequently, these methods struggle to achieve
high-quality human generation, resulting in smooth geometry and cartoon-like
appearances. In this paper, we propose HumanNorm, a novel approach for
high-quality and realistic 3D human generation. The main idea is to enhance the
model's 2D perception of 3D geometry by learning a normal-adapted diffusion
model and a normal-aligned diffusion model. The normal-adapted diffusion model
can generate high-fidelity normal maps corresponding to user prompts with
view-dependent and body-aware text. The normal-aligned diffusion model learns
to generate color images aligned with the normal maps, thereby transforming
physical geometry details into realistic appearance. Leveraging the proposed
normal diffusion model, we devise a progressive geometry generation strategy
and a multi-step Score Distillation Sampling (SDS) loss to enhance the
performance of 3D human generation. Comprehensive experiments substantiate
HumanNorm's ability to generate 3D humans with intricate geometry and realistic
appearances. HumanNorm outperforms existing text-to-3D methods in both geometry
and texture quality. The project page of HumanNorm is
https://humannorm.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05055">FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis. (arXiv:2310.05055v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1">Raman Dutt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1">Ondrej Bohdal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1">Sotirios A. Tsaftaris</a>, <a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1">Timothy Hospedales</a></p>
<p>Training models with robust group fairness properties is crucial in ethically
sensitive application areas such as medical diagnosis. Despite the growing body
of work aiming to minimise demographic bias in AI, this problem remains
challenging. A key reason for this challenge is the fairness generalisation
gap: High-capacity deep learning models can fit all training data nearly
perfectly, and thus also exhibit perfect fairness during training. In this
case, bias emerges only during testing when generalisation performance differs
across subgroups. This motivates us to take a bi-level optimisation perspective
on fair learning: Optimising the learning strategy based on validation
fairness. Specifically, we consider the highly effective workflow of adapting
pre-trained models to downstream medical imaging tasks using
parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between
updating more parameters, enabling a better fit to the task of interest vs.
fewer parameters, potentially reducing the generalisation gap. To manage this
tradeoff, we propose FairTune, a framework to optimise the choice of PEFT
parameters with respect to fairness. We demonstrate empirically that FairTune
leads to improved fairness on a range of medical imaging datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05989">DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection. (arXiv:2310.05989v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiawei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1">Yingxin Lai</a></p>
<p>3D object detection is crucial for applications like autonomous driving and
robotics. While query-based 3D object detection for BEV (Bird's Eye View)
images has seen significant advancements, most existing methods follows the
paradigm of static query. Such paradigm is incapable of adapting to complex
spatial-temporal relationships in the scene. To solve this problem, we
introduce a new paradigm in DynamicBEV, a novel approach that employs dynamic
queries for BEV-based 3D object detection. In contrast to static queries, the
proposed dynamic queries exploit K-means clustering and Top-K Attention in a
creative way to aggregate information more effectively from both local and
distant feature, which enable DynamicBEV to adapt iteratively to complex
scenes. To further boost efficiency, DynamicBEV incorporates a Lightweight
Temporal Fusion Module (LTFM), designed for efficient temporal context
integration with a significant computation reduction. Additionally, a
custom-designed Diversity Loss ensures a balanced feature representation across
scenarios. Extensive experiments on the nuScenes dataset validate the
effectiveness of DynamicBEV, establishing a new state-of-the-art and heralding
a paradigm-level breakthrough in query-based BEV object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06389">Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling. (arXiv:2310.06389v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Huangjie Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhendong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jianbo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_G/0/1/0/all/0/1">Guanghan Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pengcheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1">Quanzeng You</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a></p>
<p>Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14566">HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination &amp; Visual Illusion in Large Vision-Language Models. (arXiv:2310.14566v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1">Tianrui Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fuxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1">Ruiqi Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lichang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1">Yaser Yacoob</a>, <a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a></p>
<p>We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision) and LLaVA-1.5, by emphasizing nuanced understanding and
interpretation of visual data. The benchmark comprises 346 images paired with
1129 questions, all meticulously crafted by human experts. We introduce a novel
structure for these visual questions designed to establish control groups. This
structure enables us to conduct a quantitative analysis of the models' response
tendencies, logical consistency, and various failure modes. In our evaluation
on HallusionBench, we benchmarked 13 different models, highlighting a 31.42%
question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all
other evaluated models achieve accuracy below 16%. Moreover, our analysis not
only highlights the observed failure modes, including language hallucination
and visual illusion, but also deepens an understanding of these pitfalls. Our
comprehensive case studies within HallusionBench shed light on the challenges
of hallucination and illusion in LVLMs. Based on these insights, we suggest
potential pathways for their future improvement. The benchmark and codebase can
be accessed at https://github.com/tianyi-lab/HallusionBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16267">Student Classroom Behavior Detection based on Spatio-Temporal Network and Multi-Model Fusion. (arXiv:2310.16267v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofei Wang</a></p>
<p>Using deep learning methods to detect students' classroom behavior
automatically is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
spatio-temporal datasets on student behavior, as well as the high cost of
manually labeling such datasets, pose significant challenges for researchers in
this field. To address this issue, we proposed a method for extending the
spatio-temporal behavior dataset in Student Classroom Scenarios
(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265
images with 25810 labels, focusing on 3 behaviors: hand-raising, reading,
writing. Our proposed method can rapidly generate spatio-temporal behavior
datasets without requiring extra manual labeling. Furthermore, we proposed a
Behavior Similarity Index (BSI) to explore the similarity of behaviors. We
evaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast
algorithms, achieving a mean average precision (map) of up to 82.3%. Last, we
fused multiple models to generate student behavior-related data from various
perspectives. The experiment further demonstrates the effectiveness of our
method. And SCB-ST-Dataset4 provides a robust foundation for future research in
student behavior detection, potentially contributing to advancements in this
field. The SCB-ST-Dataset4 is available for download at:
https://github.com/Whiffe/SCB-dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17347">CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling. (arXiv:2310.17347v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadat_S/0/1/0/all/0/1">Seyedmorteza Sadat</a>, <a href="http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1">Jakob Buhmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_D/0/1/0/all/0/1">Derek Bradley</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1">Romann M. Weber</a></p>
<p>While conditional diffusion models are known to have good coverage of the
data distribution, they still face limitations in output diversity,
particularly when sampled with a high classifier-free guidance scale for
optimal image quality or when trained on small datasets. We attribute this
problem to the role of the conditioning signal in inference and offer an
improved sampling strategy for diffusion models that can increase generation
diversity, especially at high guidance scales, with minimal loss of sample
quality. Our sampling strategy anneals the conditioning signal by adding
scheduled, monotonically decreasing Gaussian noise to the conditioning vector
during inference to balance diversity and condition alignment. Our
Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained
model and sampling algorithm, and we show that it boosts the diversity of
diffusion models in various conditional generation tasks. Further, using an
existing pretrained diffusion model, CADS achieves a new state-of-the-art FID
of 1.70 and 2.31 for class-conditional ImageNet generation at 256$\times$256
and 512$\times$512 respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17462">Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1">Daniel Kienzle</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1">Julian Lorenz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1">Katja Ludwig</a>, <a href="http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1">Rainer Lienhart</a></p>
<p>We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object's
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method's potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18297">Image Clustering Conditioned on Text Criteria. (arXiv:2310.18297v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1">Sehyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jaeseung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minkyu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jaewoong Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1">Ernest K. Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kangwook Lee</a></p>
<p>Classical clustering methods do not provide users with direct control of the
clustering results, and the clustering results may not be consistent with the
relevant criterion that a user has in mind. In this work, we present a new
methodology for performing image clustering based on user-specified text
criteria by leveraging modern vision-language models and large language models.
We call our method Image Clustering Conditioned on Text Criteria (IC|TC), and
it represents a different paradigm of image clustering. IC|TC requires a
minimal and practical degree of human intervention and grants the user
significant control over the clustering results in return. Our experiments show
that IC|TC can effectively cluster images with various criteria, such as human
action, physical location, or the person's mood, while significantly
outperforming baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18348">Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tian Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1">Alessandro Achille</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1">Pramuditha Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1">Luca Zancato</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models. Our code is available at:
https://github.com/tianyu139/meaning-as-trajectories
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19583">GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo. (arXiv:2310.19583v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vats_V/0/1/0/all/0/1">Vibhas K. Vats</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Sripad Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1">David J. Crandall</a>, <a href="http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1">Md. Alimoor Reza</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1">Soon-heung Jung</a></p>
<p>Traditional multi-view stereo (MVS) methods rely heavily on photometric and
geometric consistency constraints, but newer machine learning-based MVS methods
check geometric consistency across multiple source views only as a
post-processing step. In this paper, we present a novel approach that
explicitly encourages geometric consistency of reference view depth maps across
multiple source views at different scales during learning (see Fig. 1). We find
that adding this geometric consistency loss significantly accelerates learning
by explicitly penalizing geometrically inconsistent pixels, reducing the
training iteration requirements to nearly half that of other MVS methods. Our
extensive experiments show that our approach achieves a new state-of-the-art on
the DTU and BlendedMVS datasets, and competitive results on the Tanks and
Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt
to enforce multi-view, multi-scale geometric consistency during learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20208">ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection. (arXiv:2310.20208v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">Youwei Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiaoqi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tian-Zhu Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lihe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Huchuan Lu</a></p>
<p>Recent camouflaged object detection (COD) attempts to segment objects
visually blended into their surroundings, which is extremely complex and
difficult in real-world scenarios. Apart from the high intrinsic similarity
between camouflaged objects and their background, objects are usually diverse
in scale, fuzzy in appearance, and even severely occluded. To this end, we
propose an effective unified collaborative pyramid network which mimics human
behavior when observing vague images and videos, \textit{i.e.}, zooming in and
out. Specifically, our approach employs the zooming strategy to learn
discriminative mixed-scale semantics by the multi-head scale integration and
rich granularity perception units, which are designed to fully explore
imperceptible clues between candidate objects and background surroundings. The
former's intrinsic multi-head aggregation provides more diverse visual
patterns. The latter's routing mechanism can effectively propagate inter-frame
difference in spatiotemporal scenarios and adaptively ignore static
representations. They provides a solid foundation for realizing a unified
architecture for static and dynamic COD. Moreover, considering the uncertainty
and ambiguity derived from indistinguishable textures, we construct a simple
yet effective regularization, uncertainty awareness loss, to encourage
predictions with higher confidence in candidate regions. Our highly
task-friendly framework consistently outperforms existing state-of-the-art
methods in image and video COD benchmarks. The code will be available at
\url{https://github.com/lartpang/ZoomNeXt}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00213">Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jiaxin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tianjun Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tong He</a></p>
<p>We introduce a novel and efficient approach for text-based video-to-video
editing that eliminates the need for resource-intensive per-video-per-model
finetuning. At the core of our approach is a synthetic paired video dataset
tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's
image transfer via editing instruction, we adapt this paradigm to the video
domain. Extending the Prompt-to-Prompt to videos, we efficiently generate
paired samples, each with an input video and its edited counterpart. Alongside
this, we introduce the Long Video Sampling Correction during sampling, ensuring
consistent long videos across batches. Our method surpasses current methods
like Tune-A-Video, heralding substantial progress in text-based video-to-video
editing and suggesting exciting avenues for further exploration and deployment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07184">Cross-Axis Transformer with 2D Rotary Embeddings. (arXiv:2311.07184v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erickson_L/0/1/0/all/0/1">Lily Erickson</a></p>
<p>Despite lagging behind their modal cousins in many respects, Vision
Transformers have provided an interesting opportunity to bridge the gap between
sequence modeling and image modeling. Up until now however, vision transformers
have largely been held back, due to both computational inefficiency, and lack
of proper handling of spatial dimensions. In this paper, we introduce the
Cross-Axis Transformer. CAT is a model inspired by both Axial Transformers, and
Microsoft's recent Retentive Network, that drastically reduces the required
number of floating point operations required to process an image, while
simultaneously converging faster and more accurately than the Vision
Transformers it replaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07574">To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning. (arXiv:2311.07574v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1">Lingchen Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1">Zejia Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zuxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Existing visual instruction tuning methods typically prompt large language
models with textual descriptions to generate instruction-following data.
Despite the promising performance achieved, these descriptions are derived from
image annotations, which are oftentimes coarse-grained. Furthermore, the
instructions might even contradict the visual content without observing the
entire visual context. To address this challenge, we introduce a fine-grained
visual instruction dataset, LVIS-Instruct4V, which contains 220K visually
aligned and context-aware instructions produced by prompting the powerful
GPT-4V with images from LVIS. Through experimental validation and case studies,
we demonstrate that high-quality visual instructional data could improve the
performance of LLaVA-1.5, a state-of-the-art large multimodal model, across a
wide spectrum of benchmarks by clear margins. Notably, by simply replacing the
LLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA
on most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet
(40.2 vs. 35.4). We release our data and model at
https://github.com/X2FD/LVIS-INSTRUCT4V.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08100">DeepEMplanner: An End-to-End EM Motion Planner with Iterative Interactions. (arXiv:2311.08100v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhili Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1">Maosheng Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuangjie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1">Tongyi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qifeng Chen</a></p>
<p>Motion planning is a computational problem that finds a sequence of valid
trajectories, often based on surrounding agents' forecasting, environmental
understanding, and historical and future contexts. It can also be viewed as a
game in which agents continuously plan their next move according to other
agents' intentions and the encountering environment, further achieving their
ultimate goals through incremental actions. To model the dynamic planning and
interaction process, we propose a novel framework, DeepEMplanner, which takes
the stepwise interaction into account for fine-grained behavior learning. The
ego vehicle maximizes each step motion to reach its eventual driving outcome
based on the stepwise expectation from agents and its upcoming road conditions.
On the other hand, the agents also follow the same philosophy to maximize their
stepwise behavior under the encountering environment and the expectations from
ego and other agents. Our DeepEMplanner models the interactions among ego,
agents, and the dynamic environment in an autoregressive manner by interleaving
the Expectation and Maximization processes. Further, we design ego-to-agents,
ego-to-map, and ego-to-BEV interaction mechanisms with hierarchical dynamic key
objects attention to better model the interactions. Experiments on the nuScenes
benchmark show that our approach achieves state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08972">Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging. (arXiv:2311.08972v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1">Marcello Carioni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhadip Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1">Hong Ye Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Junqi Tang</a></p>
<p>Unsupervised deep learning approaches have recently become one of the crucial
research areas in imaging owing to their ability to learn expressive and
powerful reconstruction operators even when paired high-quality training data
is scarcely available. In this chapter, we review theoretically principled
unsupervised learning schemes for solving imaging inverse problems, with a
particular focus on methods rooted in optimal transport and convex analysis. We
begin by reviewing the optimal transport-based unsupervised approaches such as
the cycle-consistency-based models and learned adversarial regularization
methods, which have clear probabilistic interpretations. Subsequently, we give
an overview of a recent line of works on provably convergent learned
optimization algorithms applied to accelerate the solution of imaging inverse
problems, alongside their dedicated unsupervised training schemes. We also
survey a number of provably convergent plug-and-play algorithms (based on
gradient-step deep denoisers), which are among the most important and widely
applied unsupervised approaches for imaging problems. At the end of this
survey, we provide an overview of a few related unsupervised learning
frameworks that complement our focused schemes. Together with a detailed
survey, we provide an overview of the key mathematical results that underlie
the methods reviewed in the chapter to keep our discussion self-contained.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09257">UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zhisheng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1">Tingbo Hou</a></p>
<p>Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10793">Traffic Sign Interpretation in Real Road Scene. (arXiv:2311.10793v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chuang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_K/0/1/0/all/0/1">Kai Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mulin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haozhao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Changxing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Han Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bingxuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qi Wang</a></p>
<p>Most existing traffic sign-related works are dedicated to detecting and
recognizing part of traffic signs individually, which fails to analyze the
global semantic logic among signs and may convey inaccurate traffic
instruction. Following the above issues, we propose a traffic sign
interpretation (TSI) task, which aims to interpret global semantic interrelated
traffic signs (e.g.,~driving instruction-related texts, symbols, and guide
panels) into a natural language for providing accurate instruction support to
autonomous or assistant driving. Meanwhile, we design a multi-task learning
architecture for TSI, which is responsible for detecting and recognizing
various traffic signs and interpreting them into a natural language like a
human. Furthermore, the absence of a public TSI available dataset prompts us to
build a traffic sign interpretation dataset, namely TSI-CN. The dataset
consists of real road scene images, which are captured from the highway and the
urban way in China from a driver's perspective. It contains rich location
labels of texts, symbols, and guide panels, and the corresponding natural
language description labels. Experiments on TSI-CN demonstrate that the TSI
task is achievable and the TSI architecture can interpret traffic signs from
scenes successfully even if there is a complex semantic logic among signs. The
TSI-CN dataset and the source code of the TSI architecture will be publicly
available after the revision process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10813">A Language Agent for Autonomous Driving. (arXiv:2311.10813v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jiageng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Junjie Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yuxi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a></p>
<p>Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11772">A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology. (arXiv:2311.11772v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolflein_G/0/1/0/all/0/1">Georg W&#xf6;lflein</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferber_D/0/1/0/all/0/1">Dyke Ferber</a>, <a href="http://arxiv.org/find/cs/1/au:+Meneghetti_A/0/1/0/all/0/1">Asier Rabasco Meneghetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Nahhas_O/0/1/0/all/0/1">Omar S. M. El Nahhas</a>, <a href="http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1">Daniel Truhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Carrero_Z/0/1/0/all/0/1">Zunamys I. Carrero</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1">David J. Harrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1">Ognjen Arandjelovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1">Jakob N. Kather</a></p>
<p>Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12754">SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction. (arXiv:2311.12754v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuanhui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wenzhao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Borui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a></p>
<p>3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird's eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on nuScenes. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12775">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering. (arXiv:2311.12775v2 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guedon_A/0/1/0/all/0/1">Antoine Gu&#xe9;don</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1">Vincent Lepetit</a></p>
<p>We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality. Our project page is the following:
https://imagine.enpc.fr/~guedona/sugar/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13187">NeISF: Neural Incident Stokes Field for Geometry and Material Estimation. (arXiv:2311.13187v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ono_T/0/1/0/all/0/1">Taishi Ono</a>, <a href="http://arxiv.org/find/cs/1/au:+Uemori_T/0/1/0/all/0/1">Takeshi Uemori</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihara_H/0/1/0/all/0/1">Hajime Mihara</a>, <a href="http://arxiv.org/find/cs/1/au:+Gatto_A/0/1/0/all/0/1">Alexander Gatto</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1">Hajime Nagahara</a>, <a href="http://arxiv.org/find/cs/1/au:+Moriuchi_Y/0/1/0/all/0/1">Yusuke Moriuchi</a></p>
<p>Multi-view inverse rendering is the problem of estimating the scene
parameters such as shapes, materials, or illuminations from a sequence of
images captured under different viewpoints. Many approaches, however, assume
single light bounce and thus fail to recover challenging scenarios like
inter-reflections. On the other hand, simply extending those methods to
consider multi-bounced light requires more assumptions to alleviate the
ambiguity. To address this problem, we propose Neural Incident Stokes Fields
(NeISF), a multi-view inverse rendering framework that reduces ambiguities
using polarization cues. The primary motivation for using polarization cues is
that it is the accumulation of multi-bounced light, providing rich information
about geometry and material. Based on this knowledge, the proposed incident
Stokes field efficiently models the accumulated polarization effect with the
aid of an original physically-based differentiable polarimetric renderer.
Lastly, experimental results show that our method outperforms the existing
works in synthetic and real scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13562">Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object. (arXiv:2311.13562v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1">Peng Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingbo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1">Hongwu Lv</a></p>
<p>Image style transfer occupies an important place in both computer graphics
and computer vision. However, most current methods require reference to
stylized images and cannot individually stylize specific objects. To overcome
this limitation, we propose the "Soulstyler" framework, which allows users to
guide the stylization of specific objects in an image through simple textual
descriptions. We introduce a large language model to parse the text and
identify stylization goals and specific styles. Combined with a CLIP-based
semantic visual embedding encoder, the model understands and matches text and
image content. We also introduce a novel localized text-image block matching
loss that ensures that style transfer is performed only on specified target
objects, while non-target regions remain in their original style. Experimental
results demonstrate that our model is able to accurately perform style transfer
on target objects according to textual descriptions without affecting the style
of background regions. Our code will be available at
https://github.com/yisuanwang/Soulstyler.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14284">Paragraph-to-Image Generation with Information-Enriched Diffusion Model. (arXiv:2311.14284v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yefei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lele Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tingting Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a></p>
<p>Text-to-image (T2I) models have recently experienced rapid development,
achieving astonishing performance in terms of fidelity and textual alignment
capabilities. However, given a long paragraph (up to 512 words), these
generation models still struggle to achieve strong alignment and are unable to
generate images depicting complex scenes. In this paper, we introduce an
information-enriched diffusion model for paragraph-to-image generation task,
termed ParaDiffusion, which delves into the transference of the extensive
semantic comprehension capabilities of large language models to the task of
image generation. At its core is using a large language model (e.g., Llama V2)
to encode long-form text, followed by fine-tuning with LORA to alignthe
text-image feature spaces in the generation task. To facilitate the training of
long-text semantic alignment, we also curated a high-quality paragraph-image
pair dataset, namely ParaImage. This dataset contains a small amount of
high-quality, meticulously annotated data, and a large-scale synthetic dataset
with long text descriptions being generated using a vision-language model.
Experiments demonstrate that ParaDiffusion outperforms state-of-the-art models
(SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45%
human voting rate improvements for visual appeal and text faithfulness,
respectively. The code and dataset will be released to foster community
research on long-text alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14897">Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network. (arXiv:2311.14897v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Bozhong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shenghua Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingna Wu</a></p>
<p>Recently, 3D anomaly detection, a crucial problem involving fine-grained
geometry discrimination, is getting more attention. However, the lack of
abundant real 3D anomaly data limits the scalability of current models. To
enable scalable anomaly data collection, we propose a 3D anomaly synthesis
pipeline to adapt existing large-scale 3Dmodels for 3D anomaly detection.
Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, basedon
ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40
categories, which provides a rich and varied collection of data, enabling
efficient training and enhancing adaptability to industrial scenarios.
Meanwhile,to enable scalable representation learning for 3D anomaly
localization, we propose a self-supervised method, i.e., Iterative Mask
Reconstruction Network (IMRNet). During training, we propose a geometry-aware
sample module to preserve potentially anomalous local regions during point
cloud down-sampling. Then, we randomly mask out point patches and sent the
visible patches to a transformer for reconstruction-based self-supervision.
During testing, the point cloud repeatedly goes through the Mask Reconstruction
Network, with each iteration's output becoming the next input. By merging and
contrasting the final reconstructed point cloud with the initial input, our
method successfully locates anomalies. Experiments show that IMRNet outperforms
previous state-of-the-art methods, achieving 66.1% in I-AUC on Anomaly-ShapeNet
dataset and 72.5% in I-AUC on Real3D-AD dataset. Our dataset will be released
at https://github.com/Chopper-233/Anomaly-ShapeNet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15556">PKU-I2IQA: An Image-to-Image Quality Assessment Database for AI Generated Images. (arXiv:2311.15556v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiquan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xinyan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changjin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fanyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jinlong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xixin Cao</a></p>
<p>As image generation technology advances, AI-based image generation has been
applied in various fields and Artificial Intelligence Generated Content (AIGC)
has garnered widespread attention. However, the development of AI-based image
generative models also brings new problems and challenges. A significant
challenge is that AI-generated images (AIGI) may exhibit unique distortions
compared to natural images, and not all generated images meet the requirements
of the real world. Therefore, it is of great significance to evaluate AIGIs
more comprehensively. Although previous work has established several human
perception-based AIGC image quality assessment (AIGCIQA) databases for
text-generated images, the AI image generation technology includes scenarios
like text-to-image and image-to-image, and assessing only the images generated
by text-to-image models is insufficient. To address this issue, we establish a
human perception-based image-to-image AIGCIQA database, named PKU-I2IQA. We
conduct a well-organized subjective experiment to collect quality labels for
AIGIs and then conduct a comprehensive analysis of the PKU-I2IQA database.
Furthermore, we have proposed two benchmark models: NR-AIGCIQA based on the
no-reference image quality assessment method and FR-AIGCIQA based on the
full-reference image quality assessment method. Finally, leveraging this
database, we conduct benchmark experiments and compare the performance of the
proposed benchmark models. The PKU-I2IQA database and benchmarks will be
released to facilitate future research on
\url{https://github.com/jiquan123/I2IQA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15994">Adversarial Doodles: Interpretable and Human-drawable Attacks Provide Describable Insights. (arXiv:2311.15994v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nara_R/0/1/0/all/0/1">Ryoya Nara</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsui_Y/0/1/0/all/0/1">Yusuke Matsui</a></p>
<p>DNN-based image classification models are susceptible to adversarial attacks.
Most previous adversarial attacks do not focus on the interpretability of the
generated adversarial examples, and we cannot gain insights into the mechanism
of the target classifier from the attacks. Therefore, we propose Adversarial
Doodles, which have interpretable shapes. We optimize black b\'ezier curves to
fool the target classifier by overlaying them onto the input image. By
introducing random perspective transformation and regularizing the doodled
area, we obtain compact attacks that cause misclassification even when humans
replicate them by hand. Adversarial doodles provide describable and intriguing
insights into the relationship between our attacks and the classifier's output.
We utilize adversarial doodles and discover the bias inherent in the target
classifier, such as "We add two strokes on its head, a triangle onto its body,
and two lines inside the triangle on a bird image. Then, the classifier
misclassifies the image as a butterfly."
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16133">Effective Quantization for Diffusion Models on CPUs. (arXiv:2311.16133v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Hanwen Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haihao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yiyang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xinyu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenzhong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wenhua Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1">Kaokao Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yintong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Heng Guo</a></p>
<p>Diffusion models have gained popularity for generating images from textual
descriptions. Nonetheless, the substantial need for computational resources
continues to present a noteworthy challenge, contributing to time-consuming
processes. Quantization, a technique employed to compress deep learning models
for enhanced efficiency, presents challenges when applied to diffusion models.
These models are notably more sensitive to quantization compared to other model
types, potentially resulting in a degradation of image quality. In this paper,
we introduce a novel approach to quantize the diffusion models by leveraging
both quantization-aware training and distillation. Our results show the
quantized models can maintain the high image quality while demonstrating the
inference efficiency on CPUs. The code is publicly available at:
https://github.com/intel/intel-extension-for-transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16337">Multi-3D-Models Registration-Based Augmented Reality (AR) Instructions for Assembly. (arXiv:2311.16337v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Canadinc_S/0/1/0/all/0/1">Seda Tuzun Canadinc</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1">Wei Yan</a></p>
<p>This paper introduces a novel, markerless, step-by-step, in-situ 3D Augmented
Reality (AR) instruction method and its application - BRICKxAR (Multi 3D
Models/M3D) - for small parts assembly. BRICKxAR (M3D) realistically visualizes
rendered 3D assembly parts at the assembly location of the physical assembly
model (Figure 1). The user controls the assembly process through a user
interface. BRICKxAR (M3D) utilizes deep learning-trained 3D model-based
registration. Object recognition and tracking become challenging as the
assembly model updates at each step. Additionally, not every part in a 3D
assembly may be visible to the camera during the assembly. BRICKxAR (M3D)
combines multiple assembly phases with a step count to address these
challenges. Thus, using fewer phases simplifies the complex assembly process
while step count facilitates accurate object recognition and precise
visualization of each step. A testing and heuristic evaluation of the BRICKxAR
(M3D) prototype and qualitative analysis were conducted with users and experts
in visualization and human-computer interaction. Providing robust 3D AR
instructions and allowing the handling of the assembly model, BRICKxAR (M3D)
has the potential to be used at different scales ranging from manufacturing
assembly to construction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16444">Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos. (arXiv:2311.16444v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1">Takehiko Ohkawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1">Takuma Yagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishimura_T/0/1/0/all/0/1">Taichi Nishimura</a>, <a href="http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1">Ryosuke Furuta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1">Atsushi Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1">Yoshitaka Ushiku</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1">Yoichi Sato</a></p>
<p>We propose a novel benchmark for cross-view knowledge transfer of dense video
captioning, adapting models from web instructional videos with exocentric views
to an egocentric view. While dense video captioning (predicting time segments
and their captions) is primarily studied with exocentric videos (e.g.,
YouCook2), benchmarks with egocentric videos are restricted due to data
scarcity. To overcome the limited video availability, transferring knowledge
from abundant exocentric web videos is demanded as a practical approach.
However, learning the correspondence between exocentric and egocentric views is
difficult due to their dynamic view changes. The web videos contain mixed views
focusing on either human body actions or close-up hand-object interactions,
while the egocentric view is constantly shifting as the camera wearer moves.
This necessitates the in-depth study of cross-view transfer under complex view
changes. In this work, we first create a real-life egocentric dataset (EgoYC2)
whose captions are shared with YouCook2, enabling transfer learning between
these datasets assuming their ground-truth is accessible. To bridge the view
gaps, we propose a view-invariant learning method using adversarial training in
both the pre-training and fine-tuning stages. While the pre-training is
designed to learn invariant features against the mixed views in the web videos,
the view-invariant fine-tuning further mitigates the view gaps between both
datasets. We validate our proposed method by studying how effectively it
overcomes the view change problem and efficiently transfers the knowledge to
the egocentric domain. Our benchmark pushes the study of the cross-view
transfer into a new task domain of dense video captioning and will envision
methodologies to describe egocentric videos in natural language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16668">LiveNVS: Neural View Synthesis on Live RGB-D Streams. (arXiv:2311.16668v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fink_L/0/1/0/all/0/1">Laura Fink</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1">Darius R&#xfc;ckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Franke_L/0/1/0/all/0/1">Linus Franke</a>, <a href="http://arxiv.org/find/cs/1/au:+Keinert_J/0/1/0/all/0/1">Joachim Keinert</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1">Marc Stamminger</a></p>
<p>Existing real-time RGB-D reconstruction approaches, like Kinect Fusion, lack
real-time photo-realistic visualization. This is due to noisy, oversmoothed or
incomplete geometry and blurry textures which are fused from imperfect depth
maps and camera poses. Recent neural rendering methods can overcome many of
such artifacts but are mostly optimized for offline usage, hindering the
integration into a live reconstruction pipeline.
</p>
<p>In this paper, we present LiveNVS, a system that allows for neural novel view
synthesis on a live RGB-D input stream with very low latency and real-time
rendering. Based on the RGB-D input stream, novel views are rendered by
projecting neural features into the target view via a densely fused depth map
and aggregating the features in image-space to a target feature map. A
generalizable neural network then translates the target feature map into a
high-quality RGB image. LiveNVS achieves state-of-the-art neural rendering
quality of unknown scenes during capturing, allowing users to virtually explore
the scene and assess reconstruction quality in real-time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16835">Unified-modal Salient Object Detection via Adaptive Prompt Learning. (arXiv:2311.16835v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kunpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhengzheng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Existing single-modal and multi-modal salient object detection (SOD) methods
focus on designing specific architectures tailored for their respective tasks.
However, developing completely different models for different tasks leads to
labor and time consumption, as well as high computational and practical
deployment costs. In this paper, we make the first attempt to address both
single-modal and multi-modal SOD in a unified framework called UniSOD.
Nevertheless, assigning appropriate strategies to modality variable inputs is
challenging. To this end, UniSOD learns modality-aware prompts with
task-specific hints through adaptive prompt learning, which are plugged into
the proposed pre-trained baseline SOD model to handle corresponding tasks,
while only requiring few learnable parameters compared to training the entire
model. Each modality-aware prompt is generated from a switchable prompt
generation block, which performs structural switching solely relied on
single-modal and multi-modal inputs. UniSOD achieves consistent performance
improvement on 14 benchmark datasets for RGB, RGB-D, and RGB-T SOD, which
demonstrates that our method effectively and efficiently unifies single-modal
and multi-modal SOD tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16854">A Unified Approach for Text- and Image-guided 4D Scene Generation. (arXiv:2311.16854v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yufeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xueting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1">Koki Nagano</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sifei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1">Karsten Kreis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1">Shalini De Mello</a></p>
<p>Large-scale diffusion generative models are greatly simplifying image, video
and 3D asset creation from user-provided text prompts and images. However, the
challenging problem of text-to-4D dynamic 3D scene generation with diffusion
guidance remains largely unexplored. We propose Dream-in-4D, which features a
novel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D
diffusion guidance to effectively learn a high-quality static 3D asset in the
first stage; (2) a deformable neural radiance field that explicitly
disentangles the learned static asset from its deformation, preserving quality
during motion learning; and (3) a multi-resolution feature grid for the
deformation field with a displacement total variation loss to effectively learn
motion with video diffusion guidance in the second stage. Through a user
preference study, we demonstrate that our approach significantly advances image
and motion quality, 3D consistency and text fidelity for text-to-4D generation
compared to baseline approaches. Thanks to its motion-disentangled
representation, Dream-in-4D can also be easily adapted for controllable
generation where appearance is defined by one or multiple images, without the
need to modify the motion learning stage. Thus, our method offers, for the
first time, a unified approach for text-to-4D, image-to-4D and personalized 4D
generation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17041">Efficient In-Context Learning in Vision-Language Models for Egocentric Videos. (arXiv:2311.17041v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Keunwoo Peter Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Fengyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Joyce Chai</a></p>
<p>Recent advancements in text-only large language models (LLMs) have
highlighted the benefit of in-context learning for adapting to new tasks with a
few demonstrations. However, extending in-context learning to large
vision-language models (VLMs) using a huge amount of naturalistic
vision-language data has shown limited success, particularly for egocentric
videos, due to high data collection costs. We propose a novel training method
$\mathbb{E}$fficient $\mathbb{I}$n-context $\mathbb{L}$earning on
$\mathbb{E}$gocentric $\mathbb{V}$ideos ($\mathbb{EILEV}$), which elicits
in-context learning in VLMs for egocentric videos without requiring massive,
naturalistic egocentric video datasets. $\mathbb{EILEV}$ involves architectural
and training data adaptations to allow the model to process contexts
interleaved with video clips and narrations, sampling of in-context examples
with clusters of similar verbs and nouns, use of data with skewed marginal
distributions with a long tail of infrequent verbs and nouns, as well as
homonyms and synonyms. Our evaluations show that $\mathbb{EILEV}$-trained
models outperform larger VLMs trained on a huge amount of naturalistic data in
in-context learning. Furthermore, they can generalize to not only
out-of-distribution, but also novel, rare egocentric videos and texts via
in-context learning, demonstrating potential for applications requiring
cost-effective training, and rapid post-deployment adaptability. Our code and
demo are available at \url{https://github.com/yukw777/EILEV}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10887">Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder. (arXiv:2311.10887v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhimin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Longlong Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bing Li</a></p>
<p>In recent years, the field of 3D self-supervised learning has witnessed
significant progress, resulting in the emergence of Multi-Modality Masked
AutoEncoders (MAE) methods that leverage both 2D images and 3D point clouds for
pre-training. However, a notable limitation of these approaches is that they do
not fully utilize the multi-view attributes inherent in 3D point clouds, which
is crucial for a deeper understanding of 3D structures. Building upon this
insight, we introduce a novel approach employing a 3D to multi-view masked
autoencoder to fully harness the multi-modal attributes of 3D point clouds. To
be specific, our method uses the encoded tokens from 3D masked point clouds to
generate original point clouds and multi-view depth images across various
poses. This approach not only enriches the model's comprehension of geometric
structures but also leverages the inherent multi-modal properties of point
clouds. Our experiments illustrate the effectiveness of the proposed method for
different tasks and under different settings. Remarkably, our method
outperforms state-of-the-art counterparts by a large margin in a variety of
downstream tasks, including 3D object classification, few-shot learning, part
segmentation, and 3D object detection. Code will be available at:
https://github.com/Zhimin-C/Multiview-MAE
</p>
</p>
</div>

    </div>
    </body>
    