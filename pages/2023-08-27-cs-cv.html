<!DOCTYPE html>
<html>
<head>
<title>2023-08-27-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.12299">Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization. (arXiv:2308.12299v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1">Xing-Yu Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Hao_S/0/1/0/all/0/1">Shaogang Hao</a></p>
<p>As the feature size of integrated circuits continues to decrease, optical
proximity correction (OPC) has emerged as a crucial resolution enhancement
technology for ensuring high printability in the lithography process. Recently,
level set-based inverse lithography technology (ILT) has drawn considerable
attention as a promising OPC solution, showcasing its powerful pattern
fidelity, especially in advanced process. However, massive computational time
consumption of ILT limits its applicability to mainly correcting partial layers
and hotspot regions. Deep learning (DL) methods have shown great potential in
accelerating ILT. However, lack of domain knowledge of inverse lithography
limits the ability of DL-based algorithms in process window (PW) enhancement
and etc. In this paper, we propose an inverse lithography physics-informed deep
neural level set (ILDLS) approach for mask optimization. This approach utilizes
level set based-ILT as a layer within the DL framework and iteratively conducts
mask prediction and correction to significantly enhance printability and PW in
comparison with results from pure DL and ILT. With this approach, computation
time is reduced by a few orders of magnitude versus ILT. By gearing up DL with
knowledge of inverse lithography physics, ILDLS provides a new and efficient
mask optimization solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12313">Gaze Estimation on Spresense. (arXiv:2308.12313v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1">Thomas Ruegg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1">Pietro Bonazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ronco_A/0/1/0/all/0/1">Andrea Ronco</a></p>
<p>Gaze estimation is a valuable technology with numerous applications in fields
such as human-computer interaction, virtual reality, and medicine. This report
presents the implementation of a gaze estimation system using the Sony
Spresense microcontroller board and explores its performance in latency,
MAC/cycle, and power consumption. The report also provides insights into the
system's architecture, including the gaze estimation model used. Additionally,
a demonstration of the system is presented, showcasing its functionality and
performance. Our lightweight model TinyTrackerS is a mere 169Kb in size, using
85.8k parameters and runs on the Spresense platform at 3 FPS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12319">RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Hongwei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kunzhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model's general semantic knowledge to maintain the
surrogate model's performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12320">Understanding Dark Scenes by Contrasting Multi-Modal Observations. (arXiv:2308.12320v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaoyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1">Naoto Yokoya</a></p>
<p>Understanding dark scenes based on multi-modal image data is challenging, as
both the visible and auxiliary modalities provide limited semantic information
for the task. Previous methods focus on fusing the two modalities but neglect
the correlations among semantic classes when minimizing losses to align pixels
with labels, resulting in inaccurate class predictions. To address these
issues, we introduce a supervised multi-modal contrastive learning approach to
increase the semantic discriminability of the learned multi-modal feature
spaces by jointly performing cross-modal and intra-modal contrast under the
supervision of the class correlations. The cross-modal contrast encourages
same-class embeddings from across the two modalities to be closer and pushes
different-class ones apart. The intra-modal contrast forces same-class or
different-class embeddings within each modality to be together or apart. We
validate our approach on a variety of tasks that cover diverse light conditions
and image modalities. Experiments show that our approach can effectively
enhance dark scene understanding based on multi-modal images with limited
semantics by shaping semantic-discriminative feature spaces. Comparisons with
previous methods demonstrate our state-of-the-art performance. Code and
pretrained models are available at https://github.com/palmdong/SMMCL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12350">Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation. (arXiv:2308.12350v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1">Duo Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Ping Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1">Qiuhong Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12364">Saliency-based Video Summarization for Face Anti-spoofing. (arXiv:2308.12364v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muhammad_U/0/1/0/all/0/1">Usman Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1">Mourad Oussalah</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1">Md Ziaul Hoque</a>, <a href="http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1">Jorma Laaksonen</a></p>
<p>Due to the growing availability of face anti-spoofing databases, researchers
are increasingly focusing on video-based methods that use hundreds to thousands
of images to assess their impact on performance. However, there is no clear
consensus on the exact number of frames in a video required to improve the
performance of face anti-spoofing tasks. Inspired by the visual saliency
theory, we present a video summarization method for face anti-spoofing tasks
that aims to enhance the performance and efficiency of deep learning models by
leveraging visual saliency. In particular, saliency information is extracted
from the differences between the Laplacian and Wiener filter outputs of the
source images, enabling identification of the most visually salient regions
within each frame. Subsequently, the source images are decomposed into base and
detail layers, enhancing representation of important information. The weighting
maps are then computed based on the saliency information, indicating the
importance of each pixel in the image. By linearly combining the base and
detail layers using the weighting maps, the method fuses the source images to
create a single representative image that summarizes the entire video. The key
contribution of our proposed method lies in demonstrating how visual saliency
can be used as a data-centric approach to improve the performance and
efficiency of face presentation attack detection models. By focusing on the
most salient images or regions within the images, a more representative and
diverse training set can be created, potentially leading to more effective
models. To validate the method's effectiveness, a simple deep learning
architecture (CNN-RNN) was used, and the experimental results showcased
state-of-the-art performance on five challenging face anti-spoofing datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12366">Continual Zero-Shot Learning through Semantically Guided Generative Random Walks. (arXiv:2308.12366v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Janson_P/0/1/0/all/0/1">Paul Janson</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kai Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1">Ivan Skorokhodov</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>Learning novel concepts, remembering previous knowledge, and adapting it to
future tasks occur simultaneously throughout a human's lifetime. To model such
comprehensive abilities, continual zero-shot learning (CZSL) has recently been
introduced. However, most existing methods overused unseen semantic information
that may not be continually accessible in realistic settings. In this paper, we
address the challenge of continual zero-shot learning where unseen information
is not provided during training, by leveraging generative modeling. The heart
of the generative-based methods is to learn quality representations from seen
classes to improve the generative understanding of the unseen visual space.
Motivated by this, we introduce generalization-bound tools and provide the
first theoretical explanation for the benefits of generative modeling to CZSL
tasks. Guided by the theoretical analysis, we then propose our learning
algorithm that employs a novel semantically guided Generative Random Walk (GRW)
loss. The GRW loss augments the training by continually encouraging the model
to generate realistic and characterized samples to represent the unseen space.
Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN
datasets, surpassing existing CZSL methods by 3-7\%. The code has been made
available here \url{https://github.com/wx-zhang/IGCZSL}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12370">AdVerb: Visually Guided Audio Dereverberation. (arXiv:2308.12370v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1">Sanjoy Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Sreyan Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1">Subhrajyoti Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratnarajah_A/0/1/0/all/0/1">Anton Ratnarajah</a>, <a href="http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1">Utkarsh Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a></p>
<p>We present AdVerb, a novel audio-visual dereverberation framework that uses
visual cues in addition to the reverberant sound to estimate clean audio.
Although audio-only dereverberation is a well-studied problem, our approach
incorporates the complementary visual modality to perform audio
dereverberation. Given an image of the environment where the reverberated sound
signal has been recorded, AdVerb employs a novel geometry-aware cross-modal
transformer architecture that captures scene geometry and audio-visual
cross-modal relationship to generate a complex ideal ratio mask, which, when
applied to the reverberant audio predicts the clean sound. The effectiveness of
our method is demonstrated through extensive quantitative and qualitative
evaluations. Our approach significantly outperforms traditional audio-only and
audio-visual baselines on three downstream tasks: speech enhancement, speech
recognition, and speaker verification, with relative improvements in the range
of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly
satisfactory RT60 error scores on the AVSpeech dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12371">Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1">Rafael Henrique Vareto</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1">Manuel G&#xfc;nther</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1">William Robson Schwartz</a></p>
<p>Open-set face recognition refers to a scenario in which biometric systems
have incomplete knowledge of all existing subjects. Therefore, they are
expected to prevent face samples of unregistered subjects from being identified
as previously enrolled identities. This watchlist context adds an arduous
requirement that calls for the dismissal of irrelevant faces by focusing mainly
on subjects of interest. As a response, this work introduces a novel method
that associates an ensemble of compact neural networks with a margin-based cost
function that explores additional samples. Supplementary negative samples can
be obtained from external databases or synthetically built at the
representation level in training time with a new mix-up feature augmentation
approach. Deep neural networks pre-trained on large face datasets serve as the
preliminary feature extraction module. We carry out experiments on well-known
LFW and IJB-C datasets where results show that the approach is able to boost
closed and open-set identification rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12372">Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1">Deblina Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1">Sabine S&#xfc;sstrunk</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a></p>
<p>We introduce the first multitasking vision transformer adapters that learn
generalizable task affinities which can be applied to novel tasks and domains.
Integrated into an off-the-shelf vision transformer backbone, our adapters can
simultaneously solve multiple dense vision tasks in a parameter-efficient
manner, unlike existing multitasking transformers that are parametrically
expensive. In contrast to concurrent methods, we do not require retraining or
fine-tuning whenever a new task or domain is added. We introduce a task-adapted
attention mechanism within our adapter framework that combines gradient-based
task similarities with attention-based ones. The learned task affinities
generalize to the following settings: zero-shot task transfer, unsupervised
domain adaptation, and generalization without fine-tuning to novel domains. We
demonstrate that our approach outperforms not only the existing convolutional
neural network-based multitasking methods but also the vision transformer-based
ones. Our project page is at \url{https://ivrl.github.io/VTAGML}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12380">FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features. (arXiv:2308.12380v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yufeng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1">Di Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1">Guoxian Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_S/0/1/0/all/0/1">Shen Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhi_T/0/1/0/all/0/1">Tiancheng Zhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Linjie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1">Mohammad Soleymani</a></p>
<p>Automatic detection of facial Action Units (AUs) allows for objective facial
expression analysis. Due to the high cost of AU labeling and the limited size
of existing benchmarks, previous AU detection methods tend to overfit the
dataset, resulting in a significant performance loss when evaluated across
corpora. To address this problem, we propose FG-Net for generalizable facial
action unit detection. Specifically, FG-Net extracts feature maps from a
StyleGAN2 model pre-trained on a large and diverse face image dataset. Then,
these features are used to detect AUs with a Pyramid CNN Interpreter, making
the training efficient and capturing essential local features. The proposed
FG-Net achieves a strong generalization ability for heatmap-based AU detection
thanks to the generalizable and semantic-rich features extracted from the
pre-trained generative model. Extensive experiments are conducted to evaluate
within- and cross-corpus AU detection with the widely-used DISFA and BP4D
datasets. Compared with the state-of-the-art, the proposed method achieves
superior cross-domain performance while maintaining competitive within-domain
performance. In addition, FG-Net is data-efficient and achieves competitive
performance even when trained on 1000 samples. Our code will be released at
\url{https://github.com/ihp-lab/FG-Net}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12383">With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1">Manuele Barraco</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1">Sara Sarto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1">Marcella Cornia</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1">Lorenzo Baraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1">Rita Cucchiara</a></p>
<p>Image captioning, like many tasks involving vision and language, currently
relies on Transformer-based architectures for extracting the semantics in an
image and translating it into linguistically coherent descriptions. Although
successful, the attention operator only considers a weighted summation of
projections of the current input sample, therefore ignoring the relevant
semantic information which can come from the joint observation of other
samples. In this paper, we devise a network which can perform attention over
activations obtained while processing other training samples, through a
prototypical memory model. Our memory models the distribution of past keys and
values through the definition of prototype vectors which are both
discriminative and compact. Experimentally, we assess the performance of the
proposed model on the COCO dataset, in comparison with carefully designed
baselines and state-of-the-art approaches, and by investigating the role of
each of the proposed components. We demonstrate that our proposal can increase
the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when
training in cross-entropy only and when fine-tuning with self-critical sequence
training. Source code and trained models are available at:
https://github.com/aimagelab/PMA-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12394">Self-Supervised Learning for Endoscopic Video Analysis. (arXiv:2308.12394v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hirsch_R/0/1/0/all/0/1">Roy Hirsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1">Mathilde Caron</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1">Regev Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Livne_A/0/1/0/all/0/1">Amir Livne</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_R/0/1/0/all/0/1">Ron Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Golany_T/0/1/0/all/0/1">Tomer Golany</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1">Roman Goldenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1">Daniel Freedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1">Ehud Rivlin</a></p>
<p>Self-supervised learning (SSL) has led to important breakthroughs in computer
vision by allowing learning from large amounts of unlabeled data. As such, it
might have a pivotal role to play in biomedicine where annotating data requires
a highly specialized expertise. Yet, there are many healthcare domains for
which SSL has not been extensively explored. One such domain is endoscopy,
minimally invasive procedures which are commonly used to detect and treat
infections, chronic inflammatory diseases or cancer. In this work, we study the
use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for
endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit
the power of SSL, we create sizable unlabeled endoscopic video datasets for
training MSNs. These strong image representations serve as a foundation for
secondary training with limited annotated datasets, resulting in
state-of-the-art performance in endoscopic benchmarks like surgical phase
recognition during laparoscopy and colonoscopic polyp characterization.
Additionally, we achieve a 50% reduction in annotated data size without
sacrificing performance. Thus, our work provides evidence that SSL can
dramatically reduce the need of annotated data in endoscopy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12408">An Initial Exploration: Learning to Generate Realistic Audio for Silent Video. (arXiv:2308.12408v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martel_M/0/1/0/all/0/1">Matthew Martel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1">Jackson Wagner</a></p>
<p>Generating realistic audio effects for movies and other media is a
challenging task that is accomplished today primarily through physical
techniques known as Foley art. Foley artists create sounds with common objects
(e.g., boxing gloves, broken glass) in time with video as it is playing to
generate captivating audio tracks. In this work, we aim to develop a
deep-learning based framework that does much the same - observes video in it's
natural sequence and generates realistic audio to accompany it. Notably, we
have reason to believe this is achievable due to advancements in realistic
audio generation techniques conditioned on other inputs (e.g., Wavenet
conditioned on text). We explore several different model architectures to
accomplish this task that process both previously-generated audio and video
context. These include deep-fusion CNN, dilated Wavenet CNN with visual
context, and transformer-based architectures. We find that the
transformer-based architecture yields the most promising results, matching
low-frequencies to visual patterns effectively, but failing to generate more
nuanced waveforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12416">Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach. (arXiv:2308.12416v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gianchandani_N/0/1/0/all/0/1">Neha Gianchandani</a>, <a href="http://arxiv.org/find/eess/1/au:+Dibaji_M/0/1/0/all/0/1">Mahsa Dibaji</a>, <a href="http://arxiv.org/find/eess/1/au:+Bento_M/0/1/0/all/0/1">Mariana Bento</a>, <a href="http://arxiv.org/find/eess/1/au:+MacDonald_E/0/1/0/all/0/1">Ethan MacDonald</a>, <a href="http://arxiv.org/find/eess/1/au:+Souza_R/0/1/0/all/0/1">Roberto Souza</a></p>
<p>Deep learning models have achieved state-of-the-art results in estimating
brain age, which is an important brain health biomarker, from magnetic
resonance (MR) images. However, most of these models only provide a global age
prediction, and rely on techniques, such as saliency maps to interpret their
results. These saliency maps highlight regions in the input image that were
significant for the model's predictions, but they are hard to be interpreted,
and saliency map values are not directly comparable across different samples.
In this work, we reframe the age prediction problem from MR images to an
image-to-image regression problem where we estimate the brain age for each
brain voxel in MR images. We compare voxel-wise age prediction models against
global age prediction models and their corresponding saliency maps. The results
indicate that voxel-wise age prediction models are more interpretable, since
they provide spatial information about the brain aging process, and they
benefit from being quantitative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12419">Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods. (arXiv:2308.12419v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bowen Shi</a></p>
<p>Sign language, which conveys meaning through gestures, is the chief means of
communication among deaf people. Recognizing sign language in natural settings
presents significant challenges due to factors such as lighting, background
clutter, and variations in signer characteristics. In this thesis, I study
automatic sign language processing in the wild, using signing videos collected
from the Internet. This thesis contributes new datasets, tasks, and methods.
Most chapters of this thesis address tasks related to fingerspelling, an
important component of sign language and yet has not been studied widely by
prior work. I present three new large-scale ASL datasets in the wild:
ChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and
ChicagoFSWild+, I address fingerspelling recognition, which consists of
transcribing fingerspelling sequences into text. I propose an end-to-end
approach based on iterative attention that allows recognition from a raw video
without explicit hand detection. I further show that using a Conformer-based
network jointly modeling handshape and mouthing can bring performance close to
that of humans. Next, I propose two tasks for building real-world
fingerspelling-based applications: fingerspelling detection and search. For
fingerspelling detection, I introduce a suite of evaluation metrics and a new
detection model via multi-task training. To address the problem of searching
for fingerspelled keywords in raw sign language videos, we propose a novel
method that jointly localizes and matches fingerspelling segments to text.
Finally, I will describe a benchmark for large-vocabulary open-domain sign
language translation based on OpenASL. To address the challenges of sign
language translation in realistic settings, we propose a set of techniques
including sign search as a pretext task for pre-training and fusion of mouthing
and handshape features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12433">A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications. (arXiv:2308.12433v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1">Aotian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1">Sanjay Ranka</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1">Anand Rangarajan</a></p>
<p>We address the problem of unsupervised semantic segmentation of outdoor LiDAR
point clouds in diverse traffic scenarios. The key idea is to leverage the
spatiotemporal nature of a dynamic point cloud sequence and introduce
drastically stronger augmentation by establishing spatiotemporal
correspondences across multiple frames. We dovetail clustering and pseudo-label
learning in this work. Essentially, we alternate between clustering points into
semantic groups and optimizing models using point-wise pseudo-spatiotemporal
labels with a simple learning objective. Therefore, our method can learn
discriminative features in an unsupervised learning fashion. We show promising
segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark
datasets covering scenarios in autonomous vehicle and intersection
infrastructure, which is competitive when compared against many existing fully
supervised learning methods. This general framework can lead to a unified
representation learning approach for LiDAR point clouds incorporating domain
knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12435">Characterising representation dynamics in recurrent neural networks for object recognition. (arXiv:2308.12435v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thorat_S/0/1/0/all/0/1">Sushrut Thorat</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerig_A/0/1/0/all/0/1">Adrien Doerig</a>, <a href="http://arxiv.org/find/cs/1/au:+Kietzmann_T/0/1/0/all/0/1">Tim C. Kietzmann</a></p>
<p>Recurrent neural networks (RNNs) have yielded promising results for both
recognizing objects in challenging conditions and modeling aspects of primate
vision. However, the representational dynamics of recurrent computations remain
poorly understood, especially in large-scale visual models. Here, we studied
such dynamics in RNNs trained for object classification on MiniEcoset, a novel
subset of ecoset. We report two main insights. First, upon inference,
representations continued to evolve after correct classification, suggesting a
lack of the notion of being ``done with classification''. Second, focusing on
``readout zones'' as a way to characterize the activation trajectories, we
observe that misclassified representations exhibit activation patterns with
lower L2 norm, and are positioned more peripherally in the readout zones. Such
arrangements help the misclassified representations move into the correct zones
as time progresses. Our findings generalize to networks with lateral and
top-down connections, and include both additive and multiplicative interactions
with the bottom-up sweep. The results therefore contribute to a general
understanding of RNN dynamics in naturalistic tasks. We hope that the analysis
framework will aid future investigations of other types of RNNs, including
understanding of representational dynamics in primate vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12439">BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tinghao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiangyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Ping He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a></p>
<p>We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12440">HNAS-reg: hierarchical neural architecture search for deformable medical image registration. (arXiv:2308.12440v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1">Jiong Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1">Yong Fan</a></p>
<p>Convolutional neural networks (CNNs) have been widely used to build deep
learning models for medical image registration, but manually designed network
architectures are not necessarily optimal. This paper presents a hierarchical
NAS framework (HNAS-Reg), consisting of both convolutional operation search and
network topology search, to identify the optimal network architecture for
deformable medical image registration. To mitigate the computational overhead
and memory constraints, a partial channel strategy is utilized without losing
optimization quality. Experiments on three datasets, consisting of 636
T1-weighted magnetic resonance images (MRIs), have demonstrated that the
proposal method can build a deep learning model with improved image
registration accuracy and reduced model size, compared with state-of-the-art
image registration approaches, including one representative traditional
approach and two unsupervised learning-based approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12443">TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction. (arXiv:2308.12443v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xueqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1">Luyao Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiongchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qiong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1">Huidong Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yi-Hwa Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Palyo_R/0/1/0/all/0/1">Richard Palyo</a>, <a href="http://arxiv.org/find/eess/1/au:+Miller_E/0/1/0/all/0/1">Edward J. Miller</a>, <a href="http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1">Albert J. Sinusas</a>, <a href="http://arxiv.org/find/eess/1/au:+Spottiswoode_B/0/1/0/all/0/1">Bruce Spottiswoode</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dvornek_N/0/1/0/all/0/1">Nicha C. Dvornek</a></p>
<p>The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of
cross-frame distribution in dynamic cardiac positron emission tomography (PET)
raise significant challenges for inter-frame motion correction, particularly
for the early frames where conventional intensity-based image registration
techniques are not applicable. Alternatively, a promising approach utilizes
generative methods to handle the tracer distribution changes to assist existing
registration methods. To improve frame-wise registration and parametric
quantification, we propose a Temporally and Anatomically Informed Generative
Adversarial Network (TAI-GAN) to transform the early frames into the late
reference frame using an all-to-one mapping. Specifically, a feature-wise
linear modulation layer encodes channel-wise parameters generated from temporal
tracer kinetics information, and rough cardiac segmentations with local shifts
serve as the anatomical information. We validated our proposed method on a
clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted
early frames with high image quality, comparable to the real reference frames.
After TAI-GAN conversion, motion estimation accuracy and clinical myocardial
blood flow (MBF) quantification were improved compared to using the original
frames. Our code is published at https://github.com/gxq1998/TAI-GAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12447">MOFO: MOtion FOcused Self-Supervision for Video Understanding. (arXiv:2308.12447v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmadian_M/0/1/0/all/0/1">Mona Ahmadian</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1">Frank Guerin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1">Andrew Gilbert</a></p>
<p>Self-supervised learning (SSL) techniques have recently produced outstanding
results in learning visual representations from unlabeled videos. Despite the
importance of motion in supervised learning techniques for action recognition,
SSL methods often do not explicitly consider motion information in videos. To
address this issue, we propose MOFO (MOtion FOcused), a novel SSL method for
focusing representation learning on the motion area of a video, for action
recognition. MOFO automatically detects motion areas in videos and uses these
to guide the self-supervision task. We use a masked autoencoder which randomly
masks out a high proportion of the input sequence; we force a specified
percentage of the inside of the motion area to be masked and the remainder from
outside. We further incorporate motion information into the finetuning step to
emphasise motion in the downstream task. We demonstrate that our motion-focused
innovations can significantly boost the performance of the currently leading
SSL method (VideoMAE) for action recognition. Our method improves the recent
self-supervised Vision Transformer (ViT), VideoMAE, by achieving +2.6%, +2.1%,
+1.3% accuracy on Epic-Kitchens verb, noun and action classification,
respectively, and +4.7% accuracy on Something-Something V2 action
classification. Our proposed approach significantly improves the performance of
the current SSL method for action recognition, indicating the importance of
explicitly encoding motion in SSL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12452">ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization. (arXiv:2308.12452v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenzhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1">Fangcheng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1">Cengiz Oztireli</a></p>
<p>The radiance fields style transfer is an emerging field that has recently
gained popularity as a means of 3D scene stylization, thanks to the outstanding
performance of neural radiance fields in 3D reconstruction and view synthesis.
We highlight a research gap in radiance fields style transfer, the lack of
sufficient perceptual controllability, motivated by the existing concept in the
2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style
transfer framework offering manageable control over perceptual factors, to
systematically explore the perceptual controllability in 3D scene stylization.
Four distinct types of controls - color preservation control, (style pattern)
scale control, spatial (selective stylization area) control, and depth
enhancement control - are proposed and integrated into this framework. Results
from real-world datasets, both quantitative and qualitative, show that the four
types of controls in our ARF-Plus framework successfully accomplish their
corresponding perceptual controls when stylizing 3D scenes. These techniques
work well for individual style inputs as well as for the simultaneous
application of multiple styles within a scene. This unlocks a realm of
limitless possibilities, allowing customized modifications of stylization
effects and flexible merging of the strengths of different styles, ultimately
enabling the creation of novel and eye-catching stylistic effects on 3D scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12453">Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sagers_L/0/1/0/all/0/1">Luke W. Sagers</a>, <a href="http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1">James A. Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1">Luke Melas-Kyriazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1">Matthew Groh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1">Pranav Rajpurkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Adamson_A/0/1/0/all/0/1">Adewole S. Adamson</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotemberg_V/0/1/0/all/0/1">Veronica Rotemberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1">Roxana Daneshjou</a>, <a href="http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1">Arjun K. Manrai</a></p>
<p>While hundreds of artificial intelligence (AI) algorithms are now approved or
cleared by the US Food and Drugs Administration (FDA), many studies have shown
inconsistent generalization or latent bias, particularly for underrepresented
populations. Some have proposed that generative AI could reduce the need for
real data, but its utility in model development remains unclear. Skin disease
serves as a useful case study in synthetic image generation due to the
diversity of disease appearance, particularly across the protected attribute of
skin tone. Here we show that latent diffusion models can scalably generate
images of skin disease and that augmenting model training with these data
improves performance in data-limited settings. These performance gains saturate
at synthetic-to-real image ratios above 10:1 and are substantially smaller than
the gains obtained from adding real images. As part of our analysis, we
generate and analyze a new dataset of 458,920 synthetic images produced using
several generation strategies. Our results suggest that synthetic data could
serve as a force-multiplier for model development, but the collection of
diverse real-world data remains the most important step to improve medical AI
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12462">Overcoming General Knowledge Loss with Selective Parameter Finetuning. (arXiv:2308.12462v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Janson_P/0/1/0/all/0/1">Paul Janson</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1">Rahaf Aljundi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in updating foundation models to accommodate novel
information while retaining their original ability. In this paper, we present a
novel approach to achieving continual model updates by effecting localized
modifications to a small subset of parameters. Guided by insights gleaned from
prior analyses of foundational models, we first localize a specific layer for
model refinement and then introduce an importance scoring mechanism designed to
update only the most crucial weights. Our method is exhaustively evaluated on
foundational vision-language models, measuring its efficacy in both learning
new information and preserving pre-established knowledge across a diverse
spectrum of continual learning tasks, including Aircraft, Birdsnap CIFAR-100,
CUB, Cars, and GTSRB. The results show that our method improves the existing
continual learning methods by 0.5\% - 10\% on average, and reduces the loss of
pre-trained knowledge from around 5\% to 0.97\%. Comprehensive ablation studies
substantiate our method design, shedding light on the contributions of each
component to controllably learning new knowledge and mitigating the forgetting
of pre-trained knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12465">InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model. (arXiv:2308.12465v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jueqi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Levman_J/0/1/0/all/0/1">Jacob Levman</a>, <a href="http://arxiv.org/find/eess/1/au:+Pinaya_W/0/1/0/all/0/1">Walter Hugo Lopez Pinaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Tudosiu_P/0/1/0/all/0/1">Petru-Daniel Tudosiu</a>, <a href="http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1">M. Jorge Cardoso</a>, <a href="http://arxiv.org/find/eess/1/au:+Marinescu_R/0/1/0/all/0/1">Razvan Marinescu</a></p>
<p>High-resolution (HR) MRI scans obtained from research-grade medical centers
provide precise information about imaged tissues. However, routine clinical MRI
scans are typically in low-resolution (LR) and vary greatly in contrast and
spatial resolution due to the adjustments of the scanning parameters to the
local needs of the medical center. End-to-end deep learning methods for MRI
super-resolution (SR) have been proposed, but they require re-training each
time there is a shift in the input distribution. To address this issue, we
propose a novel approach that leverages a state-of-the-art 3D brain generative
model, the latent diffusion model (LDM) trained on UK BioBank, to increase the
resolution of clinical MRI scans. The LDM acts as a generative prior, which has
the ability to capture the prior distribution of 3D T1-weighted brain MRI.
Based on the architecture of the brain LDM, we find that different methods are
suitable for different settings of MRI SR, and thus propose two novel
strategies: 1) for SR with more sparsity, we invert through both the decoder of
the LDM and also through a deterministic Denoising Diffusion Implicit Models
(DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity,
we invert only through the LDM decoder, an approach we will call
InverseSR(Decoder). These two approaches search different latent spaces in the
LDM model to find the optimal latent code to map the given LR MRI into HR. The
training process of the generative model is independent of the MRI
under-sampling process, ensuring the generalization of our method to many MRI
SR problems with different input measurements. We validate our method on over
100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that
powerful priors given by LDM can be used for MRI reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12469">Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion. (arXiv:2308.12469v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Junjiao Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_L/0/1/0/all/0/1">Lavisha Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Colaco_A/0/1/0/all/0/1">Andrea Colaco</a>, <a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1">Zsolt Kira</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Franco_M/0/1/0/all/0/1">Mar Gonzalez-Franco</a></p>
<p>Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12477">American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers. (arXiv:2308.12477v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1">Melissa Dell</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlson_J/0/1/0/all/0/1">Jacob Carlson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bryan_T/0/1/0/all/0/1">Tom Bryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Silcock_E/0/1/0/all/0/1">Emily Silcock</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Abhishek Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zejiang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+DAmico_Wong_L/0/1/0/all/0/1">Luca D&#x27;Amico-Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Querubin_P/0/1/0/all/0/1">Pablo Querubin</a>, <a href="http://arxiv.org/find/cs/1/au:+Heldring_L/0/1/0/all/0/1">Leander Heldring</a></p>
<p>Existing full text datasets of U.S. public domain newspapers do not recognize
the often complex layouts of newspaper scans, and as a result the digitized
content scrambles texts from articles, headlines, captions, advertisements, and
other layout regions. OCR quality can also be low. This study develops a novel,
deep learning pipeline for extracting full article texts from newspaper images
and applies it to the nearly 20 million scans in Library of Congress's public
domain Chronicling America collection. The pipeline includes layout detection,
legibility classification, custom OCR, and association of article texts
spanning multiple bounding boxes. To achieve high scalability, it is built with
efficient architectures designed for mobile phones. The resulting American
Stories dataset provides high quality data that could be used for pre-training
a large language model to achieve better understanding of historical English
and historical world knowledge. The dataset could also be added to the external
database of a retrieval-augmented language model to make historical information
- ranging from interpretations of political events to minutiae about the lives
of people's ancestors - more widely accessible. Furthermore, structured article
texts facilitate using transformer-based methods for popular social science
applications like topic classification, detection of reproduced content, and
news story clustering. Finally, American Stories provides a massive silver
quality dataset for innovating multimodal layout analysis models and other
multimodal applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12494">MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices. (arXiv:2308.12494v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhen_R/0/1/0/all/0/1">Ruiwen Zhen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaotian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanghui Wang</a></p>
<p>Image restoration aims to restore high-quality images from degraded
counterparts and has seen significant advancements through deep learning
techniques. The technique has been widely applied to mobile devices for tasks
such as mobile photography. Given the resource limitations on mobile devices,
such as memory constraints and runtime requirements, the efficiency of models
during deployment becomes paramount. Nevertheless, most previous works have
primarily concentrated on analyzing the efficiency of single modules and
improving them individually. This paper examines the efficiency across
different layers. We propose a roadmap that can be applied to further
accelerate image restoration models prior to deployment while simultaneously
increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity
Index). The roadmap first increases the model capacity by adding more
parameters to partial convolutions on FLOPs non-sensitive layers. Then, it
applies partial depthwise convolution coupled with decoupling
upsampling/downsampling layers to accelerate the model speed. Extensive
experiments demonstrate that our approach decreases runtime by up to 13% and
reduces the number of parameters by up to 23%, while increasing PSNR and SSIM
on several image restoration datasets. Source Code of our method is available
at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12495">Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis. (arXiv:2308.12495v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuqi Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinjian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Shijun Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozoki_A/0/1/0/all/0/1">Andrea Bozoki</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Huaicheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingxia Liu</a></p>
<p>Resting-state functional MRI (rs-fMRI) is increasingly employed in multi-site
research to aid neurological disorder analysis. Existing studies usually suffer
from significant cross-site/domain data heterogeneity caused by site effects
such as differences in scanners/protocols. Many methods have been proposed to
reduce fMRI heterogeneity between source and target domains, heavily relying on
the availability of source data. But acquiring source data is challenging due
to privacy concerns and/or data storage burdens in multi-site studies. To this
end, we design a source-free collaborative domain adaptation (SCDA) framework
for fMRI analysis, where only a pretrained source model and unlabeled target
data are accessible. Specifically, a multi-perspective feature enrichment
method (MFE) is developed for target fMRI analysis, consisting of multiple
collaborative branches to dynamically capture fMRI features of unlabeled target
data from multiple views. Each branch has a data-feeding module, a
spatiotemporal feature encoder, and a class predictor. A mutual-consistency
constraint is designed to encourage pair-wise consistency of latent features of
the same input generated from these branches for robust representation
learning. To facilitate efficient cross-domain knowledge transfer without
source data, we initialize MFE using parameters of a pretrained source model.
We also introduce an unsupervised pretraining strategy using 3,806 unlabeled
fMRIs from three large-scale auxiliary databases, aiming to obtain a general
feature encoder. Experimental results on three public datasets and one private
dataset demonstrate the efficacy of our method in cross-scanner and cross-study
prediction tasks. The model pretrained on large-scale rs-fMRI data has been
released to the public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12501">DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition. (arXiv:2308.12501v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yingchi Mao</a></p>
<p>Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12508">FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution. (arXiv:2308.12508v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jiao_C/0/1/0/all/0/1">Chenyue Jiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Bi_C/0/1/0/all/0/1">Chongke Bi</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1">Lu Yang</a></p>
<p>Large-scale numerical simulations are capable of generating data up to
terabytes or even petabytes. As a promising method of data reduction,
super-resolution (SR) has been widely studied in the scientific visualization
community. However, most of them are based on deep convolutional neural
networks (CNNs) or generative adversarial networks (GANs) and the scale factor
needs to be determined before constructing the network. As a result, a single
training session only supports a fixed factor and has poor generalization
ability. To address these problems, this paper proposes a Feature-Enhanced
Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of
flow field data. It can take full advantage of the implicit neural
representation in terms of model structure and sampling resolution. The neural
representation is based on a fully connected network with periodic activation
functions, which enables us to obtain lightweight models. The learned
continuous representation can decode the low-resolution flow field input data
to arbitrary spatial and temporal resolutions, allowing for flexible
upsampling. The training process of FFEINR is facilitated by introducing
feature enhancements for the input layer, which complements the contextual
information of the flow field.To demonstrate the effectiveness of the proposed
method, a series of experiments are conducted on different datasets by setting
different hyperparameters. The results show that FFEINR achieves significantly
better results than the trilinear interpolation method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12509">Parameter-Efficient Transfer Learning for Remote Sensing Image-Text Retrieval. (arXiv:2308.12509v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yang Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhitong Xiong</a></p>
<p>Vision-and-language pre-training (VLP) models have experienced a surge in
popularity recently. By fine-tuning them on specific datasets, significant
performance improvements have been observed in various tasks. However, full
fine-tuning of VLP models not only consumes a significant amount of
computational resources but also has a significant environmental impact.
Moreover, as remote sensing (RS) data is constantly being updated, full
fine-tuning may not be practical for real-world applications. To address this
issue, in this work, we investigate the parameter-efficient transfer learning
(PETL) method to effectively and efficiently transfer visual-language knowledge
from the natural domain to the RS domain on the image-text retrieval task. To
this end, we make the following contributions. 1) We construct a novel and
sophisticated PETL framework for the RS image-text retrieval (RSITR) task,
which includes the pretrained CLIP model, a multimodal remote sensing adapter,
and a hybrid multi-modal contrastive (HMMC) learning objective; 2) To deal with
the problem of high intra-modal similarity in RS data, we design a simple yet
effective HMMC loss; 3) We provide comprehensive empirical studies for
PETL-based RS image-text retrieval. Our results demonstrate that the proposed
method is promising and of great potential for practical applications. 4) We
benchmark extensive state-of-the-art PETL methods on the RSITR task. Our
proposed model only contains 0.16M training parameters, which can achieve a
parameter reduction of 98.9% compared to full fine-tuning, resulting in
substantial savings in training costs. Our retrieval performance exceeds
traditional methods by 7-13% and achieves comparable or better performance than
full fine-tuning. This work can provide new ideas and useful insights for RS
vision-language tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12510">Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1">Jiang-Tian Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xialei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1">Andrew D. Bagdanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a></p>
<p>Class Incremental Learning (CIL) aims to sequentially learn new classes while
avoiding catastrophic forgetting of previous knowledge. We propose to use
Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally
designed to learn useful representations through reconstructive unsupervised
learning, and they can be easily integrated with a supervised loss for
classification. Moreover, MAEs can reliably reconstruct original input images
from randomly selected patches, which we use to store exemplars from past tasks
more efficiently for CIL. We also propose a bilateral MAE framework to learn
from image-level and embedding-level fusion, which produces better-quality
reconstructed images and more stable representations. Our experiments confirm
that our approach performs better than the state-of-the-art on CIFAR-100,
ImageNet-Subset, and ImageNet-Full. The code is available at
https://github.com/scok30/MAE-CIL .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12512">I3DOD: Towards Incremental 3D Object Detection via Prompting. (arXiv:2308.12512v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Wenqi Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Gan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenxi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiahua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kangru Wang</a></p>
<p>3D object detection has achieved significant performance in many fields,
e.g., robotics system, autonomous driving, and augmented reality. However, most
existing methods could cause catastrophic forgetting of old classes when
performing on the class-incremental scenarios. Meanwhile, the current
class-incremental 3D object detection methods neglect the relationships between
the object localization information and category semantic information and
assume all the knowledge of old model is reliable. To address the above
challenge, we present a novel Incremental 3D Object Detection framework with
the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared
prompts mechanism to learn the matching relationships between the object
localization information and category semantic information. After training on
the current task, these prompts will be stored in our prompt pool, and perform
the relationship of old classes in the next task. Moreover, we design a
reliable distillation strategy to transfer knowledge from two aspects: a
reliable dynamic distillation is developed to filter out the negative knowledge
and transfer the reliable 3D knowledge to new detection model; the relation
feature is proposed to capture the responses relation in feature space and
protect plasticity of the model when learning novel 3D classes. To the end, we
conduct comprehensive experiments on two benchmark datasets and our method
outperforms the state-of-the-art object detection methods by 0.6% - 2.7% in
terms of mAP@0.25.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12522">Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition. (arXiv:2308.12522v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Siming Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaoxuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xinpeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuchen Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hualiang Wang</a></p>
<p>Recently, large-scale pre-trained vision-language models have presented
benefits for alleviating class imbalance in long-tailed recognition. However,
the long-tailed data distribution can corrupt the representation space, where
the distance between head and tail categories is much larger than the distance
between two tail categories. This uneven feature space distribution causes the
model to exhibit unclear and inseparable decision boundaries on the uniformly
distributed test set, which lowers its performance. To address these
challenges, we propose the uniformly category prototype-guided vision-language
framework to effectively mitigate feature space bias caused by data imbalance.
Especially, we generate a set of category prototypes uniformly distributed on a
hypersphere. Category prototype-guided mechanism for image-text matching makes
the features of different classes converge to these distinct and uniformly
distributed category prototypes, which maintain a uniform distribution in the
feature space, and improve class boundaries. Additionally, our proposed
irrelevant text filtering and attribute enhancement module allows the model to
ignore irrelevant noisy text and focus more on key attribute information,
thereby enhancing the robustness of our framework. In the image recognition
fine-tuning stage, to address the positive bias problem of the learnable
classifier, we design the class feature prototype-guided classifier, which
compensates for the performance of tail classes while maintaining the
performance of head classes. Our method outperforms previous vision-language
methods for long-tailed learning work by a large margin and achieves
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12530">SieveNet: Selecting Point-Based Features for Mesh Networks. (arXiv:2308.12530v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shengchao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1">Yishun Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Rui Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bingbing Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhong Zheng</a></p>
<p>Meshes are widely used in 3D computer vision and graphics, but their
irregular topology poses challenges in applying them to existing neural network
architectures. Recent advances in mesh neural networks turn to remeshing and
push the boundary of pioneer methods that solely take the raw meshes as input.
Although the remeshing offers a regular topology that significantly facilitates
the design of mesh network architectures, features extracted from such remeshed
proxies may struggle to retain the underlying geometry faithfully, limiting the
subsequent neural network's capacity. To address this issue, we propose
SieveNet, a novel paradigm that takes into account both the regular topology
and the exact geometry. Specifically, this method utilizes structured mesh
topology from remeshing and accurate geometric information from
distortion-aware point sampling on the surface of the original mesh.
Furthermore, our method eliminates the need for hand-crafted feature
engineering and can leverage off-the-shelf network architectures such as the
vision transformer. Comprehensive experimental results on classification and
segmentation tasks well demonstrate the effectiveness and superiority of our
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12532">FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minchan Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangmook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12534">Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation. (arXiv:2308.12534v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zikun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shukun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guoqing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhenyu He</a></p>
<p>RGB-Thermal (RGB-T) semantic segmentation has shown great potential in
handling low-light conditions where RGB-based segmentation is hindered by poor
RGB imaging quality. The key to RGB-T semantic segmentation is to effectively
leverage the complementarity nature of RGB and thermal images. Most existing
algorithms fuse RGB and thermal information in feature space via concatenation,
element-wise summation, or attention operations in either unidirectional
enhancement or bidirectional aggregation manners. However, they usually
overlook the modality gap between RGB and thermal images during feature fusion,
resulting in modality-specific information from one modality contaminating the
other. In this paper, we propose a Channel and Spatial Relation-Propagation
Network (CSRPNet) for RGB-T semantic segmentation, which propagates only
modality-shared information across different modalities and alleviates the
modality-specific information contamination issue. Our CSRPNet first performs
relation-propagation in channel and spatial dimensions to capture the
modality-shared features from the RGB and thermal features. CSRPNet then
aggregates the modality-shared features captured from one modality with the
input feature from the other modality to enhance the input feature without the
contamination issue. While being fused together, the enhanced RGB and thermal
features will be also fed into the subsequent RGB or thermal feature extraction
layers for interactive feature fusion, respectively. We also introduce a
dual-path cascaded feature refinement module that aggregates multi-layer
features to produce two refined features for semantic and boundary prediction.
Extensive experimental results demonstrate that CSRPNet performs favorably
against state-of-the-art algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12535">SCP: Spherical-Coordinate-based Learned Point Cloud Compression. (arXiv:2308.12535v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1">Ao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Linxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Nonaka_K/0/1/0/all/0/1">Keisuke Nonaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Unno_K/0/1/0/all/0/1">Kyohei Unno</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Heming Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Goto_M/0/1/0/all/0/1">Masayuki Goto</a>, <a href="http://arxiv.org/find/cs/1/au:+Katto_J/0/1/0/all/0/1">Jiro Katto</a></p>
<p>In recent years, the task of learned point cloud compression has gained
prominence. An important type of point cloud, the spinning LiDAR point cloud,
is generated by spinning LiDAR on vehicles. This process results in numerous
circular shapes and azimuthal angle invariance features within the point
clouds. However, these two features have been largely overlooked by previous
methodologies. In this paper, we introduce a model-agnostic method called
Spherical-Coordinate-based learned Point cloud compression (SCP), designed to
leverage the aforementioned features fully. Additionally, we propose a
multi-level Octree for SCP to mitigate the reconstruction error for distant
areas within the Spherical-coordinate-based Octree. SCP exhibits excellent
universality, making it applicable to various learned point cloud compression
techniques. Experimental results demonstrate that SCP surpasses previous
state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12537">HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks. (arXiv:2308.12537v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zichao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xufeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Hang Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xin Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junbo Chen</a></p>
<p>Human robot interaction is an exciting task, which aimed to guide robots
following instructions from human. Since huge gap lies between human natural
language and machine codes, end to end human robot interaction models is fair
challenging. Further, visual information receiving from sensors of robot is
also a hard language for robot to perceive. In this work, HuBo-VLM is proposed
to tackle perception tasks associated with human robot interaction including
object detection and visual grounding by a unified transformer based vision
language model. Extensive experiments on the Talk2Car benchmark demonstrate the
effectiveness of our approach. Code would be publicly available in
https://github.com/dzcgaara/HuBo-VLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12538">Mutual-Guided Dynamic Network for Image Fusion. (arXiv:2308.12538v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yuanshen Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruikang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1">Mingde Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lizhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhiwei Xiong</a></p>
<p>Image fusion aims to generate a high-quality image from multiple images
captured under varying conditions. The key problem of this task is to preserve
complementary information while filtering out irrelevant information for the
fused result. However, existing methods address this problem by leveraging
static convolutional neural networks (CNNs), suffering two inherent limitations
during feature extraction, i.e., being unable to handle spatial-variant
contents and lacking guidance from multiple inputs. In this paper, we propose a
novel mutual-guided dynamic network (MGDN) for image fusion, which allows for
effective information utilization across different locations and inputs.
Specifically, we design a mutual-guided dynamic filter (MGDF) for adaptive
feature extraction, composed of a mutual-guided cross-attention (MGCA) module
and a dynamic filter predictor, where the former incorporates additional
guidance from different inputs and the latter generates spatial-variant kernels
for different locations. In addition, we introduce a parallel feature fusion
(PFF) module to effectively fuse local and global information of the extracted
features. To further reduce the redundancy among the extracted features while
simultaneously preserving their shared structural information, we devise a
novel loss function that combines the minimization of normalized mutual
information (NMI) with an estimated gradient mask. Experimental results on five
benchmark datasets demonstrate that our proposed method outperforms existing
methods on four image fusion tasks. The code and model are publicly available
at: https://github.com/Guanys-dar/MGDN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12547">Hybrid Models for Facial Emotion Recognition in Children. (arXiv:2308.12547v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zimmer_R/0/1/0/all/0/1">Rafael Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sobral_M/0/1/0/all/0/1">Marcos Sobral</a>, <a href="http://arxiv.org/find/cs/1/au:+Azevedo_H/0/1/0/all/0/1">Helio Azevedo</a></p>
<p>This paper focuses on the use of emotion recognition techniques to assist
psychologists in performing children's therapy through remotely robot operated
sessions. In the field of psychology, the use of agent-mediated therapy is
growing increasingly given recent advances in robotics and computer science.
Specifically, the use of Embodied Conversational Agents (ECA) as an
intermediary tool can help professionals connect with children who face social
challenges such as Attention Deficit Hyperactivity Disorder (ADHD), Autism
Spectrum Disorder (ASD) or even who are physically unavailable due to being in
regions of armed conflict, natural disasters, or other circumstances. In this
context, emotion recognition represents an important feedback for the
psychotherapist. In this article, we initially present the result of a
bibliographical research associated with emotion recognition in children. This
research revealed an initial overview on algorithms and datasets widely used by
the community. Then, based on the analysis carried out on the results of the
bibliographical research, we used the technique of dense optical flow features
to improve the ability of identifying emotions in children in uncontrolled
environments. From the output of a hybrid model of Convolutional Neural
Network, two intermediary features are fused before being processed by a final
classifier. The proposed architecture was called HybridCNNFusion. Finally, we
present the initial results achieved in the recognition of children's emotions
using a dataset of Brazilian children.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12549">Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking. (arXiv:2308.12549v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Teli Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengmeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jimin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Huifeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>Siamese network has been a de facto benchmark framework for 3D LiDAR object
tracking with a shared-parametric encoder extracting features from template and
search region, respectively. This paradigm relies heavily on an additional
matching network to model the cross-correlation/similarity of the template and
search region. In this paper, we forsake the conventional Siamese paradigm and
propose a novel single-branch framework, SyncTrack, synchronizing the feature
extracting and matching to avoid forwarding encoder twice for template and
search region as well as introducing extra parameters of matching network. The
synchronization mechanism is based on the dynamic affinity of the Transformer,
and an in-depth analysis of the relevance is provided theoretically. Moreover,
based on the synchronization, we introduce a novel Attentive Points-Sampling
strategy into the Transformer layers (APST), replacing the random/Farthest
Points Sampling (FPS) method with sampling under the supervision of attentive
relations between the template and search region. It implies connecting
point-wise sampling with the feature learning, beneficial to aggregating more
distinctive and geometric features for tracking with sparse points. Extensive
experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack
achieves state-of-the-art performance in real-time tracking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12558">Hyperbolic Audio-visual Zero-shot Learning. (arXiv:2308.12558v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Jie Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayder_Z/0/1/0/all/0/1">Zeeshan Hayder</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Junlin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1">Pengfei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1">Mehrtash Harandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1">Lars Petersson</a></p>
<p>Audio-visual zero-shot learning aims to classify samples consisting of a pair
of corresponding audio and video sequences from classes that are not present
during training. An analysis of the audio-visual data reveals a large degree of
hyperbolicity, indicating the potential benefit of using a hyperbolic
transformation to achieve curvature-aware geometric learning, with the aim of
exploring more complex hierarchical data structures for this task. The proposed
approach employs a novel loss function that incorporates cross-modality
alignment between video and audio features in the hyperbolic space.
Additionally, we explore the use of multiple adaptive curvatures for hyperbolic
projections. The experimental results on this very challenging task demonstrate
that our proposed hyperbolic approach for zero-shot learning outperforms the
SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL
achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12560">NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects. (arXiv:2308.12560v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_D/0/1/0/all/0/1">Dakshit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiajie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mustikovela_S/0/1/0/all/0/1">Siva Karthik Mustikovela</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkioulekas_I/0/1/0/all/0/1">Ioannis Gkioulekas</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Ashish Shrivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1">Yuning Chai</a></p>
<p>We propose a novel-view augmentation (NOVA) strategy to train NeRFs for
photo-realistic 3D composition of dynamic objects in a static scene. Compared
to prior work, our framework significantly reduces blending artifacts when
inserting multiple dynamic objects into a 3D scene at novel views and times;
achieves comparable PSNR without the need for additional ground truth
modalities like optical flow; and overall provides ease, flexibility, and
scalability in neural composition. Our codebase is on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12570">StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction. (arXiv:2308.12570v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1">Tianyuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hang Zhao</a></p>
<p>High-Definition (HD) maps are essential for the safety of autonomous driving
systems. While existing techniques employ camera images and onboard sensors to
generate vectorized high-precision maps, they are constrained by their reliance
on single-frame input. This approach limits their stability and performance in
complex scenarios such as occlusions, largely due to the absence of temporal
information. Moreover, their performance diminishes when applied to broader
perception ranges. In this paper, we present StreamMapNet, a novel online
mapping pipeline adept at long-sequence temporal modeling of videos.
StreamMapNet employs multi-point attention and temporal information which
empowers the construction of large-range local HD maps with high stability and
further addresses the limitations of existing methods. Furthermore, we
critically examine widely used online HD Map construction benchmark and
datasets, Argoverse2 and nuScenes, revealing significant bias in the existing
evaluation protocols. We propose to resplit the benchmarks according to
geographical spans, promoting fair and precise evaluations. Experimental
results validate that StreamMapNet significantly outperforms existing methods
across all settings while maintaining an online inference speed of $14.2$ FPS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12577">REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shuai Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_D/0/1/0/all/0/1">Dongmei Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1">Waikeung Wong</a></p>
<p>Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct
industrial anomaly detection in two stages: obtain feature representations with
a pre-trained CNN model and perform distance measures for defect detection.
However, the features are not fully exploited as they ignore domain bias and
the difference of local density in feature space, which limits the detection
performance. In this paper, we propose Reducing Biases (REB) in representation
by considering the domain bias of the pre-trained model and building a
self-supervised learning task for better domain adaption with a defect
generation strategy (DefectMaker) imitating the natural defects. Additionally,
we propose a local density KNN (LDKNN) to reduce the local density bias and
obtain effective anomaly detection. We achieve a promising result of 99.5\%
AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on
the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC
to the state-of-the-art result. All results are obtained with smaller backbone
networks such as Vgg11 and Resnet18, which indicates the effectiveness and
efficiency of REB for practical industrial applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12584">LORD: Leveraging Open-Set Recognition with Unknown Data. (arXiv:2308.12584v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koch_T/0/1/0/all/0/1">Tobias Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1">Christian Riess</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_T/0/1/0/all/0/1">Thomas K&#xf6;hler</a></p>
<p>Handling entirely unknown data is a challenge for any deployed classifier.
Classification models are typically trained on a static pre-defined dataset and
are kept in the dark for the open unassigned feature space. As a result, they
struggle to deal with out-of-distribution data during inference. Addressing
this task on the class-level is termed open-set recognition (OSR). However,
most OSR methods are inherently limited, as they train closed-set classifiers
and only adapt the downstream predictions to OSR. This work presents LORD, a
framework to Leverage Open-set Recognition by exploiting unknown Data. LORD
explicitly models open space during classifier training and provides a
systematic evaluation for such approaches. We identify three model-agnostic
training strategies that exploit background data and applied them to
well-established classifiers. Due to LORD's extensive evaluation protocol, we
consistently demonstrate improved recognition of unknown data. The benchmarks
facilitate in-depth analysis across various requirement levels. To mitigate
dependency on extensive and costly background datasets, we explore mixup as an
off-the-shelf data generation technique. Our experiments highlight mixup's
effectiveness as a substitute for background datasets. Lightweight constraints
on mixup synthesis further improve OSR performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12587">Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation. (arXiv:2308.12587v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yibo Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Liang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yakun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Meishan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Ye Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_E/0/1/0/all/0/1">Erwei Yin</a></p>
<p>Cross-modal alignment is one key challenge for Vision-and-Language Navigation
(VLN). Most existing studies concentrate on mapping the global instruction or
single sub-instruction to the corresponding trajectory. However, another
critical problem of achieving fine-grained alignment at the entity level is
seldom considered. To address this problem, we propose a novel Grounded
Entity-Landmark Adaptive (GELA) pre-training paradigm for VLN tasks. To achieve
the adaptive pre-training paradigm, we first introduce grounded entity-landmark
human annotations into the Room-to-Room (R2R) dataset, named GEL-R2R.
Additionally, we adopt three grounded entity-landmark adaptive pre-training
objectives: 1) entity phrase prediction, 2) landmark bounding box prediction,
and 3) entity-landmark semantic alignment, which explicitly supervise the
learning of fine-grained cross-modal alignment between entity phrases and
environment landmarks. Finally, we validate our model on two downstream
benchmarks: VLN with descriptive instructions (R2R) and dialogue instructions
(CVDN). The comprehensive experiments show that our GELA model achieves
state-of-the-art results on both tasks, demonstrating its effectiveness and
generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12590">Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects. (arXiv:2308.12590v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baowen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiaoming Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinda Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Cuixia Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongan Wang</a></p>
<p>Learning 3D shape representation with dense correspondence for deformable
objects is a fundamental problem in computer vision. Existing approaches often
need additional annotations of specific semantic domain, e.g., skeleton poses
for human bodies or animals, which require extra annotation effort and suffer
from error accumulation, and they are limited to specific domain. In this
paper, we propose a novel self-supervised approach to learn neural implicit
shape representation for deformable objects, which can represent shapes with a
template shape and dense correspondence in 3D. Our method does not require the
priors of skeleton and skinning weight, and only requires a collection of
shapes represented in signed distance fields. To handle the large deformation,
we constrain the learned template shape in the same latent space with the
training shapes, design a new formulation of local rigid constraint that
enforces rigid transformation in local region and addresses local reflection
issue, and present a new hierarchical rigid constraint to reduce the ambiguity
due to the joint learning of template shape and correspondences. Extensive
experiments show that our model can represent shapes with large deformations.
We also show that our shape representation can support two typical
applications, such as texture transfer and shape editing, with competitive
performance. The code and models are available at
https://iscas3dv.github.io/deformshape
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12595">Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation. (arXiv:2308.12595v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1">Chen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenguan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1">Jiaxu Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>Recent advances in semi-supervised semantic segmentation have been heavily
reliant on pseudo labeling to compensate for limited labeled data, disregarding
the valuable relational knowledge among semantic concepts. To bridge this gap,
we devise LogicDiag, a brand new neural-logic semi-supervised learning
framework. Our key insight is that conflicts within pseudo labels, identified
through symbolic knowledge, can serve as strong yet commonly ignored learning
signals. LogicDiag resolves such conflicts via reasoning with logic-induced
diagnoses, enabling the recovery of (potentially) erroneous pseudo labels,
ultimately alleviating the notorious error accumulation problem. We showcase
the practical application of LogicDiag in the data-hungry segmentation
scenario, where we formalize the structured abstraction of semantic concepts as
a set of logic rules. Extensive experiments on three standard semi-supervised
semantic segmentation benchmarks demonstrate the effectiveness and generality
of LogicDiag. Moreover, LogicDiag highlights the promising opportunities
arising from the systematic integration of symbolic reasoning into the
prevalent statistical, neural learning approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12600">PoseSync: Robust pose based video synchronization. (arXiv:2308.12600v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javia_R/0/1/0/all/0/1">Rishit Javia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1">Falak Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1">Shivam Dave</a></p>
<p>Pose based video sychronization can have applications in multiple domains
such as gameplay performance evaluation, choreography or guiding athletes. The
subject's actions could be compared and evaluated against those performed by
professionals side by side. In this paper, we propose an end to end pipeline
for synchronizing videos based on pose. The first step crops the region where
the person present in the image followed by pose detection on the cropped
image. This is followed by application of Dynamic Time Warping(DTW) on angle/
distance measures between the pose keypoints leading to a scale and shift
invariant pose matching pipeline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12604">PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Haibo Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1">Haoxuan Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Automatic medical report generation (MRG) is of great research value as it
has the potential to relieve radiologists from the heavy burden of report
writing. Despite recent advancements, accurate MRG remains challenging due to
the need for precise clinical understanding and the identification of clinical
findings. Moreover, the imbalanced distribution of diseases makes the challenge
even more pronounced, as rare diseases are underrepresented in training data,
making their diagnostic performance unreliable. To address these challenges, we
propose diagnosis-driven prompts for medical report generation (PromptMRG), a
novel framework that aims to improve the diagnostic accuracy of MRG with the
guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on
encoder-decoder architecture with an extra disease classification branch. When
generating reports, the diagnostic results from the classification branch are
converted into token prompts to explicitly guide the generation process. To
further improve the diagnostic accuracy, we design cross-modal feature
enhancement, which retrieves similar reports from the database to assist the
diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP.
Moreover, the disease imbalanced issue is addressed by applying an adaptive
logit-adjusted loss to the classification branch based on the individual
learning status of each disease, which overcomes the barrier of text decoder's
inability to manipulate disease distributions. Experiments on two MRG
benchmarks show the effectiveness of the proposed method, where it obtains
state-of-the-art clinical efficacy performance on both datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12605">APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yupu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shangqi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zihan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Harry Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Liang-Jian Deng</a></p>
<p>Diffusion models have exhibited promising progress in video generation.
However, they often struggle to retain consistent details within local regions
across frames. One underlying cause is that traditional diffusion models
approximate Gaussian noise distribution by utilizing predictive noise, without
fully accounting for the impact of inherent information within the input
itself. Additionally, these models emphasize the distinction between
predictions and references, neglecting information intrinsic to the videos. To
address this limitation, inspired by the self-attention mechanism, we propose a
novel text-to-video (T2V) generation network structure based on diffusion
models, dubbed Additional Perturbation for Latent noise with Adversarial
training (APLA). Our approach only necessitates a single video as input and
builds upon pre-trained stable diffusion networks. Notably, we introduce an
additional compact network, known as the Video Generation Transformer (VGT).
This auxiliary component is designed to extract perturbations from the inherent
information contained within the input, thereby refining inconsistent pixels
during temporal predictions. We leverage a hybrid architecture of transformers
and convolutions to compensate for temporal intricacies, enhancing consistency
between different frames within the video. Experiments demonstrate a noticeable
improvement in the consistency of the generated videos both qualitatively and
quantitatively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12608">HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation. (arXiv:2308.12608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1">Zhiwu Qing</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Changxin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a></p>
<p>Point-supervised Temporal Action Localization (PSTAL) is an emerging research
direction for label-efficient learning. However, current methods mainly focus
on optimizing the network either at the snippet-level or the instance-level,
neglecting the inherent reliability of point annotations at both levels. In
this paper, we propose a Hierarchical Reliability Propagation (HR-Pro)
framework, which consists of two reliability-aware stages: Snippet-level
Discrimination Learning and Instance-level Completeness Learning, both stages
explore the efficient propagation of high-confidence cues in point annotations.
For snippet-level learning, we introduce an online-updated memory to store
reliable snippet prototypes for each class. We then employ a Reliability-aware
Attention Block to capture both intra-video and inter-video dependencies of
snippets, resulting in more discriminative and robust snippet representation.
For instance-level learning, we propose a point-based proposal generation
approach as a means of connecting snippets and instances, which produces
high-confidence proposals for further optimization at the instance level.
Through multi-level reliability-aware learning, we obtain more reliable
confidence scores and more accurate temporal boundaries of predicted proposals.
Our HR-Pro achieves state-of-the-art performance on multiple challenging
benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably,
our HR-Pro largely surpasses all previous point-supervised methods, and even
outperforms several competitive fully supervised methods. Code will be
available at https://github.com/pipixin321/HR-Pro.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12609">Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization. (arXiv:2308.12609v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songchun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chunhui Zhao</a></p>
<p>Weakly supervised temporal action localization (WSTAL) aims to localize
actions in untrimmed videos using video-level labels. Despite recent advances,
existing approaches mainly follow a localization-by-classification pipeline,
generally processing each segment individually, thereby exploiting only limited
contextual information. As a result, the model will lack a comprehensive
understanding (e.g. appearance and temporal structure) of various action
patterns, leading to ambiguity in classification learning and temporal
localization. Our work addresses this from a novel perspective, by exploring
and exploiting the cross-video contextual knowledge within the dataset to
recover the dataset-level semantic structure of action instances via weak
labels only, thereby indirectly improving the holistic understanding of
fine-grained action patterns and alleviating the aforementioned ambiguities.
Specifically, an end-to-end framework is proposed, including a Robust
Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge
Summarization and Aggregation (GKSA) module. First, the RMGCL module explores
the contrast and consistency of cross-video action features, assisting in
learning more structured and compact embedding space, thus reducing ambiguity
in classification learning. Further, the GKSA module is used to efficiently
summarize and propagate the cross-video representative action knowledge in a
learnable manner to promote holistic action patterns understanding, which in
turn allows the generation of high-confidence pseudo-labels for self-learning,
thus alleviating ambiguity in temporal localization. Extensive experiments on
THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method
outperforms the state-of-the-art methods, and can be easily plugged into other
WSTAL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12634">Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cersovsky_J/0/1/0/all/0/1">Josef Cersovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1">Sadegh Mohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1">Dagmar Kainmueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoehne_J/0/1/0/all/0/1">Johannes Hoehne</a></p>
<p>The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12642">Tag-Based Annotation for Avatar Face Creation. (arXiv:2308.12642v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1">An Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Phelps_D/0/1/0/all/0/1">Daniel Phelps</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_D/0/1/0/all/0/1">Derrick Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1">Thanyared Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1">Lucas Mathias</a>, <a href="http://arxiv.org/find/cs/1/au:+Shivamurthy_A/0/1/0/all/0/1">Anish Shivamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajmal_M/0/1/0/all/0/1">Mustafa Ajmal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1">James Davis</a></p>
<p>Currently, digital avatars can be created manually using human images as
reference. Systems such as Bitmoji are excellent producers of detailed avatar
designs, with hundreds of choices for customization. A supervised learning
model could be trained to generate avatars automatically, but the hundreds of
possible options create difficulty in securing non-noisy data to train a model.
As a solution, we train a model to produce avatars from human images using
tag-based annotations. This method provides better annotator agreement, leading
to less noisy data and higher quality model predictions. Our contribution is an
application of tag-based annotation to train a model for avatar face creation.
We design tags for 3 different facial facial features offered by Bitmoji, and
train a model using tag-based annotation to predict the nose.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12645">An All Deep System for Badminton Game Analysis. (arXiv:2308.12645v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chou_P/0/1/0/all/0/1">Po-Yung Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1">Yu-Chun Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1">Bo-Zheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Cheng-Hung Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_Y/0/1/0/all/0/1">Yu-Yung Kao</a></p>
<p>The CoachAI Badminton 2023 Track1 initiative aim to automatically detect
events within badminton match videos. Detecting small objects, especially the
shuttlecock, is of quite importance and demands high precision within the
challenge. Such detection is crucial for tasks like hit count, hitting time,
and hitting location. However, even after revising the well-regarded
shuttlecock detecting model, TrackNet, our object detection models still fall
short of the desired accuracy. To address this issue, we've implemented various
deep learning methods to tackle the problems arising from noisy detectied data,
leveraging diverse data types to improve precision. In this report, we detail
the detection model modifications we've made and our approach to the 11 tasks.
Notably, our system garnered a score of 0.78 out of 1.0 in the challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12661">Don&#x27;t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1">Paul Gavrikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1">Janis Keuper</a></p>
<p>Assessing the robustness of deep neural networks against out-of-distribution
inputs is crucial, especially in safety-critical domains like autonomous
driving, but also in safety systems where malicious actors can digitally alter
inputs to circumvent safety guards. However, designing effective
out-of-distribution tests that encompass all possible scenarios while
preserving accurate label information is a challenging task. Existing
methodologies often entail a compromise between variety and constraint levels
for attacks and sometimes even both. In a first step towards a more holistic
robustness evaluation of image classification models, we introduce an attack
method based on image solarization that is conceptually straightforward yet
avoids jeopardizing the global structure of natural images independent of the
intensity. Through comprehensive evaluations of multiple ImageNet models, we
demonstrate the attack's capacity to degrade accuracy significantly, provided
it is not integrated into the training augmentations. Interestingly, even then,
no full immunity to accuracy deterioration is achieved. In other settings, the
attack can often be simplified into a black-box attack with model-independent
parameters. Defenses against other corruptions do not consistently extend to be
effective against our specific attack.
</p>
<p>Project website: https://github.com/paulgavrikov/adversarial_solarization
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12673">Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition. (arXiv:2308.12673v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Daskalakis_D/0/1/0/all/0/1">Dimitrios Daskalakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkalelis_N/0/1/0/all/0/1">Nikolaos Gkalelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mezaris_V/0/1/0/all/0/1">Vasileios Mezaris</a></p>
<p>In this paper, we introduce Masked Feature Modelling (MFM), a novel approach
for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM
utilizes a pretrained Visual Tokenizer to reconstruct masked features of
objects within a video, leveraging the MiniKinetics dataset. We then
incorporate the pre-trained GAT block into a state-of-the-art bottom-up
supervised video-event recognition architecture, ViGAT, to improve the model's
starting point and overall accuracy. Experimental evaluations on the YLI-MED
dataset demonstrate the effectiveness of MFM in improving event recognition
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12675">A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes. (arXiv:2308.12675v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadafi_A/0/1/0/all/0/1">Ario Sadafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hehr_M/0/1/0/all/0/1">Matthias Hehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1">Carsten Marr</a></p>
<p>Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial
for clinical decision-making and patient care. In this study, we investigate
the potential presence of age and sex bias in AML subtype classification using
Multiple Instance Learning (MIL) architectures. To that end, we train multiple
MIL models using different levels of sex imbalance in the training set and
excluding certain age groups. To assess the sex bias, we evaluate the
performance of the models on male and female test sets. For age bias, models
are tested against underrepresented age groups in the training data. We find a
significant effect of sex and age bias on the performance of the model for AML
subtype classification. Specifically, we observe that females are more likely
to be affected by sex imbalance dataset and certain age groups, such as
patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype,
are significantly affected by an age bias present in the training data.
Ensuring inclusivity in the training data is thus essential for generating
reliable and equitable outcomes in AML genetic subtype classification,
ultimately benefiting diverse patient populations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12679">A Continual Learning Approach for Cross-Domain White Blood Cell Classification. (arXiv:2308.12679v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadafi_A/0/1/0/all/0/1">Ario Sadafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Salehi_R/0/1/0/all/0/1">Raheleh Salehi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gruber_A/0/1/0/all/0/1">Armin Gruber</a>, <a href="http://arxiv.org/find/cs/1/au:+Boushehri_S/0/1/0/all/0/1">Sayedali Shetab Boushehri</a>, <a href="http://arxiv.org/find/cs/1/au:+Giehr_P/0/1/0/all/0/1">Pascal Giehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1">Carsten Marr</a></p>
<p>Accurate classification of white blood cells in peripheral blood is essential
for diagnosing hematological diseases. Due to constantly evolving clinical
settings, data sources, and disease classifications, it is necessary to update
machine learning classification models regularly for practical real-world use.
Such models significantly benefit from sequentially learning from incoming data
streams without forgetting previously acquired knowledge. However, models can
suffer from catastrophic forgetting, causing a drop in performance on previous
tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual
learning approach for class incremental and domain incremental scenarios in
white blood cell classification. To choose representative samples from previous
tasks, we employ exemplar set selection based on the model's predictions. This
involves selecting the most confident samples and the most challenging samples
identified through uncertainty estimation of the model. We thoroughly evaluated
our proposed approach on three white blood cell classification datasets that
differ in color, resolution, and class composition, including scenarios where
new domains or new classes are introduced to the model with every task. We also
test a long class incremental experiment with both new domains and new classes.
Our results demonstrate that our approach outperforms established baselines in
continual learning, including existing iCaRL and EWC methods for classifying
white blood cells in cross-domain environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12700">A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions. (arXiv:2308.12700v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiawei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiaqi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shizhao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weijiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian-Guang Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Creating layouts is a fundamental step in graphic design. In this work, we
propose to use text as the guidance to create graphic layouts, i.e.,
Text-to-Layout, aiming to lower the design barriers. Text-to-Layout is a
challenging task, because it needs to consider the implicit, combined, and
incomplete layout constraints from text, each of which has not been studied in
previous work. To address this, we present a two-stage approach, named
parse-then-place. The approach introduces an intermediate representation (IR)
between text and layout to represent diverse layout constraints. With IR,
Text-to-Layout is decomposed into a parse stage and a place stage. The parse
stage takes a textual description as input and generates an IR, in which the
implicit constraints from the text are transformed into explicit ones. The
place stage generates layouts based on the IR. To model combined and incomplete
constraints, we use a Transformer-based layout generation model and carefully
design a way to represent constraints and layouts as sequences. Besides, we
adopt the pretrain-then-finetune strategy to boost the performance of the
layout generation model with large-scale unlabeled layouts. To evaluate our
approach, we construct two Text-to-Layout datasets and conduct experiments on
them. Quantitative results, qualitative analysis, and user studies demonstrate
the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12712">Ground-to-Aerial Person Search: Benchmark Dataset and Approach. (arXiv:2308.12712v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shizhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qingchun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">De Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1">Yinghui Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1">Guoqiang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanning Zhang</a></p>
<p>In this work, we construct a large-scale dataset for Ground-to-Aerial Person
Search, named G2APS, which contains 31,770 images of 260,559 annotated bounding
boxes for 2,644 identities appearing in both of the UAVs and ground
surveillance cameras. To our knowledge, this is the first dataset for
cross-platform intelligent surveillance applications, where the UAVs could work
as a powerful complement for the ground surveillance cameras. To more
realistically simulate the actual cross-platform Ground-to-Aerial surveillance
scenarios, the surveillance cameras are fixed about 2 meters above the ground,
while the UAVs capture videos of persons at different location, with a variety
of view-angles, flight attitudes and flight modes. Therefore, the dataset has
the following unique characteristics: 1) drastic view-angle changes between
query and gallery person images from cross-platform cameras; 2) diverse
resolutions, poses and views of the person images under 9 rich real-world
scenarios. On basis of the G2APS benchmark dataset, we demonstrate detailed
analysis about current two-step and end-to-end person search methods, and
further propose a simple yet effective knowledge distillation scheme on the
head of the ReID network, which achieves state-of-the-art performances on both
of the G2APS and the previous two public person search datasets, i.e., PRW and
CUHK-SYSU. The dataset and source code available on
\url{https://github.com/yqc123456/HKD_for_person_search}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12714">VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jiahui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Huaping Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaoyi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Conghui He</a></p>
<p>The integration of visual encoders and large language models (LLMs) has
driven recent progress in multimodal large language models (MLLMs). However,
the scarcity of high-quality instruction-tuning data for vision-language tasks
remains a challenge. The current leading paradigm, such as LLaVA, relies on
language-only GPT-4 to generate data, which requires pre-annotated image
captions and detection bounding boxes, suffering from understanding image
details. A practical solution to this problem would be to utilize the available
multimodal large language models (MLLMs) to generate instruction data for
vision-language tasks. However, it's worth noting that the currently accessible
MLLMs are not as powerful as their LLM counterparts, as they tend to produce
inadequate responses and generate false information. As a solution for
addressing the current issue, this paper proposes the Visual Instruction
Generation and Correction (VIGC) framework that enables multimodal large
language models to generate instruction-tuning data and progressively enhance
its quality on-the-fly. Specifically, Visual Instruction Generation (VIG)
guides the vision-language model to generate diverse instruction-tuning data.
To ensure generation quality, Visual Instruction Correction (VIC) adopts an
iterative update mechanism to correct any inaccuracies in data produced by VIG,
effectively reducing the risk of hallucination. Leveraging the diverse,
high-quality data generated by VIGC, we finetune mainstream models and validate
data quality based on various evaluations. Experimental results demonstrate
that VIGC not only compensates for the shortcomings of language-only data
generation methods, but also effectively enhances the benchmark performance.
The models, datasets, and code will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12727">DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images. (arXiv:2308.12727v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dibo_R/0/1/0/all/0/1">Razan Dibo</a>, <a href="http://arxiv.org/find/cs/1/au:+Galichin_A/0/1/0/all/0/1">Andrey Galichin</a>, <a href="http://arxiv.org/find/cs/1/au:+Astashev_P/0/1/0/all/0/1">Pavel Astashev</a>, <a href="http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1">Dmitry V. Dylov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogov_O/0/1/0/all/0/1">Oleg Y. Rogov</a></p>
<p>In recent years, computer-aided diagnosis systems have shown great potential
in assisting radiologists with accurate and efficient medical image analysis.
This paper presents a novel approach for bone pathology localization and
classification in wrist X-ray images using a combination of YOLO (You Only Look
Once) and the Shifted Window Transformer (Swin) with a newly proposed block.
The proposed methodology addresses two critical challenges in wrist X-ray
analysis: accurate localization of bone pathologies and precise classification
of abnormalities. The YOLO framework is employed to detect and localize bone
pathologies, leveraging its real-time object detection capabilities.
Additionally, the Swin, a transformer-based module, is utilized to extract
contextual information from the localized regions of interest (ROIs) for
accurate classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12736">FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI. (arXiv:2308.12736v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1">Santiago Estrada</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugler_D/0/1/0/all/0/1">David K&#xfc;gler</a>, <a href="http://arxiv.org/find/cs/1/au:+Bahrami_E/0/1/0/all/0/1">Emad Bahrami</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Peng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mousa_D/0/1/0/all/0/1">Dilshad Mousa</a>, <a href="http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1">Monique M.B. Breteler</a>, <a href="http://arxiv.org/find/cs/1/au:+Aziz_N/0/1/0/all/0/1">N. Ahmad Aziz</a>, <a href="http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1">Martin Reuter</a></p>
<p>The hypothalamus plays a crucial role in the regulation of a broad range of
physiological, behavioural, and cognitive functions. However, despite its
importance, only a few small-scale neuroimaging studies have investigated its
substructures, likely due to the lack of fully automated segmentation tools to
address scalability and reproducibility issues of manual segmentation. While
the only previous attempt to automatically sub-segment the hypothalamus with a
neural network showed promise for 1.0 mm isotropic T1-weighted (T1w) MRI, there
is a need for an automated tool to sub-segment also high-resolutional (HiRes)
MR scans, as they are becoming widely available, and include structural detail
also from multi-modal MRI. We, therefore, introduce a novel, fast, and fully
automated deep learning method named HypVINN for sub-segmentation of the
hypothalamus and adjacent structures on 0.8 mm isotropic T1w and T2w brain MR
images that is robust to missing modalities. We extensively validate our model
with respect to segmentation accuracy, generalizability, in-session test-retest
reliability, and sensitivity to replicate hypothalamic volume effects (e.g.
sex-differences). The proposed method exhibits high segmentation performance
both for standalone T1w images as well as for T1w/T2w image pairs. Even with
the additional capability to accept flexible inputs, our model matches or
exceeds the performance of state-of-the-art methods with fixed inputs. We,
further, demonstrate the generalizability of our method in experiments with 1.0
mm MR scans from both the Rhineland Study and the UK Biobank. Finally, HypVINN
can perform the segmentation in less than a minute (GPU) and will be available
in the open source FastSurfer neuroimaging software suite, offering a
validated, efficient, and scalable solution for evaluating imaging-derived
phenotypes of the hypothalamus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12737">Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification. (arXiv:2308.12737v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhongyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiangde Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dou Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaoqun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1">Xiaoying Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Meng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Long Jin</a></p>
<p>Convolutional neural networks excel in histopathological image
classification, yet their pixel-level focus hampers explainability. Conversely,
emerging graph convolutional networks spotlight cell-level features and medical
implications. However, limited by their shallowness and suboptimal use of
high-dimensional pixel data, GCNs underperform in multi-class histopathological
image classification. To make full use of pixel-level and cell-level features
dynamically, we propose an asymmetric co-training framework combining a deep
graph convolutional network and a convolutional neural network for multi-class
histopathological image classification. To improve the explainability of the
entire framework by embedding morphological and topological distribution of
cells, we build a 14-layer deep graph convolutional network to handle cell
graph data. For the further utilization and dynamic interactions between
pixel-level and cell-level information, we also design a co-training strategy
to integrate the two asymmetric branches. Notably, we collect a private
clinically acquired dataset termed LUAD7C, including seven subtypes of lung
adenocarcinoma, which is rare and more challenging. We evaluated our approach
on the private LUAD7C and public colorectal cancer datasets, showcasing its
superior performance, explainability, and generalizability in multi-class
histopathological image classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12738">Learning Heavily-Degraded Prior for Underwater Object Detection. (arXiv:2308.12738v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chenping Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jiewen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wanqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Risheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhongxuan Luo</a></p>
<p>Underwater object detection suffers from low detection performance because
the distance and wavelength dependent imaging process yield evident image
quality degradations such as haze-like effects, low visibility, and color
distortions. Therefore, we commit to resolving the issue of underwater object
detection with compounded environmental degradations. Typical approaches
attempt to develop sophisticated deep architecture to generate high-quality
images or features. However, these methods are only work for limited ranges
because imaging factors are either unstable, too sensitive, or compounded.
Unlike these approaches catering for high-quality images or features, this
paper seeks transferable prior knowledge from detector-friendly images. The
prior guides detectors removing degradations that interfere with detection. It
is based on statistical observations that, the heavily degraded regions of
detector-friendly (DFUI) and underwater images have evident feature
distribution gaps while the lightly degraded regions of them overlap each
other. Therefore, we propose a residual feature transference module (RFTM) to
learn a mapping between deep representations of the heavily degraded patches of
DFUI- and underwater- images, and make the mapping as a heavily degraded prior
(HDP) for underwater detection. Since the statistical properties are
independent to image content, HDP can be learned without the supervision of
semantic labels and plugged into popular CNNbased feature extraction networks
to improve their performance on underwater object detection. Without bells and
whistles, evaluations on URPC2020 and UODD show that our methods outperform
CNN-based detectors by a large margin. Our method with higher speeds and less
parameters still performs better than transformer-based detectors. Our code and
DFUI dataset can be found in
https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12757">PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning. (arXiv:2308.12757v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1">Mengya Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Heliang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Han Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yonggang Wen</a></p>
<p>In this work, we address the task of few-shot part segmentation, which aims
to segment the different parts of an unseen object using very few labeled
examples. It is found that leveraging the textual space of a powerful
pre-trained image-language model (such as CLIP) can be beneficial in learning
visual features. Therefore, we develop a novel method termed PartSeg for
few-shot part segmentation based on multimodal learning. Specifically, we
design a part-aware prompt learning method to generate part-specific prompts
that enable the CLIP model to better understand the concept of ``part'' and
fully utilize its textual space. Furthermore, since the concept of the same
part under different object categories is general, we establish relationships
between these parts during the prompt learning process. We conduct extensive
experiments on the PartImageNet and Pascal$\_$Part datasets, and the
experimental results demonstrated that our proposed method achieves
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12761">IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation. (arXiv:2308.12761v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Aung_N/0/1/0/all/0/1">Nyothiri Aung</a>, <a href="http://arxiv.org/find/eess/1/au:+Kechadi_T/0/1/0/all/0/1">Tahar Kechadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1">Liming Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Dhelim_S/0/1/0/all/0/1">Sahraoui Dhelim</a></p>
<p>CNNs have been widely applied for medical image analysis. However, limited
memory capacity is one of the most common drawbacks of processing
high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized
first before processing, which can result in a loss of resolution, increase
class imbalance, and affect the performance of the segmentation algorithms. In
this paper, we propose an end-to-end deep learning approach called IP-UNet.
IP-UNet is a UNet-based model that performs multi-class segmentation on
Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming
3D volumes. IP-UNet uses limited memory capability for training without losing
the original 3D image resolution. We compare the performance of three models in
terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D
segmentation of the CT scan images using a conventional 2D UNet model. 2)
IP-UNet that operates on data obtained by merging the extracted Maximum
Intensity Projection (MIP), Closest Vessel Projection (CVP), and Average
Intensity Projection (AvgIP) representations of the source 3D volumes, then
applying the UNet model on the output IP images. 3) 3D-UNet model directly
reads the 3D volumes constructed from a series of CT scan images and outputs
the 3D volume of the predicted segmentation. We test the performance of these
methods on 3D volumetric images for automatic breast calcification detection.
Experimental results show that IP-Unet can achieve similar segmentation
accuracy with 3D-Unet but with much better performance. It reduces the training
time by 70\% and memory consumption by 92\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12774">LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition. (arXiv:2308.12774v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Changxu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Da_C/0/1/0/all/0/1">Cheng Da</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1">Cong Yao</a></p>
<p>The diversity in length constitutes a significant characteristic of text. Due
to the long-tail distribution of text lengths, most existing methods for scene
text recognition (STR) only work well on short or seen-length text, lacking the
capability of recognizing longer text or performing length extrapolation. This
is a crucial issue, since the lengths of the text to be recognized are usually
not given in advance in real-world applications, but it has not been adequately
investigated in previous works. Therefore, we propose in this paper a method
called Length-Insensitive Scene TExt Recognizer (LISTER), which remedies the
limitation regarding the robustness to various text lengths. Specifically, a
Neighbor Decoder is proposed to obtain accurate character attention maps with
the assistance of a novel neighbor matrix regardless of the text lengths.
Besides, a Feature Enhancement Module is devised to model the long-range
dependency with low computation cost, which is able to perform iterations with
the neighbor decoder to enhance the feature map progressively. To the best of
our knowledge, we are the first to achieve effective length-insensitive scene
text recognition. Extensive experiments demonstrate that the proposed LISTER
algorithm exhibits obvious superiority on long text recognition and the ability
for length extrapolation, while comparing favourably with the previous
state-of-the-art methods on standard benchmarks for STR (mainly short text).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12779">On Offline Evaluation of 3D Object Detection for Autonomous Driving. (arXiv:2308.12779v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schreier_T/0/1/0/all/0/1">Tim Schreier</a>, <a href="http://arxiv.org/find/cs/1/au:+Renz_K/0/1/0/all/0/1">Katrin Renz</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1">Andreas Geiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1">Kashyap Chitta</a></p>
<p>Prior work in 3D object detection evaluates models using offline metrics like
average precision since closed-loop online evaluation on the downstream driving
task is costly. However, it is unclear how indicative offline results are of
driving performance. In this work, we perform the first empirical evaluation
measuring how predictive different detection metrics are of driving performance
when detectors are integrated into a full self-driving stack. We conduct
extensive experiments on urban driving in the CARLA simulator using 16 object
detection models. We find that the nuScenes Detection Score has a higher
correlation to driving performance than the widely used average precision
metric. In addition, our results call for caution on the exclusive reliance on
the emerging class of `planner-centric' metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12789">Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference. (arXiv:2308.12789v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Reyes_I/0/1/0/all/0/1">Ian Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1">Homa Alemzadeh</a></p>
<p>Surgical context inference has recently garnered significant attention in
robot-assisted surgery as it can facilitate workflow analysis, skill
assessment, and error detection. However, runtime context inference is
challenging since it requires timely and accurate detection of the interactions
among the tools and objects in the surgical scene based on the segmentation of
video data. On the other hand, existing state-of-the-art video segmentation
methods are often biased against infrequent classes and fail to provide
temporal consistency for segmented masks. This can negatively impact the
context inference and accurate detection of critical states. In this study, we
propose a solution to these challenges using a Space Time Correspondence
Network (STCN). STCN is a memory network that performs binary segmentation and
minimizes the effects of class imbalance. The use of a memory bank in STCN
allows for the utilization of past image and segmentation information, thereby
ensuring consistency of the masks. Our experiments using the publicly available
JIGSAWS dataset demonstrate that STCN achieves superior segmentation
performance for objects that are difficult to segment, such as needle and
thread, and improves context inference compared to the state-of-the-art. We
also demonstrate that segmentation and context inference can be performed at
runtime without compromising performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12817">MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild. (arXiv:2308.12817v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yu-Xiang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1">Jun-Wei Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Ching Chang</a></p>
<p>Detecting small scene text instances in the wild is particularly challenging,
where the influence of irregular positions and nonideal lighting often leads to
detection errors. We present MixNet, a hybrid architecture that combines the
strengths of CNNs and Transformers, capable of accurately detecting small text
from challenging natural scenes, regardless of the orientations, styles, and
lighting conditions. MixNet incorporates two key modules: (1) the Feature
Shuffle Network (FSNet) to serve as the backbone and (2) the Central
Transformer Block (CTBlock) to exploit the 1D manifold constraint of the scene
text. We first introduce a novel feature shuffling strategy in FSNet to
facilitate the exchange of features across multiple scales, generating
high-resolution features superior to popular ResNet and HRNet. The FSNet
backbone has achieved significant improvements over many existing text
detection methods, including PAN, DB, and FAST. Then we design a complementary
CTBlock to leverage center line based features similar to the medial axis of
text regions and show that it can outperform contour-based approaches in
challenging cases when small scene texts appear closely. Extensive experimental
results show that MixNet, which mixes FSNet with CTBlock, achieves
state-of-the-art results on multiple scene text detection datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12831">EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting. (arXiv:2308.12831v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zitao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1">Qiguang Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1">Yue Xi</a></p>
<p>The portrait matting task aims to extract an alpha matte with complete
semantics and finely-detailed contours. In comparison to CNN-based approaches,
transformers with self-attention allow a larger receptive field, enabling it to
better capture long-range dependencies and low-frequency semantic information
of a portrait. However, the recent research shows that self-attention mechanism
struggle with modeling high-frequency information and capturing fine contour
details, which can lead to bias while predicting the portrait's contours. To
address the problem, we propose EFormer to enhance the model's attention
towards semantic and contour features. Especially the latter, which is
surrounded by a large amount of high-frequency details. We build a semantic and
contour detector (SCD) to accurately capture the distribution of semantic and
contour features. And we further design contour-edge extraction branch and
semantic extraction branch for refining contour features and complete semantic
information. Finally, we fuse the two kinds of features and leverage the
segmentation head to generate the predicted portrait matte. Remarkably, EFormer
is an end-to-end trimap-free method and boasts a simple structure. Experiments
conducted on VideoMatte240K-JPEGSD and AIM datasets demonstrate that EFormer
outperforms previous portrait matte methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12840">FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease. (arXiv:2308.12840v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mohamed R. Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1">Terry Lyons</a></p>
<p>Through our respiratory system, many viruses and diseases frequently spread
and pass from one person to another. Covid-19 served as an example of how
crucial it is to track down and cut back on contacts to stop its spread. There
is a clear gap in finding automatic methods that can detect hand-to-face
contact in complex urban scenes or indoors. In this paper, we introduce a
computer vision framework, called FaceTouch, based on deep learning. It
comprises deep sub-models to detect humans and analyse their actions. FaceTouch
seeks to detect hand-to-face touches in the wild, such as through video chats,
bus footage, or CCTV feeds. Despite partial occlusion of faces, the introduced
system learns to detect face touches from the RGB representation of a given
scene by utilising the representation of the body gestures such as arm
movement. This has been demonstrated to be useful in complex urban scenarios
beyond simply identifying hand movement and its closeness to faces. Relying on
Supervised Contrastive Learning, the introduced model is trained on our
collected dataset, given the absence of other benchmark datasets. The framework
shows a strong validation in unseen datasets which opens the door for potential
deployment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12845">Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance. (arXiv:2308.12845v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haobo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shuo Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jin Xie</a></p>
<p>Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12861">Learned Local Attention Maps for Synthesising Vessel Segmentations. (arXiv:2308.12861v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Deo_Y/0/1/0/all/0/1">Yash Deo</a>, <a href="http://arxiv.org/find/eess/1/au:+Bonazzola_R/0/1/0/all/0/1">Rodrigo Bonazzola</a>, <a href="http://arxiv.org/find/eess/1/au:+Dou_H/0/1/0/all/0/1">Haoran Dou</a>, <a href="http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_T/0/1/0/all/0/1">Tianyou Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1">Nishant Ravikumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1">Alejandro F. Frangi</a>, <a href="http://arxiv.org/find/eess/1/au:+Lassila_T/0/1/0/all/0/1">Toni Lassila</a></p>
<p>Magnetic resonance angiography (MRA) is an imaging modality for visualising
blood vessels. It is useful for several diagnostic applications and for
assessing the risk of adverse events such as haemorrhagic stroke (resulting
from the rupture of aneurysms in blood vessels). However, MRAs are not acquired
routinely, hence, an approach to synthesise blood vessel segmentations from
more routinely acquired MR contrasts such as T1 and T2, would be useful. We
present an encoder-decoder model for synthesising segmentations of the main
cerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose a
two-phase multi-objective learning approach, which captures both global and
local features. It uses learned local attention maps generated by dilating the
segmentation labels, which forces the network to only extract information from
the T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentations
generated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ in
testing, compared to state-of-the-art segmentation networks such as transformer
U-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only a
fraction of the parameters. The main qualitative difference between our
synthetic vessel segmentations and the comparative models was in the sharper
resolution of the CoW vessel segments, especially in the posterior circulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12863">SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection. (arXiv:2308.12863v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1">Dafeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huaping Liu</a></p>
<p>Multi-modal fusion is increasingly being used for autonomous driving tasks,
as images from different modalities provide unique information for feature
extraction. However, the existing two-stream networks are only fused at a
specific network layer, which requires a lot of manual attempts to set up. As
the CNN goes deeper, the two modal features become more and more advanced and
abstract, and the fusion occurs at the feature level with a large gap, which
can easily hurt the performance. In this study, we propose a novel fusion
architecture called skip-cross networks (SkipcrossNets), which combines
adaptively LiDAR point clouds and camera images without being bound to a
certain fusion epoch. Specifically, skip-cross connects each layer to each
layer in a feed-forward manner, and for each layer, the feature maps of all
previous layers are used as input and its own feature maps are used as input to
all subsequent layers for the other modality, enhancing feature propagation and
multi-modal features fusion. This strategy facilitates selection of the most
similar feature layers from two data pipelines, providing a complementary
effect for sparse point cloud features during fusion processes. The network is
also divided into several blocks to reduce the complexity of feature fusion and
the number of model parameters. The advantages of skip-cross fusion were
demonstrated through application to the KITTI and A2D2 datasets, achieving a
MaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The model
parameters required only 2.33 MB of memory at a speed of 68.24 FPS, which could
be viable for mobile terminals and embedded devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12866">ToonTalker: Cross-Domain Face Reenactment. (arXiv:2308.12866v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yuan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1">Xiaodong Cun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Fei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yanbo Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Baoyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>We target cross-domain face reenactment in this paper, i.e., driving a
cartoon image with the video of a real person and vice versa. Recently, many
works have focused on one-shot talking face generation to drive a portrait with
a real video, i.e., within-domain reenactment. Straightforwardly applying those
methods to cross-domain animation will cause inaccurate expression transfer,
blur effects, and even apparent artifacts due to the domain shift between
cartoon and real faces. Only a few works attempt to settle cross-domain face
reenactment. The most related work AnimeCeleb requires constructing a dataset
with pose vector and cartoon image pairs by animating 3D characters, which
makes it inapplicable anymore if no paired data is available. In this paper, we
propose a novel method for cross-domain reenactment without paired data.
Specifically, we propose a transformer-based framework to align the motions
from different domains into a common latent space where motion transfer is
conducted via latent code addition. Two domain-specific motion encoders and two
learnable motion base memories are used to capture domain properties. A source
query transformer and a driving one are exploited to project domain-specific
motion to the canonical space. The edited motion is projected back to the
domain of the source with a transformer. Moreover, since no paired data is
provided, we propose a novel cross-domain training scheme using data from two
domains with the designed analogy constraint. Besides, we contribute a cartoon
dataset in Disney style. Extensive evaluations demonstrate the superiority of
our method over competing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12870">VNI-Net: Vector Neurons-based Rotation-Invariant Descriptor for LiDAR Place Recognition. (arXiv:2308.12870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_G/0/1/0/all/0/1">Gengxuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junqiao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yingfeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fenglin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1">Wenjie Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Chen Ye</a></p>
<p>LiDAR-based place recognition plays a crucial role in Simultaneous
Localization and Mapping (SLAM) and LiDAR localization.
</p>
<p>Despite the emergence of various deep learning-based and hand-crafting-based
methods, rotation-induced place recognition failure remains a critical
challenge.
</p>
<p>Existing studies address this limitation through specific training strategies
or network structures.
</p>
<p>However, the former does not produce satisfactory results, while the latter
focuses mainly on the reduced problem of SO(2) rotation invariance. Methods
targeting SO(3) rotation invariance suffer from limitations in discrimination
capability.
</p>
<p>In this paper, we propose a new method that employs Vector Neurons Network
(VNN) to achieve SO(3) rotation invariance.
</p>
<p>We first extract rotation-equivariant features from neighboring points and
map low-dimensional features to a high-dimensional space through VNN.
</p>
<p>Afterwards, we calculate the Euclidean and Cosine distance in the
rotation-equivariant feature space as rotation-invariant feature descriptors.
</p>
<p>Finally, we aggregate the features using GeM pooling to obtain global
descriptors.
</p>
<p>To address the significant information loss when formulating
rotation-invariant descriptors, we propose computing distances between features
at different layers within the Euclidean space neighborhood.
</p>
<p>This greatly improves the discriminability of the point cloud descriptors
while ensuring computational efficiency.
</p>
<p>Experimental results on public datasets show that our approach significantly
outperforms other baseline methods implementing rotation invariance, while
achieving comparable results with current state-of-the-art place recognition
methods that do not consider rotation issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12880">Multi-stage feature decorrelation constraints for improving CNN classification performance. (arXiv:2308.12880v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qiuyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zu_X/0/1/0/all/0/1">Xuewen Zu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chengfei Liu</a></p>
<p>For the convolutional neural network (CNN) used for pattern classification,
the training loss function is usually applied to the final output of the
network, except for some regularization constraints on the network parameters.
However, with the increasing of the number of network layers, the influence of
the loss function on the network front layers gradually decreases, and the
network parameters tend to fall into local optimization. At the same time, it
is found that the trained network has significant information redundancy at all
stages of features, which reduces the effectiveness of feature mapping at all
stages and is not conducive to the change of the subsequent parameters of the
network in the direction of optimality. Therefore, it is possible to obtain a
more optimized solution of the network and further improve the classification
accuracy of the network by designing a loss function for restraining the front
stage features and eliminating the information redundancy of the front stage
features .For CNN, this article proposes a multi-stage feature decorrelation
loss (MFD Loss), which refines effective features and eliminates information
redundancy by constraining the correlation of features at all stages.
Considering that there are many layers in CNN, through experimental comparison
and analysis, MFD Loss acts on multiple front layers of CNN, constrains the
output features of each layer and each channel, and performs supervision
training jointly with classification loss function during network training.
Compared with the single Softmax Loss supervised learning, the experiments on
several commonly used datasets on several typical CNNs prove that the
classification performance of Softmax Loss+MFD Loss is significantly better.
Meanwhile, the comparison experiments before and after the combination of MFD
Loss and some other typical loss functions verify its good universality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12894">Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings. (arXiv:2308.12894v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuanjian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Quan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zengchang Qin</a></p>
<p>Semantic segmentation is a computer vision task that associates a label with
each pixel in an image. Modern approaches tend to introduce class embeddings
into semantic segmentation for deeply utilizing category semantics, and regard
supervised class masks as final predictions. In this paper, we explore the
mechanism of class embeddings and have an insight that more explicit and
meaningful class embeddings can be generated based on class masks purposely.
Following this observation, we propose ECENet, a new segmentation paradigm, in
which class embeddings are obtained and enhanced explicitly during interacting
with multi-stage image features. Based on this, we revisit the traditional
decoding process and explore inverted information flow between segmentation
masks and class embeddings. Furthermore, to ensure the discriminability and
informativity of features from backbone, we propose a Feature Reconstruction
module, which combines intrinsic and diverse branches together to ensure the
concurrence of diversity and redundancy in features. Experiments show that our
ECENet outperforms its counterparts on the ADE20K dataset with much less
computational cost and achieves new state-of-the-art results on PASCAL-Context
dataset. The code will be released at https://gitee.com/mindspore/models and
https://github.com/Carol-lyh/ECENet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12896">Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Landeghem_J/0/1/0/all/0/1">Jordy Van Landeghem</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Sanket Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew B. Blaschko</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12898">Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1">Jun Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Ye Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Changxing Ding</a></p>
<p>The multimedia community has shown a significant interest in perceiving and
representing the physical world with multimodal pretrained neural network
models, and among them, the visual-language pertaining (VLP) is, currently, the
most captivating topic. However, there have been few endeavors dedicated to the
exploration of 1) whether essential linguistic knowledge (e.g., semantics and
syntax) can be extracted during VLP, and 2) how such linguistic knowledge
impact or enhance the multimodal alignment. In response, here we aim to
elucidate the impact of comprehensive linguistic knowledge, including semantic
expression and syntactic structure, on multimodal alignment. Specifically, we
design and release the SNARE, the first large-scale multimodal alignment
probing benchmark, to detect the vital linguistic components, e.g., lexical,
semantic, and syntax knowledge, containing four tasks: Semantic structure,
Negation logic, Attribute ownership, and Relationship composition. Based on our
proposed probing benchmarks, our holistic analyses of five advanced VLP models
illustrate that the VLP model: i) shows insensitivity towards complex syntax
structures and relies on content words for sentence comprehension; ii)
demonstrates limited comprehension of combinations between sentences and
negations; iii) faces challenges in determining the presence of actions or
spatial relationships within visual information and struggles with verifying
the correctness of triple combinations. We make our benchmark and code
available at \url{https://github.com/WangFei-2019/SNARE/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12902">CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shakibania_H/0/1/0/all/0/1">Hossein Shakibania</a>, <a href="http://arxiv.org/find/cs/1/au:+Raoufi_S/0/1/0/all/0/1">Sina Raoufi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khotanlou_H/0/1/0/all/0/1">Hassan Khotanlou</a></p>
<p>Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12910">SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data. (arXiv:2308.12910v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziyan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1">Kushal Kafle</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhe Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Scott Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zhihong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1">Vicente Ordonez</a></p>
<p>We propose Subject-Conditional Relation Detection SCoRD, where conditioned on
an input subject, the goal is to predict all its relations to other objects in
a scene along with their locations. Based on the Open Images dataset, we
propose a challenging OIv6-SCoRD benchmark such that the training and testing
splits have a distribution shift in terms of the occurrence statistics of
$\langle$subject, relation, object$\rangle$ triplets. To solve this problem, we
propose an auto-regressive model that given a subject, it predicts its
relations, objects, and object locations by casting this output as a sequence
of tokens. First, we show that previous scene-graph prediction methods fail to
produce as exhaustive an enumeration of relation-object pairs when conditioned
on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for
our relation-object predictions compared to the 49.75% obtained by a recent
scene graph detector. Then, we show improved generalization on both
relation-object and object-box predictions by leveraging during training
relation-object pairs obtained automatically from textual captions and for
which no object-box annotations are available. Particularly, for
$\langle$subject, relation, object$\rangle$ triplets for which no object
locations are available during training, we are able to obtain a recall@3 of
42.59% for relation-object pairs and 32.27% for their box locations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12914">Robot Pose Nowcasting: Forecast the Future to Improve the Present. (arXiv:2308.12914v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simoni_A/0/1/0/all/0/1">Alessandro Simoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchetti_F/0/1/0/all/0/1">Francesco Marchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1">Guido Borghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1">Federico Becattini</a>, <a href="http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1">Lorenzo Seidenari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vezzani_R/0/1/0/all/0/1">Roberto Vezzani</a>, <a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1">Alberto Del Bimbo</a></p>
<p>In recent years, the effective and safe collaboration between humans and
machines has gained significant importance, particularly in the Industry 4.0
scenario. A critical prerequisite for realizing this collaborative paradigm is
precisely understanding the robot's 3D pose within its environment. Therefore,
in this paper, we introduce a novel vision-based system leveraging depth data
to accurately establish the 3D locations of robotic joints. Specifically, we
prove the ability of the proposed system to enhance its current pose estimation
accuracy by jointly learning to forecast future poses. Indeed, we introduce the
concept of Pose Nowcasting, denoting the capability of a system to exploit the
learned knowledge of the future to improve the estimation of the present. The
experimental evaluation is conducted on two different datasets, providing
state-of-the-art and real-time performance and confirming the validity of the
proposed method on both the robotic and human scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12919">Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1">Lijun Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a></p>
<p>The emergence of vision-language models (VLMs), such as CLIP, has spurred a
significant research effort towards their application for downstream supervised
learning tasks. Although some previous studies have explored the unsupervised
fine-tuning of CLIP, they often rely on prior knowledge in the form of class
names associated with ground truth labels. In this paper, we delve into a
realistic unsupervised fine-tuning scenario by assuming that the unlabeled data
might contain out-of-distribution samples from unknown classes. Furthermore, we
emphasize the importance of simultaneously enhancing out-of-distribution
detection capabilities alongside the recognition of instances associated with
predefined class labels.
</p>
<p>To tackle this problem, we present a simple, efficient, and effective
fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages
sample-level confidence to approximately minimize the conditional entropy of
confident instances and maximize the marginal entropy of less confident
instances. Apart from optimizing the textual prompts, UEO also incorporates
optimization of channel-wise affine transformations within the visual branch of
CLIP. Through extensive experiments conducted across 15 domains and 4 different
types of prior knowledge, we demonstrate that UEO surpasses baseline methods in
terms of both generalization and out-of-distribution detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12937">Panoptic-Depth Color Map for Combination of Depth and Image Segmentation. (arXiv:2308.12937v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jia-Quan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1">Soo-Chang Pei</a></p>
<p>Image segmentation and depth estimation are crucial tasks in computer vision,
especially in autonomous driving scenarios. Although these tasks are typically
addressed separately, we propose an innovative approach to combine them in our
novel deep learning network, Panoptic-DepthLab. By incorporating an additional
depth estimation branch into the segmentation network, it can predict the depth
of each instance segment. Evaluating on Cityscape dataset, we demonstrate the
effectiveness of our method in achieving high-quality segmentation results with
depth and visualize it with a color map. Our proposed method demonstrates a new
possibility of combining different tasks and networks to generate a more
comprehensive image recognition result to facilitate the safety of autonomous
driving vehicles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12938">Perspective-aware Convolution for Monocular 3D Object Detection. (arXiv:2308.12938v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jia-Quan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1">Soo-Chang Pei</a></p>
<p>Monocular 3D object detection is a crucial and challenging task for
autonomous driving vehicle, while it uses only a single camera image to infer
3D objects in the scene. To address the difficulty of predicting depth using
only pictorial clue, we propose a novel perspective-aware convolutional layer
that captures long-range dependencies in images. By enforcing convolutional
kernels to extract features along the depth axis of every image pixel, we
incorporates perspective information into network architecture. We integrate
our perspective-aware convolutional layer into a 3D object detector and
demonstrate improved performance on the KITTI3D dataset, achieving a 23.9\%
average precision in the easy benchmark. These results underscore the
importance of modeling scene clues for accurate depth inference and highlight
the benefits of incorporating scene structure in network design. Our
perspective-aware convolutional layer has the potential to enhance object
detection accuracy by providing more precise and context-aware feature
extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12949">Label Budget Allocation in Multi-Task Learning. (arXiv:2308.12949v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Ximeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1">Kate Saenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Mellina_C/0/1/0/all/0/1">Clayton Mellina</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1">Xiao Bian</a></p>
<p>The cost of labeling data often limits the performance of machine learning
systems. In multi-task learning, related tasks provide information to each
other and improve overall performance, but the label cost can vary among tasks.
How should the label budget (i.e. the amount of money spent on labeling) be
allocated among different tasks to achieve optimal multi-task performance? We
are the first to propose and formally define the label budget allocation
problem in multi-task learning and to empirically show that different budget
allocation strategies make a big difference to its performance. We propose a
Task-Adaptive Budget Allocation algorithm to robustly generate the optimal
budget allocation adaptive to different multi-task learning settings.
Specifically, we estimate and then maximize the extent of new information
obtained from the allocated budget as a proxy for multi-task learning
performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy
of our approach over other widely used heuristic labeling strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12956">DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1">Huafeng Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiawu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xuefeng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Min Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>Vision-Language Pre-training (VLP) shows remarkable progress with the
assistance of extremely heavy parameters, which challenges deployment in real
applications. Knowledge distillation is well recognized as the essential
procedure in model compression. However, existing knowledge distillation
techniques lack an in-depth investigation and analysis of VLP, and practical
guidelines for VLP-oriented distillation are still not yet explored. In this
paper, we present DLIP, a simple yet efficient Distilling Language-Image
Pre-training framework, through which we investigate how to distill a light VLP
model. Specifically, we dissect the model distillation from multiple
dimensions, such as the architecture characteristics of different modules and
the information transfer of different modalities. We conduct comprehensive
experiments and provide insights on distilling a light but performant VLP
model. Experimental results reveal that DLIP can achieve a state-of-the-art
accuracy/efficiency trade-off across diverse cross-modal tasks, e.g.,
image-text retrieval, image captioning and visual question answering. For
example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while
achieving comparable or better performance. Furthermore, DLIP succeeds in
retaining more than 95% of the performance with 22.4% parameters and 24.8%
FLOPs compared to the teacher model and accelerates inference speed by 2.7x.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12960">Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment. (arXiv:2308.12960v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhiqiang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Khan</a></p>
<p>Large-scale pre-trained Vision Language Models (VLMs) have proven effective
for zero-shot classification. Despite the success, most traditional VLMs-based
methods are restricted by the assumption of partial source supervision or ideal
vocabularies, which rarely satisfy the open-world scenario. In this paper, we
aim at a more challenging setting, Realistic Zero-Shot Classification, which
assumes no annotation but instead a broad vocabulary. To address this
challenge, we propose the Self Structural Semantic Alignment (S^3A) framework,
which extracts the structural semantic information from unlabeled data while
simultaneously self-learning. Our S^3A framework adopts a unique
Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups
unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR
process includes iterative clustering on images, voting within each cluster to
identify initial class candidates from the vocabulary, generating
discriminative prompts with large language models to discern confusing
candidates, and realigning images and the vocabulary as structural semantic
alignment. Finally, we propose to self-learn the CLIP image encoder with both
individual and structural semantic alignment through a teacher-student learning
strategy. Our comprehensive experiments across various generic and fine-grained
benchmarks demonstrate that the S^3A method offers substantial improvements
over existing VLMs-based approaches, achieving a more than 15% accuracy
improvement over CLIP on average. Our codes, models, and prompts are publicly
released at https://github.com/sheng-eatamath/S3A.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12961">Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks. (arXiv:2308.12961v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangyang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bowei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a></p>
<p>To reduce the reliance on large-scale datasets, recent works in 3D
segmentation resort to few-shot learning. Current 3D few-shot semantic
segmentation methods first pre-train the models on `seen' classes, and then
evaluate their generalization performance on `unseen' classes. However, the
prior pre-training stage not only introduces excessive time overhead, but also
incurs a significant domain gap on `unseen' classes. To tackle these issues, we
propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and
a further training-based variant, TFS3D-T. Without any learnable parameters,
TFS3D extracts dense representations by trigonometric positional encodings, and
achieves comparable performance to previous training-based methods. Due to the
elimination of pre-training, TFS3D can alleviate the domain gap issue and save
a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to
train a lightweight query-support transferring attention (QUEST), which
enhances the interaction between the few-shot query and support data.
Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by
+6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the
training time by -90%, indicating superior effectiveness and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12962">Motion-Guided Masking for Spatiotemporal Representation Learning. (arXiv:2308.12962v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">David Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1">Shuai Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhat_V/0/1/0/all/0/1">Vimal Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_Villalobos_H/0/1/0/all/0/1">Hector Santos-Villalobos</a>, <a href="http://arxiv.org/find/cs/1/au:+MV_R/0/1/0/all/0/1">Rohith MV</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinyu Li</a></p>
<p>Several recent works have directly extended the image masked autoencoder
(MAE) with random masking into video domain, achieving promising results.
However, unlike images, both spatial and temporal information are important for
video understanding. This suggests that the random masking strategy that is
inherited from the image MAE is less effective for video MAE. This motivates
the design of a novel masking algorithm that can more efficiently make use of
video saliency. Specifically, we propose a motion-guided masking algorithm
(MGM) which leverages motion vectors to guide the position of each mask over
time. Crucially, these motion-based correspondences can be directly obtained
from information stored in the compressed format of the video, which makes our
method efficient and scalable. On two challenging large-scale video benchmarks
(Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and
achieve up to +$1.3\%$ improvement compared to previous state-of-the-art
methods. Additionally, our MGM achieves equivalent performance to previous
video MAE using up to $66\%$ fewer training epochs. Lastly, we show that MGM
generalizes better to downstream transfer learning and domain adaptation tasks
on the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$
improvement compared to baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12963">MapPrior: Bird&#x27;s-Eye View Map Layout Estimation with Generative Models. (arXiv:2308.12963v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiyue Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zyrianov_V/0/1/0/all/0/1">Vlas Zyrianov</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhijian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shenlong Wang</a></p>
<p>Despite tremendous advancements in bird's-eye view (BEV) perception, existing
models fall short in generating realistic and coherent semantic map layouts,
and they fail to account for uncertainties arising from partial sensor
information (such as occlusion or limited coverage). In this work, we introduce
MapPrior, a novel BEV perception framework that combines a traditional
discriminative BEV perception model with a learned generative model for
semantic map layouts. Our MapPrior delivers predictions with better accuracy,
realism, and uncertainty awareness. We evaluate our model on the large-scale
nuScenes benchmark. At the time of submission, MapPrior outperforms the
strongest competing method, with significantly improved MMD and ECE scores in
camera- and LiDAR-based BEV perception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12964">Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunji Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jiyoung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin-Hwa Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1">Jung-Woo Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun-Yan Zhu</a></p>
<p>Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12965">POCO: 3D Pose and Shape Estimation with Confidence. (arXiv:2308.12965v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1">Sai Kumar Dwivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1">Hongwei Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1">Dimitrios Tzionas</a></p>
<p>The regression of 3D Human Pose and Shape (HPS) from an image is becoming
increasingly accurate. This makes the results useful for downstream tasks like
human action recognition or 3D graphics. Yet, no regressor is perfect, and
accuracy can be affected by ambiguous image evidence or by poses and appearance
that are unseen during training. Most current HPS regressors, however, do not
report the confidence of their outputs, meaning that downstream tasks cannot
differentiate accurate estimates from inaccurate ones. To address this, we
develop POCO, a novel framework for training HPS regressors to estimate not
only a 3D human body, but also their confidence, in a single feed-forward pass.
Specifically, POCO estimates both the 3D body pose and a per-sample variance.
The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing
uncertainty that is highly correlated to pose reconstruction quality. The POCO
framework can be applied to any HPS regressor and here we evaluate it by
modifying HMR, PARE, and CLIFF. In all cases, training the network to reason
about uncertainty helps it learn to more accurately estimate 3D pose. While
this was not our goal, the improvement is modest but consistent. Our main
motivation is to provide uncertainty estimates for downstream tasks; we
demonstrate this in two ways: (1) We use the confidence estimates to bootstrap
HPS training. Given unlabelled image data, we take the confident estimates of a
POCO-trained regressor as pseudo ground truth. Retraining with this
automatically-curated data improves accuracy. (2) We exploit uncertainty in
video pose estimation by automatically identifying uncertain frames (e.g. due
to occlusion) and inpainting these from confident frames. Code and models will
be available for research at https://poco.is.tue.mpg.de.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12966">Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jinze Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Shuai Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shusheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Sinan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a></p>
<p>We introduce the Qwen-VL series, a set of large-scale vision-language models
designed to perceive and understand both text and images. Comprising Qwen-VL
and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like
image captioning, question answering, visual localization, and flexible
interaction. The evaluation covers a wide range of tasks including zero-shot
captioning, visual or document visual question answering, and grounding. We
demonstrate the Qwen-VL outperforms existing Large Vision Language Models
(LVLMs). We present their architecture, training, capabilities, and
performance, highlighting their contributions to advancing multimodal
artificial intelligence. Code, demo and models are available at
https://github.com/QwenLM/Qwen-VL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12967">NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes. (arXiv:2308.12967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1">Muhammad Zubair Irshad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1">Sergey Zakharov</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Katherine Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1">Vitor Guizilini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1">Thomas Kollar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1">Adrien Gaidon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1">Zsolt Kira</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1">Rares Ambrus</a></p>
<p>Recent implicit neural representations have shown great results for novel
view synthesis. However, existing methods require expensive per-scene
optimization from many views hence limiting their application to real-world
unbounded urban settings where the objects of interest or backgrounds are
observed from very few views. To mitigate this challenge, we introduce a new
approach called NeO 360, Neural fields for sparse view synthesis of outdoor
scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes
from a single or a few posed RGB images. The essence of our approach is in
capturing the distribution of complex real-world outdoor 3D scenes and using a
hybrid image-conditional triplanar representation that can be queried from any
world point. Our representation combines the best of both voxel-based and
bird's-eye-view (BEV) representations and is more effective and expressive than
each. NeO 360's representation allows us to learn from a large collection of
unbounded 3D scenes while offering generalizability to new views and novel
scenes from as few as a single image during inference. We demonstrate our
approach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS
360, and show that NeO 360 outperforms state-of-the-art generalizable methods
for novel view synthesis while also offering editing and composition
capabilities. Project page:
https://zubair-irshad.github.io/projects/neo360.html
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12968">Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuxin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>Automatic high-quality rendering of anime scenes from complex real-world
images is of significant practical value. The challenges of this task lie in
the complexity of the scenes, the unique features of anime style, and the lack
of high-quality datasets to bridge the domain gap. Despite promising attempts,
previous efforts are still incompetent in achieving satisfactory results with
consistent semantic preservation, evident stylization, and fine details. In
this study, we propose Scenimefy, a novel semi-supervised image-to-image
translation framework that addresses these challenges. Our approach guides the
learning with structure-consistent pseudo paired data, simplifying the pure
unsupervised setting. The pseudo data are derived uniquely from a
semantic-constrained StyleGAN leveraging rich model priors like CLIP. We
further apply segmentation-guided data selection to obtain high-quality pseudo
supervision. A patch-wise contrastive style loss is introduced to improve
stylization and fine details. Besides, we contribute a high-resolution anime
scene dataset to facilitate future research. Our extensive experiments
demonstrate the superiority of our method over state-of-the-art baselines in
terms of both perceptual quality and quantitative performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12969">ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors. (arXiv:2308.12969v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wanyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1">Rishabh Dabral</a>, <a href="http://arxiv.org/find/cs/1/au:+Leimkuhler_T/0/1/0/all/0/1">Thomas Leimk&#xfc;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1">Marc Habermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a></p>
<p>Existing automatic approaches for 3D virtual character motion synthesis
supporting scene interactions do not generalise well to new objects outside
training distributions, even when trained on extensive motion capture datasets
with diverse objects and annotated interactions. This paper addresses this
limitation and shows that robustness and generalisation to novel scene objects
in 3D object-aware character synthesis can be achieved by training a motion
model with as few as one reference object. We leverage an implicit feature
representation trained on object-only datasets, which encodes an
SE(3)-equivariant descriptor field around the object. Given an unseen object
and a reference pose-object pair, we optimise for the object-aware pose that is
closest in the feature space to the reference pose. Finally, we use l-NSM,
i.e., our motion generation model that is trained to seamlessly transition from
locomotion to object interaction with the proposed bidirectional pose blending
scheme. Through comprehensive numerical comparisons to state-of-the-art methods
and in a user study, we demonstrate substantial improvements in 3D virtual
character motion and interaction quality and robustness to scenarios with
unseen objects. Our project page is available at
https://vcai.mpi-inf.mpg.de/projects/ROAM/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.08790">P1AC: Revisiting Absolute Pose From a Single Affine Correspondence. (arXiv:2011.08790v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1">Jonathan Ventura</a>, <a href="http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1">Zuzana Kukelova</a>, <a href="http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1">Torsten Sattler</a>, <a href="http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1">D&#xe1;niel Bar&#xe1;th</a></p>
<p>Affine correspondences have traditionally been used to improve feature
matching over wide baselines. While recent work has successfully used affine
correspondences to solve various relative camera pose estimation problems, less
attention has been given to their use in absolute pose estimation. We introduce
the first general solution to the problem of estimating the pose of a
calibrated camera given a single observation of an oriented point and an affine
correspondence. The advantage of our approach (P1AC) is that it requires only a
single correspondence, in comparison to the traditional point-based approach
(P3P), significantly reducing the combinatorics in robust estimation. P1AC
provides a general solution that removes restrictive assumptions made in prior
work and is applicable to large-scale image-based localization. We propose a
minimal solution to the P1AC problem and evaluate our novel solver on synthetic
data, showing its numerical stability and performance under various types of
noise. On standard image-based localization benchmarks we show that P1AC
achieves more accurate results than the widely used P3P algorithm. Code for our
method is available at https://github.com/jonathanventura/P1AC/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.10419">Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings. (arXiv:2107.10419v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuesong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1">Meihao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1">Jing Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiebo Luo</a></p>
<p>Contrastive self-supervised learning (SSL) methods, such as MoCo and SimCLR,
have achieved great success in unsupervised visual representation learning.
They rely on a large number of negative pairs and thus require either large
memory banks or large batches. Some recent non-contrastive SSL methods, such as
BYOL and SimSiam, attempt to discard negative pairs and have also shown
remarkable performance. To avoid collapsed solutions caused by not using
negative pairs, these methods require non-trivial asymmetry designs. However,
in small data regimes, we can not obtain a sufficient number of negative pairs
or effectively avoid the over-fitting problem when negatives are not used at
all. To address this situation, we argue that negative pairs are still
important but one is generally sufficient for each positive pair. We show that
a simple Triplet-based loss (Trip) can achieve surprisingly good performance
without requiring large batches or asymmetry designs. Moreover, to alleviate
the over-fitting problem in small data regimes and further enhance the effect
of Trip, we propose a simple plug-and-play RandOm MApping (ROMA) strategy by
randomly mapping samples into other spaces and requiring these randomly
projected samples to satisfy the same relationship indicated by the triplets.
Integrating the triplet-based loss with random mapping, we obtain the proposed
method Trip-ROMA. Extensive experiments, including unsupervised representation
learning and unsupervised few-shot learning, have been conducted on ImageNet-1K
and seven small datasets. They successfully demonstrate the effectiveness of
Trip-ROMA and consistently show that ROMA can further effectively boost other
SSL methods. Code is available at https://github.com/WenbinLee/Trip-ROMA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.05742">HCDG: A Hierarchical Consistency Framework for Domain Generalization on Medical Image Segmentation. (arXiv:2109.05742v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yijun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shujun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lequan Yu</a></p>
<p>Modern deep neural networks struggle to transfer knowledge and generalize
across diverse domains when deployed to real-world applications. Currently,
domain generalization (DG) is introduced to learn a universal representation
from multiple domains to improve the network generalization ability on unseen
domains. However, previous DG methods only focus on the data-level consistency
scheme without considering the synergistic regularization among different
consistency schemes. In this paper, we present a novel Hierarchical Consistency
framework for Domain Generalization (HCDG) by integrating Extrinsic Consistency
and Intrinsic Consistency synergistically. Particularly, for the Extrinsic
Consistency, we leverage the knowledge across multiple source domains to
enforce data-level consistency. To better enhance such consistency, we design a
novel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation
called DomainUp. For the Intrinsic Consistency, we perform task-level
consistency for the same instance under the dual-task scenario. We evaluate the
proposed HCDG framework on two medical image segmentation tasks, i.e., optic
cup/disc segmentation on fundus images and prostate MRI segmentation. Extensive
experimental results manifest the effectiveness and versatility of our HCDG
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.12844">Reconstructing Pruned Filters using Cheap Spatial Transformations. (arXiv:2110.12844v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1">Roy Miles</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1">Krystian Mikolajczyk</a></p>
<p>We present an efficient alternative to the convolutional layer using cheap
spatial transformations. This construction exploits an inherent spatial
redundancy of the learned convolutional filters to enable a much greater
parameter efficiency, while maintaining the top-end accuracy of their dense
counter-parts. Training these networks is modelled as a generalised pruning
problem, whereby the pruned filters are replaced with cheap transformations
from the set of non-pruned filters. We provide an efficient implementation of
the proposed layer, followed by two natural extensions to avoid excessive
feature compression and to improve the expressivity of the transformed
features. We show that these networks can achieve comparable or improved
performance to state-of-the-art pruning models across both the CIFAR-10 and
ImageNet-1K datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.13592">Multimodal Image Synthesis and Editing: The Generative AI Era. (arXiv:2112.13592v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1">Fangneng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yingchen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Rongliang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiahui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shijian Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1">Adam Kortylewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a></p>
<p>As information exists in various modalities in real world, effective
interaction and fusion among multimodal information plays a key role for the
creation and perception of multimodal data in computer vision and deep learning
research. With superb power in modeling the interaction among multimodal
information, multimodal image synthesis and editing has become a hot research
topic in recent years. Instead of providing explicit guidance for network
training, multimodal guidance offers intuitive and flexible means for image
synthesis and editing. On the other hand, this field is also facing several
challenges in alignment of multimodal features, synthesis of high-resolution
images, faithful evaluation metrics, etc. In this survey, we comprehensively
contextualize the advance of the recent multimodal image synthesis and editing
and formulate taxonomies according to data modalities and model types. We start
with an introduction to different guidance modalities in image synthesis and
editing, and then describe multimodal image synthesis and editing approaches
extensively according to their model types. After that, we describe benchmark
datasets and evaluation metrics as well as corresponding experimental results.
Finally, we provide insights about the current research challenges and possible
directions for future research. A project associated with this survey is
available at https://github.com/fnzhan/Generative-AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.13310">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Han Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuanzhuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1">Ziteng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>Monocular 3D object detection has long been a challenging task in autonomous
driving. Most existing methods follow conventional 2D detectors to first
localize object centers, and then predict 3D attributes by neighboring
features. However, only using local visual features is insufficient to
understand the scene-level 3D spatial structures and ignores the long-range
inter-object depth relations. In this paper, we introduce the first DETR
framework for Monocular DEtection with a depth-guided TRansformer, named
MonoDETR. We modify the vanilla transformer to be depth-aware and guide the
whole detection process by contextual depth cues. Specifically, concurrent to
the visual encoder that captures object appearances, we introduce to predict a
foreground depth map, and specialize a depth encoder to extract non-local depth
embeddings. Then, we formulate 3D object candidates as learnable queries and
propose a depth-guided decoder to conduct object-scene depth interactions. In
this way, each object query estimates its 3D attributes adaptively from the
depth-guided regions on the image and is no longer constrained to local visual
features. On KITTI benchmark with monocular images as input, MonoDETR achieves
state-of-the-art performance and requires no extra dense depth annotations.
Besides, our depth-guided modules can also be plug-and-play to enhance
multi-view 3D object detectors on nuScenes dataset, demonstrating our superior
generalization capacity. Code is available at
https://github.com/ZrrSkywalker/MonoDETR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.14944">Differentiable Microscopy Designs an All Optical Phase Retrieval Microscope. (arXiv:2203.14944v4 [physics.optics] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Herath_K/0/1/0/all/0/1">Kithmini Herath</a>, <a href="http://arxiv.org/find/physics/1/au:+Haputhanthri_U/0/1/0/all/0/1">Udith Haputhanthri</a>, <a href="http://arxiv.org/find/physics/1/au:+Hettiarachchi_R/0/1/0/all/0/1">Ramith Hettiarachchi</a>, <a href="http://arxiv.org/find/physics/1/au:+Kariyawasam_H/0/1/0/all/0/1">Hasindu Kariyawasam</a>, <a href="http://arxiv.org/find/physics/1/au:+Ahmad_R/0/1/0/all/0/1">Raja N. Ahmad</a>, <a href="http://arxiv.org/find/physics/1/au:+Ahmad_A/0/1/0/all/0/1">Azeem Ahmad</a>, <a href="http://arxiv.org/find/physics/1/au:+Ahluwalia_B/0/1/0/all/0/1">Balpreet S. Ahluwalia</a>, <a href="http://arxiv.org/find/physics/1/au:+Edussooriya_C/0/1/0/all/0/1">Chamira U. S. Edussooriya</a>, <a href="http://arxiv.org/find/physics/1/au:+Wadduwage_D/0/1/0/all/0/1">Dushan N. Wadduwage</a></p>
<p>Since the late 16th century, scientists have continuously innovated and
developed new microscope types for various applications. Creating a new
architecture from the ground up requires substantial scientific expertise and
creativity, often spanning years or even decades. In this study, we propose an
alternative approach called "Differentiable Microscopy," which introduces a
top-down design paradigm for optical microscopes. Using all-optical phase
retrieval as an illustrative example, we demonstrate the effectiveness of
data-driven microscopy design through $\partial\mu$. Furthermore, we conduct
comprehensive comparisons with competing methods, showcasing the consistent
superiority of our learned designs across multiple datasets, including
biological samples. To substantiate our ideas, we experimentally validate the
functionality of one of the learned designs, providing a proof of concept. The
proposed differentiable microscopy framework supplements the creative process
of designing new optical systems and would perhaps lead to unconventional but
better optical designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07240">Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1">Sayna Ebrahimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan O. Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a></p>
<p>For visual document understanding (VDU), self-supervised pretraining has been
shown to successfully generate transferable representations, yet, effective
adaptation of such representations to distribution shifts at test-time remains
to be an unexplored area. We propose DocTTA, a novel test-time adaptation
method for documents, that does source-free domain adaptation using unlabeled
target document data. DocTTA leverages cross-modality self-supervised learning
via masked visual language modeling, as well as pseudo labeling to adapt models
learned on a \textit{source} domain to an unlabeled \textit{target} domain at
test time. We introduce new benchmarks using existing public datasets for
various VDU tasks, including entity recognition, key-value extraction, and
document visual question answering. DocTTA shows significant improvements on
these compared to the source model performance, up to 1.89\% in (F1 score),
3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark
datasets are available at \url{https://saynaebrahimi.github.io/DocTTA.html}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11723">Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1">Alexander Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1">Shinichi Nakajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a></p>
<p>Deep autoencoders provide an effective tool for learning non-linear
dimensionality reduction in an unsupervised way. Recently, they have been used
for the task of anomaly detection in the visual domain. By optimizing for the
reconstruction error using anomaly-free examples, the common belief is that a
corresponding network should fail to accurately reconstruct anomalous regions
in the application phase. This goal is typically addressed by controlling the
capacity of the network, either by reducing the size of the bottleneck layer or
by enforcing sparsity constraints on the activations. However, neither of these
techniques does explicitly penalize reconstruction of anomalous signals often
resulting in poor detection. We tackle this problem by adapting a
self-supervised learning regime that allows the use of discriminative
information during training but focuses on the data manifold of normal
examples. We emphasize that inference with our approach is very efficient
during training and prediction requiring a single forward pass for each input
image. Our experiments on the MVTec AD dataset demonstrate high detection and
localization performance. On the texture-subset, in particular, our approach
consistently outperforms recent anomaly detection methods by a significant
margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11945">Efficient Adaptive Activation Rounding for Post-Training Quantization. (arXiv:2208.11945v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Cong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhanda Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yangjie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiaotian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1">Jingwen Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1">Minyi Guo</a></p>
<p>Post-training quantization attracts increasing attention due to its
convenience in deploying quantized neural networks. Although
rounding-to-nearest remains the prevailing method for DNN quantization, prior
research has demonstrated its suboptimal nature when applied to weight
quantization. They propose optimizing weight rounding schemes by leveraging
output error rather than the traditional weight quantization error. Our study
reveals that similar rounding challenges also extend to activation
quantization. Despite the easy generalization, the challenges lie in the
dynamic nature of activation. Adaptive rounding is expected for varying
activations and the method is subjected to runtime overhead. To tackle this, we
propose the AQuant quantization framework with a novel perspective to reduce
output error by adjusting rounding schemes of activations. Instead of using the
constant rounding border 0.5 of the rounding-to-nearest operation, we make the
border become a function w.r.t. the activation value to change the activation
rounding by the adaptive border. To deal with the runtime overhead, we use a
coarse-grained version of the border function. Finally, we introduce our
framework to optimize the border function. Extensive experiments show that
AQuant achieves notable improvements compared to state-of-the-art works and
pushes the accuracy of ResNet-18 up to 60.31% under the 2-bit weight and
activation quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.00939">Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Susung Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gyuseong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1">Wooseok Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungryong Kim</a></p>
<p>Denoising diffusion models (DDMs) have attracted attention for their
exceptional generation quality and diversity. This success is largely
attributed to the use of class- or text-conditional diffusion guidance methods,
such as classifier and classifier-free guidance. In this paper, we present a
more comprehensive perspective that goes beyond the traditional guidance
methods. From this generalized perspective, we introduce novel condition- and
training-free strategies to enhance the quality of generated images. As a
simple solution, blur guidance improves the suitability of intermediate samples
for their fine-scale information and structures, enabling diffusion models to
generate higher quality samples with a moderate guidance scale. Improving upon
this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps
of diffusion models to enhance their stability and efficacy. Specifically, SAG
adversarially blurs only the regions that diffusion models attend to at each
iteration and guides them accordingly. Our experimental results show that our
SAG improves the performance of various diffusion models, including ADM, IDDPM,
Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidance
methods leads to further improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01249">LOPR: Latent Occupancy PRediction using Generative Models. (arXiv:2210.01249v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lange_B/0/1/0/all/0/1">Bernard Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1">Masha Itkina</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Environment prediction frameworks are integral for autonomous vehicles,
enabling safe navigation in dynamic environments. LiDAR generated occupancy
grid maps (L-OGMs) offer a robust bird's eye-view scene representation that
facilitates joint scene predictions without relying on manual labeling unlike
commonly used trajectory prediction frameworks. Prior approaches have optimized
deterministic L-OGM prediction architectures directly in grid cell space. While
these methods have achieved some degree of success in prediction, they
occasionally grapple with unrealistic and incorrect predictions. We claim that
the quality and realism of the forecasted occupancy grids can be enhanced with
the use of generative models. We propose a framework that decouples occupancy
prediction into: representation learning and stochastic prediction within the
learned latent space. Our approach allows for conditioning the model on other
available sensor modalities such as RGB-cameras and high definition maps. We
demonstrate that our approach achieves state-of-the-art performance and is
readily transferable between different robotic platforms on the real-world
NuScenes, Waymo Open, and a custom dataset we collected on an experimental
vehicle platform.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.09945">VeriCompress: A Tool to Streamline the Synthesis of Verified Robust Compressed Neural Networks from Scratch. (arXiv:2211.09945v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1">Sawinder Kaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yi Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Salekin_A/0/1/0/all/0/1">Asif Salekin</a></p>
<p>AI's widespread integration has led to neural networks (NNs) deployment on
edge and similar limited-resource platforms for safety-critical scenarios. Yet,
NN's fragility raises concerns about reliable inference. Moreover, constrained
platforms demand compact networks. This study introduces VeriCompress, a tool
that automates the search and training of compressed models with robustness
guarantees. These models are well-suited for safety-critical applications and
adhere to predefined architecture and size limitations, making them deployable
on resource-restricted platforms. The method trains models 2-3 times faster
than the state-of-the-art approaches, surpassing relevant baseline approaches
by average accuracy and robustness gains of 15.1 and 9.8 percentage points,
respectively. When deployed on a resource-restricted generic platform, these
models require 5-8 times less memory and 2-4 times less inference time than
models used in verified robustness literature. Our comprehensive evaluation
across various model architectures and datasets, including MNIST, CIFAR, SVHN,
and a relevant pedestrian detection dataset, showcases VeriCompress's capacity
to identify compressed verified robust models with reduced computation overhead
compared to current standards. This underscores its potential as a valuable
tool for end users, such as developers of safety-critical applications on edge
or Internet of Things platforms, empowering them to create suitable models for
safety-critical, resource-constrained platforms in their respective domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11825">Multi-Directional Subspace Editing in Style-Space. (arXiv:2211.11825v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naveh_C/0/1/0/all/0/1">Chen Naveh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hel_Or_Y/0/1/0/all/0/1">Yacov Hel-Or</a></p>
<p>This paper describes a new technique for finding disentangled semantic
directions in the latent space of StyleGAN. Our method identifies meaningful
orthogonal subspaces that allow editing of one human face attribute, while
minimizing undesired changes in other attributes. Our model is capable of
editing a single attribute in multiple directions, resulting in a range of
possible generated images. We compare our scheme with three state-of-the-art
models and show that our method outperforms them in terms of face editing and
disentanglement capabilities. Additionally, we suggest quantitative measures
for evaluating attribute separation and disentanglement, and exhibit the
superiority of our model with respect to those measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00143">FIESTA: Autoencoders for accurate fiber segmentation in tractography. (arXiv:2212.00143v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dumais_F/0/1/0/all/0/1">F&#xe9;lix Dumais</a>, <a href="http://arxiv.org/find/cs/1/au:+Legarreta_J/0/1/0/all/0/1">Jon Haitz Legarreta</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemaire_C/0/1/0/all/0/1">Carl Lemaire</a>, <a href="http://arxiv.org/find/cs/1/au:+Poulin_P/0/1/0/all/0/1">Philippe Poulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rheault_F/0/1/0/all/0/1">Fran&#xe7;ois Rheault</a>, <a href="http://arxiv.org/find/cs/1/au:+Petit_L/0/1/0/all/0/1">Laurent Petit</a>, <a href="http://arxiv.org/find/cs/1/au:+Barakovic_M/0/1/0/all/0/1">Muhamed Barakovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Magon_S/0/1/0/all/0/1">Stefano Magon</a>, <a href="http://arxiv.org/find/cs/1/au:+Descoteaux_M/0/1/0/all/0/1">Maxime Descoteaux</a>, <a href="http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1">Pierre-Marc Jodoin</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)</p>
<p>White matter bundle segmentation is a cornerstone of modern tractography to
study the brain's structural connectivity in domains such as neurological
disorders, neurosurgery, and aging. In this study, we present FIESTA (FIbEr
Segmentation in Tractography using Autoencoders), a reliable and robust, fully
automated, and easily semi-automatically calibrated pipeline based on deep
autoencoders that can dissect and fully populate white matter bundles. This
pipeline is built upon previous works that demonstrated how autoencoders can be
used successfully for streamline filtering, bundle segmentation, and streamline
generation in tractography. Our proposed method improves bundle segmentation
coverage by recovering hard-to-track bundles with generative sampling through
the latent space seeding of the subject bundle and the atlas bundle. A latent
space of streamlines is learned using autoencoder-based modeling combined with
contrastive learning. Using an atlas of bundles in standard space (MNI), our
proposed method segments new tractograms using the autoencoder latent distance
between each tractogram streamline and its closest neighbor bundle in the atlas
of bundles. Intra-subject bundle reliability is improved by recovering
hard-to-track streamlines, using the autoencoder to generate new streamlines
that increase the spatial coverage of each bundle while remaining anatomically
correct. Results show that our method is more reliable than state-of-the-art
automated virtual dissection methods such as RecoBundles, RecoBundlesX,
TractSeg, White Matter Analysis and XTRACT. Our framework allows for the
transition from one anatomical bundle definition to another with marginal
calibration efforts. Overall, these results show that our framework improves
the practicality and usability of current state-of-the-art bundle segmentation
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01735">Neural Fourier Filter Bank. (arXiv:2212.01735v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhijie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yuhe Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kwang Moo Yi</a></p>
<p>We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02011">PointCaM: Cut-and-Mix for Open-Set Point Cloud Learning. (arXiv:2212.02011v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Jie Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Shi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1">Saeed Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1">Mehrtash Harandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1">Nick Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1">Lars Petersson</a></p>
<p>Point cloud learning is receiving increasing attention, however, most
existing point cloud models lack the practical ability to deal with the
unavoidable presence of unknown objects. This paper mainly discusses point
cloud learning under open-set settings, where we train the model without data
from unknown classes and identify them in the inference stage. Basically, we
propose to solve open-set point cloud learning using a novel Point Cut-and-Mix
mechanism consisting of Unknown-Point Simulator and Unknown-Point Estimator
modules. Specifically, we use the Unknown-Point Simulator to simulate
out-of-distribution data in the training stage by manipulating the geometric
context of partial known data. Based on this, the Unknown-Point Estimator
module learns to exploit the point cloud's feature context for discriminating
the known and unknown data. Extensive experiments show the plausibility of
open-set point cloud learning and the effectiveness of our proposed solutions.
Our code is available at \url{https://github.com/ShiQiu0419/pointcam}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05153">Algorithmic progress in computer vision. (arXiv:2212.05153v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1">Ege Erdil</a>, <a href="http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1">Tamay Besiroglu</a></p>
<p>We investigate algorithmic progress in image classification on ImageNet,
perhaps the most well-known test bed for computer vision. We estimate a model,
informed by work on neural scaling laws, and infer a decomposition of progress
into the scaling of compute, data, and algorithms. Using Shapley values to
attribute performance improvements, we find that algorithmic improvements have
been roughly as important as the scaling of compute for progress computer
vision. Our estimates indicate that algorithmic innovations mostly take the
form of compute-augmenting algorithmic advances (which enable researchers to
get better performance from less compute), not data-augmenting algorithmic
advances. We find that compute-augmenting algorithmic advances are made at a
pace more than twice as fast as the rate usually associated with Moore's law.
In particular, we estimate that compute-augmenting innovations halve compute
requirements every nine months (95\% confidence interval: 4 to 25 months).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01805">Unsupervised Manifold Linearizing and Clustering. (arXiv:2301.01805v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1">Tianjiao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Shengbang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xili Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1">Benjamin D. Haeffele</a></p>
<p>We consider the problem of simultaneously clustering and learning a linear
representation of data lying close to a union of low-dimensional manifolds, a
fundamental task in machine learning and computer vision. When the manifolds
are assumed to be linear subspaces, this reduces to the classical problem of
subspace clustering, which has been studied extensively over the past two
decades. Unfortunately, many real-world datasets such as natural images can not
be well approximated by linear subspaces. On the other hand, numerous works
have attempted to learn an appropriate transformation of the data, such that
data is mapped from a union of general non-linear manifolds to a union of
linear subspaces (with points from the same manifold being mapped to the same
subspace). However, many existing works have limitations such as assuming
knowledge of the membership of samples to clusters, requiring high sampling
density, or being shown theoretically to learn trivial representations. In this
paper, we propose to optimize the Maximal Coding Rate Reduction metric with
respect to both the data representation and a novel doubly stochastic cluster
membership, inspired by state-of-the-art subspace clustering results. We give a
parameterization of such a representation and membership, allowing efficient
mini-batching and one-shot initialization. Experiments on CIFAR-10, -20, -100,
and TinyImageNet-200 datasets show that the proposed method is much more
accurate and scalable than state-of-the-art deep clustering methods, and
further learns a latent linear representation of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04634">Street-View Image Generation from a Bird&#x27;s-Eye View Layout. (arXiv:2301.04634v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Swerdlow_A/0/1/0/all/0/1">Alexander Swerdlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Runsheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bolei Zhou</a></p>
<p>Bird's-Eye View (BEV) Perception has received increasing attention in recent
years as it provides a concise and unified spatial representation across views
and benefits a diverse set of downstream driving applications. While the focus
has been placed on discriminative tasks such as BEV segmentation, the dual
generative task of creating street-view images from a BEV layout has rarely
been explored. The ability to generate realistic street-view images that align
with a given HD map and traffic layout is critical for visualizing complex
traffic scenarios and developing robust perception models for autonomous
driving. In this paper, we propose BEVGen, a conditional generative model that
synthesizes a set of realistic and spatially consistent surrounding images that
match the BEV layout of a traffic scenario. BEVGen incorporates a novel
cross-view transformation and spatial attention design which learn the
relationship between cameras and map views to ensure their consistency. Our
model can accurately render road and lane lines, as well as generate traffic
scenes under different weather conditions and times of day. The code will be
made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06496">Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning. (arXiv:2301.06496v3 [physics.optics] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lim_J/0/1/0/all/0/1">Joowon Lim</a>, <a href="http://arxiv.org/find/physics/1/au:+Gladrow_J/0/1/0/all/0/1">Jannes Gladrow</a>, <a href="http://arxiv.org/find/physics/1/au:+Kelly_D/0/1/0/all/0/1">Douglas Kelly</a>, <a href="http://arxiv.org/find/physics/1/au:+OShea_G/0/1/0/all/0/1">Greg O&#x27;Shea</a>, <a href="http://arxiv.org/find/physics/1/au:+Verkes_G/0/1/0/all/0/1">Govert Verkes</a>, <a href="http://arxiv.org/find/physics/1/au:+Stefanovici_I/0/1/0/all/0/1">Ioan Stefanovici</a>, <a href="http://arxiv.org/find/physics/1/au:+Nowozin_S/0/1/0/all/0/1">Sebastian Nowozin</a>, <a href="http://arxiv.org/find/physics/1/au:+Thomsen_B/0/1/0/all/0/1">Benn Thomsen</a></p>
<p>Retrieving images transmitted through multi-mode fibers is of growing
interest, thanks to their ability to confine and transport light efficiently in
a compact system. Here, we demonstrate machine-learning-based decoding of
large-scale digital images (pages), maximizing page capacity for optical
storage applications. Using a millimeter-sized square cross-section waveguide,
we image an 8-bit spatial light modulator, presenting data as a matrix of
symbols. Normally, decoders will incur a prohibitive O(n^2) computational
scaling to decode n symbols in spatially scrambled data. However, by combining
a digital twin of the setup with a U-Net, we can retrieve up to 66 kB using
efficient convolutional operations only. We compare trainable ray-tracing-based
with eigenmode-based twins and show the former to be superior thanks to its
ability to overcome the simulation-to-experiment gap by adjusting to optical
imperfections. We train the pipeline end-to-end using a differentiable
mutual-information estimator based on the von-Mises distribution, generally
applicable to phase-coding channels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.09091">BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Minjung Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1">Yunji Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1">Jeongmin Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Young Sun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1">Hyeran Byun</a>, <a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1">Youngjung Uh</a></p>
<p>3D-aware GANs aim to synthesize realistic 3D scenes such that they can be
rendered in arbitrary perspectives to produce images. Although previous methods
produce realistic images, they suffer from unstable training or degenerate
solutions where the 3D geometry is unnatural. We hypothesize that the 3D
geometry is underdetermined due to the insufficient constraint, i.e., being
classified as real image to the discriminator is not enough. To solve this
problem, we propose to approximate the background as a spherical surface and
represent a scene as a union of the foreground placed in the sphere and the
thin spherical background. It reduces the degree of freedom in the background
field. Accordingly, we modify the volume rendering equation and incorporate
dedicated constraints to design a novel 3D-aware GAN framework named BallGAN.
BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D
geometry; the images of a scene across different viewpoints have better
photometric consistency and fidelity than the state-of-the-art methods. 2) The
training becomes much more stable. 3) The foreground can be separately rendered
on top of different arbitrary backgrounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00903">No One Left Behind: Real-World Federated Class-Incremental Learning. (arXiv:2302.00903v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiahua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongliu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1">Yang Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Gan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yulun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>Federated learning (FL) is a hot collaborative training framework via
aggregating model parameters of decentralized local clients. However, most FL
methods unreasonably assume data categories of FL framework are known and fixed
in advance. Moreover, some new local clients that collect novel categories
unseen by other clients may be introduced to FL training irregularly. These
issues render global model to undergo catastrophic forgetting on old
categories, when local clients receive new categories consecutively under
limited memory of storing old categories. To tackle the above issues, we
propose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local
clients are left behind as they learn new classes continually, by addressing
local and global catastrophic forgetting. Specifically, considering tackling
class imbalance of local client to surmount local forgetting, we develop a
category-balanced gradient-adaptive compensation loss and a category
gradient-induced semantic distillation loss. They can balance heterogeneous
forgetting speeds of hard-to-forget and easy-to-forget old categories, while
ensure consistent class-relations within different tasks. Moreover, a proxy
server is designed to tackle global forgetting caused by Non-IID class
imbalance between different clients. It augments perturbed prototype images of
new categories collected from local clients via self-supervised prototype
augmentation, thus improving robustness to choose the best old global model for
local-side semantic distillation loss. Experiments on representative datasets
verify superior performance of our model against comparison methods. The code
is available at https://github.com/JiahuaDong/LGA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05699">Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Saemi Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Seunghyuk Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongwoo Kim</a></p>
<p>We tackle the problem of feature unlearning from a pre-trained image
generative model: GANs and VAEs. Unlike a common unlearning task where an
unlearning target is a subset of the training set, we aim to unlearn a specific
feature, such as hairstyle from facial images, from the pre-trained generative
models. As the target feature is only presented in a local region of an image,
unlearning the entire image from the pre-trained model may result in losing
other details in the remaining region of the image. To specify which features
to unlearn, we collect randomly generated images that contain the target
features. We then identify a latent representation corresponding to the target
feature and then use the representation to fine-tune the pre-trained model.
Through experiments on MNIST and CelebA datasets, we show that target features
are successfully removed while keeping the fidelity of the original models.
Further experiments with an adversarial attack show that the unlearned model is
more robust under the presence of malicious parties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09790">Reliable Multimodality Eye Disease Screening via Mixture of Student&#x27;s t Distributions. (arXiv:2303.09790v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zou_K/0/1/0/all/0/1">Ke Zou</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_T/0/1/0/all/0/1">Tian Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1">Xuedong Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1">Haoyu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1">Xiaojing Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a></p>
<p>Multimodality eye disease screening is crucial in ophthalmology as it
integrates information from diverse sources to complement their respective
performances. However, the existing methods are weak in assessing the
reliability of each unimodality, and directly fusing an unreliable modality may
cause screening errors. To address this issue, we introduce a novel
multimodality evidential fusion pipeline for eye disease screening, EyeMoSt,
which provides a measure of confidence for unimodality and elegantly integrates
the multimodality information from a multi-distribution fusion perspective.
Specifically, our model estimates both local uncertainty for unimodality and
global uncertainty for the fusion modality to produce reliable classification
results. More importantly, the proposed mixture of Student's $t$ distributions
adaptively integrates different modalities to endow the model with heavy-tailed
properties, increasing robustness and reliability. Our experimental findings on
both public and in-house datasets show that our model is more reliable than
current methods. Additionally, EyeMost has the potential ability to serve as a
data quality discriminator, enabling reliable decision-making for multimodality
eye disease screening.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11086">Pluralistic Aging Diffusion Autoencoder. (arXiv:2303.11086v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peipei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huaibo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhaofeng He</a></p>
<p>Face aging is an ill-posed problem because multiple plausible aging patterns
may correspond to a given input. Most existing methods often produce one
deterministic estimation. This paper proposes a novel CLIP-driven Pluralistic
Aging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.
First, we employ diffusion models to generate diverse low-level aging details
via a sequential denoising reverse process. Second, we present Probabilistic
Aging Embedding (PAE) to capture diverse high-level aging patterns, which
represents age information as probabilistic distributions in the common CLIP
latent space. A text-guided KL-divergence loss is designed to guide this
learning. Our method can achieve pluralistic face aging conditioned on
open-world aging texts and arbitrary unseen face images. Qualitative and
quantitative experiments demonstrate that our method can generate more diverse
and high-quality plausible aging results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12077">VAD: Vectorized Scene Representation for Efficient Autonomous Driving. (arXiv:2303.12077v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1">Bencheng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Helong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a></p>
<p>Autonomous driving requires a comprehensive understanding of the surrounding
environment for reliable trajectory planning. Previous works rely on dense
rasterized scene representation (e.g., agent occupancy and semantic map) to
perform planning, which is computationally intensive and misses the
instance-level structure information. In this paper, we propose VAD, an
end-to-end vectorized paradigm for autonomous driving, which models the driving
scene as a fully vectorized representation. The proposed vectorized paradigm
has two significant advantages. On one hand, VAD exploits the vectorized agent
motion and map elements as explicit instance-level planning constraints which
effectively improves planning safety. On the other hand, VAD runs much faster
than previous end-to-end planning methods by getting rid of
computation-intensive rasterized representation and hand-designed
post-processing steps. VAD achieves state-of-the-art end-to-end planning
performance on the nuScenes dataset, outperforming the previous best method by
a large margin. Our base model, VAD-Base, greatly reduces the average collision
rate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny,
greatly improves the inference speed (up to 9.3x) while achieving comparable
planning performance. We believe the excellent performance and the high
efficiency of VAD are critical for the real-world deployment of an autonomous
driving system. Code and models are available at https://github.com/hustvl/VAD
for facilitating future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12343">LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation. (arXiv:2303.12343v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pnvr_K/0/1/0/all/0/1">Koutilya Pnvr</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1">Bharat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1">Pallabi Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddiquie_B/0/1/0/all/0/1">Behjat Siddiquie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1">David Jacobs</a></p>
<p>Large-scale pre-training tasks like image classification, captioning, or
self-supervised techniques do not incentivize learning the semantic boundaries
of objects. However, recent generative foundation models built using text-based
latent diffusion techniques may learn semantic boundaries. This is because they
have to synthesize intricate details about all objects in an image based on a
text description. Therefore, we present a technique for segmenting real and
AI-generated images using latent diffusion models (LDMs) trained on
internet-scale datasets. First, we show that the latent space of LDMs (z-space)
is a better input representation compared to other feature representations like
RGB images or CLIP encodings for text-based image segmentation. By training the
segmentation models on the latent z-space, which creates a compressed
representation across several domains like different forms of art, cartoons,
illustrations, and photographs, we are also able to bridge the domain gap
between real and AI-generated images. We show that the internal features of
LDMs contain rich semantic information and present a technique in the form of
LD-ZNet to further boost the performance of text-based segmentation. Overall,
we show up to 6% improvement over standard baselines for text-to-image
segmentation on natural images. For AI-generated imagery, we show close to 20%
improvement compared to state-of-the-art techniques. The project is available
at https://koutilya-pnvr.github.io/LD-ZNet/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13111">Boosting Convolution with Efficient MLP-Permutation for Volumetric Medical Image Segmentation. (arXiv:2303.13111v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_X/0/1/0/all/0/1">Xiao Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1">Kwang-Ting Cheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Recently, the advent of vision Transformer (ViT) has brought substantial
advancements in 3D dataset benchmarks, particularly in 3D volumetric medical
image segmentation (Vol-MedSeg). Concurrently, multi-layer perceptron (MLP)
network has regained popularity among researchers due to their comparable
results to ViT, albeit with the exclusion of the resource-intensive
self-attention module. In this work, we propose a novel permutable hybrid
network for Vol-MedSeg, named PHNet, which capitalizes on the strengths of both
convolution neural networks (CNNs) and MLP. PHNet addresses the intrinsic
isotropy problem of 3D volumetric data by employing a combination of 2D and 3D
CNNs to extract local features. Besides, we propose an efficient multi-layer
permute perceptron (MLPP) module that captures long-range dependence while
preserving positional information. This is achieved through an axis
decomposition operation that permutes the input tensor along different axes,
thereby enabling the separate encoding of the positional information.
Furthermore, MLPP tackles the resolution sensitivity issue of MLP in Vol-MedSeg
with a token segmentation operation, which divides the feature into smaller
tokens and processes them individually. Extensive experimental results validate
that PHNet outperforms the state-of-the-art methods with lower computational
costs on the widely-used yet challenging COVID-19-20 and Synapse benchmarks.
The ablation study also demonstrates the effectiveness of PHNet in harnessing
the strengths of both CNNs and MLP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13796">Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction. (arXiv:2303.13796v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yongtao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Haiyi Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qingping Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanjun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a></p>
<p>As it is hard to calibrate single-view RGB images in the wild, existing 3D
human mesh reconstruction (3DHMR) methods either use a constant large focal
length or estimate one based on the background environment context, which can
not tackle the problem of the torso, limb, hand or face distortion caused by
perspective camera projection when the camera is close to the human body. The
naive focal length assumptions can harm this task with the incorrectly
formulated projection matrices. To solve this, we propose Zolly, the first
3DHMR method focusing on perspective-distorted images. Our approach begins with
analysing the reason for perspective distortion, which we find is mainly caused
by the relative location of the human body to the camera center. We propose a
new camera model and a novel 2D representation, termed distortion image, which
describes the 2D dense distortion scale of the human body. We then estimate the
distance from distortion scale features rather than environment context
features. Afterwards, we integrate the distortion feature with image features
to reconstruct the body mesh. To formulate the correct projection matrix and
locate the human body position, we simultaneously use perspective and
weak-perspective projection loss. Since existing datasets could not handle this
task, we propose the first synthetic dataset PDHuman and extend two real-world
datasets tailored for this task, all containing perspective-distorted human
images. Extensive experiments show that Zolly outperforms existing
state-of-the-art methods on both perspective-distorted datasets and the
standard benchmark (3DPW).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05669">Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation. (arXiv:2304.05669v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Liwen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaldiz_M/0/1/0/all/0/1">Mustafa B. Yaldiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yinhao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Matai_J/0/1/0/all/0/1">Janarbek Matai</a>, <a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1">Fatih Porikli</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tzu-Mao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1">Manmohan Chandraker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1">Ravi Ramamoorthi</a></p>
<p>Inverse path tracing has recently been applied to joint material and lighting
estimation, given geometry and multi-view HDR observations of an indoor scene.
However, it has two major limitations: path tracing is expensive to compute,
and ambiguities exist between reflection and emission. Our Factorized Inverse
Path Tracing (FIPT) addresses these challenges by using a factored light
transport formulation and finds emitters driven by rendering errors. Our
algorithm enables accurate material and lighting optimization faster than
previous work, and is more effective at resolving ambiguities. The exhaustive
experiments on synthetic scenes show that our method (1) outperforms
state-of-the-art indoor inverse rendering and relighting methods particularly
in the presence of complex illumination effects; (2) speeds up inverse path
tracing optimization to less than an hour. We further demonstrate robustness to
noisy inputs through material and lighting estimates that allow plausible
relighting in a real scene. The source code is available at:
https://github.com/lwwu2/fipt
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08134">Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1">Martin Knoche</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1">Gerhard Rigoll</a></p>
<p>Nowadays, face recognition systems surpass human performance on several
datasets. However, there are still edge cases that the machine can't correctly
classify. This paper investigates the effect of a combination of machine and
human operators in the face verification task. First, we look closer at the
edge cases for several state-of-the-art models to discover common datasets'
challenging settings. Then, we conduct a study with 60 participants on these
selected tasks with humans and provide an extensive analysis. Finally, we
demonstrate that combining machine and human decisions can further improve the
performance of state-of-the-art face verification systems on various benchmark
datasets. Code and data are publicly available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09818">What Should Be Balanced in a &quot;Balanced&quot; Face Recognition Dataset?. (arXiv:2304.09818v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haiyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1">Kevin W. Bowyer</a></p>
<p>The issue of demographic disparities in face recognition accuracy has
attracted increasing attention in recent years. Various face image datasets
have been proposed as 'fair' or 'balanced' to assess the accuracy of face
recognition algorithms across demographics. These datasets typically balance
the number of identities and images across demographics. It is important to
note that the number of identities and images in an evaluation dataset are {\em
not} driving factors for 1-to-1 face matching accuracy. Moreover, balancing the
number of identities and images does not ensure balance in other factors known
to impact accuracy, such as head pose, brightness, and image quality. We
demonstrate these issues using several recently proposed datasets. To improve
the ability to perform less biased evaluations, we propose a bias-aware toolkit
that facilitates creation of cross-demographic evaluation datasets balanced on
factors mentioned in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10856">Towards an Accurate and Secure Detector against Adversarial Perturbations. (arXiv:2305.10856v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1">Shuren Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiqiu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yushu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_R/0/1/0/all/0/1">Rushi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xiaochun Cao</a></p>
<p>The vulnerability of deep neural networks to adversarial perturbations has
been widely perceived in the computer vision community. From a security
perspective, it poses a critical risk for modern vision systems, e.g., the
popular Deep Learning as a Service (DLaaS) frameworks. For protecting
off-the-shelf deep models while not modifying them, current algorithms
typically detect adversarial patterns through discriminative decomposition of
natural-artificial data. However, these decompositions are biased towards
frequency or spatial discriminability, thus failing to capture adversarial
patterns comprehensively. More seriously, successful defense-aware (secondary)
adversarial attack (i.e., evading the detector as well as fooling the model) is
practical under the assumption that the adversary is fully aware of the
detector (i.e., the Kerckhoffs's principle). Motivated by such facts, we
propose an accurate and secure adversarial example detector, relying on a
spatial-frequency discriminative decomposition with secret keys. It expands the
above works on two aspects: 1) the introduced Krawtchouk basis provides better
spatial-frequency discriminability and thereby is more suitable for capturing
adversarial patterns than the common trigonometric or wavelet basis; 2) the
extensive parameters for decomposition are generated by a pseudo-random
function with secret keys, hence blocking the defense-aware adversarial attack.
Theoretical and numerical analysis demonstrates the increased accuracy and
security of our detector with respect to a number of state-of-the-art
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17420">CCDWT-GAN: Generative Adversarial Networks Based on Color Channel Using Discrete Wavelet Transform for Document Image Binarization. (arXiv:2305.17420v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1">Rui-Yang Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yu-Shian Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1">Jen-Shiun Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chih-Chia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei-Han Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chien_C/0/1/0/all/0/1">Chun-Tse Chien</a></p>
<p>To efficiently extract textual information from color degraded document
images is a significant research area. The prolonged imperfect preservation of
ancient documents has led to various types of degradation, such as page
staining, paper yellowing, and ink bleeding. These types of degradation badly
impact the image processing for features extraction. This paper introduces a
novelty method employing generative adversarial networks based on color channel
using discrete wavelet transform (CCDWT-GAN). The proposed method involves
three stages: image preprocessing, image enhancement, and image binarization.
In the initial step, we apply discrete wavelet transform (DWT) to retain the
low-low (LL) subband image, thereby enhancing image quality. Subsequently, we
divide the original input image into four single-channel colors (red, green,
blue, and gray) to separately train adversarial networks. For the extraction of
global and local features, we utilize the output image from the image
enhancement stage and the entire input image to train adversarial networks
independently, and then combine these two results as the final output. To
validate the positive impact of the image enhancement and binarization stages
on model performance, we conduct an ablation study. This work compares the
performance of the proposed method with other state-of-the-art (SOTA) methods
on DIBCO and H-DIBCO ((Handwritten) Document Image Binarization Competition)
datasets. The experimental results demonstrate that CCDWT-GAN achieves a top
two performance on multiple benchmark datasets. Notably, on DIBCO 2013 and 2016
dataset, our method achieves F-measure (FM) values of 95.24 and 91.46,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01891">DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System. (arXiv:2306.01891v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Soliman_A/0/1/0/all/0/1">Abanob Soliman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonardi_F/0/1/0/all/0/1">Fabien Bonardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidibe_D/0/1/0/all/0/1">D&#xe9;sir&#xe9; Sidib&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchafa_S/0/1/0/all/0/1">Samia Bouchafa</a></p>
<p>This paper presents a robust approach for a visual parallel tracking and
mapping (PTAM) system that excels in challenging environments. Our proposed
method combines the strengths of heterogeneous multi-modal visual sensors,
including stereo event-based and frame-based sensors, in a unified reference
frame through a novel spatio-temporal synchronization of stereo visual frames
and stereo event streams. We employ deep learning-based feature extraction and
description for estimation to enhance robustness further. We also introduce an
end-to-end parallel tracking and mapping optimization layer complemented by a
simple loop-closure algorithm for efficient SLAM behavior. Through
comprehensive experiments on both small-scale and large-scale real-world
sequences of VECtor and TUM-VIE benchmarks, our proposed method (DH-PTAM)
demonstrates superior performance in terms of robustness and accuracy in
adverse conditions, especially in large-scale HDR scenarios. Our
implementation's research-based Python API is publicly available on GitHub for
further research and development: https://github.com/AbanobSoliman/DH-PTAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06494">Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark. (arXiv:2306.06494v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Li Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Ameer Hamza Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lu Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiao-Ming Wu</a></p>
<p>With the availability of large-scale, comprehensive, and general-purpose
vision-language (VL) datasets such as MSCOCO, vision-language pre-training
(VLP) has become an active area of research and proven to be effective for
various VL tasks such as visual-question answering. However, studies on VLP in
the medical domain have so far been scanty. To provide a comprehensive
perspective on VLP for medical VL tasks, we conduct a thorough experimental
analysis to study key factors that may affect the performance of VLP with a
unified vision-language Transformer. To allow making sound and quick
pre-training decisions, we propose RadioGraphy Captions (RGC), a high-quality,
multi-modality radiographic dataset containing 18,434 image-caption pairs
collected from an open-access online database MedPix. RGC can be used as a
pre-training dataset or a new benchmark for medical report generation and
medical image-text retrieval. By utilizing RGC and other available datasets for
pre-training, we develop several key insights that can guide future medical VLP
research and new strong baselines for various medical VL tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07703">E2E-LOAD: End-to-End Long-form Online Action Detection. (arXiv:2306.07703v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shuqiang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weixin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bairui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lin Ma</a></p>
<p>Recently, there has been a growing trend toward feature-based approaches for
Online Action Detection (OAD). However, these approaches have limitations due
to their fixed backbone design, which ignores the potential capability of a
trainable backbone. In this paper, we propose the first end-to-end OAD model,
termed E2E-LOAD, designed to address the major challenge of OAD, namely,
long-term understanding and efficient online reasoning. Specifically, our
proposed approach adopts an initial spatial model that is shared by all frames
and maintains a long sequence cache for inference at a low computational cost.
We also advocate an asymmetric spatial-temporal model for long-form and
short-form modeling effectively. Furthermore, we propose a novel and efficient
inference mechanism that accelerates heavy spatial-temporal exploration.
Extensive ablation studies and experiments demonstrate the effectiveness and
efficiency of our proposed method. Notably, we achieve 17.3 (+12.6) FPS for
end-to-end OAD with 72.4%~(+1.2%), 90.3%~(+0.7%), and 48.1%~(+26.0%) mAP on
THMOUS14, TVSeries, and HDD, respectively, which is 3x faster than previous
approaches. The source code will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08713">What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations. (arXiv:2306.08713v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1">Chiara Plizzari</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrett_T/0/1/0/all/0/1">Toby Perrett</a>, <a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1">Barbara Caputo</a>, <a href="http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1">Dima Damen</a></p>
<p>We propose and address a new generalisation problem: can a model trained for
action recognition successfully classify actions when they are performed within
a previously unseen scenario and in a previously unseen location? To answer
this question, we introduce the Action Recognition Generalisation Over
scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from
the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We
demonstrate recognition models struggle to generalise over 10 proposed test
splits, each of an unseen scenario in an unseen location. We thus propose CIR,
a method to represent each video as a Cross-Instance Reconstruction of videos
from other domains. Reconstructions are paired with text narrations to guide
the learning of a domain generalisable representation. We provide extensive
analysis and ablations on ARGO1M that show CIR outperforms prior domain
generalisation works on all test splits. Code and data:
https://chiaraplizz.github.io/what-can-a-cook/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06947">Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1">Syed Talal Wasim</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1">Muhammad Uzair Khattak</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a></p>
<p>Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on five large-scale datasets
(Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lower
computational cost. Our code/models are released at
https://github.com/TalalWasim/Video-FocalNets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06948">Self-regulating Prompts: Foundational Model Adaptation without Forgetting. (arXiv:2307.06948v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1">Muhammad Uzair Khattak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1">Syed Talal Wasim</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a></p>
<p>Prompt learning has emerged as an efficient alternative for fine-tuning
foundational models, such as CLIP, for various downstream tasks. Conventionally
trained using the task-specific objective, i.e., cross-entropy loss, prompts
tend to overfit downstream data distributions and find it challenging to
capture task-agnostic general features from the frozen CLIP. This leads to the
loss of the model's original generalization capability. To address this issue,
our work introduces a self-regularization framework for prompting called
PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the
prompts to optimize for both task-specific and task-agnostic general
representations using a three-pronged approach by: (a) regulating prompted
representations via mutual agreement maximization with the frozen model, (b)
regulating with self-ensemble of prompts over the training trajectory to encode
their complementary strengths, and (c) regulating with textual diversity to
mitigate sample diversity imbalance with the visual branch. To the best of our
knowledge, this is the first regularization framework for prompt learning that
avoids overfitting by jointly attending to pre-trained model features, the
training trajectory during prompting, and the textual diversity. PromptSRC
explicitly steers the prompts to learn a representation space that maximizes
performance on downstream tasks without compromising CLIP generalization. We
perform extensive experiments on 4 benchmarks where PromptSRC overall performs
favorably well compared to the existing methods. Our code and pre-trained
models are publicly available at: https://github.com/muzairkhattak/PromptSRC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07540">Flow-Guided Controllable Line Drawing Generation. (arXiv:2307.07540v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Chengyu Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xianfeng Han</a></p>
<p>In this paper, we investigate the problem of automatically controllable
artistic character line drawing generation from photographs by proposing a
Vector Flow Aware and Line Controllable Image-to-Image Translation
architecture, which can be viewed as an appealing intersection between
Artificial Intelligence and Arts. Specifically, we first present an
Image-to-Flow network (I2FNet) to efficiently and robustly create the vector
flow field in a learning-based manner, which can provide a direction guide for
drawing lines. Then, we introduce our well-designed Double Flow Generator (DFG)
framework to fuse features from learned vector flow and input image flow
guaranteeing the spatial coherence of lines. Meanwhile, in order to allow for
controllable character line drawing generation, we integrate a Line Control
Matrix (LCM) into DFG and train a Line Control Regressor (LCR) to synthesize
drawings with different styles by elaborately controlling the level of details,
such as thickness, smoothness, and continuity, of lines. Finally, we design a
Fourier Transformation Loss to further constrain the character line generation
from the frequency domain view of the point. Quantitative and qualitative
experiments demonstrate that our approach can obtain superior performance in
producing high-resolution character line-drawing images with perceptually
realistic characteristics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09323">Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis. (arXiv:2307.09323v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1">Lin Gu</a></p>
<p>This paper presents ER-NeRF, a novel conditional Neural Radiance Fields
(NeRF) based architecture for talking portrait synthesis that can concurrently
achieve fast convergence, real-time rendering, and state-of-the-art performance
with small model size. Our idea is to explicitly exploit the unequal
contribution of spatial regions to guide talking portrait modeling.
Specifically, to improve the accuracy of dynamic head reconstruction, a compact
and expressive NeRF-based Tri-Plane Hash Representation is introduced by
pruning empty spatial regions with three planar hash encoders. For speech
audio, we propose a Region Attention Module to generate region-aware condition
feature via an attention mechanism. Different from existing methods that
utilize an MLP-based encoder to learn the cross-modal relation implicitly, the
attention mechanism builds an explicit connection between audio features and
spatial regions to capture the priors of local motions. Moreover, a direct and
fast Adaptive Pose Encoding is introduced to optimize the head-torso separation
problem by mapping the complex transformation of the head pose into spatial
coordinates. Extensive experiments demonstrate that our method renders better
high-fidelity and audio-lips synchronized talking portrait videos, with
realistic details and high efficiency compared to previous methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11067">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation. (arXiv:2307.11067v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Van Nguyen Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hodan_T/0/1/0/all/0/1">Tomas Hodan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponimatkin_G/0/1/0/all/0/1">Georgy Ponimatkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1">Thibault Groueix</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1">Vincent Lepetit</a></p>
<p>We propose a simple three-stage approach to segment unseen objects in RGB
images using their CAD models. Leveraging recent powerful foundation models,
DINOv2 and Segment Anything, we create descriptors and generate proposals,
including binary masks for a given input RGB image. By matching proposals with
reference descriptors created from CAD models, we achieve precise object ID
assignment along with modal masks. We experimentally demonstrate that our
method achieves state-of-the-art results in CAD-based novel object
segmentation, surpassing existing approaches on the seven core datasets of the
BOP challenge by 19.8% AP using the same BOP evaluation protocol. Our source
code is available at https://github.com/nv-nguyen/cnos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11413">A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moyo_R/0/1/0/all/0/1">Reuben Moyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndebvu_S/0/1/0/all/0/1">Stanley Ndebvu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimba_M/0/1/0/all/0/1">Michael Zimba</a>, <a href="http://arxiv.org/find/cs/1/au:+Mbelwa_J/0/1/0/all/0/1">Jimmy Mbelwa</a></p>
<p>Examinations are a crucial part of the learning process, and academic
institutions invest significant resources into maintaining their integrity by
preventing cheating from students or facilitators. However, cheating has become
rampant in examination setups, compromising their integrity. The traditional
method of relying on invigilators to monitor every student is impractical and
ineffective. To address this issue, there is a need to continuously record exam
sessions to monitor students for suspicious activities. However, these
recordings are often too lengthy for invigilators to analyze effectively, and
fatigue may cause them to miss significant details. To widen the coverage,
invigilators could use fixed overhead or wearable cameras. This paper
introduces a framework that uses automation to analyze videos and detect
suspicious activities during examinations efficiently and effectively. We
utilized the OpenPose framework and Convolutional Neural Network (CNN) to
identify students exchanging objects during exams. This detection system is
vital in preventing cheating and promoting academic integrity, fairness, and
quality education for institutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11482">R2Det: Redemption from Range-view for Accurate 3D Object Detection. (arXiv:2307.11482v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qiao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a></p>
<p>LiDAR-based 3D object detection is of paramount importance for autonomous
driving. Recent trends show a remarkable improvement for bird's-eye-view (BEV)
based and point-based methods as they demonstrate superior performance compared
to range-view counterparts. This paper presents an insight that leverages
range-view representation to enhance 3D points for accurate 3D object
detection. Specifically, we introduce a Redemption from Range-view Module
(R2M), a plug-and-play approach for 3D surface texture enhancement from the 2D
range view to the 3D point view. R2M comprises BasicBlock for 2D feature
extraction, Hierarchical-dilated (HD) Meta Kernel for expanding the 3D
receptive field, and Feature Points Redemption (FPR) for recovering 3D surface
texture information. R2M can be seamlessly integrated into state-of-the-art
LiDAR-based 3D object detectors as preprocessing and achieve appealing
improvement, e.g., 1.39%, 1.67%, and 1.97% mAP improvement on easy, moderate,
and hard difficulty level of KITTI val set, respectively. Based on R2M, we
further propose R2Detector (R2Det) with the Synchronous-Grid RoI Pooling for
accurate box refinement. R2Det outperforms existing range-view-based methods by
a significant margin on both the KITTI benchmark and the Waymo Open Dataset.
Codes will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12907">GridMM: Grid Memory Map for Vision-and-Language Navigation. (arXiv:2307.12907v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiahao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yeqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shuqiang Jiang</a></p>
<p>Vision-and-language navigation (VLN) enables the agent to navigate to a
remote location following the natural language instruction in 3D environments.
To represent the previously visited environment, most approaches for VLN
implement memory using recurrent states, topological maps, or top-down semantic
maps. In contrast to these approaches, we build the top-down egocentric and
dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited
environment. From a global perspective, historical observations are projected
into a unified grid map in a top-down view, which can better represent the
spatial relations of the environment. From a local perspective, we further
propose an instruction relevance aggregation method to capture fine-grained
visual clues in each grid region. Extensive experiments are conducted on both
the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE
dataset in the continuous environments, showing the superiority of our proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13226">Strivec: Sparse Tri-Vector Radiance Fields. (arXiv:2307.13226v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Quankai Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiangeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1">Ulrich Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zexiang Xu</a></p>
<p>We propose Strivec, a novel neural representation that models a 3D scene as a
radiance field with sparsely distributed and compactly factorized local tensor
feature grids. Our approach leverages tensor decomposition, following the
recent work TensoRF, to model the tensor grids. In contrast to TensoRF which
uses a global tensor and focuses on their vector-matrix decomposition, we
propose to utilize a cloud of local tensors and apply the classic
CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple
vectors that express local feature distributions along spatial axes and
compactly encode a local neural field. We also apply multi-scale tensor grids
to discover the geometry and appearance commonalities and exploit spatial
coherence with the tri-vector factorization at multiple local scales. The final
radiance field properties are regressed by aggregating neural features from
multiple local tensors across all scales. Our tri-vector tensors are sparsely
distributed around the actual scene surface, discovered by a fast coarse
reconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our
model can achieve better rendering quality while using significantly fewer
parameters than previous methods, including TensoRF and Instant-NGP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03374">Heterogeneous Forgetting Compensation for Class-Incremental Learning. (arXiv:2308.03374v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiahua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Wenqi Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1">Yang Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Gan Sun</a></p>
<p>Class-incremental learning (CIL) has achieved remarkable successes in
learning new classes consecutively while overcoming catastrophic forgetting on
old categories. However, most existing CIL methods unreasonably assume that all
old categories have the same forgetting pace, and neglect negative influence of
forgetting heterogeneity among different old classes on forgetting
compensation. To surmount the above challenges, we develop a novel
Heterogeneous Forgetting Compensation (HFC) model, which can resolve
heterogeneous forgetting of easy-to-forget and hard-to-forget old categories
from both representation and gradient aspects. Specifically, we design a
task-semantic aggregation block to alleviate heterogeneous forgetting from
representation aspect. It aggregates local category information within each
task to learn task-shared global representations. Moreover, we develop two
novel plug-and-play losses: a gradient-balanced forgetting compensation loss
and a gradient-balanced relation distillation loss to alleviate forgetting from
gradient aspect. They consider gradient-balanced compensation to rectify
forgetting heterogeneity of old categories and heterogeneous relation
consistency. Experiments on several representative datasets illustrate
effectiveness of our HFC model. The code is available at
https://github.com/JiahuaDong/HFC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04928">GeodesicPSIM: Predicting the Quality of Static Mesh with Texture Map via Geodesic Patch Similarity. (arXiv:2308.04928v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1">Joel Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaozhong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shan Liu</a></p>
<p>Static meshes with texture maps have attracted considerable attention in both
industrial manufacturing and academic research, leading to an urgent
requirement for effective and robust objective quality evaluation. However,
current model-based static mesh quality metrics have obvious limitations: most
of them only consider geometry information, while color information is ignored,
and they have strict constraints for the meshes' geometrical topology. Other
metrics, such as image-based and point-based metrics, are easily influenced by
the prepossessing algorithms, e.g., projection and sampling, hampering their
ability to perform at their best. In this paper, we propose Geodesic Patch
Similarity (GeodesicPSIM), a novel model-based metric to accurately predict
human perception quality for static meshes. After selecting a group keypoints,
1-hop geodesic patches are constructed based on both the reference and
distorted meshes cleaned by an effective mesh cleaning algorithm. A two-step
patch cropping algorithm and a patch texture mapping module refine the size of
1-hop geodesic patches and build the relationship between the mesh geometry and
color information, resulting in the generation of 1-hop textured geodesic
patches. Three types of features are extracted to quantify the distortion:
patch color smoothness, patch discrete mean curvature, and patch pixel color
average and variance. To the best of our knowledge, GeodesicPSIM is the first
model-based metric especially designed for static meshes with texture maps.
GeodesicPSIM provides state-of-the-art performance in comparison with
image-based, point-based, and video-based metrics on a newly created and
challenging database. We also prove the robustness of GeodesicPSIM by
introducing different settings of hyperparameters. Ablation studies also
exhibit the effectiveness of three proposed features and the patch cropping
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05983">Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Siyuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Aishan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Lihua Jing</a></p>
<p>Billions of people are sharing their daily live images on social media
everyday. However, malicious collectors use deep face recognition systems to
easily steal their biometric information (e.g., faces) from these images. Some
studies are being conducted to generate encrypted face photos using adversarial
attacks by introducing imperceptible perturbations to reduce face information
leakage. However, existing studies need stronger black-box scenario feasibility
and more natural visual appearances, which challenge the feasibility of privacy
protection. To address these problems, we propose a frequency-restricted
identity-agnostic (FRIA) framework to encrypt face images from unauthorized
face recognition without access to personal information. As for the weak
black-box scenario feasibility, we obverse that representations of the average
feature in multiple face recognition models are similar, thus we propose to
utilize the average feature via the crawled dataset from the Internet as the
target to guide the generation, which is also agnostic to identities of unknown
face recognition systems; in nature, the low-frequency perturbations are more
visually perceptible by the human vision system. Inspired by this, we restrict
the perturbation in the low-frequency facial regions by discrete cosine
transform to achieve the visual naturalness guarantee. Extensive experiments on
several face recognition models demonstrate that our FRIA outperforms other
state-of-the-art methods in generating more natural encrypted faces while
attaining high black-box attack success rates of 96%. In addition, we validate
the efficacy of FRIA using real-world black-box commercial API, which reveals
the potential of FRIA in practice. Our codes can be found in
https://github.com/XinDong10/FRIA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06534">Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolf_D/0/1/0/all/0/1">Daniel Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Payer_T/0/1/0/all/0/1">Tristan Payer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1">Catharina Silvia Lisson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1">Christoph Gerhard Lisson</a>, <a href="http://arxiv.org/find/cs/1/au:+Beer_M/0/1/0/all/0/1">Meinrad Beer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1">Timo Ropinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotz_M/0/1/0/all/0/1">Michael G&#xf6;tz</a></p>
<p>Deep learning in medical imaging has the potential to minimize the risk of
diagnostic errors, reduce radiologist workload, and accelerate diagnosis.
Training such deep learning models requires large and accurate datasets, with
annotations for all training samples. However, in the medical imaging domain,
annotated datasets for specific tasks are often small due to the high
complexity of annotations, limited access, or the rarity of diseases. To
address this challenge, deep learning models can be pre-trained on large image
datasets without annotations using methods from the field of self-supervised
learning. After pre-training, small annotated datasets are sufficient to
fine-tune the models for a specific task. The most popular self-supervised
pre-training approaches in medical imaging are based on contrastive learning.
However, recent studies in natural image processing indicate a strong potential
for masked autoencoder approaches. Our work compares state-of-the-art
contrastive learning methods with the recently introduced masked autoencoder
approach "SparK" for convolutional neural networks (CNNs) on medical images.
Therefore we pre-train on a large unannotated CT image dataset and fine-tune on
several CT classification tasks. Due to the challenge of obtaining sufficient
annotated training data in medical imaging, it is of particular interest to
evaluate how the self-supervised pre-training methods perform when fine-tuning
on small datasets. By experimenting with gradually reducing the training
dataset size for fine-tuning, we find that the reduction has different effects
depending on the type of pre-training chosen. The SparK pre-training method is
more robust to the training dataset size than the contrastive methods. Based on
our results, we propose the SparK pre-training for medical imaging tasks with
only small annotated datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08730">Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration. (arXiv:2308.08730v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jinshan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhixun Su</a></p>
<p>Recent years have witnessed the remarkable performance of diffusion models in
various vision tasks. However, for image restoration that aims to recover clear
images with sharper details from given degraded observations, diffusion-based
methods may fail to recover promising results due to inaccurate noise
estimation. Moreover, simple constraining noises cannot effectively learn
complex degradation information, which subsequently hinders the model capacity.
To solve the above problems, we propose a coarse-to-fine diffusion Transformer
(C2F-DFT) for image restoration. Specifically, our C2F-DFT contains diffusion
self-attention (DFSA) and diffusion feed-forward network (DFN) within a new
coarse-to-fine training scheme. The DFSA and DFN respectively capture the
long-range diffusion dependencies and learn hierarchy diffusion representation
to facilitate better restoration. In the coarse training stage, our C2F-DFT
estimates noises and then generates the final clean image by a sampling
algorithm. To further improve the restoration quality, we propose a simple yet
effective fine training scheme. It first exploits the coarse-trained diffusion
model with fixed steps to generate restoration results, which then would be
constrained with corresponding ground-truth ones to optimize the models to
remedy the unsatisfactory results affected by inaccurate noise estimation.
Extensive experiments show that C2F-DFT significantly outperforms
diffusion-based restoration method IR-SDE and achieves competitive performance
compared with Transformer-based state-of-the-art methods on $3$ tasks,
including deraining, deblurring, and real denoising. The code is available at
https://github.com/wlydlut/C2F-DFT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08741">MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online Neural RGB-D Reconstruction. (arXiv:2308.08741v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yijie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiazhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhinan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">He Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a></p>
<p>We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstruction
method based on a novel neural implicit representation --
multi-implicit-submap. Different from existing neural RGB-D reconstruction
methods lacking either flexibility with a single neural map or scalability due
to extra storage of feature grids, we propose a pure neural representation
tackling both difficulties with a divide-and-conquer design. In our method,
neural submaps are incrementally allocated alongside the scanning trajectory
and efficiently learned with local neural bundle adjustments. The submaps can
be refined individually in a back-end optimization and optimized jointly to
realize submap-level loop closure. Meanwhile, we propose a hybrid tracking
approach combining randomized and gradient-based pose optimizations. For the
first time, randomized optimization is made possible in neural tracking with
several key designs to the learning process, enabling efficient and robust
tracking even under fast camera motions. The extensive evaluation demonstrates
that our method attains higher reconstruction quality than the state of the
arts for large-scale scenes and under fast camera motions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10438">Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks. (arXiv:2308.10438v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaixin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xue Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoli Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weisi Lin</a></p>
<p>In this paper, we propose a novel layer-adaptive weight-pruning approach for
Deep Neural Networks (DNNs) that addresses the challenge of optimizing the
output distortion minimization while adhering to a target pruning ratio
constraint. Our approach takes into account the collective influence of all
layers to design a layer-adaptive pruning scheme. We discover and utilize a
very important additivity property of output distortion caused by pruning
weights on multiple layers. This property enables us to formulate the pruning
as a combinatorial optimization problem and efficiently solve it through
dynamic programming. By decomposing the problem into sub-problems, we achieve
linear time complexity, making our optimization algorithm fast and feasible to
run on CPUs. Our extensive experiments demonstrate the superiority of our
approach over existing methods on the ImageNet and CIFAR-10 datasets. On
CIFAR-10, our method achieves remarkable improvements, outperforming others by
up to 1.0% for ResNet-32, 0.5% for VGG-16, and 0.7% for DenseNet-121 in terms
of top-1 accuracy. On ImageNet, we achieve up to 4.7% and 4.6% higher top-1
accuracy compared to other methods for VGG-16 and ResNet-50, respectively.
These results highlight the effectiveness and practicality of our approach for
enhancing DNN performance through layer-adaptive weight pruning. Code will be
available on https://github.com/Akimoto-Cris/RD_VIT_PRUNE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10677">Visual Crowd Analysis: Open Research Problems. (arXiv:2308.10677v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Muhammad Asif Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1">Hamid Menouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1">Ridha Hamila</a></p>
<p>Over the last decade, there has been a remarkable surge in interest in
automated crowd monitoring within the computer vision community. Modern
deep-learning approaches have made it possible to develop fully-automated
vision-based crowd-monitoring applications. However, despite the magnitude of
the issue at hand, the significant technological advancements, and the
consistent interest of the research community, there are still numerous
challenges that need to be overcome. In this article, we delve into six major
areas of visual crowd analysis, emphasizing the key developments in each of
these areas. We outline the crucial unresolved issues that must be tackled in
future works, in order to ensure that the field of automated crowd monitoring
continues to progress and thrive. Several surveys related to this topic have
been conducted in the past. Nonetheless, this article thoroughly examines and
presents a more intuitive categorization of works, while also depicting the
latest breakthroughs within the field, incorporating more recent studies
carried out within the last few years in a concise manner. By carefully
choosing prominent works with significant contributions in terms of novelty or
performance gains, this paper presents a more comprehensive exposition of
advancements in the current state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11381">DALNet: A Rail Detection Network Based on Dynamic Anchor Line. (arXiv:2308.11381v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zichen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Quanli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liyong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiaoguang Zhao</a></p>
<p>Rail detection is one of the key factors for intelligent train. In the paper,
motivated by the anchor line-based lane detection methods, we propose a rail
detection network called DALNet based on dynamic anchor line. Aiming to solve
the problem that the predefined anchor line is image agnostic, we design a
novel dynamic anchor line mechanism. It utilizes a dynamic anchor line
generator to dynamically generate an appropriate anchor line for each rail
instance based on the position and shape of the rails in the input image. These
dynamically generated anchor lines can be considered as better position
references to accurately localize the rails than the predefined anchor lines.
In addition, we present a challenging urban rail detection dataset DL-Rail with
high-quality annotations and scenario diversity. DL-Rail contains 7000 pairs of
images and annotations along with scene tags, and it is expected to encourage
the development of rail detection. We extensively compare DALNet with many
competitive lane methods. The results show that our DALNet achieves
state-of-the-art performance on our DL-Rail rail detection dataset and the
popular Tusimple and LLAMAS lane detection benchmarks. The code will be
released at https://github.com/Yzichen/mmLaneDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11471">Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bong_H/0/1/0/all/0/1">Haechan Mark Bong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Azambuja_R/0/1/0/all/0/1">Ricardo de Azambuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Beltrame_G/0/1/0/all/0/1">Giovanni Beltrame</a></p>
<p>This work targets what we consider to be the foundational step for urban
airborne robots, a safe landing. Our attention is directed toward what we deem
the most crucial aspect of the safe landing perception stack: segmentation. We
present a streamlined reactive UAV system that employs visual servoing by
harnessing the capabilities of open vocabulary image segmentation. This
approach can adapt to various scenarios with minimal adjustments, bypassing the
necessity for extensive data accumulation for refining internal models, thanks
to its open vocabulary methodology. Given the limitations imposed by local
authorities, our primary focus centers on operations originating from altitudes
of 100 meters. This choice is deliberate, as numerous preceding works have
dealt with altitudes up to 30 meters, aligning with the capabilities of small
stereo cameras. Consequently, we leave the remaining 20m to be navigated using
conventional 3D path planning methods. Utilizing monocular cameras and image
segmentation, our findings demonstrate the system's capability to successfully
execute landing maneuvers at altitudes as low as 20 meters. However, this
approach is vulnerable to intermittent and occasionally abrupt fluctuations in
the segmentation between frames in a video stream. To address this challenge,
we enhance the image segmentation output by introducing what we call a dynamic
focus: a masking mechanism that self adjusts according to the current landing
stage. This dynamic focus guides the control system to avoid regions beyond the
drone's safety radius projected onto the ground, thus mitigating the problems
with fluctuations. Through the implementation of this supplementary layer, our
experiments have reached improvements in the landing success rate of almost
tenfold when compared to global segmentation. All the source code is open
source and available online (github.com/MISTLab/DOVESEI).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11877">Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach. (arXiv:2308.11877v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1">Yash Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1">Tirth Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1">Mrinal Kanti Dhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Taiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niezgoda_J/0/1/0/all/0/1">Jeffrey Niezgoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1">Sandeep Gopalakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zeyun Yu</a></p>
<p>The global burden of acute and chronic wounds presents a compelling case for
enhancing wound classification methods, a vital step in diagnosing and
determining optimal treatments. Recognizing this need, we introduce an
innovative multi-modal network based on a deep convolutional neural network for
categorizing wounds into four categories: diabetic, pressure, surgical, and
venous ulcers. Our multi-modal network uses wound images and their
corresponding body locations for more precise classification. A unique aspect
of our methodology is incorporating a body map system that facilitates accurate
wound location tagging, improving upon traditional wound image classification
techniques. A distinctive feature of our approach is the integration of models
such as VGG16, ResNet152, and EfficientNet within a novel architecture. This
architecture includes elements like spatial and channel-wise
Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated
Multi-Layer Perceptron, providing a robust foundation for classification. Our
multi-modal network was trained and evaluated on two distinct datasets
comprising relevant images and corresponding location information. Notably, our
proposed network outperformed traditional methods, reaching an accuracy range
of 74.79% to 100% for Region of Interest (ROI) without location
classifications, 73.98% to 100% for ROI with location classifications, and
78.10% to 100% for whole image classifications. This marks a significant
enhancement over previously reported performance metrics in the literature. Our
results indicate the potential of our multi-modal network as an effective
decision-support tool for wound image classification, paving the way for its
application in various clinical contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11909">Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data. (arXiv:2308.11909v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Cheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiayi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lijuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Ping Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Honghan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Ying Tan</a></p>
<p>Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial
dependence between different brain regions, and the graph pooling operator in
GCNs is key to enhancing the representation learning capability and acquiring
abnormal brain maps. However, the majority of existing research designs graph
pooling operators only from the perspective of nodes while disregarding the
original edge features, in a way that not only confines graph pooling
application scenarios, but also diminishes its ability to capture critical
substructures. In this study, a clustering graph pooling method that first
supports multidimensional edge features, called Edge-aware hard clustering
graph pooling (EHCPool), is developed. EHCPool proposes the first
'Edge-to-node' score evaluation criterion based on edge features to assess node
feature significance. To more effectively capture the critical subgraphs, a
novel Iteration n-top strategy is further designed to adaptively learn sparse
hard clustering assignments for graphs. Subsequently, an innovative N-E
Aggregation strategy is presented to aggregate node and edge feature
information in each independent subgraph. The proposed model was evaluated on
multi-site brain imaging public datasets and yielded state-of-the-art
performance. We believe this method is the first deep learning tool with the
potential to probe different types of abnormal functional brain networks from
data-driven perspective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11911">ACLS: Adaptive and Conditional Label Smoothing for Network Calibration. (arXiv:2308.11911v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1">Hyekang Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1">Jongyoun Noh</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1">Youngmin Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_D/0/1/0/all/0/1">Donghyeon Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1">Bumsub Ham</a></p>
<p>We address the problem of network calibration adjusting miscalibrated
confidences of deep neural networks. Many approaches to network calibration
adopt a regularization-based method that exploits a regularization term to
smooth the miscalibrated confidences. Although these approaches have shown the
effectiveness on calibrating the networks, there is still a lack of
understanding on the underlying principles of regularization in terms of
network calibration. We present in this paper an in-depth analysis of existing
regularization-based methods, providing a better understanding on how they
affect to network calibration. Specifically, we have observed that 1) the
regularization-based methods can be interpreted as variants of label smoothing,
and 2) they do not always behave desirably. Based on the analysis, we introduce
a novel loss function, dubbed ACLS, that unifies the merits of existing
regularization methods, while avoiding the limitations. We show extensive
experimental results for image classification and semantic segmentation on
standard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL
VOC, demonstrating the effectiveness of our loss function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12213">CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hualiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huifeng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaomeng Li</a></p>
<p>Out-of-distribution (OOD) detection refers to training the model on an
in-distribution (ID) dataset to classify whether the input images come from
unknown classes. Considerable effort has been invested in designing various OOD
detection methods based on either convolutional neural networks or
transformers. However, zero-shot OOD detection methods driven by CLIP, which
only require class names for ID, have received less attention. This paper
presents a novel method, namely CLIP saying no (CLIPN), which empowers the
logic of saying no within CLIP. Our key motivation is to equip CLIP with the
capability of distinguishing OOD and ID samples using positive-semantic prompts
and negation-semantic prompts. Specifically, we design a novel learnable no
prompt and a no text encoder to capture negation semantics within images.
Subsequently, we introduce two loss functions: the image-text binary-opposite
loss and the text semantic-opposite loss, which we use to teach CLIPN to
associate images with no prompts, thereby enabling it to identify unknown
samples. Furthermore, we propose two threshold-free inference algorithms to
perform OOD detection by utilizing negation semantics from no prompts and the
text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6
OOD datasets) for the OOD detection task demonstrate that CLIPN, based on
ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in
terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN
can serve as a solid foundation for effectively leveraging CLIP in downstream
OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1910.09642">The SWAX Benchmark: Attacking Biometric Systems with Wax Figures. (arXiv:1910.09642v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1">Rafael Henrique Vareto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandanha_A/0/1/0/all/0/1">Araceli Marcia Sandanha</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1">William Robson Schwartz</a></p>
<p>A face spoofing attack occurs when an intruder attempts to impersonate
someone who carries a gainful authentication clearance. It is a trending topic
due to the increasing demand for biometric authentication on mobile devices,
high-security areas, among others. This work introduces a new database named
Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images
and videos that endorse the problem of face spoofing detection. The dataset
consists of more than 1800 face images and 110 videos of 55 people/waxworks,
arranged in training, validation and test sets with a large range in
expression, illumination and pose variations. Experiments performed with
baseline methods show that despite the progress in recent years, advanced
spoofing methods are still vulnerable to high-quality violation attempts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07445">Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1">Rafael Henrique Vareto</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1">William Robson Schwartz</a></p>
<p>Open-set face recognition describes a scenario where unknown subjects, unseen
during the training stage, appear on test time. Not only it requires methods
that accurately identify individuals of interest, but also demands approaches
that effectively deal with unfamiliar faces. This work details a scalable
open-set face identification approach to galleries composed of hundreds and
thousands of subjects. It is composed of clustering and an ensemble of binary
learning algorithms that estimates when query face samples belong to the face
gallery and then retrieves their correct identity. The approach selects the
most suitable gallery subjects and uses the ensemble to improve prediction
performance. We carry out experiments on well-known LFW and YTF benchmarks.
Results show that competitive performance can be achieved even when targeting
scalability.
</p>
</p>
</div>

    </div>
    </body>
    