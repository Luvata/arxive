<!DOCTYPE html>
<html>
<head>
<title>2023-10-09-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2310.03757">Enhancing Healthcare with EOG: A Novel Approach to Sleep Stage Classification. (arXiv:2310.03757v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maiti_S/0/1/0/all/0/1">Suvadeep Maiti</a>, <a href="http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1">Shivam Kumar Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Bapi_R/0/1/0/all/0/1">Raju S. Bapi</a></p>
<p>We introduce an innovative approach to automated sleep stage classification
using EOG signals, addressing the discomfort and impracticality associated with
EEG data acquisition. In addition, it is important to note that this approach
is untapped in the field, highlighting its potential for novel insights and
contributions. Our proposed SE-Resnet-Transformer model provides an accurate
classification of five distinct sleep stages from raw EOG signal. Extensive
validation on publically available databases (SleepEDF-20, SleepEDF-78, and
SHHS) reveals noteworthy performance, with macro-F1 scores of 74.72, 70.63, and
69.26, respectively. Our model excels in identifying REM sleep, a crucial
aspect of sleep disorder investigations. We also provide insight into the
internal mechanisms of our model using techniques such as 1D-GradCAM and t-SNE
plots. Our method improves the accessibility of sleep stage classification
while decreasing the need for EEG modalities. This development will have
promising implications for healthcare and the incorporation of wearable
technology into sleep studies, thereby advancing the field's potential for
enhanced diagnostics and patient comfort.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03773">Functional data learning using convolutional neural networks. (arXiv:2310.03773v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galarza_J/0/1/0/all/0/1">Jose Galarza</a>, <a href="http://arxiv.org/find/cs/1/au:+Oraby_T/0/1/0/all/0/1">Tamer Oraby</a></p>
<p>In this paper, we show how convolutional neural networks (CNN) can be used in
regression and classification learning problems of noisy and non-noisy
functional data. The main idea is to transform the functional data into a 28 by
28 image. We use a specific but typical architecture of a convolutional neural
network to perform all the regression exercises of parameter estimation and
functional form classification. First, we use some functional case studies of
functional data with and without random noise to showcase the strength of the
new method. In particular, we use it to estimate exponential growth and decay
rates, the bandwidths of sine and cosine functions, and the magnitudes and
widths of curve peaks. We also use it to classify the monotonicity and
curvatures of functional data, algebraic versus exponential growth, and the
number of peaks of functional data. Second, we apply the same convolutional
neural networks to Lyapunov exponent estimation in noisy and non-noisy chaotic
data, in estimating rates of disease transmission from epidemic curves, and in
detecting the similarity of drug dissolution profiles. Finally, we apply the
method to real-life data to detect Parkinson's disease patients in a
classification problem. The method, although simple, shows high accuracy and is
promising for future use in engineering and medical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03821">WLST: Weak Labels Guided Self-training for Weakly-supervised Domain Adaptation on 3D Object Detection. (arXiv:2310.03821v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsou_T/0/1/0/all/0/1">Tsung-Lin Tsou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tsung-Han Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">Winston H. Hsu</a></p>
<p>In the field of domain adaptation (DA) on 3D object detection, most of the
work is dedicated to unsupervised domain adaptation (UDA). Yet, without any
target annotations, the performance gap between the UDA approaches and the
fully-supervised approach is still noticeable, which is impractical for
real-world applications. On the other hand, weakly-supervised domain adaptation
(WDA) is an underexplored yet practical task that only requires few labeling
effort on the target domain. To improve the DA performance in a cost-effective
way, we propose a general weak labels guided self-training framework, WLST,
designed for WDA on 3D object detection. By incorporating autolabeler, which
can generate 3D pseudo labels from 2D bounding boxes, into the existing
self-training pipeline, our method is able to generate more robust and
consistent pseudo labels that would benefit the training process on the target
domain. Extensive experiments demonstrate the effectiveness, robustness, and
detector-agnosticism of our WLST framework. Notably, it outperforms previous
state-of-the-art methods on all evaluation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03827">Integrating Audio-Visual Features for Multimodal Deepfake Detection. (arXiv:2310.03827v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muppalla_S/0/1/0/all/0/1">Sneha Muppalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1">Shan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Siwei Lyu</a></p>
<p>Deepfakes are AI-generated media in which an image or video has been
digitally modified. The advancements made in deepfake technology have led to
privacy and security issues. Most deepfake detection techniques rely on the
detection of a single modality. Existing methods for audio-visual detection do
not always surpass that of the analysis based on single modalities. Therefore,
this paper proposes an audio-visual-based method for deepfake detection, which
integrates fine-grained deepfake identification with binary classification. We
categorize the samples into four types by combining labels specific to each
single modality. This method enhances the detection under intra-domain and
cross-domain testing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03843">Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks. (arXiv:2310.03843v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xu Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Difan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lianli Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zenglin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a></p>
<p>Transferring a pretrained model to a downstream task can be as easy as
conducting linear probing with target data, that is, training a linear
classifier upon frozen features extracted from the pretrained model. As there
may exist significant gaps between pretraining and downstream datasets, one may
ask whether all dimensions of the pretrained features are useful for a given
downstream task. We show that, for linear probing, the pretrained features can
be extremely redundant when the downstream data is scarce, or few-shot. For
some cases such as 5-way 1-shot tasks, using only 1\% of the most important
feature dimensions is able to recover the performance achieved by using the
full representation. Interestingly, most dimensions are redundant only under
few-shot settings and gradually become useful when the number of shots
increases, suggesting that feature redundancy may be the key to characterizing
the "few-shot" nature of few-shot transfer problems. We give a theoretical
understanding of this phenomenon and show how dimensions with high variance and
small distance between class centroids can serve as confounding factors that
severely disturb classification results under few-shot settings. As an attempt
at solving this problem, we find that the redundant features are difficult to
identify accurately with a small number of training samples, but we can instead
adjust feature magnitude with a soft mask based on estimated feature
importance. We show that this method can generally improve few-shot transfer
performance across various pretrained models and downstream datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03848">OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning. (arXiv:2310.03848v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiawen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Grohnfeldt_C/0/1/0/all/0/1">Claas Grohnfeldt</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1">Odej Kao</a></p>
<p>In most works on deep incremental learning research, it is assumed that novel
samples are pre-identified for neural network retraining. However, practical
deep classifiers often misidentify these samples, leading to erroneous
predictions. Such misclassifications can degrade model performance. Techniques
like open set recognition offer a means to detect these novel samples,
representing a significant area in the machine learning domain.
</p>
<p>In this paper, we introduce a deep class-incremental learning framework
integrated with open set recognition. Our approach refines class-incrementally
learned features to adapt them for distance-based open set recognition.
Experimental results validate that our method outperforms state-of-the-art
incremental learning techniques and exhibits superior performance in open set
recognition compared to baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03870">Consistency Regularization Improves Placenta Segmentation in Fetal EPI MRI Time Series. (arXiv:2310.03870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yingcheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Karani_N/0/1/0/all/0/1">Neerav Karani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1">Neel Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Abulnaga_S/0/1/0/all/0/1">S. Mazdak Abulnaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Junshen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_P/0/1/0/all/0/1">P. Ellen Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Turk_E/0/1/0/all/0/1">Esra Abaci Turk</a>, <a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1">Polina Golland</a></p>
<p>The placenta plays a crucial role in fetal development. Automated 3D placenta
segmentation from fetal EPI MRI holds promise for advancing prenatal care. This
paper proposes an effective semi-supervised learning method for improving
placenta segmentation in fetal EPI MRI time series. We employ consistency
regularization loss that promotes consistency under spatial transformation of
the same image and temporal consistency across nearby images in a time series.
The experimental results show that the method improves the overall segmentation
accuracy and provides better performance for outliers and hard samples. The
evaluation also indicates that our method improves the temporal coherency of
the prediction, which could lead to more accurate computation of temporal
placental biomarkers. This work contributes to the study of the placenta and
prenatal clinical decision-making. Code is available at
https://github.com/firstmover/cr-seg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03872">FNOSeg3D: Resolution-Robust 3D Image Segmentation with Fourier Neural Operator. (arXiv:2310.03872v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1">Ken C. L. Wong</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Hongzhi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1">Tanveer Syeda-Mahmood</a></p>
<p>Due to the computational complexity of 3D medical image segmentation,
training with downsampled images is a common remedy for out-of-memory errors in
deep learning. Nevertheless, as standard spatial convolution is sensitive to
variations in image resolution, the accuracy of a convolutional neural network
trained with downsampled images can be suboptimal when applied on the original
resolution. To address this limitation, we introduce FNOSeg3D, a 3D
segmentation model robust to training image resolution based on the Fourier
neural operator (FNO). The FNO is a deep learning framework for learning
mappings between functions in partial differential equations, which has the
appealing properties of zero-shot super-resolution and global receptive field.
We improve the FNO by reducing its parameter requirement and enhancing its
learning capability through residual connections and deep supervision, and
these result in our FNOSeg3D model which is parameter efficient and resolution
robust. When tested on the BraTS'19 dataset, it achieved superior robustness to
training image resolution than other tested models with less than 1% of their
model parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03890">Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Veluswami_P/0/1/0/all/0/1">Praveen Raj Veluswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_H/0/1/0/all/0/1">Harsh Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1">Sathya N. Ravi</a></p>
<p>Many neural networks deployed in the real world scenarios are trained using
cross entropy based loss functions. From the optimization perspective, it is
known that the behavior of first order methods such as gradient descent
crucially depend on the separability of datasets. In fact, even in the most
simplest case of binary classification, the rate of convergence depends on two
factors: (1) condition number of data matrix, and (2) separability of the
dataset. With no further pre-processing techniques such as
over-parametrization, data augmentation etc., separability is an intrinsic
quantity of the data distribution under consideration. We focus on the
landscape design of the logistic function and derive a novel sequence of {\em
strictly} convex functions that are at least as strict as logistic loss. The
minimizers of these functions coincide with those of the minimum norm solution
wherever possible. The strict convexity of the derived function can be extended
to finetune state-of-the-art models and applications. In empirical experimental
analysis, we apply our proposed rooted logistic objective to multiple deep
models, e.g., fully-connected neural networks and transformers, on various of
classification benchmarks. Our results illustrate that training with rooted
loss function is converged faster and gains performance improvements.
Furthermore, we illustrate applications of our novel rooted loss function in
generative modeling based downstream applications, such as finetuning StyleGAN
model with the rooted loss. The code implementing our losses and models can be
found here for open source software development purposes:
https://anonymous.4open.science/r/rooted_loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03893">Characterizing the Features of Mitotic Figures Using a Conditional Diffusion Probabilistic Model. (arXiv:2310.03893v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bahadir_C/0/1/0/all/0/1">Cagla Deniz Bahadir</a>, <a href="http://arxiv.org/find/cs/1/au:+Liechty_B/0/1/0/all/0/1">Benjamin Liechty</a>, <a href="http://arxiv.org/find/cs/1/au:+Pisapia_D/0/1/0/all/0/1">David J. Pisapia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1">Mert R. Sabuncu</a></p>
<p>Mitotic figure detection in histology images is a hard-to-define, yet
clinically significant task, where labels are generated with pathologist
interpretations and where there is no ``gold-standard'' independent
ground-truth. However, it is well-established that these interpretation based
labels are often unreliable, in part, due to differences in expertise levels
and human subjectivity. In this paper, our goal is to shed light on the
inherent uncertainty of mitosis labels and characterize the mitotic figure
classification task in a human interpretable manner. We train a probabilistic
diffusion model to synthesize patches of cell nuclei for a given mitosis label
condition. Using this model, we can then generate a sequence of synthetic
images that correspond to the same nucleus transitioning into the mitotic
state. This allows us to identify different image features associated with
mitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity
and high contrast between the nucleus and the cell body. Our approach offers a
new tool for pathologists to interpret and communicate the features driving the
decision to recognize a mitotic figure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03895">TWICE Dataset: Digital Twin of Test Scenarios in a Controlled Environment. (arXiv:2310.03895v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neto_L/0/1/0/all/0/1">Leonardo Novicki Neto</a>, <a href="http://arxiv.org/find/cs/1/au:+Reway_F/0/1/0/all/0/1">Fabio Reway</a>, <a href="http://arxiv.org/find/cs/1/au:+Poledna_Y/0/1/0/all/0/1">Yuri Poledna</a>, <a href="http://arxiv.org/find/cs/1/au:+Drechsler_M/0/1/0/all/0/1">Maikol Funk Drechsler</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_E/0/1/0/all/0/1">Eduardo Parente Ribeiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_W/0/1/0/all/0/1">Werner Huber</a>, <a href="http://arxiv.org/find/cs/1/au:+Icking_C/0/1/0/all/0/1">Christian Icking</a></p>
<p>Ensuring the safe and reliable operation of autonomous vehicles under adverse
weather remains a significant challenge. To address this, we have developed a
comprehensive dataset composed of sensor data acquired in a real test track and
reproduced in the laboratory for the same test scenarios. The provided dataset
includes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data
recorded under adverse weather conditions (rainy, night-time, and snowy
conditions). We recorded test scenarios using objects of interest such as car,
cyclist, truck and pedestrian -- some of which are inspired by EURONCAP
(European New Car Assessment Programme). The sensor data generated in the
laboratory is acquired by the execution of simulation-based tests in
hardware-in-the-loop environment with the digital twin of each real test
scenario. The dataset contains more than 2 hours of recording, which totals
more than 280GB of data. Therefore, it is a valuable resource for researchers
in the field of autonomous vehicles to test and improve their algorithms in
adverse weather conditions, as well as explore the simulation-to-reality gap.
The dataset is available for download at: https://twicedataset.github.io/site/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03911">Coloring Deep CNN Layers with Activation Hue Loss. (arXiv:2310.03911v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bouchard_L/0/1/0/all/0/1">Louis-Fran&#xe7;ois Bouchard</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazreg_M/0/1/0/all/0/1">Mohsen Ben Lazreg</a>, <a href="http://arxiv.org/find/cs/1/au:+Toews_M/0/1/0/all/0/1">Matthew Toews</a></p>
<p>This paper proposes a novel hue-like angular parameter to model the structure
of deep convolutional neural network (CNN) activation space, referred to as the
{\em activation hue}, for the purpose of regularizing models for more effective
learning. The activation hue generalizes the notion of color hue angle in
standard 3-channel RGB intensity space to $N$-channel activation space. A
series of observations based on nearest neighbor indexing of activation vectors
with pre-trained networks indicate that class-informative activations are
concentrated about an angle $\theta$ in both the $(x,y)$ image plane and in
multi-channel activation space. A regularization term in the form of hue-like
angular $\theta$ labels is proposed to complement standard one-hot loss.
Training from scratch using combined one-hot + activation hue loss improves
classification performance modestly for a wide variety of classification tasks,
including ImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03923">Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation. (arXiv:2310.03923v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1">Kashu Yamazaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanyu_T/0/1/0/all/0/1">Taisei Hanyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1">Khoa Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Thang Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1">Gianfranco Doretto</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>Precise 3D environmental mapping is pivotal in robotics. Existing methods
often rely on predefined concepts during training or are time-intensive when
generating semantic maps. This paper presents Open-Fusion, a groundbreaking
approach for real-time open-vocabulary 3D mapping and queryable scene
representation using RGB-D data. Open-Fusion harnesses the power of a
pre-trained vision-language foundation model (VLFM) for open-set semantic
comprehension and employs the Truncated Signed Distance Function (TSDF) for
swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based
embeddings and their associated confidence maps. These are then integrated with
3D knowledge from TSDF using an enhanced Hungarian-based feature-matching
mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D
segmentation for open-vocabulary without necessitating additional 3D training.
Benchmark tests on the ScanNet dataset against leading zero-shot methods
highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the
strengths of region-based VLFM and TSDF, facilitating real-time 3D scene
comprehension that includes object concepts and open-world semantics. We
encourage the readers to view the demos on our project page:
https://uark-aicv.github.io/OpenFusion
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03937">Diffusion Models as Masked Audio-Video Learners. (arXiv:2310.03937v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nunez_E/0/1/0/all/0/1">Elvis Nunez</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yanzi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Sachin Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1">Maxwell Horton</a></p>
<p>Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model's performance
in downstream audio-classification tasks when compared to MAViL's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03940">Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1">Fabio Ferreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1">Ivo Rapant</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a></p>
<p>Many Contrastive Learning (CL) methods train their models to be invariant to
different "views" of an image input for which a good data augmentation pipeline
is crucial. While considerable efforts were directed towards improving pre-text
tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax
centering), the majority of these methods remain strongly reliant on the random
sampling of operations within the image augmentation pipeline, such as the
random resized crop or color distortion operation. In this paper, we argue that
the role of the view generation and its effect on performance has so far
received insufficient attention. To address this, we propose an easy,
learning-free, yet powerful Hard View Selection (HVS) strategy designed to
extend the random view generation to expose the pretrained model to harder
samples during CL training. It encompasses the following iterative steps: 1)
randomly sample multiple views and create pairs of two views, 2) run forward
passes for each view pair on the currently trained model, 3) adversarially
select the pair yielding the worst loss, and 4) run the backward pass with the
selected pair. In our empirical analysis we show that under the hood, HVS
increases task difficulty by controlling the Intersection over Union of views
during pretraining. With only 300-epoch pretraining, HVS is able to closely
rival the 800-epoch DINO baseline which remains very favorable even when
factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.55% and 1.9% on linear evaluation and similar improvements on
transfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03952">ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis. (arXiv:2310.03952v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiali Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Youngkyoon Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Papaioannou_A/0/1/0/all/0/1">Athanasios Papaioannou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampouris_C/0/1/0/all/0/1">Christos Kampouris</a>, <a href="http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1">Rolandos Alexandros Potamias</a>, <a href="http://arxiv.org/find/cs/1/au:+Papantoniou_F/0/1/0/all/0/1">Foivos Paraperas Papantoniou</a>, <a href="http://arxiv.org/find/cs/1/au:+Galanakis_E/0/1/0/all/0/1">Efstathios Galanakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1">Ales Leonardis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1">Stefanos Zafeiriou</a></p>
<p>This paper introduces the Imperial Light-Stage Head (ILSH) dataset, a novel
light-stage-captured human head dataset designed to support view synthesis
academic challenges for human heads. The ILSH dataset is intended to facilitate
diverse approaches, such as scene-specific or generic neural rendering,
multiple-view geometry, 3D vision, and computer graphics, to further advance
the development of photo-realistic human avatars. This paper details the setup
of a light-stage specifically designed to capture high-resolution (4K) human
head images and describes the process of addressing challenges (preprocessing,
ethical issues) in collecting high-quality data. In addition to the data
collection, we address the split of the dataset into train, validation, and
test sets. Our goal is to design and support a fair view synthesis challenge
task for this novel dataset, such that a similar level of performance can be
maintained and expected when using the test set, as when using the validation
set. The ILSH dataset consists of 52 subjects captured using 24 cameras with
all 82 lighting sources turned on, resulting in a total of 1,248 close-up head
images, border masks, and camera pose pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03956">Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction. (arXiv:2310.03956v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fridovich_Keil_S/0/1/0/all/0/1">Sara Fridovich-Keil</a>, <a href="http://arxiv.org/find/cs/1/au:+Valdivia_F/0/1/0/all/0/1">Fabrizio Valdivia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1">Gordon Wetzstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1">Benjamin Recht</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1">Mahdi Soltanolkotabi</a></p>
<p>In computed tomography (CT), the forward model consists of a linear Radon
transform followed by an exponential nonlinearity based on the attenuation of
light according to the Beer-Lambert Law. Conventional reconstruction often
involves inverting this nonlinearity as a preprocessing step and then solving a
convex inverse problem. However, this nonlinear measurement preprocessing
required to use the Radon transform is poorly conditioned in the vicinity of
high-density materials, such as metal. This preprocessing makes CT
reconstruction methods numerically sensitive and susceptible to artifacts near
high-density regions. In this paper, we study a technique where the signal is
directly reconstructed from raw measurements through the nonlinear forward
model. Though this optimization is nonconvex, we show that gradient descent
provably converges to the global optimum at a geometric rate, perfectly
reconstructing the underlying signal with a near minimal number of random
measurements. We also prove similar results in the under-determined setting
where the number of measurements is significantly smaller than the dimension of
the signal. This is achieved by enforcing prior structural information about
the signal through constraints on the optimization variables. We illustrate the
benefits of direct nonlinear CT reconstruction with cone-beam CT experiments on
synthetic and real 3D volumes. We show that this approach reduces metal
artifacts compared to a commercial reconstruction of a human skull with metal
dental crowns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03957">Understanding prompt engineering may not require rethinking generalization. (arXiv:2310.03957v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1">Victor Akinwande</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yiding Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sam_D/0/1/0/all/0/1">Dylan Sam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a></p>
<p>Zero-shot learning in prompted vision-language models, the practice of
crafting prompts to build classifiers without an explicit training process, has
achieved impressive performance in many settings. This success presents a
seemingly surprising observation: these methods suffer relatively little from
overfitting, i.e., when a prompt is manually engineered to achieve low error on
a given training set (thus rendering the method no longer actually zero-shot),
the approach still performs well on held-out test data. In this paper, we show
that we can explain such performance well via recourse to classical PAC-Bayes
bounds. Specifically, we show that the discrete nature of prompts, combined
with a PAC-Bayes prior given by a language model, results in generalization
bounds that are remarkably tight by the standards of the literature: for
instance, the generalization bound of an ImageNet classifier is often within a
few percentage points of the true test error. We demonstrate empirically that
this holds for existing handcrafted prompts and prompts generated through
simple greedy search. Furthermore, the resulting bound is well-suited for model
selection: the models with the best bound typically also have the best test
performance. This work thus provides a possible justification for the
widespread practice of prompt engineering, even if it seems that such methods
could potentially overfit the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03959">Towards Increasing the Robustness of Predictive Steering-Control Autonomous Navigation Systems Against Dash Cam Image Angle Perturbations Due to Pothole Encounters. (arXiv:2310.03959v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aarya_S/0/1/0/all/0/1">Shivam Aarya</a> (Johns Hopkins University)</p>
<p>Vehicle manufacturers are racing to create autonomous navigation and steering
control algorithms for their vehicles. These software are made to handle
various real-life scenarios such as obstacle avoidance and lane maneuvering.
There is some ongoing research to incorporate pothole avoidance into these
autonomous systems. However, there is very little research on the effect of
hitting a pothole on the autonomous navigation software that uses cameras to
make driving decisions. Perturbations in the camera angle when hitting a
pothole can cause errors in the predicted steering angle. In this paper, we
present a new model to compensate for such angle perturbations and reduce any
errors in steering control prediction algorithms. We evaluate our model on
perturbations of publicly available datasets and show our model can reduce the
errors in the estimated steering angle from perturbed images to 2.3%, making
autonomous steering control robust against the dash cam image angle
perturbations induced when one wheel of a car goes over a pothole.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03967">Sub-token ViT Embedding via Stochastic Resonance Transformers. (arXiv:2310.03967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1">Dong Lao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yangchao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tian Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1">Alex Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We discover the presence of quantization artifacts in Vision Transformers
(ViTs), which arise due to the image tokenization step inherent in these
architectures. These artifacts result in coarsely quantized features, which
negatively impact performance, especially on downstream dense prediction tasks.
We present a zero-shot method to improve how pre-trained ViTs handle spatial
quantization. In particular, we propose to ensemble the features obtained from
perturbing input images via sub-token spatial translations, inspired by
Stochastic Resonance, a method traditionally applied to climate dynamics and
signal processing. We term our method ``Stochastic Resonance Transformer"
(SRT), which we show can effectively super-resolve features of pre-trained
ViTs, capturing more of the local fine-grained structures that might otherwise
be neglected as a result of tokenization. SRT can be applied at any layer, on
any task, and does not require any fine-tuning. The advantage of the former is
evident when applied to monocular depth prediction, where we show that
ensembling model outputs are detrimental while applying SRT on intermediate ViT
features outperforms the baseline models by an average of 4.7% and 14.9% on the
RMSE and RMSE-log metrics across three different architectures. When applied to
semi-supervised video object segmentation, SRT also improves over the baseline
models uniformly across all metrics, and by an average of 2.4% in F&amp;J score. We
further show that these quantization artifacts can be attenuated to some extent
via self-distillation. On the unsupervised salient region segmentation, SRT
improves upon the base model by an average of 2.1% on the maxF metric. Finally,
despite operating purely on pixel-level features, SRT generalizes to non-dense
prediction tasks such as image retrieval and object discovery, yielding
consistent improvements of up to 2.6% and 1.0% respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03981">CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation. (arXiv:2310.03981v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Weibin Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qingzhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhaozheng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Haoyi Xiong</a></p>
<p>While pre-training on object detection tasks, such as Common Objects in
Contexts (COCO) [1], could significantly boost the performance of cell
segmentation, it still consumes on massive fine-annotated cell images [2] with
bounding boxes, masks, and cell types for every cell in every image, to
fine-tune the pre-trained model. To lower the cost of annotation, this work
considers the problem of pre-training DNN models for few-shot cell
segmentation, where massive unlabeled cell images are available but only a
small proportion is annotated. Hereby, we propose Cross-domain Unsupervised
Pre-training, namely CUPre, transferring the capability of object detection and
instance segmentation for common visual objects (learned from COCO) to the
visual domain of cells using unlabeled images. Given a standard COCO
pre-trained network with backbone, neck, and head modules, CUPre adopts an
alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in
every iteration of pre-training, AMT2 first trains the backbone with cell
images from multiple cell datasets via unsupervised momentum contrastive
learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets
via instance segmentation. After pre-training, CUPre fine-tunes the whole model
on the cell segmentation task using a few annotated images. We carry out
extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4]
datasets in few-shot instance segmentation settings. The experiment shows that
CUPre can outperform existing pre-training methods, achieving the highest
average precision (AP) for few-shot cell segmentation and detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03986">Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1">Md Kaykobad Reza</a>, <a href="http://arxiv.org/find/cs/1/au:+Prater_Bennette_A/0/1/0/all/0/1">Ashley Prater-Bennette</a>, <a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1">M. Salman Asif</a></p>
<p>Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose simple and parameter-efficient adaptation
procedures for pretrained multimodal networks. In particular, we exploit
low-rank adaptation and modulation of intermediate features to compensate for
the missing modalities. We demonstrate that such adaptation can partially
bridge performance drop due to missing modalities and outperform independent,
dedicated networks trained for the available modality combinations in some
cases. The proposed adaptation requires extremely small number of parameters
(e.g., fewer than 0.7% of the total parameters in most experiments). We conduct
a series of experiments to highlight the robustness of our proposed method
using diverse datasets for RGB-thermal and RGB-Depth semantic segmentation,
multimodal material segmentation, and multimodal sentiment analysis tasks. Our
proposed method demonstrates versatility across various tasks and datasets, and
outperforms existing methods for robust multimodal learning with missing
modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04010">Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking. (arXiv:2310.04010v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">YeongHyeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Sungho Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Myung Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yeonho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1">Juneho Yi</a></p>
<p>Anomaly detection (AD) in surface inspection is an essential yet challenging
task in manufacturing due to the quantity imbalance problem of scarce abnormal
data. To overcome the above, a reconstruction encoder-decoder (ED) such as
autoencoder or U-Net which is trained with only anomaly-free samples is widely
adopted, in the hope that unseen abnormals should yield a larger reconstruction
error than normal. Over the past years, researches on self-supervised
reconstruction-by-inpainting have been reported. They mask out suspected
defective regions for inpainting in order to make them invisible to the
reconstruction ED to deliberately cause inaccurate reconstruction for
abnormals. However, their limitation is multiple random masking to cover the
whole input image due to defective regions not being known in advance. We
propose a novel reconstruction-by-inpainting method dubbed Excision and
Recovery (EAR) that features single deterministic masking. For this, we exploit
a pre-trained spatial attention model to predict potential suspected defective
regions that should be masked out. We also employ a variant of U-Net as our ED
to further limit the reconstruction ability of the U-Net model for abnormals,
in which skip connections of different layers can be selectively disabled. In
the training phase, all the skip connections are switched on to fully take the
benefits from the U-Net architecture. In contrast, for inferencing, we only
keep deeper skip connections with shallower connections off. We validate the
effectiveness of EAR using an MNIST pre-trained attention for a commonly used
surface AD dataset, KolektorSDD2. The experimental results show that EAR
achieves both better AD performance and higher throughput than state-of-the-art
methods. We expect that the proposed EAR model can be widely adopted as
training and inference strategies for AD purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04043">In the Blink of an Eye: Event-based Emotion Recognition. (arXiv:2310.04043v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bo Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Peers_P/0/1/0/all/0/1">Pieter Peers</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaopeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1">Felix Heide</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a></p>
<p>We introduce a wearable single-eye emotion recognition device and a real-time
approach to recognizing emotions from partial observations of an emotion that
is robust to changes in lighting conditions. At the heart of our method is a
bio-inspired event-based camera setup and a newly designed lightweight Spiking
Eye Emotion Network (SEEN). Compared to conventional cameras, event-based
cameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher
temporal resolution. Thus, the captured events can encode rich temporal cues
under challenging lighting conditions. However, these events lack texture
information, posing problems in decoding temporal information effectively. SEEN
tackles this issue from two different perspectives. First, we adopt
convolutional spiking layers to take advantage of the spiking neural network's
ability to decode pertinent temporal information. Second, SEEN learns to
extract essential spatial cues from corresponding intensity frames and
leverages a novel weight-copy scheme to convey spatial attention to the
convolutional spiking layers during training and inference. We extensively
validate and demonstrate the effectiveness of our approach on a specially
collected Single-eye Event-based Emotion (SEE) dataset. To the best of our
knowledge, our method is the first eye-based emotion recognition method that
leverages event-based cameras and spiking neural network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04081">A Deeply Supervised Semantic Segmentation Method Based on GAN. (arXiv:2310.04081v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1">Qiyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zeng Zeng</a></p>
<p>In recent years, the field of intelligent transportation has witnessed rapid
advancements, driven by the increasing demand for automation and efficiency in
transportation systems. Traffic safety, one of the tasks integral to
intelligent transport systems, requires accurately identifying and locating
various road elements, such as road cracks, lanes, and traffic signs. Semantic
segmentation plays a pivotal role in achieving this task, as it enables the
partition of images into meaningful regions with accurate boundaries. In this
study, we propose an improved semantic segmentation model that combines the
strengths of adversarial learning with state-of-the-art semantic segmentation
techniques. The proposed model integrates a generative adversarial network
(GAN) framework into the traditional semantic segmentation model, enhancing the
model's performance in capturing complex and subtle features in transportation
images. The effectiveness of our approach is demonstrated by a significant
boost in performance on the road crack dataset compared to the existing
methods, \textit{i.e.,} SEGAN. This improvement can be attributed to the
synergistic effect of adversarial learning and semantic segmentation, which
leads to a more refined and accurate representation of road structures and
conditions. The enhanced model not only contributes to better detection of road
cracks but also to a wide range of applications in intelligent transportation,
such as traffic sign recognition, vehicle detection, and lane segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04086">End-to-End Chess Recognition. (arXiv:2310.04086v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masouris_A/0/1/0/all/0/1">Athanasios Masouris</a>, <a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1">Jan van Gemert</a></p>
<p>Chess recognition refers to the task of identifying the chess pieces
configuration from a chessboard image. Contrary to the predominant approach
that aims to solve this task through the pipeline of chessboard detection,
square localization, and piece classification, we rely on the power of deep
learning models and introduce two novel methodologies to circumvent this
pipeline and directly predict the chessboard configuration from the entire
image. In doing so, we avoid the inherent error accumulation of the sequential
approaches and the need for intermediate annotations. Furthermore, we introduce
a new dataset, Chess Recognition Dataset (ChessReD), specifically designed for
chess recognition that consists of 10,800 images and their corresponding
annotations. In contrast to existing synthetic datasets with limited angles,
this dataset comprises a diverse collection of real images of chess formations
captured from various angles using smartphone cameras; a sensor choice made to
ensure real-world applicability. We use this dataset to both train our model
and evaluate and compare its performance to that of the current
state-of-the-art. Our approach in chess recognition on this new benchmark
dataset outperforms related approaches, achieving a board recognition accuracy
of 15.26% ($\approx$7x better than the current state-of-the-art).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04099">ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer. (arXiv:2310.04099v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1">Pourya Shamsolmoali</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Visual place recognition (VPR) is a highly challenging task that has a wide
range of applications, including robot navigation and self-driving vehicles.
VPR is particularly difficult due to the presence of duplicate regions and the
lack of attention to small objects in complex scenes, resulting in recognition
deviations. In this paper, we present ClusVPR, a novel approach that tackles
the specific issues of redundant information in duplicate regions and
representations of small objects. Different from existing methods that rely on
Convolutional Neural Networks (CNNs) for feature map generation, ClusVPR
introduces a unique paradigm called Clustering-based Weighted Transformer
Network (CWTNet). CWTNet leverages the power of clustering-based weighted
feature maps and integrates global dependencies to effectively address visual
deviations encountered in large-scale VPR problems. We also introduce the
optimized-VLAD (OptLAD) layer that significantly reduces the number of
parameters and enhances model efficiency. This layer is specifically designed
to aggregate the information obtained from scale-wise image patches.
Additionally, our pyramid self-supervised strategy focuses on extracting
representative and diverse information from scale-wise image patches instead of
entire images, which is crucial for capturing representative and diverse
information in VPR. Extensive experiments on four VPR datasets show our model's
superior performance compared to existing models while being less complex.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04110">Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023 Challenge. (arXiv:2310.04110v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Myronenko_A/0/1/0/all/0/1">Andriy Myronenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yufan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Daguang Xu</a></p>
<p>Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform
for researchers to compare their solutions to segmentation from 3D CT. In this
work, we describe our submission to the challenge using automated segmentation
of Auto3DSeg available in MONAI. Our solution achieves the average dice of
0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023
challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04111">Dense Random Texture Detection using Beta Distribution Statistics. (arXiv:2310.04111v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Molander_S/0/1/0/all/0/1">Soeren Molander</a></p>
<p>This note describes a method for detecting dense random texture using fully
connected points sampled on image edges. An edge image is randomly sampled with
points, the standard L2 distance is calculated between all connected points in
a neighbourhood. For each point, a check is made if the point intersects with
an image edge. If this is the case, a unity value is added to the distance,
otherwise zero. From this an edge excess index is calculated for the fully
connected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The
ratio can be interpreted as a sampled Bernoulli process with unknown
probability. The Bayesian posterior estimate of the probability can be
associated with its conjugate prior which is a Beta($\alpha$, $\beta$)
distribution, with hyper parameters $\alpha$ and $\beta$ related to the number
of edge crossings. Low values of $\beta$ indicate a texture rich area, higher
values less rich. The method has been applied to real-time SLAM-based moving
object detection, where points are confined to tracked boxes (rois).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04114">Aorta Segmentation from 3D CT in MICCAI SEG.A. 2023 Challenge. (arXiv:2310.04114v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1">Andriy Myronenko</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1">Dong Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1">Yufan He</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1">Daguang Xu</a></p>
<p>Aorta provides the main blood supply of the body. Screening of aorta with
imaging helps for early aortic disease detection and monitoring. In this work,
we describe our solution to the Segmentation of the Aorta (SEG.A.231) from 3D
CT challenge. We use automated segmentation method Auto3DSeg available in
MONAI. Our solution achieves an average Dice score of 0.920 and 95th percentile
of the Hausdorff Distance (HD95) of 6.013, which ranks first and wins the
SEG.A. 2023 challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04122">VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for Single Modality Labeled Visible-Infrared Person Re-identification. (arXiv:2310.04122v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Han Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a></p>
<p>Visible-Infrared person re-identification (VI-ReID) in real-world scenarios
poses a significant challenge due to the high cost of cross-modality data
annotation. Different sensing cameras, such as RGB/IR cameras for good/poor
lighting conditions, make it costly and error-prone to identify the same person
across modalities. To overcome this, we explore the use of single-modality
labeled data for the VI-ReID task, which is more cost-effective and practical.
By labeling pedestrians in only one modality (e.g., visible images) and
retrieving in another modality (e.g., infrared images), we aim to create a
training set containing both originally labeled and modality-translated data
using unpaired image-to-image translation techniques. In this paper, we propose
VI-Diff, a diffusion model that effectively addresses the task of
Visible-Infrared person image translation. Through comprehensive experiments,
we demonstrate that VI-Diff outperforms existing diffusion and GAN models,
making it a promising solution for VI-ReID with single-modality labeled data.
Our approach can be a promising solution to the VI-ReID task with
single-modality labeled data and serves as a good starting point for future
study. Code will be available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04134">TiC: Exploring Vision Transformer in Convolution. (arXiv:2310.04134v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Song Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qingzhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Haoyi Xiong</a></p>
<p>While models derived from Vision Transformers (ViTs) have been phonemically
surging, pre-trained models cannot seamlessly adapt to arbitrary resolution
images without altering the architecture and configuration, such as sampling
the positional encoding, limiting their flexibility for various vision tasks.
For instance, the Segment Anything Model (SAM) based on ViT-Huge requires all
input images to be resized to 1024$\times$1024. To overcome this limitation, we
propose the Multi-Head Self-Attention Convolution (MSA-Conv) that incorporates
Self-Attention within generalized convolutions, including standard, dilated,
and depthwise ones. Enabling transformers to handle images of varying sizes
without retraining or rescaling, the use of MSA-Conv further reduces
computational costs compared to global attention in ViT, which grows costly as
image size increases. Later, we present the Vision Transformer in Convolution
(TiC) as a proof of concept for image classification with MSA-Conv, where two
capacity enhancing strategies, namely Multi-Directional Cyclic Shifted
Mechanism and Inter-Pooling Mechanism, have been proposed, through establishing
long-distance connections between tokens and enlarging the effective receptive
field. Extensive experiments have been carried out to validate the overall
effectiveness of TiC. Additionally, ablation studies confirm the performance
improvement made by MSA-Conv and the two capacity enhancing strategies
separately. Note that our proposal aims at studying an alternative to the
global attention used in ViT, while MSA-Conv meets our goal by making TiC
comparable to state-of-the-art on ImageNet-1K. Code will be released at
https://github.com/zs670980918/MSA-Conv.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04148">Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning. (arXiv:2310.04148v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yinda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shenglong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhiwei Xiong</a></p>
<p>The performance of existing supervised neuron segmentation methods is highly
dependent on the number of accurate annotations, especially when applied to
large scale electron microscopy (EM) data. By extracting semantic information
from unlabeled data, self-supervised methods can improve the performance of
downstream tasks, among which the mask image model (MIM) has been widely used
due to its simplicity and effectiveness in recovering original information from
masked images. However, due to the high degree of structural locality in EM
images, as well as the existence of considerable noise, many voxels contain
little discriminative information, making MIM pretraining inefficient on the
neuron segmentation task. To overcome this challenge, we propose a
decision-based MIM that utilizes reinforcement learning (RL) to automatically
search for optimal image masking ratio and masking strategy. Due to the vast
exploration space, using single-agent RL for voxel prediction is impractical.
Therefore, we treat each input patch as an agent with a shared behavior policy,
allowing for multi-agent collaboration. Furthermore, this multi-agent model can
capture dependencies between voxels, which is beneficial for the downstream
segmentation task. Experiments conducted on representative EM datasets
demonstrate that our approach has a significant advantage over alternative
self-supervised methods on the task of neuron segmentation. Code is available
at \url{https://github.com/ydchen0806/dbMiM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04152">Improving Neural Radiance Field using Near-Surface Sampling with Point Cloud Generation. (arXiv:2310.04152v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1">Hye Bin Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Hyun Min Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Soo Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_I/0/1/0/all/0/1">Il Yong Chun</a></p>
<p>Neural radiance field (NeRF) is an emerging view synthesis method that
samples points in a three-dimensional (3D) space and estimates their existence
and color probabilities. The disadvantage of NeRF is that it requires a long
training time since it samples many 3D points. In addition, if one samples
points from occluded regions or in the space where an object is unlikely to
exist, the rendering quality of NeRF can be degraded. These issues can be
solved by estimating the geometry of 3D scene. This paper proposes a
near-surface sampling framework to improve the rendering quality of NeRF. To
this end, the proposed method estimates the surface of a 3D object using depth
images of the training set and sampling is performed around there only. To
obtain depth information on a novel view, the paper proposes a 3D point cloud
generation method and a simple refining method for projected depth from a point
cloud. Experimental results show that the proposed near-surface sampling NeRF
framework can significantly improve the rendering quality, compared to the
original NeRF and a state-of-the-art depth-based NeRF method. In addition, one
can significantly accelerate the training time of a NeRF model with the
proposed near-surface sampling framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04179">Entropic Score metric: Decoupling Topology and Size in Training-free NAS. (arXiv:2310.04179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cavagnero_N/0/1/0/all/0/1">Niccol&#xf2; Cavagnero</a>, <a href="http://arxiv.org/find/cs/1/au:+Robbiano_L/0/1/0/all/0/1">Luca Robbiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Pistilli_F/0/1/0/all/0/1">Francesca Pistilli</a>, <a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1">Barbara Caputo</a>, <a href="http://arxiv.org/find/cs/1/au:+Averta_G/0/1/0/all/0/1">Giuseppe Averta</a></p>
<p>Neural Networks design is a complex and often daunting task, particularly for
resource-constrained scenarios typical of mobile-sized models. Neural
Architecture Search is a promising approach to automate this process, but
existing competitive methods require large training time and computational
resources to generate accurate models. To overcome these limits, this paper
contributes with: i) a novel training-free metric, named Entropic Score, to
estimate model expressivity through the aggregated element-wise entropy of its
activations; ii) a cyclic search algorithm to separately yet synergistically
search model size and topology. Entropic Score shows remarkable ability in
searching for the topology of the network, and a proper combination with
LogSynflow, to search for model size, yields superior capability to completely
design high-performance Hybrid Transformers for edge applications in less than
1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04180">Degradation-Aware Self-Attention Based Transformer for Blind Image Super-Resolution. (arXiv:2310.04180v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingguo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Pan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ningzhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wei Xiang</a></p>
<p>Compared to CNN-based methods, Transformer-based methods achieve impressive
image restoration outcomes due to their abilities to model remote dependencies.
However, how to apply Transformer-based methods to the field of blind
super-resolution (SR) and further make an SR network adaptive to degradation
information is still an open problem. In this paper, we propose a new
degradation-aware self-attention-based Transformer model, where we incorporate
contrastive learning into the Transformer network for learning the degradation
representations of input images with unknown noise. In particular, we integrate
both CNN and Transformer components into the SR network, where we first use the
CNN modulated by the degradation information to extract local features, and
then employ the degradation-aware Transformer to extract global semantic
features. We apply our proposed model to several popular large-scale benchmark
datasets for testing, and achieve the state-of-the-art performance compared to
existing methods. In particular, our method yields a PSNR of 32.43 dB on the
Urban100 dataset at $\times$2 scale, 0.94 dB higher than DASR, and 26.62 dB on
the Urban100 dataset at $\times$4 scale, 0.26 dB improvement over KDSR, setting
a new benchmark in this area. Source code is available at:
https://github.com/I2-Multimedia-Lab/DSAT/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04181">DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions. (arXiv:2310.04181v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalwar_S/0/1/0/all/0/1">Sanket Kalwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ungarala_M/0/1/0/all/0/1">Mihir Ungarala</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Shruti Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Monis_A/0/1/0/all/0/1">Aaron Monis</a>, <a href="http://arxiv.org/find/cs/1/au:+Konda_K/0/1/0/all/0/1">Krishna Reddy Konda</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Sourav Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">K Madhava Krishna</a></p>
<p>Semantic segmentation in adverse weather scenarios is a critical task for
autonomous driving systems. While foundation models have shown promise, the
need for specialized adaptors becomes evident for handling more challenging
scenarios. We introduce DiffPrompter, a novel differentiable visual and latent
prompting mechanism aimed at expanding the learning capabilities of existing
adaptors in foundation models. Our proposed $\nabla$HFC image processing block
excels particularly in adverse weather conditions, where conventional methods
often fall short. Furthermore, we investigate the advantages of jointly
training visual and latent prompts, demonstrating that this combined approach
significantly enhances performance in out-of-distribution scenarios. Our
differentiable visual prompts leverage parallel and series architectures to
generate prompts, effectively improving object segmentation tasks in adverse
conditions. Through a comprehensive series of experiments and evaluations, we
provide empirical evidence to support the efficacy of our approach. Project
page at https://diffprompter.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04187">Whole Slide Multiple Instance Learning for Predicting Axillary Lymph Node Metastasis. (arXiv:2310.04187v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shkembi_G/0/1/0/all/0/1">Glejdis Shk&#xeb;mbi</a>, <a href="http://arxiv.org/find/eess/1/au:+Muller_J/0/1/0/all/0/1">Johanna P. M&#xfc;ller</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1">Katharina Breininger</a>, <a href="http://arxiv.org/find/eess/1/au:+Schuffler_P/0/1/0/all/0/1">Peter Sch&#xfc;ffler</a>, <a href="http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1">Bernhard Kainz</a></p>
<p>Breast cancer is a major concern for women's health globally, with axillary
lymph node (ALN) metastasis identification being critical for prognosis
evaluation and treatment guidance. This paper presents a deep learning (DL)
classification pipeline for quantifying clinical information from digital
core-needle biopsy (CNB) images, with one step less than existing methods. A
publicly available dataset of 1058 patients was used to evaluate the
performance of different baseline state-of-the-art (SOTA) DL models in
classifying ALN metastatic status based on CNB images. An extensive ablation
study of various data augmentation techniques was also conducted. Finally, the
manual tumor segmentation and annotation step performed by the pathologists was
assessed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04189">Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases. (arXiv:2310.04189v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong-Lu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Ailing Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zizheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yang You</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cewu Lu</a></p>
<p>The goal of motion understanding is to establish a reliable mapping between
motion and action semantics, while it is a challenging many-to-many problem. An
abstract action semantic (i.e., walk forwards) could be conveyed by
perceptually diverse motions (walk with arms up or swinging), while a motion
could carry different semantics w.r.t. its context and intention. This makes an
elegant mapping between them difficult. Previous attempts adopted
direct-mapping paradigms with limited reliability. Also, current automatic
metrics fail to provide reliable assessments of the consistency between motions
and action semantics. We identify the source of these problems as the
significant gap between the two modalities. To alleviate this gap, we propose
Kinematic Phrases (KP) that take the objective kinematic facts of human motion
with proper abstraction, interpretability, and generality characteristics.
Based on KP as a mediator, we can unify a motion knowledge base and build a
motion understanding system. Meanwhile, KP can be automatically converted from
motions and to text descriptions with no subjective bias, inspiring Kinematic
Prompt Generation (KPG) as a novel automatic motion generation benchmark. In
extensive experiments, our approach shows superiority over other methods. Our
code and data would be made publicly available at https://foruck.github.io/KP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04194">Enhancing the Authenticity of Rendered Portraits with Identity-Consistent Transfer Learning. (arXiv:2310.04194v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Luyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yiqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongliang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaogang Jin</a></p>
<p>Despite rapid advances in computer graphics, creating high-quality
photo-realistic virtual portraits is prohibitively expensive. Furthermore, the
well-know ''uncanny valley'' effect in rendered portraits has a significant
impact on the user experience, especially when the depiction closely resembles
a human likeness, where any minor artifacts can evoke feelings of eeriness and
repulsiveness. In this paper, we present a novel photo-realistic portrait
generation framework that can effectively mitigate the ''uncanny valley''
effect and improve the overall authenticity of rendered portraits. Our key idea
is to employ transfer learning to learn an identity-consistent mapping from the
latent space of rendered portraits to that of real portraits. During the
inference stage, the input portrait of an avatar can be directly transferred to
a realistic portrait by changing its appearance style while maintaining the
facial identity. To this end, we collect a new dataset, Daz-Rendered-Faces-HQ
(DRFHQ), that is specifically designed for rendering-style portraits. We
leverage this dataset to fine-tune the StyleGAN2 generator, using our carefully
crafted framework, which helps to preserve the geometric and color features
relevant to facial identity. We evaluate our framework using portraits with
diverse gender, age, and race variations. Qualitative and quantitative
evaluations and ablation studies show the advantages of our method compared to
state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04247">Semantic segmentation of longitudinal thermal images for identification of hot and cool spots in urban areas. (arXiv:2310.04247v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramani_V/0/1/0/all/0/1">Vasantha Ramani</a>, <a href="http://arxiv.org/find/cs/1/au:+Arjunan_P/0/1/0/all/0/1">Pandarasamy Arjunan</a>, <a href="http://arxiv.org/find/cs/1/au:+Poolla_K/0/1/0/all/0/1">Kameshwar Poolla</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1">Clayton Miller</a></p>
<p>This work presents the analysis of semantically segmented, longitudinally,
and spatially rich thermal images collected at the neighborhood scale to
identify hot and cool spots in urban areas. An infrared observatory was
operated over a few months to collect thermal images of different types of
buildings on the educational campus of the National University of Singapore. A
subset of the thermal image dataset was used to train state-of-the-art deep
learning models to segment various urban features such as buildings,
vegetation, sky, and roads. It was observed that the U-Net segmentation model
with `resnet34' CNN backbone has the highest mIoU score of 0.99 on the test
dataset, compared to other models such as DeepLabV3, DeeplabV3+, FPN, and
PSPnet. The masks generated using the segmentation models were then used to
extract the temperature from thermal images and correct for differences in the
emissivity of various urban features. Further, various statistical measure of
the temperature extracted using the predicted segmentation masks is shown to
closely match the temperature extracted using the ground truth masks. Finally,
the masks were used to identify hot and cool spots in the urban feature at
various instances of time. This forms one of the very few studies demonstrating
the automated analysis of thermal images, which can be of potential use to
urban planners for devising mitigation strategies for reducing the urban heat
island (UHI) effect, improving building energy efficiency, and maximizing
outdoor thermal comfort.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04253">Collaborative Camouflaged Object Detection: A Large-Scale Dataset and Benchmark. (arXiv:2310.04253v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_H/0/1/0/all/0/1">Hongbo Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tian-Zhu Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ranwan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1">Jinghui Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiufang Wang</a></p>
<p>In this paper, we provide a comprehensive study on a new task called
collaborative camouflaged object detection (CoCOD), which aims to
simultaneously detect camouflaged objects with the same properties from a group
of relevant images. To this end, we meticulously construct the first
large-scale dataset, termed CoCOD8K, which consists of 8,528 high-quality and
elaborately selected images with object mask annotations, covering 5
superclasses and 70 subclasses. The dataset spans a wide range of natural and
artificial camouflage scenes with diverse object appearances and backgrounds,
making it a very challenging dataset for CoCOD. Besides, we propose the first
baseline model for CoCOD, named bilateral-branch network (BBNet), which
explores and aggregates co-camouflaged cues within a single image and between
images within a group, respectively, for accurate camouflaged object detection
in given images. This is implemented by an inter-image collaborative feature
exploration (CFE) module, an intra-image object feature search (OFS) module,
and a local-global refinement (LGR) module. We benchmark 18 state-of-the-art
models, including 12 COD algorithms and 6 CoSOD algorithms, on the proposed
CoCOD8K dataset under 5 widely used evaluation metrics. Extensive experiments
demonstrate the effectiveness of the proposed method and the significantly
superior performance compared to other competitors. We hope that our proposed
dataset and model will boost growth in the COD community. The dataset, model,
and results will be available at: https://github.com/zc199823/BBNet--CoCOD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04271">Compositional Servoing by Recombining Demonstrations. (arXiv:2310.04271v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1">Max Argus</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Abhijeet Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Buchner_M/0/1/0/all/0/1">Martin B&#xfc;chner</a>, <a href="http://arxiv.org/find/cs/1/au:+Galesso_S/0/1/0/all/0/1">Silvio Galesso</a>, <a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1">Abhinav Valada</a>, <a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1">Thomas Brox</a></p>
<p>Learning-based manipulation policies from image inputs often show weak task
transfer capabilities. In contrast, visual servoing methods allow efficient
task transfer in high-precision scenarios while requiring only a few
demonstrations. In this work, we present a framework that formulates the visual
servoing task as graph traversal. Our method not only extends the robustness of
visual servoing, but also enables multitask capability based on a few
task-specific demonstrations. We construct demonstration graphs by splitting
existing demonstrations and recombining them. In order to traverse the
demonstration graph in the inference case, we utilize a similarity function
that helps select the best demonstration for a specific task. This enables us
to compute the shortest path through the graph. Ultimately, we show that
recombining demonstrations leads to higher task-respective success. We present
extensive simulation and real-world experimental results that demonstrate the
efficacy of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04285">Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1">Marcel Kollovieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gosch_L/0/1/0/all/0/1">Lukas Gosch</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1">Yan Scholten</a>, <a href="http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1">Marten Lienen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Most adversarial attacks and defenses focus on perturbations within small
$\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all
relevant semantic-preserving perturbations, and hence, the scope of robustness
evaluations is limited. In this work, we introduce Score-Based Adversarial
Generation (ScoreAG), a novel framework that leverages the advancements in
score-based generative models to generate adversarial examples beyond
$\ell_p$-norm constraints, so-called unrestricted adversarial examples,
overcoming their limitations. Unlike traditional methods, ScoreAG maintains the
core semantics of images while generating realistic adversarial examples,
either by transforming existing images or synthesizing new ones entirely from
scratch. We further exploit the generative capability of ScoreAG to purify
images, empirically enhancing the robustness of classifiers. Our extensive
empirical evaluation demonstrates that ScoreAG matches the performance of
state-of-the-art attacks and defenses across multiple benchmarks. This work
highlights the importance of investigating adversarial examples bounded by
semantics rather than $\ell_p$-norm constraints. ScoreAG represents an
important step towards more encompassing robustness assessments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04294">Graph learning in robotics: a survey. (arXiv:2310.04294v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pistilli_F/0/1/0/all/0/1">Francesca Pistilli</a>, <a href="http://arxiv.org/find/cs/1/au:+Averta_G/0/1/0/all/0/1">Giuseppe Averta</a></p>
<p>Deep neural networks for graphs have emerged as a powerful tool for learning
on complex non-euclidean data, which is becoming increasingly common for a
variety of different applications. Yet, although their potential has been
widely recognised in the machine learning community, graph learning is largely
unexplored for downstream tasks such as robotics applications. To fully unlock
their potential, hence, we propose a review of graph neural architectures from
a robotics perspective. The paper covers the fundamentals of graph-based
models, including their architecture, training procedures, and applications. It
also discusses recent advancements and challenges that arise in applied
settings, related for example to the integration of perception,
decision-making, and control. Finally, the paper provides an extensive review
of various robotic applications that benefit from learning on graph structures,
such as bodies and contacts modelling, robotic manipulation, action
recognition, fleet motion planning, and many more. This survey aims to provide
readers with a thorough understanding of the capabilities and limitations of
graph neural architectures in robotics, and to highlight potential avenues for
future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04299">Convergent ADMM Plug and Play PET Image Reconstruction. (arXiv:2310.04299v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sureau_F/0/1/0/all/0/1">Florent Sureau</a>, <a href="http://arxiv.org/find/eess/1/au:+Latreche_M/0/1/0/all/0/1">Mahdi Latreche</a>, <a href="http://arxiv.org/find/eess/1/au:+Savanier_M/0/1/0/all/0/1">Marion Savanier</a>, <a href="http://arxiv.org/find/eess/1/au:+Comtat_C/0/1/0/all/0/1">Claude Comtat</a></p>
<p>In this work, we investigate hybrid PET reconstruction algorithms based on
coupling a model-based variational reconstruction and the application of a
separately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play
framework. Following recent results in optimization, fixed point convergence of
the scheme can be achieved by enforcing an additional constraint on network
parameters during learning. We propose such an ADMM algorithm and show in a
realistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead
experimentally to convergence to a meaningful fixed point. When the proposed
constraint is not enforced during learning of the DNN, the proposed ADMM
algorithm was observed experimentally not to converge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04306">Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning. (arXiv:2310.04306v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qirong Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jialin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaohua Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wenming Zheng</a></p>
<p>Group-level emotion recognition (GER) is an inseparable part of human
behavior analysis, aiming to recognize an overall emotion in a multi-person
scene. However, the existing methods are devoted to combing diverse emotion
cues while ignoring the inherent uncertainties under unconstrained
environments, such as congestion and occlusion occurring within a group.
Additionally, since only group-level labels are available, inconsistent emotion
predictions among individuals in one group can confuse the network. In this
paper, we propose an uncertainty-aware learning (UAL) method to extract more
robust representations for GER. By explicitly modeling the uncertainty of each
individual, we utilize stochastic embedding drawn from a Gaussian distribution
instead of deterministic point embedding. This representation captures the
probabilities of different emotions and generates diverse predictions through
this stochasticity during the inference stage. Furthermore,
uncertainty-sensitive scores are adaptively assigned as the fusion weights of
individuals' face within each group. Moreover, we develop an image enhancement
module to enhance the model's robustness against severe noise. The overall
three-branch model, encompassing face, object, and scene component, is guided
by a proportional-weighted fusion strategy and integrates the proposed
uncertainty-aware method to produce the final group-level output. Experimental
results demonstrate the effectiveness and generalization ability of our method
across three widely used databases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04311">Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information. (arXiv:2310.04311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1">Selim F. Yilmaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozyilkan_E/0/1/0/all/0/1">Ezgi Ozyilkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1">Deniz Gunduz</a>, <a href="http://arxiv.org/find/cs/1/au:+Erkip_E/0/1/0/all/0/1">Elza Erkip</a></p>
<p>We consider low-latency image transmission over a noisy wireless channel when
correlated side information is present only at the receiver side (the Wyner-Ziv
scenario). In particular, we are interested in developing practical schemes
using a data-driven joint source-channel coding (JSCC) approach, which has been
previously shown to outperform conventional separation-based approaches in the
practical finite blocklength regimes, and to provide graceful degradation with
channel quality. We propose a novel neural network architecture that
incorporates the decoder-only side information at multiple stages at the
receiver side. Our results demonstrate that the proposed method succeeds in
integrating the side information, yielding improved performance at all channel
noise levels in terms of the various distortion criteria considered here,
especially at low channel signal-to-noise ratios (SNRs) and small bandwidth
ratios (BRs). We also provide the source code of the proposed method to enable
further research and reproducibility of the results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04360">SwimXYZ: A large-scale dataset of synthetic swimming motions and videos. (arXiv:2310.04360v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guenole_F/0/1/0/all/0/1">Fiche Gu&#xe9;nol&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1">Sevestre Vincent</a>, <a href="http://arxiv.org/find/cs/1/au:+Camila_G/0/1/0/all/0/1">Gonzalez-Barral Camila</a>, <a href="http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1">Leglaive Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+Renaud_S/0/1/0/all/0/1">S&#xe9;guier Renaud</a></p>
<p>Technologies play an increasingly important role in sports and become a real
competitive advantage for the athletes who benefit from it. Among them, the use
of motion capture is developing in various sports to optimize sporting
gestures. Unfortunately, traditional motion capture systems are expensive and
constraining. Recently developed computer vision-based approaches also struggle
in certain sports, like swimming, due to the aquatic environment. One of the
reasons for the gap in performance is the lack of labeled datasets with
swimming videos. In an attempt to address this issue, we introduce SwimXYZ, a
synthetic dataset of swimming motions and videos. SwimXYZ contains 3.4 million
frames annotated with ground truth 2D and 3D joints, as well as 240 sequences
of swimming motions in the SMPL parameters format. In addition to making this
dataset publicly available, we present use cases for SwimXYZ in swimming stroke
clustering and 2D pose estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04378">Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. (arXiv:2310.04378v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Simian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yiqin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longbo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hang Zhao</a></p>
<p>Latent Diffusion models (LDMs) have achieved remarkable results in
synthesizing high-resolution images. However, the iterative sampling process is
computationally intensive and leads to slow generation. Inspired by Consistency
Models (song et al.), we propose Latent Consistency Models (LCMs), enabling
swift inference with minimal steps on any pre-trained LDMs, including Stable
Diffusion (rombach et al). Viewing the guided reverse diffusion process as
solving an augmented probability flow ODE (PF-ODE), LCMs are designed to
directly predict the solution of such ODE in latent space, mitigating the need
for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently
distilled from pre-trained classifier-free guided diffusion models, a
high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training.
Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method
that is tailored for fine-tuning LCMs on customized image datasets. Evaluation
on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve
state-of-the-art text-to-image generation performance with few-step inference.
Project Page: https://latent-consistency-models.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04406">Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Andy Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlapentokh_Rothman_M/0/1/0/all/0/1">Michal Shlapentokh-Rothman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Xiong Wang</a></p>
<p>While large language models (LLMs) have demonstrated impressive performance
on a range of decision-making tasks, they rely on simple acting processes and
fall short of broad deployment as autonomous agents. We introduce LATS
(Language Agent Tree Search), a general framework that synergizes the
capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration
from Monte Carlo tree search in model-based reinforcement learning, LATS
employs LLMs as agents, value functions, and optimizers, repurposing their
latent strengths for enhanced decision-making. What is crucial in this method
is the use of an environment for external feedback, which offers a more
deliberate and adaptive problem-solving mechanism that moves beyond the
limitations of existing techniques. Our experimental evaluation across diverse
domains, such as programming, HotPotQA, and WebShop, illustrates the
applicability of LATS for both reasoning and acting. In particular, LATS
achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of
75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness
and generality of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04412">FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning. (arXiv:2310.04412v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Peiran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zeyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jieru Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Liangqiong Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Cihang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyin Zhou</a></p>
<p>Federated learning (FL) is an emerging paradigm in machine learning, where a
shared model is collaboratively learned using data from multiple devices to
mitigate the risk of data leakage. While recent studies posit that Vision
Transformer (ViT) outperforms Convolutional Neural Networks (CNNs) in
addressing data heterogeneity in FL, the specific architectural components that
underpin this advantage have yet to be elucidated. In this paper, we
systematically investigate the impact of different architectural elements, such
as activation functions and normalization layers, on the performance within
heterogeneous FL. Through rigorous empirical analyses, we are able to offer the
first-of-its-kind general guidance on micro-architecture design principles for
heterogeneous FL.
</p>
<p>Intriguingly, our findings indicate that with strategic architectural
modifications, pure CNNs can achieve a level of robustness that either matches
or even exceeds that of ViTs when handling heterogeneous data clients in FL.
Additionally, our approach is compatible with existing FL techniques and
delivers state-of-the-art solutions across a broad spectrum of FL benchmarks.
The code is publicly available at https://github.com/UCSC-VLAA/FedConv
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04414">CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis. (arXiv:2310.04414v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaoxiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_X/0/1/0/all/0/1">Xingjian Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zijian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a></p>
<p>Analyzing model performance in various unseen environments is a critical
research problem in the machine learning community. To study this problem, it
is important to construct a testbed with out-of-distribution test sets that
have broad coverage of environmental discrepancies. However, existing testbeds
typically either have a small number of domains or are synthesized by image
corruptions, hindering algorithm design that demonstrates real-world
effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of
180 datasets collected by prompting image search engines and diffusion models
in various ways. Generally sized between 300 and 8,000 images, the datasets
contain natural images, cartoons, certain colors, or objects that do not
naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen
the understanding of two generalization tasks: domain generalization and model
accuracy prediction in various out-of-distribution environments. We conduct
extensive benchmarking and comparison experiments and show that CIFAR-10-W
offers new and interesting insights inherent to these tasks. We also discuss
other fields that would benefit from CIFAR-10-W.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04416">Alice Benchmarks: Connecting Real World Object Re-Identification with the Synthetic. (arXiv:2310.04416v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaoxiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yue Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengjin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongdong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a></p>
<p>For object re-identification (re-ID), learning from synthetic data has become
a promising strategy to cheaply acquire large-scale annotated datasets and
effective models, with few privacy concerns. Many interesting research problems
arise from this strategy, e.g., how to reduce the domain gap between synthetic
source and real-world target. To facilitate developing more new approaches in
learning from synthetic data, we introduce the Alice benchmarks, large-scale
datasets providing benchmarks as well as evaluation protocols to the research
community. Within the Alice benchmarks, two object re-ID tasks are offered:
person and vehicle re-ID. We collected and annotated two challenging real-world
target datasets: AlicePerson and AliceVehicle, captured under various
illuminations, image resolutions, etc. As an important feature of our real
target, the clusterability of its training set is not manually guaranteed to
make it closer to a real domain adaptation test scenario. Correspondingly, we
reuse existing PersonX and VehicleX as synthetic source domains. The primary
goal is to train models from synthetic data that can work effectively in the
real world. In this paper, we detail the settings of Alice benchmarks, provide
an analysis of existing commonly-used domain adaptation methods, and discuss
some interesting future directions. An online server will be set up for the
community to evaluate methods conveniently and fairly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1806.06298">Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry. (arXiv:1806.06298v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xianglei Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tian Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a></p>
<p>We present a deformable generator model to disentangle the appearance and
geometric information for both image and video data in a purely unsupervised
manner. The appearance generator network models the information related to
appearance, including color, illumination, identity or category, while the
geometric generator performs geometric warping, such as rotation and
stretching, through generating deformation field which is used to warp the
generated appearance to obtain the final image or video sequences. Two
generators take independent latent vectors as input to disentangle the
appearance and geometric information from image or video sequences. For video
data, a nonlinear transition model is introduced to both the appearance and
geometric generators to capture the dynamics over time. The proposed scheme is
general and can be easily integrated into different generative models. An
extensive set of qualitative and quantitative experiments shows that the
appearance and geometric information can be well disentangled, and the learned
geometric generator can be conveniently transferred to other image datasets to
facilitate knowledge transfer tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.03893">Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zandigohar_M/0/1/0/all/0/1">Mehrshad Zandigohar</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1">Mo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1">Mohammadreza Sharif</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunay_S/0/1/0/all/0/1">Sezen Yagmur Gunay</a>, <a href="http://arxiv.org/find/cs/1/au:+Furmanek_M/0/1/0/all/0/1">Mariusz P. Furmanek</a>, <a href="http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1">Mathew Yarossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonato_P/0/1/0/all/0/1">Paolo Bonato</a>, <a href="http://arxiv.org/find/cs/1/au:+Onal_C/0/1/0/all/0/1">Cagdas Onal</a>, <a href="http://arxiv.org/find/cs/1/au:+Padir_T/0/1/0/all/0/1">Taskin Padir</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1">Deniz Erdogmus</a>, <a href="http://arxiv.org/find/cs/1/au:+Schirner_G/0/1/0/all/0/1">Gunar Schirner</a></p>
<p>Objective: For lower arm amputees, robotic prosthetic hands promise to regain
the capability to perform daily living activities. Current control methods
based on physiological signals such as electromyography (EMG) are prone to
yielding poor inference outcomes due to motion artifacts, muscle fatigue, and
many more. Vision sensors are a major source of information about the
environment state and can play a vital role in inferring feasible and intended
gestures. However, visual evidence is also susceptible to its own artifacts,
most often due to object occlusion, lighting changes, etc. Multimodal evidence
fusion using physiological and vision sensor measurements is a natural approach
due to the complementary strengths of these modalities. Methods: In this paper,
we present a Bayesian evidence fusion framework for grasp intent inference
using eye-view video, eye-gaze, and EMG from the forearm processed by neural
network models. We analyze individual and fused performance as a function of
time as the hand approaches the object to grasp it. For this purpose, we have
also developed novel data processing and augmentation techniques to train
neural network components. Results: Our results indicate that, on average,
fusion improves the instantaneous upcoming grasp type classification accuracy
while in the reaching phase by 13.66% and 14.8%, relative to EMG and visual
evidence individually, resulting in an overall fusion accuracy of 95.3%.
Conclusion: Our experimental data analyses demonstrate that EMG and visual
evidence show complementary strengths, and as a consequence, fusion of
multimodal evidence can outperform each individual evidence modality at any
given time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.03379">Deep Efficient Continuous Manifold Learning for Time Series Modeling. (arXiv:2112.03379v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Seungwoo Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1">Wonjun Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulyadi_A/0/1/0/all/0/1">Ahmad Wisnu Mulyadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Suk_H/0/1/0/all/0/1">Heung-Il Suk</a></p>
<p>Modeling non-Euclidean data is drawing extensive attention along with the
unprecedented successes of deep neural networks in diverse fields.
Particularly, a symmetric positive definite matrix is being actively studied in
computer vision, signal processing, and medical image analysis, due to its
ability to learn beneficial statistical representations. However, owing to its
rigid constraints, it remains challenging to optimization problems and
inefficient computational costs, especially, when incorporating it with a deep
learning framework. In this paper, we propose a framework to exploit a
diffeomorphism mapping between Riemannian manifolds and a Cholesky space, by
which it becomes feasible not only to efficiently solve optimization problems
but also to greatly reduce computation costs. Further, for dynamic modeling of
time-series data, we devise a continuous manifold learning method by
systematically integrating a manifold ordinary differential equation and a
gated recurrent neural network. It is worth noting that due to the nice
parameterization of matrices in a Cholesky space, training our proposed network
equipped with Riemannian geometric metrics is straightforward. We demonstrate
through experiments over regular and irregular time-series datasets that our
proposed model can be efficiently and reliably trained and outperforms existing
manifold methods and state-of-the-art methods in various time-series tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.11717">A Survey of Dataset Refinement for Problems in Computer Vision Datasets. (arXiv:2210.11717v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1">Zhijing Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhixiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1">CheukTing Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a></p>
<p>Large-scale datasets have played a crucial role in the advancement of
computer vision. However, they often suffer from problems such as class
imbalance, noisy labels, dataset bias, or high resource costs, which can
inhibit model performance and reduce trustworthiness. With the advocacy of
data-centric research, various data-centric solutions have been proposed to
solve the dataset problems mentioned above. They improve the quality of
datasets by re-organizing them, which we call dataset refinement. In this
survey, we provide a comprehensive and structured overview of recent advances
in dataset refinement for problematic computer vision datasets. Firstly, we
summarize and analyze the various problems encountered in large-scale computer
vision datasets. Then, we classify the dataset refinement algorithms into three
categories based on the refinement process: data sampling, data subset
selection, and active learning. In addition, we organize these dataset
refinement methods according to the addressed data problems and provide a
systematic comparative description. We point out that these three types of
dataset refinement have distinct advantages and disadvantages for dataset
problems, which informs the choice of the data-centric method appropriate to a
particular research objective. Finally, we summarize the current literature and
propose potential future research topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14358">Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations. (arXiv:2210.14358v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Allan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a></p>
<p>There is an inescapable long-tailed class-imbalance issue in many real-world
classification problems. Current methods for addressing this problem only
consider scenarios where all examples come from the same distribution. However,
in many cases, there are multiple domains with distinct class imbalance. We
study this multi-domain long-tailed learning problem and aim to produce a model
that generalizes well across all classes and domains. Towards that goal, we
introduce TALLY, a method that addresses this multi-domain long-tailed learning
problem. Built upon a proposed selective balanced sampling strategy, TALLY
achieves this by mixing the semantic representation of one example with the
domain-associated nuisances of another, producing a new representation for use
as data augmentation. To improve the disentanglement of semantic
representations, TALLY further utilizes a domain-invariant class prototype that
averages out domain-specific effects. We evaluate TALLY on several benchmarks
and real-world datasets and find that it consistently outperforms other
state-of-the-art methods in both subpopulation and domain shift. Our code and
data have been released at https://github.com/huaxiuyao/TALLY.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16785">SafeSpace MFNet: Precise and Efficient MultiFeature Drone Detection Network. (arXiv:2211.16785v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Misha Urooj Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dil_M/0/1/0/all/0/1">Mahnoor Dil</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Muhammad Zeshan Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Orakazi_F/0/1/0/all/0/1">Farooq Alam Orakazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Almasoud_A/0/1/0/all/0/1">Abdullah M. Almasoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaleem_Z/0/1/0/all/0/1">Zeeshan Kaleem</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuen_C/0/1/0/all/0/1">Chau Yuen</a></p>
<p>The increasing prevalence of unmanned aerial vehicles (UAVs), commonly known
as drones, has generated a demand for reliable detection systems. The
inappropriate use of drones presents potential security and privacy hazards,
particularly concerning sensitive facilities. To overcome those obstacles, we
proposed the concept of MultiFeatureNet (MFNet), a solution that enhances
feature representation by capturing the most concentrated feature maps.
Additionally, we present MultiFeatureNet-Feature Attention (MFNet-FA), a
technique that adaptively weights different channels of the input feature maps.
To meet the requirements of multi-scale detection, we presented the versions of
MFNet and MFNet-FA, namely the small (S), medium (M), and large (L). The
outcomes reveal notable performance enhancements. For optimal bird detection,
MFNet-M (Ablation study 2) achieves an impressive precision of 99.8\%, while
for UAV detection, MFNet-L (Ablation study 2) achieves a precision score of
97.2\%. Among the options, MFNet-FA-S (Ablation study 3) emerges as the most
resource-efficient alternative, considering its small feature map size,
computational demands (GFLOPs), and operational efficiency (in frame per
second). This makes it particularly suitable for deployment on hardware with
limited capabilities. Additionally, MFNet-FA-S (Ablation study 3) stands out
for its swift real-time inference and multiple-object detection due to the
incorporation of the FA module. The proposed MFNet-L with the focus module
(Ablation study 2) demonstrates the most remarkable classification outcomes,
boasting an average precision of 98.4\%, average recall of 96.6\%, average mean
average precision (mAP) of 98.3\%, and average intersection over union (IoU) of
72.8\%. To encourage reproducible research, the dataset, and code for MFNet are
freely available as an open-source project:
github.com/ZeeshanKaleem/MultiFeatureNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06096">Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhdanov_M/0/1/0/all/0/1">Maksim Zhdanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffmann_N/0/1/0/all/0/1">Nico Hoffmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_G/0/1/0/all/0/1">Gabriele Cesa</a></p>
<p>Steerable convolutional neural networks (CNNs) provide a general framework
for building neural networks equivariant to translations and other
transformations belonging to an origin-preserving group $G$, such as
reflections and rotations. They rely on standard convolutions with
$G$-steerable kernels obtained by analytically solving the group-specific
equivariance constraint imposed onto the kernel space. As the solution is
tailored to a particular group $G$, the implementation of a kernel basis does
not generalize to other symmetry transformations, which complicates the
development of general group equivariant models. We propose using implicit
neural representation via multi-layer perceptrons (MLPs) to parameterize
$G$-steerable kernels. The resulting framework offers a simple and flexible way
to implement Steerable CNNs and generalizes to any group $G$ for which a
$G$-equivariant MLP can be built. We prove the effectiveness of our method on
multiple tasks, including N-body simulations, point cloud classification and
molecular property prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12291">CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans. (arXiv:2301.12291v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jieneng Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1">Yingda Xia</a>, <a href="http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1">Jiawen Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Yan_K/0/1/0/all/0/1">Ke Yan</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jianpeng Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1">Le Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1">Fakai Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Qiu_M/0/1/0/all/0/1">Mingyan Qiu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_Q/0/1/0/all/0/1">Qihang Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1">Mingze Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_W/0/1/0/all/0/1">Wei Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1">Yuxing Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Minfeng Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jian Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1">Yuqian Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1">Qifeng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1">Xianghua Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Yin_X/0/1/0/all/0/1">Xiaoli Yin</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1">Yu Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">Zaiyi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Ling Zhang</a></p>
<p>Human readers or radiologists routinely perform full-body multi-organ
multi-disease detection and diagnosis in clinical practice, while most medical
AI systems are built to focus on single organs with a narrow list of a few
diseases. This might severely limit AI's clinical adoption. A certain number of
AI models need to be assembled non-trivially to match the diagnostic process of
a human reading a CT scan. In this paper, we construct a Unified Tumor
Transformer (CancerUniT) model to jointly detect tumor existence &amp; location and
diagnose tumor characteristics for eight major cancers in CT scans. CancerUniT
is a query-based Mask Transformer model with the output of multi-tumor
prediction. We decouple the object queries into organ queries, tumor detection
queries and tumor diagnosis queries, and further establish hierarchical
relationships among the three groups. This clinically-inspired architecture
effectively assists inter- and intra-organ representation learning of tumors
and facilitates the resolution of these complex, anatomically related
multi-organ cancer image reading tasks. CancerUniT is trained end-to-end using
a curated large-scale CT images of 10,042 patients including eight major types
of cancers and occurring non-cancer tumors (all are pathology-confirmed with 3D
tumor masks annotated by radiologists). On the test set of 631 patients,
CancerUniT has demonstrated strong performance under a set of clinically
relevant evaluation metrics, substantially outperforming both multi-disease
methods and an assembly of eight single-organ expert models in tumor detection,
segmentation, and diagnosis. This moves one step closer towards a universal
high performance cancer screening tool.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01665">CVTNet: A Cross-View Transformer Network for Place Recognition Using LiDAR Data. (arXiv:2302.01665v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Junyi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1">Guangming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingyi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xieyuanli Chen</a></p>
<p>LiDAR-based place recognition (LPR) is one of the most crucial components of
autonomous vehicles to identify previously visited places in GPS-denied
environments. Most existing LPR methods use mundane representations of the
input point cloud without considering different views, which may not fully
exploit the information from LiDAR sensors. In this paper, we propose a
cross-view transformer-based network, dubbed CVTNet, to fuse the range image
views (RIVs) and bird's eye views (BEVs) generated from the LiDAR data. It
extracts correlations within the views themselves using intra-transformers and
between the two different views using inter-transformers. Based on that, our
proposed CVTNet generates a yaw-angle-invariant global descriptor for each
laser scan end-to-end online and retrieves previously seen places by descriptor
matching between the current query scan and the pre-built database. We evaluate
our approach on three datasets collected with different sensor setups and
environmental conditions. The experimental results show that our method
outperforms the state-of-the-art LPR methods with strong robustness to
viewpoint changes and long-time spans. Furthermore, our approach has a good
real-time performance that can run faster than the typical LiDAR frame rate.
The implementation of our method is released as open source at:
https://github.com/BIT-MJY/CVTNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05686">Generative AI for Rapid Diffusion MRI with Improved Image Quality, Reliability and Generalizability. (arXiv:2303.05686v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sadikov_A/0/1/0/all/0/1">Amir Sadikov</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1">Xinlei Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_H/0/1/0/all/0/1">Hannah Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Cai_L/0/1/0/all/0/1">Lanya T. Cai</a>, <a href="http://arxiv.org/find/eess/1/au:+Mukherjee_P/0/1/0/all/0/1">Pratik Mukherjee</a></p>
<p>Diffusion MRI is a non-invasive, in-vivo biomedical imaging method for
mapping tissue microstructure. Applications include structural connectivity
imaging of the human brain and detecting microstructural neural changes.
However, acquiring high signal-to-noise ratio dMRI datasets with high angular
and spatial resolution requires prohibitively long scan times, limiting usage
in many important clinical settings, especially for children, the elderly, and
in acute neurological disorders that may require conscious sedation or general
anesthesia. We employ a Swin UNEt Transformers model, trained on augmented
Human Connectome Project data and conditioned on registered T1 scans, to
perform generalized denoising of dMRI. We also qualitatively demonstrate
super-resolution with artificially downsampled HCP data in normal adult
volunteers. Remarkably, Swin UNETR can be fine-tuned for an out-of-domain
dataset with a single example scan, as we demonstrate on dMRI of children with
neurodevelopmental disorders and of adults with acute evolving traumatic brain
injury, each cohort scanned on different models of scanners with different
imaging protocols at different sites. We exceed current state-of-the-art
denoising methods in accuracy and test-retest reliability of rapid diffusion
tensor imaging requiring only 90 seconds of scan time. Applied to tissue
microstructural modeling of dMRI, Swin UNETR denoising achieves dramatic
improvements over the state-of-the-art for test-retest reliability of
intracellular volume fraction and free water fraction measurements and can
remove heavy-tail noise, improving biophysical modeling fidelity. Swin UNeTR
enables rapid diffusion MRI with unprecedented accuracy and reliability,
especially for probing biological tissues for scientific and clinical
applications. The code and model are publicly available at
https://github.com/ucsfncl/dmri-swin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00570">FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1">Huidong Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qiong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiongchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xueqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_Z/0/1/0/all/0/1">Zhicheng Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1">Jun Hou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Biao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Rominger_A/0/1/0/all/0/1">Axel Rominger</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_K/0/1/0/all/0/1">Kuangyu Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a></p>
<p>Low-count PET is an efficient way to reduce radiation exposure and
acquisition time, but the reconstructed images often suffer from low
signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream
tasks. Recent advances in deep learning have shown great potential in improving
low-count PET image quality, but acquiring a large, centralized, and diverse
dataset from multiple institutions for training a robust model is difficult due
to privacy and security concerns of patient data. Moreover, low-count PET data
at different institutions may have different data distribution, thus requiring
personalized models. While previous federated learning (FL) algorithms enable
multi-institution collaborative training without the need of aggregating local
data, addressing the large domain shift in the application of
multi-institutional low-count PET denoising remains a challenge and is still
highly under-explored. In this work, we propose FedFTN, a personalized
federated learning strategy that addresses these challenges. FedFTN uses a
local deep feature transformation network (FTN) to modulate the feature outputs
of a globally shared denoising network, enabling personalized low-count PET
denoising for each institution. During the federated learning process, only the
denoising network's weights are communicated and aggregated, while the FTN
remains at the local institutions for feature transformation. We evaluated our
method using a large-scale dataset of multi-institutional low-count PET imaging
data from three medical centers located across three continents, and showed
that FedFTN provides high-quality low-count PET images, outperforming previous
baseline FL reconstruction methods across all low-count levels at all three
institutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01814">CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization. (arXiv:2304.01814v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1">Qi Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zilong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Junping Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1">Hongming Shan</a></p>
<p>Low-dose computed tomography (CT) images suffer from noise and artifacts due
to photon starvation and electronic noise. Recently, some works have attempted
to use diffusion models to address the over-smoothness and training instability
encountered by previous deep-learning-based denoising models. However,
diffusion models suffer from long inference times due to the large number of
sampling steps involved. Very recently, cold diffusion model generalizes
classical diffusion models and has greater flexibility. Inspired by the cold
diffusion, this paper presents a novel COntextual eRror-modulated gEneralized
Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,
CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs
a novel mean-preserving degradation operator to mimic the physical process of
CT degradation, significantly reducing sampling steps thanks to the informative
LDCT images as the starting point of the sampling process. Second, to alleviate
the error accumulation problem caused by the imperfect restoration operator in
the sampling process, we propose a novel ContextuaL Error-modulAted Restoration
Network (CLEAR-Net), which can leverage contextual information to constrain the
sampling process from structural distortion and modulate time step embedding
features for better alignment with the input at the next time step. Third, to
rapidly generalize to a new, unseen dose level with as few resources as
possible, we devise a one-shot learning framework to make CoreDiff generalize
faster and better using only a single LDCT image (un)paired with NDCT.
Extensive experimental results on two datasets demonstrate that our CoreDiff
outperforms competing methods in denoising and generalization performance, with
a clinically acceptable inference time. Source code is made available at
https://github.com/qgao21/CoreDiff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03040">TUVF: Learning Generalizable Texture UV Radiance Fields. (arXiv:2305.03040v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1">An-Chieh Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xueting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sifei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>Textures are a vital aspect of creating visually appealing and realistic 3D
models. In this paper, we study the problem of generating high-fidelity texture
given shapes of 3D assets, which has been relatively less explored compared
with generic 3D shape modeling. Our goal is to facilitate a controllable
texture generation process, such that one texture code can correspond to a
particular appearance style independent of any input shapes from a category. We
introduce Texture UV Radiance Fields (TUVF) that generate textures in a
learnable UV sphere space rather than directly on the 3D shape. This allows the
texture to be disentangled from the underlying shape and transferable to other
shapes that share the same UV space, i.e., from the same category. We integrate
the UV sphere space with the radiance field, which provides a more efficient
and accurate representation of textures than traditional texture maps. We
perform our experiments on synthetic and real-world object datasets where we
achieve not only realistic synthesis but also substantial improvements over
state-of-the-arts on texture controlling and editing. Project Page:
https://www.anjiecheng.me/TUVF
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08851">MV-Map: Offboard HD-Map Generation with Multi-view Consistency. (arXiv:2305.08851v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Ziyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1">Ziqi Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxiong Wang</a></p>
<p>While bird's-eye-view (BEV) perception models can be useful for building
high-definition maps (HD-Maps) with less human labor, their results are often
unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps
from different viewpoints. This is because BEV perception is typically set up
in an 'onboard' manner, which restricts the computation and consequently
prevents algorithms from reasoning multiple views simultaneously. This paper
overcomes these limitations and advocates a more practical 'offboard' HD-Map
generation setup that removes the computation constraints, based on the fact
that HD-Maps are commonly reusable infrastructures built offline in data
centers. To this end, we propose a novel offboard pipeline called MV-Map that
capitalizes multi-view consistency and can handle an arbitrary number of frames
with the key design of a 'region-centric' framework. In MV-Map, the target
HD-Maps are created by aggregating all the frames of onboard predictions,
weighted by the confidence scores assigned by an 'uncertainty network'. To
further enhance multi-view consistency, we augment the uncertainty network with
the global 3D structure optimized by a voxelized neural radiance field
(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map
significantly improves the quality of HD-Maps, further highlighting the
importance of offboard methods for HD-Map generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18030">Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1">Luming Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1">Tianyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1">Ilya Zharkov</a></p>
<p>To search an optimal sub-network within a general deep neural network (DNN),
existing neural architecture search (NAS) methods typically rely on
handcrafting a search space beforehand. Such requirements make it challenging
to extend them onto general scenarios without significant human expertise and
manual intervention. To overcome the limitations, we propose Automated
Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first
automated system to train general DNNs that cover all candidate connections and
operations and produce high-performing sub-networks in the one shot manner.
Technologically, ASGNAS delivers three noticeable contributions to minimize
human efforts: (i) automated search space generation for general DNNs; (ii) a
Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy
and dependency within generated search space to ensure the network validity
during optimization, and reliably produces a solution with both high
performance and hierarchical group sparsity; and (iii) automated sub-network
construction upon the H2SPG solution. Numerically, we demonstrate the
effectiveness of ASGNAS on a variety of general DNNs, including RegNet,
StackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,
Fashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS
achieve competitive even superior performance compared to the starting full
DNNs and other state-of-the-arts. The library will be released at
https://github.com/tianyic/only_train_once.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02099">NeuroSURF: Neural Uncertainty-aware Robust Surface Reconstruction. (arXiv:2306.02099v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sang_L/0/1/0/all/0/1">Lu Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Saroha_A/0/1/0/all/0/1">Abhishek Saroha</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Maolin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>Neural implicit functions have become popular for representing surfaces
because they offer an adaptive resolution and support arbitrary topologies.
While previous works rely on ground truth point clouds, they often ignore the
effect of input quality and sampling methods on the reconstruction. In this
paper, we introduce NeuroSURF, which generates significantly improved
qualitative and quantitative reconstructions driven by a novel sampling and
interpolation technique. We show that employing a sampling technique that
considers the geometric characteristics of inputs can enhance the training
process. To this end, we introduce a strategy that efficiently computes
differentiable geometric features, namely, mean curvatures, to augment the
sampling phase during the training period. Moreover, we augment the neural
implicit surface representation with uncertainty, which offers insights into
the occupancy and reliability of the output signed distance value, thereby
expanding representation capabilities into open surfaces. Finally, we
demonstrate that NeuroSURF leads to state-of-the-art reconstructions on both
synthetic and real-world data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11363">Masked Diffusion Models Are Fast Distribution Learners. (arXiv:2306.11363v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jiachen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qinglong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1">Zhongjie Ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenguang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>Diffusion models have emerged as the \emph{de-facto} generative model for
image synthesis, yet they entail significant training overhead, hindering the
technique's broader adoption in the research community. We observe that these
models are commonly trained to learn all fine-grained visual information from
scratch, thus motivating our investigation on its necessity. In this work, we
show that it suffices to set up pre-training stage to initialize a diffusion
model by encouraging it to learn some primer distribution of the unknown real
image distribution. Then the pre-trained model can be fine-tuned for specific
generation tasks efficiently. To approximate the primer distribution, our
approach centers on masking a high proportion (e.g., up to 90\%) of an input
image and employing masked denoising score matching to denoise visible areas.
Utilizing the learned primer distribution in subsequent fine-tuning, we
efficiently train a ViT-based diffusion model on CelebA-HQ $256 \times 256$ in
the raw pixel space, achieving superior training acceleration compared to
denoising diffusion probabilistic model (DDPM) counterpart and a new FID score
record of 6.73 for ViT-based diffusion models. Moreover, our masked
pre-training technique can be universally applied to various diffusion models
that directly generate images in the pixel space, aiding in the learning of
pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning on only 10\% data from a different dataset. Our code is available
at \url{https://github.com/jiachenlei/maskdm}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14435">DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yujun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1">Chuhui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1">Jun Hao Liew</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiachun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hanshu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1">Vincent Y. F. Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Song Bai</a></p>
<p>Accurate and controllable image editing is a challenging task that has
attracted significant attention recently. Notably, DragGAN is an interactive
point-based image editing framework that achieves impressive editing results
with pixel-level precision. However, due to its reliance on generative
adversarial networks (GANs), its generality is limited by the capacity of
pretrained GAN models. In this work, we extend this editing framework to
diffusion models and propose a novel approach DragDiffusion. By harnessing
large-scale pretrained diffusion models, we greatly enhance the applicability
of interactive point-based editing on both real and diffusion-generated images.
Our approach involves optimizing the diffusion latents to achieve precise
spatial control. The supervision signal of this optimization process is from
the diffusion model's UNet features, which are known to contain rich semantic
and geometric information. Moreover, we introduce two additional techniques,
namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity
of the original image. Lastly, we present a challenging benchmark dataset
called DragBench -- the first benchmark to evaluate the performance of
interactive point-based image editing methods. Experiments across a wide range
of challenging cases (e.g., images with multiple objects, diverse object
categories, various styles, etc.) demonstrate the versatility and generality of
DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15350">CellViT: Vision Transformers for Precise Cell Segmentation and Classification. (arXiv:2306.15350v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Horst_F/0/1/0/all/0/1">Fabian H&#xf6;rst</a>, <a href="http://arxiv.org/find/eess/1/au:+Rempe_M/0/1/0/all/0/1">Moritz Rempe</a>, <a href="http://arxiv.org/find/eess/1/au:+Heine_L/0/1/0/all/0/1">Lukas Heine</a>, <a href="http://arxiv.org/find/eess/1/au:+Seibold_C/0/1/0/all/0/1">Constantin Seibold</a>, <a href="http://arxiv.org/find/eess/1/au:+Keyl_J/0/1/0/all/0/1">Julius Keyl</a>, <a href="http://arxiv.org/find/eess/1/au:+Baldini_G/0/1/0/all/0/1">Giulia Baldini</a>, <a href="http://arxiv.org/find/eess/1/au:+Ugurel_S/0/1/0/all/0/1">Selma Ugurel</a>, <a href="http://arxiv.org/find/eess/1/au:+Siveke_J/0/1/0/all/0/1">Jens Siveke</a>, <a href="http://arxiv.org/find/eess/1/au:+Grunwald_B/0/1/0/all/0/1">Barbara Gr&#xfc;nwald</a>, <a href="http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1">Jan Egger</a>, <a href="http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a></p>
<p>Nuclei detection and segmentation in hematoxylin and eosin-stained (H&amp;E)
tissue images are important clinical tasks and crucial for a wide range of
applications. However, it is a challenging task due to nuclei variances in
staining and size, overlapping boundaries, and nuclei clustering. While
convolutional neural networks have been extensively used for this task, we
explore the potential of Transformer-based networks in this domain. Therefore,
we introduce a new method for automated instance segmentation of cell nuclei in
digitized tissue samples using a deep learning architecture based on Vision
Transformer called CellViT. CellViT is trained and evaluated on the PanNuke
dataset, which is one of the most challenging nuclei instance segmentation
datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically
important classes in 19 tissue types. We demonstrate the superiority of
large-scale in-domain and out-of-domain pre-trained Vision Transformers by
leveraging the recently published Segment Anything Model and a ViT-encoder
pre-trained on 104 million histological image patches - achieving
state-of-the-art nuclei detection and instance segmentation performance on the
PanNuke dataset with a mean panoptic quality of 0.50 and an F1-detection score
of 0.83. The code is publicly available at https://github.com/TIO-IKIM/CellViT
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03887">Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Netzorg_R/0/1/0/all/0/1">Robin Netzorg</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a></p>
<p>In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model's output to specific
features of the data. One such of these methods is the Prototypical Part
Network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
it often learns to classify from spurious or inconsistent parts of the image.
Hoping to remedy this, we take inspiration from the recent developments in
Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns
human preferences and identify non-spurious prototypes. In place of a full RL
update, we propose the Reweighed, Reselected, and Retrained Prototypical Part
Network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet
training loop. The first two steps are reward-based reweighting and
reselection, which align prototypes with human feedback. The final step is
retraining to realign the model's features with the updated prototypes. We find
that R3-ProtoPNet improves the overall meaningfulness of the prototypes, and
maintains or improves individual model performance. When multiple trained
R3-ProtoPNets are incorporated into an ensemble, we find increases in both
interpretability and predictive performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13717">On the Leakage of Fuzzy Matchers. (arXiv:2307.13717v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1">Axel Durbet</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1">Kevin Thiry-Atighehchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chagnon_D/0/1/0/all/0/1">Dorine Chagnon</a>, <a href="http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1">Paul-Marie Grollemund</a></p>
<p>In a biometric recognition system, the matcher compares an old and a fresh
template to decide if it is a match or not. Beyond the binary output (`yes' or
`no'), more information is computed. This paper provides an in-depth analysis
of information leakage during distance evaluation, with an emphasis on
threshold-based obfuscated distance (\textit{i.e.}, Fuzzy Matcher). Leakage can
occur due to a malware infection or the use of a weakly privacy-preserving
matcher, exemplified by side channel attacks or partially obfuscated designs.
We provide an exhaustive catalog of information leakage scenarios as well as
their impacts on the security concerning data privacy. Each of the scenarios
leads to generic attacks whose impacts are expressed in terms of computational
costs, hence allowing the establishment of upper bounds on the security level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01251">Hyper-pixel-wise Contrastive Learning Augmented Segmentation Network for Old Landslide Detection through Fusing High-Resolution Remote Sensing Images and Digital Elevation Model Data. (arXiv:2308.01251v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yuexing Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Junchuan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_D/0/1/0/all/0/1">Daqing Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wei Xiang</a></p>
<p>As a natural disaster, landslide often brings tremendous losses to human
lives, so it urgently demands reliable detection of landslide risks. When
detecting old landslides that present important information for landslide risk
warning, problems such as visual blur and small-sized dataset cause great
challenges when using remote sensing data. To extract accurate semantic
features, a hyper-pixel-wise contrastive learning augmented segmentation
network (HPCL-Net) is proposed, which augments the local salient feature
extraction from boundaries of landslides through HPCL-Net and fuses
heterogeneous infromation in the semantic space from high-resolution remote
sensing images and digital elevation model data. For full utilization of
precious samples, a global hyper-pixel-wise sample pair queues-based
contrastive learning method is developed, which includes the construction of
global queues that store hyper-pixel-wise samples and the updating scheme of a
momentum encoder, reliably enhancing the extraction ability of semantic
features. The proposed HPCL-Net is evaluated on the Loess Plateau old landslide
dataset and experimental results verify that the proposed HPCL-Net greatly
outperforms existing models, where the mIoU is increased from 0.620 to 0.651,
the Landslide IoU is improved from 0.334 to 0.394 and the F1score is enhanced
from 0.501 to 0.565.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09375">Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package. (arXiv:2308.09375v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rasti_B/0/1/0/all/0/1">Behnood Rasti</a> (HZDR), <a href="http://arxiv.org/find/eess/1/au:+Zouaoui_A/0/1/0/all/0/1">Alexandre Zouaoui</a> (Thoth), <a href="http://arxiv.org/find/eess/1/au:+Mairal_J/0/1/0/all/0/1">Julien Mairal</a> (Thoth), <a href="http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1">Jocelyn Chanussot</a> (Thoth)</p>
<p>Spectral pixels are often a mixture of the pure spectra of the materials,
called endmembers, due to the low spatial resolution of hyperspectral sensors,
double scattering, and intimate mixtures of materials in the scenes. Unmixing
estimates the fractional abundances of the endmembers within the pixel.
Depending on the prior knowledge of endmembers, linear unmixing can be divided
into three main groups: supervised, semi-supervised, and unsupervised (blind)
linear unmixing. Advances in Image processing and machine learning
substantially affected unmixing. This paper provides an overview of advanced
and conventional unmixing approaches. Additionally, we draw a critical
comparison between advanced and conventional techniques from the three
categories. We compare the performance of the unmixing techniques on three
simulated and two real datasets. The experimental results reveal the advantages
of different unmixing categories for different unmixing scenarios. Moreover, we
provide an open-source Python-based package available at
https://github.com/BehnoodRasti/HySUPP to reproduce the results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12462">Overcoming General Knowledge Loss with Selective Parameter Update. (arXiv:2308.12462v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Janson_P/0/1/0/all/0/1">Paul Janson</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1">Rahaf Aljundi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in continuously updating foundation models to
accommodate novel information while retaining their original capabilities.
Leveraging the fact that foundation models have initial knowledge on various
tasks and domains, we propose a novel approach that, instead of updating all
parameters equally, localizes the updates to a sparse set of parameters
relevant to the task being learned. We strike a balance between efficiency and
new tasks performance, while maintaining the transferability and
generalizability of foundation models. We extensively evaluate our method on
foundational vision-language models with a diverse spectrum of continual
learning tasks. Our method achieves improvements on the newly learned tasks
accuracy up to 7% while preserving the pretraining knowledge with a negligible
decrease of 0.9% on a representative control set accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05257">FusionFormer: A Multi-sensory Fusion in Bird&#x27;s-Eye-View and Temporal Consistent Transformer for 3D Object Detection. (arXiv:2309.05257v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chunyong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianyun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Weibo Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Maochun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lingxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingxia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1">Qihao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kaixuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiru Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_P/0/1/0/all/0/1">Peihan Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minzhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kaicheng Yu</a></p>
<p>Multi-sensor modal fusion has demonstrated strong advantages in 3D object
detection tasks. However, existing methods that fuse multi-modal features
require transforming features into the bird's eye view space and may lose
certain information on Z-axis, thus leading to inferior performance. To this
end, we propose a novel end-to-end multi-modal fusion transformer-based
framework, dubbed FusionFormer, that incorporates deformable attention and
residual structures within the fusion encoding module. Specifically, by
developing a uniform sampling strategy, our method can easily sample from 2D
image and 3D voxel features spontaneously, thus exploiting flexible
adaptability and avoiding explicit transformation to the bird's eye view space
during the feature concatenation process. We further implement a residual
structure in our feature encoder to ensure the model's robustness in case of
missing an input modality. Through extensive experiments on a popular
autonomous driving benchmark dataset, nuScenes, our method achieves
state-of-the-art single model performance of 72.6% mAP and 75.1% NDS in the 3D
object detection task without test time augmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09464">Reducing Adversarial Training Cost with Gradient Approximation. (arXiv:2309.09464v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1">Huihui Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Siqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1">Seyit Camtepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1">Surya Nepal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a></p>
<p>Deep learning models have achieved state-of-the-art performances in various
domains, while they are vulnerable to the inputs with well-crafted but small
perturbations, which are named after adversarial examples (AEs). Among many
strategies to improve the model robustness against AEs, Projected Gradient
Descent (PGD) based adversarial training is one of the most effective methods.
Unfortunately, the prohibitive computational overhead of generating strong
enough AEs, due to the maximization of the loss function, sometimes makes the
regular PGD adversarial training impractical when using larger and more
complicated models. In this paper, we propose that the adversarial loss can be
approximated by the partial sum of Taylor series. Furthermore, we approximate
the gradient of adversarial loss and propose a new and efficient adversarial
training method, adversarial training with gradient approximation (GAAT), to
reduce the cost of building up robust models. Additionally, extensive
experiments demonstrate that this efficiency improvement can be achieved
without any or with very little loss in accuracy on natural and adversarial
examples, which show that our proposed method saves up to 60\% of the training
time with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12245">Adaptive Input-image Normalization for Solving the Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Saad_M/0/1/0/all/0/1">Muhammad Muneeb Saad</a>, <a href="http://arxiv.org/find/eess/1/au:+Rehmani_M/0/1/0/all/0/1">Mubashir Husain Rehmani</a>, <a href="http://arxiv.org/find/eess/1/au:+OReilly_R/0/1/0/all/0/1">Ruairi O&#x27;Reilly</a></p>
<p>Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment datasets.
It is important to generate synthetic images that incorporate a diverse range
of features to accurately represent the distribution of features present in the
training imagery. Furthermore, the absence of diverse features in synthetic
images can degrade the performance of machine learning classifiers. The mode
collapse problem impacts Generative Adversarial Networks' capacity to generate
diversified images. Mode collapse comes in two varieties: intra-class and
inter-class. In this paper, both varieties of the mode collapse problem are
investigated, and their subsequent impact on the diversity of synthetic X-ray
images is evaluated. This work contributes an empirical demonstration of the
benefits of integrating the adaptive input-image normalization with the Deep
Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse
problems. Synthetically generated images are utilized for data augmentation and
training a Vision Transformer model. The classification performance of the
model is evaluated using accuracy, recall, and precision scores. Results
demonstrate that the DCGAN and the ACGAN with adaptive input-image
normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as
evidenced by the superior diversity scores and classification scores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13196">ClusterFormer: Clustering As A Universal Visual Learner. (arXiv:2309.13196v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">James C. Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yiming Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1">Tong Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenguan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dongfang Liu</a></p>
<p>This paper presents CLUSTERFORMER, a universal vision model that is based on
the CLUSTERing paradigm with TransFORMER. It comprises two novel designs: 1.
recurrent cross-attention clustering, which reformulates the cross-attention
mechanism in Transformer and enables recursive updates of cluster centers to
facilitate strong representation learning; and 2. feature dispatching, which
uses the updated cluster centers to redistribute image features through
similarity-based metrics, resulting in a transparent pipeline. This elegant
design streamlines an explainable and transferable workflow, capable of
tackling heterogeneous vision tasks (i.e., image classification, object
detection, and image segmentation) with varying levels of clustering
granularity (i.e., image-, box-, and pixel-level). Empirical results
demonstrate that CLUSTERFORMER outperforms various well-known specialized
architectures, achieving 83.41% top-1 acc. over ImageNet-1K for image
classification, 54.2% and 47.0% mAP over MS COCO for object detection and
instance segmentation, 52.4% mIoU over ADE20K for semantic segmentation, and
55.8% PQ over COCO Panoptic for panoptic segmentation. For its efficacy, we
hope our work can catalyze a paradigm shift in universal models in computer
vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14293">NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1">Saeejith Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1">Mohammad Javad Shafiee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1">Alexander Wong</a></p>
<p>Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16108">Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1">Yujia Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivanandan_S/0/1/0/all/0/1">Srinivasan Sivanandan</a>, <a href="http://arxiv.org/find/cs/1/au:+Karaletsos_T/0/1/0/all/0/1">Theofanis Karaletsos</a></p>
<p>Vision Transformer (ViT) has emerged as a powerful architecture in the realm
of modern computer vision. However, its application in certain imaging fields,
such as microscopy and satellite imaging, presents unique challenges. In these
domains, images often contain multiple channels, each carrying semantically
distinct and independent information. Furthermore, the model must demonstrate
robustness to sparsity in input channels, as they may not be densely available
during training or testing. In this paper, we propose a modification to the ViT
architecture that enhances reasoning across the input channels and introduce
Hierarchical Channel Sampling (HCS) as an additional regularization technique
to ensure robustness when only partial channels are presented during test time.
Our proposed model, ChannelViT, constructs patch tokens independently from each
input channel and utilizes a learnable channel embedding that is added to the
patch tokens, similar to positional embeddings. We evaluate the performance of
ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat
(satellite imaging). Our results show that ChannelViT outperforms ViT on
classification tasks and generalizes well, even when a subset of input channels
is used during testing. Across our experiments, HCS proves to be a powerful
regularizer, independent of the architecture employed, suggesting itself as a
straightforward technique for robust ViT training. Lastly, we find that
ChannelViT generalizes effectively even when there is limited access to all
channels during training, highlighting its potential for multi-channel imaging
under real-world conditions with sparse sensors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00558">Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes. (arXiv:2310.00558v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Alloy Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Sanket Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1">Umapada Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1">Josep Llad&#xf3;s</a></p>
<p>When used in a real-world noisy environment, the capacity to generalize to
multiple domains is essential for any autonomous scene text spotting system.
However, existing state-of-the-art methods employ pretraining and fine-tuning
strategies on natural scene datasets, which do not exploit the feature
interaction across other complex domains. In this work, we explore and
investigate the problem of domain-agnostic scene text spotting, i.e., training
a model on multi-domain source data such that it can directly generalize to
target domains rather than being specialized for a specific domain or scenario.
In this regard, we present the community a text spotting validation benchmark
called Under-Water Text (UWT) for noisy underwater scenes to establish an
important case study. Moreover, we also design an efficient super-resolution
based end-to-end transformer baseline called DA-TextSpotter which achieves
comparable or superior performance over existing text spotting architectures
for both regular and arbitrary-shaped scene text spotting benchmarks in terms
of both accuracy and model efficiency. The dataset, code and pre-trained models
will be released upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00917">Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards Enhancing Text Spotting Performance. (arXiv:2310.00917v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Alloy Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Sanket Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1">Ayan Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Saumik Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1">Josep Llad&#xf3;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1">Umapada Pal</a></p>
<p>The adaptation capability to a wide range of domains is crucial for scene
text spotting models when deployed to real-world conditions. However, existing
state-of-the-art (SOTA) approaches usually incorporate scene text detection and
recognition simply by pretraining on natural scene text datasets, which do not
directly exploit the intermediate feature representations between multiple
domains. Here, we investigate the problem of domain-adaptive scene text
spotting, i.e., training a model on multi-domain source data such that it can
directly adapt to target domains rather than being specialized for a specific
domain or scenario. Further, we investigate a transformer baseline called
Swin-TESTR to focus on solving scene-text spotting for both regular and
arbitrary-shaped scene text along with an exhaustive evaluation. The results
clearly demonstrate the potential of intermediate representations to achieve
significant performance on text spotting benchmarks across multiple domains
(e.g. language, synth-to-real, and documents). both in terms of accuracy and
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02239">MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. (arXiv:2310.02239v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kaizhi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuehai He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a></p>
<p>Large Language Models (LLMs) have garnered significant attention for their
advancements in natural language processing, demonstrating unparalleled prowess
in text comprehension and generation. Yet, the simultaneous generation of
images with coherent textual narratives remains an evolving frontier. In
response, we introduce an innovative interleaved vision-and-language generation
technique anchored by the concept of "generative vokens," acting as the bridge
for harmonized image-text outputs. Our approach is characterized by a
distinctive two-staged training strategy focusing on description-free
multimodal generation, where the training requires no comprehensive
descriptions of images. To bolster model integrity, classifier-free guidance is
incorporated, enhancing the effectiveness of vokens on image generation. Our
model, MiniGPT-5, exhibits substantial improvement over the baseline Divter
model on the MMDialog dataset and consistently delivers superior or comparable
multimodal outputs in human evaluations on the VIST dataset, highlighting its
efficacy across diverse benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02492">Harvard Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling. (arXiv:2310.02492v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Min Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elze_T/0/1/0/all/0/1">Tobias Elze</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengyu Wang</a></p>
<p>Fairness or equity in machine learning is profoundly important for societal
well-being, but limited public datasets hinder its progress, especially in the
area of medicine. It is undeniable that fairness in medicine is one of the most
important areas for fairness learning's applications. Currently, no large-scale
public medical datasets with 3D imaging data for fairness learning are
available, while 3D imaging data in modern clinics are standard tests for
disease diagnosis. In addition, existing medical fairness datasets are actually
repurposed datasets, and therefore they typically have limited demographic
identity attributes with at most three identity attributes of age, gender, and
race for fairness modeling. To address this gap, we introduce our Eye Fairness
dataset with 30,000 subjects (Harvard-EF) covering three major eye diseases
including age-related macular degeneration, diabetic retinopathy, and glaucoma
affecting 380 million patients globally. Our Harvard-EF dataset includes both
2D fundus photos and 3D optical coherence tomography scans with six demographic
identity attributes including age, gender, race, ethnicity, preferred language,
and marital status. We also propose a fair identity scaling (FIS) approach
combining group and individual scaling together to improve model fairness. Our
FIS approach is compared with various state-of-the-art fairness learning
methods with superior performance in the racial, gender, and ethnicity fairness
tasks with 2D and 3D imaging data, which demonstrate the utilities of our
Harvard-EF dataset for fairness learning. To facilitate fairness comparisons
between different models, we propose performance-scaled disparity measures,
which can be used to compare model fairness accounting for overall performance
levels. The dataset and code are publicly accessible via
https://ophai.hms.harvard.edu/datasets/harvard-ef30k.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02687">USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields. (arXiv:2310.02687v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Moyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lingzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1">Bangyan Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peidong Liu</a></p>
<p>Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03149">Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jonathan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a></p>
<p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03205">A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1">Kim Youwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1">Lee Hyun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1">Kim Sung-Bin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1">Suekyeong Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1">Janghoon Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1">Tae-Hyun Oh</a></p>
<p>We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03559">MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images. (arXiv:2310.03559v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1">Li Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Visweswaran_S/0/1/0/all/0/1">Shyam Visweswaran</a>, <a href="http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1">Kayhan Batmanghelich</a></p>
<p>This paper introduces an innovative methodology for producing high-quality 3D
lung CT images guided by textual information. While diffusion-based generative
models are increasingly used in medical imaging, current state-of-the-art
approaches are limited to low-resolution outputs and underutilize radiology
reports' abundant information. The radiology reports can enhance the generation
process by providing additional guidance and offering fine-grained control over
the synthesis of images. Nevertheless, expanding text-guided generation to
high-resolution 3D images poses significant memory and anatomical
detail-preserving challenges. Addressing the memory issue, we introduce a
hierarchical scheme that uses a modified UNet architecture. We start by
synthesizing low-resolution images conditioned on the text, serving as a
foundation for subsequent generators for complete volumetric data. To ensure
the anatomical plausibility of the generated samples, we provide further
guidance by generating vascular, airway, and lobular segmentation masks in
conjunction with the CT images. The model demonstrates the capability to use
textual input and segmentation tasks to generate synthesized images. The
results of comparative assessments indicate that our approach exhibits superior
performance compared to the most advanced models based on GAN and diffusion
techniques, especially in accurately retaining crucial anatomical features such
as fissure lines, airways, and vascular structures. This innovation introduces
novel possibilities. This study focuses on two main objectives: (1) the
development of a method for creating images based on textual prompts and
anatomical components, and (2) the capability to generate new images
conditioning on anatomical elements. The advancements in image generation can
be applied to enhance numerous downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05608">Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1">Sourav Medya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1">Sathya N. Ravi</a></p>
<p>Often, deep network models are purely inductive during training and while
performing inference on unseen data. Thus, when such models are used for
predictions, it is well known that they often fail to capture the semantic
information and implicit dependencies that exist among objects (or concepts) on
a population level. Moreover, it is still unclear how domain or prior modal
knowledge can be specified in a backpropagation friendly manner, especially in
large-scale and noisy settings. In this work, we propose an end-to-end vision
and language model incorporating explicit knowledge graphs. We also introduce
an interactive out-of-distribution (OOD) layer using implicit network operator.
The layer is used to filter noise that is brought by external knowledge base.
In practice, we apply our model on several vision and language downstream tasks
including visual question answering, visual reasoning, and image-text retrieval
on different datasets. Our experiments show that it is possible to design
models that perform similarly to state-of-art results but with significantly
fewer samples and training time.
</p>
</p>
</div>

    </div>
    </body>
    