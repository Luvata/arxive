<!DOCTYPE html>
<html>
<head>
<title>2023-12-26-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.14180">Dynamic Topic Language Model on Heterogeneous Children&#x27;s Mental Health Clinical Notes. (arXiv:2312.14180v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Hanwen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreno_T/0/1/0/all/0/1">Tatiana Moreno</a>, <a href="http://arxiv.org/find/cs/1/au:+Alpern_A/0/1/0/all/0/1">Adrianne Alpern</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehwerhemuepha_L/0/1/0/all/0/1">Louis Ehwerhemuepha</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_A/0/1/0/all/0/1">Annie Qu</a></p>
<p>Mental health diseases affect children's lives and well-beings which have
received increased attention since the COVID-19 pandemic. Analyzing psychiatric
clinical notes with topic models is critical to evaluate children's mental
status over time. However, few topic models are built for longitudinal
settings, and they fail to keep consistent topics and capture temporal
trajectories for each document. To address these challenges, we develop a
longitudinal topic model with time-invariant topics and individualized temporal
dependencies on the evolving document metadata. Our model preserves the
semantic meaning of discovered topics over time and incorporates heterogeneity
among documents. In particular, when documents can be categorized, we propose
an unsupervised topics learning approach to maximize topic heterogeneity across
different document groups. We also present an efficient variational
optimization procedure adapted for the multistage longitudinal setting. In this
case study, we apply our method to the psychiatric clinical notes from a large
tertiary pediatric hospital in Southern California and achieve a 38% increase
in the overall coherence of extracted topics. Our real data analysis reveals
that children tend to express more negative emotions during state shutdowns and
more positive when schools reopen. Furthermore, it suggests that sexual and
gender minority (SGM) children display more pronounced reactions to major
COVID-19 events and a greater sensitivity to vaccine-related news than non-SGM
children. This study examines the progression of children's mental health
during the pandemic and offers clinicians valuable insights to recognize the
disparities in children's mental health related to their sexual and gender
identities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14182">Find the Lady: Permutation and Re-Synchronization of Deep Neural Networks. (arXiv:2312.14182v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trias_C/0/1/0/all/0/1">Carl De Sousa Trias</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitrea_M/0/1/0/all/0/1">Mihai Petru Mitrea</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiandrotti_A/0/1/0/all/0/1">Attilio Fiandrotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cagnazzo_M/0/1/0/all/0/1">Marco Cagnazzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Sumanta Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1">Enzo Tartaglione</a></p>
<p>Deep neural networks are characterized by multiple symmetrical, equi-loss
solutions that are redundant. Thus, the order of neurons in a layer and feature
maps can be given arbitrary permutations, without affecting (or minimally
affecting) their output. If we shuffle these neurons, or if we apply to them
some perturbations (like fine-tuning) can we put them back in the original
order i.e. re-synchronize? Is there a possible corruption threat? Answering
these questions is important for applications like neural network white-box
watermarking for ownership tracking and integrity verification. We advance a
method to re-synchronize the order of permuted neurons. Our method is also
effective if neurons are further altered by parameter pruning, quantization,
and fine-tuning, showing robustness to integrity attacks. Additionally, we
provide theoretical and practical evidence for the usual means to corrupt the
integrity of the model, resulting in a solution to counter it. We test our
approach on popular computer vision datasets and models, and we illustrate the
threat and our countermeasure on a popular white-box watermarking method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14184">Large Language Models in Medical Term Classification and Unexpected Misalignment Between Response and Reasoning. (arXiv:2312.14184v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaodan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_S/0/1/0/all/0/1">Sandeep Vemulapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_N/0/1/0/all/0/1">Nabasmita Talukdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sumyeong Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiankun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Han Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Murtaza_S/0/1/0/all/0/1">Sardar Mehtab Bin Murtaza</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1">Aakash Ajay Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Leshchiner_D/0/1/0/all/0/1">Dmitry Leshchiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Joseph_D/0/1/0/all/0/1">Dimitri F. Joseph</a>, <a href="http://arxiv.org/find/cs/1/au:+Witteveen_Lane_M/0/1/0/all/0/1">Martin Witteveen-Lane</a>, <a href="http://arxiv.org/find/cs/1/au:+Chesla_D/0/1/0/all/0/1">Dave Chesla</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiayu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bin Chen</a></p>
<p>This study assesses the ability of state-of-the-art large language models
(LLMs) including GPT-3.5, GPT-4, Falcon, and LLaMA 2 to identify patients with
mild cognitive impairment (MCI) from discharge summaries and examines instances
where the models' responses were misaligned with their reasoning. Utilizing the
MIMIC-IV v2.2 database, we focused on a cohort aged 65 and older, verifying MCI
diagnoses against ICD codes and expert evaluations. The data was partitioned
into training, validation, and testing sets in a 7:2:1 ratio for model
fine-tuning and evaluation, with an additional metastatic cancer dataset from
MIMIC III used to further assess reasoning consistency. GPT-4 demonstrated
superior interpretative capabilities, particularly in response to complex
prompts, yet displayed notable response-reasoning inconsistencies. In contrast,
open-source models like Falcon and LLaMA 2 achieved high accuracy but lacked
explanatory reasoning, underscoring the necessity for further research to
optimize both performance and interpretability. The study emphasizes the
significance of prompt engineering and the need for further exploration into
the unexpected reasoning-response misalignment observed in GPT-4. The results
underscore the promise of incorporating LLMs into healthcare diagnostics,
contingent upon methodological advancements to ensure accuracy and clinical
coherence of AI-generated outputs, thereby improving the trustworthiness of
LLMs for medical decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14185">Auto311: A Confidence-guided Automated System for Non-emergency Call. (arXiv:2312.14185v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zirong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xutong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Meiyi Ma</a></p>
<p>Emergency and non-emergency response systems are essential services provided
by local governments and critical to protecting lives, the environment, and
property. The effective handling of (non-)emergency calls is critical for
public safety and well-being. By reducing the burden through non-emergency
callers, residents in critical need of assistance through 911 will receive a
fast and effective response. Collaborating with the Department of Emergency
Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call
recordings and developed Auto311, the first automated system to handle 311
non-emergency calls, which (1) effectively and dynamically predicts ongoing
non-emergency incident types to generate tailored case reports during the call;
(2) itemizes essential information from dialogue contexts to complete the
generated reports; and (3) strategically structures system-caller dialogues
with optimized confidence. We used real-world data to evaluate the system's
effectiveness and deployability. The experimental results indicate that the
system effectively predicts incident type with an average F-1 score of 92.54%.
Moreover, the system successfully itemizes critical information from relevant
contexts to complete reports, evincing a 0.93 average consistency score
compared to the ground truth. Additionally, emulations demonstrate that the
system effectively decreases conversation turns as the utterance size gets more
extensive and categorizes the ongoing call with 94.49% mean accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14188">Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method. (arXiv:2312.14188v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vishwakarma_R/0/1/0/all/0/1">Rahul Vishwakarma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Subhankar Mishra</a></p>
<p>Theorem proving is a fundamental task in mathematics. With the advent of
large language models (LLMs) and interactive theorem provers (ITPs) like Lean,
there has been growing interest in integrating LLMs and ITPs to automate
theorem proving. In this approach, the LLM generates proof steps (tactics), and
the ITP checks the applicability of the tactics at the current goal. The two
systems work together to complete the proof. In this paper, we introduce
DS-Prover, a novel dynamic sampling method for theorem proving. This method
dynamically determines the number of tactics to apply to expand the current
goal, taking into account the remaining time compared to the total allocated
time for proving a theorem. This makes the proof search process more efficient
by adjusting the balance between exploration and exploitation as time passes.
We also augment the training dataset by decomposing simplification and rewrite
tactics with multiple premises into tactics with single premises. This gives
the model more examples to learn from and helps it to predict the tactics with
premises more accurately. We perform our experiments using the Mathlib dataset
of the Lean theorem prover and report the performance on two standard datasets,
MiniF2F and ProofNet. Our methods achieve significant performance gains on both
datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the
ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the
best-reported Pass@1 of 29.6% using Lean.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14190">Machine Learning for Anomaly Detection in Particle Physics. (arXiv:2312.14190v1 [physics.data-an])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Belis_V/0/1/0/all/0/1">Vasilis Belis</a>, <a href="http://arxiv.org/find/physics/1/au:+Odagiu_P/0/1/0/all/0/1">Patrick Odagiu</a>, <a href="http://arxiv.org/find/physics/1/au:+%7B%5CAA%7Drrestad_T/0/1/0/all/0/1">Thea Kl&#xe6;boe &#xc5;rrestad</a></p>
<p>The detection of out-of-distribution data points is a common task in particle
physics. It is used for monitoring complex particle detectors or for
identifying rare and unexpected events that may be indicative of new phenomena
or physics beyond the Standard Model. Recent advances in Machine Learning for
anomaly detection have encouraged the utilization of such techniques on
particle physics problems. This review article provides an overview of the
state-of-the-art techniques for anomaly detection in particle physics using
machine learning. We discuss the challenges associated with anomaly detection
in large and complex data sets, such as those produced by high-energy particle
colliders, and highlight some of the successful applications of anomaly
detection in particle physics experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14193">Clustering and Uncertainty Analysis to Improve the Machine Learning-based Predictions of SAFARI-1 Control Follower Assembly Axial Neutron Flux Profiles. (arXiv:2312.14193v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moloko_L/0/1/0/all/0/1">Lesego Moloko</a>, <a href="http://arxiv.org/find/cs/1/au:+Bokov_P/0/1/0/all/0/1">Pavel Bokov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivanov_K/0/1/0/all/0/1">Kostadin Ivanov</a></p>
<p>The goal of this work is to develop accurate Machine Learning (ML) models for
predicting the assembly axial neutron flux profiles in the SAFARI-1 research
reactor, trained by measurement data from historical cycles. The data-driven
nature of ML models makes them susceptible to uncertainties which are
introduced by sources such as noise in training data, incomplete coverage of
the domain, extrapolation and imperfect model architectures. To this end, we
also aim at quantifying the approximation uncertainties of the ML model
predictions. Previous work using Deep Neural Networks (DNNs) has been
successful for fuel assemblies in SAFARI-1, however, not as accurate for
control follower assemblies. The aim of this work is to improve the ML models
for the control assemblies by a combination of supervised and unsupervised ML
algorithms. The $k$-means and Affinity Propagation unsupervised ML algorithms
are employed to identify clusters in the set of the measured axial neutron flux
profiles. Then, regression-based supervised ML models using DNN (with
prediction uncertainties quantified with Monte Carlo dropout) and Gaussian
Process (GP) are trained for different clusters and the prediction uncertainty
is estimated. It was found that applying the proposed procedure improves the
prediction accuracy for the control assemblies and reduces the prediction
uncertainty. Flux shapes predicted by DNN and GP are very close, and the
overall accuracy became comparable to the fuel assemblies. The prediction
uncertainty is however smaller for GP models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14196">Optimizing Heat Alert Issuance for Public Health in the United States with Reinforcement Learning. (arXiv:2312.14196v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Considine_E/0/1/0/all/0/1">Ellen M. Considine</a>, <a href="http://arxiv.org/find/cs/1/au:+Nethery_R/0/1/0/all/0/1">Rachel C. Nethery</a>, <a href="http://arxiv.org/find/cs/1/au:+Wellenius_G/0/1/0/all/0/1">Gregory A. Wellenius</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominici_F/0/1/0/all/0/1">Francesca Dominici</a>, <a href="http://arxiv.org/find/cs/1/au:+Tec_M/0/1/0/all/0/1">Mauricio Tec</a></p>
<p>Alerting the public when heat may harm their health is a crucial service,
especially considering that extreme heat events will be more frequent under
climate change. Current practice for issuing heat alerts in the US does not
take advantage of modern data science methods for optimizing local alert
criteria. Specifically, application of reinforcement learning (RL) has the
potential to inform more health-protective policies, accounting for regional
and sociodemographic heterogeneity as well as sequential dependence of alerts.
In this work, we formulate the issuance of heat alerts as a sequential decision
making problem and develop modifications to the RL workflow to address
challenges commonly encountered in environmental health settings. Key
modifications include creating a simulator that pairs hierarchical Bayesian
modeling of low-signal health effects with sampling of real weather
trajectories (exogenous features), constraining the total number of alerts
issued as well as preventing alerts on less-hot days, and optimizing
location-specific policies. Post-hoc contrastive analysis offers insights into
scenarios when using RL for heat alert issuance may protect public health
better than the current or alternative policies. This work contributes to a
broader movement of advancing data-driven policy optimization for public health
and climate change adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14203">Shai: A large language model for asset management. (arXiv:2312.14203v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Guo_Z/0/1/0/all/0/1">Zhongyang Guo</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jiang_G/0/1/0/all/0/1">Guanran Jiang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongdan Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Wang_Z/0/1/0/all/0/1">Zhefeng Wang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Wang_Y/0/1/0/all/0/1">Yinchun Wang</a></p>
<p>This paper introduces "Shai" a 10B level large language model specifically
designed for the asset management industry, built upon an open-source
foundational model. With continuous pre-training and fine-tuning using a
targeted corpus, Shai demonstrates enhanced performance in tasks relevant to
its domain, outperforming baseline models. Our research includes the
development of an innovative evaluation framework, which integrates
professional qualification exams, tailored tasks, open-ended question
answering, and safety assessments, to comprehensively assess Shai's
capabilities. Furthermore, we discuss the challenges and implications of
utilizing large language models like GPT-4 for performance assessment in asset
management, suggesting a combination of automated evaluation and human
judgment. Shai's development, showcasing the potential and versatility of
10B-level large language models in the financial sector with significant
performance and modest computational requirements, hopes to provide practical
insights and methodologies to assist industry peers in their similar endeavors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14204">Meta Transfer of Self-Supervised Knowledge: Foundation Model in Action for Post-Traumatic Epilepsy Prediction. (arXiv:2312.14204v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cui_W/0/1/0/all/0/1">Wenhui Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Akrami_H/0/1/0/all/0/1">Haleh Akrami</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1">Ganning Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1">Anand A. Joshi</a>, <a href="http://arxiv.org/find/eess/1/au:+Leahy_R/0/1/0/all/0/1">Richard M. Leahy</a></p>
<p>Despite the impressive advancements achieved using deep-learning for
functional brain activity analysis, the heterogeneity of functional patterns
and scarcity of imaging data still pose challenges in tasks such as prediction
of future onset of Post-Traumatic Epilepsy (PTE) from data acquired shortly
after traumatic brain injury (TBI). Foundation models pre-trained on separate
large-scale datasets can improve the performance from scarce and heterogeneous
datasets. For functional Magnetic Resonance Imaging (fMRI), while data may be
abundantly available from healthy controls, clinical data is often scarce,
limiting the ability of foundation models to identify clinically-relevant
features. We overcome this limitation by introducing a novel training strategy
for our foundation model by integrating meta-learning with self-supervised
learning to improve the generalization from normal to clinical features. In
this way we enable generalization to other downstream clinical tasks, in our
case prediction of PTE. To achieve this, we perform self-supervised training on
the control dataset to focus on inherent features that are not limited to a
particular supervised task while applying meta-learning, which strongly
improves the model's generalizability using bi-level optimization. Through
experiments on neurological disorder classification tasks, we demonstrate that
the proposed strategy significantly improves task performance on small-scale
clinical datasets. To explore the generalizability of the foundation model in
downstream applications, we then apply the model to an unseen TBI dataset for
prediction of PTE using zero-shot learning. Results further demonstrated the
enhanced generalizability of our foundation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14210">Forecasting Fold Bifurcations through Physics-Informed Convolutional Neural Networks. (arXiv:2312.14210v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Habib_G/0/1/0/all/0/1">Giuseppe Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1">&#xc1;d&#xe1;m Horv&#xe1;th</a></p>
<p>This study proposes a physics-informed convolutional neural network (CNN) for
identifying dynamical systems' time series near a fold bifurcation. The
peculiarity of this work is that the CNN is trained with a relatively small
amount of data and on a single, very simple system. In contrast, the CNN is
validated on much more complicated systems. A similar task requires significant
extrapolation capabilities, which are obtained by exploiting physics-based
information. Physics-based information is provided through a specific
pre-processing of the input data, consisting mostly of a transformation into
polar coordinates, normalization, transformation into the logarithmic scale,
and filtering through a moving mean. The results illustrate that such data
pre-processing enables the CNN to grasp the important features related to
approaching a fold bifurcation, namely, the trend of the oscillation amplitude,
and neglect other characteristics that are not particularly relevant, such as
the vibration frequency. The developed CNN was able to correctly classify
trajectories near a fold for a mass-on-moving-belt system, a van der
Pol-Duffing oscillator with an attached tuned mass damper, and a
pitch-and-plunge wing profile. The results obtained pave the way for the
development of similar CNNs effective in real-life applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14212">Beyond mirkwood: Enhancing SED Modeling with Conformal Predictions. (arXiv:2312.14212v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Gilda_S/0/1/0/all/0/1">Sankalp Gilda</a></p>
<p>Traditional spectral energy distribution (SED) fitting techniques face
uncertainties due to assumptions in star formation histories and dust
attenuation curves. We propose an advanced machine learning-based approach that
enhances flexibility and uncertainty quantification in SED fitting. Unlike the
fixed NGBoost model used in mirkwood, our approach allows for any
sklearn-compatible model, including deterministic models. We incorporate
conformalized quantile regression to convert point predictions into error bars,
enhancing interpretability and reliability. Using CatBoost as the base
predictor, we compare results with and without conformal prediction,
demonstrating improved performance using metrics such as coverage and interval
width. Our method offers a more versatile and accurate tool for deriving galaxy
physical properties from observational data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14213">A Reinforcement-Learning-based Multiple-Column Selection Strategy for Column Generation. (arXiv:2312.14213v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Yuan_H/0/1/0/all/0/1">Haofeng Yuan</a>, <a href="http://arxiv.org/find/math/1/au:+Fang_L/0/1/0/all/0/1">Lichang Fang</a>, <a href="http://arxiv.org/find/math/1/au:+Song_S/0/1/0/all/0/1">Shiji Song</a></p>
<p>Column generation (CG) is one of the most successful approaches for solving
large-scale linear programming (LP) problems. Given an LP with a prohibitively
large number of variables (i.e., columns), the idea of CG is to explicitly
consider only a subset of columns and iteratively add potential columns to
improve the objective value. While adding the column with the most negative
reduced cost can guarantee the convergence of CG, it has been shown that adding
multiple columns per iteration rather than a single column can lead to faster
convergence. However, it remains a challenge to design a multiple-column
selection strategy to select the most promising columns from a large number of
candidate columns. In this paper, we propose a novel
reinforcement-learning-based (RL) multiple-column selection strategy. To the
best of our knowledge, it is the first RL-based multiple-column selection
strategy for CG. The effectiveness of our approach is evaluated on two sets of
problems: the cutting stock problem and the graph coloring problem. Compared to
several widely used single-column and multiple-column selection strategies, our
RL-based multiple-column selection strategy leads to faster convergence and
achieves remarkable reductions in the number of CG iterations and runtime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14217">Adversarial Infrared Curves: An Attack on Infrared Pedestrian Detectors in the Physical World. (arXiv:2312.14217v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengyin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weiwen Shi</a></p>
<p>Deep neural network security is a persistent concern, with considerable
research on visible light physical attacks but limited exploration in the
infrared domain. Existing approaches, like white-box infrared attacks using
bulb boards and QR suits, lack realism and stealthiness. Meanwhile, black-box
methods with cold and hot patches often struggle to ensure robustness. To
bridge these gaps, we propose Adversarial Infrared Curves (AdvIC). Using
Particle Swarm Optimization, we optimize two Bezier curves and employ cold
patches in the physical realm to introduce perturbations, creating infrared
curve patterns for physical sample generation. Our extensive experiments
confirm AdvIC's effectiveness, achieving 94.8\% and 67.2\% attack success rates
for digital and physical attacks, respectively. Stealthiness is demonstrated
through a comparative analysis, and robustness assessments reveal AdvIC's
superiority over baseline methods. When deployed against diverse advanced
detectors, AdvIC achieves an average attack success rate of 76.8\%, emphasizing
its robust nature. we explore adversarial defense strategies against AdvIC and
examine its impact under various defense mechanisms. Given AdvIC's substantial
security implications for real-world vision-based applications, urgent
attention and mitigation efforts are warranted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14219">DCFL: Non-IID awareness Data Condensation aided Federated Learning. (arXiv:2312.14219v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sha_S/0/1/0/all/0/1">Shaohan Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">YaFeng Sun</a></p>
<p>Federated learning is a decentralized learning paradigm wherein a central
server trains a global model iteratively by utilizing clients who possess a
certain amount of private datasets. The challenge lies in the fact that the
client side private data may not be identically and independently distributed,
significantly impacting the accuracy of the global model. Existing methods
commonly address the Non-IID challenge by focusing on optimization, client
selection and data complement. However, most approaches tend to overlook the
perspective of the private data itself due to privacy constraints.Intuitively,
statistical distinctions among private data on the client side can help
mitigate the Non-IID degree. Besides, the recent advancements in dataset
condensation technology have inspired us to investigate its potential
applicability in addressing Non-IID issues while maintaining privacy. Motivated
by this, we propose DCFL which divides clients into groups by using the
Centered Kernel Alignment (CKA) method, then uses dataset condensation methods
with non-IID awareness to complete clients. The private data from clients
within the same group is complementary and their condensed data is accessible
to all clients in the group. Additionally, CKA-guided client selection
strategy, filtering mechanisms, and data enhancement techniques are
incorporated to efficiently and precisely utilize the condensed data, enhance
model performance, and minimize communication time. Experimental results
demonstrate that DCFL achieves competitive performance on popular federated
learning benchmarks including MNIST, FashionMNIST, SVHN, and CIFAR-10 with
existing FL protocol.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14220">Single-Cell RNA-seq Synthesis with Latent Diffusion Model. (arXiv:2312.14220v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1">Yixuan Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1">Shuangyin Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DI_S/0/1/0/all/0/1">Shimin DI</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a></p>
<p>The single-cell RNA sequencing (scRNA-seq) technology enables researchers to
study complex biological systems and diseases with high resolution. The central
challenge is synthesizing enough scRNA-seq samples; insufficient samples can
impede downstream analysis and reproducibility. While various methods have been
attempted in past research, the resulting scRNA-seq samples were often of poor
quality or limited in terms of useful specific cell subpopulations. To address
these issues, we propose a novel method called Single-Cell Latent Diffusion
(SCLD) based on the Diffusion Model. This method is capable of synthesizing
large-scale, high-quality scRNA-seq samples, including both 'holistic' or
targeted specific cellular subpopulations within a unified framework. A
pre-guidance mechanism is designed for synthesizing specific cellular
subpopulations, while a post-guidance mechanism aims to enhance the quality of
scRNA-seq samples. The SCLD can synthesize large-scale and high-quality
scRNA-seq samples for various downstream tasks. Our experimental results
demonstrate state-of-the-art performance in cell classification and data
distribution distances when evaluated on two scRNA-seq benchmarks.
Additionally, visualization experiments show the SCLD's capability in
synthesizing specific cellular subpopulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14221">Noninvasive Estimation of Mean Pulmonary Artery Pressure Using MRI, Computer Models, and Machine Learning. (arXiv:2312.14221v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Grzeszczyk_M/0/1/0/all/0/1">Michal K. Grzeszczyk</a>, <a href="http://arxiv.org/find/eess/1/au:+Satlawa_T/0/1/0/all/0/1">Tadeusz Satlawa</a>, <a href="http://arxiv.org/find/eess/1/au:+Lungu_A/0/1/0/all/0/1">Angela Lungu</a>, <a href="http://arxiv.org/find/eess/1/au:+Swift_A/0/1/0/all/0/1">Andrew Swift</a>, <a href="http://arxiv.org/find/eess/1/au:+Narracott_A/0/1/0/all/0/1">Andrew Narracott</a>, <a href="http://arxiv.org/find/eess/1/au:+Hose_R/0/1/0/all/0/1">Rod Hose</a>, <a href="http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzcinski</a>, <a href="http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1">Arkadiusz Sitek</a></p>
<p>Pulmonary Hypertension (PH) is a severe disease characterized by an elevated
pulmonary artery pressure. The gold standard for PH diagnosis is measurement of
mean Pulmonary Artery Pressure (mPAP) during an invasive Right Heart
Catheterization. In this paper, we investigate noninvasive approach to PH
detection utilizing Magnetic Resonance Imaging, Computer Models and Machine
Learning. We show using the ablation study, that physics-informed feature
engineering based on models of blood circulation increases the performance of
Gradient Boosting Decision Trees-based algorithms for classification of PH and
regression of values of mPAP. We compare results of regression (with
thresholding of estimated mPAP) and classification and demonstrate that metrics
achieved in both experiments are comparable. The predicted mPAP values are more
informative to the physicians than the probability of PH returned by
classification models. They provide the intuitive explanation of the outcome of
the machine learning model (clinicians are accustomed to the mPAP metric,
contrary to the PH probability).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14222">Hierarchical Topology Isomorphism Expertise Embedded Graph Contrastive Learning. (arXiv:2312.14222v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangmeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yifan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a></p>
<p>Graph contrastive learning (GCL) aims to align the positive features while
differentiating the negative features in the latent space by minimizing a
pair-wise contrastive loss. As the embodiment of an outstanding discriminative
unsupervised graph representation learning approach, GCL achieves impressive
successes in various graph benchmarks. However, such an approach falls short of
recognizing the topology isomorphism of graphs, resulting in that graphs with
relatively homogeneous node features cannot be sufficiently discriminated. By
revisiting classic graph topology recognition works, we disclose that the
corresponding expertise intuitively complements GCL methods. To this end, we
propose a novel hierarchical topology isomorphism expertise embedded graph
contrastive learning, which introduces knowledge distillations to empower GCL
models to learn the hierarchical topology isomorphism expertise, including the
graph-tier and subgraph-tier. On top of this, the proposed method holds the
feature of plug-and-play, and we empirically demonstrate that the proposed
method is universal to multiple state-of-the-art GCL models. The solid
theoretical analyses are further provided to prove that compared with
conventional GCL methods, our method acquires the tighter upper bound of Bayes
classification error. We conduct extensive experiments on real-world benchmarks
to exhibit the performance superiority of our method over candidate GCL
methods, e.g., for the real-world graph representation learning experiments,
the proposed method beats the state-of-the-art method by 0.23\% on unsupervised
representation learning setting, 0.43\% on transfer learning setting. Our code
is available at https://github.com/jyf123/HTML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14226">Deep de Finetti: Recovering Topic Distributions from Large Language Models. (arXiv:2312.14226v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1">R. Thomas McCoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1">Theodore R. Sumers</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jian-Qiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a></p>
<p>Large language models (LLMs) can produce long, coherent passages of text,
suggesting that LLMs, although trained on next-word prediction, must represent
the latent structure that characterizes a document. Prior work has found that
internal representations of LLMs encode one aspect of latent structure, namely
syntax; here we investigate a complementary aspect, namely the document's topic
structure. We motivate the hypothesis that LLMs capture topic structure by
connecting LLM optimization to implicit Bayesian inference. De Finetti's
theorem shows that exchangeable probability distributions can be represented as
a mixture with respect to a latent generating distribution. Although text is
not exchangeable at the level of syntax, exchangeability is a reasonable
starting assumption for topic structure. We thus hypothesize that predicting
the next token in text will lead LLMs to recover latent topic distributions. We
examine this hypothesis using Latent Dirichlet Allocation (LDA), an
exchangeable probabilistic topic model, as a target, and we show that the
representations formed by LLMs encode both the topics used to generate
synthetic data and those used to explain natural corpus data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14227">ElasticTrainer: Speeding Up On-Device Training with Runtime Elastic Tensor Selection. (arXiv:2312.14227v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kai Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Boyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wei Gao</a></p>
<p>On-device training is essential for neural networks (NNs) to continuously
adapt to new online data, but can be time-consuming due to the device's limited
computing power. To speed up on-device training, existing schemes select
trainable NN portion offline or conduct unrecoverable selection at runtime, but
the evolution of trainable NN portion is constrained and cannot adapt to the
current need for training. Instead, runtime adaptation of on-device training
should be fully elastic, i.e., every NN substructure can be freely removed from
or added to the trainable NN portion at any time in training. In this paper, we
present ElasticTrainer, a new technique that enforces such elasticity to
achieve the required training speedup with the minimum NN accuracy loss.
Experiment results show that ElasticTrainer achieves up to 3.5x more training
speedup in wall-clock time and reduces energy consumption by 2x-3x more
compared to the existing schemes, without noticeable accuracy loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14229">Real-time Neural Network Inference on Extremely Weak Devices: Agile Offloading with Explainable AI. (arXiv:2312.14229v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kai Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wei Gao</a></p>
<p>With the wide adoption of AI applications, there is a pressing need of
enabling real-time neural network (NN) inference on small embedded devices, but
deploying NNs and achieving high performance of NN inference on these small
devices is challenging due to their extremely weak capabilities. Although NN
partitioning and offloading can contribute to such deployment, they are
incapable of minimizing the local costs at embedded devices. Instead, we
suggest to address this challenge via agile NN offloading, which migrates the
required computations in NN offloading from online inference to offline
learning. In this paper, we present AgileNN, a new NN offloading technique that
achieves real-time NN inference on weak embedded devices by leveraging
eXplainable AI techniques, so as to explicitly enforce feature sparsity during
the training phase and minimize the online computation and communication costs.
Experiment results show that AgileNN's inference latency is &gt;6x lower than the
existing schemes, ensuring that sensory data on embedded devices can be timely
consumed. It also reduces the local device's resource consumption by &gt;8x,
without impairing the inference accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14237">AI-Lorenz: A physics-data-driven framework for black-box and gray-box identification of chaotic systems with symbolic regression. (arXiv:2312.14237v1 [physics.comp-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Florio_M/0/1/0/all/0/1">Mario De Florio</a>, <a href="http://arxiv.org/find/physics/1/au:+Kevrekidis_I/0/1/0/all/0/1">Ioannis G. Kevrekidis</a>, <a href="http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>Discovering mathematical models that characterize the observed behavior of
dynamical systems remains a major challenge, especially for systems in a
chaotic regime. The challenge is even greater when the physics underlying such
systems is not yet understood, and scientific inquiry must solely rely on
empirical data. Driven by the need to fill this gap, we develop a framework
that learns mathematical expressions modeling complex dynamical behaviors by
identifying differential equations from noisy and sparse observable data. We
train a small neural network to learn the dynamics of a system, its rate of
change in time, and missing model terms, which are used as input for a symbolic
regression algorithm to autonomously distill the explicit mathematical terms.
This, in turn, enables us to predict the future evolution of the dynamical
behavior. The performance of this framework is validated by recovering the
right-hand sides and unknown terms of certain complex, chaotic systems such as
the well-known Lorenz system, a six-dimensional hyperchaotic system, and the
non-autonomous Sprott chaotic system, and comparing them with their known
analytical expressions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14247">Deep Reinforcement Learning Based Placement for Integrated Access Backhauling in UAV-Assisted Wireless Networks. (arXiv:2312.14247v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Farooq_J/0/1/0/all/0/1">Junaid Farooq</a></p>
<p>The advent of fifth generation (5G) networks has opened new avenues for
enhancing connectivity, particularly in challenging environments like remote
areas or disaster-struck regions. Unmanned aerial vehicles (UAVs) have been
identified as a versatile tool in this context, particularly for improving
network performance through the Integrated access and backhaul (IAB) feature of
5G. However, existing approaches to UAV-assisted network enhancement face
limitations in dynamically adapting to varying user locations and network
demands. This paper introduces a novel approach leveraging deep reinforcement
learning (DRL) to optimize UAV placement in real-time, dynamically adjusting to
changing network conditions and user requirements. Our method focuses on the
intricate balance between fronthaul and backhaul links, a critical aspect often
overlooked in current solutions. The unique contribution of this work lies in
its ability to autonomously position UAVs in a way that not only ensures robust
connectivity to ground users but also maintains seamless integration with
central network infrastructure. Through various simulated scenarios, we
demonstrate how our approach effectively addresses these challenges, enhancing
coverage and network performance in critical areas. This research fills a
significant gap in UAV-assisted 5G networks, providing a scalable and adaptive
solution for future mobile networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14249">GenoCraft: A Comprehensive, User-Friendly Web-Based Platform for High-Throughput Omics Data Analysis and Visualization. (arXiv:2312.14249v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Lu_Y/0/1/0/all/0/1">Yingzhou Lu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shen_M/0/1/0/all/0/1">Minjie Shen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhao_Y/0/1/0/all/0/1">Yue Zhao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1">Chenhao Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Meng_F/0/1/0/all/0/1">Fan Meng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Herrington_D/0/1/0/all/0/1">David Herrington</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fu_T/0/1/0/all/0/1">Tim Fu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Rechem_C/0/1/0/all/0/1">Capucine Van Rechem</a></p>
<p>The surge in high-throughput omics data has reshaped the landscape of
biological research, underlining the need for powerful, user-friendly data
analysis and interpretation tools. This paper presents GenoCraft, a web-based
comprehensive software solution designed to handle the entire pipeline of omics
data processing. GenoCraft offers a unified platform featuring advanced
bioinformatics tools, covering all aspects of omics data analysis. It
encompasses a range of functionalities, such as normalization, quality control,
differential analysis, network analysis, pathway analysis, and diverse
visualization techniques. This software makes state-of-the-art omics data
analysis more accessible to a wider range of users. With GenoCraft, researchers
and data scientists have access to an array of cutting-edge bioinformatics
tools under a user-friendly interface, making it a valuable resource for
managing and analyzing large-scale omics data. The API with an interactive web
interface is publicly available at https://genocraft.stanford. edu/. We also
release all the codes in https://github.com/futianfan/GenoCraft.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14254">Contextual Feature Selection with Conditional Stochastic Gates. (arXiv:2312.14254v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sristi_R/0/1/0/all/0/1">Ram Dyuthi Sristi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindenbaum_O/0/1/0/all/0/1">Ofir Lindenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavzin_M/0/1/0/all/0/1">Maria Lavzin</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiller_J/0/1/0/all/0/1">Jackie Schiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishne_G/0/1/0/all/0/1">Gal Mishne</a>, <a href="http://arxiv.org/find/cs/1/au:+Benisty_H/0/1/0/all/0/1">Hadas Benisty</a></p>
<p>We study the problem of contextual feature selection, where the goal is to
learn a predictive function while identifying subsets of informative features
conditioned on specific contexts. Towards this goal, we generalize the recently
proposed stochastic gates (STG) Yamada et al. [2020] by modeling the
probabilistic gates as conditional Bernoulli variables whose parameters are
predicted based on the contextual variables. Our new scheme, termed
conditional-STG (c-STG), comprises two networks: a hypernetwork that
establishes the mapping between contextual variables and probabilistic feature
selection parameters and a prediction network that maps the selected feature to
the response variable. Training the two networks simultaneously ensures the
comprehensive incorporation of context and feature selection within a unified
model. We provide a theoretical analysis to examine several properties of the
proposed framework. Importantly, our model leads to improved flexibility and
adaptability of feature selection and, therefore, can better capture the
nuances and variations in the data. We apply c-STG to simulated and real-world
datasets, including healthcare, housing, and neuroscience, and demonstrate that
it effectively selects contextually meaningful features, thereby enhancing
predictive performance and interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14259">Multi-Agent Bandit Learning through Heterogeneous Action Erasure Channels. (arXiv:2312.14259v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanna_O/0/1/0/all/0/1">Osama A. Hanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Karakas_M/0/1/0/all/0/1">Merve Karakas</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin F. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fragouli_C/0/1/0/all/0/1">Christina Fragouli</a></p>
<p>Multi-Armed Bandit (MAB) systems are witnessing an upswing in applications
within multi-agent distributed environments, leading to the advancement of
collaborative MAB algorithms. In such settings, communication between agents
executing actions and the primary learner making decisions can hinder the
learning process. A prevalent challenge in distributed learning is action
erasure, often induced by communication delays and/or channel noise. This
results in agents possibly not receiving the intended action from the learner,
subsequently leading to misguided feedback. In this paper, we introduce novel
algorithms that enable learners to interact concurrently with distributed
agents across heterogeneous action erasure channels with different action
erasure probabilities. We illustrate that, in contrast to existing bandit
algorithms, which experience linear regret, our algorithms assure sub-linear
regret guarantees. Our proposed solutions are founded on a meticulously crafted
repetition protocol and scheduling of learning across heterogeneous channels.
To our knowledge, these are the first algorithms capable of effectively
learning through heterogeneous action erasure channels. We substantiate the
superior performance of our algorithm through numerical experiments,
emphasizing their practical significance in addressing issues related to
communication constraints and delays in multi-agent environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14260">Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience. (arXiv:2312.14260v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thakkar_J/0/1/0/all/0/1">Janvi Thakkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zizzo_G/0/1/0/all/0/1">Giulio Zizzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Maffeis_S/0/1/0/all/0/1">Sergio Maffeis</a></p>
<p>Machine learning models are being used in an increasing number of critical
applications; thus, securing their integrity and ownership is critical. Recent
studies observed that adversarial training and watermarking have a conflicting
interaction. This work introduces a novel framework to integrate adversarial
training with watermarking techniques to fortify against evasion attacks and
provide confident model verification in case of intellectual property theft. We
use adversarial training together with adversarial watermarks to train a robust
watermarked model. The key intuition is to use a higher perturbation budget to
generate adversarial watermarks compared to the budget used for adversarial
training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets
to evaluate our proposed technique on various model stealing attacks. The
results obtained consistently outperform the existing baseline in terms of
robustness performance and further prove the resilience of this defense against
pruning and fine-tuning removal attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14276">Deep Neural Networks and Finite Elements of Any Order on Arbitrary Dimensions. (arXiv:2312.14276v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+He_J/0/1/0/all/0/1">Juncai He</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_J/0/1/0/all/0/1">Jinchao Xu</a></p>
<p>In this study, we establish that deep neural networks employing ReLU and
ReLU$^2$ activation functions are capable of representing Lagrange finite
element functions of any order on simplicial meshes across arbitrary
dimensions. We introduce a novel global formulation of the basis functions for
Lagrange elements, grounded in a geometric decomposition of these elements and
leveraging two essential properties of high-dimensional simplicial meshes and
barycentric coordinate functions. This representation theory facilitates a
natural approximation result for such deep neural networks. Our findings
present the first demonstration of how deep neural networks can systematically
generate general continuous piecewise polynomial functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14279">Characterizing and Classifying Developer Forum Posts with their Intentions. (arXiv:2312.14279v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingfang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Laufer_E/0/1/0/all/0/1">Eric Laufer</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Santhosh Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jayden Luo</a></p>
<p>With the rapid growth of the developer community, the amount of posts on
online technical forums has been growing rapidly, which poses difficulties for
users to filter useful posts and find important information. Tags provide a
concise feature dimension for users to locate their interested posts and for
search engines to index the most relevant posts according to the queries.
However, most tags are only focused on the technical perspective (e.g., program
language, platform, tool). In most cases, forum posts in online developer
communities reveal the author's intentions to solve a problem, ask for advice,
share information, etc. The modeling of the intentions of posts can provide an
extra dimension to the current tag taxonomy. By referencing previous studies
and learning from industrial perspectives, we create a refined taxonomy for the
intentions of technical forum posts. Through manual labeling and analysis on a
sampled post dataset extracted from online forums, we understand the relevance
between the constitution of posts (code, error messages) and their intentions.
Furthermore, inspired by our manual study, we design a pre-trained
transformer-based model to automatically predict post intentions. The best
variant of our intention prediction framework, which achieves a Micro F1-score
of 0.589, Top 1-3 accuracy of 62.6% to 87.8%, and an average AUC of 0.787,
outperforms the state-of-the-art baseline approach. Our characterization and
automated classification of forum posts regarding their intentions may help
forum maintainers or third-party tool developers improve the organization and
retrieval of posts on technical forums. We have released our annotated dataset
and codes in our supplementary material package.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14280">Fine-grained Forecasting Models Via Gaussian Process Blurring Effect. (arXiv:2312.14280v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koohfar_S/0/1/0/all/0/1">Sepideh Koohfar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietz_L/0/1/0/all/0/1">Laura Dietz</a></p>
<p>Time series forecasting is a challenging task due to the existence of complex
and dynamic temporal dependencies. This can lead to incorrect predictions by
even the best forecasting models. Using more training data is one way to
improve the accuracy, but this source is often limited. In contrast, we are
building on successful denoising approaches for image generation by advocating
for an end-to-end forecasting and denoising paradigm.
</p>
<p>We propose an end-to-end forecast-blur-denoise forecasting framework by
encouraging a division of labors between the forecasting and the denoising
models. The initial forecasting model is directed to focus on accurately
predicting the coarse-grained behavior, while the denoiser model focuses on
capturing the fine-grained behavior that is locally blurred by integrating a
Gaussian Process model. All three parts are interacting for the best end-to-end
performance. Our extensive experiments demonstrate that our proposed approach
is able to improve the forecasting accuracy of several state-of-the-art
forecasting models as well as several other denoising approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14285">Probing Biological and Artificial Neural Networks with Task-dependent Neural Manifolds. (arXiv:2312.14285v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kuoch_M/0/1/0/all/0/1">Michael Kuoch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chou_C/0/1/0/all/0/1">Chi-Ning Chou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Parthasarathy_N/0/1/0/all/0/1">Nikhil Parthasarathy</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dapello_J/0/1/0/all/0/1">Joel Dapello</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DiCarlo_J/0/1/0/all/0/1">James J. DiCarlo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sompolinsky_H/0/1/0/all/0/1">Haim Sompolinsky</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chung_S/0/1/0/all/0/1">SueYeon Chung</a></p>
<p>Recently, growth in our understanding of the computations performed in both
biological and artificial neural networks has largely been driven by either
low-level mechanistic studies or global normative approaches. However, concrete
methodologies for bridging the gap between these levels of abstraction remain
elusive. In this work, we investigate the internal mechanisms of neural
networks through the lens of neural population geometry, aiming to provide
understanding at an intermediate level of abstraction, as a way to bridge that
gap. Utilizing manifold capacity theory (MCT) from statistical physics and
manifold alignment analysis (MAA) from high-dimensional statistics, we probe
the underlying organization of task-dependent manifolds in deep neural networks
and macaque neural recordings. Specifically, we quantitatively characterize how
different learning objectives lead to differences in the organizational
strategies of these models and demonstrate how these geometric analyses are
connected to the decodability of task-relevant information. These analyses
present a strong direction for bridging mechanistic and normative theories in
neural networks through neural population geometry, potentially opening up many
future research avenues in both machine learning and neuroscience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14292">Benchmarking Multi-Agent Preference-based Reinforcement Learning for Human-AI Teaming. (arXiv:2312.14292v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1">Siddhant Bhambri</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1">Mudit Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Murthy_A/0/1/0/all/0/1">Anil Murthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1">Subbarao Kambhampati</a></p>
<p>Preference-based Reinforcement Learning (PbRL) is an active area of research,
and has made significant strides in single-agent actor and in observer
human-in-the-loop scenarios. However, its application within the co-operative
multi-agent RL frameworks, where humans actively participate and express
preferences for agent behavior, remains largely uncharted. We consider a
two-agent (Human-AI) cooperative setup where both the agents are rewarded
according to human's reward function for the team. However, the agent does not
have access to it, and instead, utilizes preference-based queries to elicit its
objectives and human's preferences for the robot in the human-robot team. We
introduce the notion of Human-Flexibility, i.e. whether the human partner is
amenable to multiple team strategies, with a special case being Specified
Orchestration where the human has a single team policy in mind (most
constrained case). We propose a suite of domains to study PbRL for Human-AI
cooperative setup which explicitly require forced cooperation. Adapting
state-of-the-art single-agent PbRL algorithms to our two-agent setting, we
conduct a comprehensive benchmarking study across our domain suite. Our
findings highlight the challenges associated with high degree of
Human-Flexibility and the limited access to the human's envisioned policy in
PbRL for Human-AI cooperation. Notably, we observe that PbRL algorithms exhibit
effective performance exclusively in the case of Specified Orchestration which
can be seen as an upper bound PbRL performance for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14299">Fairness in Submodular Maximization over a Matroid Constraint. (arXiv:2312.14299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Halabi_M/0/1/0/all/0/1">Marwa El Halabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarnawski_J/0/1/0/all/0/1">Jakub Tarnawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Norouzi_Fard_A/0/1/0/all/0/1">Ashkan Norouzi-Fard</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1">Thuy-Duong Vuong</a></p>
<p>Submodular maximization over a matroid constraint is a fundamental problem
with various applications in machine learning. Some of these applications
involve decision-making over datapoints with sensitive attributes such as
gender or race. In such settings, it is crucial to guarantee that the selected
solution is fairly distributed with respect to this attribute. Recently,
fairness has been investigated in submodular maximization under a cardinality
constraint in both the streaming and offline settings, however the more general
problem with matroid constraint has only been considered in the streaming
setting and only for monotone objectives. This work fills this gap. We propose
various algorithms and impossibility results offering different trade-offs
between quality, fairness, and generality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14302">Exploiting Novel GPT-4 APIs. (arXiv:2312.14302v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1">Kellin Pelrine</a>, <a href="http://arxiv.org/find/cs/1/au:+Taufeeque_M/0/1/0/all/0/1">Mohammad Taufeeque</a>, <a href="http://arxiv.org/find/cs/1/au:+Zajac_M/0/1/0/all/0/1">Micha&#x142; Zaj&#x105;c</a>, <a href="http://arxiv.org/find/cs/1/au:+McLean_E/0/1/0/all/0/1">Euan McLean</a>, <a href="http://arxiv.org/find/cs/1/au:+Gleave_A/0/1/0/all/0/1">Adam Gleave</a></p>
<p>Language model attacks typically assume one of two extreme threat models:
full white-box access to model weights, or black-box access limited to a text
generation API. However, real-world APIs are often more flexible than just text
generation: these APIs expose ``gray-box'' access leading to new threat
vectors. To explore this, we red-team three new functionalities exposed in the
GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that
fine-tuning a model on as few as 15 harmful examples or 100 benign examples can
remove core safeguards from GPT-4, enabling a range of harmful outputs.
Furthermore, we find that GPT-4 Assistants readily divulge the function call
schema and can be made to execute arbitrary function calls. Finally, we find
that knowledge retrieval can be hijacked by injecting instructions into
retrieval documents. These vulnerabilities highlight that any additions to the
functionality exposed by an API can create new vulnerabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14303">Geo2SigMap: High-Fidelity RF Signal Mapping Using Geographic Databases. (arXiv:2312.14303v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zeyu Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1">Zhihui Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1">Tingjun Chen</a></p>
<p>Radio frequency (RF) signal mapping, which is the process of analyzing and
predicting the RF signal strength and distribution across specific areas, is
crucial for cellular network planning and deployment. Traditional approaches to
RF signal mapping rely on statistical models constructed based on measurement
data, which offer low complexity but often lack accuracy, or ray tracing tools,
which provide enhanced precision for the target area but suffer from increased
computational complexity. Recently, machine learning (ML) has emerged as a
data-driven method for modeling RF signal propagation, which leverages models
trained on synthetic datasets to perform RF signal mapping in "unseen" areas.
</p>
<p>In this paper, we present Geo2SigMap, an ML-based framework for efficient and
high-fidelity RF signal mapping using geographic databases. First, we develop
an automated framework that seamlessly integrates three open-source tools:
OpenStreetMap (geographic databases), Blender (computer graphics), and Sionna
(ray tracing), enabling the efficient generation of large-scale 3D building
maps and ray tracing models. Second, we propose a cascaded U-Net model, which
is pre-trained on synthetic datasets and employed to generate detailed RF
signal maps, leveraging environmental information and sparse measurement data.
Finally, we evaluate the performance of Geo2SigMap via a real-world measurement
campaign, where three types of user equipment (UE) collect over 45,000 data
points related to cellular information from six LTE cells operating in the
citizens broadband radio service (CBRS) band. Our results show that Geo2SigMap
achieves an average root-mean-square-error (RMSE) of 6.04 dB for predicting the
reference signal received power (RSRP) at the UE, representing an average RMSE
improvement of 3.59 dB compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14309">Federated Quantum Long Short-term Memory (FedQLSTM). (arXiv:2312.14309v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chehimi_M/0/1/0/all/0/1">Mahdi Chehimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Samuel Yen-Chi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1">Walid Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Shinjae Yoo</a></p>
<p>Quantum federated learning (QFL) can facilitate collaborative learning across
multiple clients using quantum machine learning (QML) models, while preserving
data privacy. Although recent advances in QFL span different tasks like
classification while leveraging several data types, no prior work has focused
on developing a QFL framework that utilizes temporal data to approximate
functions useful to analyze the performance of distributed quantum sensing
networks. In this paper, a novel QFL framework that is the first to integrate
quantum long short-term memory (QLSTM) models with temporal data is proposed.
The proposed federated QLSTM (FedQLSTM) framework is exploited for performing
the task of function approximation. In this regard, three key use cases are
presented: Bessel function approximation, sinusoidal delayed quantum feedback
control function approximation, and Struve function approximation. Simulation
results confirm that, for all considered use cases, the proposed FedQLSTM
framework achieves a faster convergence rate under one local training epoch,
minimizing the overall computations, and saving 25-33% of the number of
communication rounds needed until convergence compared to an FL framework with
classical LSTM models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14322">Data Needs and Challenges of Quantum Dot Devices Automation: Workshop Report. (arXiv:2312.14322v1 [cond-mat.mes-hall])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1">Justyna P. Zwolak</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Taylor_J/0/1/0/all/0/1">Jacob M. Taylor</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Andrews_R/0/1/0/all/0/1">Reed Andrews</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Benson_J/0/1/0/all/0/1">Jared Benson</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Bryant_G/0/1/0/all/0/1">Garnett Bryant</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Buterakos_D/0/1/0/all/0/1">Donovan Buterakos</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Chatterjee_A/0/1/0/all/0/1">Anasua Chatterjee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sarma_S/0/1/0/all/0/1">Sankar Das Sarma</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Eriksson_M/0/1/0/all/0/1">Mark A. Eriksson</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Greplova_E/0/1/0/all/0/1">Eli&#x161;ka Greplov&#xe1;</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gullans_M/0/1/0/all/0/1">Michael J. Gullans</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hader_F/0/1/0/all/0/1">Fabian Hader</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kovach_T/0/1/0/all/0/1">Tyler J. Kovach</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Mundada_P/0/1/0/all/0/1">Pranav S. Mundada</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ramsey_M/0/1/0/all/0/1">Mick Ramsey</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Rasmussen_T/0/1/0/all/0/1">Torbjoern Rasmussen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Severin_B/0/1/0/all/0/1">Brandon Severin</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sigillito_A/0/1/0/all/0/1">Anthony Sigillito</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Undseth_B/0/1/0/all/0/1">Brennan Undseth</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Weber_B/0/1/0/all/0/1">Brian Weber</a></p>
<p>Gate-defined quantum dots are a promising candidate system to realize
scalable, coupled qubit systems and serve as a fundamental building block for
quantum computers. However, present-day quantum dot devices suffer from
imperfections that must be accounted for, which hinders the characterization,
tuning, and operation process. Moreover, with an increasing number of quantum
dot qubits, the relevant parameter space grows sufficiently to make heuristic
control infeasible. Thus, it is imperative that reliable and scalable
autonomous tuning approaches are developed. In this report, we outline current
challenges in automating quantum dot device tuning and operation with a
particular focus on datasets, benchmarking, and standardization. We also
present ideas put forward by the quantum dot community on how to overcome them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14329">Invariant Anomaly Detection under Distribution Shifts: A Causal Perspective. (arXiv:2312.14329v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1">Jo&#xe3;o B. S. Carvalho</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengtao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geyer_R/0/1/0/all/0/1">Robin Geyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotrini_C/0/1/0/all/0/1">Carlos Cotrini</a>, <a href="http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1">Joachim M. Buhmann</a></p>
<p>Anomaly detection (AD) is the machine learning task of identifying highly
discrepant abnormal samples by solely relying on the consistency of the normal
training samples. Under the constraints of a distribution shift, the assumption
that training samples and test samples are drawn from the same distribution
breaks down. In this work, by leveraging tools from causal inference we attempt
to increase the resilience of anomaly detection models to different kinds of
distribution shifts. We begin by elucidating a simple yet necessary statistical
property that ensures invariant representations, which is critical for robust
AD under both domain and covariate shifts. From this property, we derive a
regularization term which, when minimized, leads to partial distribution
invariance across environments. Through extensive experimental evaluation on
both synthetic and real-world tasks, covering a range of six different AD
methods, we demonstrated significant improvements in out-of-distribution
performance. Under both covariate and domain shift, models regularized with our
proposed term showed marked increased robustness. Code is available at:
https://github.com/JoaoCarv/invariant-anomaly-detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14331">Maximum entropy GFlowNets with soft Q-learning. (arXiv:2312.14331v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadpour_S/0/1/0/all/0/1">Sobhan Mohammadpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1">Emmanuel Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Frejinger_E/0/1/0/all/0/1">Emma Frejinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1">Pierre-Luc Bacon</a></p>
<p>Generative Flow Networks (GFNs) have emerged as a powerful tool for sampling
discrete objects from unnormalized distributions, offering a scalable
alternative to Markov Chain Monte Carlo (MCMC) methods. While GFNs draw
inspiration from maximum entropy reinforcement learning (RL), the connection
between the two has largely been unclear and seemingly applicable only in
specific cases. This paper addresses the connection by constructing an
appropriate reward function, thereby establishing an exact relationship between
GFNs and maximum entropy RL. This construction allows us to introduce maximum
entropy GFNs, which, in contrast to GFNs with uniform backward policy, achieve
the maximum entropy attainable by GFNs without constraints on the state space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14333">Behaviour Modelling of Social Animals via Causal Structure Discovery and Graph Neural Networks. (arXiv:2312.14333v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gendron_G/0/1/0/all/0/1">Ga&#xeb;l Gendron</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_M/0/1/0/all/0/1">Mitchell Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Azhar_M/0/1/0/all/0/1">Mihailo Azhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Heidari_S/0/1/0/all/0/1">Shahrokh Heidari</a>, <a href="http://arxiv.org/find/cs/1/au:+Valdez_D/0/1/0/all/0/1">David Arturo Soriano Valdez</a>, <a href="http://arxiv.org/find/cs/1/au:+Knowles_K/0/1/0/all/0/1">Kobe Knowles</a>, <a href="http://arxiv.org/find/cs/1/au:+OLeary_P/0/1/0/all/0/1">Padriac O&#x27;Leary</a>, <a href="http://arxiv.org/find/cs/1/au:+Eyre_S/0/1/0/all/0/1">Simon Eyre</a>, <a href="http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1">Michael Witbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1">Gillian Dobbie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiamou Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Delmas_P/0/1/0/all/0/1">Patrice Delmas</a></p>
<p>Better understanding the natural world is a crucial task with a wide range of
applications. In environments with close proximity between humans and animals,
such as zoos, it is essential to better understand the causes behind animal
behaviour and what interventions are responsible for changes in their
behaviours. This can help to predict unusual behaviours, mitigate detrimental
effects and increase the well-being of animals. There has been work on
modelling the dynamics behind swarms of birds and insects but the complex
social behaviours of mammalian groups remain less explored. In this work, we
propose a method to build behavioural models using causal structure discovery
and graph neural networks for time series. We apply this method to a mob of
meerkats in a zoo environment and study its ability to predict future actions
and model the behaviour distribution at an individual-level and at a group
level. We show that our method can match and outperform standard deep learning
architectures and generate more realistic data, while using fewer parameters
and providing increased interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14334">DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction). (arXiv:2312.14334v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Qiaoyue Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shpilevskiy_F/0/1/0/all/0/1">Frederick Shpilevskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecuyer_M/0/1/0/all/0/1">Mathias L&#xe9;cuyer</a></p>
<p>The Adam optimizer is a popular choice in contemporary deep learning, due to
its strong empirical performance. However we observe that in privacy sensitive
scenarios, the traditional use of Differential Privacy (DP) with the Adam
optimizer leads to sub-optimal performance on several tasks. We find that this
performance degradation is due to a DP bias in Adam's second moment estimator,
introduced by the addition of independent noise in the gradient computation to
enforce DP guarantees. This DP bias leads to a different scaling for low
variance parameter updates, that is inconsistent with the behavior of
non-private Adam. We propose DP-AdamBC, an optimization algorithm which removes
the bias in the second moment estimation and retrieves the expected behaviour
of Adam. Empirically, DP-AdamBC significantly improves the optimization
performance of DP-Adam by up to 3.5% in final accuracy in image, text, and
graph node classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14359">Training Neural Networks with Internal State, Unconstrained Connectivity, and Discrete Activations. (arXiv:2312.14359v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grushin_A/0/1/0/all/0/1">Alexander Grushin</a></p>
<p>Today's most powerful machine learning approaches are typically designed to
train stateless architectures with predefined layers and differentiable
activation functions. While these approaches have led to unprecedented
successes in areas such as natural language processing and image recognition,
the trained models are also susceptible to making mistakes that a human would
not. In this paper, we take the view that true intelligence may require the
ability of a machine learning model to manage internal state, but that we have
not yet discovered the most effective algorithms for training such models. We
further postulate that such algorithms might not necessarily be based on
gradient descent over a deep architecture, but rather, might work best with an
architecture that has discrete activations and few initial topological
constraints (such as multiple predefined layers). We present one attempt in our
ongoing efforts to design such a training algorithm, applied to an architecture
with binary activations and only a single matrix of weights, and show that it
is able to form useful representations of natural language text, but is also
limited in its ability to leverage large quantities of training data. We then
provide ideas for improving the algorithm and for designing other training
algorithms for similar architectures. Finally, we discuss potential benefits
that could be gained if an effective training algorithm is found, and suggest
experiments for evaluating whether these benefits exist in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14369">Quality-Diversity Generative Sampling for Learning with Synthetic Data. (arXiv:2312.14369v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Allen Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fontaine_M/0/1/0/all/0/1">Matthew C. Fontaine</a>, <a href="http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1">Serena Booth</a>, <a href="http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1">Maja J. Matari&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolaidis_S/0/1/0/all/0/1">Stefanos Nikolaidis</a></p>
<p>Generative models can serve as surrogates for some real data sources by
creating synthetic training datasets, but in doing so they may transfer biases
to downstream tasks. We focus on protecting quality and diversity when
generating synthetic training datasets. We propose quality-diversity generative
sampling (QDGS), a framework for sampling data uniformly across a user-defined
measure space, despite the data coming from a biased generator. QDGS is a
model-agnostic framework that uses prompt guidance to optimize a quality
objective across measures of diversity for synthetically generated data,
without fine-tuning the generative model. Using balanced synthetic datasets
generated by QDGS, we first debias classifiers trained on color-biased shape
datasets as a proof-of-concept. By applying QDGS to facial data synthesis, we
prompt for desired semantic concepts, such as skin tone and age, to create an
intersectional dataset with a combined blend of visual features. Leveraging
this balanced data for training classifiers improves fairness while maintaining
accuracy on facial recognition benchmarks. Code available at:
https://github.com/Cylumn/qd-generative-sampling
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14372">Generative Models for Simulation of KamLAND-Zen. (arXiv:2312.14372v1 [physics.data-an])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Fu_Z/0/1/0/all/0/1">Z. Fu</a>, <a href="http://arxiv.org/find/physics/1/au:+Grant_C/0/1/0/all/0/1">C. Grant</a>, <a href="http://arxiv.org/find/physics/1/au:+Krawiec_D/0/1/0/all/0/1">D. M. Krawiec</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_A/0/1/0/all/0/1">A. Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Winslow_L/0/1/0/all/0/1">L. Winslow</a></p>
<p>The next generation of searches for neutrinoless double beta decay
(0{\nu}\b{eta}\b{eta}) are poised to answer deep questions on the nature of
neutrinos and the source of the Universe's matter-antimatter asymmetry. They
will be looking for event rates of less than one event per ton of instrumented
isotope per year. To claim discovery, accurate and efficient simulations of
detector events that mimic 0{\nu}\b{eta}\b{eta} is critical. Traditional Monte
Carlo (MC) simulations can be supplemented by machine-learning-based generative
models. In this work, we describe the performance of generative models designed
for monolithic liquid scintillator detectors like KamLAND to produce highly
accurate simulation data without a predefined physics model. We demonstrate its
ability to recover low-level features and perform interpolation. In the future,
the results of these generative models can be used to improve event
classification and background rejection by providing high-quality abundant
generated data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14378">Multimodal Attention Merging for Improved Speech Recognition and Audio Event Classification. (arXiv:2312.14378v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sundar_A/0/1/0/all/0/1">Anirudh S. Sundar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao-Han Huck Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1">David M. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shalini Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1">Venkatesh Ravichandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nidadavolu_P/0/1/0/all/0/1">Phani Sankar Nidadavolu</a></p>
<p>Training large foundation models using self-supervised objectives on
unlabeled data, followed by fine-tuning on downstream tasks, has emerged as a
standard procedure. Unfortunately, the efficacy of this approach is often
constrained by both limited fine-tuning compute and scarcity in labeled
downstream data. We introduce Multimodal Attention Merging (MAM), an attempt
that facilitates direct knowledge transfer from attention matrices of models
rooted in high resource modalities, text and images, to those in
resource-constrained domains, speech and audio, employing a zero-shot paradigm.
MAM reduces the relative Word Error Rate (WER) of an Automatic Speech
Recognition (ASR) model by up to 6.70%, and relative classification error of an
Audio Event Classification (AEC) model by 10.63%. In cases where some
data/compute is available, we present Learnable-MAM, a data-driven approach to
merging attention matrices, resulting in a further 2.90% relative reduction in
WER for ASR and 18.42% relative reduction in AEC compared to fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14380">Federated Learning with Projected Trajectory Regularization. (arXiv:2312.14380v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tiejin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuanpu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yujia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinghui Chen</a></p>
<p>Federated learning enables joint training of machine learning models from
distributed clients without sharing their local data. One key challenge in
federated learning is to handle non-identically distributed data across the
clients, which leads to deteriorated model training performances. Prior works
in this line of research mainly focus on utilizing last-step global model
parameters/gradients or the linear combinations of the past model
parameters/gradients, which do not fully exploit the potential of global
information from the model training trajectory. In this paper, we propose a
novel federated learning framework with projected trajectory regularization
(FedPTR) for tackling the data heterogeneity issue, which proposes a unique way
to better extract the essential global information from the model training
trajectory. Specifically, FedPTR allows local clients or the server to optimize
an auxiliary (synthetic) dataset that mimics the learning dynamics of the
recent model update and utilizes it to project the next-step model trajectory
for local training regularization. We conduct rigorous theoretical analysis for
our proposed framework under nonconvex stochastic settings to verify its fast
convergence under heterogeneous data distributions. Experiments on various
benchmark datasets and non-i.i.d. settings validate the effectiveness of our
proposed framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14385">Generative AI Beyond LLMs: System Implications of Multi-Modal Generation. (arXiv:2312.14385v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golden_A/0/1/0/all/0/1">Alicia Golden</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsia_S/0/1/0/all/0/1">Samuel Hsia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Acun_B/0/1/0/all/0/1">Bilge Acun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosmer_B/0/1/0/all/0/1">Basil Hosmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yejin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+DeVito_Z/0/1/0/all/0/1">Zachary DeVito</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1">Jeff Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1">Gu-Yeon Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1">David Brooks</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Carole-Jean Wu</a></p>
<p>As the development of large-scale Generative AI models evolve beyond text
(1D) generation to include image (2D) and video (3D) generation, processing
spatial and temporal information presents unique challenges to quality,
performance, and efficiency. We present the first work towards understanding
this new system design space for multi-modal text-to-image (TTI) and
text-to-video (TTV) generation models. Current model architecture designs are
bifurcated into 2 categories: Diffusion- and Transformer-based models. Our
systematic performance characterization on a suite of eight representative
TTI/TTV models shows that after state-of-the-art optimization techniques such
as Flash Attention are applied, Convolution accounts for up to 44% of execution
time for Diffusion-based TTI models, while Linear layers consume up to 49% of
execution time for Transformer-based models. We additionally observe that
Diffusion-based TTI models resemble the Prefill stage of LLM inference, and
benefit from 1.1-2.5x greater speedup from Flash Attention than
Transformer-based TTI models that resemble the Decode phase. Since
optimizations designed for LLMs do not map directly onto TTI/TTV models, we
must conduct a thorough characterization of these workloads to gain insights
for new optimization opportunities. In doing so, we define sequence length in
the context of TTI/TTV models and observe sequence length can vary up to 4x in
Diffusion model inference. We additionally observe temporal aspects of TTV
workloads pose unique system bottlenecks, with Temporal Attention accounting
for over 60% of total Attention time. Overall, our in-depth system performance
characterization is a critical first step towards designing efficient and
deployable systems for emerging TTI/TTV workloads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14405">Graph Attention-Based Symmetry Constraint Extraction for Analog Circuits. (arXiv:2312.14405v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Song Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yi Kang</a></p>
<p>In recent years, analog circuits have received extensive attention and are
widely used in many emerging applications. The high demand for analog circuits
necessitates shorter circuit design cycles. To achieve the desired performance
and specifications, various geometrical symmetry constraints must be carefully
considered during the analog layout process. However, the manual labeling of
these constraints by experienced analog engineers is a laborious and
time-consuming process. To handle the costly runtime issue, we propose a
graph-based learning framework to automatically extract symmetric constraints
in analog circuit layout. The proposed framework leverages the connection
characteristics of circuits and the devices'information to learn the general
rules of symmetric constraints, which effectively facilitates the extraction of
device-level constraints on circuit netlists. The experimental results
demonstrate that compared to state-of-the-art symmetric constraint detection
approaches, our framework achieves higher accuracy and lower false positive
rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14406">Generative Pretraining at Scale: Transformer-Based Encoding of Transactional Behavior for Fraud Detection. (arXiv:2312.14406v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Ze Yu Zhao</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zheng Zhu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guilin Li</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhan Wang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a> (1) ((1) Tencent, WeChat Pay)</p>
<p>In this work, we introduce an innovative autoregressive model leveraging
Generative Pretrained Transformer (GPT) architectures, tailored for fraud
detection in payment systems. Our approach innovatively confronts token
explosion and reconstructs behavioral sequences, providing a nuanced
understanding of transactional behavior through temporal and contextual
analysis. Utilizing unsupervised pretraining, our model excels in feature
representation without the need for labeled data. Additionally, we integrate a
differential convolutional approach to enhance anomaly detection, bolstering
the security and efficacy of one of the largest online payment merchants in
China. The scalability and adaptability of our model promise broad
applicability in various transactional contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14418">Sharp error estimates for target measure diffusion maps with applications to the committor problem. (arXiv:2312.14418v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Sule_S/0/1/0/all/0/1">Shashank Sule</a>, <a href="http://arxiv.org/find/math/1/au:+Evans_L/0/1/0/all/0/1">Luke Evans</a>, <a href="http://arxiv.org/find/math/1/au:+Cameron_M/0/1/0/all/0/1">Maria Cameron</a></p>
<p>We obtain asymptotically sharp error estimates for the consistency error of
the Target Measure Diffusion map (TMDmap) (Banisch et al. 2020), a variant of
diffusion maps featuring importance sampling and hence allowing input data
drawn from an arbitrary density. The derived error estimates include the bias
error and the variance error. The resulting convergence rates are consistent
with the approximation theory of graph Laplacians. The key novelty of our
results lies in the explicit quantification of all the prefactors on
leading-order terms. We also prove an error estimate for solutions of Dirichlet
BVPs obtained using TMDmap, showing that the solution error is controlled by
consistency error. We use these results to study an important application of
TMDmap in the analysis of rare events in systems governed by overdamped
Langevin dynamics using the framework of transition path theory (TPT). The
cornerstone ingredient of TPT is the solution of the committor problem, a
boundary value problem for the backward Kolmogorov PDE. Remarkably, we find
that the TMDmap algorithm is particularly suited as a meshless solver to the
committor problem due to the cancellation of several error terms in the
prefactor formula. Furthermore, significant improvements in bias and variance
errors occur when using a quasi-uniform sampling density. Our numerical
experiments show that these improvements in accuracy are realizable in practice
when using $\delta$-nets as spatially uniform inputs to the TMDmap algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14426">Room Occupancy Prediction: Exploring the Power of Machine Learning and Temporal Insights. (arXiv:2312.14426v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Siqi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yaping Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinpu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziren Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuanxin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yixin Kang</a></p>
<p>Energy conservation in buildings is a paramount concern to combat greenhouse
gas emissions and combat climate change. The efficient management of room
occupancy, involving actions like lighting control and climate adjustment, is a
pivotal strategy to curtail energy consumption. In contexts where surveillance
technology isn't viable, non-intrusive sensors are employed to estimate room
occupancy. In this study, we present a predictive framework for room occupancy
that leverages a diverse set of machine learning models, with Random Forest
consistently achieving the highest predictive accuracy. Notably, this dataset
encompasses both temporal and spatial dimensions, revealing a wealth of
information. Intriguingly, our framework demonstrates robust performance even
in the absence of explicit temporal modeling. These findings underscore the
remarkable predictive power of traditional machine learning models. The success
can be attributed to the presence of feature redundancy, the simplicity of
linear spatial and temporal patterns, and the advantages of high-frequency data
sampling. While these results are compelling, it's essential to remain open to
the possibility that explicitly modeling the temporal dimension could unlock
deeper insights or further enhance predictive capabilities in specific
scenarios. In summary, our research not only validates the effectiveness of our
prediction framework for continuous and classification tasks but also
underscores the potential for improvements through the inclusion of temporal
aspects. The study highlights the promise of machine learning in shaping
energy-efficient practices and room occupancy management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14428">A Unified Industrial Large Knowledge Model Framework in Smart Manufacturing. (arXiv:2312.14428v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jay Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hanqi Su</a></p>
<p>The recent emergence of large language models (LLMs) shows the potential for
artificial general intelligence, revealing new opportunities in industry 4.0
and smart manufacturing. However, a notable gap exists in applying these LLMs
in industry, primarily due to their training on general knowledge rather than
domain-specific knowledge. Such specialized domain knowledge is vital for
effectively addressing the complex needs of industrial applications. To bridge
this gap, this paper proposes an Industrial Large Knowledge Model (ILKM)
framework emphasizing their potential to revolutionize the industry in smart
manufacturing. In addition, ILKMs and LLMs are compared from eight
perspectives. Finally, "6S Principle" is proposed as the guideline for the
development of ILKMs in smart manufacturing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14432">Scalable 3D Reconstruction From Single Particle X-Ray Diffraction Images Based on Online Machine Learning. (arXiv:2312.14432v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shenoy_J/0/1/0/all/0/1">Jay Shenoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1">Axel Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Poitevin_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;ric Poitevin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1">Gordon Wetzstein</a></p>
<p>X-ray free-electron lasers (XFELs) offer unique capabilities for measuring
the structure and dynamics of biomolecules, helping us understand the basic
building blocks of life. Notably, high-repetition-rate XFELs enable single
particle imaging (X-ray SPI) where individual, weakly scattering biomolecules
are imaged under near-physiological conditions with the opportunity to access
fleeting states that cannot be captured in cryogenic or crystallized
conditions. Existing X-ray SPI reconstruction algorithms, which estimate the
unknown orientation of a particle in each captured image as well as its shared
3D structure, are inadequate in handling the massive datasets generated by
these emerging XFELs. Here, we introduce X-RAI, an online reconstruction
framework that estimates the structure of a 3D macromolecule from large X-ray
SPI datasets. X-RAI consists of a convolutional encoder, which amortizes pose
estimation over large datasets, as well as a physics-based decoder, which
employs an implicit neural representation to enable high-quality 3D
reconstruction in an end-to-end, self-supervised manner. We demonstrate that
X-RAI achieves state-of-the-art performance for small-scale datasets in
simulation and challenging experimental settings and demonstrate its
unprecedented ability to process large datasets containing millions of
diffraction images in an online fashion. These abilities signify a paradigm
shift in X-ray SPI towards real-time capture and reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14436">REBEL: A Regularization-Based Solution for Reward Overoptimization in Reinforcement Learning from Human Feedback. (arXiv:2312.14436v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Souradip Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaskar_A/0/1/0/all/0/1">Amisha Bhaskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Anukriti Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Tokekar_P/0/1/0/all/0/1">Pratap Tokekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1">Amrit Singh Bedi</a></p>
<p>In this work, we propose REBEL, an algorithm for sample efficient reward
regularization based robotic reinforcement learning from human feedback
(RRLHF). Reinforcement learning (RL) performance for continuous control
robotics tasks is sensitive to the underlying reward function. In practice, the
reward function often ends up misaligned with human intent, values, social
norms, etc., leading to catastrophic failures in the real world. We leverage
human preferences to learn regularized reward functions and eventually align
the agents with the true intended behavior. We introduce a novel notion of
reward regularization to the existing RRLHF framework, which is termed as agent
preferences. So, we not only consider human feedback in terms of preferences,
we also propose to take into account the preference of the underlying RL agent
while learning the reward function. We show that this helps to improve the
over-optimization associated with the design of reward functions in RL. We
experimentally show that REBEL exhibits up to 70% improvement in sample
efficiency to achieve a similar level of episodic reward returns as compared to
the state-of-the-art methods such as PEBBLE and PEBBLE+SURF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14438">PC-Conv: Unifying Homophily and Heterophily with Two-fold Filtering. (arXiv:2312.14438v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bingheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_E/0/1/0/all/0/1">Erlin Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1">Zhao Kang</a></p>
<p>Recently, many carefully crafted graph representation learning methods have
achieved impressive performance on either strong heterophilic or homophilic
graphs, but not both. Therefore, they are incapable of generalizing well across
real-world graphs with different levels of homophily. This is attributed to
their neglect of homophily in heterophilic graphs, and vice versa. In this
paper, we propose a two-fold filtering mechanism to extract homophily in
heterophilic graphs and vice versa. In particular, we extend the graph heat
equation to perform heterophilic aggregation of global information from a long
distance. The resultant filter can be exactly approximated by the
Possion-Charlier (PC) polynomials. To further exploit information at multiple
orders, we introduce a powerful graph convolution PC-Conv and its instantiation
PCNet for the node classification task. Compared with state-of-the-art GNNs,
PCNet shows competitive performance on well-known homophilic and heterophilic
graphs. Our implementation is available at https://github.com/uestclbh/PC-Conv.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14439">PUMA: Efficient Continual Graph Learning with Graph Condensation. (arXiv:2312.14439v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yilun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1">Ruihong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yanran Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hongzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zi Huang</a></p>
<p>When handling streaming graphs, existing graph representation learning models
encounter a catastrophic forgetting problem, where previously learned knowledge
of these models is easily overwritten when learning with newly incoming graphs.
In response, Continual Graph Learning emerges as a novel paradigm enabling
graph representation learning from static to streaming graphs. Our prior work,
CaT is a replay-based framework with a balanced continual learning procedure,
which designs a small yet effective memory bank for replaying data by
condensing incoming graphs. Although the CaT alleviates the catastrophic
forgetting problem, there exist three issues: (1) The graph condensation
algorithm derived in CaT only focuses on labelled nodes while neglecting
abundant information carried by unlabelled nodes; (2) The continual training
scheme of the CaT overemphasises on the previously learned knowledge, limiting
the model capacity to learn from newly added memories; (3) Both the
condensation process and replaying process of the CaT are time-consuming. In
this paper, we propose a psudo-label guided memory bank (PUMA) CGL framework,
extending from the CaT to enhance its efficiency and effectiveness by
overcoming the above-mentioned weaknesses and limits. To fully exploit the
information in a graph, PUMA expands the coverage of nodes during graph
condensation with both labelled and unlabelled nodes. Furthermore, a
training-from-scratch strategy is proposed to upgrade the previous continual
learning scheme for a balanced training between the historical and the new
graphs. Besides, PUMA uses a one-time prorogation and wide graph encoders to
accelerate the graph condensation and the graph encoding process in the
training stage to improve the efficiency of the whole framework. Extensive
experiments on four datasets demonstrate the state-of-the-art performance and
efficiency over existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14440">Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks. (arXiv:2312.14440v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shahgir_H/0/1/0/all/0/1">Haz Sameen Shahgir</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xianghao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1">Greg Ver Steeg</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yue Dong</a></p>
<p>The widespread use of Text-to-Image (T2I) models in content generation
requires careful examination of their safety, including their robustness to
adversarial attacks. Despite extensive research into this, the reasons for
their effectiveness are underexplored. This paper presents an empirical study
on adversarial attacks against T2I models, focusing on analyzing factors
associated with attack success rates (ASRs). We introduce a new attack
objective - entity swapping using adversarial suffixes and two gradient-based
attack algorithms. Human and automatic evaluations reveal the asymmetric nature
of ASRs on entity swap: for example, it is easier to replace "human" with
"robot" in the prompt "a human dancing in the rain." with an adversarial suffix
but is significantly harder in reverse. We further propose probing metrics to
establish indicative signals from the model's beliefs to the adversarial ASR.
We identify conditions resulting in a 60% success probability for adversarial
attacks and others where this likelihood drops below 5%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14441">DMC4ML: Data Movement Complexity for Machine Learning. (arXiv:2312.14441v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanan_C/0/1/0/all/0/1">Christopher Kanan</a>, <a href="http://arxiv.org/find/eess/1/au:+McKellips_D/0/1/0/all/0/1">Dylan McKellips</a>, <a href="http://arxiv.org/find/eess/1/au:+Ozawa_T/0/1/0/all/0/1">Toranosuke Ozawa</a>, <a href="http://arxiv.org/find/eess/1/au:+Shahmirza_A/0/1/0/all/0/1">Arian Shahmirza</a>, <a href="http://arxiv.org/find/eess/1/au:+Smith_W/0/1/0/all/0/1">Wesley Smith</a></p>
<p>The greatest demand for today's computing is machine learning. This paper
analyzes three machine learning algorithms: transformers, spatial convolution,
and FFT. The analysis is novel in three aspects. First, it measures the cost of
memory access on an abstract memory hierarchy, instead of traditional time or
space complexity. Second, the analysis is asymptotic and identifies the primary
sources of the memory cost. Finally, the result is symbolic, which can be used
to select algorithmic parameters such as the group size in grouped query
attention for any dimension size and number of heads and the batch size for
batched convolution for any image size and kernel size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14452">How to Overcome Curse-of-Dimensionality for Out-of-Distribution Detection?. (arXiv:2312.14452v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosal_S/0/1/0/all/0/1">Soumya Suvra Ghosal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yiyou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixuan Li</a></p>
<p>Machine learning models deployed in the wild can be challenged by
out-of-distribution (OOD) data from unknown classes. Recent advances in OOD
detection rely on distance measures to distinguish samples that are relatively
far away from the in-distribution (ID) data. Despite the promise,
distance-based methods can suffer from the curse-of-dimensionality problem,
which limits the efficacy in high-dimensional feature space. To combat this
problem, we propose a novel framework, Subspace Nearest Neighbor (SNN), for OOD
detection. In training, our method regularizes the model and its feature
representation by leveraging the most relevant subset of dimensions (i.e.
subspace). Subspace learning yields highly distinguishable distance measures
between ID and OOD data. We provide comprehensive experiments and ablations to
validate the efficacy of SNN. Compared to the current best distance-based
method, SNN reduces the average FPR95 by 15.96% on the CIFAR-100 benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14458">Multiagent Copilot Approach for Shared Autonomy between Human EEG and TD3 Deep Reinforcement Learning. (arXiv:2312.14458v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phang_C/0/1/0/all/0/1">Chun-Ren Phang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirata_A/0/1/0/all/0/1">Akimasa Hirata</a></p>
<p>Deep reinforcement learning (RL) algorithms enable the development of fully
autonomous agents that can interact with the environment. Brain-computer
interface (BCI) systems decipher human implicit brain signals regardless of the
explicit environment. In this study, we integrated deep RL and BCI to improve
beneficial human interventions in autonomous systems and the performance in
decoding brain activities by considering environmental factors. Shared autonomy
was allowed between the action command decoded from the electroencephalography
(EEG) of the human agent and the action generated from the twin delayed DDPG
(TD3) agent for a given environment. Our proposed copilot control scheme with a
full blocker (Co-FB) significantly outperformed the individual EEG (EEG-NB) or
TD3 control. The Co-FB model achieved a higher target approaching score, lower
failure rate, and lower human workload than the EEG-NB model. The Co-FB control
scheme had a higher invisible target score and level of allowed human
intervention than the TD3 model. We also proposed a disparity d-index to
evaluate the effect of contradicting agent decisions on the control accuracy
and authority of the copilot model. We found a significant correlation between
the control authority of the TD3 agent and the performance improvement of human
EEG classification with respect to the d-index. We also observed that shifting
control authority to the TD3 agent improved performance when BCI decoding was
not optimal. These findings indicate that the copilot system can effectively
handle complex environments and that BCI performance can be improved by
considering environmental factors. Future work should employ continuous action
space and different multi-agent approaches to evaluate copilot performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14461">Attacking Byzantine Robust Aggregation in High Dimensions. (arXiv:2312.14461v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1">Sarthak Choudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolluri_A/0/1/0/all/0/1">Aashish Kolluri</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1">Prateek Saxena</a></p>
<p>Training modern neural networks or models typically requires averaging over a
sample of high-dimensional vectors. Poisoning attacks can skew or bias the
average vectors used to train the model, forcing the model to learn specific
patterns or avoid learning anything useful. Byzantine robust aggregation is a
principled algorithmic defense against such biasing. Robust aggregators can
bound the maximum bias in computing centrality statistics, such as mean, even
when some fraction of inputs are arbitrarily corrupted. Designing such
aggregators is challenging when dealing with high dimensions. However, the
first polynomial-time algorithms with strong theoretical bounds on the bias
have recently been proposed. Their bounds are independent of the number of
dimensions, promising a conceptual limit on the power of poisoning attacks in
their ongoing arms race against defenses.
</p>
<p>In this paper, we show a new attack called HIDRA on practical realization of
strong defenses which subverts their claim of dimension-independent bias. HIDRA
highlights a novel computational bottleneck that has not been a concern of
prior information-theoretic analysis. Our experimental evaluation shows that
our attacks almost completely destroy the model performance, whereas existing
attacks with the same goal fail to have much effect. Our findings leave the
arms race between poisoning attacks and provable defenses wide open.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14470">Safe Reinforcement Learning with Instantaneous Constraints: The Role of Aggressive Exploration. (arXiv:2312.14470v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Honghao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1">Lei Ying</a></p>
<p>This paper studies safe Reinforcement Learning (safe RL) with linear function
approximation and under hard instantaneous constraints where unsafe actions
must be avoided at each step. Existing studies have considered safe RL with
hard instantaneous constraints, but their approaches rely on several key
assumptions: $(i)$ the RL agent knows a safe action set for {\it every} state
or knows a {\it safe graph} in which all the state-action-state triples are
safe, and $(ii)$ the constraint/cost functions are {\it linear}. In this paper,
we consider safe RL with instantaneous hard constraints without assumption
$(i)$ and generalize $(ii)$ to Reproducing Kernel Hilbert Space (RKHS). Our
proposed algorithm, LSVI-AE, achieves $\tilde{\cO}(\sqrt{d^3H^4K})$ regret and
$\tilde{\cO}(H \sqrt{dK})$ hard constraint violation when the cost function is
linear and $\cO(H\gamma_K \sqrt{K})$ hard constraint violation when the cost
function belongs to RKHS. Here $K$ is the learning horizon, $H$ is the length
of each episode, and $\gamma_K$ is the information gain w.r.t the kernel used
to approximate cost functions. Our results achieve the optimal dependency on
the learning horizon $K$, matching the lower bound we provide in this paper and
demonstrating the efficiency of LSVI-AE. Notably, the design of our approach
encourages aggressive policy exploration, providing a unique perspective on
safe RL with general cost functions and no prior knowledge of safe actions,
which may be of independent interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14478">Federated Learning via Input-Output Collaborative Distillation. (arXiv:2312.14478v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xuan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shanglin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1">Yuxiang Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Barry Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yawen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziyan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baochang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1">David Doermann</a></p>
<p>Federated learning (FL) is a machine learning paradigm in which distributed
local nodes collaboratively train a central model without sharing individually
held private data. Existing FL methods either iteratively share local model
parameters or deploy co-distillation. However, the former is highly susceptible
to private data leakage, and the latter design relies on the prerequisites of
task-relevant real data. Instead, we propose a data-free FL framework based on
local-to-central collaborative distillation with direct input and output space
exploitation. Our design eliminates any requirement of recursive local
parameter exchange or auxiliary task-relevant data to transfer knowledge,
thereby giving direct privacy control to local users. In particular, to cope
with the inherent data heterogeneity across locals, our technique learns to
distill input on which each local model produces consensual yet unique results
to represent each expertise. Our proposed FL framework achieves notable
privacy-utility trade-offs with extensive experiments on image classification
and segmentation tasks under various real-world heterogeneous federated
learning settings on both natural and medical images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14499">Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks. (arXiv:2312.14499v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zheyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zekun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>Physics-Informed Neural Networks (PINNs) have proven effective in solving
partial differential equations (PDEs), especially when some data are available
by blending seamlessly data and physics. However, extending PINNs to
high-dimensional and even high-order PDEs encounters significant challenges due
to the computational cost associated with automatic differentiation in the
residual loss. Herein, we address the limitations of PINNs in handling
high-dimensional and high-order PDEs by introducing Hutchinson Trace Estimation
(HTE). Starting with the second-order high-dimensional PDEs ubiquitous in
scientific computing, HTE transforms the calculation of the entire Hessian
matrix into a Hessian vector product (HVP). This approach alleviates the
computational bottleneck via Taylor-mode automatic differentiation and
significantly reduces memory consumption from the Hessian matrix to HVP. We
further showcase HTE's convergence to the original PINN loss and its unbiased
behavior under specific conditions. Comparisons with Stochastic Dimension
Gradient Descent (SDGD) highlight the distinct advantages of HTE, particularly
in scenarios with significant variance among dimensions. We further extend HTE
to higher-order and higher-dimensional PDEs, specifically addressing the
biharmonic equation. By employing tensor-vector products (TVP), HTE efficiently
computes the colossal tensor associated with the fourth-order high-dimensional
biharmonic equation, saving memory and enabling rapid computation. The
effectiveness of HTE is illustrated through experimental setups, demonstrating
comparable convergence rates with SDGD under memory and speed constraints.
Additionally, HTE proves valuable in accelerating the Gradient-Enhanced PINN
(gPINN) version as well as the Biharmonic equation. Overall, HTE opens up a new
capability in scientific machine learning for tackling high-order and
high-dimensional PDEs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14504">Theory of Hallucinations based on Equivariance. (arXiv:2312.14504v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shibata_H/0/1/0/all/0/1">Hisaichi Shibata</a></p>
<p>Equivariance is an important feature in machine learning, including language
models. It ensures that any sequences of phrases with the same meanings are
interpreted consistently. For example, the sentence 'There is a cat on the
table' should be interpreted by language models as it is, regardless of
variations in its token-level expression. Building on this insight, I propose a
new theory suggesting that insufficient equivariance in language models can
lead to hallucinations. According to this theory, which is both intuitive and
novel, language models trained on relatively small datasets tend to
misinterpret input texts and/or generate incorrect texts (i.e.,
hallucinations). To test this theory, I developed a toy model known as 'dancing
men', which is a character-level substitution cipher. Additionally, I propose a
novel technique based on the T5 (Text To Text Transfer Transformer) model to
efficiently decipher these codes without relying on frequency analysis. I have
found that this T5 model can almost completely solve the cipher, demonstrating
its ability to acquire equivariance in this frame. This method could be scaled
up to word-level and sentence-level substitution ciphers, analogous to large
language models without tokenizers or dictionaries. This scalability makes it
suitable for investigating the proposed link between inadequate equivariance
acquisition and the emergence of hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14507">Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport. (arXiv:2312.14507v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Torres_B/0/1/0/all/0/1">Bernardo Torres</a> (S2A, IDS, LTCI), <a href="http://arxiv.org/find/cs/1/au:+Peeters_G/0/1/0/all/0/1">Geoffroy Peeters</a> (S2A, IDS, LTCI), <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Ga&#xeb;l Richard</a> (S2A, IDS, LTCI)</p>
<p>In neural audio signal processing, pitch conditioning has been used to
enhance the performance of synthesizers. However, jointly training pitch
estimators and synthesizers is a challenge when using standard audio-to-audio
reconstruction loss, leading to reliance on external pitch trackers. To address
this issue, we propose using a spectral loss function inspired by optimal
transportation theory that minimizes the displacement of spectral energy. We
validate this approach through an unsupervised autoencoding task that fits a
harmonic template to harmonic signals. We jointly estimate the fundamental
frequency and amplitudes of harmonics using a lightweight encoder and
reconstruct the signals using a differentiable harmonic synthesizer. The
proposed approach offers a promising direction for improving unsupervised
parameter estimation in neural audio applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14528">An effective and efficient green federated learning method for one-layer neural networks. (arXiv:2312.14528v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fontenla_Romero_O/0/1/0/all/0/1">Oscar Fontenla-Romero</a>, <a href="http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1">Bertha Guijarro-Berdi&#xf1;as</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Pereira_E/0/1/0/all/0/1">Elena Hern&#xe1;ndez-Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Sanchez_B/0/1/0/all/0/1">Beatriz P&#xe9;rez-S&#xe1;nchez</a></p>
<p>Nowadays, machine learning algorithms continue to grow in complexity and
require a substantial amount of computational resources and energy. For these
reasons, there is a growing awareness of the development of new green
algorithms and distributed AI can contribute to this. Federated learning (FL)
is one of the most active research lines in machine learning, as it allows the
training of collaborative models in a distributed way, an interesting option in
many real-world environments, such as the Internet of Things, allowing the use
of these models in edge computing devices. In this work, we present a FL
method, based on a neural network without hidden layers, capable of generating
a global collaborative model in a single training round, unlike traditional FL
methods that require multiple rounds for convergence. This allows obtaining an
effective and efficient model that simplifies the management of the training
process. Moreover, this method preserve data privacy by design, a crucial
aspect in current data protection regulations. We conducted experiments with
large datasets and a large number of federated clients. Despite being based on
a network model without hidden layers, it maintains in all cases competitive
accuracy results compared to more complex state-of-the-art machine learning
models. Furthermore, we show that the method performs equally well in both
identically and non-identically distributed scenarios. Finally, it is an
environmentally friendly algorithm as it allows significant energy savings
during the training process compared to its centralized counterpart.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14532">DuaLight: Enhancing Traffic Signal Control by Leveraging Scenario-Specific and Scenario-Shared Knowledge. (arXiv:2312.14532v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiaming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jingqing Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haoyuan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Hangyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a></p>
<p>Reinforcement learning has been revolutionizing the traditional traffic
signal control task, showing promising power to relieve congestion and improve
efficiency. However, the existing methods lack effective learning mechanisms
capable of absorbing dynamic information inherent to a specific scenario and
universally applicable dynamic information across various scenarios. Moreover,
within each specific scenario, they fail to fully capture the essential
empirical experiences about how to coordinate between neighboring and target
intersections, leading to sub-optimal system-wide outcomes.
</p>
<p>Viewing these issues, we propose DuaLight, which aims to leverage both the
experiential information within a single scenario and the generalizable
information across various scenarios for enhanced decision-making.
Specifically, DuaLight introduces a scenario-specific experiential weight
module with two learnable parts: Intersection-wise and Feature-wise, guiding
how to adaptively utilize neighbors and input features for each scenario, thus
providing a more fine-grained understanding of different intersections.
Furthermore, we implement a scenario-shared Co-Train module to facilitate the
learning of generalizable dynamics information across different scenarios.
Empirical results on both real-world and synthetic scenarios show DuaLight
achieves competitive performance across various metrics, offering a promising
solution to alleviate traffic congestion, with 3-7\% improvements. The code is
available under: https://github.com/lujiaming-12138/DuaLight.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14533">Multi-view user representation learning for user matching without personal information. (arXiv:2312.14533v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hongliu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Baamrani_I/0/1/0/all/0/1">Ilias El Baamrani</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_E/0/1/0/all/0/1">Eoin Thomas</a></p>
<p>As the digitization of travel industry accelerates, analyzing and
understanding travelers' behaviors becomes increasingly important. However,
traveler data frequently exhibit high data sparsity due to the relatively low
frequency of user interactions with travel providers. Compounding this effect
the multiplication of devices, accounts and platforms while browsing travel
products online also leads to data dispersion. To deal with these challenges,
probabilistic traveler matching can be used. Most existing solutions for user
matching are not suitable for traveler matching as a traveler's browsing
history is typically short and URLs in the travel industry are very
heterogeneous with many tokens. To deal with these challenges, we propose the
similarity based multi-view information fusion to learn a better user
representation from URLs by treating the URLs as multi-view data. The
experimental results show that the proposed multi-view user representation
learning can take advantage of the complementary information from different
views, highlight the key information in URLs and perform significantly better
than other representation learning solutions for the user matching task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14535">ADA-GAD: Anomaly-Denoised Autoencoders for Graph Anomaly Detection. (arXiv:2312.14535v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qianqian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yangbangyan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zitai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qingming Huang</a></p>
<p>Graph anomaly detection is crucial for identifying nodes that deviate from
regular behavior within graphs, benefiting various domains such as fraud
detection and social network. Although existing reconstruction-based methods
have achieved considerable success, they may face the \textit{Anomaly
Overfitting} and \textit{Homophily Trap} problems caused by the abnormal
patterns in the graph, breaking the assumption that normal nodes are often
better reconstructed than abnormal ones. Our observations indicate that models
trained on graphs with fewer anomalies exhibit higher detection performance.
Based on this insight, we introduce a novel two-stage framework called
Anomaly-Denoised Autoencoders for Graph Anomaly Detection (ADA-GAD). In the
first stage, we design a learning-free anomaly-denoised augmentation method to
generate graphs with reduced anomaly levels. We pretrain graph autoencoders on
these augmented graphs at multiple levels, which enables the graph autoencoders
to capture normal patterns. In the next stage, the decoders are retrained for
detection on the original graph, benefiting from the multi-level
representations learned in the previous stage. Meanwhile, we propose the node
anomaly distribution regularization to further alleviate \textit{Anomaly
Overfitting}. We validate the effectiveness of our approach through extensive
experiments on both synthetic and real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14552">Machine learning for structure-guided materials and process design. (arXiv:2312.14552v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Morand_L/0/1/0/all/0/1">Lukas Morand</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Iraki_T/0/1/0/all/0/1">Tarek Iraki</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Dornheim_J/0/1/0/all/0/1">Johannes Dornheim</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sandfeld_S/0/1/0/all/0/1">Stefan Sandfeld</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Link_N/0/1/0/all/0/1">Norbert Link</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Helm_D/0/1/0/all/0/1">Dirk Helm</a></p>
<p>In recent years, there has been a growing interest in accelerated materials
innovation in both, research and industry. However, to truly add value to the
development of new advanced materials, it is inevitable to take into account
manufacturing processes and thereby tailor materials design approaches to
support downstream process design approaches. As a major step into this
direction, we present a holistic optimization approach that covers the entire
materials process-structure-property chain. Our approach specifically employs
machine learning techniques to address two critical identification problems.
The first is to solve a materials design problem, which involves identifying
near-optimal material structures that exhibit desired macroscopic properties.
The second is to solve a process design problem that is to find an optimal
processing path to manufacture these material structures. Both identification
problems are typically ill-posed, which presents a significant challenge for
solution approaches. However, the non-unique nature of these problems also
offers an important advantage for processing: By having several target
structures that perform similarly well, the corresponding processes can be
efficiently guided towards manufacturing the best reachable structure. In
particular, we apply deep reinforcement learning for process design in
combination with a multi-task learning-based optimization approach for
materials design. The functionality of the approach will be demonstrated by
using it to manufacture crystallographic textures with desired properties in a
metal forming process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14564">Online Covering with Multiple Experts. (arXiv:2312.14564v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kevi_E/0/1/0/all/0/1">Enik&#x151; Kevi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Kim-Thang Nguyen</a></p>
<p>Designing online algorithms with machine learning predictions is a recent
technique beyond the worst-case paradigm for various practically relevant
online problems (scheduling, caching, clustering, ski rental, etc.). While most
previous learning-augmented algorithm approaches focus on integrating the
predictions of a single oracle, we study the design of online algorithms with
\emph{multiple} experts. To go beyond the popular benchmark of a static best
expert in hindsight, we propose a new \emph{dynamic} benchmark (linear
combinations of predictions that change over time). We present a competitive
algorithm in the new dynamic benchmark with a performance guarantee of $O(\log
K)$, where $K$ is the number of experts, for $0-1$ online optimization
problems. Furthermore, our multiple-expert approach provides a new perspective
on how to combine in an online manner several online algorithms - a
long-standing central subject in the online algorithm research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14567">Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise. (arXiv:2312.14567v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1">Rui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuxing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>Heavy-ball momentum with decaying learning rates is widely used with SGD for
optimizing deep learning models. In contrast to its empirical popularity, the
understanding of its theoretical property is still quite limited, especially
under the standard anisotropic gradient noise condition for quadratic
regression problems. Although it is widely conjectured that heavy-ball momentum
method can provide accelerated convergence and should work well in large batch
settings, there is no rigorous theoretical analysis. In this paper, we fill
this theoretical gap by establishing a non-asymptotic convergence bound for
stochastic heavy-ball methods with step decay scheduler on quadratic
objectives, under the anisotropic gradient noise condition. As a direct
implication, we show that heavy-ball momentum can provide
$\tilde{\mathcal{O}}(\sqrt{\kappa})$ accelerated convergence of the bias term
of SGD while still achieving near-optimal convergence rate with respect to the
stochastic variance term. The combined effect implies an overall convergence
rate within log factors from the statistical minimax rate. This means SGD with
heavy-ball momentum is useful in the large-batch settings such as distributed
machine learning or federated learning, where a smaller number of iterations
can significantly reduce the number of communication rounds, leading to
acceleration in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14571">Data is Moody: Discovering Data Modification Rules from Process Event Logs. (arXiv:2312.14571v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schuster_M/0/1/0/all/0/1">Marco Bjarne Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiegand_B/0/1/0/all/0/1">Boris Wiegand</a>, <a href="http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1">Jilles Vreeken</a></p>
<p>Although event logs are a powerful source to gain insight about the behavior
of the underlying business process, existing work primarily focuses on finding
patterns in the activity sequences of an event log, while ignoring event
attribute data. Event attribute data has mostly been used to predict event
occurrences and process outcome, but the state of the art neglects to mine
succinct and interpretable rules how event attribute data changes during
process execution. Subgroup discovery and rule-based classification approaches
lack the ability to capture the sequential dependencies present in event logs,
and thus lead to unsatisfactory results with limited insight into the process
behavior.
</p>
<p>Given an event log, we are interested in finding accurate yet succinct and
interpretable if-then rules how the process modifies data. We formalize the
problem in terms of the Minimum Description Length (MDL) principle, by which we
choose the model with the best lossless description of the data. Additionally,
we propose the greedy Moody algorithm to efficiently search for rules. By
extensive experiments on both synthetic and real-world data, we show Moody
indeed finds compact and interpretable rules, needs little data for accurate
discovery, and is robust to noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14574">MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning. (arXiv:2312.14574v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Liang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1">Songyue Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zongqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1">Huifang Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxiao Li</a></p>
<p>Prompt learning has demonstrated impressive efficacy in the fine-tuning of
multimodal large models to a wide range of downstream tasks. Nonetheless,
applying existing prompt learning methods for the diagnosis of neurological
disorder still suffers from two issues: (i) existing methods typically treat
all patches equally, despite the fact that only a small number of patches in
neuroimaging are relevant to the disease, and (ii) they ignore the structural
information inherent in the brain connection network which is crucial for
understanding and diagnosing neurological disorders. To tackle these issues, we
introduce a novel prompt learning model by learning graph prompts during the
fine-tuning process of multimodal large models for diagnosing neurological
disorders. Specifically, we first leverage GPT-4 to obtain relevant disease
concepts and compute semantic similarity between these concepts and all
patches. Secondly, we reduce the weight of irrelevant patches according to the
semantic similarity between each patch and disease-related concepts. Moreover,
we construct a graph among tokens based on these concepts and employ a graph
convolutional network layer to extract the structural information of the graph,
which is used to prompt the pre-trained multimodal large models for diagnosing
neurological disorders. Extensive experiments demonstrate that our method
achieves superior performance for neurological disorder diagnosis compared with
state-of-the-art methods and validated by clinicians.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14589">Non-Denoising Forward-Time Diffusions. (arXiv:2312.14589v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peluchetti_S/0/1/0/all/0/1">Stefano Peluchetti</a></p>
<p>The scope of this paper is generative modeling through diffusion processes.
An approach falling within this paradigm is the work of Song et al. (2021),
which relies on a time-reversal argument to construct a diffusion process
targeting the desired data distribution. We show that the time-reversal
argument, common to all denoising diffusion probabilistic modeling proposals,
is not necessary. We obtain diffusion processes targeting the desired data
distribution by taking appropriate mixtures of diffusion bridges. The resulting
transport is exact by construction, allows for greater flexibility in choosing
the dynamics of the underlying diffusion, and can be approximated by means of a
neural network via novel training objectives. We develop a unifying view of the
drift adjustments corresponding to our and to time-reversal approaches and make
use of this representation to inspect the inner workings of diffusion-based
generative models. Finally, we leverage on scalable simulation and inference
techniques common in spatial statistics to move beyond fully factorial
distributions in the underlying diffusion dynamics. The methodological advances
contained in this work contribute toward establishing a general framework for
generative modeling based on diffusion processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14590">SIG: Speaker Identification in Literature via Prompt-Based Generation. (arXiv:2312.14590v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhenlin Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liyan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huangfu_M/0/1/0/all/0/1">Mingdu Huangfu</a></p>
<p>Identifying speakers of quotations in narratives is an important task in
literary analysis, with challenging scenarios including the out-of-domain
inference for unseen speakers, and non-explicit cases where there are no
speaker mentions in surrounding context. In this work, we propose a simple and
effective approach SIG, a generation-based method that verbalizes the task and
quotation input based on designed prompt templates, which also enables easy
integration of other auxiliary tasks that further bolster the speaker
identification performance. The prediction can either come from direct
generation by the model, or be determined by the highest generation probability
of each speaker candidate. Based on our approach design, SIG supports
out-of-domain evaluation, and achieves open-world classification paradigm that
is able to accept any forms of candidate input. We perform both cross-domain
evaluation and in-domain evaluation on PDNC, the largest dataset of this task,
where empirical results suggest that SIG outperforms previous baselines of
complicated designs, as well as the zero-shot ChatGPT, especially excelling at
those hard non-explicit scenarios by up to 17% improvement. Additional
experiments on another dataset WP further corroborate the efficacy of SIG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14606">Explainable Multi-Camera 3D Object Detection with Transformer-Based Saliency Maps. (arXiv:2312.14606v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beemelmanns_T/0/1/0/all/0/1">Till Beemelmanns</a>, <a href="http://arxiv.org/find/cs/1/au:+Zahr_W/0/1/0/all/0/1">Wassim Zahr</a>, <a href="http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1">Lutz Eckstein</a></p>
<p>Vision Transformers (ViTs) have achieved state-of-the-art results on various
computer vision tasks, including 3D object detection. However, their end-to-end
implementation also makes ViTs less explainable, which can be a challenge for
deploying them in safety-critical applications, such as autonomous driving,
where it is important for authorities, developers, and users to understand the
model's reasoning behind its predictions. In this paper, we propose a novel
method for generating saliency maps for a DetR-like ViT with multiple camera
inputs used for 3D object detection. Our method is based on the raw attention
and is more efficient than gradient-based methods. We evaluate the proposed
method on the nuScenes dataset using extensive perturbation tests and show that
it outperforms other explainability methods in terms of visual quality and
quantitative metrics. We also demonstrate the importance of aggregating
attention across different layers of the transformer. Our work contributes to
the development of explainable AI for ViTs, which can help increase trust in AI
applications by establishing more transparency regarding the inner workings of
AI models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14625">Hierarchical Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks. (arXiv:2312.14625v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eghtesad_T/0/1/0/all/0/1">Taha Eghtesad</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sirui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1">Yevgeniy Vorobeychik</a>, <a href="http://arxiv.org/find/cs/1/au:+Laszka_A/0/1/0/all/0/1">Aron Laszka</a></p>
<p>The increasing reliance of drivers on navigation applications has made
transportation networks more susceptible to data-manipulation attacks by
malicious actors. Adversaries may exploit vulnerabilities in the data
collection or processing of navigation services to inject false information,
and to thus interfere with the drivers' route selection. Such attacks can
significantly increase traffic congestions, resulting in substantial waste of
time and resources, and may even disrupt essential services that rely on road
networks. To assess the threat posed by such attacks, we introduce a
computational framework to find worst-case data-injection attacks against
transportation networks. First, we devise an adversarial model with a threat
actor who can manipulate drivers by increasing the travel times that they
perceive on certain roads. Then, we employ hierarchical multi-agent
reinforcement learning to find an approximate optimal adversarial strategy for
data manipulation. We demonstrate the applicability of our approach through
simulating attacks on the Sioux Falls, ND network topology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14628">Towards more sustainable enterprise data and application management with cross silo Federated Learning and Analytics. (arXiv:2312.14628v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hongliu Cao</a></p>
<p>To comply with new legal requirements and policies committed to privacy
protection, more and more companies start to deploy cross-silo Federated
Learning at global scale, where several clients/silos collaboratively train a
global model under the coordination of a central server. Instead of data
sharing and transmission, clients train models using their private local data
and exchange model updates. However, there is little understanding of the
carbon emission impact of cross silo Federated Learning due to the lack of
related works. In this study, we first analyze the sustainability aspect of
cross-silo Federated Learning, across the AI product life cycle instead of
focusing only on the model training, with the comparison to the centralized
method. A more holistic quantitative cost and CO2 emission estimation method
for real world cross-silo Federated Learning setting is proposed. Secondly, we
propose a novel data and application management system using cross silo
Federated Learning and analytics to make IT companies more sustainable and cost
effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14635">Fluid Simulation on Neural Flow Maps. (arXiv:2312.14635v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yitong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong-Xing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Diyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bo Zhu</a></p>
<p>We introduce Neural Flow Maps, a novel simulation method bridging the
emerging paradigm of implicit neural representations with fluid simulation
based on the theory of flow maps, to achieve state-of-the-art simulation of
inviscid fluid phenomena. We devise a novel hybrid neural field representation,
Spatially Sparse Neural Fields (SSNF), which fuses small neural networks with a
pyramid of overlapping, multi-resolution, and spatially sparse grids, to
compactly represent long-term spatiotemporal velocity fields at high accuracy.
With this neural velocity buffer in hand, we compute long-term, bidirectional
flow maps and their Jacobians in a mechanistically symmetric manner, to
facilitate drastic accuracy improvement over existing solutions. These
long-range, bidirectional flow maps enable high advection accuracy with low
dissipation, which in turn facilitates high-fidelity incompressible flow
simulations that manifest intricate vortical structures. We demonstrate the
efficacy of our neural fluid simulation in a variety of challenging simulation
scenarios, including leapfrogging vortices, colliding vortices, vortex
reconnections, as well as vortex generation from moving obstacles and density
differences. Our examples show increased performance over existing methods in
terms of energy conservation, visual complexity, adherence to experimental
observations, and preservation of detailed vortical structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14638">Balancing Energy Efficiency and Distributional Robustness in Over-the-Air Federated Learning. (arXiv:2312.14638v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Badi_M/0/1/0/all/0/1">Mohamed Badi</a>, <a href="http://arxiv.org/find/cs/1/au:+Issaid_C/0/1/0/all/0/1">Chaouki Ben Issaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Elgabli_A/0/1/0/all/0/1">Anis Elgabli</a>, <a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1">Mehdi Bennis</a></p>
<p>The growing number of wireless edge devices has magnified challenges
concerning energy, bandwidth, latency, and data heterogeneity. These challenges
have become bottlenecks for distributed learning. To address these issues, this
paper presents a novel approach that ensures energy efficiency for
distributionally robust federated learning (FL) with over air computation
(AirComp). In this context, to effectively balance robustness with energy
efficiency, we introduce a novel client selection method that integrates two
complementary insights: a deterministic one that is designed for energy
efficiency, and a probabilistic one designed for distributional robustness.
Simulation results underscore the efficacy of the proposed algorithm, revealing
its superior performance compared to baselines from both robustness and energy
efficiency perspectives, achieving more than 3-fold energy savings compared to
the considered baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14646">Collaborative Synthesis of Patient Records through Multi-Visit Health State Inference. (arXiv:2312.14646v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hongda Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hongzhan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Electronic health records (EHRs) have become the foundation of machine
learning applications in healthcare, while the utility of real patient records
is often limited by privacy and security concerns. Synthetic EHR generation
provides an additional perspective to compensate for this limitation. Most
existing methods synthesize new records based on real EHR data, without
consideration of different types of events in EHR data, which cannot control
the event combinations in line with medical common sense. In this paper, we
propose MSIC, a Multi-visit health Status Inference model for Collaborative EHR
synthesis to address these limitations. First, we formulate the synthetic EHR
generation process as a probabilistic graphical model and tightly connect
different types of events by modeling the latent health states. Then, we derive
a health state inference method tailored for the multi-visit scenario to
effectively utilize previous records to synthesize current and future records.
Furthermore, we propose to generate medical reports to add textual descriptions
for each medical event, providing broader applications for synthesized EHR
data. For generating different paragraphs in each visit, we incorporate a
multi-generator deliberation framework to collaborate the message passing of
multiple generators and employ a two-phase decoding strategy to generate
high-quality reports. Our extensive experiments on the widely used benchmarks,
MIMIC-III and MIMIC-IV, demonstrate that MSIC advances state-of-the-art results
on the quality of synthetic data while maintaining low privacy risks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14647">Pub/Sub Message Brokers for GenAI. (arXiv:2312.14647v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1">Alaa Saleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirttikangas_S/0/1/0/all/0/1">Susanna Pirttikangas</a>, <a href="http://arxiv.org/find/cs/1/au:+Loven_L/0/1/0/all/0/1">Lauri Lov&#xe9;n</a></p>
<p>In today's digital world, Generative Artificial Intelligence (GenAI) such as
Large Language Models (LLMs) is becoming increasingly prevalent, extending its
reach across diverse applications. This surge in adoption has sparked a
significant increase in demand for data-centric GenAI models, highlighting the
necessity for robust data communication infrastructures. Central to this need
are message brokers, which serve as essential channels for data transfer within
various system components. This survey aims to delve into a comprehensive
analysis of traditional and modern message brokers, offering a comparative
study of prevalent platforms. Our study considers numerous criteria including,
but not limited to, open-source availability, integrated monitoring tools,
message prioritization mechanisms, capabilities for parallel processing,
reliability, distribution and clustering functionalities, authentication
processes, data persistence strategies, fault tolerance, and scalability.
Furthermore, we explore the intrinsic constraints that the design and operation
of each message broker might impose, recognizing that these limitations are
crucial in understanding their real-world applicability. We then leverage these
insights to propose a sophisticated message broker framework -- one designed
with the adaptability and robustness necessary to meet the evolving requisites
of GenAI applications. Finally, this study examines the enhancement of message
broker mechanisms specifically for GenAI contexts, emphasizing the criticality
of developing a versatile message broker framework. Such a framework would be
poised for quick adaptation, catering to the dynamic and growing demands of
GenAI in the foreseeable future. Through this dual-pronged approach, we intend
to contribute a foundational compendium that can guide future innovations and
infrastructural advancements in the realm of GenAI data communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14651">SAVAE: Leveraging the variational Bayes autoencoder for survival analysis. (arXiv:2312.14651v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Apellaniz_P/0/1/0/all/0/1">Patricia A. Apell&#xe1;niz</a>, <a href="http://arxiv.org/find/cs/1/au:+Parras_J/0/1/0/all/0/1">Juan Parras</a>, <a href="http://arxiv.org/find/cs/1/au:+Zazo_S/0/1/0/all/0/1">Santiago Zazo</a></p>
<p>As in many fields of medical research, survival analysis has witnessed a
growing interest in the application of deep learning techniques to model
complex, high-dimensional, heterogeneous, incomplete, and censored medical
data. Current methods often make assumptions about the relations between data
that may not be valid in practice. In response, we introduce SAVAE (Survival
Analysis Variational Autoencoder), a novel approach based on Variational
Autoencoders. SAVAE contributes significantly to the field by introducing a
tailored ELBO formulation for survival analysis, supporting various parametric
distributions for covariates and survival time (as long as the log-likelihood
is differentiable). It offers a general method that consistently performs well
on various metrics, demonstrating robustness and stability through different
experiments. Our proposal effectively estimates time-to-event, accounting for
censoring, covariate interactions, and time-varying risk associations. We
validate our model in diverse datasets, including genomic, clinical, and
demographic data, with varying levels of censoring. This approach demonstrates
competitive performance compared to state-of-the-art techniques, as assessed by
the Concordance Index and the Integrated Brier Score. SAVAE also offers an
interpretable model that parametrically models covariates and time. Moreover,
its generative architecture facilitates further applications such as
clustering, data imputation, and the generation of synthetic patient data
through latent space inference from survival data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14657">Deep Non-Parametric Time Series Forecaster. (arXiv:2312.14657v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rangapuram_S/0/1/0/all/0/1">Syama Sundar Rangapuram</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1">Jan Gasthaus</a>, <a href="http://arxiv.org/find/cs/1/au:+Stella_L/0/1/0/all/0/1">Lorenzo Stella</a>, <a href="http://arxiv.org/find/cs/1/au:+Flunkert_V/0/1/0/all/0/1">Valentin Flunkert</a>, <a href="http://arxiv.org/find/cs/1/au:+Salinas_D/0/1/0/all/0/1">David Salinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Januschowski_T/0/1/0/all/0/1">Tim Januschowski</a></p>
<p>This paper presents non-parametric baseline models for time series
forecasting. Unlike classical forecasting models, the proposed approach does
not assume any parametric form for the predictive distribution and instead
generates predictions by sampling from the empirical distribution according to
a tunable strategy. By virtue of this, the model is always able to produce
reasonable forecasts (i.e., predictions within the observed data range) without
fail unlike classical models that suffer from numerical stability on some data
distributions. Moreover, we develop a global version of the proposed method
that automatically learns the sampling strategy by exploiting the information
across multiple related time series. The empirical evaluation shows that the
proposed methods have reasonable and consistent performance across all
datasets, proving them to be strong baselines to be considered in one's
forecasting toolbox.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14667">Token-Level Contrastive Learning with Modality-Aware Prompting for Multimodal Intent Recognition. (arXiv:2312.14667v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qianrui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hua Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanlei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kai Gao</a></p>
<p>Multimodal intent recognition aims to leverage diverse modalities such as
expressions, body movements and tone of speech to comprehend user's intent,
constituting a critical task for understanding human language and behavior in
real-world multimodal scenarios. Nevertheless, the majority of existing methods
ignore potential correlations among different modalities and own limitations in
effectively learning semantic features from nonverbal modalities. In this
paper, we introduce a token-level contrastive learning method with
modality-aware prompting (TCL-MAP) to address the above challenges. To
establish an optimal multimodal semantic environment for text modality, we
develop a modality-aware prompting module (MAP), which effectively aligns and
fuses features from text, video and audio modalities with similarity-based
modality alignment and cross-modality attention mechanism. Based on the
modality-aware prompt and ground truth labels, the proposed token-level
contrastive learning framework (TCL) constructs augmented samples and employs
NT-Xent loss on the label token. Specifically, TCL capitalizes on the optimal
textual semantic insights derived from intent labels to guide the learning
processes of other modalities in return. Extensive experiments show that our
method achieves remarkable improvements compared to state-of-the-art methods.
Additionally, ablation analyses demonstrate the superiority of the
modality-aware prompt over the handcrafted prompt, which holds substantial
significance for multimodal prompt learning. The codes are released at
https://github.com/thuiar/TCL-MAP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14681">Engineered Ordinary Differential Equations as Classification Algorithm (EODECA): thorough characterization and testing. (arXiv:2312.14681v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marino_R/0/1/0/all/0/1">Raffaele Marino</a>, <a href="http://arxiv.org/find/cs/1/au:+Buffoni_L/0/1/0/all/0/1">Lorenzo Buffoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Chicchi_L/0/1/0/all/0/1">Lorenzo Chicchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Giambagli_L/0/1/0/all/0/1">Lorenzo Giambagli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanelli_D/0/1/0/all/0/1">Duccio Fanelli</a></p>
<p>EODECA (Engineered Ordinary Differential Equations as Classification
Algorithm) is a novel approach at the intersection of machine learning and
dynamical systems theory, presenting a unique framework for classification
tasks [1]. This method stands out with its dynamical system structure,
utilizing ordinary differential equations (ODEs) to efficiently handle complex
classification challenges. The paper delves into EODECA's dynamical properties,
emphasizing its resilience against random perturbations and robust performance
across various classification scenarios. Notably, EODECA's design incorporates
the ability to embed stable attractors in the phase space, enhancing
reliability and allowing for reversible dynamics. In this paper, we carry out a
comprehensive analysis by expanding on the work [1], and employing a Euler
discretization scheme. In particular, we evaluate EODECA's performance across
five distinct classification problems, examining its adaptability and
efficiency. Significantly, we demonstrate EODECA's effectiveness on the MNIST
and Fashion MNIST datasets, achieving impressive accuracies of $98.06\%$ and
$88.21\%$, respectively. These results are comparable to those of a multi-layer
perceptron (MLP), underscoring EODECA's potential in complex data processing
tasks. We further explore the model's learning journey, assessing its evolution
in both pre and post training environments and highlighting its ability to
navigate towards stable attractors. The study also investigates the
invertibility of EODECA, shedding light on its decision-making processes and
internal workings. This paper presents a significant step towards a more
transparent and robust machine learning paradigm, bridging the gap between
machine learning algorithms and dynamical systems methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14688">A Mathematical Guide to Operator Learning. (arXiv:2312.14688v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Boulle_N/0/1/0/all/0/1">Nicolas Boull&#xe9;</a>, <a href="http://arxiv.org/find/math/1/au:+Townsend_A/0/1/0/all/0/1">Alex Townsend</a></p>
<p>Operator learning aims to discover properties of an underlying dynamical
system or partial differential equation (PDE) from data. Here, we present a
step-by-step guide to operator learning. We explain the types of problems and
PDEs amenable to operator learning, discuss various neural network
architectures, and explain how to employ numerical PDE solvers effectively. We
also give advice on how to create and manage training data and conduct
optimization. We offer intuition behind the various neural network
architectures employed in operator learning by motivating them from the
point-of-view of numerical linear algebra.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14698">Time-changed normalizing flows for accurate SDE modeling. (arXiv:2312.14698v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bekri_N/0/1/0/all/0/1">Naoufal El Bekri</a>, <a href="http://arxiv.org/find/cs/1/au:+Drumetz_L/0/1/0/all/0/1">Lucas Drumetz</a>, <a href="http://arxiv.org/find/cs/1/au:+Vermet_F/0/1/0/all/0/1">Franck Vermet</a></p>
<p>The generative paradigm has become increasingly important in machine learning
and deep learning models. Among popular generative models are normalizing
flows, which enable exact likelihood estimation by transforming a base
distribution through diffeomorphic transformations. Extending the normalizing
flow framework to handle time-indexed flows gave dynamic normalizing flows, a
powerful tool to model time series, stochastic processes, and neural stochastic
differential equations (SDEs). In this work, we propose a novel variant of
dynamic normalizing flows, a Time Changed Normalizing Flow (TCNF), based on
time deformation of a Brownian motion which constitutes a versatile and
extensive family of Gaussian processes. This approach enables us to effectively
model some SDEs, that cannot be modeled otherwise, including standard ones such
as the well-known Ornstein-Uhlenbeck process, and generalizes prior
methodologies, leading to improved results and better inference and prediction
capability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14705">SCUNet++: Assessment of Pulmonary Embolism CT Image Segmentation Leveraging Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection. (arXiv:2312.14705v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zou_B/0/1/0/all/0/1">Binfeng Zou</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1">Zhaoxin Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yiyu Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yifan Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Qin_F/0/1/0/all/0/1">Feiwei Qin</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1">Qinhai Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Changmiao Wang</a></p>
<p>Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14712">Can Machines Learn Robustly, Privately, and Efficiently?. (arXiv:2312.14712v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Allouah_Y/0/1/0/all/0/1">Youssef Allouah</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1">Rachid Guerraoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1">John Stephan</a></p>
<p>The success of machine learning (ML) applications relies on vast datasets and
distributed architectures, which, as they grow, present challenges for ML. In
real-world scenarios, where data often contains sensitive information, issues
like data poisoning and hardware failures are common. Ensuring privacy and
robustness is vital for the broad adoption of ML in public life. This paper
examines the costs associated with achieving these objectives in distributed
architectures. We overview the meanings of privacy and robustness in
distributed ML, and clarify how they can be achieved efficiently in isolation.
However, we contend that the integration of these objectives entails a notable
compromise in computational efficiency. We delve into this intricate balance,
exploring the challenges and solutions for privacy, robustness, and
computational efficiency in ML applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14748">Progressing from Anomaly Detection to Automated Log Labeling and Pioneering Root Cause Analysis. (arXiv:2312.14748v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wittkopp_T/0/1/0/all/0/1">Thorsten Wittkopp</a>, <a href="http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1">Alexander Acker</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1">Odej Kao</a></p>
<p>The realm of AIOps is transforming IT landscapes with the power of AI and ML.
Despite the challenge of limited labeled data, supervised models show promise,
emphasizing the importance of leveraging labels for training, especially in
deep learning contexts. This study enhances the field by introducing a taxonomy
for log anomalies and exploring automated data labeling to mitigate labeling
challenges. It goes further by investigating the potential of diverse anomaly
detection techniques and their alignment with specific anomaly types. However,
the exploration doesn't stop at anomaly detection. The study envisions a future
where root cause analysis follows anomaly detection, unraveling the underlying
triggers of anomalies. This uncharted territory holds immense potential for
revolutionizing IT systems management. In essence, this paper enriches our
understanding of anomaly detection, and automated labeling, and sets the stage
for transformative root cause analysis. Together, these advances promise more
resilient IT systems, elevating operational efficiency and user satisfaction in
an ever-evolving technological landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14751">Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models. (arXiv:2312.14751v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1">Alan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucknall_B/0/1/0/all/0/1">Ben Bucknall</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1">Herbie Bradley</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Krueger</a></p>
<p>Public release of the weights of pretrained foundation models, otherwise
known as downloadable access \citep{solaiman_gradient_2023}, enables
fine-tuning without the prohibitive expense of pretraining. Our work argues
that increasingly accessible fine-tuning of downloadable models may increase
hazards. First, we highlight research to improve the accessibility of
fine-tuning. We split our discussion into research that A) reduces the
computational cost of fine-tuning and B) improves the ability to share that
cost across more actors. Second, we argue that increasingly accessible
fine-tuning methods may increase hazard through facilitating malicious use and
making oversight of models with potentially dangerous capabilities more
difficult. Third, we discuss potential mitigatory measures, as well as benefits
of more accessible fine-tuning. Given substantial remaining uncertainty about
hazards, we conclude by emphasizing the urgent need for the development of
mitigations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14758">Diffusion Maps for Signal Filtering in Graph Learning. (arXiv:2312.14758v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hildebrant_T/0/1/0/all/0/1">Todd Hildebrant</a></p>
<p>This paper explores the application diffusion maps as graph shift operators
in understanding the underlying geometry of graph signals. The study evaluates
the improvements in graph learning when using diffusion map generated filters
to the Markov Variation minimization problem. The paper showcases the
effectiveness of this approach through examples involving synthetically
generated and real-world temperature sensor data. These examples also compare
the diffusion map graph signal model with other commonly used graph signal
operators. The results provide new approaches for the analysis and
understanding of complex, non-Euclidean data structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14763">Enhanced Latent Multi-view Subspace Clustering. (arXiv:2312.14763v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Long Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Lei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Badong Chen</a></p>
<p>Latent multi-view subspace clustering has been demonstrated to have desirable
clustering performance. However, the original latent representation method
vertically concatenates the data matrices from multiple views into a single
matrix along the direction of dimensionality to recover the latent
representation matrix, which may result in an incomplete information recovery.
To fully recover the latent space representation, we in this paper propose an
Enhanced Latent Multi-view Subspace Clustering (ELMSC) method. The ELMSC method
involves constructing an augmented data matrix that enhances the representation
of multi-view data. Specifically, we stack the data matrices from various views
into the block-diagonal locations of the augmented matrix to exploit the
complementary information. Meanwhile, the non-block-diagonal entries are
composed based on the similarity between different views to capture the
consistent information. In addition, we enforce a sparse regularization for the
non-diagonal blocks of the augmented self-representation matrix to avoid
redundant calculations of consistency information. Finally, a novel iterative
algorithm based on the framework of Alternating Direction Method of Multipliers
(ADMM) is developed to solve the optimization problem for ELMSC. Extensive
experiments on real-world datasets demonstrate that our proposed ELMSC is able
to achieve higher clustering performance than some state-of-art multi-view
clustering methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14769">Large Language Model (LLM) Bias Index -- LLMBI. (arXiv:2312.14769v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1">Abiodun Finbarrs Oketunji</a>, <a href="http://arxiv.org/find/cs/1/au:+Anas_M/0/1/0/all/0/1">Muhammad Anas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saina_D/0/1/0/all/0/1">Deepthi Saina</a></p>
<p>The Large Language Model Bias Index (LLMBI) is a pioneering approach designed
to quantify and address biases inherent in large language models (LLMs), such
as GPT-4. We recognise the increasing prevalence and impact of LLMs across
diverse sectors. This research introduces a novel metric, LLMBI, to
systematically measure and mitigate biases potentially skewing model responses.
We formulated LLMBI using a composite scoring system incorporating multiple
dimensions of bias, including but not limited to age, gender, and racial
biases.
</p>
<p>To operationalise this metric, we engaged in a multi-step process involving
collecting and annotating LLM responses, applying sophisticated Natural
Language Processing (NLP) techniques for bias detection, and computing the
LLMBI score through a specially crafted mathematical formula. The formula
integrates weighted averages of various bias dimensions, a penalty for dataset
diversity deficiencies, and a correction for sentiment biases. Our empirical
analysis, conducted using responses from OpenAI's API, employs advanced
sentiment analysis as a representative method for bias detection.
</p>
<p>The research reveals LLMs, whilst demonstrating impressive capabilities in
text generation, exhibit varying degrees of bias across different dimensions.
LLMBI provides a quantifiable measure to compare biases across models and over
time, offering a vital tool for systems engineers, researchers and regulators
in enhancing the fairness and reliability of LLMs. It highlights the potential
of LLMs in mimicking unbiased human-like responses. Additionally, it
underscores the necessity of continuously monitoring and recalibrating such
models to align with evolving societal norms and ethical standards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14770">Integration Of Evolutionary Automated Machine Learning With Structural Sensitivity Analysis For Composite Pipelines. (arXiv:2312.14770v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1">Nikolay O. Nikitin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinchuk_M/0/1/0/all/0/1">Maiia Pinchuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Pokrovskii_V/0/1/0/all/0/1">Valerii Pokrovskii</a>, <a href="http://arxiv.org/find/cs/1/au:+Shevchenko_P/0/1/0/all/0/1">Peter Shevchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Getmanov_A/0/1/0/all/0/1">Andrey Getmanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksenkin_Y/0/1/0/all/0/1">Yaroslav Aksenkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Revin_I/0/1/0/all/0/1">Ilia Revin</a>, <a href="http://arxiv.org/find/cs/1/au:+Stebenkov_A/0/1/0/all/0/1">Andrey Stebenkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Poslavskaya_E/0/1/0/all/0/1">Ekaterina Poslavskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1">Anna V. Kalyuzhnaya</a></p>
<p>Automated machine learning (AutoML) systems propose an end-to-end solution to
a given machine learning problem, creating either fixed or flexible pipelines.
Fixed pipelines are task independent constructs: their general composition
remains the same, regardless of the data. In contrast, the structure of
flexible pipelines varies depending on the input, making them finely tailored
to individual tasks. However, flexible pipelines can be structurally
overcomplicated and have poor explainability. We propose the EVOSA approach
that compensates for the negative points of flexible pipelines by incorporating
a sensitivity analysis which increases the robustness and interpretability of
the flexible solutions. EVOSA quantitatively estimates positive and negative
impact of an edge or a node on a pipeline graph, and feeds this information to
the evolutionary AutoML optimizer. The correctness and efficiency of EVOSA was
validated in tabular, multimodal and computer vision tasks, suggesting
generalizability of the proposed approach across domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14792">The Rate-Distortion-Perception-Classification Tradeoff: Joint Source Coding and Modulation via Inverse-Domain GANs. (arXiv:2312.14792v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Junli Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mota_J/0/1/0/all/0/1">Jo&#xe3;o F. C. Mota</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1">Baoshan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1">Xuemin Hong</a></p>
<p>The joint source coding and modulation (JSCM) framework was enabled by recent
developments in deep learning, which allows to automatically learn from data,
and in an end-to-end fashion, the best compression codes and modulation
schemes. In this paper, we show the existence of a strict tradeoff between
channel rate, distortion, perception, and classification accuracy in a JSCM
scenario. We then propose two image compression methods to navigate that
tradeoff: an inverse-domain generative adversarial network (ID-GAN), which
achieves extreme compression, and a simpler, heuristic method that reveals
insights about the performance of ID-GAN. Experiment results not only
corroborate the theoretical findings, but also demonstrate that the proposed
ID-GAN algorithm significantly improves system performance compared to
traditional separation-based methods and recent deep JSCM architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14795">On support vector machines under a multiple-cost scenario. (arXiv:2312.14795v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Benitez_Pena_S/0/1/0/all/0/1">Sandra Ben&#xed;tez-Pe&#xf1;a</a>, <a href="http://arxiv.org/find/stat/1/au:+Blanquero_R/0/1/0/all/0/1">Rafael Blanquero</a>, <a href="http://arxiv.org/find/stat/1/au:+Carrizosa_E/0/1/0/all/0/1">Emilio Carrizosa</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramirez_Cobo_P/0/1/0/all/0/1">Pepa Ram&#xed;rez-Cobo</a></p>
<p>Support Vector Machine (SVM) is a powerful tool in binary classification,
known to attain excellent misclassification rates. On the other hand, many
realworld classification problems, such as those found in medical diagnosis,
churn or fraud prediction, involve misclassification costs which may be
different in the different classes. However, it may be hard for the user to
provide precise values for such misclassification costs, whereas it may be much
easier to identify acceptable misclassification rates values. In this paper we
propose a novel SVM model in which misclassification costs are considered by
incorporating performance constraints in the problem formulation. Specifically,
our aim is to seek the hyperplane with maximal margin yielding
misclassification rates below given threshold values. Such maximal margin
hyperplane is obtained by solving a quadratic convex problem with linear
constraints and integer variables. The reported numerical experience shows that
our model gives the user control on the misclassification rates in one class
(possibly at the expense of an increase in misclassification rates for the
other class) and is feasible in terms of running times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14806">The Effects of Signal-to-Noise Ratio on Generative Adversarial Networks Applied to Marine Bioacoustic Data. (arXiv:2312.14806v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atkinson_G/0/1/0/all/0/1">Georgia Atkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_N/0/1/0/all/0/1">Nick Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1">A. Stephen McGough</a>, <a href="http://arxiv.org/find/cs/1/au:+Berggren_P/0/1/0/all/0/1">Per Berggren</a></p>
<p>In recent years generative adversarial networks (GANs) have been used to
supplement datasets within the field of marine bioacoustics. This is driven by
factors such as the cost to collect data, data sparsity and aid preprocessing.
One notable challenge with marine bioacoustic data is the low signal-to-noise
ratio (SNR) posing difficulty when applying deep learning techniques such as
GANs. This work investigates the effect SNR has on the audio-based GAN
performance and examines three different evaluation methodologies for GAN
performance, yielding interesting results on the effects of SNR on GANs,
specifically WaveGAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14812">PARDINUS: Weakly supervised discarding of photo-trapping empty images based on autoencoders. (arXiv:2312.14812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rosa_D/0/1/0/all/0/1">David de la Rosa</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1">Antonio J Rivera</a>, <a href="http://arxiv.org/find/cs/1/au:+Jesus_M/0/1/0/all/0/1">Mar&#xed;a J del Jesus</a>, <a href="http://arxiv.org/find/cs/1/au:+Charte_F/0/1/0/all/0/1">Francisco Charte</a></p>
<p>Photo-trapping cameras are widely employed for wildlife monitoring. Those
cameras take photographs when motion is detected to capture images where
animals appear. A significant portion of these images are empty - no wildlife
appears in the image. Filtering out those images is not a trivial task since it
requires hours of manual work from biologists. Therefore, there is a notable
interest in automating this task. Automatic discarding of empty photo-trapping
images is still an open field in the area of Machine Learning. Existing
solutions often rely on state-of-the-art supervised convolutional neural
networks that require the annotation of the images in the training phase.
PARDINUS (Weakly suPervised discARDINg of photo-trapping empty images based on
aUtoencoderS) is constructed on the foundation of weakly supervised learning
and proves that this approach equals or even surpasses other fully supervised
methods that require further labeling work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14820">Understanding the Regularity of Self-Attention with Optimal Transport. (arXiv:2312.14820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castin_V/0/1/0/all/0/1">Val&#xe9;rie Castin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ablin_P/0/1/0/all/0/1">Pierre Ablin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1">Gabriel Peyr&#xe9;</a></p>
<p>Transformers and their multi-head attention mechanism have completely changed
the machine learning landscape in just a few years, by outperforming
state-of-art models in a wide range of domains. Still, little is known about
their robustness from a theoretical perspective. We tackle this problem by
studying the local Lipschitz constant of self-attention, that provides an
attack-agnostic way of measuring the robustness of a neural network. We adopt a
measure-theoretic framework, by viewing inputs as probability measures equipped
with the Wasserstein distance. This allows us to generalize attention to inputs
of infinite length, and to derive an upper bound and a lower bound on the
Lipschitz constant of self-attention on compact sets. The lower bound
significantly improves prior results, and grows more than exponentially with
the radius of the compact set, which rules out the possibility of obtaining
robustness guarantees without any additional constraint on the input space. Our
results also point out that measures with a high local Lipschitz constant are
typically made of a few diracs, with a very unbalanced distribution of mass.
Finally, we analyze the stability of self-attention under perturbations that
change the number of tokens, which appears to be a natural question in the
measure-theoretic framework. In particular, we show that for some inputs,
attacks that duplicate tokens before perturbing them are more efficient than
attacks that simply move tokens. We call this phenomenon mass splitting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14836">Learning Lagrangian Multipliers for the Travelling Salesman Problem. (arXiv:2312.14836v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parjadis_A/0/1/0/all/0/1">Augustin Parjadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1">Quentin Cappart</a>, <a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1">Bistra Dilkina</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1">Aaron Ferber</a>, <a href="http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1">Louis-Martin Rousseau</a></p>
<p>Lagrangian relaxation is a versatile mathematical technique employed to relax
constraints in an optimization problem, enabling the generation of dual bounds
to prove the optimality of feasible solutions and the design of efficient
propagators in constraint programming (such as the weighted circuit
constraint). However, the conventional process of deriving Lagrangian
multipliers (e.g., using subgradient methods) is often computationally
intensive, limiting its practicality for large-scale or time-sensitive
problems. To address this challenge, we propose an innovative unsupervised
learning approach that harnesses the capabilities of graph neural networks to
exploit the problem structure, aiming to generate accurate Lagrangian
multipliers efficiently. We apply this technique to the well-known Held-Karp
Lagrangian relaxation for the travelling salesman problem. The core idea is to
predict accurate Lagrangian multipliers and to employ them as a warm start for
generating Held-Karp relaxation bounds. These bounds are subsequently utilized
to enhance the filtering process carried out by branch-and-bound algorithms. In
contrast to much of the existing literature, which primarily focuses on finding
feasible solutions, our approach operates on the dual side, demonstrating that
learning can also accelerate the proof of optimality. We conduct experiments
across various distributions of the metric travelling salesman problem,
considering instances with up to 200 cities. The results illustrate that our
approach can improve the filtering level of the weighted circuit global
constraint, reduce the optimality gap by a factor two for unsolved instances up
to a timeout, and reduce the execution time for solved instances by 10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14847">Large Scale Traning of Graph Neural Networks for Optimal Markov-Chain Partitioning Using the Kemeny Constant. (arXiv:2312.14847v1 [physics.bio-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Martino_S/0/1/0/all/0/1">Sam Alexander Martino</a>, <a href="http://arxiv.org/find/physics/1/au:+Morado_J/0/1/0/all/0/1">Jo&#xe3;o Morado</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_C/0/1/0/all/0/1">Chenghao Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Lu_Z/0/1/0/all/0/1">Zhenghao Lu</a>, <a href="http://arxiv.org/find/physics/1/au:+Rosta_E/0/1/0/all/0/1">Edina Rosta</a></p>
<p>Traditional clustering algorithms often struggle to capture the complex
relationships within graphs and generalise to arbitrary clustering criteria.
The emergence of graph neural networks (GNNs) as a powerful framework for
learning representations of graph data provides new approaches to solving the
problem. Previous work has shown GNNs to be capable of proposing partitionings
using a variety of criteria, however, these approaches have not yet been
extended to work on Markov chains or kinetic networks. These arise frequently
in the study of molecular systems and are of particular interest to the
biochemical modelling community. In this work, we propose several GNN-based
architectures to tackle the graph partitioning problem for Markov Chains
described as kinetic networks. This approach aims to minimize how much a
proposed partitioning changes the Kemeny constant. We propose using an
encoder-decoder architecture and show how simple GraphSAGE-based GNNs with
linear layers can outperform much larger and more expressive attention-based
models in this context. As a proof of concept, we first demonstrate the
method's ability to cluster randomly connected graphs. We also use a linear
chain architecture corresponding to a 1D free energy profile as our kinetic
network. Subsequently, we demonstrate the effectiveness of our method through
experiments on a data set derived from molecular dynamics. We compare the
performance of our method to other partitioning techniques such as PCCA+. We
explore the importance of feature and hyperparameter selection and propose a
general strategy for large-scale parallel training of GNNs for discovering
optimal graph partitionings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14869">Spatiotemporal-Linear: Towards Universal Multivariate Time Series Forecasting. (arXiv:2312.14869v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_A/0/1/0/all/0/1">Aiyinsi Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haixi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zirui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Ce Zheng</a></p>
<p>Within the field of complicated multivariate time series forecasting (TSF),
popular techniques frequently rely on intricate deep learning architectures,
ranging from transformer-based designs to recurrent neural networks. However,
recent findings suggest that simple Linear models can surpass sophisticated
constructs on diverse datasets. These models directly map observation to
multiple future time steps, thereby minimizing error accumulation in iterative
multi-step prediction. Yet, these models fail to incorporate spatial and
temporal information within the data, which is critical for capturing patterns
and dependencies that drive insightful predictions. This oversight often leads
to performance bottlenecks, especially under specific sequence lengths and
dataset conditions, preventing their universal application. In response, we
introduce the SpatioTemporal-Linear (STL) framework. STL seamlessly integrates
time-embedded and spatially-informed bypasses to augment the Linear-based
architecture. These extra routes offer a more robust and refined regression to
the data, particularly when the amount of observation is limited and the
capacity of simple linear layers to capture dependencies declines. Empirical
evidence highlights STL's prowess, outpacing both Linear and Transformer
benchmarks across varied observation and prediction durations and datasets.
Such robustness accentuates its suitability across a spectrum of applications,
including but not limited to, traffic trajectory and rare disease progression
forecasting. Through this discourse, we not only validate the STL's distinctive
capacities to become a more general paradigm in multivariate time-series
prediction using deep-learning techniques but also stress the need to tackle
data-scarce prediction scenarios for universal application. Code will be made
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14878">Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning. (arXiv:2312.14878v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1">Filippos Christianos</a>, <a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1">Georgios Papoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1">Matthieu Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Coste_T/0/1/0/all/0/1">Thomas Coste</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingxuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_K/0/1/0/all/0/1">Khyati Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Doran_J/0/1/0/all/0/1">James Doran</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xidong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiacheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zheng Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yicheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jianye Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_K/0/1/0/all/0/1">Kun Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bou_Ammar_H/0/1/0/all/0/1">Haitham Bou-Ammar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a></p>
<p>A key method for creating Artificial Intelligence (AI) agents is
Reinforcement Learning (RL). However, constructing a standalone RL policy that
maps perception to action directly encounters severe problems, chief among them
being its lack of generality across multiple tasks and the need for a large
amount of training data. The leading cause is that it cannot effectively
integrate prior information into the perception-action cycle when devising the
policy. Large language models (LLMs) emerged as a fundamental way to
incorporate cross-domain knowledge into AI agents but lack crucial learning and
adaptation toward specific decision problems. This paper presents a general
framework model for integrating and learning structured reasoning into AI
agents' policies. Our methodology is motivated by the modularity found in the
human brain. The framework utilises the construction of intrinsic and extrinsic
functions to add previous understandings of reasoning structures. It also
provides the adaptive ability to learn models inside every module or function,
consistent with the modular structure of cognitive processes. We describe the
framework in-depth and compare it with other AI pipelines and existing
frameworks. The paper explores practical applications, covering experiments
that show the effectiveness of our method. Our results indicate that AI agents
perform and adapt far better when organised reasoning and prior knowledge are
embedded. This opens the door to more resilient and general AI agent systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14880">SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting. (arXiv:2312.14880v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bergsma_S/0/1/0/all/0/1">Shane Bergsma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeyl_T/0/1/0/all/0/1">Timothy Zeyl</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Lei Guo</a></p>
<p>We propose SutraNets, a novel method for neural probabilistic forecasting of
long-sequence time series. SutraNets use an autoregressive generative model to
factorize the likelihood of long sequences into products of conditional
probabilities. When generating long sequences, most autoregressive approaches
suffer from harmful error accumulation, as well as challenges in modeling
long-distance dependencies. SutraNets treat long, univariate prediction as
multivariate prediction over lower-frequency sub-series. Autoregression
proceeds across time and across sub-series in order to ensure coherent
multivariate (and, hence, high-frequency univariate) outputs. Since sub-series
can be generated using fewer steps, SutraNets effectively reduce error
accumulation and signal path distances. We find SutraNets to significantly
improve forecasting accuracy over competitive alternatives on six real-world
datasets, including when we vary the number of sub-series and scale up the
depth and width of the underlying sequence models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14886">Sample Path Regularity of Gaussian Processes from the Covariance Kernel. (arXiv:2312.14886v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Costa_N/0/1/0/all/0/1">Natha&#xeb;l Da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfortner_M/0/1/0/all/0/1">Marvin Pf&#xf6;rtner</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1">Lancelot Da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1">Philipp Hennig</a></p>
<p>Gaussian processes (GPs) are the most common formalism for defining
probability distributions over spaces of functions. While applications of GPs
are myriad, a comprehensive understanding of GP sample paths, i.e. the function
spaces over which they define a probability measure on, is lacking. In
practice, GPs are not constructed through a probability measure, but instead
through a mean function and a covariance kernel. In this paper we provide
necessary and sufficient conditions on the covariance kernel for the sample
paths of the corresponding GP to attain a given regularity. We use the
framework of H\"older regularity as it grants us particularly straightforward
conditions, which simplify further in the cases of stationary and isotropic
GPs. We then demonstrate that our results allow for novel and unusually tight
characterisations of the sample path regularities of the GPs commonly used in
machine learning applications, such as the Mat\'ern GPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14889">On rate-optimal classification from non-private and from private data. (arXiv:2312.14889v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1">Bal&#xe1;zs Csan&#xe1;d Cs&#xe1;ji</a>, <a href="http://arxiv.org/find/stat/1/au:+Gyorfi_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; Gy&#xf6;rfi</a>, <a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1">Ambrus Tam&#xe1;s</a></p>
<p>In this paper we revisit the classical problem of classification, but impose
privacy constraints. Under such constraints, the raw data
$(X_1,Y_1),\ldots,(X_n,Y_n)$ cannot be directly observed, and all classifiers
are functions of the randomised outcome of a suitable local differential
privacy mechanism. The statistician is free to choose the form of this privacy
mechanism, and here we add Laplace distributed noise to a discretisation of the
location of each feature vector $X_i$ and to its label $Y_i$. The
classification rule is the privatized version of the well-studied partitioning
classification rule. In addition to the standard Lipschitz and margin
conditions, a novel characteristic is introduced, by which the exact rate of
convergence of the classification error probability is calculated, both for
non-private and private data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14890">NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lizhou Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Haoyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1">Libby Hemphill</a></p>
<p>Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14891">DRStageNet: Deep Learning for Diabetic Retinopathy Staging from Fundus Images. (arXiv:2312.14891v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Men_Y/0/1/0/all/0/1">Yevgeniy Men</a>, <a href="http://arxiv.org/find/eess/1/au:+Fhima_J/0/1/0/all/0/1">Jonathan Fhima</a>, <a href="http://arxiv.org/find/eess/1/au:+Celi_L/0/1/0/all/0/1">Leo Anthony Celi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ribeiro_L/0/1/0/all/0/1">Lucas Zago Ribeiro</a>, <a href="http://arxiv.org/find/eess/1/au:+Nakayama_L/0/1/0/all/0/1">Luis Filipe Nakayama</a>, <a href="http://arxiv.org/find/eess/1/au:+Behar_J/0/1/0/all/0/1">Joachim A. Behar</a></p>
<p>Diabetic retinopathy (DR) is a prevalent complication of diabetes associated
with a significant risk of vision loss. Timely identification is critical to
curb vision impairment. Algorithms for DR staging from digital fundus images
(DFIs) have been recently proposed. However, models often fail to generalize
due to distribution shifts between the source domain on which the model was
trained and the target domain where it is deployed. A common and particularly
challenging shift is often encountered when the source- and target-domain
supports do not fully overlap. In this research, we introduce DRStageNet, a
deep learning model designed to mitigate this challenge. We used seven publicly
available datasets, comprising a total of 93,534 DFIs that cover a variety of
patient demographics, ethnicities, geographic origins and comorbidities. We
fine-tune DINOv2, a pretrained model of self-supervised vision transformer, and
implement a multi-source domain fine-tuning strategy to enhance generalization
performance. We benchmark and demonstrate the superiority of our method to two
state-of-the-art benchmarks, including a recently published foundation model.
We adapted the grad-rollout method to our regression task in order to provide
high-resolution explainability heatmaps. The error analysis showed that 59\% of
the main errors had incorrect reference labels. DRStageNet is accessible at URL
[upon acceptance of the manuscript].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14895">FAST: Feature Aware Similarity Thresholding for Weak Unlearning in Black-Box Generative Models. (arXiv:2312.14895v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1">Subhodip Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1">Prathosh AP</a></p>
<p>The heightened emphasis on the regulation of deep generative models,
propelled by escalating concerns pertaining to privacy and compliance with
regulatory frameworks, underscores the imperative need for precise control
mechanisms over these models. This urgency is particularly underscored by
instances in which generative models generate outputs that encompass
objectionable, offensive, or potentially injurious content. In response,
machine unlearning has emerged to selectively forget specific knowledge or
remove the influence of undesirable data subsets from pre-trained models.
However, modern machine unlearning approaches typically assume access to model
parameters and architectural details during unlearning, which is not always
feasible. In multitude of downstream tasks, these models function as black-box
systems, with inaccessible pre-trained parameters, architectures, and training
data. In such scenarios, the possibility of filtering undesired outputs becomes
a practical alternative. The primary goal of this study is twofold: first, to
elucidate the relationship between filtering and unlearning processes, and
second, to formulate a methodology aimed at mitigating the display of
undesirable outputs generated from models characterized as black-box systems.
Theoretical analysis in this study demonstrates that, in the context of
black-box models, filtering can be seen as a form of weak unlearning. Our
proposed \textbf{\textit{Feature Aware Similarity Thresholding(FAST)}} method
effectively suppresses undesired outputs by systematically encoding the
representation of unwanted features in the latent space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14919">Lift-Attend-Splat: Bird&#x27;s-eye-view camera-lidar fusion using transformers. (arXiv:2312.14919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gunn_J/0/1/0/all/0/1">James Gunn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenyk_Z/0/1/0/all/0/1">Zygmunt Lenyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Anuj Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1">Andrea Donati</a>, <a href="http://arxiv.org/find/cs/1/au:+Buburuzan_A/0/1/0/all/0/1">Alexandru Buburuzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1">John Redford</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1">Romain Mueller</a></p>
<p>Combining complementary sensor modalities is crucial to providing robust
perception for safety-critical robotics applications such as autonomous driving
(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely on
monocular depth estimation which is a notoriously difficult task compared to
using depth information from the lidar directly. Here, we find that this
approach does not leverage depth as expected and show that naively improving
depth estimation does not lead to improvements in object detection performance
and that, strikingly, removing depth estimation altogether does not degrade
object detection performance. This suggests that relying on monocular depth
could be an unnecessary architectural bottleneck during camera-lidar fusion. In
this work, we introduce a novel fusion method that bypasses monocular depth
estimation altogether and instead selects and fuses camera and lidar features
in a bird's-eye-view grid using a simple attention mechanism. We show that our
model can modulate its use of camera features based on the availability of
lidar features and that it yields better 3D object detection on the nuScenes
dataset than baselines relying on monocular depth estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14920">A Novel Sampled Clustering Algorithm for Rice Phenotypic Data. (arXiv:2312.14920v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mithun Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1">Kapil Ahuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratnaparkhe_M/0/1/0/all/0/1">Milind B. Ratnaparkhe</a></p>
<p>Phenotypic (or Physical) characteristics of plant species are commonly used
to perform clustering. In one of our recent works (Shastri et al. (2021)), we
used a probabilistically sampled (using pivotal sampling) and spectrally
clustered algorithm to group soybean species. These techniques were used to
obtain highly accurate clusterings at a reduced cost. In this work, we extend
the earlier algorithm to cluster rice species. We improve the base algorithm in
three ways. First, we propose a new function to build the similarity matrix in
Spectral Clustering. Commonly, a natural exponential function is used for this
purpose. Based upon the spectral graph theory and the involved Cheeger's
inequality, we propose the use a base "a" exponential function instead. This
gives a similarity matrix spectrum favorable for clustering, which we support
via an eigenvalue analysis.
</p>
<p>Second, the function used to build the similarity matrix in Spectral
Clustering was earlier scaled with a fixed factor (called global scaling).
Based upon the idea of Zelnik-Manor and Perona (2004), we now use a factor that
varies with matrix elements (called local scaling) and works better. Third, to
compute the inclusion probability of a specie in the pivotal sampling
algorithm, we had earlier used the notion of deviation that captured how far
specie's characteristic values were from their respective base values (computed
over all species). A maximum function was used before to find the base values.
We now use a median function, which is more intuitive. We support this choice
using a statistical analysis. With experiments on 1865 rice species, we
demonstrate that in terms of silhouette values, our new Sampled Spectral
Clustering is 61% better than Hierarchical Clustering (currently prevalent).
Also, our new algorithm is significantly faster than Hierarchical Clustering
due to the involved sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14922">Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks. (arXiv:2312.14922v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Szekely_E/0/1/0/all/0/1">Eszter Sz&#xe9;kely</a>, <a href="http://arxiv.org/find/stat/1/au:+Bardone_L/0/1/0/all/0/1">Lorenzo Bardone</a>, <a href="http://arxiv.org/find/stat/1/au:+Gerace_F/0/1/0/all/0/1">Federica Gerace</a>, <a href="http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1">Sebastian Goldt</a></p>
<p>Neural networks excel at discovering statistical patterns in high-dimensional
data sets. In practice, higher-order cumulants, which quantify the non-Gaussian
correlations between three or more variables, are particularly important for
the performance of neural networks. But how efficient are neural networks at
extracting features from higher-order cumulants? We study this question in the
spiked cumulant model, where the statistician needs to recover a privileged
direction or "spike" from the order-$p\ge 4$ cumulants of~$d$-dimensional
inputs. We first characterise the fundamental statistical and computational
limits of recovering the spike by analysing the number of samples~$n$ required
to strongly distinguish between inputs from the spiked cumulant model and
isotropic Gaussian inputs. We find that statistical distinguishability requires
$n\gtrsim d$ samples, while distinguishing the two distributions in polynomial
time requires $n \gtrsim d^2$ samples for a wide class of algorithms, i.e.
those covered by the low-degree conjecture. These results suggest the existence
of a wide statistical-to-computational gap in this problem. Numerical
experiments show that neural networks learn to distinguish the two
distributions with quadratic sample complexity, while "lazy" methods like
random features are not better than random guessing in this regime. Our results
show that neural networks extract information from higher-order correlations in
the spiked cumulant model efficiently, and reveal a large gap in the amount of
data required by neural networks and random features to learn from higher-order
cumulants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14923">Fast-NTK: Parameter-Efficient Unlearning for Large-Scale Models. (arXiv:2312.14923v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guihong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1">Hsiang Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun-Fu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1">Radu Marculescu</a></p>
<p>The rapid growth of machine learning has spurred legislative initiatives such
as ``the Right to be Forgotten,'' allowing users to request data removal. In
response, ``machine unlearning'' proposes the selective removal of unwanted
data without the need for retraining from scratch. While the
Neural-Tangent-Kernel-based (NTK-based) unlearning method excels in
performance, it suffers from significant computational complexity, especially
for large-scale models and datasets. Our work introduces ``Fast-NTK,'' a novel
NTK-based unlearning algorithm that significantly reduces the computational
complexity by incorporating parameter-efficient fine-tuning methods, such as
fine-tuning batch normalization layers in a CNN or visual prompts in a vision
transformer. Our experimental results demonstrate scalability to much larger
neural networks and datasets (e.g., 88M parameters; 5k images), surpassing the
limitations of previous full-model NTK-based approaches designed for smaller
cases (e.g., 8M parameters; 500 images). Notably, our approach maintains a
performance comparable to the traditional method of retraining on the retain
set alone. Fast-NTK can thus enable for practical and scalable NTK-based
unlearning in deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14925">A Survey of Reinforcement Learning from Human Feedback. (arXiv:2312.14925v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaufmann_T/0/1/0/all/0/1">Timo Kaufmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_P/0/1/0/all/0/1">Paul Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1">Viktor Bengs</a>, <a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1">Eyke H&#xfc;llermeier</a></p>
<p>Reinforcement learning from human feedback (RLHF) is a variant of
reinforcement learning (RL) that learns from human feedback instead of relying
on an engineered reward function. Building on prior work on the related setting
of preference-based reinforcement learning (PbRL), it stands at the
intersection of artificial intelligence and human-computer interaction. This
positioning offers a promising avenue to enhance the performance and
adaptability of intelligent systems while also improving the alignment of their
objectives with human values. The training of Large Language Models (LLMs) has
impressively demonstrated this potential in recent years, where RLHF played a
decisive role in targeting the model's capabilities toward human objectives.
This article provides a comprehensive overview of the fundamentals of RLHF,
exploring the intricate dynamics between machine agents and human input. While
recent focus has been on RLHF for LLMs, our survey adopts a broader
perspective, examining the diverse applications and wide-ranging impact of the
technique. We delve into the core principles that underpin RLHF, shedding light
on the symbiotic relationship between algorithms and human feedback, and
discuss the main research trends in the field. By synthesizing the current
landscape of RLHF research, this article aims to provide researchers as well as
practitioners with a comprehensive understanding of this rapidly growing field
of research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1812.02207">Better Trees: An empirical study on hyperparameter tuning of classification decision tree induction algorithms. (arXiv:1812.02207v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mantovani_R/0/1/0/all/0/1">Rafael Gomes Mantovani</a>, <a href="http://arxiv.org/find/cs/1/au:+Horvath_T/0/1/0/all/0/1">Tom&#xe1;&#x161; Horv&#xe1;th</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1">Andr&#xe9; L. D. Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cerri_R/0/1/0/all/0/1">Ricardo Cerri</a>, <a href="http://arxiv.org/find/cs/1/au:+Junior_S/0/1/0/all/0/1">Sylvio Barbon Junior</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1">Joaquin Vanschoren</a>, <a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1">Andr&#xe9; Carlos Ponce de Leon Ferreira de Carvalho</a></p>
<p>Machine learning algorithms often contain many hyperparameters (HPs) whose
values affect the predictive performance of the induced models in intricate
ways. Due to the high number of possibilities for these HP configurations and
their complex interactions, it is common to use optimization techniques to find
settings that lead to high predictive performance. However, insights into
efficiently exploring this vast space of configurations and dealing with the
trade-off between predictive and runtime performance remain challenging.
Furthermore, there are cases where the default HPs fit the suitable
configuration. Additionally, for many reasons, including model validation and
attendance to new legislation, there is an increasing interest in interpretable
models, such as those created by the Decision Tree (DT) induction algorithms.
This paper provides a comprehensive approach for investigating the effects of
hyperparameter tuning for the two DT induction algorithms most often used, CART
and C4.5. DT induction algorithms present high predictive performance and
interpretable classification models, though many HPs need to be adjusted.
Experiments were carried out with different tuning strategies to induce models
and to evaluate HPs' relevance using 94 classification datasets from OpenML.
The experimental results point out that different HP profiles for the tuning of
each algorithm provide statistically significant improvements in most of the
datasets for CART, but only in one-third for C4.5. Although different
algorithms may present different tuning scenarios, the tuning techniques
generally required few evaluations to find accurate solutions. Furthermore, the
best technique for all the algorithms was the IRACE. Finally, we found out that
tuning a specific small subset of HPs is a good alternative for achieving
optimal predictive performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.10425">Model-based Clustering with Missing Not At Random Data. (arXiv:2112.10425v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sportisse_A/0/1/0/all/0/1">Aude Sportisse</a> (UCA, MAASAI), <a href="http://arxiv.org/find/stat/1/au:+Marbac_M/0/1/0/all/0/1">Matthieu Marbac</a> (UR, ENSAI, CNRS, CREST), <a href="http://arxiv.org/find/stat/1/au:+Laporte_F/0/1/0/all/0/1">Fabien Laporte</a> (Nantes Univ, CNRS, ITX-lab), <a href="http://arxiv.org/find/stat/1/au:+Celeux_G/0/1/0/all/0/1">Gilles Celeux</a> (CELESTE), <a href="http://arxiv.org/find/stat/1/au:+Boyer_C/0/1/0/all/0/1">Claire Boyer</a> (SU, LPSM (UMR\_8001), MOKAPLAN), <a href="http://arxiv.org/find/stat/1/au:+Josse_J/0/1/0/all/0/1">Julie Josse</a> (IDESP, PREMEDICAL), <a href="http://arxiv.org/find/stat/1/au:+Biernacki_C/0/1/0/all/0/1">Christophe Biernacki</a> (CNRS, MODAL)</p>
<p>Model-based unsupervised learning, as any learning task, stalls as soon as
missing data occurs. This is even more true when the missing data are
informative, or said missing not at random (MNAR). In this paper, we propose
model-based clustering algorithms designed to handle very general types of
missing data, including MNAR data. To do so, we introduce a mixture model for
different types of data (continuous, count, categorical and mixed) to jointly
model the data distribution and the MNAR mechanism, remaining vigilant to the
relative degrees of freedom of each. Several MNAR models are discussed, for
which the cause of the missingness can depend on both the values of the missing
variable themselves and on the class membership. However, we focus on a
specific MNAR model, called MNARz, for which the missingness only depends on
the class membership. We first underline its ease of estimation, by showing
that the statistical inference can be carried out on the data matrix
concatenated with the missing mask considering finally a standard MAR
mechanism. Consequently, we propose to perform clustering using the Expectation
Maximization algorithm, specially developed for this simplified
reinterpretation. Finally, we assess the numerical performances of the proposed
methods on synthetic data and on the real medical registry TraumaBase as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11004">Auto-Encoding Adversarial Imitation Learning. (arXiv:2206.11004v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaifeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a></p>
<p>Reinforcement learning (RL) provides a powerful framework for
decision-making, but its application in practice often requires a carefully
designed reward function. Adversarial Imitation Learning (AIL) sheds light on
automatic policy acquisition without access to the reward signal from the
environment. In this work, we propose Auto-Encoding Adversarial Imitation
Learning (AEAIL), a robust and scalable AIL framework. To induce expert
policies from demonstrations, AEAIL utilizes the reconstruction error of an
auto-encoder as a reward signal, which provides more information for optimizing
policies than the prior discriminator-based ones. Subsequently, we use the
derived objective functions to train the auto-encoder and the agent policy.
Experiments show that our AEAIL performs superior compared to state-of-the-art
methods on both state and image based environments. More importantly, AEAIL
shows much better robustness when the expert demonstrations are noisy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11267">Neural Implicit Manifold Learning for Topology-Aware Density Estimation. (arXiv:2206.11267v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ross_B/0/1/0/all/0/1">Brendan Leigh Ross</a>, <a href="http://arxiv.org/find/stat/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1">Gabriel Loaiza-Ganem</a>, <a href="http://arxiv.org/find/stat/1/au:+Caterini_A/0/1/0/all/0/1">Anthony L. Caterini</a>, <a href="http://arxiv.org/find/stat/1/au:+Cresswell_J/0/1/0/all/0/1">Jesse C. Cresswell</a></p>
<p>Natural data observed in $\mathbb{R}^n$ is often constrained to an
$m$-dimensional manifold $\mathcal{M}$, where $m &lt; n$. This work focuses on the
task of building theoretically principled generative models for such data.
Current generative models learn $\mathcal{M}$ by mapping an $m$-dimensional
latent variable through a neural network $f_\theta: \mathbb{R}^m \to
\mathbb{R}^n$. These procedures, which we call pushforward models, incur a
straightforward limitation: manifolds cannot in general be represented with a
single parameterization, meaning that attempts to do so will incur either
computational instability or the inability to learn probability densities
within the manifold. To remedy this problem, we propose to model $\mathcal{M}$
as a neural implicit manifold: the set of zeros of a neural network. We then
learn the probability density within $\mathcal{M}$ with a constrained
energy-based model, which employs a constrained variant of Langevin dynamics to
train and sample from the learned manifold. In experiments on synthetic and
natural data, we show that our model can learn manifold-supported distributions
with complex topologies more accurately than pushforward models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12459">Meta Objective Guided Disambiguation for Partial Label Learning. (arXiv:2208.12459v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1">Bo-Shi Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Ming-Kun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sheng-Jun Huang</a></p>
<p>Partial label learning (PLL) is a typical weakly supervised learning
framework, where each training instance is associated with a candidate label
set, among which only one label is valid. To solve PLL problems, typically
methods try to perform disambiguation for candidate sets by either using prior
knowledge, such as structure information of training data, or refining model
outputs in a self-training manner. Unfortunately, these methods often fail to
obtain a favorable performance due to the lack of prior information or
unreliable predictions in the early stage of model training. In this paper, we
propose a novel framework for partial label learning with meta objective guided
disambiguation (MoGD), which aims to recover the ground-truth label from
candidate labels set by solving a meta objective on a small validation set.
Specifically, to alleviate the negative impact of false positive labels, we
re-weight each candidate label based on the meta loss on the validation set.
Then, the classifier is trained by minimizing the weighted cross entropy loss.
The proposed method can be easily implemented by using various deep networks
with the ordinary SGD optimizer. Theoretically, we prove the convergence
property of meta objective and derive the estimation error bounds of the
proposed method. Extensive experiments on various benchmark datasets and
real-world PLL datasets demonstrate that the proposed method can achieve
competent performance when compared with the state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.11899">Two Bicomplex and One Multicomplex Least Mean Square algorithms. (arXiv:2209.11899v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alpay_D/0/1/0/all/0/1">Daniel Alpay</a>, <a href="http://arxiv.org/find/cs/1/au:+Diki_K/0/1/0/all/0/1">Kamal Diki</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajiac_M/0/1/0/all/0/1">Mihaela Vajiac</a></p>
<p>We study and introduce new gradient operators in the complex and bicomplex
settings, inspired from the well-known Least Mean Square (LMS) algorithm
invented in 1960 by Widrow and Hoff for Adaptive Linear Neuron (ADALINE).
</p>
<p>These gradient operators will be used to formulate new learning rules for the
Bicomplex Least Mean Square (BLMS) algorithms and we will also formulate these
learning rules will for the case of multicomplex LMS algorithms (MLMS). This
approach extends both the classical real and complex LMS algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.11413">Minimizing low-rank models of high-order tensors: Hardness, span, tight relaxation, and applications. (arXiv:2210.11413v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sidiropoulos_N/0/1/0/all/0/1">Nicholas D. Sidiropoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Karakasis_P/0/1/0/all/0/1">Paris Karakasis</a>, <a href="http://arxiv.org/find/eess/1/au:+Konar_A/0/1/0/all/0/1">Aritra Konar</a></p>
<p>We consider the problem of finding the smallest or largest entry of a tensor
of order N that is specified via its rank decomposition. Stated in a different
way, we are given N sets of R-dimensional vectors and we wish to select one
vector from each set such that the sum of the Hadamard product of the selected
vectors is minimized or maximized. We show that this fundamental tensor problem
is NP-hard for any tensor rank higher than one, and polynomial-time solvable in
the rank-one case. We also propose a continuous relaxation and prove that it is
tight for any rank. For low-enough ranks, the proposed continuous reformulation
is amenable to low-complexity gradient-based optimization, and we propose a
suite of gradient-based optimization algorithms drawing from projected gradient
descent, Frank-Wolfe, or explicit parametrization of the relaxed constraints.
We also show that our core results remain valid no matter what kind of polyadic
tensor model is used to represent the tensor of interest, including Tucker,
HOSVD/MLSVD, tensor train, or tensor ring. Next, we consider the class of
problems that can be posed as special instances of the problem of interest. We
show that this class includes the partition problem (and thus all NP-complete
problems via polynomial-time transformation), integer least squares, integer
linear programming, integer quadratic programming, sign retrieval (a special
kind of mixed integer programming / restricted version of phase retrieval), and
maximum likelihood decoding of parity check codes. We demonstrate promising
experimental results on a number of hard problems, including state-of-art
performance in decoding low density parity check codes and general parity check
codes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16940">FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs. (arXiv:2210.16940v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yujia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1">Ivan Dario Jimenez Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuanyuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a></p>
<p>Forward invariance is a long-studied property in control theory that is used
to certify that a dynamical system stays within some pre-specified set of
states for all time, and also admits robustness guarantees (e.g., the
certificate holds under perturbations). We propose a general framework for
training and provably certifying robust forward invariance in Neural ODEs. We
apply this framework to provide certified safety in robust continuous control.
To our knowledge, this is the first instance of training Neural ODE policies
with such non-vacuous certified guarantees. In addition, we explore the
generality of our framework by using it to certify adversarial robustness for
image classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03131">Explainability as statistical inference. (arXiv:2212.03131v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Senetaire_H/0/1/0/all/0/1">Hugo Henri Joseph Senetaire</a>, <a href="http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1">Damien Garreau</a>, <a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1">Jes Frellsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattei_P/0/1/0/all/0/1">Pierre-Alexandre Mattei</a></p>
<p>A wide variety of model explanation approaches have been proposed in recent
years, all guided by very different rationales and heuristics. In this paper,
we take a new route and cast interpretability as a statistical inference
problem. We propose a general deep probabilistic model designed to produce
interpretable predictions. The model parameters can be learned via maximum
likelihood, and the method can be adapted to any predictor network architecture
and any type of prediction problem. Our method is a case of amortized
interpretability models, where a neural network is used as a selector to allow
for fast interpretation at inference time. Several popular interpretability
methods are shown to be particular cases of regularised maximum likelihood for
our general model. We propose new datasets with ground truth selection which
allow for the evaluation of the features importance map. Using these datasets,
we show experimentally that using multiple imputation provides more reasonable
interpretations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11997">Prompt-Based Editing for Text Style Transfer. (arXiv:2301.11997v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1">Guoqing Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yu Tong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1">Lili Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Firdaus_M/0/1/0/all/0/1">Mauajama Firdaus</a></p>
<p>Prompting approaches have been recently explored in text style transfer,
where a textual prompt is used to query a pretrained language model to generate
style-transferred texts word by word in an autoregressive manner. However, such
a generation process is less controllable and early prediction errors may
affect future word predictions. In this paper, we present a prompt-based
editing approach for text style transfer. Specifically, we prompt a pretrained
language model for style classification and use the classification probability
to compute a style score. Then, we perform discrete search with word-level
editing to maximize a comprehensive scoring function for the style-transfer
task. In this way, we transform a prompt-based generation problem into a
classification one, which is a training-free process and more controllable than
the autoregressive generation of sentences. In our experiments, we performed
both automatic and human evaluation on three style-transfer benchmark datasets,
and show that our approach largely outperforms the state-of-the-art systems
that have 20 times more parameters. Additional empirical analyses further
demonstrate the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00845">Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1">A. Feder Cooper</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wentao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1">Khiem Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1">Tiancheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_C/0/1/0/all/0/1">Charlie F. Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yucheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1">Christopher De Sa</a></p>
<p>Recent research on online Gradient Balancing (GraB) has revealed that there
exist permutation-based example orderings for SGD that are guaranteed to
outperform random reshuffling (RR). Whereas RR arbitrarily permutes training
examples, GraB leverages stale gradients from prior epochs to order examples --
achieving a provably faster convergence rate than RR. However, GraB is limited
by design: while it demonstrates an impressive ability to scale-up training on
centralized data, it does not naturally extend to modern distributed ML
workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which
uses insights from prior work on kernel thinning to translate the benefits of
provably faster permutation-based example ordering to distributed settings.
With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate
over centralized GraB and outperforms distributed RR on a variety of benchmark
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06117">The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment. (arXiv:2302.06117v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1">Jared Fernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1">Jacob Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_C/0/1/0/all/0/1">Clara Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1">Yonatan Bisk</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a></p>
<p>Increased focus on the computational efficiency of NLP systems has motivated
the design of efficient model architectures and improvements to underlying
hardware accelerators. However, the resulting increases in computational
throughput and reductions in floating point operations have not directly
translated to improvements in wall-clock inference latency. We demonstrate that
these discrepancies can be largely attributed to bottlenecks introduced by deep
learning frameworks. We denote this phenomenon as the \textit{framework tax},
and observe that the disparity is growing as hardware speed increases over
time. In this work, we examine this phenomenon through a series of case studies
analyzing the effects of model design decisions, framework paradigms, and
hardware platforms on total model latency. Code is available at
https://github.com/JaredFern/Framework-Tax.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11959">Optimizing Trading Strategies in Quantitative Markets using Multi-Agent Reinforcement Learning. (arXiv:2303.11959v2 [q-fin.TR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_H/0/1/0/all/0/1">Hengxi Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Shi_Z/0/1/0/all/0/1">Zhendong Shi</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Hu_Y/0/1/0/all/0/1">Yuanquan Hu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Ding_W/0/1/0/all/0/1">Wenbo Ding</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Kuruoglu_E/0/1/0/all/0/1">Ercan E. Kuruoglu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_X/0/1/0/all/0/1">Xiao-Ping Zhang</a></p>
<p>Quantitative markets are characterized by swift dynamics and abundant
uncertainties, making the pursuit of profit-driven stock trading actions
inherently challenging. Within this context, reinforcement learning (RL), which
operates on a reward-centric mechanism for optimal control, has surfaced as a
potentially effective solution to the intricate financial decision-making
conundrums presented. This paper delves into the fusion of two established
financial trading strategies, namely the constant proportion portfolio
insurance (CPPI) and the time-invariant portfolio protection (TIPP), with the
multi-agent deep deterministic policy gradient (MADDPG) framework. As a result,
we introduce two novel multi-agent RL (MARL) methods, CPPI-MADDPG and
TIPP-MADDPG, tailored for probing strategic trading within quantitative
markets. To validate these innovations, we implemented them on a diverse
selection of 100 real-market shares. Our empirical findings reveal that the
CPPI-MADDPG and TIPP-MADDPG strategies consistently outpace their traditional
counterparts, affirming their efficacy in the realm of quantitative trading.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00917">Diffusion Bridge Mixture Transports, Schr\&quot;odinger Bridge Problems and Generative Modeling. (arXiv:2304.00917v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Peluchetti_S/0/1/0/all/0/1">Stefano Peluchetti</a></p>
<p>The dynamic Schr\"odinger bridge problem seeks a stochastic process that
defines a transport between two target probability measures, while optimally
satisfying the criteria of being closest, in terms of Kullback-Leibler
divergence, to a reference process. We propose a novel sampling-based iterative
algorithm, the iterated diffusion bridge mixture (IDBM) procedure, aimed at
solving the dynamic Schr\"odinger bridge problem. The IDBM procedure exhibits
the attractive property of realizing a valid transport between the target
probability measures at each iteration. We perform an initial theoretical
investigation of the IDBM procedure, establishing its convergence properties.
The theoretical findings are complemented by numerical experiments illustrating
the competitive performance of the IDBM procedure. Recent advancements in
generative modeling employ the time-reversal of a diffusion process to define a
generative process that approximately transports a simple distribution to the
data distribution. As an alternative, we propose utilizing the first iteration
of the IDBM procedure as an approximation-free method for realizing this
transport. This approach offers greater flexibility in selecting the generative
process dynamics and exhibits accelerated training and superior sample quality
over larger discretization intervals. In terms of implementation, the necessary
modifications are minimally intrusive, being limited to the training loss
definition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02086">Decentralized and Privacy-Preserving Learning of Approximate Stackelberg Solutions in Energy Trading Games with Demand Response Aggregators. (arXiv:2304.02086v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kampezidou_S/0/1/0/all/0/1">Styliani I. Kampezidou</a>, <a href="http://arxiv.org/find/cs/1/au:+Romberg_J/0/1/0/all/0/1">Justin Romberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvoudakis_K/0/1/0/all/0/1">Kyriakos G. Vamvoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mavris_D/0/1/0/all/0/1">Dimitri N. Mavris</a></p>
<p>In this work, a novel Stackelberg game theoretic framework is proposed for
trading energy bidirectionally between the demand-response (DR) aggregator and
the prosumers. This formulation allows for flexible energy arbitrage and
additional monetary rewards while ensuring that the prosumers' desired daily
energy demand is met. Then, a scalable (linear with the number of prosumers),
decentralized, privacy-preserving algorithm is proposed to find approximate
equilibria with online sampling and learning of the prosumers' cumulative best
response, which finds applications beyond this energy game. Moreover, cost
bounds are provided on the quality of the approximate equilibrium solution.
Finally, real data from the California day-ahead market and the UC Davis campus
building energy demands are utilized to demonstrate the efficacy of the
proposed framework and algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents. (arXiv:2304.11241v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1">Pierre Marza</a>, <a href="http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1">Laetitia Matignon</a>, <a href="http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1">Olivier Simonin</a>, <a href="http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1">Dhruv Batra</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1">Christian Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1">Devendra Singh Chaplot</a></p>
<p>Implicit representations such as Neural Radiance Fields (NeRF) have been
shown to be very effective at novel view synthesis. However, these models
typically require manual and careful human data collection for training. In
this paper, we present AutoNeRF, a method to collect data required to train
NeRFs using autonomous embodied agents. Our method allows an agent to explore
an unseen environment efficiently and use the experience to build an implicit
map representation autonomously. We compare the impact of different exploration
strategies including handcrafted frontier-based exploration, end-to-end and
modular approaches composed of trained high-level planners and classical
low-level path followers. We train these models with different reward functions
tailored to this problem and evaluate the quality of the learned
representations on four different downstream tasks: classical viewpoint
rendering, map reconstruction, planning, and pose refinement. Empirical results
show that NeRFs can be trained on actively collected data using just a single
episode of experience in an unseen environment, and can be used for several
downstream robotic tasks, and that modular trained exploration models
outperform other classical and end-to-end baselines. Finally, we show that
AutoNeRF can reconstruct large-scale scenes, and is thus a useful tool to
perform scene-specific adaptation as the produced 3D environment models can be
loaded into a simulator to fine-tune a policy of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01658">FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Dongyue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a></p>
<p>Flight Trajectory Prediction (FTP) is an essential task in Air Traffic
Control (ATC), which can assist air traffic controllers in managing airspace
more safely and efficiently. Existing approaches generally perform
multi-horizon FTP tasks in an autoregressive manner, thereby suffering from
error accumulation and low-efficiency problems. In this paper, a novel
framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight
trajectories directly in a non-autoregressive way, and ii) improve the
limitation of the binary encoding (BE) representation in the FlightBERT.
Specifically, the FlightBERT++ is implemented by a generalized encoder-decoder
architecture, in which the encoder learns the temporal-spatial patterns from
historical observations and the decoder predicts the flight status for the
future horizons. Compared with conventional architecture, an innovative
horizon-aware contexts generator is dedicatedly designed to consider the prior
horizon information, which further enables non-autoregressive multi-horizon
prediction. Moreover, a differential prompted decoder is proposed to enhance
the capability of the differential predictions by leveraging the stationarity
of the differential sequence. The experimental results on a real-world dataset
demonstrated that the FlightBERT++ outperformed the competitive baselines in
both FTP performance and computational efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05400">Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Siedel_G/0/1/0/all/0/1">Georg Siedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1">Weijia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vock_S/0/1/0/all/0/1">Silvia Vock</a>, <a href="http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1">Andrey Morozov</a></p>
<p>Robustness is a fundamental property of machine learning classifiers required
to achieve safety and reliability. In the field of adversarial robustness of
image classifiers, robustness is commonly defined as the stability of a model
to all input changes within a p-norm distance. However, in the field of random
corruption robustness, variations observed in the real world are used, while
p-norm corruptions are rarely considered. This study investigates the use of
random p-norm corruptions to augment the training and test data of image
classifiers. We evaluate the model robustness against imperceptible random
p-norm corruptions and propose a novel robustness metric. We empirically
investigate whether robustness transfers across different p-norms and derive
conclusions on which p-norm corruptions a model should be trained and
evaluated. We find that training data augmentation with a combination of p-norm
corruptions significantly improves corruption robustness, even on top of
state-of-the-art data augmentation schemes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15930">End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maraval_A/0/1/0/all/0/1">Alexandre Maraval</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1">Matthieu Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1">Antoine Grosnit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ammar_H/0/1/0/all/0/1">Haitham Bou Ammar</a></p>
<p>Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of
Bayesian optimisation by leveraging data from related tasks. While previous
methods successfully meta-learn either a surrogate model or an acquisition
function independently, joint training of both components remains an open
challenge. This paper proposes the first end-to-end differentiable meta-BO
framework that generalises neural processes to learn acquisition functions via
transformer architectures. We enable this end-to-end framework with
reinforcement learning (RL) to tackle the lack of labelled acquisition data.
Early on, we notice that training transformer-based neural processes from
scratch with RL is challenging due to insufficient supervision, especially when
rewards are sparse. We formalise this claim with a combinatorial analysis
showing that the widely used notion of regret as a reward signal exhibits a
logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we
augment the RL objective with an auxiliary task that guides part of the
architecture to learn a valid probabilistic model as an inductive bias. We
demonstrate that our method achieves state-of-the-art regret results against
various baselines in experiments on standard hyperparameter optimisation tasks
and also outperforms others in the real-world problems of mixed-integer
programming tuning, antibody design, and logic synthesis for electronic design
automation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03638">Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1">Justin Domke</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrigos_G/0/1/0/all/0/1">Guillaume Garrigos</a>, <a href="http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1">Robert Gower</a></p>
<p>Black-box variational inference is widely used in situations where there is
no proof that its stochastic optimization succeeds. We suggest this is due to a
theoretical gap in existing stochastic optimization proofs: namely the
challenge of gradient estimators with unusual noise bounds, and a composite
non-smooth objective. For dense Gaussian variational families, we observe that
existing gradient estimators based on reparameterization satisfy a quadratic
noise bound and give novel convergence guarantees for proximal and projected
stochastic gradient descent using this bound. This provides rigorous guarantees
that methods similar to those used in practice converge on realistic inference
problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05059">Reconciling Predictive and Statistical Parity: A Causal Approach. (arXiv:2306.05059v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Plecko_D/0/1/0/all/0/1">Drago Plecko</a>, <a href="http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1">Elias Bareinboim</a></p>
<p>Since the rise of fair machine learning as a critical field of inquiry, many
different notions on how to quantify and measure discrimination have been
proposed in the literature. Some of these notions, however, were shown to be
mutually incompatible. Such findings make it appear that numerous different
kinds of fairness exist, thereby making a consensus on the appropriate measure
of fairness harder to reach, hindering the applications of these tools in
practice. In this paper, we investigate one of these key impossibility results
that relates the notions of statistical and predictive parity. Specifically, we
derive a new causal decomposition formula for the fairness measures associated
with predictive parity, and obtain a novel insight into how this criterion is
related to statistical parity through the legal doctrines of disparate
treatment, disparate impact, and the notion of business necessity. Our results
show that through a more careful causal analysis, the notions of statistical
and predictive parity are not really mutually exclusive, but complementary and
spanning a spectrum of fairness notions through the concept of business
necessity. Finally, we demonstrate the importance of our findings on a
real-world example.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05745">Two Independent Teachers are Better Role Model. (arXiv:2306.05745v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Khaled_A/0/1/0/all/0/1">Afifa Khaled</a>, <a href="http://arxiv.org/find/eess/1/au:+Mubarak_A/0/1/0/all/0/1">Ahmed A. Mubarak</a>, <a href="http://arxiv.org/find/eess/1/au:+He_K/0/1/0/all/0/1">Kun He</a></p>
<p>Recent deep learning models have attracted substantial attention in infant
brain analysis. These models have performed state-of-the-art performance, such
as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher).
However, these models depend on an encoder-decoder structure with stacked local
operators to gather long-range information, and the local operators limit the
efficiency and effectiveness. Besides, the $MRI$ data contain different tissue
properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models
is that they use both data as inputs to the segment process, i.e., the models
are trained on the dataset once, and it requires much computational and memory
requirements during inference. In this work, we address the above limitations
by designing a new deep-learning model, called 3D-DenseUNet, which works as
adaptable global aggregation blocks in down-sampling to solve the issue of
spatial information loss. The self-attention module connects the down-sampling
blocks to up-sampling blocks, and integrates the feature maps in three
dimensions of spatial and channel, effectively improving the representation
potential and discriminating ability of the model. Additionally, we propose a
new method called Two Independent Teachers ($2IT$), that summarizes the model
weights instead of label predictions. Each teacher model is trained on
different types of brain data, $T1$ and $T2$, respectively. Then, a fuse model
is added to improve test accuracy and enable training with fewer parameters and
labels compared to the Temporal Ensembling method without modifying the network
architecture. Empirical results demonstrate the effectiveness of the proposed
method. The code is available at
https://github.com/AfifaKhaled/Two-Independent-Teachers-are-Better-Role-Model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06209">Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yinghua Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xueluan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qian Wang</a></p>
<p>Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the
adversary manipulates a small portion of training data such that the victim
model predicts normally on the benign samples but classifies the triggered
samples as the target class. The backdoor attack is an emerging yet threatening
training-phase threat, leading to serious risks in DNN-based applications. In
this paper, we revisit the trigger patterns of existing backdoor attacks. We
reveal that they are either visible or not sparse and therefore are not
stealthy enough. More importantly, it is not feasible to simply combine
existing methods to design an effective sparse and invisible backdoor attack.
To address this problem, we formulate the trigger generation as a bi-level
optimization problem with sparsity and invisibility constraints and propose an
effective method to solve it. The proposed method is dubbed sparse and
invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark
datasets under different settings, which verify the effectiveness of our attack
and its resistance to existing backdoor defenses. The codes for reproducing
main experiments are available at \url{https://github.com/YinghuaGao/SIBA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11706">RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation. (arXiv:2306.11706v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bousmalis_K/0/1/0/all/0/1">Konstantinos Bousmalis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vezzani_G/0/1/0/all/0/1">Giulia Vezzani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1">Dushyant Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Devin_C/0/1/0/all/0/1">Coline Devin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1">Alex X. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1">Maria Bauza</a>, <a href="http://arxiv.org/find/cs/1/au:+Davchev_T/0/1/0/all/0/1">Todor Davchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuxiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Agrim Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1">Akhil Raju</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurens_A/0/1/0/all/0/1">Antoine Laurens</a>, <a href="http://arxiv.org/find/cs/1/au:+Fantacci_C/0/1/0/all/0/1">Claudio Fantacci</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalibard_V/0/1/0/all/0/1">Valentin Dalibard</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambelli_M/0/1/0/all/0/1">Martina Zambelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_M/0/1/0/all/0/1">Murilo Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Pevceviciute_R/0/1/0/all/0/1">Rugile Pevceviciute</a>, <a href="http://arxiv.org/find/cs/1/au:+Blokzijl_M/0/1/0/all/0/1">Michiel Blokzijl</a>, <a href="http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1">Misha Denil</a>, <a href="http://arxiv.org/find/cs/1/au:+Batchelor_N/0/1/0/all/0/1">Nathan Batchelor</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1">Thomas Lampe</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1">Emilio Parisotto</a>, <a href="http://arxiv.org/find/cs/1/au:+Zolna_K/0/1/0/all/0/1">Konrad &#x17b;o&#x142;na</a>, <a href="http://arxiv.org/find/cs/1/au:+Reed_S/0/1/0/all/0/1">Scott Reed</a>, <a href="http://arxiv.org/find/cs/1/au:+Colmenarejo_S/0/1/0/all/0/1">Sergio G&#xf3;mez Colmenarejo</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholz_J/0/1/0/all/0/1">Jon Scholz</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1">Abbas Abdolmaleki</a>, <a href="http://arxiv.org/find/cs/1/au:+Groth_O/0/1/0/all/0/1">Oliver Groth</a>, <a href="http://arxiv.org/find/cs/1/au:+Regli_J/0/1/0/all/0/1">Jean-Baptiste Regli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sushkov_O/0/1/0/all/0/1">Oleg Sushkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rothorl_T/0/1/0/all/0/1">Tom Roth&#xf6;rl</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jos&#xe9; Enrique Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Aytar_Y/0/1/0/all/0/1">Yusuf Aytar</a>, <a href="http://arxiv.org/find/cs/1/au:+Barker_D/0/1/0/all/0/1">Dave Barker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1">Joy Ortiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1">Jost Tobias Springenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1">Raia Hadsell</a>, <a href="http://arxiv.org/find/cs/1/au:+Nori_F/0/1/0/all/0/1">Francesco Nori</a>, <a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1">Nicolas Heess</a></p>
<p>The ability to leverage heterogeneous robotic experience from different
robots and tasks to quickly master novel skills and embodiments has the
potential to transform robot learning. Inspired by recent advances in
foundation models for vision and language, we propose a multi-embodiment,
multi-task generalist agent for robotic manipulation. This agent, named
RoboCat, is a visual goal-conditioned decision transformer capable of consuming
action-labelled visual experience. This data spans a large repertoire of motor
control skills from simulated and real robotic arms with varying sets of
observations and actions. With RoboCat, we demonstrate the ability to
generalise to new tasks and robots, both zero-shot as well as through
adaptation using only 100-1000 examples for the target task. We also show how a
trained model itself can be used to generate data for subsequent training
iterations, thus providing a basic building block for an autonomous improvement
loop. We investigate the agent's capabilities, with large-scale evaluations
both in simulation and on three different real robot embodiments. We find that
as we grow and diversify its training data, RoboCat not only shows signs of
cross-task transfer, but also becomes more efficient at adapting to new tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11758">MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haitong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xinghua Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaowei Li</a></p>
<p>To ensure resilient neural network processing on even unreliable hardware,
comprehensive reliability analysis against various hardware faults is generally
required before the deep neural network models are deployed, and efficient
error injection tools are highly demanded. However, most existing fault
injection tools remain rather limited to basic fault injection to neurons and
fail to provide fine-grained vulnerability analysis capability. In addition,
many of the fault injection tools still need to change the neural network
models and make the fault injection closely coupled with normal neural network
processing, which further complicates the use of the fault injection tools and
slows down the fault simulation. In this work, we propose MRFI, a highly
configurable multi-resolution fault injection tool for deep neural networks. It
enables users to modify an independent fault configuration file rather than
neural network models for the fault injection and vulnerability analysis.
Particularly, it integrates extensive fault analysis functionalities from
different perspectives and enables multi-resolution investigation of the
vulnerability of neural networks. In addition, it does not modify the major
neural network computing framework of PyTorch. Hence, it allows parallel
processing on GPUs naturally and exhibits fast fault simulation according to
our experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15774">Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang &#x27;Anthony&#x27; Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Burke_J/0/1/0/all/0/1">Jeff Burke</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1">Ruofei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1">Matthew K. Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_J/0/1/0/all/0/1">Jennifer Jacobs</a>, <a href="http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1">Philippe Laban</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dingzeyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1">Karl D. D. Willis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chien-Sheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bolei Zhou</a></p>
<p>Through iterative, cross-disciplinary discussions, we define and propose
next-steps for Human-centered Generative AI (HGAI). We contribute a
comprehensive research agenda that lays out future directions of Generative AI
spanning three levels: aligning with human values; assimilating human intents;
and augmenting human abilities. By identifying these next-steps, we intend to
draw interdisciplinary research teams to pursue a coherent set of emergent
ideas in HGAI, focusing on their interested topics while maintaining a coherent
big picture of the future work landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09619">Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning. (arXiv:2307.09619v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1">Zachary Charles</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_N/0/1/0/all/0/1">Nicole Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1">Krishna Pillutla</a>, <a href="http://arxiv.org/find/cs/1/au:+Reneer_M/0/1/0/all/0/1">Michael Reneer</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1">Zachary Garrett</a></p>
<p>We introduce Dataset Grouper, a library to create large-scale
group-structured (e.g., federated) datasets, enabling federated learning
simulation at the scale of foundation models. This library facilitates the
creation of group-structured versions of existing datasets based on
user-specified partitions and directly leads to a variety of useful
heterogeneous datasets that can be plugged into existing software frameworks.
Dataset Grouper offers three key advantages. First, it scales to settings where
even a single group's dataset is too large to fit in memory. Second, it
provides flexibility, both in choosing the base (non-partitioned) dataset and
in defining partitions. Finally, it is framework-agnostic. We empirically
demonstrate that Dataset Grouper enables large-scale federated language
modeling simulations on datasets that are orders of magnitude larger than in
previous work, allowing for federated training of language models with hundreds
of millions, and even billions, of parameters. Our experimental results show
that algorithms like FedAvg operate more as meta-learning methods than as
empirical risk minimization methods at this scale, suggesting their utility in
downstream personalization and task-specific adaptation. Dataset Grouper is
available at https://github.com/google-research/dataset_grouper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16184">UnIVAL: Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1">Mustafa Shukor</a>, <a href="http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1">Corentin Dancette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1">Alexandre Rame</a>, <a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1">Matthieu Cord</a></p>
<p>Large Language Models (LLMs) have made the ambitious quest for generalist
agents significantly far from being a fantasy. A key hurdle for building such
general models is the diversity and heterogeneity of tasks and modalities. A
promising solution is unification, allowing the support of a myriad of tasks
and modalities within one unified framework. While few large models (e.g.,
Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more
than two modalities, current small to mid-scale unified models are still
limited to 2 modalities, usually image-text or video-text. The question that we
ask is: is it possible to build efficiently a unified model that can support
all modalities? To answer this, we propose UnIVAL, a step further towards this
ambitious goal. Without relying on fancy datasets sizes or models with billions
of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities
and unifies text, images, video, and audio into a single model. Our model is
efficiently pretrained on many tasks, based on task balancing and multimodal
curriculum learning. UnIVAL shows competitive performance to existing
state-of-the-art approaches, across image and video-text tasks. The feature
representations learned from image and video-text modalities, allows the model
to achieve competitive performance when finetuned on audio-text tasks, despite
not being pretrained on audio. Thanks to the unified model, we propose a novel
study on multimodal model merging via weight interpolation of models trained on
different multimodal tasks, showing their benefits in particular for
out-of-distribution generalization. Finally, we motivate unification by showing
the synergy between tasks. The model weights and code are released here:
https://github.com/mshukor/UnIVAL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04119">Constructing Custom Thermodynamics Using Deep Learning. (arXiv:2308.04119v3 [cond-mat.soft] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Chen_X/0/1/0/all/0/1">Xiaoli Chen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Soh_B/0/1/0/all/0/1">Beatrice W. Soh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ooi_Z/0/1/0/all/0/1">Zi-En Ooi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Vissol_Gaudin_E/0/1/0/all/0/1">Eleonore Vissol-Gaudin</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yu_H/0/1/0/all/0/1">Haijun Yu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Novoselov_K/0/1/0/all/0/1">Kostya S. Novoselov</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hippalgaonkar_K/0/1/0/all/0/1">Kedar Hippalgaonkar</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_Q/0/1/0/all/0/1">Qianxiao Li</a></p>
<p>One of the most exciting applications of artificial intelligence (AI) is
automated scientific discovery based on previously amassed data, coupled with
restrictions provided by known physical principles, including symmetries and
conservation laws. Such automated hypothesis creation and verification can
assist scientists in studying complex phenomena, where traditional physical
intuition may fail. Here we develop a platform based on a generalized Onsager
principle to learn macroscopic dynamical descriptions of arbitrary stochastic
dissipative systems directly from observations of their microscopic
trajectories. Our method simultaneously constructs reduced thermodynamic
coordinates and interprets the dynamics on these coordinates. We demonstrate
its effectiveness by studying theoretically and validating experimentally the
stretching of long polymer chains in an externally applied field. Specifically,
we learn three interpretable thermodynamic coordinates and build a dynamical
landscape of polymer stretching, including the identification of stable and
transition states and the control of the stretching rate. Our general
methodology can be used to address a wide range of scientific and technological
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09552">Attesting Distributional Properties of Training Data for Machine Learning. (arXiv:2308.09552v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duddu_V/0/1/0/all/0/1">Vasisht Duddu</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Anudeep Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khayata_N/0/1/0/all/0/1">Nora Khayata</a>, <a href="http://arxiv.org/find/cs/1/au:+Yalame_H/0/1/0/all/0/1">Hossein Yalame</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_T/0/1/0/all/0/1">Thomas Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1">N. Asokan</a></p>
<p>The success of machine learning (ML) has been accompanied by increased
concerns about its trustworthiness. Several jurisdictions are preparing ML
regulatory frameworks. One such concern is ensuring that model training data
has desirable distributional properties for certain sensitive attributes. For
example, draft regulations indicate that model trainers are required to show
that training datasets have specific distributional properties, such as
reflecting diversity of the population.
</p>
<p>We propose the notion of property attestation allowing a prover (e.g., model
trainer) to demonstrate relevant distributional properties of training data to
a verifier (e.g., a customer) without revealing the data. We present an
effective hybrid property attestation combining property inference with
cryptographic mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11197">Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Estimating Sample Size and Reducing Overfitting. (arXiv:2308.11197v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1">Hamzeh Ghasemzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hillman_R/0/1/0/all/0/1">Robert E. Hillman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1">Daryush D. Mehta</a></p>
<p>This study's first purpose is to provide quantitative evidence that would
incentivize researchers to instead use the more robust method of nested
cross-validation. The second purpose is to present methods and MATLAB codes for
doing power analysis for ML-based analysis during the design of a study. Monte
Carlo simulations were used to quantify the interactions between the employed
cross-validation method, the discriminative power of features, the
dimensionality of the feature space, and the dimensionality of the model. Four
different cross-validations (single holdout, 10-fold, train-validation-test,
and nested 10-fold) were compared based on the statistical power and
statistical confidence of the ML models. Distributions of the null and
alternative hypotheses were used to determine the minimum required sample size
for obtaining a statistically significant outcome ({\alpha}=0.05,
1-\b{eta}=0.8). Statistical confidence of the model was defined as the
probability of correct features being selected and hence being included in the
final model. Our analysis showed that the model generated based on the single
holdout method had very low statistical power and statistical confidence and
that it significantly overestimated the accuracy. Conversely, the nested
10-fold cross-validation resulted in the highest statistical confidence and the
highest statistical power, while providing an unbiased estimate of the
accuracy. The required sample size with a single holdout could be 50% higher
than what would be needed if nested cross-validation were used. Confidence in
the model based on nested cross-validation was as much as four times higher
than the confidence in the single holdout-based model. A computational model,
MATLAB codes, and lookup tables are provided to assist researchers with
estimating the sample size during the design of their future studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01108">Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v3 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maharana_S/0/1/0/all/0/1">Sarthak Kumar Maharana</a>, <a href="http://arxiv.org/find/eess/1/au:+Adidam_K/0/1/0/all/0/1">Krishna Kamal Adidam</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandi_S/0/1/0/all/0/1">Shoumik Nandi</a>, <a href="http://arxiv.org/find/eess/1/au:+Srivastava_A/0/1/0/all/0/1">Ajitesh Srivastava</a></p>
<p>Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic
to the articulatory space. Signal-processing features like the MFCCs, have been
widely used for the AAI task. For subjects with dysarthric speech, AAI is
challenging because of an imprecise and indistinct pronunciation. In this work,
we perform AAI for dysarthric speech using representations from pre-trained
self-supervised learning (SSL) models. We demonstrate the impact of different
pre-trained features on this challenging AAI task, at low-resource conditions.
In addition, we also condition x-vectors to the extracted SSL features to train
a BLSTM network. In the seen case, we experiment with three AAI training
schemes (subject-specific, pooled, and fine-tuned). The results, consistent
across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves
a relative improvement of the Pearson Correlation Coefficient (CC) by ~1.81%
and ~4.56% for healthy controls and patients, respectively, over MFCCs. We
observe similar average trends for different SSL features in the unseen case.
Overall, SSL networks like wav2vec, APC, and DeCoAR, trained with feature
reconstruction or future timestep prediction tasks, perform well in predicting
dysarthric articulatory trajectories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12204">PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements. (arXiv:2309.12204v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1">Xu Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1">Keck Voon Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haochen Liu</a></p>
<p>We present a neural network for mitigating biased errors in pseudoranges to
improve localization performance with data collected from mobile phones. A
satellite-wise Multilayer Perceptron (MLP) is designed to regress the
pseudorange bias correction from six satellite, receiver, context-related
features derived from Android raw Global Navigation Satellite System (GNSS)
measurements. To train the MLP, we carefully calculate the target values of
pseudorange bias using location ground truth and smoothing techniques and
optimize a loss function involving the estimation residuals of smartphone clock
bias. The corrected pseudoranges are then used by a model-based localization
engine to compute locations. The Google Smartphone Decimeter Challenge (GSDC)
dataset, which contains Android smartphone data collected from both rural and
urban areas, is utilized for evaluation. Both fingerprinting and cross-trace
localization results demonstrate that our proposed method outperforms
model-based and state-of-the-art data-driven approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15639">Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bingcong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1">Georgios B. Giannakis</a></p>
<p>Sharpness-aware minimization (SAM) has well documented merits in enhancing
generalization of deep neural networks, even without sizable data augmentation.
Embracing the geometry of the loss function, where neighborhoods of 'flat
minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing
the maximum loss caused by an adversary perturbing parameters within the
neighborhood. Although critical to account for sharpness of the loss function,
such an 'over-friendly adversary' can curtail the outmost level of
generalization. The novel approach of this contribution fosters stabilization
of adversaries through variance suppression (VaSSO) to avoid such friendliness.
VaSSO's provable stability safeguards its numerical improvement over SAM in
model-agnostic tasks, including image classification and machine translation.
In addition, experiments confirm that VaSSO endows SAM with robustness against
high levels of label noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01438">Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1">Aakash Tripathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Waqas_A/0/1/0/all/0/1">Asim Waqas</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatesan_K/0/1/0/all/0/1">Kavya Venkatesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yilmaz_Y/0/1/0/all/0/1">Yasin Yilmaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1">Ghulam Rasool</a></p>
<p>The advancements in data acquisition, storage, and processing techniques have
resulted in the rapid growth of heterogeneous medical data. Integrating
radiological scans, histopathology images, and molecular information with
clinical data is essential for developing a holistic understanding of the
disease and optimizing treatment. The need for integrating data from multiple
sources is further pronounced in complex diseases such as cancer for enabling
precision medicine and personalized treatments. This work proposes Multimodal
Integration of Oncology Data System (MINDS) - a flexible, scalable, and
cost-effective metadata framework for efficiently fusing disparate data from
public sources such as the Cancer Research Data Commons (CRDC) into an
interconnected, patient-centric framework. MINDS offers an interface for
exploring relationships across data types and building cohorts for developing
large-scale multimodal machine learning models. By harmonizing multimodal data,
MINDS aims to potentially empower researchers with greater analytical ability
to uncover diagnostic and prognostic insights and enable evidence-based
personalized care. MINDS tracks granular end-to-end data provenance, ensuring
reproducibility and transparency. The cloud-native architecture of MINDS can
handle exponential data growth in a secure, cost-optimized manner while
ensuring substantial storage optimization, replication avoidance, and dynamic
access capabilities. Auto-scaling, access controls, and other mechanisms
guarantee pipelines' scalability and security. MINDS overcomes the limitations
of existing biomedical data silos via an interoperable metadata-driven approach
that represents a pivotal step toward the future of oncology data integration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05707">Guiding Language Model Reasoning with Planning Tokens. (arXiv:2310.05707v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1">Lucas Caccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostapenko_O/0/1/0/all/0/1">Oleksiy Ostapenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xingdi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1">Alessandro Sordoni</a></p>
<p>Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
reasoning. However, most of the existing approaches to enhance this ability
rely heavily on data-driven methods, while neglecting the structural aspects of
the model's reasoning capacity. We find that while LLMs can manage individual
reasoning steps well, they struggle with maintaining consistency across an
entire reasoning chain. To solve this, we introduce 'planning tokens' at the
start of each reasoning step, serving as a guide for the model. These token
embeddings are then fine-tuned along with the rest of the model parameters. Our
approach requires a negligible increase in trainable parameters (just 0.001%)
and can be applied through either full fine-tuning or a more
parameter-efficient scheme. We demonstrate our method's effectiveness by
applying it to three different LLMs, showing notable accuracy improvements
across three math word problem datasets w.r.t. plain chain-of-thought
fine-tuning baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09433">Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing. (arXiv:2310.09433v2 [physics.optics] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Castro_B/0/1/0/all/0/1">Bernard J. Giron Castro</a>, <a href="http://arxiv.org/find/physics/1/au:+Peucheret_C/0/1/0/all/0/1">Christophe Peucheret</a>, <a href="http://arxiv.org/find/physics/1/au:+Zibar_D/0/1/0/all/0/1">Darko Zibar</a>, <a href="http://arxiv.org/find/physics/1/au:+Ros_F/0/1/0/all/0/1">Francesco Da Ros</a></p>
<p>Microring resonators (MRRs) are promising devices for time-delay photonic
reservoir computing, but the impact of the different physical effects taking
place in the MRRs on the reservoir computing performance is yet to be fully
understood. We numerically analyze the impact of linear losses as well as
thermo-optic and free-carrier effects relaxation times on the prediction error
of the time-series task NARMA-10. We demonstrate the existence of three
regions, defined by the input power and the frequency detuning between the
optical source and the microring resonance, that reveal the cavity transition
from linear to nonlinear regimes. One of these regions offers very low error in
time-series prediction under relatively low input power and number of nodes
while the other regions either lack nonlinearity or become unstable. This study
provides insight into the design of the MRR and the optimization of its
physical properties for improving the prediction performance of time-delay
reservoir computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13230">Absolute Policy Optimization. (arXiv:2310.13230v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weiye Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feihan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Rui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tianhao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changliu Liu</a></p>
<p>In recent years, trust region on-policy reinforcement learning has achieved
impressive results in addressing complex control tasks and gaming scenarios.
However, contemporary state-of-the-art algorithms within this category
primarily emphasize improvement in expected performance, lacking the ability to
control over the worst-case performance outcomes. To address this limitation,
we introduce a novel objective function; by optimizing which, it will lead to
guaranteed monotonic improvement in the lower bound of near-total performance
samples (absolute performance). Considering this groundbreaking theoretical
advancement, we then refine this theoretically grounded algorithm through a
series of approximations, resulting in a practical solution called Absolute
Policy Optimization (APO). Our experiments demonstrate the effectiveness of our
approach across challenging continuous control benchmark tasks and extend its
applicability to mastering Atari games. Our findings reveal that APO
significantly outperforms state-of-the-art policy gradient algorithms,
resulting in substantial improvements in both expected performance and
worst-case performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19958">PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tianyue Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mengwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Laoutaris_N/0/1/0/all/0/1">Nikolaos Laoutaris</a>, <a href="http://arxiv.org/find/cs/1/au:+Markopoulou_A/0/1/0/all/0/1">Athina Markopoulou</a></p>
<p>Federated learning (FL) is a paradigm that allows several client devices and
a server to collaboratively train a global model, by exchanging only model
updates, without the devices sharing their local training data. These devices
are often constrained in terms of communication and computation resources, and
can further benefit from model pruning -- a paradigm that is widely used to
reduce the size and complexity of models. Intuitively, by making local models
coarser, pruning is expected to also provide some protection against privacy
attacks in the context of FL. However this protection has not been previously
characterized, formally or experimentally, and it is unclear if it is
sufficient against state-of-the-art attacks.
</p>
<p>In this paper, we perform the first investigation of privacy guarantees for
model pruning in FL. We derive information-theoretic upper bounds on the amount
of information leaked by pruned FL models. We complement and validate these
theoretical findings, with comprehensive experiments that involve
state-of-the-art privacy attacks, on several state-of-the-art FL pruning
schemes, using benchmark datasets. This evaluation provides valuable insights
into the choices and parameters that can affect the privacy protection provided
by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware
algorithm for local model pruning, which uses a personalized per-client defense
mask and adapts the defense pruning rate so as to jointly optimize privacy and
model performance. PriPrune is universal in that can be applied after any
pruned FL scheme on the client, without modification, and protects against any
inversion attack by the server. Our empirical evaluation demonstrates that
PriPrune significantly improves the privacy-accuracy tradeoff compared to
state-of-the-art pruned FL schemes that do not take privacy into account.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08655">Review of AlexNet for Medical Image Classification. (arXiv:2311.08655v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1">Wenhao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Junding Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuihua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yudong Zhang</a></p>
<p>In recent years, the rapid development of deep learning has led to a wide
range of applications in the field of medical image classification. The
variants of neural network models with ever-increasing performance share some
commonalities: to try to mitigate overfitting, improve generalization, avoid
gradient vanishing and exploding, etc. AlexNet first utilizes the dropout
technique to mitigate overfitting and the ReLU activation function to avoid
gradient vanishing. Therefore, we focus our discussion on AlexNet, which has
contributed greatly to the development of CNNs in 2012. After reviewing over 40
papers, including journal papers and conference papers, we give a narrative on
the technical details, advantages, and application areas of AlexNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13613">Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning. (arXiv:2311.13613v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jiawei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a></p>
<p>Dataset pruning aims to construct a coreset capable of achieving performance
comparable to the original, full dataset. Most existing dataset pruning methods
rely on snapshot-based criteria to identify representative samples, often
resulting in poor generalization across various pruning and cross-architecture
scenarios. Recent studies have addressed this issue by expanding the scope of
training dynamics considered, including factors such as forgetting event and
probability change, typically using an averaging approach. However, these works
struggle to integrate a broader range of training dynamics without overlooking
well-generalized samples, which may not be sufficiently highlighted in an
averaging manner. In this study, we propose a novel dataset pruning method
termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS
utilizes a dual-depth strategy to achieve a balance between incorporating
extensive training dynamics and identifying representative samples for dataset
pruning. In the first depth, we estimate the series of each sample's individual
contributions spanning the training progress, ensuring comprehensive
integration of training dynamics. In the second depth, we focus on the
variability of the sample-wise contributions identified in the first depth to
highlight well-generalized samples. Extensive experiments conducted on CIFAR
and ImageNet datasets verify the superiority of TDDS over previous SOTA
methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with
only 10% training data, surpassing random selection by 7.83% and other
comparison methods by at least 12.69%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18083">Meta Co-Training: Two Views are Better than One. (arXiv:2311.18083v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rothenberger_J/0/1/0/all/0/1">Jay C. Rothenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Diochnos_D/0/1/0/all/0/1">Dimitrios I. Diochnos</a></p>
<p>In many practical computer vision scenarios unlabeled data is plentiful, but
labels are scarce and difficult to obtain. As a result, semi-supervised
learning which leverages unlabeled data to boost the performance of supervised
classifiers have received significant attention in recent literature. One major
class of semi-supervised algorithms is co-training. In co-training two
different models leverage different independent and sufficient "views" of the
data to jointly make better predictions. During co-training each model creates
pseudo labels on unlabeled points which are used to improve the other model. We
show that in the common case when independent views are not available we can
construct such views inexpensively using pre-trained models. Co-training on the
constructed views yields a performance improvement over any of the individual
views we construct and performance comparable with recent approaches in
semi-supervised learning, but has some undesirable properties. To alleviate the
issues present with co-training we present Meta Co-Training which is an
extension of the successful Meta Pseudo Labels approach to two views. Our
method achieves new state-of-the-art performance on ImageNet-10% with very few
training resources, as well as outperforming prior semi-supervised work on
several other fine-grained image classification datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01479">OpenVoice: Versatile Instant Voice Cloning. (arXiv:2312.01479v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zengyi Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenliang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xumin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xin Sun</a></p>
<p>We introduce OpenVoice, a versatile voice cloning approach that requires only
a short audio clip from the reference speaker to replicate their voice and
generate speech in multiple languages. OpenVoice represents a significant
advancement in addressing the following open challenges in the field: 1)
Flexible Voice Style Control. OpenVoice enables granular control over voice
styles, including emotion, accent, rhythm, pauses, and intonation, in addition
to replicating the tone color of the reference speaker. The voice styles are
not directly copied from and constrained by the style of the reference speaker.
Previous approaches lacked the ability to flexibly manipulate voice styles
after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves
zero-shot cross-lingual voice cloning for languages not included in the
massive-speaker training set. Unlike previous approaches, which typically
require extensive massive-speaker multi-lingual (MSML) dataset for all
languages, OpenVoice can clone voices into a new language without any
massive-speaker training data for that language. OpenVoice is also
computationally efficient, costing tens of times less than commercially
available APIs that offer even inferior performance. To foster further research
in the field, we have made the source code and trained model publicly
accessible. We also provide qualitative results in our demo website. Prior to
its public release, our internal version of OpenVoice was used tens of millions
of times by users worldwide between May and October 2023, serving as the
backend of MyShell.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03824">nbi: the Astronomer&#x27;s Package for Neural Posterior Estimation. (arXiv:2312.03824v2 [astro-ph.IM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Zhang_K/0/1/0/all/0/1">Keming Zhang</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bloom_J/0/1/0/all/0/1">Joshua S. Bloom</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Walt_S/0/1/0/all/0/1">St&#xe9;fan van der Walt</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hernitschek_N/0/1/0/all/0/1">Nina Hernitschek</a></p>
<p>Despite the promise of Neural Posterior Estimation (NPE) methods in
astronomy, the adaptation of NPE into the routine inference workflow has been
slow. We identify three critical issues: the need for custom featurizer
networks tailored to the observed data, the inference inexactness, and the
under-specification of physical forward models. To address the first two
issues, we introduce a new framework and open-source software nbi (Neural
Bayesian Inference), which supports both amortized and sequential NPE. First,
nbi provides built-in "featurizer" networks with demonstrated efficacy on
sequential data, such as light curve and spectra, thus obviating the need for
this customization on the user end. Second, we introduce a modified algorithm
SNPE-IS, which facilities asymptotically exact inference by using the surrogate
posterior under NPE only as a proposal distribution for importance sampling.
These features allow nbi to be applied off-the-shelf to astronomical inference
problems involving light curves and spectra. We discuss how nbi may serve as an
effective alternative to existing methods such as Nested Sampling. Our package
is at https://github.com/kmzzhang/nbi.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06275">DG-TTA: Out-of-domain medical image segmentation through Domain Generalization and Test-Time Adaptation. (arXiv:2312.06275v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weihsbach_C/0/1/0/all/0/1">Christian Weihsbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruse_C/0/1/0/all/0/1">Christian N. Kruse</a>, <a href="http://arxiv.org/find/cs/1/au:+Bigalke_A/0/1/0/all/0/1">Alexander Bigalke</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1">Mattias P. Heinrich</a></p>
<p>Applying pre-trained medical segmentation models on out-of-domain images
often yields predictions of insufficient quality. Several strategies have been
proposed to maintain model performance, such as finetuning or unsupervised- and
source-free domain adaptation. These strategies set restrictive requirements
for data availability. In this study, we propose to combine domain
generalization and test-time adaptation to create a highly effective approach
for reusing pre-trained models in unseen target domains. Domain-generalized
pre-training on source data is used to obtain the best initial performance in
the target domain. We introduce the MIND descriptor previously used in image
registration tasks as a further technique to achieve generalization and present
superior performance for small-scale datasets compared to existing approaches.
At test-time, high-quality segmentation for every single unseen scan is ensured
by optimizing the model weights for consistency given different image
augmentations. That way, our method enables separate use of source and target
data and thus removes current data availability barriers. Moreover, the
presented method is highly modular as it does not require specific model
architectures or prior knowledge of involved domains and labels. We demonstrate
this by integrating it into the nnUNet, which is currently the most popular and
accurate framework for medical image segmentation. We employ multiple datasets
covering abdominal, cardiac, and lumbar spine scans and compose several
out-of-domain scenarios in this study. We demonstrate that our method, combined
with pre-trained whole-body CT models, can effectively segment MR images with
high accuracy in all of the aforementioned scenarios. Open-source code can be
found here: https://github.com/multimodallearning/DG-TTA
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06585">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models. (arXiv:2312.06585v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Avi Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1">John D. Co-Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1">Rishabh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankesh Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Patil_P/0/1/0/all/0/1">Piyush Patil</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1">Xavier Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1">James Harrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Abhishek Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alex Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1">Alex Rizkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1">Ben Adlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1">Gamaleldin Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1">Isabelle Simpson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1">Izzeddin Gur</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1">Jasper Snoek</a>, <a href="http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1">Jeffrey Pennington</a>, <a href="http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1">Jiri Hron</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenealy_K/0/1/0/all/0/1">Kathleen Kenealy</a>, <a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1">Kevin Swersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kshiteej Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1">Laura Culp</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lechao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1">Maxwell L. Bileschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1">Noah Constant</a>, <a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1">Roman Novak</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Warkentin_T/0/1/0/all/0/1">Tris Warkentin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yundi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1">Yamini Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1">Ethan Dyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1">Behnam Neyshabur</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1">Noah Fiedel</a></p>
<p>Fine-tuning language models~(LMs) on human-generated data remains a prevalent
practice. However, the performance of such models is often limited by the
quantity and diversity of high-quality human data. In this paper, we explore
whether we can go beyond human data on tasks where we have access to scalar
feedback, for example, on math problems where one can verify correctness. To do
so, we investigate a simple self-training method based on
expectation-maximization, which we call ReST$^{EM}$, where we (1) generate
samples from the model and filter them using binary feedback, (2) fine-tune the
model on these samples, and (3) repeat this process a few times. Testing on
advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find
that ReST$^{EM}$ scales favorably with model size and significantly surpasses
fine-tuning only on human data. Overall, our findings suggest self-training
with feedback can substantially reduce dependence on human-generated data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06914">Exploring Novel Object Recognition and Spontaneous Location Recognition Machine Learning Analysis Techniques in Alzheimer&#x27;s Mice. (arXiv:2312.06914v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bafana_S/0/1/0/all/0/1">Soham Bafana</a></p>
<p>Understanding object recognition patterns in mice is crucial for advancing
behavioral neuroscience and has significant implications for human health,
particularly in the realm of Alzheimer's research. This study is centered on
the development, application, and evaluation of a state-of-the-art
computational pipeline designed to analyze such behaviors, specifically
focusing on Novel Object Recognition (NOR) and Spontaneous Location Recognition
(SLR) tasks. The pipeline integrates three advanced computational models:
Any-Maze for initial data collection, DeepLabCut for detailed pose estimation,
and Convolutional Neural Networks (CNNs) for nuanced behavioral classification.
Employed across four distinct mouse groups, this pipeline demonstrated high
levels of accuracy and robustness. Despite certain challenges like video
quality limitations and the need for manual calculations, the results affirm
the pipeline's efficacy and potential for scalability. The study serves as a
proof of concept for a multidimensional computational approach to behavioral
neuroscience, emphasizing the pipeline's versatility and readiness for future,
more complex analyses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07661">CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor. (arXiv:2312.07661v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shuyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Runjia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiuye Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyang Li</a></p>
<p>Existing open-vocabulary image segmentation methods require a fine-tuning
step on mask annotations and/or image-text datasets. Mask labels are
labor-intensive, which limits the number of categories in segmentation
datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is
severely reduced after fine-tuning. However, without fine-tuning, VLMs trained
under weak image-text supervision tend to make suboptimal mask predictions when
there are text queries referring to non-existing concepts in the image. To
alleviate these issues, we introduce a novel recurrent framework that
progressively filters out irrelevant texts and enhances mask quality without
training efforts. The recurrent unit is a two-stage segmenter built upon a VLM
with frozen weights. Thus, our model retains the VLM's broad vocabulary space
and strengthens its segmentation capability. Experimental results show that our
method outperforms not only the training-free counterparts, but also those
fine-tuned with millions of additional data samples, and sets new
state-of-the-art records for both zero-shot semantic and referring image
segmentation tasks. Specifically, we improve the current record by 28.8, 16.0,
and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10303">Online Restless Multi-Armed Bandits with Long-Term Fairness Constraints. (arXiv:2312.10303v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shufan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1">Guojun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a></p>
<p>Restless multi-armed bandits (RMAB) have been widely used to model sequential
decision making problems with constraints. The decision maker (DM) aims to
maximize the expected total reward over an infinite horizon under an
"instantaneous activation constraint" that at most B arms can be activated at
any decision epoch, where the state of each arm evolves stochastically
according to a Markov decision process (MDP). However, this basic model fails
to provide any fairness guarantee among arms. In this paper, we introduce
RMAB-F, a new RMAB model with "long-term fairness constraints", where the
objective now is to maximize the long term reward while a minimum long-term
activation fraction for each arm must be satisfied. For the online RMAB-F
setting (i.e., the underlying MDPs associated with each arm are unknown to the
DM), we develop a novel reinforcement learning (RL) algorithm named Fair-UCRL.
We prove that Fair-UCRL ensures probabilistic sublinear bounds on both the
reward regret and the fairness violation regret. Compared with off-the-shelf RL
methods, our Fair-UCRL is much more computationally efficient since it contains
a novel exploitation that leverages a low-complexity index policy for making
decisions. Experimental results further demonstrate the effectiveness of our
Fair-UCRL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10794">A mathematical perspective on Transformers. (arXiv:2312.10794v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geshkovski_B/0/1/0/all/0/1">Borjan Geshkovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Letrouit_C/0/1/0/all/0/1">Cyril Letrouit</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyanskiy_Y/0/1/0/all/0/1">Yury Polyanskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigollet_P/0/1/0/all/0/1">Philippe Rigollet</a></p>
<p>Transformers play a central role in the inner workings of large language
models. We develop a mathematical framework for analyzing Transformers based on
their interpretation as interacting particle systems, which reveals that
clusters emerge in long time. Our study explores the underlying theory and
offers new perspectives for mathematicians as well as computer scientists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13091">MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading. (arXiv:2312.13091v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dib_A/0/1/0/all/0/1">Abdallah Dib</a>, <a href="http://arxiv.org/find/cs/1/au:+Hafemann_L/0/1/0/all/0/1">Luiz Gustavo Hafemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Got_E/0/1/0/all/0/1">Emeline Got</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1">Trevor Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Fadaeinejad_A/0/1/0/all/0/1">Amin Fadaeinejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1">Rafael M. O. Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1">Marc-Andre Carbonneau</a></p>
<p>Reconstructing an avatar from a portrait image has many applications in
multimedia, but remains a challenging research problem. Extracting reflectance
maps and geometry from one image is ill-posed: recovering geometry is a
one-to-many mapping problem and reflectance and light are difficult to
disentangle. Accurate geometry and reflectance can be captured under the
controlled conditions of a light stage, but it is costly to acquire large
datasets in this fashion. Moreover, training solely with this type of data
leads to poor generalization with in-the-wild images. This motivates the
introduction of MoSAR, a method for 3D avatar generation from monocular images.
We propose a semi-supervised training scheme that improves generalization by
learning from both light stage and in-the-wild datasets. This is achieved using
a novel differentiable shading formulation. We show that our approach
effectively disentangles the intrinsic face parameters, producing relightable
avatars. As a result, MoSAR estimates a richer set of skin reflectance maps,
and generates more realistic avatars than existing state-of-the-art methods. We
also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public
dataset providing intrinsic face attributes at scale (diffuse, specular,
ambient occlusion and translucency maps) for a total of 10k subjects. The
project website and the dataset are available on the following link:
https://ubisoft-laforge.github.io/character/mosar/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13970">On Partial Optimal Transport: Revising the Infeasibility of Sinkhorn and Efficient Gradient Methods. (arXiv:2312.13970v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Duc Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tuan Dung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quang Minh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoang H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Toh_K/0/1/0/all/0/1">Kim-Chuan Toh</a></p>
<p>This paper studies the Partial Optimal Transport (POT) problem between two
unbalanced measures with at most $n$ supports and its applications in various
AI tasks such as color transfer or domain adaptation. There is hence the need
for fast approximations of POT with increasingly large problem sizes in arising
applications. We first theoretically and experimentally investigate the
infeasibility of the state-of-the-art Sinkhorn algorithm for POT due to its
incompatible rounding procedure, which consequently degrades its qualitative
performance in real world applications like point-cloud registration. To this
end, we propose a novel rounding algorithm for POT, and then provide a feasible
Sinkhorn procedure with a revised computation complexity of
$\mathcal{\widetilde O}(n^2/\varepsilon^4)$. Our rounding algorithm also
permits the development of two first-order methods to approximate the POT
problem. The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent
(APDAGD), finds an $\varepsilon$-approximate solution to the POT problem in
$\mathcal{\widetilde O}(n^{2.5}/\varepsilon)$, which is better in $\varepsilon$
than revised Sinkhorn. The second method, Dual Extrapolation, achieves the
computation complexity of $\mathcal{\widetilde O}(n^2/\varepsilon)$, thereby
being the best in the literature. We further demonstrate the flexibility of POT
compared to standard OT as well as the practicality of our algorithms on real
applications where two marginal distributions are unbalanced.
</p>
</p>
</div>

    </div>
    </body>
    