<!DOCTYPE html>
<html>
<head>
<title>2023-11-29-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.14675">Fast and Expressive Gesture Recognition using a Combination-Homomorphic Electromyogram Encoder. (arXiv:2311.14675v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smedemark_Margulies_N/0/1/0/all/0/1">Niklas Smedemark-Margulies</a>, <a href="http://arxiv.org/find/cs/1/au:+Bicer_Y/0/1/0/all/0/1">Yunus Bicer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunger_E/0/1/0/all/0/1">Elifnur Sunger</a>, <a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1">Tales Imbiriba</a>, <a href="http://arxiv.org/find/cs/1/au:+Tunik_E/0/1/0/all/0/1">Eugene Tunik</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1">Deniz Erdogmus</a>, <a href="http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1">Mathew Yarossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1">Robin Walters</a></p>
<p>We study the task of gesture recognition from electromyography (EMG), with
the goal of enabling expressive human-computer interaction at high accuracy,
while minimizing the time required for new subjects to provide calibration
data. To fulfill these goals, we define combination gestures consisting of a
direction component and a modifier component. New subjects only demonstrate the
single component gestures and we seek to extrapolate from these to all possible
single or combination gestures. We extrapolate to unseen combination gestures
by combining the feature vectors of real single gestures to produce synthetic
training data. This strategy allows us to provide a large and flexible gesture
vocabulary, while not requiring new subjects to demonstrate combinatorially
many example gestures. We pre-train an encoder and a combination operator using
self-supervision, so that we can produce useful synthetic training data for
unseen test subjects. To evaluate the proposed method, we collect a real-world
EMG dataset, and measure the effect of augmented supervision against two
baselines: a partially-supervised model trained with only single gesture data
from the unseen subject, and a fully-supervised model trained with real single
and real combination gesture data from the unseen subject. We find that the
proposed method provides a dramatic improvement over the partially-supervised
model, and achieves a useful classification accuracy that in some cases
approaches the performance of the fully-supervised model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14677">Filter bubbles and affective polarization in user-personalized large language model outputs. (arXiv:2311.14677v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lazovich_T/0/1/0/all/0/1">Tomo Lazovich</a></p>
<p>Echoing the history of search engines and social media content rankings, the
advent of large language models (LLMs) has led to a push for increased
personalization of model outputs to individual users. In the past, personalized
recommendations and ranking systems have been linked to the development of
filter bubbles (serving content that may confirm a user's existing biases) and
affective polarization (strong negative sentiment towards those with differing
views). In this work, we explore how prompting a leading large language model,
ChatGPT-3.5, with a user's political affiliation prior to asking factual
questions about public figures and organizations leads to differing results. We
observe that left-leaning users tend to receive more positive statements about
left-leaning political figures and media outlets, while right-leaning users see
more positive statements about right-leaning entities. This pattern holds
across presidential candidates, members of the U.S. Senate, and media
organizations with ratings from AllSides. When qualitatively evaluating some of
these outputs, there is evidence that particular facts are included or excluded
based on the user's political affiliation. These results illustrate that
personalizing LLMs based on user demographics carry the same risks of affective
polarization and filter bubbles that have been seen in other personalized
internet technologies. This ``failure mode" should be monitored closely as
there are more attempts to monetize and personalize these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14681">Instance-Specific Asymmetric Sensitivity in Differential Privacy. (arXiv:2311.14681v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Durfee_D/0/1/0/all/0/1">David Durfee</a></p>
<p>We provide a new algorithmic framework for differentially private estimation
of general functions that adapts to the hardness of the underlying dataset. We
build upon previous work that gives a paradigm for selecting an output through
the exponential mechanism based upon closeness of the inverse to the underlying
dataset, termed the inverse sensitivity mechanism. Our framework will slightly
modify the closeness metric and instead give a simple and efficient application
of the sparse vector technique. While the inverse sensitivity mechanism was
shown to be instance optimal, it was only with respect to a class of unbiased
mechanisms such that the most likely outcome matches the underlying data. We
break this assumption in order to more naturally navigate the bias-variance
tradeoff, which will also critically allow for extending our method to
unbounded data. In consideration of this tradeoff, we provide strong intuition
and empirical validation that our technique will be particularly effective when
the distances to the underlying dataset are asymmetric. This asymmetry is
inherent to a range of important problems including fundamental statistics such
as variance, as well as commonly used machine learning performance metrics for
both classification and regression tasks. We efficiently instantiate our method
in $O(n)$ time for these problems and empirically show that our techniques will
give substantially improved differentially private estimations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14685">Comprehensive Assessment of Toxicity in ChatGPT. (arXiv:2311.14685v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Boyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xinyue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_W/0/1/0/all/0/1">Wai Man Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_Z/0/1/0/all/0/1">Zeyang Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Salem_A/0/1/0/all/0/1">Ahmed Salem</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1">Michael Backes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a></p>
<p>Moderating offensive, hateful, and toxic language has always been an
important but challenging topic in the domain of safe use in NLP. The emerging
large language models (LLMs), such as ChatGPT, can potentially further
accentuate this threat. Previous works have discovered that ChatGPT can
generate toxic responses using carefully crafted inputs. However, limited
research has been done to systematically examine when ChatGPT generates toxic
responses. In this paper, we comprehensively evaluate the toxicity in ChatGPT
by utilizing instruction-tuning datasets that closely align with real-world
scenarios. Our results show that ChatGPT's toxicity varies based on different
properties and settings of the prompts, including tasks, domains, length, and
languages. Notably, prompts in creative writing tasks can be 2x more likely
than others to elicit toxic responses. Prompting in German and Portuguese can
also double the response toxicity. Additionally, we discover that certain
deliberately toxic prompts, designed in earlier studies, no longer yield
harmful responses. We hope our discoveries can guide model developers to better
regulate these AI systems and the users to avoid undesirable outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14688">Procedural Fairness Through Decoupling Objectionable Data Generating Components. (arXiv:2311.14688v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zeyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Spirtes_P/0/1/0/all/0/1">Peter Spirtes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a></p>
<p>We reveal and address the frequently overlooked yet important issue of
disguised procedural unfairness, namely, the potentially inadvertent
alterations on the behavior of neutral (i.e., not problematic) aspects of data
generating process, and/or the lack of procedural assurance of the greatest
benefit of the least advantaged individuals. Inspired by John Rawls's advocacy
for pure procedural justice, we view automated decision-making as a microcosm
of social institutions, and consider how the data generating process itself can
satisfy the requirements of procedural fairness. We propose a framework that
decouples the objectionable data generating components from the neutral ones by
utilizing reference points and the associated value instantiation rule. Our
findings highlight the necessity of preventing disguised procedural unfairness,
drawing attention not only to the objectionable data generating components that
we aim to mitigate, but also more importantly, to the neutral components that
we intend to keep unaffected.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14698">Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash. (arXiv:2311.14698v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1">Yixin Tang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yicong/0/1/0/all/0/1">Yicong</a> (Nicole)Lin, <a href="http://arxiv.org/find/stat/1/au:+Sahni_N/0/1/0/all/0/1">Navdeep S. Sahni</a></p>
<p>This paper investigates an approach to both speed up business decision-making
and lower the cost of learning through experimentation by factorizing business
policies and employing fractional factorial experimental designs for their
evaluation. We illustrate how this method integrates with advances in the
estimation of heterogeneous treatment effects, elaborating on its advantages
and foundational assumptions. We empirically demonstrate the implementation and
benefits of our approach and assess its validity in evaluating consumer
promotion policies at DoorDash, which is one of the largest delivery platforms
in the US. Our approach discovers a policy with 5% incremental profit at 67%
lower implementation cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14707">Knowledge Tracing Challenge: Optimal Activity Sequencing for Students. (arXiv:2311.14707v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1">Yann Hicke</a></p>
<p>Knowledge tracing is a method used in education to assess and track the
acquisition of knowledge by individual learners. It involves using a variety of
techniques, such as quizzes, tests, and other forms of assessment, to determine
what a learner knows and does not know about a particular subject. The goal of
knowledge tracing is to identify gaps in understanding and provide targeted
instruction to help learners improve their understanding and retention of
material. This can be particularly useful in situations where learners are
working at their own pace, such as in online learning environments. By
providing regular feedback and adjusting instruction based on individual needs,
knowledge tracing can help learners make more efficient progress and achieve
better outcomes. Effectively solving the KT problem would unlock the potential
of computer-aided education applications such as intelligent tutoring systems,
curriculum learning, and learning materials recommendations. In this paper, we
will present the results of the implementation of two Knowledge Tracing
algorithms on a newly released dataset as part of the AAAI2023 Global Knowledge
Tracing Challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14709">Towards Long-term Annotators: A Supervised Label Aggregation Baseline. (arXiv:2311.14709v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Minmin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Runze Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Renyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shiwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1">Tangjie Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Changjie Fan</a></p>
<p>Relying on crowdsourced workers, data crowdsourcing platforms are able to
efficiently provide vast amounts of labeled data. Due to the variability in the
annotation quality of crowd workers, modern techniques resort to redundant
annotations and subsequent label aggregation to infer true labels. However,
these methods require model updating during the inference, posing challenges in
real-world implementation. Meanwhile, in recent years, many data labeling tasks
have begun to require skilled and experienced annotators, leading to an
increasing demand for long-term annotators. These annotators could leave
substantial historical annotation records on the crowdsourcing platforms, which
can benefit label aggregation, but are ignored by previous works. Hereby, in
this paper, we propose a novel label aggregation technique, which does not need
any model updating during inference and can extensively explore the historical
annotation records. We call it SuperLA, a Supervised Label Aggregation method.
Inside this model, we design three types of input features and a
straightforward neural network structure to merge all the information together
and subsequently produce aggregated labels. Based on comparison experiments
conducted on 22 public datasets and 11 baseline methods, we find that SuperLA
not only outperforms all those baselines in inference performance but also
offers significant advantages in terms of efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14710">Neuroscience inspired scientific machine learning (Part-2): Variable spiking wavelet neural operator. (arXiv:2311.14710v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Shailesh Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Souvik Chakraborty</a></p>
<p>We propose, in this paper, a Variable Spiking Wavelet Neural Operator
(VS-WNO), which aims to bridge the gap between theoretical and practical
implementation of Artificial Intelligence (AI) algorithms for mechanics
applications. With recent developments like the introduction of neural
operators, AI's potential for being used in mechanics applications has
increased significantly. However, AI's immense energy and resource requirements
are a hurdle in its practical field use case. The proposed VS-WNO is based on
the principles of spiking neural networks, which have shown promise in reducing
the energy requirements of the neural networks. This makes possible the use of
such algorithms in edge computing. The proposed VS-WNO utilizes variable
spiking neurons, which promote sparse communication, thus conserving energy,
and its use is further supported by its ability to tackle regression tasks,
often faced in the field of mechanics. Various examples dealing with partial
differential equations, like Burger's equation, Allen Cahn's equation, and
Darcy's equation, have been shown. Comparisons have been shown against wavelet
neural operator utilizing leaky integrate and fire neurons (direct and encoded
inputs) and vanilla wavelet neural operator utilizing artificial neurons. The
results produced illustrate the ability of the proposed VS-WNO to converge to
ground truth while promoting sparse communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14722">Zero-Shot Question Answering over Financial Documents using Large Language Models. (arXiv:2311.14722v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phogat_K/0/1/0/all/0/1">Karmvir Singh Phogat</a>, <a href="http://arxiv.org/find/cs/1/au:+Harsha_C/0/1/0/all/0/1">Chetan Harsha</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasaratha_S/0/1/0/all/0/1">Sridhar Dasaratha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishna_S/0/1/0/all/0/1">Shashishekar Ramakrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Puranam_S/0/1/0/all/0/1">Sai Akhil Puranam</a></p>
<p>We introduce a large language model (LLM) based approach to answer complex
questions requiring multi-hop numerical reasoning over financial reports. While
LLMs have exhibited remarkable performance on various natural language and
reasoning tasks, complex reasoning problems often rely on few-shot prompts that
require carefully crafted examples. In contrast, our approach uses novel
zero-shot prompts that guide the LLM to encode the required reasoning into a
Python program or a domain specific language. The generated program is then
executed by a program interpreter, thus mitigating the limitations of LLM in
performing accurate arithmetic calculations.
</p>
<p>We evaluate the proposed approach on three financial datasets using some of
the recently developed generative pretrained transformer (GPT) models and
perform comparisons with various zero-shot baselines. The experimental results
demonstrate that our approach significantly improves the accuracy for all the
LLMs over their respective baselines. We provide a detailed analysis of the
results, generating insights to support our findings. The success of our
approach demonstrates the enormous potential to extract complex domain specific
numerical reasoning by designing zero-shot prompts to effectively exploit the
knowledge embedded in LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14725">Unsupervised learning of site percolation based on shuffled configurations. (arXiv:2311.14725v1 [cond-mat.stat-mech])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Xu_D/0/1/0/all/0/1">Dian Xu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_S/0/1/0/all/0/1">Shanshan Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Shen_J/0/1/0/all/0/1">Jianmin Shen</a></p>
<p>In the field of statistical physics, machine learning has gained significant
popularity and has achieved remarkable results in recent studies on phase
transitions.In this paper, we apply Principal Component Analysis (PCA) and
Autoencoder(AE) based on Unsupervised learning to study the various
configurations of the percolation model in equilibrium phase transition. In
certain phase transition models, such as the DP model in non-equilibrium phase
transitions, the order parameter is particle density. However, in some other
phase transition models, such as the percolation model, it is not. This study
involved randomizing and selecting percolation graphs to be used as input for a
neural network, and analyzed the obtained results, indicating that the outputs
of the single latent variable of AE and the first principal component of PCA
are signals related to particle density.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14727">Optimal Strategies to Perform Multilingual Analysis of Social Content for a Novel Dataset in the Tourism Domain. (arXiv:2311.14727v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masson_M/0/1/0/all/0/1">Maxime Masson</a>, <a href="http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1">Rodrigo Agerri</a>, <a href="http://arxiv.org/find/cs/1/au:+Sallaberry_C/0/1/0/all/0/1">Christian Sallaberry</a>, <a href="http://arxiv.org/find/cs/1/au:+Bessagnet_M/0/1/0/all/0/1">Marie-Noelle Bessagnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacayrelle_A/0/1/0/all/0/1">Annig Le Parc Lacayrelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Roose_P/0/1/0/all/0/1">Philippe Roose</a></p>
<p>The rising influence of social media platforms in various domains, including
tourism, has highlighted the growing need for efficient and automated natural
language processing (NLP) approaches to take advantage of this valuable
resource. However, the transformation of multilingual, unstructured, and
informal texts into structured knowledge often poses significant challenges.
</p>
<p>In this work, we evaluate and compare few-shot, pattern-exploiting and
fine-tuning machine learning techniques on large multilingual language models
(LLMs) to establish the best strategy to address the lack of annotated data for
3 common NLP tasks in the tourism domain: (1) Sentiment Analysis, (2) Named
Entity Recognition, and (3) Fine-grained Thematic Concept Extraction (linked to
a semantic resource). Furthermore, we aim to ascertain the quantity of
annotated examples required to achieve good performance in those 3 tasks,
addressing a common challenge encountered by NLP researchers in the
construction of domain-specific datasets.
</p>
<p>Extensive experimentation on a newly collected and annotated multilingual
(French, English, and Spanish) dataset composed of tourism-related tweets shows
that current few-shot learning techniques allow us to obtain competitive
results for all three tasks with very little annotation data: 5 tweets per
label (15 in total) for Sentiment Analysis, 10% of the tweets for location
detection (around 160) and 13% (200 approx.) of the tweets annotated with
thematic concepts, a highly fine-grained sequence labeling task based on an
inventory of 315 classes.
</p>
<p>This comparative analysis, grounded in a novel dataset, paves the way for
applying NLP to new domain-specific applications, reducing the need for manual
annotations and circumventing the complexities of rule-based, ad hoc solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14729">App for Resume-Based Job Matching with Speech Interviews and Grammar Analysis: A Review. (arXiv:2311.14729v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1">Tanmay Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardeshi_Y/0/1/0/all/0/1">Yuvraj Pardeshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_Y/0/1/0/all/0/1">Yash Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakat_V/0/1/0/all/0/1">Vaishnvi Sakat</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhirud_S/0/1/0/all/0/1">Sapana Bhirud</a></p>
<p>Through the advancement in natural language processing (NLP), specifically in
speech recognition, fully automated complex systems functioning on voice input
have started proliferating in areas such as home automation. These systems have
been termed Automatic Speech Recognition Systems (ASR). In this review paper,
we explore the feasibility of an end-to-end system providing speech and text
based natural language processing for job interview preparation as well as
recommendation of relevant job postings. We also explore existing
recommender-based systems and note their limitations. This literature review
would help us identify the approaches and limitations of the various similar
use-cases of NLP technology for our upcoming project.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14730">MemoryCompanion: A Smart Healthcare Solution to Empower Efficient Alzheimer&#x27;s Care Via Unleashing Generative AI. (arXiv:2311.14730v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lifei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1">Yeonie Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yi Fang</a></p>
<p>With the rise of Large Language Models (LLMs), notably characterized by GPT
frameworks, there emerges a catalyst for novel healthcare applications. Earlier
iterations of chatbot caregivers, though existent, have yet to achieve a
dimension of human-like authenticity. This paper unveils `MemoryCompanion' a
pioneering digital health solution explicitly tailored for Alzheimer's disease
(AD) patients and their caregivers. Drawing upon the nuances of GPT technology
and prompt engineering, MemoryCompanion manifests a personalized caregiving
paradigm, fostering interactions via voice-cloning and talking-face mechanisms
that resonate with the familiarity of known companions. Using advanced
prompt-engineering, the system intricately adapts to each patient's distinct
profile, curating its content and communication style accordingly. This
approach strives to counteract prevalent issues of social isolation and
loneliness frequently observed in AD demographics. Our methodology, grounded in
its innovative design, addresses both the caregiving and technological
challenges intrinsic to this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14731">Deep State-Space Model for Predicting Cryptocurrency Price. (arXiv:2311.14731v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Sharma_S/0/1/0/all/0/1">Shalini Sharma</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Majumdar_A/0/1/0/all/0/1">Angshul Majumdar</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chouzenoux_E/0/1/0/all/0/1">Emilie Chouzenoux</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Elvira_V/0/1/0/all/0/1">Victor Elvira</a></p>
<p>Our work presents two fundamental contributions. On the application side, we
tackle the challenging problem of predicting day-ahead crypto-currency prices.
On the methodological side, a new dynamical modeling approach is proposed. Our
approach keeps the probabilistic formulation of the state-space model, which
provides uncertainty quantification on the estimates, and the function
approximation ability of deep neural networks. We call the proposed approach
the deep state-space model. The experiments are carried out on established
cryptocurrencies (obtained from Yahoo Finance). The goal of the work has been
to predict the price for the next day. Benchmarking has been done with both
state-of-the-art and classical dynamical modeling techniques. Results show that
the proposed approach yields the best overall results in terms of accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14733">Thinking Outside the Box: Orthogonal Approach to Equalizing Protected Attributes. (arXiv:2311.14733v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiahui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiaohao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1">Mahesan Niranjan</a></p>
<p>There is growing concern that the potential of black box AI may exacerbate
health-related disparities and biases such as gender and ethnicity in clinical
decision-making. Biased decisions can arise from data availability and
collection processes, as well as from the underlying confounding effects of the
protected attributes themselves. This work proposes a machine learning-based
orthogonal approach aiming to analyze and suppress the effect of the confounder
through discriminant dimensionality reduction and orthogonalization of the
protected attributes against the primary attribute information. By doing so,
the impact of the protected attributes on disease diagnosis can be realized,
undesirable feature correlations can be mitigated, and the model prediction
performance can be enhanced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14735">Generative Machine Learning for Multivariate Equity Returns. (arXiv:2311.14735v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Tepelyan_R/0/1/0/all/0/1">Ruslan Tepelyan</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Gopal_A/0/1/0/all/0/1">Achintya Gopal</a></p>
<p>The use of machine learning to generate synthetic data has grown in
popularity with the proliferation of text-to-image models and especially large
language models. The core methodology these models use is to learn the
distribution of the underlying data, similar to the classical methods common in
finance of fitting statistical models to data. In this work, we explore the
efficacy of using modern machine learning methods, specifically conditional
importance weighted autoencoders (a variant of variational autoencoders) and
conditional normalizing flows, for the task of modeling the returns of
equities. The main problem we work to address is modeling the joint
distribution of all the members of the S&amp;P 500, or, in other words, learning a
500-dimensional joint distribution. We show that this generative model has a
broad range of applications in finance, including generating realistic
synthetic data, volatility and correlation estimation, risk analysis (e.g.,
value at risk, or VaR, of portfolios), and portfolio optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14736">Data Diversity Matters for Robust Instruction Tuning. (arXiv:2311.14736v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bukharin_A/0/1/0/all/0/1">Alexander Bukharin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a></p>
<p>Instruction tuning has emerged as a key step in aligning large language
models. One of the central challenges of instruction tuning is dataset
selection, as the composition of the instruction tuning dataset can
significantly impact downstream performance. In particular, researchers have
hypothesized that dataset diversity and dataset quality are important
indicators of downstream performance. However, it is not clear how to
automatically select high quality and diverse data or how exactly quality and
diversity affect instruction following ability. To resolve these issues, we
propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT
provides a principled algorithm to control dataset diversity and quality,
allowing us to conduct an in depth study on the effect of diversity and quality
on instruction tuning performance. From this study we draw two key insights (1)
there is a natural tradeoff between dataset diversity and quality and (2)
increasing dataset diversity significantly improves the worst case instruction
following performance, therefore improving robustness. We validate the
performance of QDIT on several large scale instruction tuning datasets, where
we find it can improve worst case performance by 18% while maintaining or
improving average performance compared to quality driven baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14737">Positional Description Matters for Transformers Arithmetic. (arXiv:2311.14737v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1">Ruoqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1">S&#xe9;bastien Bubeck</a>, <a href="http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1">Ronen Eldan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yin Tat Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a></p>
<p>Transformers, central to the successes in modern Natural Language Processing,
often falter on arithmetic tasks despite their vast capabilities --which
paradoxically include remarkable coding abilities. We observe that a crucial
challenge is their naive reliance on positional information to solve arithmetic
problems with a small number of digits, leading to poor performance on larger
numbers. Herein, we delve deeper into the role of positional encoding, and
propose several ways to fix the issue, either by modifying the positional
encoding directly, or by modifying the representation of the arithmetic task to
leverage standard positional encoding differently. We investigate the value of
these modifications for three tasks: (i) classical multiplication, (ii) length
extrapolation in addition, and (iii) addition in natural language context. For
(i) we train a small model on a small dataset (100M parameters and 300k
samples) with remarkable aptitude in (direct, no scratchpad) 15 digits
multiplication and essentially perfect up to 12 digits, while usual training in
this context would give a model failing at 4 digits multiplication. In the
experiments on addition, we use a mere 120k samples to demonstrate: for (ii)
extrapolation from 10 digits to testing on 12 digits numbers while usual
training would have no extrapolation, and for (iii) almost perfect accuracy up
to 5 digits while usual training would be correct only up to 3 digits (which is
essentially memorization with a training set of 120k samples).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align an LLM. These reward models are additionally used
at inference-time to estimate how well LLM responses adhere to those desired
behaviors. However, there is little work measuring how robust these reward
models are to distribution shifts. In this work, we evaluate how reward model
performance - measured via accuracy and calibration (i.e. alignment between
accuracy and confidence) - is affected by distribution shift. We show novel
calibration patterns and accuracy drops due to OOD prompts and responses, and
that the reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting in order to detect these
distribution shifts in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14744">Coarse-Grained Configurational Polymer Fingerprints for Property Prediction using Machine Learning. (arXiv:2311.14744v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kumar_I/0/1/0/all/0/1">Ishan Kumar</a>, <a href="http://arxiv.org/find/physics/1/au:+Jha_P/0/1/0/all/0/1">Prateek K Jha</a></p>
<p>In this work, we present a method to generate a configurational level
fingerprint for polymers using the Bead-Spring-Model. Unlike some of the
previous fingerprinting approaches that employ monomer-level information where
atomistic descriptors are computed using quantum chemistry calculations, this
approach incorporates configurational information from a coarse-grained model
of a long polymer chain. The proposed approach may be advantageous for the
study of behavior resulting from large molecular weights. To create this
fingerprint, we make use of two kinds of descriptors. First, we calculate
certain geometric descriptors like Re2, Rg2 etc. and label them as Calculated
Descriptors. Second, we generate a set of data-driven descriptors using an
unsupervised autoencoder model and call them Learnt Descriptors. Using a
combination of both of them, we are able to learn mappings from the structure
to various properties of the polymer chain by training ML models. We test our
fingerprint to predict the probability of occurrence of a configuration at
equilibrium, which is approximated by a simple linear relationship between the
instantaneous internal energy and equilibrium average internal energy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14754">ExCeL : Combined Extreme and Collective Logit Information for Enhancing Out-of-Distribution Detection. (arXiv:2311.14754v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karunanayake_N/0/1/0/all/0/1">Naveen Karunanayake</a>, <a href="http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1">Suranga Seneviratne</a>, <a href="http://arxiv.org/find/cs/1/au:+Chawla_S/0/1/0/all/0/1">Sanjay Chawla</a></p>
<p>Deep learning models often exhibit overconfidence in predicting
out-of-distribution (OOD) data, underscoring the crucial role of OOD detection
in ensuring reliability in predictions. Among various OOD detection approaches,
post-hoc detectors have gained significant popularity, primarily due to their
ease of use and implementation. However, the effectiveness of most post-hoc OOD
detectors has been constrained as they rely solely either on extreme
information, such as the maximum logit, or on the collective information (i.e.,
information spanned across classes or training samples) embedded within the
output layer. In this paper, we propose ExCeL that combines both extreme and
collective information within the output layer for enhanced accuracy in OOD
detection. We leverage the logit of the top predicted class as the extreme
information (i.e., the maximum logit), while the collective information is
derived in a novel approach that involves assessing the likelihood of other
classes appearing in subsequent ranks across various training samples. Our idea
is motivated by the observation that, for in-distribution (ID) data, the
ranking of classes beyond the predicted class is more deterministic compared to
that in OOD data. Experiments conducted on CIFAR100 and ImageNet-200 datasets
demonstrate that ExCeL consistently is among the five top-performing methods
out of twenty-one existing post-hoc baselines when the joint performance on
near-OOD and far-OOD is considered (i.e., in terms of AUROC and FPR95).
Furthermore, ExCeL shows the best overall performance across both datasets,
unlike other baselines that work best on one dataset but has a performance drop
in the other.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14756">Task-Distributionally Robust Data-Free Meta-Learning. (arXiv:2311.14756v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zixuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yongxian Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Baoyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Data-Free Meta-Learning (DFML) aims to efficiently learn new tasks by
leveraging multiple pre-trained models without requiring their original
training data. Existing inversion-based DFML methods construct pseudo tasks
from a learnable dataset, which is inversely generated from the pre-trained
model pool. For the first time, we reveal two major challenges hindering their
practical deployments: Task-Distribution Shift (TDS) and Task-Distribution
Corruption (TDC). TDS leads to a biased meta-learner because of the skewed task
distribution towards newly generated tasks. TDC occurs when untrusted models
characterized by misleading labels or poor quality pollute the task
distribution. To tackle these issues, we introduce a robust DFML framework that
ensures task distributional robustness. We propose to meta-learn from a pseudo
task distribution, diversified through task interpolation within a compact
task-memory buffer. This approach reduces the meta-learner's overreliance on
newly generated tasks by maintaining consistent performance across a broader
range of interpolated memory tasks, thus ensuring its generalization for unseen
tasks. Additionally, our framework seamlessly incorporates an automated model
selection mechanism into the meta-training phase, parameterizing each model's
reliability as a learnable weight. This is optimized with a policy gradient
algorithm inspired by reinforcement learning, effectively addressing the
non-differentiable challenge posed by model selection. Comprehensive
experiments across various datasets demonstrate the framework's effectiveness
in mitigating TDS and TDC, underscoring its potential to improve DFML in
real-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14759">Forecasting Cryptocurrency Prices Using Deep Learning: Integrating Financial, Blockchain, and Text Data. (arXiv:2311.14759v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Gurgul_V/0/1/0/all/0/1">Vincent Gurgul</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Lessmann_S/0/1/0/all/0/1">Stefan Lessmann</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Hardle_W/0/1/0/all/0/1">Wolfgang Karl H&#xe4;rdle</a></p>
<p>This paper explores the application of Machine Learning (ML) and Natural
Language Processing (NLP) techniques in cryptocurrency price forecasting,
specifically Bitcoin (BTC) and Ethereum (ETH). Focusing on news and social
media data, primarily from Twitter and Reddit, we analyse the influence of
public sentiment on cryptocurrency valuations using advanced deep learning NLP
methods. Alongside conventional price regression, we treat cryptocurrency price
forecasting as a classification problem. This includes both the prediction of
price movements (up or down) and the identification of local extrema. We
compare the performance of various ML models, both with and without NLP data
integration. Our findings reveal that incorporating NLP data significantly
enhances the forecasting performance of our models. We discover that
pre-trained models, such as Twitter-RoBERTa and BART MNLI, are highly effective
in capturing market sentiment, and that fine-tuning Large Language Models
(LLMs) also yields substantial forecasting improvements. Notably, the BART MNLI
zero-shot classification model shows considerable proficiency in extracting
bullish and bearish signals from textual data. All of our models consistently
generate profit across different validation scenarios, with no observed decline
in profits or reduction in the impact of NLP data over time. The study
highlights the potential of text analysis in improving financial forecasts and
demonstrates the effectiveness of various NLP techniques in capturing nuanced
market sentiment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14766">Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing. (arXiv:2311.14766v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Feiyang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yimin Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhaofeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yanxing Qi</a></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has played a crucial role
in the success of large models such as ChatGPT. RLHF is a reinforcement
learning framework which combines human feedback to improve learning
effectiveness and performance. However, obtaining preferences feedback manually
is quite expensive in commercial applications. Some statistical commercial
indicators are usually more valuable and always ignored in RLHF. There exists a
gap between commercial target and model training. In our research, we will
attempt to fill this gap with statistical business feedback instead of human
feedback, using AB testing which is a well-established statistical method.
Reinforcement Learning from Statistical Feedback (RLSF) based on AB testing is
proposed. Statistical inference methods are used to obtain preferences for
training the reward network, which fine-tunes the pre-trained model in
reinforcement learning framework, achieving greater business value.
Furthermore, we extend AB testing with double selections at a single time-point
to ANT testing with multiple selections at different feedback time points.
Moreover, we design numerical experiences to validate the effectiveness of our
algorithm framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14767">Low-Cost HEM with Arduino and Zigbee Technologies in the Energy Sector in Colombia. (arXiv:2311.14767v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maury_Z/0/1/0/all/0/1">Zurisaddai de la Cruz Severiche Maury</a>, <a href="http://arxiv.org/find/eess/1/au:+Vilas_A/0/1/0/all/0/1">Ana Fernandez Vilas</a>, <a href="http://arxiv.org/find/eess/1/au:+Redondo_R/0/1/0/all/0/1">Rebeca Diaz Redondo</a></p>
<p>Since no solutions have been proposed in Colombia that seek to reduce the
consumption of electricity at the residential level, this paper describes the
design and implementation of a simple prototype of a low-cost home energy
management system (HEMS). The objective of this plat-form is to monitor the
energy consumption of typical household devices so that users can access the
consumption of each device separately and then establish the strategy that
allows them to reduce energy consumption at home. In order to demonstrate that
our system is viable, the system has been evaluated by measuring weekly energy
consumption with the on-line and off-line HEMS using a test bench with typical
household devices in a Sincelejo typical household. The evaluation has shown
that with the installation of this HEMS, consumption is reduced by 27%. This
shows that it is possible to achieve a good reduction percentage with a
low-cost system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14770">Learning to Cooperate and Communicate Over Imperfect Channels. (arXiv:2311.14770v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weil_J/0/1/0/all/0/1">Jannis Weil</a>, <a href="http://arxiv.org/find/cs/1/au:+Ekinci_G/0/1/0/all/0/1">Gizem Ekinci</a>, <a href="http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1">Heinz Koeppl</a>, <a href="http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1">Tobias Meuser</a></p>
<p>Information exchange in multi-agent systems improves the cooperation among
agents, especially in partially observable settings. In the real world,
communication is often carried out over imperfect channels. This requires
agents to handle uncertainty due to potential information loss. In this paper,
we consider a cooperative multi-agent system where the agents act and exchange
information in a decentralized manner using a limited and unreliable channel.
To cope with such channel constraints, we propose a novel communication
approach based on independent Q-learning. Our method allows agents to
dynamically adapt how much information to share by sending messages of
different sizes, depending on their local observations and the channel's
properties. In addition to this message size selection, agents learn to encode
and decode messages to improve their jointly trained policies. We show that our
approach outperforms approaches without adaptive capabilities in a novel
cooperative digit-prediction environment and discuss its limitations in the
traffic junction environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14772">Trainwreck: A damaging adversarial attack on image classifiers. (arXiv:2311.14772v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zahalka_J/0/1/0/all/0/1">Jan Zah&#xe1;lka</a></p>
<p>Adversarial attacks are an important security concern for computer vision
(CV), as they enable malicious attackers to reliably manipulate CV models.
Existing attacks aim to elicit an output desired by the attacker, but keep the
model fully intact on clean data. With CV models becoming increasingly valuable
assets in applied practice, a new attack vector is emerging: disrupting the
models as a form of economic sabotage. This paper opens up the exploration of
damaging adversarial attacks (DAAs) that seek to damage the target model and
maximize the total cost incurred by the damage. As a pioneer DAA, this paper
proposes Trainwreck, a train-time attack that poisons the training data of
image classifiers to degrade their performance. Trainwreck conflates the data
of similar classes using stealthy ($\epsilon \leq 8/255$) class-pair universal
perturbations computed using a surrogate model. Trainwreck is a black-box,
transferable attack: it requires no knowledge of the target model's
architecture, and a single poisoned dataset degrades the performance of any
model trained on it. The experimental evaluation on CIFAR-10 and CIFAR-100
demonstrates that Trainwreck is indeed an effective attack across various model
architectures including EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16.
The strength of the attack can be customized by the poison rate parameter.
Finally, data redundancy with file hashing and/or pixel difference are
identified as a reliable defense technique against Trainwreck or similar DAAs.
The code is available at https://github.com/JanZahalka/trainwreck.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14773">Set Features for Anomaly Detection. (arXiv:2311.14773v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1">Niv Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzachor_I/0/1/0/all/0/1">Issar Tzachor</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1">Yedid Hoshen</a></p>
<p>This paper proposes set features for detecting anomalies in samples that
consist of unusual combinations of normal elements. Many leading methods
discover anomalies by detecting an unusual part of a sample. For example,
state-of-the-art segmentation-based approaches, first classify each element of
the sample (e.g., image patch) as normal or anomalous and then classify the
entire sample as anomalous if it contains anomalous elements. However, such
approaches do not extend well to scenarios where the anomalies are expressed by
an unusual combination of normal elements. In this paper, we overcome this
limitation by proposing set features that model each sample by the distribution
of its elements. We compute the anomaly score of each sample using a simple
density estimation method, using fixed features. Our approach outperforms the
previous state-of-the-art in image-level logical anomaly detection and
sequence-level time series anomaly detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14782">One Fits All: Universal Time Series Analysis by Pretrained LM and Specially Designed Adaptors. (arXiv:2311.14782v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_P/0/1/0/all/0/1">Peisong Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Liang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Rong Jin</a></p>
<p>Despite the impressive achievements of pre-trained models in the fields of
natural language processing (NLP) and computer vision (CV), progress in the
domain of time series analysis has been limited. In contrast to NLP and CV,
where a single model can handle various tasks, time series analysis still
relies heavily on task-specific methods for activities such as classification,
anomaly detection, forecasting, and few-shot learning. The primary obstacle to
developing a pre-trained model for time series analysis is the scarcity of
sufficient training data. In our research, we overcome this obstacle by
utilizing pre-trained models from language or CV, which have been trained on
billions of data points, and apply them to time series analysis. We assess the
effectiveness of the pre-trained transformer model in two ways. Initially, we
maintain the original structure of the self-attention and feedforward layers in
the residual blocks of the pre-trained language or image model, using the
Frozen Pre-trained Transformer (FPT) for time series analysis with the addition
of projection matrices for input and output. Additionally, we introduce four
unique adapters, designed specifically for downstream tasks based on the
pre-trained model, including forecasting and anomaly detection. These adapters
are further enhanced with efficient parameter tuning, resulting in superior
performance compared to all state-of-the-art methods.Our comprehensive
experimental studies reveal that (a) the simple FPT achieves top-tier
performance across various time series analysis tasks; and (b) fine-tuning the
FPT with the custom-designed adapters can further elevate its performance,
outshining specialized task-specific models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14823">Revisiting Quantum Algorithms for Linear Regressions: Quadratic Speedups without Data-Dependent Parameters. (arXiv:2311.14823v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yin_J/0/1/0/all/0/1">Junze Yin</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_R/0/1/0/all/0/1">Ruizhe Zhang</a></p>
<p>Linear regression is one of the most fundamental linear algebra problems.
Given a dense matrix $A \in \mathbb{R}^{n \times d}$ and a vector $b$, the goal
is to find $x'$ such that
</p>
<p>$ \| Ax' - b \|_2^2 \leq (1+\epsilon) \min_{x} \| A x - b \|_2^2 $. The best
classical algorithm takes $O(nd) + \mathrm{poly}(d/\epsilon)$ time [Clarkson
and Woodruff STOC 2013, Nelson and Nguyen FOCS 2013]. On the other hand,
quantum linear regression algorithms can achieve exponential quantum speedups,
as shown in [Wang Phys. Rev. A 96, 012335, Kerenidis and Prakash ITCS 2017,
Chakraborty, Gily{\'e}n and Jeffery ICALP 2019]. However, the running times of
these algorithms depend on some quantum linear algebra-related parameters, such
as $\kappa(A)$, the condition number of $A$. In this work, we develop a quantum
algorithm that runs in $\widetilde{O}(\epsilon^{-1}\sqrt{n}d^{1.5}) +
\mathrm{poly}(d/\epsilon)$ time. It provides a quadratic quantum speedup in $n$
over the classical lower bound without any dependence on data-dependent
parameters. In addition, we also show our result can be generalized to multiple
regression and ridge linear regression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14824">A Reusable AI-Enabled Defect Detection System for Railway Using Ensembled CNN. (arXiv:2311.14824v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferdousi_R/0/1/0/all/0/1">Rahatara Ferdousi</a>, <a href="http://arxiv.org/find/cs/1/au:+Laamarti_F/0/1/0/all/0/1">Fedwa Laamarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chunsheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1">Abdulmotaleb El Saddik</a></p>
<p>Accurate Defect detection is crucial for ensuring the trustworthiness of
intelligent railway systems. Current approaches rely on single deep-learning
models, like CNNs, which employ a large amount of data to capture underlying
patterns. Training a new defect classifier with limited samples often leads to
overfitting and poor performance on unseen images. To address this, researchers
have advocated transfer learning and fine-tuning the pre-trained models.
However, using a single backbone network in transfer learning still may cause
bottleneck issues and inconsistent performance if it is not suitable for a
specific problem domain. To overcome these challenges, we propose a reusable
AI-enabled defect detection approach. By combining ensemble learning with
transfer learning models (VGG-19, MobileNetV3, and ResNet-50), we improved the
classification accuracy and achieved consistent performance at a certain phase
of training. Our empirical analysis demonstrates better and more consistent
performance compared to other state-of-the-art approaches. The consistency
substantiates the reusability of the defect detection system for newly evolved
defected rail parts. Therefore we anticipate these findings to benefit further
research and development of reusable AI-enabled solutions for railway systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14828">Deep Latent Force Models: ODE-based Process Convolutions for Bayesian Deep Learning. (arXiv:2311.14828v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Baldwin_McDonald_T/0/1/0/all/0/1">Thomas Baldwin-McDonald</a>, <a href="http://arxiv.org/find/stat/1/au:+Alvarez_M/0/1/0/all/0/1">Mauricio A. &#xc1;lvarez</a></p>
<p>Effectively modeling phenomena present in highly nonlinear dynamical systems
whilst also accurately quantifying uncertainty is a challenging task, which
often requires problem-specific techniques. We outline the deep latent force
model (DLFM), a domain-agnostic approach to tackling this problem, which
consists of a deep Gaussian process architecture where the kernel at each layer
is derived from an ordinary differential equation using the framework of
process convolutions. Two distinct formulations of the DLFM are presented which
utilise weight-space and variational inducing points-based Gaussian process
approximations, both of which are amenable to doubly stochastic variational
inference. We provide evidence that our model is capable of capturing highly
nonlinear behaviour in real-world multivariate time series data. In addition,
we find that our approach achieves comparable performance to a number of other
probabilistic models on benchmark regression tasks. We also empirically assess
the negative impact of the inducing points framework on the extrapolation
capabilities of LFM-based models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14856">Disruption Prediction in Fusion Devices through Feature Extraction and Logistic Regression. (arXiv:2311.14856v1 [physics.plasm-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Ferreira_D/0/1/0/all/0/1">Diogo R. Ferreira</a></p>
<p>This document describes an approach used in the Multi-Machine Disruption
Prediction Challenge for Fusion Energy by ITU, a data science competition which
ran from September to November 2023, on the online platform Zindi. The
competition involved data from three fusion devices - C-Mod, HL-2A, and J-TEXT
- with most of the training data coming from the last two, and the test data
coming from the first one. Each device has multiple diagnostics and signals,
and it turns out that a critical issue in this competition was to identify
which signals, and especially which features from those signals, were most
relevant to achieve accurate predictions. The approach described here is based
on extracting features from signals, and then applying logistic regression on
top of those features. Each signal is treated as a separate predictor and, in
the end, a combination of such predictors achieved the first place on the
leaderboard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14859">An Empirical Investigation into Benchmarking Model Multiplicity for Trustworthy Machine Learning: A Case Study on Image Classification. (arXiv:2311.14859v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1">Prakhar Ganesh</a></p>
<p>Deep learning models have proven to be highly successful. Yet, their
over-parameterization gives rise to model multiplicity, a phenomenon in which
multiple models achieve similar performance but exhibit distinct underlying
behaviours. This multiplicity presents a significant challenge and necessitates
additional specifications in model selection to prevent unexpected failures
during deployment. While prior studies have examined these concerns, they focus
on individual metrics in isolation, making it difficult to obtain a
comprehensive view of multiplicity in trustworthy machine learning. Our work
stands out by offering a one-stop empirical benchmark of multiplicity across
various dimensions of model design and its impact on a diverse set of
trustworthy metrics. In this work, we establish a consistent language for
studying model multiplicity by translating several trustworthy metrics into
accuracy under appropriate interventions. We also develop a framework, which we
call multiplicity sheets, to benchmark multiplicity in various scenarios. We
demonstrate the advantages of our setup through a case study in image
classification and provide actionable insights into the impact and trends of
different hyperparameters on model multiplicity. Finally, we show that
multiplicity persists in deep learning models even after enforcing additional
specifications during model selection, highlighting the severity of
over-parameterization. The concerns of under-specification thus remain, and we
seek to promote a more comprehensive discussion of multiplicity in trustworthy
machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14864">Effective Structural Encodings via Local Curvature Profiles. (arXiv:2311.14864v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fesser_L/0/1/0/all/0/1">Lukas Fesser</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1">Melanie Weber</a></p>
<p>Structural and Positional Encodings can significantly improve the performance
of Graph Neural Networks in downstream tasks. Recent literature has begun to
systematically investigate differences in the structural properties that these
approaches encode, as well as performance trade-offs between them. However, the
question of which structural properties yield the most effective encoding
remains open. In this paper, we investigate this question from a geometric
perspective. We propose a novel structural encoding based on discrete Ricci
curvature (Local Curvature Profiles, short LCP) and show that it significantly
outperforms existing encoding approaches. We further show that combining local
structural encodings, such as LCP, with global positional encodings improves
downstream performance, suggesting that they capture complementary geometric
information. Finally, we compare different encoding types with
(curvature-based) rewiring techniques. Rewiring has recently received a surge
of interest due to its ability to improve the performance of Graph Neural
Networks by mitigating over-smoothing and over-squashing effects. Our results
suggest that utilizing curvature information for structural encodings delivers
significantly larger performance increases than rewiring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14874">Advancing Fluid-Based Thermal Management Systems Design: Leveraging Graph Neural Networks for Graph Regression and Efficient Enumeration Reduction. (arXiv:2311.14874v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bayat_S/0/1/0/all/0/1">Saeid Bayat</a>, <a href="http://arxiv.org/find/eess/1/au:+Shahmansouri_N/0/1/0/all/0/1">Nastaran Shahmansouri</a>, <a href="http://arxiv.org/find/eess/1/au:+Peddada_S/0/1/0/all/0/1">Satya RT Peddada</a>, <a href="http://arxiv.org/find/eess/1/au:+Tessier_A/0/1/0/all/0/1">Alex Tessier</a>, <a href="http://arxiv.org/find/eess/1/au:+Butscher_A/0/1/0/all/0/1">Adrian Butscher</a>, <a href="http://arxiv.org/find/eess/1/au:+Allison_J/0/1/0/all/0/1">James T Allison</a></p>
<p>In this research, we developed a graph-based framework to represent various
aspects of optimal thermal management system design, with the aim of rapidly
and efficiently identifying optimal design candidates. Initially, the
graph-based framework is utilized to generate diverse thermal management system
architectures. The dynamics of these system architectures are modeled under
various loading conditions, and an open-loop optimal controller is employed to
determine each system's optimal performance. These modeled cases constitute the
dataset, with the corresponding optimal performance values serving as the
labels for the data. In the subsequent step, a Graph Neural Network (GNN) model
is trained on 30% of the labeled data to predict the systems' performance,
effectively addressing a regression problem. Utilizing this trained model, we
estimate the performance values for the remaining 70% of the data, which serves
as the test set. In the third step, the predicted performance values are
employed to rank the test data, facilitating prioritized evaluation of the
design scenarios. Specifically, a small subset of the test data with the
highest estimated ranks undergoes evaluation via the open-loop optimal control
solver. This targeted approach concentrates on evaluating higher-ranked designs
identified by the GNN, replacing the exhaustive search (enumeration-based) of
all design cases. The results demonstrate a significant average reduction of
over 92% in the number of system dynamic modeling and optimal control analyses
required to identify optimal design scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14885">Projected Off-Policy Q-Learning (POP-QL) for Stabilizing Offline Reinforcement Learning. (arXiv:2311.14885v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roderick_M/0/1/0/all/0/1">Melrose Roderick</a>, <a href="http://arxiv.org/find/cs/1/au:+Manek_G/0/1/0/all/0/1">Gaurav Manek</a>, <a href="http://arxiv.org/find/cs/1/au:+Berkenkamp_F/0/1/0/all/0/1">Felix Berkenkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a></p>
<p>A key problem in off-policy Reinforcement Learning (RL) is the mismatch, or
distribution shift, between the dataset and the distribution over states and
actions visited by the learned policy. This problem is exacerbated in the fully
offline setting. The main approach to correct this shift has been through
importance sampling, which leads to high-variance gradients. Other approaches,
such as conservatism or behavior-regularization, regularize the policy at the
cost of performance. In this paper, we propose a new approach for stable
off-policy Q-Learning. Our method, Projected Off-Policy Q-Learning (POP-QL), is
a novel actor-critic algorithm that simultaneously reweights off-policy samples
and constrains the policy to prevent divergence and reduce value-approximation
error. In our experiments, POP-QL not only shows competitive performance on
standard benchmarks, but also out-performs competing methods in tasks where the
data-collection policy is significantly sub-optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14886">A unified framework for learning with nonlinear model classes from arbitrary linear samples. (arXiv:2311.14886v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adcock_B/0/1/0/all/0/1">Ben Adcock</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardenas_J/0/1/0/all/0/1">Juan M. Cardenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dexter_N/0/1/0/all/0/1">Nick Dexter</a></p>
<p>This work considers the fundamental problem of learning an unknown object
from training data using a given model class. We introduce a unified framework
that allows for objects in arbitrary Hilbert spaces, general types of (random)
linear measurements as training data and general types of nonlinear model
classes. We establish a series of learning guarantees for this framework. These
guarantees provide explicit relations between the amount of training data and
properties of the model class to ensure near-best generalization bounds. In
doing so, we also introduce and develop the key notion of the variation of a
model class with respect to a distribution of sampling operators. To exhibit
the versatility of this framework, we show that it can accommodate many
different types of well-known problems of interest. We present examples such as
matrix sketching by random sampling, compressed sensing with isotropic vectors,
active learning in regression and compressed sensing with generative models. In
all cases, we show how known results become straightforward corollaries of our
general learning guarantees. For compressed sensing with generative models, we
also present a number of generalizations and improvements of recent results. In
summary, our work not only introduces a unified way to study learning unknown
objects from general types of data, but also establishes a series of general
theoretical guarantees which consolidate and improve various known results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14904">LLM-Assisted Code Cleaning For Training Accurate Code Generators. (arXiv:2311.14904v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1">Naman Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1">Wei-Lin Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_K/0/1/0/all/0/1">Koushik Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a></p>
<p>Natural language to code generation is an important application area of LLMs
and has received wide attention from the community. The majority of relevant
studies have exclusively concentrated on increasing the quantity and functional
correctness of training sets while disregarding other stylistic elements of
programs. More recently, data quality has garnered a lot of interest and
multiple works have showcased its importance for improving performance. In this
work, we investigate data quality for code and find that making the code more
structured and readable leads to improved code generation performance of the
system. We build a novel data-cleaning pipeline that uses these principles to
transform existing programs by 1.) renaming variables, 2.) modularizing and
decomposing complex code into smaller helper sub-functions, and 3.) inserting
natural-language based plans via LLM based transformations. We evaluate our
approach on two challenging algorithmic code generation benchmarks and find
that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves
the performance by up to 30% compared to fine-tuning on the original dataset.
Additionally, we demonstrate improved performance from using a smaller amount
of higher-quality data, finding that a model fine-tuned on the entire original
dataset is outperformed by a model trained on 15% of our cleaned dataset. Even
in comparison to closed-source models, our models outperform the much larger
AlphaCoder models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14908">Support Vector Machine Implementation on MPI-CUDA and Tensorflow Framework. (arXiv:2311.14908v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elgarhy_I/0/1/0/all/0/1">Islam Elgarhy</a></p>
<p>Support Vector Machine (SVM) algorithm requires a high computational cost
(both in memory and time) to solve a complex quadratic programming (QP)
optimization problem during the training process. Consequently, SVM
necessitates high computing hardware capabilities. The central processing unit
(CPU) clock frequency cannot be increased due to physical limitations in the
miniaturization process. However, the potential of parallel multi-architecture,
available in both multi-core CPUs and highly scalable GPUs, emerges as a
promising solution to enhance algorithm performance. Therefore, there is an
opportunity to reduce the high computational time required by SVM for solving
the QP optimization problem. This paper presents a comparative study that
implements the SVM algorithm on different parallel architecture frameworks. The
experimental results show that SVM MPI-CUDA implementation achieves a speedup
over SVM TensorFlow implementation on different datasets. Moreover, SVM
TensorFlow implementation provides a cross-platform solution that can be
migrated to alternative hardware components, which will reduces the development
time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14910">A latent linear model for nonlinear coupled oscillators on graphs. (arXiv:2311.14910v1 [math.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Goyal_A/0/1/0/all/0/1">Agam Goyal</a>, <a href="http://arxiv.org/find/math/1/au:+Wu_Z/0/1/0/all/0/1">Zhaoxing Wu</a>, <a href="http://arxiv.org/find/math/1/au:+Yim_R/0/1/0/all/0/1">Richard P. Yim</a>, <a href="http://arxiv.org/find/math/1/au:+Chen_B/0/1/0/all/0/1">Binhao Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1">Zihong Xu</a>, <a href="http://arxiv.org/find/math/1/au:+Lyu_H/0/1/0/all/0/1">Hanbaek Lyu</a></p>
<p>A system of coupled oscillators on an arbitrary graph is locally driven by
the tendency to mutual synchronization between nearby oscillators, but can and
often exhibit nonlinear behavior on the whole graph. Understanding such
nonlinear behavior has been a key challenge in predicting whether all
oscillators in such a system will eventually synchronize. In this paper, we
demonstrate that, surprisingly, such nonlinear behavior of coupled oscillators
can be effectively linearized in certain latent dynamic spaces. The key insight
is that there is a small number of `latent dynamics filters', each with a
specific association with synchronizing and non-synchronizing dynamics on
subgraphs so that any observed dynamics on subgraphs can be approximated by a
suitable linear combination of such elementary dynamic patterns. Taking an
ensemble of subgraph-level predictions provides an interpretable predictor for
whether the system on the whole graph reaches global synchronization. We
propose algorithms based on supervised matrix factorization to learn such
latent dynamics filters. We demonstrate that our method performs competitively
in synchronization prediction tasks against baselines and black-box
classification algorithms, despite its simple and interpretable architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14931">One-Shot Transfer Learning for Nonlinear ODEs. (arXiv:2311.14931v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1">Wanzhou Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1">Pavlos Protopapas</a>, <a href="http://arxiv.org/find/cs/1/au:+Parikh_J/0/1/0/all/0/1">Joy Parikh</a></p>
<p>We introduce a generalizable approach that combines perturbation method and
one-shot transfer learning to solve nonlinear ODEs with a single polynomial
term, using Physics-Informed Neural Networks (PINNs). Our method transforms
non-linear ODEs into linear ODE systems, trains a PINN across varied
conditions, and offers a closed-form solution for new instances within the same
non-linear ODE class. We demonstrate the effectiveness of this approach on the
Duffing equation and suggest its applicability to similarly structured PDEs and
ODE systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14934">Robust Graph Neural Networks via Unbiased Aggregation. (arXiv:2311.14934v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1">Ruiqi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1">Zhichao Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaorui Liu</a></p>
<p>The adversarial robustness of Graph Neural Networks (GNNs) has been
questioned due to the false sense of security uncovered by strong adaptive
attacks despite the existence of numerous defenses. In this work, we delve into
the robustness analysis of representative robust GNNs and provide a unified
robust estimation point of view to understand their robustness and limitations.
Our novel analysis of estimation bias motivates the design of a robust and
unbiased graph signal estimator. We then develop an efficient Quasi-Newton
iterative reweighted least squares algorithm to solve the estimation problem,
which unfolds as robust unbiased aggregation layers in GNNs with a theoretical
convergence guarantee. Our comprehensive experiments confirm the strong
robustness of our proposed model, and the ablation study provides a deep
understanding of its advantages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14939">OpenNet: Incremental Learning for Autonomous Driving Object Detection with Balanced Loss. (arXiv:2311.14939v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zezhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1">Guitao Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1">Xidong Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiangtao Wang</a></p>
<p>Automated driving object detection has always been a challenging task in
computer vision due to environmental uncertainties. These uncertainties include
significant differences in object sizes and encountering the class unseen. It
may result in poor performance when traditional object detection models are
directly applied to automated driving detection. Because they usually presume
fixed categories of common traffic participants, such as pedestrians and cars.
Worsely, the huge class imbalance between common and novel classes further
exacerbates performance degradation. To address the issues stated, we propose
OpenNet to moderate the class imbalance with the Balanced Loss, which is based
on Cross Entropy Loss. Besides, we adopt an inductive layer based on gradient
reshaping to fast learn new classes with limited samples during incremental
learning. To against catastrophic forgetting, we employ normalized feature
distillation. By the way, we improve multi-scale detection robustness and
unknown class recognition through FPN and energy-based detection, respectively.
The Experimental results upon the CODA dataset show that the proposed method
can obtain better performance than that of the existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14948">Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Sahil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1">Soumye Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav Mohanty Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1">Chirag Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1">Jeff Bilmes</a></p>
<p>Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach.
</p>
<p>In this work, we demonstrate that the efficacy of CleanCLIP in mitigating
backdoors is highly dependent on the particular objective used during model
pre-training.
</p>
<p>We observe that stronger pre-training objectives correlate with harder to
remove backdoors behaviors. We show this by training multimodal models on two
large datasets consisting of 3 million (CC3M) and 6 million (CC6M) datapoints,
under various pre-training objectives, followed by poison removal using
CleanCLIP. We find that CleanCLIP is ineffective when stronger pre-training
objectives are used, even with extensive hyperparameter tuning.
</p>
<p>Our findings underscore critical considerations for ML practitioners who
pre-train models using large-scale web-curated data and are concerned about
potential backdoor threats. Notably, our results suggest that simpler
pre-training objectives are more amenable to effective backdoor removal. This
insight is pivotal for practitioners seeking to balance the trade-offs between
using stronger pre-training objectives and security against backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14955">Identification of morphological fingerprint in perinatal brains using quasi-conformal mapping and contrastive learning. (arXiv:2311.14955v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Weihao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Yuchen Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Minmin Wang</a></p>
<p>The morphological fingerprint in the brain is capable of identifying the
uniqueness of an individual. However, whether such individual patterns are
present in perinatal brains, and which morphological attributes or cortical
regions better characterize the individual differences of ne-onates remain
unclear. In this study, we proposed a deep learning framework that projected
three-dimensional spherical meshes of three morphological features (i.e.,
cortical thickness, mean curvature, and sulcal depth) onto two-dimensional
planes through quasi-conformal mapping, and employed the ResNet18 and
contrastive learning for individual identification. We used the cross-sectional
structural MRI data of 682 infants, incorporating with data augmentation, to
train the model and fine-tuned the parameters based on 60 infants who had
longitudinal scans. The model was validated on 30 longitudinal scanned infant
data, and remarkable Top1 and Top5 accuracies of 71.37% and 84.10% were
achieved, respectively. The sensorimotor and visual cortices were recognized as
the most contributive regions in individual identification. Moreover, the
folding morphology demonstrated greater discriminative capability than the
cortical thickness, which could serve as the morphological fingerprint in
perinatal brains. These findings provided evidence for the emergence of
morphological fingerprints in the brain at the beginning of the third
trimester, which may hold promising implications for understanding the
formation of in-dividual uniqueness in the brain during early development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14964">Selective Inference for Changepoint detection by Recurrent Neural Network. (arXiv:2311.14964v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Shiraishi_T/0/1/0/all/0/1">Tomohiro Shiraishi</a>, <a href="http://arxiv.org/find/stat/1/au:+Miwa_D/0/1/0/all/0/1">Daiki Miwa</a>, <a href="http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1">Vo Nguyen Le Duy</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a></p>
<p>In this study, we investigate the quantification of the statistical
reliability of detected change points (CPs) in time series using a Recurrent
Neural Network (RNN). Thanks to its flexibility, RNN holds the potential to
effectively identify CPs in time series characterized by complex dynamics.
However, there is an increased risk of erroneously detecting random noise
fluctuations as CPs. The primary goal of this study is to rigorously control
the risk of false detections by providing theoretically valid p-values to the
CPs detected by RNN. To achieve this, we introduce a novel method based on the
framework of Selective Inference (SI). SI enables valid inferences by
conditioning on the event of hypothesis selection, thus mitigating selection
bias. In this study, we apply SI framework to RNN-based CP detection, where
characterizing the complex process of RNN selecting CPs is our main technical
challenge. We demonstrate the validity and effectiveness of the proposed method
through artificial and real data experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14971">Segmentation of diagnostic tissue compartments on whole slide images with renal thrombotic microangiopathies (TMAs). (arXiv:2311.14971v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1">Huy Q. Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cicalese_P/0/1/0/all/0/1">Pietro A. Cicalese</a>, <a href="http://arxiv.org/find/cs/1/au:+Seshan_S/0/1/0/all/0/1">Surya Seshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizvi_S/0/1/0/all/0/1">Syed A. Rizvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vathul_A/0/1/0/all/0/1">Aneesh Vathul</a>, <a href="http://arxiv.org/find/cs/1/au:+Bueno_G/0/1/0/all/0/1">Gloria Bueno</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorado_A/0/1/0/all/0/1">Anibal Pedraza Dorado</a>, <a href="http://arxiv.org/find/cs/1/au:+Grabe_N/0/1/0/all/0/1">Niels Grabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Stolle_K/0/1/0/all/0/1">Katharina Stolle</a>, <a href="http://arxiv.org/find/cs/1/au:+Pesce_F/0/1/0/all/0/1">Francesco Pesce</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1">Joris J.T.H. Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1">Jesper Kers</a>, <a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_V/0/1/0/all/0/1">Vitoantonio Bevilacqua</a>, <a href="http://arxiv.org/find/cs/1/au:+Altinini_N/0/1/0/all/0/1">Nicola Altinini</a>, <a href="http://arxiv.org/find/cs/1/au:+Schroppel_B/0/1/0/all/0/1">Bernd Schr&#xf6;ppel</a>, <a href="http://arxiv.org/find/cs/1/au:+Roccatello_D/0/1/0/all/0/1">Dario Roccatello</a>, <a href="http://arxiv.org/find/cs/1/au:+Barreca_A/0/1/0/all/0/1">Antonella Barreca</a>, <a href="http://arxiv.org/find/cs/1/au:+Sciascia_S/0/1/0/all/0/1">Savino Sciascia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1">Chandra Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hien V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Becker_J/0/1/0/all/0/1">Jan U. Becker</a></p>
<p>The thrombotic microangiopathies (TMAs) manifest in renal biopsy histology
with a broad spectrum of acute and chronic findings. Precise diagnostic
criteria for a renal biopsy diagnosis of TMA are missing. As a first step
towards a machine learning- and computer vision-based analysis of wholes slide
images from renal biopsies, we trained a segmentation model for the decisive
diagnostic kidney tissue compartments artery, arteriole, glomerulus on a set of
whole slide images from renal biopsies with TMAs and Mimickers (distinct
diseases with a similar nephropathological appearance as TMA like severe benign
nephrosclerosis, various vasculitides, Bevacizumab-plug glomerulopathy,
arteriolar light chain deposition disease). Our segmentation model combines a
U-Net-based tissue detection with a Shifted windows-transformer architecture to
reach excellent segmentation results for even the most severely altered
glomeruli, arterioles and arteries, even on unseen staining domains from a
different nephropathology lab. With accurate automatic segmentation of the
decisive renal biopsy compartments in human renal vasculopathies, we have laid
the foundation for large-scale compartment-specific machine learning and
computer vision analysis of renal biopsy repositories with TMAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14975">Eliminating Domain Bias for Federated Learning in Representation Space. (arXiv:2311.14975v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yang Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jian Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zhengui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruhui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Haibing Guan</a></p>
<p>Recently, federated learning (FL) is popular for its privacy-preserving and
collaborative learning abilities. However, under statistically heterogeneous
scenarios, we observe that biased data domains on clients cause a
representation bias phenomenon and further degenerate generic representations
during local training, i.e., the representation degeneration phenomenon. To
address these issues, we propose a general framework Domain Bias Eliminator
(DBE) for FL. Our theoretical analysis reveals that DBE can promote
bi-directional knowledge transfer between server and client, as it reduces the
domain discrepancy between server and client in representation space. Besides,
extensive experiments on four datasets show that DBE can greatly improve
existing FL methods in both generalization and personalization abilities. The
DBE-equipped FL method can outperform ten state-of-the-art personalized FL
methods by a large margin. Our code is public at
https://github.com/TsingZ0/DBE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14983">Neural Network Based Approach to Recognition of Meteor Tracks in the Mini-EUSO Telescope Data. (arXiv:2311.14983v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Zotov_M/0/1/0/all/0/1">Mikhail Zotov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Anzhiganov_D/0/1/0/all/0/1">Dmitry Anzhiganov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kryazhenkov_A/0/1/0/all/0/1">Aleksandr Kryazhenkov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Barghini_D/0/1/0/all/0/1">Dario Barghini</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Battisti_M/0/1/0/all/0/1">Matteo Battisti</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Belov_A/0/1/0/all/0/1">Alexander Belov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bertaina_M/0/1/0/all/0/1">Mario Bertaina</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bianciotto_M/0/1/0/all/0/1">Marta Bianciotto</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bisconti_F/0/1/0/all/0/1">Francesca Bisconti</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Blaksley_C/0/1/0/all/0/1">Carl Blaksley</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Blin_S/0/1/0/all/0/1">Sylvie Blin</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Cambie_G/0/1/0/all/0/1">Giorgio Cambi&#xe8;</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Capel_F/0/1/0/all/0/1">Francesca Capel</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Casolino_M/0/1/0/all/0/1">Marco Casolino</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ebisuzaki_T/0/1/0/all/0/1">Toshikazu Ebisuzaki</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Eser_J/0/1/0/all/0/1">Johannes Eser</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Fenu_F/0/1/0/all/0/1">Francesco Fenu</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Franceschi_M/0/1/0/all/0/1">Massimo Alberto Franceschi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Golzio_A/0/1/0/all/0/1">Alessio Golzio</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Gorodetzky_P/0/1/0/all/0/1">Philippe Gorodetzky</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kajino_F/0/1/0/all/0/1">Fumiyoshi Kajino</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kasuga_H/0/1/0/all/0/1">Hiroshi Kasuga</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Klimov_P/0/1/0/all/0/1">Pavel Klimov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Manfrin_M/0/1/0/all/0/1">Massimiliano Manfrin</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Marcelli_L/0/1/0/all/0/1">Laura Marcelli</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Miyamoto_H/0/1/0/all/0/1">Hiroko Miyamoto</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Murashov_A/0/1/0/all/0/1">Alexey Murashov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Napolitano_T/0/1/0/all/0/1">Tommaso Napolitano</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ohmori_H/0/1/0/all/0/1">Hiroshi Ohmori</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Olinto_A/0/1/0/all/0/1">Angela Olinto</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Parizot_E/0/1/0/all/0/1">Etienne Parizot</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Picozza_P/0/1/0/all/0/1">Piergiorgio Picozza</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Piotrowski_L/0/1/0/all/0/1">Lech Wiktor Piotrowski</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Plebaniak_Z/0/1/0/all/0/1">Zbigniew Plebaniak</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Prevot_G/0/1/0/all/0/1">Guillaume Pr&#xe9;v&#xf4;t</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Reali_E/0/1/0/all/0/1">Enzo Reali</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ricci_M/0/1/0/all/0/1">Marco Ricci</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Romoli_G/0/1/0/all/0/1">Giulia Romoli</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Sakaki_N/0/1/0/all/0/1">Naoto Sakaki</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Shinozaki_K/0/1/0/all/0/1">Kenji Shinozaki</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Taille_C/0/1/0/all/0/1">Christophe De La Taille</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Takizawa_Y/0/1/0/all/0/1">Yoshiyuki Takizawa</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Vrabel_M/0/1/0/all/0/1">Michal Vr&#xe1;bel</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Wiencke_L/0/1/0/all/0/1">Lawrence Wiencke</a></p>
<p>Mini-EUSO is a wide-angle fluorescence telescope that registers ultraviolet
(UV) radiation in the nocturnal atmosphere of Earth from the International
Space Station. Meteors are among multiple phenomena that manifest themselves
not only in the visible range but also in the UV. We present two simple
artificial neural networks that allow for recognizing meteor signals in the
Mini-EUSO data with high accuracy in terms of a binary classification problem.
We expect that similar architectures can be effectively used for signal
recognition in other fluorescence telescopes, regardless of the nature of the
signal. Due to their simplicity, the networks can be implemented in onboard
electronics of future orbital or balloon experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14990">View it like a radiologist: Shifted windows for deep learning augmentation of CT images. (arXiv:2311.14990v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ostmo_E/0/1/0/all/0/1">Eirik A. &#xd8;stmo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wickstrom_K/0/1/0/all/0/1">Kristoffer K. Wickstr&#xf8;m</a>, <a href="http://arxiv.org/find/eess/1/au:+Radiya_K/0/1/0/all/0/1">Keyur Radiya</a>, <a href="http://arxiv.org/find/eess/1/au:+Kampffmeyer_M/0/1/0/all/0/1">Michael C. Kampffmeyer</a>, <a href="http://arxiv.org/find/eess/1/au:+Jenssen_R/0/1/0/all/0/1">Robert Jenssen</a></p>
<p>Deep learning has the potential to revolutionize medical practice by
automating and performing important tasks like detecting and delineating the
size and locations of cancers in medical images. However, most deep learning
models rely on augmentation techniques that treat medical images as natural
images. For contrast-enhanced Computed Tomography (CT) images in particular,
the signals producing the voxel intensities have physical meaning, which is
lost during preprocessing and augmentation when treating such images as natural
images. To address this, we propose a novel preprocessing and intensity
augmentation scheme inspired by how radiologists leverage multiple viewing
windows when evaluating CT images. Our proposed method, window shifting,
randomly places the viewing windows around the region of interest during
training. This approach improves liver lesion segmentation performance and
robustness on images with poorly timed contrast agent. Our method outperforms
classical intensity augmentations as well as the intensity augmentation
pipeline of the popular nn-UNet on multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14994">Exploring Causal Learning through Graph Neural Networks: An In-depth Review. (arXiv:2311.14994v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Job_S/0/1/0/all/0/1">Simi Job</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xiaohui Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Taotao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Haoran Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1">Jianming Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a></p>
<p>In machine learning, exploring data correlations to predict outcomes is a
fundamental task. Recognizing causal relationships embedded within data is
pivotal for a comprehensive understanding of system dynamics, the significance
of which is paramount in data-driven decision-making processes. Beyond
traditional methods, there has been a surge in the use of graph neural networks
(GNNs) for causal learning, given their capabilities as universal data
approximators. Thus, a thorough review of the advancements in causal learning
using GNNs is both relevant and timely. To structure this review, we introduce
a novel taxonomy that encompasses various state-of-the-art GNN methods employed
in studying causality. GNNs are further categorized based on their applications
in the causality domain. We further provide an exhaustive compilation of
datasets integral to causal learning with GNNs to serve as a resource for
practical study. This review also touches upon the application of causal
learning across diverse sectors. We conclude the review with insights into
potential challenges and promising avenues for future exploration in this
rapidly evolving field of machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15000">Satellite-based feature extraction and multivariate time-series prediction of biotoxin contamination in shellfish. (arXiv:2311.15000v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tavares_S/0/1/0/all/0/1">Sergio Tavares</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_P/0/1/0/all/0/1">Pedro R. Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Krippahl_L/0/1/0/all/0/1">Ludwig Krippahl</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopes_M/0/1/0/all/0/1">Marta B. Lopes</a></p>
<p>Shellfish production constitutes an important sector for the economy of many
Portuguese coastal regions, yet the challenge of shellfish biotoxin
contamination poses both public health concerns and significant economic risks.
Thus, predicting shellfish contamination levels holds great potential for
enhancing production management and safeguarding public health. In our study,
we utilize a dataset with years of Sentinel-3 satellite imagery for marine
surveillance, along with shellfish biotoxin contamination data from various
production areas along Portugal's western coastline, collected by Portuguese
official control. Our goal is to evaluate the integration of satellite data in
forecasting models for predicting toxin concentrations in shellfish given
forecasting horizons up to four weeks, which implies extracting a small set of
useful features and assessing their impact on the predictive models. We framed
this challenge as a time-series forecasting problem, leveraging historical
contamination levels and satellite images for designated areas. While
contamination measurements occurred weekly, satellite images were accessible
multiple times per week. Unsupervised feature extraction was performed using
autoencoders able to handle non-valid pixels caused by factors like cloud
cover, land, or anomalies. Finally, several Artificial Neural Networks models
were applied to compare univariate (contamination only) and multivariate
(contamination and satellite data) time-series forecasting. Our findings show
that incorporating these features enhances predictions, especially beyond one
week in lagoon production areas (RIAV) and for the 1-week and 2-week horizons
in the L5B area (oceanic). The methodology shows the feasibility of integrating
information from a high-dimensional data source like remote sensing without
compromising the model's predictive ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15036">On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level Sensor Data. (arXiv:2311.15036v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1">Tianheng Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chao Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiele_G/0/1/0/all/0/1">Gregor Schiele</a></p>
<p>Soft sensors are crucial in bridging autonomous systems' physical and digital
realms, enhancing sensor fusion and perception. Instead of deploying soft
sensors on the Cloud, this study shift towards employing on-device soft
sensors, promising heightened efficiency and bolstering data security. Our
approach substantially improves energy efficiency by deploying Artificial
Intelligence (AI) directly on devices within a wireless sensor network.
Furthermore, the synergistic integration of the Microcontroller Unit and
Field-Programmable Gate Array (FPGA) leverages the rapid AI inference
capabilities of the latter. Empirical evidence from our real-world use case
demonstrates that FPGA-based soft sensors achieve inference times ranging
remarkably from 1.04 to 12.04 microseconds. These compelling results highlight
the considerable potential of our innovative approach for executing real-time
inference tasks efficiently, thereby presenting a feasible alternative that
effectively addresses the latency challenges intrinsic to Cloud-based
deployments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15041">MPCNN: A Novel Matrix Profile Approach for CNN-based Sleep Apnea Classification. (arXiv:2311.15041v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hieu X. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duong V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hieu H. Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_C/0/1/0/all/0/1">Cuong D. Do</a></p>
<p>Sleep apnea (SA) is a significant respiratory condition that poses a major
global health challenge. Previous studies have investigated several machine and
deep learning models for electrocardiogram (ECG)-based SA diagnoses. Despite
these advancements, conventional feature extractions derived from ECG signals,
such as R-peaks and RR intervals, may fail to capture crucial information
encompassed within the complete PQRST segments. In this study, we propose an
innovative approach to address this diagnostic gap by delving deeper into the
comprehensive segments of the ECG signal. The proposed methodology draws
inspiration from Matrix Profile algorithms, which generate an Euclidean
distance profile from fixed-length signal subsequences. From this, we derived
the Min Distance Profile (MinDP), Max Distance Profile (MaxDP), and Mean
Distance Profile (MeanDP) based on the minimum, maximum, and mean of the
profile distances, respectively. To validate the effectiveness of our approach,
we use the modified LeNet-5 architecture as the primary CNN model, along with
two existing lightweight models, BAFNet and SE-MSCNN, for ECG classification
tasks. Our extensive experimental results on the PhysioNet Apnea-ECG dataset
revealed that with the new feature extraction method, we achieved a per-segment
accuracy up to 92.11 \% and a per-recording accuracy of 100\%. Moreover, it
yielded the highest correlation compared to state-of-the-art methods, with a
correlation coefficient of 0.989. By introducing a new feature extraction
method based on distance relationships, we enhanced the performance of certain
lightweight models, showing potential for home sleep apnea test (HSAT) and SA
detection in IoT devices. The source code for this work is made publicly
available in GitHub: https://github.com/vinuni-vishc/MPCNN-Sleep-Apnea.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15047">Training a Hopfield Variational Autoencoder with Equilibrium Propagation. (arXiv:2311.15047v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meersch_T/0/1/0/all/0/1">Tom Van Der Meersch</a>, <a href="http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1">Johannes Deleu</a>, <a href="http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1">Thomas Demeester</a></p>
<p>On dedicated analog hardware, equilibrium propagation is an energy-efficient
alternative to backpropagation. In spite of its theoretical guarantees, its
application in the AI domain remains limited to the discriminative setting.
Meanwhile, despite its high computational demands, generative AI is on the
rise. In this paper, we demonstrate the application of Equilibrium Propagation
in training a variational autoencoder (VAE) for generative modeling. Leveraging
the symmetric nature of Hopfield networks, we propose using a single model to
serve as both the encoder and decoder which could effectively halve the
required chip size for VAE implementations, paving the way for more efficient
analog hardware configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15051">Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study. (arXiv:2311.15051v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phunyaphibarn_P/0/1/0/all/0/1">Prin Phunyaphibarn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Junghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huishuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1">Chulhee Yun</a></p>
<p>Although gradient descent with momentum is widely used in modern deep
learning, a concrete understanding of its effects on the training trajectory
still remains elusive. In this work, we empirically show that momentum gradient
descent with a large learning rate and learning rate warmup displays large
catapults, driving the iterates towards flatter minima than those found by
gradient descent. We then provide empirical evidence and theoretical intuition
that the large catapult is caused by momentum "amplifying" the
self-stabilization effect (Damian et al., 2023).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15053">Task adaption by biologically inspired stochastic comodulation. (arXiv:2311.15053v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boeshertz_G/0/1/0/all/0/1">Gauthier Boeshertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Haimerl_C/0/1/0/all/0/1">Caroline Haimerl</a>, <a href="http://arxiv.org/find/cs/1/au:+Savin_C/0/1/0/all/0/1">Cristina Savin</a></p>
<p>Brain representations must strike a balance between generalizability and
adaptability. Neural codes capture general statistical regularities in the
world, while dynamically adjusting to reflect current goals. One aspect of this
adaptation is stochastically co-modulating neurons' gains based on their task
relevance. These fluctuations then propagate downstream to guide
decision-making. Here, we test the computational viability of such a scheme in
the context of multi-task learning. We show that fine-tuning convolutional
networks by stochastic gain modulation improves on deterministic gain
modulation, achieving state-of-the-art results on the CelebA dataset. To better
understand the mechanisms supporting this improvement, we explore how
fine-tuning performance is affected by architecture using Cifar-100. Overall,
our results suggest that stochastic comodulation can enhance learning
efficiency and performance in multi-task learning, without additional learnable
parameters. This offers a promising new direction for developing more flexible
and robust intelligent systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15054">Detection of developmental language disorder in Cypriot Greek children using a machine learning neural network algorithm. (arXiv:2311.15054v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Georgiou_G/0/1/0/all/0/1">Georgios P. Georgiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1">Elena Theodorou</a></p>
<p>Children with developmental language disorder (DLD) encounter difficulties in
acquiring various language structures. Early identification and intervention
are crucial to prevent negative long-term outcomes impacting the academic,
social, and emotional development of children. The study aims to develop an
automated method for the identification of DLD using artificial intelligence,
specifically a neural network machine learning algorithm. This protocol is
applied for the first time in Cypriot Greek children, which is generally
considered underresearched in the context of DLD. The neural network model was
trained using perceptual and production data elicited from children with DLD
and healthy controls. The k-fold technique was used to crossvalidate the
algorithm. The performance of the model was evaluated using metrics such as
accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability
to make accurate predictions on a set of unseen data. The results demonstrated
high classification values for all metrics (between 0.92 and 0.98), indicating
the high accuracy of the neural model in classifying children with DLD.
Additionally, the variable importance analysis revealed that the language
production skills of children had a more significant impact on the performance
of the model compared to perception skills. Neural networks represent powerful
tools for detecting DLD, providing early and quick assessments of the disorder,
and having the potential to improve clinical outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15056">Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning. (arXiv:2311.15056v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zaifei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1">Quanming Yao</a></p>
<p>Background: Discovering potential drug-drug interactions (DDIs) is a
long-standing challenge in clinical treatments and drug developments. Recently,
deep learning techniques have been developed for DDI prediction. However, they
generally require a huge number of samples, while known DDIs are rare.
</p>
<p>Methods: In this work, we present KnowDDI, a graph neural network-based
method that addresses the above challenge. KnowDDI enhances drug
representations by adaptively leveraging rich neighborhood information from
large biomedical knowledge graphs. Then, it learns a knowledge subgraph for
each drug-pair to interpret the predicted DDI, where each of the edges is
associated with a connection strength indicating the importance of a known DDI
or resembling strength between a drug-pair whose connection is unknown. Thus,
the lack of DDIs is implicitly compensated by the enriched drug representations
and propagated drug similarities.
</p>
<p>Results: We evaluate KnowDDI on two benchmark DDI datasets. Results show that
KnowDDI obtains the state-of-the-art prediction performance with better
interpretability. We also find that KnowDDI suffers less than existing works
given a sparser knowledge graph. This indicates that the propagated drug
similarities play a more important role in compensating for the lack of DDIs
when the drug representations are less enriched.
</p>
<p>Conclusions: KnowDDI nicely combines the efficiency of deep learning
techniques and the rich prior knowledge in biomedical knowledge graphs. As an
original open-source tool, KnowDDI can help detect possible interactions in a
broad range of relevant interaction prediction tasks, such as protein-protein
interactions, drug-target interactions and disease-gene interactions,
eventually promoting the development of biomedicine and healthcare.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1807.03418">AudioMNIST: Exploring Explainable Artificial Intelligence for Audio Analysis on a Simple Benchmark. (arXiv:1807.03418v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1">S&#xf6;ren Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Vielhaben_J/0/1/0/all/0/1">Johanna Vielhaben</a>, <a href="http://arxiv.org/find/cs/1/au:+Ackermann_M/0/1/0/all/0/1">Marcel Ackermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1">Sebastian Lapuschkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a></p>
<p>Explainable Artificial Intelligence (XAI) is targeted at understanding how
models perform feature selection and derive their classification decisions.
This paper explores post-hoc explanations for deep neural networks in the audio
domain. Notably, we present a novel Open Source audio dataset consisting of
30,000 audio samples of English spoken digits which we use for classification
tasks on spoken digits and speakers' biological sex. We use the popular XAI
technique Layer-wise Relevance Propagation (LRP) to identify relevant features
for two neural network architectures that process either waveform or
spectrogram representations of the data. Based on the relevance scores obtained
from LRP, hypotheses about the neural networks' feature selection are derived
and subsequently tested through systematic manipulations of the input data.
Further, we take a step beyond visual explanations and introduce audible
heatmaps. We demonstrate the superior interpretability of audible explanations
over visual ones in a human user study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.14660">Ergodicity of the underdamped mean-field Langevin dynamics. (arXiv:2007.14660v3 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kazeykina_A/0/1/0/all/0/1">Anna Kazeykina</a>, <a href="http://arxiv.org/find/math/1/au:+Ren_Z/0/1/0/all/0/1">Zhenjie Ren</a>, <a href="http://arxiv.org/find/math/1/au:+Tan_X/0/1/0/all/0/1">Xiaolu Tan</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_J/0/1/0/all/0/1">Junjian Yang</a></p>
<p>We study the long time behavior of an underdamped mean-field Langevin (MFL)
equation, and provide a general convergence as well as an exponential
convergence rate result under different conditions. The results on the MFL
equation can be applied to study the convergence of the Hamiltonian gradient
descent algorithm for the overparametrized optimization. We then provide a
numerical example of the algorithm to train a generative adversarial networks
(GAN).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.12920">Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning. (arXiv:2102.12920v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shaoxiong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yue Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Saravirta_T/0/1/0/all/0/1">Teemu Saravirta</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiqin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasankari_L/0/1/0/all/0/1">Lauri Vasankari</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">Guodong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Walid_A/0/1/0/all/0/1">Anwar Walid</a></p>
<p>Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. This survey reviews the state of the art, challenges,
and future directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.03543">Artificial Neural Networks generated by Low Discrepancy Sequences. (arXiv:2103.03543v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1">Alexander Keller</a>, <a href="http://arxiv.org/find/cs/1/au:+keirsbilck_M/0/1/0/all/0/1">Matthijs Van keirsbilck</a></p>
<p>Artificial neural networks can be represented by paths. Generated as random
walks on a dense network graph, we find that the resulting sparse networks
allow for deterministic initialization and even weights with fixed sign. Such
networks can be trained sparse from scratch, avoiding the expensive procedure
of training a dense network and compressing it afterwards. Although sparse,
weights are accessed as contiguous blocks of memory. In addition, enumerating
the paths using deterministic low discrepancy sequences, for example the Sobol'
sequence, amounts to connecting the layers of neural units by progressive
permutations, which naturally avoids bank conflicts in parallel computer
hardware. We demonstrate that the artificial neural networks generated by low
discrepancy sequences can achieve an accuracy within reach of their dense
counterparts at a much lower computational complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.08693">Elastic Shape Analysis of Tree-like 3D Objects using Extended SRVF Representation. (arXiv:2110.08693v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Laga_H/0/1/0/all/0/1">Hamid Laga</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1">Anuj Srivastava</a></p>
<p>How can one analyze detailed 3D biological objects, such as neurons and
botanical trees, that exhibit complex geometrical and topological variation? In
this paper, we develop a novel mathematical framework for representing,
comparing, and computing geodesic deformations between the shapes of such
tree-like 3D objects. A hierarchical organization of subtrees characterizes
these objects -- each subtree has the main branch with some side branches
attached -- and one needs to match these structures across objects for
meaningful comparisons. We propose a novel representation that extends the
Square-Root Velocity Function (SRVF), initially developed for Euclidean curves,
to tree-shaped 3D objects. We then define a new metric that quantifies the
bending, stretching, and branch sliding needed to deform one tree-shaped object
into the other. Compared to the current metrics, such as the Quotient Euclidean
Distance (QED) and the Tree Edit Distance (TED), the proposed representation
and metric capture the full elasticity of the branches (i.e., bending and
stretching) as well as the topological variations (i.e., branch death/birth and
sliding). It completely avoids the shrinkage that results from the edge
collapse and node split operations of the QED and TED metrics. We demonstrate
the utility of this framework in comparing, matching, and computing geodesics
between biological objects such as neurons and botanical trees. The framework
is also applied to various shape analysis tasks: (i) symmetry analysis and
symmetrization of tree-shaped 3D objects, (ii) computing summary statistics
(means and modes of variations) of populations of tree-shaped 3D objects, (iii)
fitting parametric probability distributions to such populations, and (iv)
finally synthesizing novel tree-shaped 3D objects through random sampling from
estimated probability distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.08239">Assessing Deep Neural Networks as Probability Estimators. (arXiv:2111.08239v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_K/0/1/0/all/0/1">Kwo-Sen Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rilee_M/0/1/0/all/0/1">Michael L. Rilee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hongfeng Yu</a></p>
<p>Deep Neural Networks (DNNs) have performed admirably in classification tasks.
However, the characterization of their classification uncertainties, required
for certain applications, has been lacking. In this work, we investigate the
issue by assessing DNNs' ability to estimate conditional probabilities and
propose a framework for systematic uncertainty characterization. Denoting the
input sample as x and the category as y, the classification task of assigning a
category y to a given input x can be reduced to the task of estimating the
conditional probabilities p(y|x), as approximated by the DNN at its last layer
using the softmax function. Since softmax yields a vector whose elements all
fall in the interval (0, 1) and sum to 1, it suggests a probabilistic
interpretation to the DNN's outcome. Using synthetic and real-world datasets,
we look into the impact of various factors, e.g., probability density f(x) and
inter-categorical sparsity, on the precision of DNNs' estimations of p(y|x),
and find that the likelihood probability density and the inter-categorical
sparsity have greater impacts than the prior probability to DNNs'
classification uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.08805">Online Estimation and Optimization of Utility-Based Shortfall Risk. (arXiv:2111.08805v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hegde_V/0/1/0/all/0/1">Vishwajit Hegde</a>, <a href="http://arxiv.org/find/stat/1/au:+Menon_A/0/1/0/all/0/1">Arvind S. Menon</a>, <a href="http://arxiv.org/find/stat/1/au:+Prashanth_L/0/1/0/all/0/1">L.A. Prashanth</a>, <a href="http://arxiv.org/find/stat/1/au:+Jagannathan_K/0/1/0/all/0/1">Krishna Jagannathan</a></p>
<p>Utility-Based Shortfall Risk (UBSR) is a risk metric that is increasingly
popular in financial applications, owing to certain desirable properties that
it enjoys. We consider the problem of estimating UBSR in a recursive setting,
where samples from the underlying loss distribution are available
one-at-a-time. We cast the UBSR estimation problem as a root finding problem,
and propose stochastic approximation-based estimations schemes. We derive
non-asymptotic bounds on the estimation error in the number of samples. We also
consider the problem of UBSR optimization within a parameterized class of
random variables. We propose a stochastic gradient descent based algorithm for
UBSR optimization, and derive non-asymptotic bounds on its convergence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.12589">A deep reinforcement learning model for predictive maintenance planning of road assets: Integrating LCA and LCCA. (arXiv:2112.12589v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Latifi_M/0/1/0/all/0/1">Moein Latifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Darvishvand_F/0/1/0/all/0/1">Fateme Golivand Darvishvand</a>, <a href="http://arxiv.org/find/cs/1/au:+Khandel_O/0/1/0/all/0/1">Omid Khandel</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowsoud_M/0/1/0/all/0/1">Mobin Latifi Nowsoud</a></p>
<p>Road maintenance planning is an integral part of road asset management. One
of the main challenges in Maintenance and Rehabilitation (M&amp;R) practices is to
determine maintenance type and timing. This research proposes a framework using
Reinforcement Learning (RL) based on the Long Term Pavement Performance (LTPP)
database to determine the type and timing of M&amp;R practices. A predictive DNN
model is first developed in the proposed algorithm, which serves as the
Environment for the RL algorithm. For the Policy estimation of the RL model,
both DQN and PPO models are developed. However, PPO has been selected in the
end due to better convergence and higher sample efficiency. Indicators used in
this study are International Roughness Index (IRI) and Rutting Depth (RD).
Initially, we considered Cracking Metric (CM) as the third indicator, but it
was then excluded due to the much fewer data compared to other indicators,
which resulted in lower accuracy of the results. Furthermore, in
cost-effectiveness calculation (reward), we considered both the economic and
environmental impacts of M&amp;R treatments. Costs and environmental impacts have
been evaluated with paLATE 2.0 software. Our method is tested on a hypothetical
case study of a six-lane highway with 23 kilometers length located in Texas,
which has a warm and wet climate. The results propose a 20-year M&amp;R plan in
which road condition remains in an excellent condition range. Because the early
state of the road is at a good level of service, there is no need for heavy
maintenance practices in the first years. Later, after heavy M&amp;R actions, there
are several 1-2 years of no need for treatments. All of these show that the
proposed plan has a logical result. Decision-makers and transportation agencies
can use this scheme to conduct better maintenance practices that can prevent
budget waste and, at the same time, minimize the environmental impacts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.01628">Bridging Adversarial and Nonstationary Multi-armed Bandit. (arXiv:2201.01628v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Ningyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuoguang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hailun Zhang</a></p>
<p>In the multi-armed bandit framework, there are two formulations that are
commonly employed to handle time-varying reward distributions: adversarial
bandit and nonstationary bandit. Although their oracles, algorithms, and regret
analysis differ significantly, we provide a unified formulation in this paper
that smoothly bridges the two as special cases. The formulation uses an oracle
that takes the best-fixed arm within time windows. Depending on the window
size, it turns into the oracle in hindsight in the adversarial bandit and
dynamic oracle in the nonstationary bandit. We provide algorithms that attain
the optimal regret with the matching lower bound.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.05400">Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation. (arXiv:2203.05400v5 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Karvonen_T/0/1/0/all/0/1">Toni Karvonen</a></p>
<p>It is common to model a deterministic response function, such as the output
of a computer experiment, as a Gaussian process with a Mat\'ern covariance
kernel. The smoothness parameter of a Mat\'ern kernel determines many important
properties of the model in the large data limit, including the rate of
convergence of the conditional mean to the response function. We prove that the
maximum likelihood estimate of the smoothness parameter cannot asymptotically
undersmooth the truth when the data are obtained on a fixed bounded subset of
$\mathbb{R}^d$. That is, if the data-generating response function has Sobolev
smoothness $\nu_0 &gt; d/2$, then the smoothness parameter estimate cannot be
asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we
show that maximum likelihood estimation recovers the true smoothness for a
class of compactly supported self-similar functions. For cross-validation we
prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be
sharp. The results are based on approximation theory in Sobolev spaces and some
general theorems that restrict the set of values that the parameter estimators
can take.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.09347">Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Eckstein_S/0/1/0/all/0/1">Stephan Eckstein</a>, <a href="http://arxiv.org/find/stat/1/au:+Iske_A/0/1/0/all/0/1">Armin Iske</a>, <a href="http://arxiv.org/find/stat/1/au:+Trabs_M/0/1/0/all/0/1">Mathias Trabs</a></p>
<p>In a high-dimensional regression framework, we study consequences of the
naive two-step procedure where first the dimension of the input variables is
reduced and second, the reduced input variables are used to predict the output
variable with kernel regression. In order to analyze the resulting regression
errors, a novel stability result for kernel regression with respect to the
Wasserstein distance is derived. This allows us to bound errors that occur when
perturbed input data is used to fit the regression function. We apply the
general stability result to principal component analysis (PCA). Exploiting
known estimates from the literature on both principal component analysis and
kernel regression, we deduce convergence rates for the two-step procedure. The
latter turns out to be particularly useful in a semi-supervised setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.09659">Low-degree learning and the metric entropy of polynomials. (arXiv:2203.09659v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eskenazis_A/0/1/0/all/0/1">Alexandros Eskenazis</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivanisvili_P/0/1/0/all/0/1">Paata Ivanisvili</a>, <a href="http://arxiv.org/find/cs/1/au:+Streck_L/0/1/0/all/0/1">Lauritz Streck</a></p>
<p>Let $\mathscr{F}_{n,d}$ be the class of all functions $f:\{-1,1\}^n\to[-1,1]$
on the $n$-dimensional discrete hypercube of degree at most $d$. In the first
part of this paper, we prove that any (deterministic or randomized) algorithm
which learns $\mathscr{F}_{n,d}$ with $L_2$-accuracy $\varepsilon$ requires at
least $\Omega((1-\sqrt{\varepsilon})2^d\log n)$ queries for large enough $n$,
thus establishing the sharpness as $n\to\infty$ of a recent upper bound of
Eskenazis and Ivanisvili (2021). To do this, we show that the $L_2$-packing
numbers $\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon)$ of the
concept class $\mathscr{F}_{n,d}$ satisfy the two-sided estimate
$$c(1-\varepsilon)2^d\log n \leq \log
\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon) \leq \frac{2^{Cd}\log
n}{\varepsilon^4}$$ for large enough $n$, where $c, C&gt;0$ are universal
constants. In the second part of the paper, we present a logarithmic upper
bound for the randomized query complexity of classes of bounded approximate
polynomials whose Fourier spectra are concentrated on few subsets. As an
application, we prove new estimates for the number of random queries required
to learn approximate juntas of a given degree, functions with rapidly decaying
Fourier tails and constant depth circuits of given size. Finally, we obtain
bounds for the number of queries required to learn the polynomial class
$\mathscr{F}_{n,d}$ without error in the query and random example models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.08253">Momentum-Based Policy Gradient with Second-Order Information. (arXiv:2205.08253v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1">Saber Salehkaleybar</a>, <a href="http://arxiv.org/find/cs/1/au:+Khorasani_S/0/1/0/all/0/1">Sadegh Khorasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1">Negar Kiyavash</a>, <a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1">Niao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1">Patrick Thiran</a></p>
<p>Variance-reduced gradient estimators for policy gradient methods have been
one of the main focus of research in the reinforcement learning in recent years
as they allow acceleration of the estimation process. We propose a
variance-reduced policy-gradient method, called SHARP, which incorporates
second-order information into stochastic gradient descent (SGD) using momentum
with a time-varying learning rate. SHARP algorithm is parameter-free, achieving
$\epsilon$-approximate first-order stationary point with $O(\epsilon^{-3})$
number of trajectories, while using a batch size of $O(1)$ at each iteration.
Unlike most previous work, our proposed algorithm does not require importance
sampling which can compromise the advantage of variance reduction process.
Moreover, the variance of estimation error decays with the fast rate of
$O(1/t^{2/3})$ where $t$ is the number of iterations. Our extensive
experimental evaluations show the effectiveness of the proposed algorithm on
various control tasks and its advantage over the state of the art in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.10089">Kernel Normalized Convolutional Networks. (arXiv:2205.10089v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1">Reza Nasirigerdeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1">Reihaneh Torkzadehmahani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a></p>
<p>Existing convolutional neural network architectures frequently rely upon
batch normalization (BatchNorm) to effectively train the model. BatchNorm,
however, performs poorly with small batch sizes, and is inapplicable to
differential privacy. To address these limitations, we propose kernel
normalization and kernel normalized convolutional layers, and incorporate them
into kernel normalized convolutional networks (KNConvNets) as the main building
blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets
while forgoing BatchNorm layers. Through extensive experiments, we illustrate
KNConvNets achieve higher or competitive performance compared to the BatchNorm
counterparts in image classification and semantic segmentation. They also
significantly outperform their batch-independent competitors including layer
and group normalization in non-private and differentially private training.
Given that, KNConvNets combine the batch-independence property of layer and
group normalization with the performance advantage of BatchNorm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.10952">Analysis of functional neural codes of deep learning models: Functional Telescope Hypothesis. (arXiv:2205.10952v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jung Hoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Vijayan_S/0/1/0/all/0/1">Sujith Vijayan</a></p>
<p>Deep neural networks (DNNs), the agents of deep learning (DL), require a
massive number of parallel/sequential operations. This makes it difficult to
comprehend DNNs' operations and impedes proper diagnosis. Without better
knowledge of their internal process, deploying DNNs in high-stakes domains can
lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be
deployed in high-stakes real-world problems, it is imperative that we gain
insights into DNNs' internal operations underlying their decision-making. Here,
we use the self-organizing map (SOM) to analyze DL models' internal codes
associated with DNNs' decision-making. Our analyses suggest that shallow layers
close to the input layer compress features into condensed space and that deep
layers close to the output layer expand feature space. We also found evidence
indicating that compressed features may underlie DNNs' vulnerabilities to
adversarial perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.13748">Auto-PINN: Understanding and Optimizing Physics-Informed Neural Architecture. (arXiv:2205.13748v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yicheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaotian Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chia-Yuan Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Braga_Neto_U/0/1/0/all/0/1">Ulisses Braga-Neto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a></p>
<p>Physics-informed neural networks (PINNs) are revolutionizing science and
engineering practice by bringing together the power of deep learning to bear on
scientific computation. In forward modeling problems, PINNs are meshless
partial differential equation (PDE) solvers that can handle irregular,
high-dimensional physical domains. Naturally, the neural architecture
hyperparameters have a large impact on the efficiency and accuracy of the PINN
solver. However, this remains an open and challenging problem because of the
large search space and the difficulty of identifying a proper search objective
for PDEs. Here, we propose Auto-PINN, the first systematic, automated
hyperparameter optimization approach for PINNs, which employs Neural
Architecture Search (NAS) techniques to PINN design. Auto-PINN avoids manually
or exhaustively searching the hyperparameter space associated with PINNs. A
comprehensive set of pre-experiments using standard PDE benchmarks allows us to
probe the structure-performance relationship in PINNs. We find that the
different hyperparameters can be decoupled, and that the training loss function
of PINNs is a good search objective. Comparison experiments with baseline
methods demonstrate that Auto-PINN produces neural architectures with superior
stability and accuracy over alternative baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15580">A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting. (arXiv:2205.15580v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyurin_A/0/1/0/all/0/1">Alexander Tyurin</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a></p>
<p>We present a new method that includes three key components of distributed
optimization and federated learning: variance reduction of stochastic
gradients, partial participation, and compressed communication. We prove that
the new method has optimal oracle complexity and state-of-the-art communication
complexity in the partial participation setting. Regardless of the
communication compression feature, our method successfully combines variance
reduction and partial participation: we get the optimal oracle complexity,
never need the participation of all nodes, and do not require the bounded
gradients (dissimilarity) assumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.06817">Residual-based physics-informed transfer learning: A hybrid method for accelerating long-term CFD simulations via deep learning. (arXiv:2206.06817v3 [physics.flu-dyn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Jeon_J/0/1/0/all/0/1">Joongoo Jeon</a>, <a href="http://arxiv.org/find/physics/1/au:+Lee_J/0/1/0/all/0/1">Juhyeong Lee</a>, <a href="http://arxiv.org/find/physics/1/au:+Vinuesa_R/0/1/0/all/0/1">Ricardo Vinuesa</a>, <a href="http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1">Sung Joong Kim</a></p>
<p>While a big wave of artificial intelligence (AI) has propagated to the field
of computational fluid dynamics (CFD) acceleration studies, recent research has
highlighted that the development of AI techniques that reconciles the following
goals remains our primary task: (1) accurate prediction of unseen (future) time
series in long-term CFD simulations (2) acceleration of simulations (3) an
acceptable amount of training data and time (4) within a multiple PDEs
condition. In this study, we propose a residual-based physics-informed transfer
learning (RePIT) strategy to achieve these four objectives using ML-CFD hybrid
computation. Our hypothesis is that long-term CFD simulation is feasible with
the hybrid method where CFD and AI alternately calculate time series while
monitoring the first principle's residuals. The feasibility of RePIT strategy
was verified through a CFD case study on natural convection. In a single
training approach, a residual scale change occurred around 100th timestep,
resulting in predicted time series exhibiting non-physical patterns as well as
a significant deviations from the ground truth. Conversely, RePIT strategy
maintained the residuals within the defined range and demonstrated good
accuracy throughout the entire simulation period. The maximum error from the
ground truth was below 0.4 K for temperature and 0.024 m/s for x-axis velocity.
Furthermore, the average time for 1 timestep by the ML-GPU and CFD-CPU
calculations was 0.171 s and 0.015 s, respectively. Including the
parameter-updating time, the simulation was accelerated by a factor of 1.9. In
conclusion, our RePIT strategy is a promising technique to reduce the cost of
CFD simulations in industry. However, more vigorous optimization and
improvement studies are still necessary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.07417">Near-Linear Time and Fixed-Parameter Tractable Algorithms for Tensor Decompositions. (arXiv:2207.07417v3 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahankali_A/0/1/0/all/0/1">Arvind V. Mahankali</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyu Zhang</a></p>
<p>We study low rank approximation of tensors, focusing on the tensor train and
Tucker decompositions, as well as approximations with tree tensor networks and
more general tensor networks. For tensor train decomposition, we give a
bicriteria $(1 + \eps)$-approximation algorithm with a small bicriteria rank
and $O(q \cdot \nnz(A))$ running time, up to lower order terms, which improves
over the additive error algorithm of \cite{huber2017randomized}. We also show
how to convert the algorithm of \cite{huber2017randomized} into a relative
error algorithm, but their algorithm necessarily has a running time of $O(qr^2
\cdot \nnz(A)) + n \cdot \poly(qk/\eps)$ when converted to a $(1 +
\eps)$-approximation algorithm with bicriteria rank $r$. To the best of our
knowledge, our work is the first to achieve polynomial time relative error
approximation for tensor train decomposition. Our key technique is a method for
obtaining subspace embeddings with a number of rows polynomial in $q$ for a
matrix which is the flattening of a tensor train of $q$ tensors. We extend our
algorithm to tree tensor networks. In addition, we extend our algorithm to
tensor networks with arbitrary graphs (which we refer to as general tensor
networks), by using a result of
\cite{ms08_simulating_quantum_tensor_contraction} and showing that a general
tensor network of rank $k$ can be contracted to a binary tree network of rank
$k^{O(\deg(G)\tw(G))}$, allowing us to reduce to the case of tree tensor
networks. Finally, we give new fixed-parameter tractable algorithms for the
tensor train, Tucker, and CP decompositions, which are simpler than those of
\cite{swz19_tensor_low_rank} since they do not make use of polynomial system
solvers. Our technique of Gaussian subspace embeddings with exactly $k$ rows
(and thus exponentially small success probability) may be of independent
interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06991">Towards Interpretable Sleep Stage Classification Using Cross-Modal Transformers. (arXiv:2208.06991v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pradeepkumar_J/0/1/0/all/0/1">Jathurshan Pradeepkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandakumar_M/0/1/0/all/0/1">Mithunjha Anandakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugathasan_V/0/1/0/all/0/1">Vinith Kugathasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Suntharalingham_D/0/1/0/all/0/1">Dhinesh Suntharalingham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kappel_S/0/1/0/all/0/1">Simon L. Kappel</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1">Anjula C. De Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Edussooriya_C/0/1/0/all/0/1">Chamira U. S. Edussooriya</a></p>
<p>Accurate sleep stage classification is significant for sleep health
assessment. In recent years, several machine-learning based sleep staging
algorithms have been developed , and in particular, deep-learning based
algorithms have achieved performance on par with human annotation. Despite
improved performance, a limitation of most deep-learning based algorithms is
their black-box behavior, which have limited their use in clinical settings.
Here, we propose a cross-modal transformer, which is a transformer-based method
for sleep stage classification. The proposed cross-modal transformer consists
of a novel cross-modal transformer encoder architecture along with a
multi-scale one-dimensional convolutional neural network for automatic
representation learning. Our method outperforms the state-of-the-art methods
and eliminates the black-box behavior of deep-learning models by utilizing the
interpretability aspect of the attention modules. Furthermore, our method
provides considerable reductions in the number of parameters and training time
compared to the state-of-the-art methods. Our code is available at
https://github.com/Jathurshan0330/Cross-Modal-Transformer. A demo of our work
can be found at https://bit.ly/Cross_modal_transformer_demo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.14362">AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels. (arXiv:2208.14362v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1">Nicholas Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xintong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tzu-Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1">Dyah Adila</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoenberg_S/0/1/0/all/0/1">Spencer Schoenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pick_L/0/1/0/all/0/1">Lauren Pick</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haotian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1">Aws Albarghouthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a></p>
<p>Weak supervision (WS) is a powerful method to build labeled datasets for
training supervised models in the face of little-to-no labeled data. It
replaces hand-labeling data with aggregating multiple noisy-but-cheap label
estimates expressed by labeling functions (LFs). While it has been used
successfully in many domains, weak supervision's application scope is limited
by the difficulty of constructing labeling functions for domains with complex
or high-dimensional features. To address this, a handful of methods have
proposed automating the LF design process using a small set of ground truth
labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating
automated WS (AutoWS) techniques in challenging WS settings -- a set of diverse
application domains on which it has been previously difficult or impossible to
apply traditional WS techniques. While AutoWS is a promising direction toward
expanding the application-scope of WS, the emergence of powerful methods such
as zero-shot foundation models reveals the need to understand how AutoWS
techniques compare or cooperate with modern zero-shot or few-shot learners.
This informs the central question of AutoWS-Bench-101: given an initial set of
100 labels for each task, we ask whether a practitioner should use an AutoWS
method to generate additional labels or use some simpler baseline, such as
zero-shot predictions from a foundation model or supervised learning. We
observe that in many settings, it is necessary for AutoWS methods to
incorporate signal from foundation models if they are to outperform simple
few-shot baselines, and AutoWS-Bench-101 promotes future research in this
direction. We conclude with a thorough ablation study of AutoWS methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.00462">MA-RECON: Mask-aware deep-neural-network for robust fast MRI k-space interpolation. (arXiv:2209.00462v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Avidan_N/0/1/0/all/0/1">Nitzan Avidan</a>, <a href="http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1">Moti Freiman</a></p>
<p>High-quality reconstruction of MRI images from under-sampled `k-space' data,
which is in the Fourier domain, is crucial for shortening MRI acquisition times
and ensuring superior temporal resolution. Over recent years, a wealth of deep
neural network (DNN) methods have emerged, aiming to tackle the complex,
ill-posed inverse problem linked to this process. However, their instability
against variations in the acquisition process and anatomical distribution
exposes a deficiency in the generalization of relevant physical models within
these DNN architectures. The goal of our work is to enhance the generalization
capabilities of DNN methods for k-space interpolation by introducing
`MA-RECON', an innovative mask-aware DNN architecture and associated training
method. Unlike preceding approaches, our `MA-RECON' architecture encodes not
only the observed data but also the under-sampling mask within the model
structure. It implements a tailored training approach that leverages data
generated with a variety of under-sampling masks to stimulate the model's
generalization of the under-sampled MRI reconstruction problem. Therefore,
effectively represents the associated inverse problem, akin to the classical
compressed sensing approach. The benefits of our MA-RECON approach were
affirmed through rigorous testing with the widely accessible fastMRI dataset.
Compared to standard DNN methods and DNNs trained with under-sampling mask
augmentation, our approach demonstrated superior generalization capabilities.
This resulted in a considerable improvement in robustness against variations in
both the acquisition process and anatomical distribution, especially in regions
with pathology. In conclusion, our mask-aware strategy holds promise for
enhancing the generalization capacity and robustness of DNN-based methodologies
for MRI reconstruction from undersampled k-space data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03829">Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1">Seyed Mojtaba Marvasti-Zadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1">Devin Goodsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1">Nilanjan Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1">Nadir Erbilgin</a></p>
<p>This paper provides a comprehensive review of past and current advances in
the early detection of bark beetle-induced tree mortality from three primary
perspectives: bark beetle &amp; host interactions, RS, and ML/DL. In contrast to
prior efforts, this review encompasses all RS systems and emphasizes ML/DL
methods to investigate their strengths and weaknesses. We parse existing
literature based on multi- or hyper-spectral analyses and distill their
knowledge based on: bark beetle species &amp; attack phases with a primary emphasis
on early stages of attacks, host trees, study regions, RS platforms &amp; sensors,
spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation
indices (SVIs), ML approaches, learning schemes, task categories, models,
algorithms, classes/clusters, features, and DL networks &amp; architectures.
Although DL-based methods and the random forest (RF) algorithm showed promising
results, highlighting their potential to detect subtle changes across visible,
thermal, and short-wave infrared (SWIR) spectral regions, they still have
limited effectiveness and high uncertainties. To inspire novel solutions to
these shortcomings, we delve into the principal challenges &amp; opportunities from
different perspectives, enabling a deeper understanding of the current state of
research and guiding future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06462">Self-Guided Diffusion Models. (arXiv:2210.06462v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1">Vincent Tao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">David W Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki M. Asano</a>, <a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1">Gertjan J. Burghouts</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G. M. Snoek</a></p>
<p>Diffusion models have demonstrated remarkable progress in image generation
quality, especially when guidance is used to control the generative process.
However, guidance requires a large amount of image-annotation pairs for
training and is thus dependent on their availability, correctness and
unbiasedness. In this paper, we eliminate the need for such annotation by
instead leveraging the flexibility of self-supervision signals to design a
framework for self-guided diffusion models. By leveraging a feature extraction
function and a self-annotation function, our method provides guidance signals
at various image granularities: from the level of holistic images to object
boxes and even segmentation masks. Our experiments on single-label and
multi-label image datasets demonstrate that self-labeled guidance always
outperforms diffusion models without guidance and may even surpass guidance
based on ground-truth labels, especially on unbalanced data. When equipped with
self-supervised box or mask proposals, our method further generates visually
diverse yet semantically consistent images, without the need for any class,
box, or segment label annotation. Self-guided diffusion is simple, flexible and
expected to profit from deployment at scale. Source code will be at:
https://taohu.me/sgdm/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09846">G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System. (arXiv:2210.09846v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1">Aryan Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Rameshan_R/0/1/0/all/0/1">Renu M. Rameshan</a></p>
<p>Navigating dynamic physical environments without obstructing or damaging
human assets is of quintessential importance for social robots. In this work,
we solve autonomous drone navigation's sub-problem of predicting out-of-domain
human and agent trajectories using a deep generative model. Our method:
General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final
Displacement Error (FDE) on 2020's benchmark: PECNet through a combination of
architectural improvements inspired by periodic activation functions and
synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and
Reinforcement Learning (RL). Additionally, we propose a simple
geometry-inspired metric for trajectory non-linearity and outlier detection,
helpful for the task. Code available at
$\href{https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git}{GitHub}$
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00460">Augmentation Invariant Manifold Learning. (arXiv:2211.00460v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1">Shulei Wang</a></p>
<p>Data augmentation is a widely used technique and an essential ingredient in
the recent advance in self-supervised representation learning. By preserving
the similarity between augmented data, the resulting data representation can
improve various downstream analyses and achieve state-of-the-art performance in
many applications. Despite the empirical effectiveness, most existing methods
lack theoretical understanding under a general nonlinear setting. To fill this
gap, we develop a statistical framework on a low-dimension product manifold to
model the data augmentation transformation. Under this framework, we introduce
a new representation learning method called augmentation invariant manifold
learning and design a computationally efficient algorithm by reformulating it
as a stochastic optimization problem. Compared with existing self-supervised
methods, the new method simultaneously exploits the manifold's geometric
structure and invariant property of augmented data and has an explicit
theoretical guarantee. Our theoretical investigation characterizes the role of
data augmentation in the proposed method and reveals why and how the data
representation learned from augmented data can improve the $k$-nearest neighbor
classifier in the downstream analysis, showing that a more complex data
augmentation leads to more improvement in downstream analysis. Finally,
numerical experiments on simulated and real datasets are presented to
demonstrate the merit of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04686">Directional Privacy for Deep Learning. (arXiv:2211.04686v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Faustini_P/0/1/0/all/0/1">Pedro Faustini</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandes_N/0/1/0/all/0/1">Natasha Fernandes</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonni_S/0/1/0/all/0/1">Shakila Tonni</a>, <a href="http://arxiv.org/find/cs/1/au:+McIver_A/0/1/0/all/0/1">Annabelle McIver</a>, <a href="http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1">Mark Dras</a></p>
<p>Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method
for applying privacy in the training of deep learning models. It applies
isotropic Gaussian noise to gradients during training, which can perturb these
gradients in any direction, damaging utility. Metric DP, however, can provide
alternative mechanisms based on arbitrary metrics that might be more suitable
for preserving utility. In this paper, we apply \textit{directional privacy},
via a mechanism based on the von Mises-Fisher (VMF) distribution, to perturb
gradients in terms of \textit{angular distance} so that gradient direction is
broadly preserved. We show that this provides both $\epsilon$-DP and $\epsilon
d$-privacy for deep learning training, rather than the $(\epsilon,
\delta)$-privacy of the Gaussian mechanism. Experiments on key datasets then
indicate that the VMF mechanism can outperform the Gaussian in the
utility-privacy trade-off. In particular, our experiments provide a direct
empirical comparison of privacy between the two approaches in terms of their
ability to defend against reconstruction and membership inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14309">FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations. (arXiv:2211.14309v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diller_C/0/1/0/all/0/1">Christian Diller</a>, <a href="http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1">Thomas Funkhouser</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Angela Dai</a></p>
<p>We present a generative approach to forecast long-term future human behavior
in 3D, requiring only weak supervision from readily available 2D human action
data. This is a fundamental task enabling many downstream applications. The
required ground-truth data is hard to capture in 3D (mocap suits, expensive
setups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our
method to only require 2D RGB data while being able to generate 3D human motion
sequences. We use a differentiable 2D projection scheme in an autoregressive
manner for weak supervision, and an adversarial loss for 3D regularization. Our
method predicts long and complex behavior sequences (e.g. cooking, assembly)
consisting of multiple sub-actions. We tackle this in a semantically
hierarchical manner, jointly predicting high-level coarse action labels
together with their low-level fine-grained realizations as characteristic 3D
human poses. We observe that these two action representations are coupled in
nature, and joint prediction benefits both action and pose forecasting. Our
experiments demonstrate the complementary nature of joint action and 3D pose
prediction: our joint approach outperforms each task treated individually,
enables robust longer-term sequence prediction, and outperforms alternative
approaches to forecast actions and characteristic 3D poses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14400">Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1">Jonathan W. Siegel</a></p>
<p>Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the
problem of how efficiently, in terms of the number of parameters, deep neural
networks with the ReLU activation function can approximate functions in the
Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with
error measured in the $L_p(\Omega)$ norm. This problem is important when
studying the application of neural networks in a variety of fields, including
scientific computing and signal processing, and has previously been solved only
when $p=q=\infty$. Our contribution is to provide a complete solution for all
$1\leq p,q\leq \infty$ and $s &gt; 0$ for which the corresponding Sobolev or Besov
space compactly embeds into $L_p$. The key technical tool is a novel
bit-extraction technique which gives an optimal encoding of sparse vectors.
This enables us to obtain sharp upper bounds in the non-linear regime where $p
&gt; q$. We also provide a novel method for deriving $L_p$-approximation lower
bounds based upon VC-dimension when $p &lt; \infty$. Our results show that very
deep ReLU networks significantly outperform classical methods of approximation
in terms of the number of parameters, but that this comes at the cost of
parameters which are not encodable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.12015">Stochastic analysis of the Elo rating algorithm in round-robin tournaments. (arXiv:2212.12015v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zanco_D/0/1/0/all/0/1">Daniel Gomes de Pinho Zanco</a>, <a href="http://arxiv.org/find/cs/1/au:+Szczecinski_L/0/1/0/all/0/1">Leszek Szczecinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhn_E/0/1/0/all/0/1">Eduardo Vinicius Kuhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Seara_R/0/1/0/all/0/1">Rui Seara</a></p>
<p>The Elo algorithm, renowned for its simplicity, is widely used for rating in
sports tournaments and other applications. However, despite its widespread use,
a detailed understanding of the convergence characteristics of the Elo
algorithm is still lacking. Aiming to fill this gap, this paper presents a
comprehensive (stochastic) analysis of the Elo algorithm, considering
round-robin tournaments. Specifically, analytical expressions are derived
describing the evolution of the skills and performance metrics. Then, taking
into account the relationship between the behavior of the algorithm and the
step-size value, which is a hyperparameter that can be controlled, design
guidelines and discussions about the performance of the algorithm are provided.
Experimental results are shown confirming the accuracy of the analysis and
illustrating the applicability of the theoretical findings using real-world
data obtained from SuperLega, the Italian volleyball league.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06732">Predictive Modeling of Coronal Hole Areas Using Long Short-Term Memory Networks. (arXiv:2301.06732v6 [astro-ph.SR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Yun_J/0/1/0/all/0/1">Juyoung Yun</a></p>
<p>In the era of space exploration, the implications of space weather have
become increasingly evident. Central to this is the phenomenon of coronal
holes, which can significantly influence the functioning of satellites and
aircraft. These coronal holes, present on the sun, are distinguished by their
open magnetic field lines and comparatively cooler temperatures, leading to the
emission of solar winds at heightened rates. To anticipate the effects of these
coronal holes on Earth, our study harnesses computer vision to pinpoint the
coronal hole regions and estimate their dimensions using imagery from the Solar
Dynamics Observatory (SDO). Further, we deploy deep learning methodologies,
specifically the Long Short-Term Memory (LSTM) approach, to analyze the trends
in the data related to the area of the coronal holes and predict their
dimensions across various solar regions over a span of seven days. By
evaluating the time series data concerning the area of the coronal holes, our
research seeks to uncover patterns in the behavior of coronal holes and
comprehend their potential influence on space weather occurrences. This
investigation marks a pivotal stride towards bolstering our capacity to
anticipate and brace for space weather events that could have ramifications for
Earth and its technological apparatuses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01486">Understanding plasticity in neural networks. (arXiv:2303.01486v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyle_C/0/1/0/all/0/1">Clare Lyle</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zeyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikishin_E/0/1/0/all/0/1">Evgenii Nikishin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pires_B/0/1/0/all/0/1">Bernardo Avila Pires</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1">Razvan Pascanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabney_W/0/1/0/all/0/1">Will Dabney</a></p>
<p>Plasticity, the ability of a neural network to quickly change its predictions
in response to new information, is essential for the adaptability and
robustness of deep reinforcement learning systems. Deep neural networks are
known to lose plasticity over the course of training even in relatively simple
learning problems, but the mechanisms driving this phenomenon are still poorly
understood. This paper conducts a systematic empirical analysis into plasticity
loss, with the goal of understanding the phenomenon mechanistically in order to
guide the future development of targeted solutions. We find that loss of
plasticity is deeply connected to changes in the curvature of the loss
landscape, but that it often occurs in the absence of saturated units. Based on
this insight, we identify a number of parameterization and optimization design
choices which enable networks to better preserve plasticity over the course of
training. We validate the utility of these findings on larger-scale RL
benchmarks in the Arcade Learning Environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03092">Environment Invariant Linear Least Squares. (arXiv:2303.03092v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Fan_J/0/1/0/all/0/1">Jianqing Fan</a>, <a href="http://arxiv.org/find/math/1/au:+Fang_C/0/1/0/all/0/1">Cong Fang</a>, <a href="http://arxiv.org/find/math/1/au:+Gu_Y/0/1/0/all/0/1">Yihong Gu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>This paper considers a multi-environment linear regression model in which
data from multiple experimental settings are collected. The joint distribution
of the response variable and covariates may vary across different environments,
yet the conditional expectations of $y$ given the unknown set of important
variables are invariant. Such a statistical model is related to the problem of
endogeneity, causal inference, and transfer learning. The motivation behind it
is illustrated by how the goals of prediction and attribution are inherent in
estimating the true parameter and the important variable set. We construct a
novel environment invariant linear least squares (EILLS) objective function, a
multi-environment version of linear least-squares regression that leverages the
above conditional expectation invariance structure and heterogeneity among
different environments to determine the true parameter. Our proposed method is
applicable without any additional structural knowledge and can identify the
true parameter under a near-minimal identification condition. We establish
non-asymptotic $\ell_2$ error bounds on the estimation error for the EILLS
estimator in the presence of spurious variables. Moreover, we further show that
the $\ell_0$ penalized EILLS estimator can achieve variable selection
consistency in high-dimensional regimes. These non-asymptotic results
demonstrate the sample efficiency of the EILLS estimator and its capability to
circumvent the curse of endogeneity in an algorithmic manner without any prior
structural knowledge. To the best of our knowledge, this paper is the first to
realize statistically efficient invariance learning in the general linear
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08112">Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belrose_N/0/1/0/all/0/1">Nora Belrose</a>, <a href="http://arxiv.org/find/cs/1/au:+Furman_Z/0/1/0/all/0/1">Zach Furman</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1">Logan Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Halawi_D/0/1/0/all/0/1">Danny Halawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostrovsky_I/0/1/0/all/0/1">Igor Ostrovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+McKinney_L/0/1/0/all/0/1">Lev McKinney</a>, <a href="http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1">Stella Biderman</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1">Jacob Steinhardt</a></p>
<p>We analyze transformers from the perspective of iterative inference, seeking
to understand how model predictions are refined layer by layer. To do so, we
train an affine probe for each block in a frozen pretrained model, making it
possible to decode every hidden state into a distribution over the vocabulary.
Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit
lens'' technique, which yielded useful insights but is often brittle.
</p>
<p>We test our method on various autoregressive language models with up to 20B
parameters, showing it to be more predictive, reliable and unbiased than the
logit lens. With causal experiments, we show the tuned lens uses similar
features to the model itself. We also find the trajectory of latent predictions
can be used to detect malicious inputs with high accuracy. All code needed to
reproduce our results can be found at
https://github.com/AlignmentResearch/tuned-lens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10594">AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (arXiv:2303.10594v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1">Lijun Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a></p>
<p>Model adaptation aims at solving the domain transfer problem under the
constraint of only accessing the pretrained source models. With the increasing
considerations of data privacy and transmission efficiency, this paradigm has
been gaining recent popularity. This paper studies the vulnerability to
universal attacks transferred from the source domain during model adaptation
algorithms due to the existence of malicious providers. We explore both
universal adversarial perturbations and backdoor attacks as loopholes on the
source side and discover that they still survive in the target models after
adaptation. To address this issue, we propose a model preprocessing framework,
named AdaptGuard, to improve the security of model adaptation algorithms.
AdaptGuard avoids direct use of the risky source parameters through knowledge
distillation and utilizes the pseudo adversarial samples under adjusted radius
to enhance the robustness. AdaptGuard is a plug-and-play module that requires
neither robust pretrained models nor any changes for the following model
adaptation algorithms. Extensive results on three commonly used datasets and
two popular adaptation methods validate that AdaptGuard can effectively defend
against universal attacks and maintain clean accuracy in the target domain
simultaneously. We hope this research will shed light on the safety and
robustness of transfer learning. Code is available at
https://github.com/TomSheng21/AdaptGuard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16585">Quantum Deep Hedging. (arXiv:2303.16585v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Cherrat_E/0/1/0/all/0/1">El Amine Cherrat</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Raj_S/0/1/0/all/0/1">Snehal Raj</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kerenidis_I/0/1/0/all/0/1">Iordanis Kerenidis</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Shekhar_A/0/1/0/all/0/1">Abhishek Shekhar</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wood_B/0/1/0/all/0/1">Ben Wood</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Dee_J/0/1/0/all/0/1">Jon Dee</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chakrabarti_S/0/1/0/all/0/1">Shouvanik Chakrabarti</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_R/0/1/0/all/0/1">Richard Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Herman_D/0/1/0/all/0/1">Dylan Herman</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Hu_S/0/1/0/all/0/1">Shaohan Hu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Minssen_P/0/1/0/all/0/1">Pierre Minssen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Shaydulin_R/0/1/0/all/0/1">Ruslan Shaydulin</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sun_Y/0/1/0/all/0/1">Yue Sun</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yalovetzky_R/0/1/0/all/0/1">Romina Yalovetzky</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pistoia_M/0/1/0/all/0/1">Marco Pistoia</a></p>
<p>Quantum machine learning has the potential for a transformative impact across
industry sectors and in particular in finance. In our work we look at the
problem of hedging where deep reinforcement learning offers a powerful
framework for real markets. We develop quantum reinforcement learning methods
based on policy-search and distributional actor-critic algorithms that use
quantum neural network architectures with orthogonal and compound layers for
the policy and value functions. We prove that the quantum neural networks we
use are trainable, and we perform extensive simulations that show that quantum
models can reduce the number of trainable parameters while achieving comparable
performance and that the distributional approach obtains better performance
than other standard approaches, both classical and quantum. We successfully
implement the proposed models on a trapped-ion quantum processor, utilizing
circuits with up to $16$ qubits, and observe performance that agrees well with
noiseless simulation. Our quantum techniques are general and can be applied to
other reinforcement learning problems beyond hedging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00553">From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong-Lu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zehao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1">Yiming Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yikun Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Jingru Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xudong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cewu Lu</a></p>
<p>As a vital step toward the intelligent agent, Action understanding matters
for intelligent agents and has attracted long-term attention. It can be formed
as the mapping from the action physical space to the semantic space. Typically,
researchers built action datasets according to idiosyncratic choices to define
classes and push the envelope of benchmarks respectively. Thus, datasets are
incompatible with each other like "Isolated Islands" due to semantic gaps and
various class granularities, e.g., do housework in dataset A and wash plate in
dataset B. We argue that a more principled semantic space is an urgent need to
concentrate the community efforts and enable us to use all datasets together to
pursue generalizable action learning. To this end, we design a structured
action semantic space in view of verb taxonomy hierarchy and covering massive
actions. By aligning the classes of previous datasets to our semantic space, we
gather (image/video/skeleton/MoCap) datasets into a unified database in a
unified label system, i.e., bridging ``isolated islands'' into a "Pangea".
Accordingly, we propose a novel model mapping from the physical space to
semantic space to fully use Pangea. In extensive experiments, our new system
shows significant superiority, especially in transfer learning. Code and data
will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01203">Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tongzhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Amy Zhang</a></p>
<p>In goal-reaching reinforcement learning (RL), the optimal value function has
a particular geometry, called quasimetric structure. This paper introduces
Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes
quasimetric models to learn optimal value functions. Distinct from prior
approaches, the QRL objective is specifically designed for quasimetrics, and
provides strong theoretical recovery guarantees. Empirically, we conduct
thorough analyses on a discretized MountainCar environment, identifying
properties of QRL and its advantages over alternatives. On offline and online
goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and
performance, across both state-based and image-based observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02858">A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Azal Ahmad Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_O/0/1/0/all/0/1">Omkar Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1">Rohitash Chandra</a></p>
<p>Class imbalance (CI) in classification problems arises when the number of
observations belonging to one class is lower than the other. Ensemble learning
combines multiple models to obtain a robust model and has been prominently used
with data augmentation methods to address class imbalance problems. In the last
decade, a number of strategies have been added to enhance ensemble learning and
data augmentation methods, along with new methods such as generative
adversarial networks (GANs). A combination of these has been applied in many
studies, and the evaluation of different combinations would enable a better
understanding and guidance for different application domains. In this paper, we
present a computational study to evaluate data augmentation and ensemble
learning methods used to address prominent benchmark CI problems. We present a
general framework that evaluates 9 data augmentation and 9 ensemble learning
methods for CI problems. Our objective is to identify the most effective
combination for improving classification performance on imbalanced datasets.
The results indicate that combinations of data augmentation methods with
ensemble learning can significantly improve classification performance on
imbalanced datasets. We find that traditional data augmentation methods such as
the synthetic minority oversampling technique (SMOTE) and random oversampling
(ROS) are not only better in performance for selected CI problems, but also
computationally less expensive than GANs. Our study is vital for the
development of novel models for handling imbalanced datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04258">A Note on &quot;Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms&quot;. (arXiv:2304.04258v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a></p>
<p>Data valuation is a growing research field that studies the influence of
individual data points for machine learning (ML) models. Data Shapley, inspired
by cooperative game theory and economics, is an effective method for data
valuation. However, it is well-known that the Shapley value (SV) can be
computationally expensive. Fortunately, Jia et al. (2019) showed that for
K-Nearest Neighbors (KNN) models, the computation of Data Shapley is
surprisingly simple and efficient.
</p>
<p>In this note, we revisit the work of Jia et al. (2019) and propose a more
natural and interpretable utility function that better reflects the performance
of KNN models. We derive the corresponding calculation procedure for the Data
Shapley of KNN classifiers/regressors with the new utility functions. Our new
approach, dubbed soft-label KNN-SV, achieves the same time complexity as the
original method. We further provide an efficient approximation algorithm for
soft-label KNN-SV based on locality sensitive hashing (LSH). Our experimental
results demonstrate that Soft-label KNN-SV outperforms the original method on
most datasets in the task of mislabeled data detection, making it a better
baseline for future work on data valuation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06026">Uncertainty in GNN Learning Evaluations: The Importance of a Consistent Benchmark for Community Detection. (arXiv:2305.06026v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leeney_W/0/1/0/all/0/1">William Leeney</a>, <a href="http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1">Ryan McConville</a></p>
<p>Graph Neural Networks (GNNs) have improved unsupervised community detection
of clustered nodes due to their ability to encode the dual dimensionality of
the connectivity and feature information spaces of graphs. Identifying the
latent communities has many practical applications from social networks to
genomics. Current benchmarks of real world performance are confusing due to the
variety of decisions influencing the evaluation of GNNs at this task. To
address this, we propose a framework to establish a common evaluation protocol.
We motivate and justify it by demonstrating the differences with and without
the protocol. The W Randomness Coefficient is a metric proposed for assessing
the consistency of algorithm rankings to quantify the reliability of results
under the presence of randomness. We find that by ensuring the same evaluation
criteria is followed, there may be significant differences from the reported
performance of methods at this task, but a more complete evaluation and
comparison of methods is possible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07637">Text2Cohort: Facilitating Intuitive Access to Biomedical Data with Natural Language Cohort Discovery. (arXiv:2305.07637v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1">Pranav Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanhere_A/0/1/0/all/0/1">Adway Kanhere</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1">Paul H. Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1">Vishwa S. Parekh</a></p>
<p>The Imaging Data Commons (IDC) is a cloud-based database that provides
researchers with open access to cancer imaging data, with the goal of
facilitating collaboration. However, cohort discovery within the IDC database
has a significant technical learning curve. Recently, large language models
(LLM) have demonstrated exceptional utility for natural language processing
tasks. We developed Text2Cohort, a LLM-powered toolkit to facilitate
user-friendly natural language cohort discovery in the IDC. Our method
translates user input into IDC queries using grounding techniques and returns
the query's response. We evaluate Text2Cohort on 50 natural language inputs,
from information extraction to cohort discovery. Our toolkit successfully
generated responses with an 88% accuracy and 0.94 F1 score. We demonstrate that
Text2Cohort can enable researchers to discover and curate cohorts on IDC with
high levels of accuracy using natural language in a more intuitive and
user-friendly way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07721">Designing Optimal Behavioral Experiments Using Machine Learning. (arXiv:2305.07721v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valentin_S/0/1/0/all/0/1">Simon Valentin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleinegesse_S/0/1/0/all/0/1">Steven Kleinegesse</a>, <a href="http://arxiv.org/find/cs/1/au:+Bramley_N/0/1/0/all/0/1">Neil R. Bramley</a>, <a href="http://arxiv.org/find/cs/1/au:+Series_P/0/1/0/all/0/1">Peggy Seri&#xe8;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutmann_M/0/1/0/all/0/1">Michael U. Gutmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_C/0/1/0/all/0/1">Christopher G. Lucas</a></p>
<p>Computational models are powerful tools for understanding human cognition and
behavior. They let us express our theories clearly and precisely, and offer
predictions that can be subtle and often counter-intuitive. However, this same
richness and ability to surprise means our scientific intuitions and
traditional tools are ill-suited to designing experiments to test and compare
these models. To avoid these pitfalls and realize the full potential of
computational modeling, we require tools to design experiments that provide
clear answers about what models explain human behavior and the auxiliary
assumptions those models must make. Bayesian optimal experimental design (BOED)
formalizes the search for optimal experimental designs by identifying
experiments that are expected to yield informative data. In this work, we
provide a tutorial on leveraging recent advances in BOED and machine learning
to find optimal experiments for any kind of model that we can simulate data
from, and show how by-products of this procedure allow for quick and
straightforward evaluation of models and their parameters against real
experimental data. As a case study, we consider theories of how people balance
exploration and exploitation in multi-armed bandit decision-making tasks. We
validate the presented approach using simulations and a real-world experiment.
As compared to experimental designs commonly used in the literature, we show
that our optimal designs more efficiently determine which of a set of models
best account for individual human behavior, and more efficiently characterize
behavior given a preferred model. At the same time, formalizing a scientific
question such that it can be adequately addressed with BOED can be challenging
and we discuss several potential caveats and pitfalls that practitioners should
be aware of. We provide code and tutorial notebooks to replicate all analyses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09620">AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction. (arXiv:2305.09620v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junsol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byungkyu Lee</a></p>
<p>Large language models (LLMs) that produce human-like responses have begun to
revolutionize research practices in the social sciences. This paper shows how
we can integrate LLMs and social surveys to accurately predict individual
responses to survey questions that were not asked before. We develop a novel
methodological framework to personalize LLMs by considering the meaning of
survey questions derived from their text, the latent beliefs of individuals
inferred from their response patterns, and the temporal contexts across
different survey periods through fine-tuning LLMs with survey data. Using the
General Social Survey from 1972 to 2021, we show that the fine-tuned model
based on Alpaca-7b can predict individual responses to survey questions that
are partially missing as well as entirely missing. The remarkable prediction
capabilities allow us to fill in missing trends with high confidence and
pinpoint when public attitudes changed, such as the rising support for same-sex
marriage. We discuss practical constraints, socio-demographic representation,
and ethical concerns regarding individual autonomy and privacy when using LLMs
for opinion prediction. This study demonstrates that LLMs and surveys can
mutually enhance each other's capabilities: LLMs broaden survey potential,
while surveys improve the alignment of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11650">Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1">Mingtian Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Hawkins_Hooker_A/0/1/0/all/0/1">Alex Hawkins-Hooker</a>, <a href="http://arxiv.org/find/stat/1/au:+Paige_B/0/1/0/all/0/1">Brooks Paige</a>, <a href="http://arxiv.org/find/stat/1/au:+Barber_D/0/1/0/all/0/1">David Barber</a></p>
<p>Energy-Based Models (EBMs) offer a versatile framework for modeling complex
data distributions. However, training and sampling from EBMs continue to pose
significant challenges. The widely-used Denoising Score Matching (DSM) method
for scalable EBM training suffers from inconsistency issues, causing the energy
model to learn a `noisy' data distribution. In this work, we propose an
efficient sampling framework: (pseudo)-Gibbs sampling with moment matching,
which enables effective sampling from the underlying clean model when given a
`noisy' model that has been well-trained via DSM. We explore the benefits of
our approach compared to related methods and demonstrate how to scale the
method to high-dimensional datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14067">DIVA: A Dirichlet Process Mixtures Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder. (arXiv:2305.14067v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1">Zhenshan Bing</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1">Yuan Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1">Yuqi Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xiaojie Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kai Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1">Alois Knoll</a></p>
<p>Generative model-based deep clustering frameworks excel in classifying
complex data, but are limited in handling dynamic and complex features because
they require prior knowledge of the number of clusters. In this paper, we
propose a nonparametric deep clustering framework that employs an infinite
mixture of Gaussians as a prior. Our framework utilizes a memoized online
variational inference method that enables the "birth" and "merge" moves of
clusters, allowing our framework to cluster data in a "dynamic-adaptive"
manner, without requiring prior knowledge of the number of features. We name
the framework as DIVA, a Dirichlet Process-based Incremental deep clustering
framework via Variational Auto-Encoder. Our framework, which outperforms
state-of-the-art baselines, exhibits superior performance in classifying
complex data with dynamically changing features, particularly in the case of
incremental features. We released our source code implementation at:
https://github.com/Ghiara/diva
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14930">In-Context Impersonation Reveals Large Language Models&#x27; Strengths and Biases. (arXiv:2305.14930v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1">Leonard Salewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaniz_S/0/1/0/all/0/1">Stephan Alaniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rio_Torto_I/0/1/0/all/0/1">Isabel Rio-Torto</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1">Eric Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1">Zeynep Akata</a></p>
<p>In everyday conversations, humans can take on different roles and adapt their
vocabulary to their chosen roles. We explore whether LLMs can take on, that is
impersonate, different roles when they generate text in-context. We ask LLMs to
assume different personas before solving vision and language tasks. We do this
by prefixing the prompt with a persona that is associated either with a social
identity or domain expertise. In a multi-armed bandit task, we find that LLMs
pretending to be children of different ages recover human-like developmental
stages of exploration. In a language-based reasoning task, we find that LLMs
impersonating domain experts perform better than LLMs impersonating non-domain
experts. Finally, we test whether LLMs' impersonations are complementary to
visual information when describing different categories. We find that
impersonation can improve performance: an LLM prompted to be a bird expert
describes birds better than one prompted to be a car expert. However,
impersonation can also uncover LLMs' biases: an LLM prompted to be a man
describes cars better than one prompted to be a woman. These findings
demonstrate that LLMs are capable of taking on diverse roles and that this
in-context impersonation can be used to uncover their hidden strengths and
biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15363">Inverse Preference Learning: Preference-based RL without a Reward Function. (arXiv:2305.15363v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hejna_J/0/1/0/all/0/1">Joey Hejna</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a></p>
<p>Reward functions are difficult to design and often hard to align with human
intent. Preference-based Reinforcement Learning (RL) algorithms address these
problems by learning reward functions from human feedback. However, the
majority of preference-based RL methods na\"ively combine supervised reward
models with off-the-shelf RL algorithms. Contemporary approaches have sought to
improve performance and query complexity by using larger and more complex
reward architectures such as transformers. Instead of using highly complex
architectures, we develop a new and parameter-efficient algorithm, Inverse
Preference Learning (IPL), specifically designed for learning from offline
preference data. Our key insight is that for a fixed policy, the $Q$-function
encodes all information about the reward function, effectively making them
interchangeable. Using this insight, we completely eliminate the need for a
learned reward function. Our resulting algorithm is simpler and more
parameter-efficient. Across a suite of continuous control and robotics
benchmarks, IPL attains competitive performance compared to more complex
approaches that leverage transformer-based and non-Markovian reward functions
while having fewer algorithmic hyperparameters and learned network parameters.
Our code is publicly released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15617">ISLE: An Intelligent Streaming Framework for High-Throughput AI Inference in Medical Imaging. (arXiv:2305.15617v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kulkarni_P/0/1/0/all/0/1">Pranav Kulkarni</a>, <a href="http://arxiv.org/find/eess/1/au:+Garin_S/0/1/0/all/0/1">Sean Garin</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanhere_A/0/1/0/all/0/1">Adway Kanhere</a>, <a href="http://arxiv.org/find/eess/1/au:+Siegel_E/0/1/0/all/0/1">Eliot Siegel</a>, <a href="http://arxiv.org/find/eess/1/au:+Yi_P/0/1/0/all/0/1">Paul H. Yi</a>, <a href="http://arxiv.org/find/eess/1/au:+Parekh_V/0/1/0/all/0/1">Vishwa S. Parekh</a></p>
<p>As the adoption of Artificial Intelligence (AI) systems within the clinical
environment grows, limitations in bandwidth and compute can create
communication bottlenecks when streaming imaging data, leading to delays in
patient care and increased cost. As such, healthcare providers and AI vendors
will require greater computational infrastructure, therefore dramatically
increasing costs. To that end, we developed ISLE, an intelligent streaming
framework for high-throughput, compute- and bandwidth- optimized, and cost
effective AI inference for clinical decision making at scale. In our
experiments, ISLE on average reduced data transmission by 98.02% and decoding
time by 98.09%, while increasing throughput by 2,730%. We show that ISLE
results in faster turnaround times, and reduced overall cost of data,
transmission, and compute, without negatively impacting clinical decision
making using AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16491">SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise. (arXiv:2305.16491v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alomar_A/0/1/0/all/0/1">Abdullah Alomar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahleh_M/0/1/0/all/0/1">Munther Dahleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mann_S/0/1/0/all/0/1">Sean Mann</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Devavrat Shah</a></p>
<p>The well-established practice of time series analysis involves estimating
deterministic, non-stationary trend and seasonality components followed by
learning the residual stochastic, stationary components. Recently, it has been
shown that one can learn the deterministic non-stationary components accurately
using multivariate Singular Spectrum Analysis (mSSA) in the absence of a
correlated stationary component; meanwhile, in the absence of deterministic
non-stationary components, the Autoregressive (AR) stationary component can
also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a
theoretical underpinning of multi-stage learning algorithms involving both
deterministic and stationary components has been absent in the literature
despite its pervasiveness. We resolve this open question by establishing
desirable theoretical guarantees for a natural two-stage algorithm, where mSSA
is first applied to estimate the non-stationary components despite the presence
of a correlated stationary AR component, which is subsequently learned from the
residual time series. We provide a finite-sample forecasting consistency bound
for the proposed algorithm, SAMoSSA, which is data-driven and thus requires
minimal parameter tuning. To establish theoretical guarantees, we overcome
three hurdles: (i) we characterize the spectra of Page matrices of stable AR
processes, thus extending the analysis of mSSA; (ii) we extend the analysis of
AR process identification in the presence of arbitrary bounded perturbations;
(iii) we characterize the out-of-sample or forecasting error, as opposed to
solely considering model identification. Through representative empirical
studies, we validate the superior performance of SAMoSSA compared to existing
baselines. Notably, SAMoSSA's ability to account for AR noise structure yields
improvements ranging from 5% to 37% across various benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17005">Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices. (arXiv:2305.17005v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_K/0/1/0/all/0/1">Kilian Pfeiffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalili_R/0/1/0/all/0/1">Ramin Khalili</a>, <a href="http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1">J&#xf6;rg Henkel</a></p>
<p>Federated learning (FL) is usually performed on resource-constrained edge
devices, e.g., with limited memory for the computation. If the required memory
to train a model exceeds this limit, the device will be excluded from the
training. This can lead to a lower accuracy as valuable data and computation
resources are excluded from training, also causing bias and unfairness. The FL
training process should be adjusted to such constraints. The state-of-the-art
techniques propose training subsets of the FL model at constrained devices,
reducing their resource requirements for training. But these techniques largely
limit the co-adaptation among parameters of the model and are highly
inefficient, as we show: it is actually better to train a smaller (less
accurate) model by the system where all the devices can train the model
end-to-end, than applying such techniques. We propose a new method that enables
successive freezing and training of the parameters of the FL model at devices,
reducing the training's resource requirements at the devices, while still
allowing enough co-adaptation between parameters. We show through extensive
experimental evaluation that our technique greatly improves the accuracy of the
trained model (by 52.4 p.p.) compared with the state of the art, efficiently
aggregating the computation capacity available on distributed devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18434">Parallel Coordinates for Discovery of Interpretable Machine Learning Models. (arXiv:2305.18434v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayes_D/0/1/0/all/0/1">Dustin Hayes</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1">Boris Kovalerchuk</a></p>
<p>This work uses visual knowledge discovery in parallel coordinates to advance
methods of interpretable machine learning. The graphic data representation in
parallel coordinates made the concepts of hypercubes and hyperblocks (HBs)
simple to understand for end users. It is suggested to use mixed and pure
hyperblocks in the proposed data classifier algorithm Hyper. It is shown that
Hyper models generalize decision trees. The algorithm is presented in several
settings and options to discover interactively or automatically overlapping or
non-overlapping hyperblocks. Additionally, the use of hyperblocks in
conjunction with language descriptions of visual patterns is demonstrated. The
benchmark data from the UCI ML repository were used to evaluate the Hyper
algorithm. It enabled the discovery of mixed and pure HBs evaluated using
10-fold cross validation. Connections among hyperblocks, dimension reduction
and visualization have been established. The capability of end users to find
and observe hyperblocks, as well as the ability of side-by-side visualizations
to make patterns evident, are among major advantages ofhyperblock technology
and the Hyper algorithm. A new method to visualize incomplete n-D data with
missing values is proposed, while the traditional parallel coordinates do not
support it. The ability of HBs to better prevent both overgeneralization and
overfitting of data over decision trees is demonstrated as another benefit of
the hyperblocks. The features of VisCanvas 2.0 software tool that implements
Hyper technology are presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19569">Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis. (arXiv:2305.19569v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1">Jong Moon Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>Extensive research has been conducted on fault diagnosis of planetary
gearboxes using vibration signals and deep learning (DL) approaches. However,
DL-based methods are susceptible to the domain shift problem caused by varying
operating conditions of the gearbox. Although domain adaptation and data
synthesis methods have been proposed to overcome such domain shifts, they are
often not directly applicable in real-world situations where only healthy data
is available in the target domain. To tackle the challenge of extreme domain
shift scenarios where only healthy data is available in the target domain, this
paper proposes two novel domain knowledge-informed data synthesis methods
utilizing the health data map (HDMap). The two proposed approaches are referred
to as scaled CutPaste and FaultPaste. The HDMap is used to physically represent
the vibration signal of the planetary gearbox as an image-like matrix, allowing
for visualization of fault-related features. CutPaste and FaultPaste are then
applied to generate faulty samples based on the healthy data in the target
domain, using domain knowledge and fault signatures extracted from the source
domain, respectively. In addition to generating realistic faults, the proposed
methods introduce scaling of fault signatures for controlled synthesis of
faults with various severity levels. A case study is conducted on a planetary
gearbox testbed to evaluate the proposed approaches. The results show that the
proposed methods are capable of accurately diagnosing faults, even in cases of
extreme domain shift, and can estimate the severity of faults that have not
been previously observed in the target domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19575">On the Linear Convergence of Policy Gradient under Hadamard Parameterization. (arXiv:2305.19575v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1">Jiacai Liu</a>, <a href="http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1">Jinchi Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Wei_K/0/1/0/all/0/1">Ke Wei</a></p>
<p>The convergence of deterministic policy gradient under the Hadamard
parameterization is studied in the tabular setting and the linear convergence
of the algorithm is established. To this end, we first show that the error
decreases at an $O(\frac{1}{k})$ rate for all the iterations. Based on this
result, we further show that the algorithm has a faster local linear
convergence rate after $k_0$ iterations, where $k_0$ is a constant that only
depends on the MDP problem and the initialization. To show the local linear
convergence of the algorithm, we have indeed established the contraction of the
sub-optimal probability $b_s^k$ (i.e., the probability of the output policy
$\pi^k$ on non-optimal actions) when $k\ge k_0$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00349">CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception. (arXiv:2306.00349v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiachen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Haizhong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingzhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1">Atul Prakash</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Z. Morley Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a></p>
<p>Perception is crucial in the realm of autonomous driving systems, where
bird's eye view (BEV)-based architectures have recently reached
state-of-the-art performance. The desirability of self-supervised
representation learning stems from the expensive and laborious process of
annotating 2D and 3D data. Although previous research has investigated
pretraining methods for both LiDAR and camera-based 3D object detection, a
unified pretraining framework for multimodal BEV perception is missing. In this
study, we introduce CALICO, a novel framework that applies contrastive
objectives to both LiDAR and camera backbones. Specifically, CALICO
incorporates two stages: point-region contrast (PRC) and region-aware
distillation (RAD). PRC better balances the region- and scene-level
representation learning on the LiDAR modality and offers significant
performance improvement compared to existing methods. RAD effectively achieves
contrastive distillation on our self-trained teacher model. CALICO's efficacy
is substantiated by extensive evaluations on 3D object detection and BEV map
segmentation tasks, where it delivers significant performance improvements.
Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS and
mAP. Moreover, CALICO boosts the robustness of multimodal 3D object detection
against adversarial attacks and corruption. Additionally, our framework can be
tailored to different backbones and heads, positioning it as a promising
approach for multimodal BEV perception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00577">TorchRL: A data-driven decision-making library for PyTorch. (arXiv:2306.00577v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bou_A/0/1/0/all/0/1">Albert Bou</a>, <a href="http://arxiv.org/find/cs/1/au:+Bettini_M/0/1/0/all/0/1">Matteo Bettini</a>, <a href="http://arxiv.org/find/cs/1/au:+Dittert_S/0/1/0/all/0/1">Sebastian Dittert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vikash Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1">Shagun Sodhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaomeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabritiis_G/0/1/0/all/0/1">Gianni De Fabritiis</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_V/0/1/0/all/0/1">Vincent Moens</a></p>
<p>PyTorch has ascended as a premier machine learning framework, yet it lacks a
native and comprehensive library for decision and control tasks suitable for
large development teams dealing with complex real-world data and environments.
To address this issue, we propose TorchRL, a generalistic control library for
PyTorch that provides well-integrated, yet standalone components. We introduce
a new and flexible PyTorch primitive, the TensorDict, which facilitates
streamlined algorithm development across the many branches of Reinforcement
Learning (RL) and control. We provide a detailed description of the building
blocks and an extensive overview of the library across domains and tasks.
Finally, we experimentally demonstrate its reliability and flexibility and show
comparative benchmarks to demonstrate its computational efficiency. TorchRL
fosters long-term support and is publicly available on GitHub for greater
reproducibility and collaboration within the research community. The code is
open-sourced on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01460">ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages. (arXiv:2306.01460v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jesson_A/0/1/0/all/0/1">Andrew Jesson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1">Gunshi Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Filos_A/0/1/0/all/0/1">Angelos Filos</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Nicolaus Foerster</a>, <a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1">Yarin Gal</a></p>
<p>This paper introduces an effective and practical step toward approximate
Bayesian inference in on-policy actor-critic deep reinforcement learning. This
step manifests as three simple modifications to the Asynchronous Advantage
Actor-Critic (A3C) algorithm: (1) applying a ReLU function to advantage
estimates, (2) spectral normalization of actor-critic weights, and (3)
incorporating dropout as a Bayesian approximation. We prove under standard
assumptions that restricting policy updates to positive advantages optimizes
for value by maximizing a lower bound on the value function plus an additive
term. We show that the additive term is bounded proportional to the Lipschitz
constant of the value function, which offers theoretical grounding for spectral
normalization of critic weights. Finally, our application of dropout
corresponds to approximate Bayesian inference over both the actor and critic
parameters, which enables prudent state-aware exploration around the modes of
the actor via Thompson sampling. Extensive empirical evaluations on diverse
benchmarks reveal the superior performance of our approach compared to existing
on- and off-policy algorithms. We demonstrate significant improvements for
median and interquartile mean metrics over PPO, SAC, and TD3 on the MuJoCo
continuous control benchmark. Moreover, we see improvement over PPO in the
challenging ProcGen generalization benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05734">DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework. (arXiv:2306.05734v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Sheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huanyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weijie J. Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Milan Shen</a></p>
<p>Hyperparameter optimization, also known as hyperparameter tuning, is a widely
recognized technique for improving model performance. Regrettably, when
training private ML models, many practitioners often overlook the privacy risks
associated with hyperparameter optimization, which could potentially expose
sensitive information about the underlying dataset. Currently, the sole
existing approach to allow privacy-preserving hyperparameter optimization is to
uniformly and randomly select hyperparameters for a number of runs,
subsequently reporting the best-performing hyperparameter. In contrast, in
non-private settings, practitioners commonly utilize ``adaptive''
hyperparameter optimization methods such as Gaussian process-based
optimization, which select the next candidate based on information gathered
from previous outputs. This substantial contrast between private and
non-private hyperparameter optimization underscores a critical concern. In our
paper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private
hyperparameter optimization, aiming to bridge the gap between private and
non-private hyperparameter optimization. To accomplish this, we provide a
comprehensive differential privacy analysis of our framework. Furthermore, we
empirically demonstrate the effectiveness of DP-HyPO on a diverse set of
real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06517">Probabilistic Multi-Dimensional Classification. (arXiv:2306.06517v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Vu-Linh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Campos_C/0/1/0/all/0/1">Cassio de Campos</a></p>
<p>Multi-dimensional classification (MDC) can be employed in a range of
applications where one needs to predict multiple class variables for each given
instance. Many existing MDC methods suffer from at least one of inaccuracy,
scalability, limited use to certain types of data, hardness of interpretation
or lack of probabilistic (uncertainty) estimations. This paper is an attempt to
address all these disadvantages simultaneously. We propose a formal framework
for probabilistic MDC in which learning an optimal multi-dimensional classifier
can be decomposed, without loss of generality, into learning a set of (smaller)
single-variable multi-class probabilistic classifiers and a directed acyclic
graph. Current and future developments of both probabilistic classification and
graphical model learning can directly enhance our framework, which is flexible
and provably optimal. A collection of experiments is conducted to highlight the
usefulness of this MDC framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07294">Computational and Storage Efficient Quadratic Neurons for Deep Neural Networks. (arXiv:2306.07294v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chuangtao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Grace Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xunzhao Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1">Cheng Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlichtmann_U/0/1/0/all/0/1">Ulf Schlichtmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bing Li</a></p>
<p>Deep neural networks (DNNs) have been widely deployed across diverse domains
such as computer vision and natural language processing. However, the
impressive accomplishments of DNNs have been realized alongside extensive
computational demands, thereby impeding their applicability on
resource-constrained devices. To address this challenge, many researchers have
been focusing on basic neuron structures, the fundamental building blocks of
neural networks, to alleviate the computational and storage cost. In this work,
an efficient quadratic neuron architecture distinguished by its enhanced
utilization of second-order computational information is introduced. By virtue
of their better expressivity, DNNs employing the proposed quadratic neurons can
attain similar accuracy with fewer neurons and computational cost. Experimental
results have demonstrated that the proposed quadratic neuron structure exhibits
superior computational and storage efficiency across various tasks when
compared with both linear and non-linear neurons in prior work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09478">Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks. (arXiv:2306.09478v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fesser_L/0/1/0/all/0/1">Lukas Fesser</a>, <a href="http://arxiv.org/find/cs/1/au:+DAmico_Wong_L/0/1/0/all/0/1">Luca D&#x27;Amico-Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1">Richard Qiu</a></p>
<p>Physics-informed Neural Networks (PINNs) have recently gained popularity due
to their effective approximation of partial differential equations (PDEs) using
deep neural networks (DNNs). However, their out of domain behavior is not well
understood, with previous work speculating that the presence of high frequency
components in the solution function might be to blame for poor extrapolation
performance. In this paper, we study the extrapolation behavior of PINNs on a
representative set of PDEs of different types, including high-dimensional PDEs.
We find that failure to extrapolate is not caused by high frequencies in the
solution function, but rather by shifts in the support of the Fourier spectrum
over time. We term these spectral shifts and quantify them by introducing a
Weighted Wasserstein-Fourier distance (WWF). We show that the WWF can be used
to predict PINN extrapolation performance, and that in the absence of
significant spectral shifts, PINN predictions stay close to the true solution
even in extrapolation. Finally, we propose a transfer learning-based strategy
to mitigate the effects of larger spectral shifts, which decreases
extrapolation errors by up to 82%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06255">Machine learning and Topological data analysis identify unique features of human papillae in 3D scans. (arXiv:2307.06255v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andreeva_R/0/1/0/all/0/1">Rayna Andreeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1">Anwesha Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1">Rik Sarkar</a></p>
<p>The tongue surface houses a range of papillae that are integral to the
mechanics and chemistry of taste and textural sensation. Although gustatory
function of papillae is well investigated, the uniqueness of papillae within
and across individuals remains elusive. Here, we present the first machine
learning framework on 3D microscopic scans of human papillae (n = 2092),
uncovering the uniqueness of geometric and topological features of papillae.
The finer differences in shapes of papillae are investigated computationally
based on a number of features derived from discrete differential geometry and
computational topology. Interpretable machine learning techniques show that
persistent homology features of the papillae shape are the most effective in
predicting the biological variables. Models trained on these features with
small volumes of data samples predict the type of papillae with an accuracy of
85%. The papillae type classification models can map the spatial arrangement of
filiform and fungiform papillae on a surface. Remarkably, the papillae are
found to be distinctive across individuals and an individual can be identified
with an accuracy of 48% among the 15 participants from a single papillae.
Collectively, this is the first unprecedented evidence demonstrating that
tongue papillae can serve as a unique identifier inspiring new research
direction for food preferences and oral diagnostics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06431">Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Schroder_T/0/1/0/all/0/1">Tobias Schr&#xf6;der</a>, <a href="http://arxiv.org/find/stat/1/au:+Ou_Z/0/1/0/all/0/1">Zijing Ou</a>, <a href="http://arxiv.org/find/stat/1/au:+Lim_J/0/1/0/all/0/1">Jen Ning Lim</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1">Yingzhen Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Vollmer_S/0/1/0/all/0/1">Sebastian J. Vollmer</a>, <a href="http://arxiv.org/find/stat/1/au:+Duncan_A/0/1/0/all/0/1">Andrew B. Duncan</a></p>
<p>Energy-based models are a simple yet powerful class of probabilistic models,
but their widespread adoption has been limited by the computational burden of
training them. We propose a novel loss function called Energy Discrepancy (ED)
which does not rely on the computation of scores or expensive Markov chain
Monte Carlo. We show that ED approaches the explicit score matching and
negative log-likelihood loss under different limits, effectively interpolating
between both. Consequently, minimum ED estimation overcomes the problem of
nearsightedness encountered in score-based estimation methods, while also
enjoying theoretical guarantees. Through numerical experiments, we demonstrate
that ED learns low-dimensional data distributions faster and more accurately
than explicit score matching or contrastive divergence. For high-dimensional
image data, we describe how the manifold hypothesis puts limitations on our
approach and demonstrate the effectiveness of energy discrepancy by training
the energy-based model as a prior of a variational decoder model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15176">RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1">Katherine A. Keith</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sergey Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a>, <a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1">Jonathan Bragg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1">Rohit Bhattacharya</a></p>
<p>Confounding is a significant obstacle to unbiased estimation of causal
effects from observational data. For settings with high-dimensional covariates
-- such as text data, genomics, or the behavioral social sciences --
researchers have proposed methods to adjust for confounding by adapting machine
learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In
this work, we build on a promising empirical evaluation strategy that
simplifies evaluation design and uses real data: subsampling randomized
controlled trials (RCTs) to create confounded observational datasets while
using the average causal effects from the RCTs as ground-truth. We contribute a
new sampling algorithm, which we call RCT rejection sampling, and provide
theoretical guarantees that causal identification holds in the observational
data to allow for valid comparisons to the ground-truth RCT. Using synthetic
data, we show our algorithm indeed results in low bias when oracle estimators
are evaluated on the confounded samples, which is not always the case for a
previously proposed algorithm. In addition to this identification result, we
highlight several finite data considerations for evaluation designers who plan
to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data
considerations with a novel, real-world RCT -- which we release publicly --
consisting of approximately 70k observations and text data as high-dimensional
covariates. Together, these contributions build towards a broader agenda of
improved empirical evaluation for causal estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00709">DeepTSF: Codeless machine learning operations for time series forecasting. (arXiv:2308.00709v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pelekis_S/0/1/0/all/0/1">Sotiris Pelekis</a>, <a href="http://arxiv.org/find/cs/1/au:+Karakolis_E/0/1/0/all/0/1">Evangelos Karakolis</a>, <a href="http://arxiv.org/find/cs/1/au:+Pountridis_T/0/1/0/all/0/1">Theodosios Pountridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kormpakis_G/0/1/0/all/0/1">George Kormpakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampropoulos_G/0/1/0/all/0/1">George Lampropoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouzakitis_S/0/1/0/all/0/1">Spiros Mouzakitis</a>, <a href="http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1">Dimitris Askounis</a></p>
<p>This paper presents DeepTSF, a comprehensive machine learning operations
(MLOps) framework aiming to innovate time series forecasting through workflow
automation and codeless modeling. DeepTSF automates key aspects of the ML
lifecycle, making it an ideal tool for data scientists and MLops engineers
engaged in machine learning (ML) and deep learning (DL)-based forecasting.
DeepTSF empowers users with a robust and user-friendly solution, while it is
designed to seamlessly integrate with existing data analysis workflows,
providing enhanced productivity and compatibility. The framework offers a
front-end user interface (UI) suitable for data scientists, as well as other
higher-level stakeholders, enabling comprehensive understanding through
insightful visualizations and evaluation metrics. DeepTSF also prioritizes
security through identity management and access authorization mechanisms. The
application of DeepTSF in real-life use cases of the I-NERGY project has
already proven DeepTSF's efficacy in DL-based load forecasting, showcasing its
significant added value in the electrical power and energy systems domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07037">Bayesian Flow Networks. (arXiv:2308.07037v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Graves_A/0/1/0/all/0/1">Alex Graves</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1">Rupesh Kumar Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Atkinson_T/0/1/0/all/0/1">Timothy Atkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1">Faustino Gomez</a></p>
<p>This paper introduces Bayesian Flow Networks (BFNs), a new class of
generative model in which the parameters of a set of independent distributions
are modified with Bayesian inference in the light of noisy data samples, then
passed as input to a neural network that outputs a second, interdependent
distribution. Starting from a simple prior and iteratively updating the two
distributions yields a generative procedure similar to the reverse process of
diffusion models; however it is conceptually simpler in that no forward process
is required. Discrete and continuous-time loss functions are derived for
continuous, discretised and discrete data, along with sample generation
procedures. Notably, the network inputs for discrete data lie on the
probability simplex, and are therefore natively differentiable, paving the way
for gradient-based sample guidance and few-step generation in discrete domains
such as language modelling. The loss function directly optimises data
compression and places no restrictions on the network architecture. In our
experiments BFNs achieve competitive log-likelihoods for image modelling on
dynamically binarized MNIST and CIFAR-10, and outperform all known discrete
diffusion models on the text8 character-level language modelling task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08736">On the Effectiveness of Log Representation for Log-based Anomaly Detection. (arXiv:2308.08736v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingfang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a></p>
<p>Logs are an essential source of information for people to understand the
running status of a software system. Due to the evolving modern software
architecture and maintenance methods, more research efforts have been devoted
to automated log analysis. In particular, machine learning (ML) has been widely
used in log analysis tasks. In ML-based log analysis tasks, converting textual
log data into numerical feature vectors is a critical and indispensable step.
However, the impact of using different log representation techniques on the
performance of the downstream models is not clear, which limits researchers and
practitioners' opportunities of choosing the optimal log representation
techniques in their automated log analysis workflows. Therefore, this work
investigates and compares the commonly adopted log representation techniques
from previous log analysis research. Particularly, we select six log
representation techniques and evaluate them with seven ML models and four
public log datasets (i.e., HDFS, BGL, Spirit and Thunderbird) in the context of
log-based anomaly detection. We also examine the impacts of the log parsing
process and the different feature aggregation approaches when they are employed
with log representation techniques. From the experiments, we provide some
heuristic guidelines for future researchers and developers to follow when
designing an automated log analysis workflow. We believe our comprehensive
comparison of log representation techniques can help researchers and
practitioners better understand the characteristics of different log
representation techniques and provide them with guidance for selecting the most
suitable ones for their ML-based log analysis workflow.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09084">MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices. (arXiv:2308.09084v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dongyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhirui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1">Wangpeng An</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanhong Yang</a></p>
<p>We present MovePose, an optimized lightweight convolutional neural network
designed specifically for real-time body pose estimation on CPU-based mobile
devices. The current solutions do not provide satisfactory accuracy and speed
for human posture estimation, and MovePose addresses this gap. It aims to
maintain real-time performance while improving the accuracy of human posture
estimation for mobile devices. The network produces 17 keypoints for each
individual at a rate exceeding 11 frames per second, making it suitable for
real-time applications such as fitness tracking, sign language interpretation,
and advanced mobile human posture estimation. Our MovePose algorithm has
attained an Mean Average Precision (mAP) score of 67.7 on the COCO
\cite{cocodata} validation dataset. The MovePose algorithm displayed efficiency
with a performance of 69+ frames per second (fps) when run on an Intel
i9-10920x CPU. Additionally, it showcased an increased performance of 452+ fps
on an NVIDIA RTX3090 GPU. On an Android phone equipped with a Snapdragon 8 + 4G
processor, the fps reached above 11. To enhance accuracy, we incorporated three
techniques: deconvolution, large kernel convolution, and coordinate
classification methods. Compared to basic upsampling, deconvolution is
trainable, improves model capacity, and enhances the receptive field. Large
kernel convolution strengthens these properties at a decreased computational
cost. In summary, MovePose provides high accuracy and real-time performance,
marking it a potential tool for a variety of applications, including those
focused on mobile-side human posture estimation. The code and models for this
algorithm will be made publicly accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11978">Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xiandong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiren Zhao</a></p>
<p>Graph generation poses a significant challenge as it involves predicting a
complete graph with multiple nodes and edges based on simply a given label.
This task also carries fundamental importance to numerous real-world
applications, including de-novo drug and molecular design. In recent years,
several successful methods have emerged in the field of graph generation.
However, these approaches suffer from two significant shortcomings: (1) the
underlying Graph Neural Network (GNN) architectures used in these methods are
often underexplored; and (2) these methods are often evaluated on only a
limited number of metrics. To fill this gap, we investigate the expressiveness
of GNNs under the context of the molecular graph generation task, by replacing
the underlying GNNs of graph generative models with more expressive GNNs.
Specifically, we analyse the performance of six GNNs on six different molecular
generative objectives on the ZINC-250k dataset in two different generative
frameworks: autoregressive generation models, such as GCPN and GraphAF, and
one-shot generation models, such as GraphEBM. Through our extensive
experiments, we demonstrate that advanced GNNs can indeed improve the
performance of GCPN, GraphAF, and GraphEBM on molecular generation tasks, but
GNN expressiveness is not a necessary condition for a good GNN-based generative
model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve
state-of-the-art results across 17 other non-GNN-based graph generative
approaches, such as variational autoencoders and Bayesian optimisation models,
on the proposed molecular generative objectives (DRD2, Median1, Median2), which
are important metrics for de-novo molecular design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12532">FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minchan Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangmook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14969">Uncovering the Hidden Cost of Model Compression. (arXiv:2308.14969v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1">Diganta Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Agam Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Runwal_B/0/1/0/all/0/1">Bharat Runwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin Yu Chen</a></p>
<p>In the era of resource-intensive foundation models, efficient adaptation in
downstream tasks has become paramount. Visual Prompting (VP), inspired by
prompting in Large Language Models (LLMs), has emerged as a key transfer
learning method in computer vision. Aligned with the growing significance of
efficiency, research in model compression has become pivotal to alleviate the
computational burden in both training and deploying over-parameterized neural
networks. A key goal in model compression is the development of sparse models
capable of matching or surpassing the performance of their over-parameterized,
dense counterparts. While prior research has explored the impact of model
sparsity on transfer learning, its effects on visual prompting-based transfer
remain unclear. This study addresses this gap, revealing that model sparsity
adversely affects the performance of visual prompting-based transfer,
particularly in low-data-volume scenarios. Furthermore, our findings highlight
the negative influence of sparsity on the calibration of downstream
visual-prompted models. This empirical exploration calls for a nuanced
understanding beyond accuracy in sparse settings, opening avenues for further
research in Visual Prompting for sparse models. Code and logs can be accessed
at https://github.com/landskape-ai/Reprogram_LT .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15709">Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation. (arXiv:2308.15709v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuqing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a></p>
<p>Data valuation aims to quantify the usefulness of individual data sources in
training machine learning (ML) models, and is a critical aspect of data-centric
ML research. However, data valuation faces significant yet frequently
overlooked privacy challenges despite its importance. This paper studies these
challenges with a focus on KNN-Shapley, one of the most practical data
valuation methods nowadays. We first emphasize the inherent privacy risks of
KNN-Shapley, and demonstrate the significant technical difficulties in adapting
KNN-Shapley to accommodate differential privacy (DP). To overcome these
challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is
privacy-friendly, allowing for straightforward modifications to incorporate DP
guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several
advantages and offers a superior privacy-utility tradeoff compared to naively
privatized KNN-Shapley in discerning data quality. Moreover, even non-private
TKNN-Shapley achieves comparable performance as KNN-Shapley. Overall, our
findings suggest that TKNN-Shapley is a promising alternative to KNN-Shapley,
particularly for real-world applications involving sensitive data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16781">StratMed: Relevance Stratification between Biomedical Entities for Sparsity on Medication Recommendation. (arXiv:2308.16781v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Shunpan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yulei Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengfei Ma</a></p>
<p>With the growing imbalance between limited medical resources and escalating
demands, AI-based clinical tasks have become paramount. As a sub-domain,
medication recommendation aims to amalgamate longitudinal patient history with
medical knowledge, assisting physicians in prescribing safer and more accurate
medication combinations. Existing works ignore the inherent long-tailed
distribution of medical data, have uneven learning strengths for hot and sparse
data, and fail to balance safety and accuracy. To address the above
limitations, we propose StratMed, which introduces a stratification strategy
that overcomes the long-tailed problem and achieves fuller learning of sparse
data. It also utilizes a dual-property network to address the issue of mutual
constraints on the safety and accuracy of medication combinations,
synergistically enhancing these two properties. Specifically, we construct a
pre-training method using deep learning networks to obtain medication and
disease representations. After that, we design a pyramid-like stratification
method based on relevance to strengthen the expressiveness of sparse data.
Based on this relevance, we design two graph structures to express medication
safety and precision at the same level to obtain patient representations.
Finally, the patient's historical clinical information is fitted to generate
medication combinations for the current health condition. We employed the
MIMIC-III dataset to evaluate our model against state-of-the-art methods in
three aspects comprehensively. Compared to the sub-optimal baseline model, our
model reduces safety risk by 15.08\%, improves accuracy by 0.36\%, and reduces
training time consumption by 81.66\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01897">Inferring Actual Treatment Pathways from Patient Records. (arXiv:2309.01897v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilkins_Caruana_A/0/1/0/all/0/1">Adrian Wilkins-Caruana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1">Madhushi Bandara</a>, <a href="http://arxiv.org/find/cs/1/au:+Musial_K/0/1/0/all/0/1">Katarzyna Musial</a>, <a href="http://arxiv.org/find/cs/1/au:+Catchpoole_D/0/1/0/all/0/1">Daniel Catchpoole</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_P/0/1/0/all/0/1">Paul J. Kennedy</a></p>
<p>Treatment pathways are step-by-step plans outlining the recommended medical
care for specific diseases; they get revised when different treatments are
found to improve patient outcomes. Examining health records is an important
part of this revision process, but inferring patients' actual treatments from
health data is challenging due to complex event-coding schemes and the absence
of pathway-related annotations. This study aims to infer the actual treatment
steps for a particular patient group from administrative health records (AHR) -
a common form of tabular healthcare data - and address several technique- and
methodology-based gaps in treatment pathway-inference research. We introduce
Defrag, a method for examining AHRs to infer the real-world treatment steps for
a particular patient group. Defrag learns the semantic and temporal meaning of
healthcare event sequences, allowing it to reliably infer treatment steps from
complex healthcare data. To our knowledge, Defrag is the first
pathway-inference method to utilise a neural network (NN), an approach made
possible by a novel, self-supervised learning objective. We also developed a
testing and validation framework for pathway inference, which we use to
characterise and evaluate Defrag's pathway inference ability and compare
against baselines. We demonstrate Defrag's effectiveness by identifying
best-practice pathway fragments for breast cancer, lung cancer, and melanoma in
public healthcare records. Additionally, we use synthetic data experiments to
demonstrate the characteristics of the Defrag method, and to compare Defrag to
several baselines where it significantly outperforms non-NN-based methods.
Defrag significantly outperforms several existing pathway-inference methods and
offers an innovative and effective approach for inferring treatment pathways
from AHRs. Open-source code is provided to encourage further research in this
area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01947">TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models. (arXiv:2309.01947v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1">Yuan Shangguan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haichuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Danni Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chunyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fathullah_Y/0/1/0/all/0/1">Yassir Fathullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalmia_A/0/1/0/all/0/1">Ayushi Dalmia</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1">Raghuraman Krishnamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1">Ozlem Kalinli</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Junteng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1">Jay Mahadeokar</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1">Mike Seltzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a></p>
<p>Automatic Speech Recognition (ASR) models need to be optimized for specific
hardware before they can be deployed on devices. This can be done by tuning the
model's hyperparameters or exploring variations in its architecture.
Re-training and re-validating models after making these changes can be a
resource-intensive task. This paper presents TODM (Train Once Deploy Many), a
new approach to efficiently train many sizes of hardware-friendly on-device ASR
models with comparable GPU-hours to that of a single training job. TODM
leverages insights from prior work on Supernet, where Recurrent Neural Network
Transducer (RNN-T) models share weights within a Supernet. It reduces layer
sizes and widths of the Supernet to obtain subnetworks, making them smaller
models suitable for all hardware types. We introduce a novel combination of
three techniques to improve the outcomes of the TODM Supernet: adaptive
dropouts, an in-place Alpha-divergence knowledge distillation, and the use of
ScaledAdam optimizer. We validate our approach by comparing Supernet-trained
versus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using
LibriSpeech. Results demonstrate that our TODM Supernet either matches or
surpasses the performance of manually tuned models by up to a relative of 3%
better in word error rate (WER), while efficiently keeping the cost of training
many models at a small constant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07176">Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Angela Zhou</a></p>
<p>In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. In these same
domains, there may be heterogeneity both in who responds in taking-up
treatment, and heterogeneity in treatment efficacy. While optimal treatment
rules can maximize causal outcomes across the population, access parity
constraints or other fairness considerations can be relevant in the case of
encouragement. For example, in social services, a persistent puzzle is the gap
in take-up of beneficial services among those who may benefit from them the
most. When in addition the decision-maker has distributional preferences over
both access and average outcomes, the optimal decision rule changes. We study
causal identification, statistical variance-reduced estimation, and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in two case studies based on data from randomized encouragement to
enroll in insurance and from pretrial supervised release with electronic
monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07983">SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems. (arXiv:2309.07983v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guangke Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yedi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1">Fu Song</a></p>
<p>Membership inference attacks allow adversaries to determine whether a
particular example was contained in the model's training dataset. While
previous works have confirmed the feasibility of such attacks in various
applications, none has focused on speaker recognition (SR), a promising
voice-based biometric recognition technique. In this work, we propose SLMIA-SR,
the first membership inference attack tailored to SR. In contrast to
conventional example-level attack, our attack features speaker-level membership
inference, i.e., determining if any voices of a given speaker, either the same
as or different from the given inference voices, have been involved in the
training of a model. It is particularly useful and practical since the training
and inference voices are usually distinct, and it is also meaningful
considering the open-set nature of SR, namely, the recognition speakers were
often not present in the training data. We utilize intra-similarity and
inter-dissimilarity, two training objectives of SR, to characterize the
differences between training and non-training speakers and quantify them with
two groups of features driven by carefully-established feature engineering to
mount the attack. To improve the generalizability of our attack, we propose a
novel mixing ratio training strategy to train attack models. To enhance the
attack performance, we introduce voice chunk splitting to cope with the limited
number of inference voices and propose to train attack models dependent on the
number of inference voices. Our attack is versatile and can work in both
white-box and black-box scenarios. Additionally, we propose two novel
techniques to reduce the number of black-box queries while maintaining the
attack performance. Extensive experiments demonstrate the effectiveness of
SLMIA-SR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09111">Reducing sequential change detection to sequential estimation. (arXiv:2309.09111v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Shekhar_S/0/1/0/all/0/1">Shubhanshu Shekhar</a>, <a href="http://arxiv.org/find/math/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p>
<p>We consider the problem of sequential change detection, where the goal is to
design a scheme for detecting any changes in a parameter or functional $\theta$
of the data stream distribution that has small detection delay, but guarantees
control on the frequency of false alarms in the absence of changes. In this
paper, we describe a simple reduction from sequential change detection to
sequential estimation using confidence sequences: we begin a new
$(1-\alpha)$-confidence sequence at each time step, and proclaim a change when
the intersection of all active confidence sequences becomes empty. We prove
that the average run length is at least $1/\alpha$, resulting in a change
detection scheme with minimal structural assumptions~(thus allowing for
possibly dependent observations, and nonparametric distribution classes), but
strong guarantees. Our approach bears an interesting parallel with the
reduction from change detection to sequential testing of Lorden (1971) and the
e-detector of Shin et al. (2022).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13593">Self-Tuning Hamiltonian Monte Carlo for Accelerated Sampling. (arXiv:2309.13593v2 [physics.comp-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Christiansen_H/0/1/0/all/0/1">Henrik Christiansen</a>, <a href="http://arxiv.org/find/physics/1/au:+Errica_F/0/1/0/all/0/1">Federico Errica</a>, <a href="http://arxiv.org/find/physics/1/au:+Alesiani_F/0/1/0/all/0/1">Francesco Alesiani</a></p>
<p>The performance of Hamiltonian Monte Carlo simulations crucially depends on
both the integration timestep and the number of integration steps. We present
an adaptive general-purpose framework to automatically tune such parameters,
based on a local loss function which promotes the fast exploration of
phase-space. We show that a good correspondence between loss and
autocorrelation time can be established, allowing for gradient-based
optimization using a fully-differentiable set-up. The loss is constructed in
such a way that it also allows for gradient-driven learning of a distribution
over the number of integration steps. Our approach is demonstrated for the
one-dimensional harmonic oscillator and alanine dipeptide, a small protein
common as a test case for simulation methods. Through the application to the
harmonic oscillator, we highlight the importance of not using a fixed timestep
to avoid a rugged loss surface with many local minima, otherwise trapping the
optimization. In the case of alanine dipeptide, by tuning the only free
parameter of our loss definition, we find a good correspondence between it and
the autocorrelation times, resulting in a $&gt;100$ fold speed up in optimization
of simulation parameters compared to a grid-search. For this system, we also
extend the integrator to allow for atom-dependent timesteps, providing a
further reduction of $25\%$ in autocorrelation times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13658">Fantastic Generalization Measures are Nowhere to be Found. (arXiv:2309.13658v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1">Michael Gastpar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1">Ido Nachum</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1">Jonathan Shafer</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_T/0/1/0/all/0/1">Thomas Weinberger</a></p>
<p>We study the notion of a generalization bound being uniformly tight, meaning
that the difference between the bound and the population loss is small for all
learning algorithms and all population distributions. Numerous generalization
bounds have been proposed in the literature as potential explanations for the
ability of neural networks to generalize in the overparameterized setting.
However, in their paper ``Fantastic Generalization Measures and Where to Find
Them,'' Jiang et al. (2020) examine more than a dozen generalization bounds,
and show empirically that none of them are uniformly tight. This raises the
question of whether uniformly-tight generalization bounds are at all possible
in the overparameterized setting. We consider two types of generalization
bounds: (1) bounds that may depend on the training set and the learned
hypothesis (e.g., margin bounds). We prove mathematically that no such bound
can be uniformly tight in the overparameterized setting; (2) bounds that may in
addition also depend on the learning algorithm (e.g., stability bounds). For
these bounds, we show a trade-off between the algorithm's performance and the
bound's tightness. Namely, if the algorithm achieves good accuracy on certain
distributions, then no generalization bound can be uniformly tight for it in
the overparameterized setting. We explain how these formal results can, in our
view, inform research on generalization bounds for neural networks, while
stressing that other interpretations of these results are also possible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14681">Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a></p>
<p>Despite the promising few-shot ability of large language models (LLMs), the
standard paradigm of In-context Learning (ICL) suffers the disadvantages of
susceptibility to selected demonstrations and the intricacy to generate these
demonstrations. In this paper, we raise the fundamental question that whether
human-generated demonstrations are necessary for ICL. To answer this question,
we propose self-contemplation prompting strategy (SEC), a paradigm free from
human-crafted demonstrations. The key point of SEC is that, instead of using
hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create
demonstrations on their own, based on which the final output is generated. SEC
is a flexible framework and can be adapted to both the vanilla ICL and the
chain-of-thought (CoT), but with greater ease: as the manual-generation process
of both examples and rationale can be saved. Extensive experiments in
arithmetic reasoning, commonsense reasoning, multi-task language understanding,
and code generation benchmarks, show that SEC, which does not require
hand-crafted demonstrations, significantly outperforms the zero-shot learning
strategy, and achieves comparable results to ICL with hand-crafted
demonstrations. This demonstrates that, for many tasks, contemporary LLMs
possess a sufficient level of competence to exclusively depend on their own
capacity for decision making, removing the need for external training data.
Code is available at https://github.com/ruili33/SEC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16883">The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delattre_B/0/1/0/all/0/1">Blaise Delattre</a>, <a href="http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1">Alexandre Araujo</a>, <a href="http://arxiv.org/find/cs/1/au:+Barthelemy_Q/0/1/0/all/0/1">Quentin Barth&#xe9;lemy</a>, <a href="http://arxiv.org/find/cs/1/au:+Allauzen_A/0/1/0/all/0/1">Alexandre Allauzen</a></p>
<p>Real-life applications of deep neural networks are hindered by their unsteady
predictions when faced with noisy inputs and adversarial attacks. The certified
radius is in this context a crucial indicator of the robustness of models.
However how to design an efficient classifier with a sufficient certified
radius? Randomized smoothing provides a promising framework by relying on noise
injection in inputs to obtain a smoothed and more robust classifier. In this
paper, we first show that the variance introduced by randomized smoothing
closely interacts with two other important properties of the classifier,
\textit{i.e.} its Lipschitz constant and margin. More precisely, our work
emphasizes the dual impact of the Lipschitz constant of the base classifier, on
both the smoothed classifier and the empirical variance. Moreover, to increase
the certified robust radius, we introduce a different simplex projection
technique for the base classifier to leverage the variance-margin trade-off
thanks to Bernstein's concentration inequality, along with an enhanced
Lipschitz bound. Experimental results show a significant improvement in
certified accuracy compared to current state-of-the-art methods. Our novel
certification procedure allows us to use pre-trained models that are used with
randomized smoothing, effectively improving the current certification radius in
a zero-shot manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17338">Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chib_P/0/1/0/all/0/1">Pranav Singh Chib</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pravendra Singh</a></p>
<p>The inherently diverse and uncertain nature of trajectories presents a
formidable challenge in accurately modeling them. Motion prediction systems
must effectively learn spatial and temporal information from the past to
forecast the future trajectories of the agent. Many existing methods learn
temporal motion via separate components within stacked models to capture
temporal features. Furthermore, prediction methods often operate under the
assumption that observed trajectory waypoint sequences are complete,
disregarding scenarios where missing values may occur, which can influence
their performance. Moreover, these models may be biased toward particular
waypoint sequences when making predictions. We propose a novel approach called
Temporal Waypoint Dropping (TWD) that explicitly incorporates temporal
dependencies during the training of a trajectory prediction model. By
stochastically dropping waypoints from past observed trajectories, the model is
forced to learn the underlying temporal representation from the remaining
waypoints, resulting in an improved model. Incorporating stochastic temporal
waypoint dropping into the model learning process significantly enhances its
performance in scenarios with missing values. Experimental results demonstrate
our approach's substantial improvement in trajectory prediction capabilities.
Our approach can complement existing trajectory prediction methods to improve
their prediction accuracy. We evaluate our proposed approach on three datasets:
NBA Sports VU, ETH-UCY, and TrajNet++.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17401">Adversarial Machine Learning in Latent Representations of Neural Networks. (arXiv:2309.17401v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Milin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdi_M/0/1/0/all/0/1">Mohammad Abdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1">Francesco Restuccia</a></p>
<p>Distributed deep neural networks (DNNs) have been shown to reduce the
computational burden of mobile devices and decrease the end-to-end inference
latency in edge computing scenarios. While distributed DNNs have been studied,
to the best of our knowledge the resilience of distributed DNNs to adversarial
action still remains an open problem. In this paper, we fill the existing
research gap by rigorously analyzing the robustness of distributed DNNs against
adversarial action. We cast this problem in the context of information theory
and introduce two new measurements for distortion and robustness. Our
theoretical findings indicate that (i) assuming the same level of information
distortion, latent features are always more robust than input representations;
(ii) the adversarial robustness is jointly determined by the feature dimension
and the generalization capability of the DNN. To test our theoretical findings,
we perform extensive experimental analysis by considering 6 different DNN
architectures, 6 different approaches for distributed DNN and 10 different
adversarial attacks to the ImageNet-1K dataset. Our experimental results
support our theoretical findings by showing that the compressed latent
representations can reduce the success rate of adversarial attacks by 88% in
the best case and by 57% on the average compared to attacks to the input space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00967">MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training. (arXiv:2310.00967v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1">Daegun Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Sangyoon Oh</a></p>
<p>Gradient sparsification is a communication optimisation technique for scaling
and accelerating distributed deep neural network (DNN) training. It reduces the
increasing communication traffic for gradient aggregation. However, existing
sparsifiers have poor scalability because of the high computational cost of
gradient selection and/or increase in communication traffic. In particular, an
increase in communication traffic is caused by gradient build-up and
inappropriate threshold for gradient selection.
</p>
<p>To address these challenges, we propose a novel gradient sparsification
method called MiCRO. In MiCRO, the gradient vector is partitioned, and each
partition is assigned to the corresponding worker. Each worker then selects
gradients from its partition, and the aggregated gradients are free from
gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain
the communication traffic as per user requirement by minimising the compression
ratio error. MiCRO enables near-zero cost gradient sparsification by solving
existing problems that hinder the scalability and acceleration of distributed
DNN training. In our extensive experiments, MiCRO outperformed state-of-the-art
sparsifiers with an outstanding convergence rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01144">The Map Equation Goes Neural. (arXiv:2310.01144v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blocker_C/0/1/0/all/0/1">Christopher Bl&#xf6;cker</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chester Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholtes_I/0/1/0/all/0/1">Ingo Scholtes</a></p>
<p>Community detection and graph clustering are essential for unsupervised data
exploration and understanding the high-level organisation of networked systems.
Recently, graph clustering has received attention as a primary task for graph
neural networks. Although hierarchical graph pooling has been shown to improve
performance in graph and node classification tasks, it performs poorly in
identifying meaningful clusters. Community detection has a long history in
network science, but typically relies on optimising objective functions with
custom-tailored search algorithms, not leveraging recent advances in deep
learning, particularly from graph neural networks. In this paper, we narrow
this gap between the deep learning and network science communities. We consider
the map equation, an information-theoretic objective function for unsupervised
community detection. Expressing it in a fully differentiable tensor form that
produces soft cluster assignments, we optimise the map equation with deep
learning through gradient descent. More specifically, the reformulated map
equation is a loss function compatible with any graph neural network
architecture, enabling flexible clustering and graph pooling that clusters both
graph structure and data features in an end-to-end way, automatically finding
an optimum number of clusters without explicit regularisation by following the
minimum description length principle. We evaluate our approach experimentally
using different neural network architectures for unsupervised clustering in
synthetic and real data. Our results show that our approach achieves
competitive performance against baselines, naturally detects overlapping
communities, and avoids over-partitioning sparse graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01547">On the near-optimality of betting confidence sets for bounded means. (arXiv:2310.01547v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Shekhar_S/0/1/0/all/0/1">Shubhanshu Shekhar</a>, <a href="http://arxiv.org/find/math/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p>
<p>Constructing nonasymptotic confidence intervals (CIs) for the mean of a
univariate distribution from independent and identically distributed (i.i.d.)
observations is a fundamental task in statistics. For bounded observations, a
classical nonparametric approach proceeds by inverting standard concentration
bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an
alternative betting-based approach for defining CIs and their time-uniform
variants called confidence sequences (CSs), has been shown to be empirically
superior to the classical methods. In this paper, we provide theoretical
justification for this improved empirical performance of betting CIs and CSs.
</p>
<p>Our main contributions are as follows: (i) We first compare CIs using the
values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show
that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting
width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two
lower bounds that characterize the minimum width achievable by any method for
constructing CIs/CSs in terms of certain inverse information projections. (iii)
Finally, we show that the betting CI and CS match the fundamental limits,
modulo an additive logarithmic term and a multiplicative constant. Overall
these results imply that the betting CI~(and CS) admit stronger theoretical
guarantees than the existing state-of-the-art EB-CI~(and CS); both in the
asymptotic and finite-sample regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01825">Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zahweh_M/0/1/0/all/0/1">Mohamad Hasan Zahweh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1">Hasan Nasrallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1">Mustafa Shukor</a>, <a href="http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1">Ghaleb Faour</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1">Ali J. Ghandour</a></p>
<p>Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01828">Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shreim_H/0/1/0/all/0/1">Hossein Shreim</a>, <a href="http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1">Abdul Karim Gizzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1">Ali J. Ghandour</a></p>
<p>eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03915">Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tumma_N/0/1/0/all/0/1">Neehal Tumma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1">Mathias Lechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Loo_N/0/1/0/all/0/1">Noel Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>Developing autonomous agents that can interact with changing environments is
an open challenge in machine learning. Robustness is particularly important in
these settings as agents are often fit offline on expert demonstrations but
deployed online where they must generalize to the closed feedback loop within
the environment. In this work, we explore the application of recurrent neural
networks to tasks of this nature and understand how a parameterization of their
recurrent connectivity influences robustness in closed-loop settings.
Specifically, we represent the recurrent connectivity as a function of rank and
sparsity and show both theoretically and empirically that modulating these two
variables has desirable effects on network dynamics. The proposed low-rank,
sparse connectivity induces an interpretable prior on the network that proves
to be most amenable for a class of models known as closed-form continuous-time
neural networks (CfCs). We find that CfCs with fewer parameters can outperform
their full-rank, fully-connected counterparts in the online setting under
distribution shift. This yields memory-efficient and robust agents while
opening a new perspective on how we can modulate network dynamics through
connectivity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06627">What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models. (arXiv:2310.06627v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Letian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaotong Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhongkai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1">Yongshuo Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bingchen Zhao</a></p>
<p>Counterfactual reasoning, a fundamental aspect of human cognition, involves
contemplating alternatives to established facts or past events, significantly
enhancing our abilities in planning and decision-making. In light of the
advancements in current multi-modal large language models, we explore their
effectiveness in counterfactual reasoning. To facilitate this investigation, we
introduce a novel dataset, C-VQA, specifically designed to test the
counterfactual reasoning capabilities of modern multi-modal large language
models. This dataset is constructed by infusing original questions with
counterfactual presuppositions, spanning various types such as numerical and
boolean queries. It encompasses a mix of real and synthetic data, representing
a wide range of difficulty levels. Our thorough evaluations of contemporary
vision-language models using this dataset have revealed substantial performance
drops, with some models showing up to a 40\% decrease, highlighting a
significant gap between current models and human-like vision reasoning
capabilities. We hope our dataset will serve as a vital benchmark for
evaluating the counterfactual reasoning capabilities of models. Code and
dataset are publicly available at https://bzhao.me/C-VQA/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06756">Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory. (arXiv:2310.06756v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanpeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>The behavior of neural networks still remains opaque, and a recently widely
noted phenomenon is that networks often achieve similar performance when
initialized with different random parameters. This phenomenon has attracted
significant attention in measuring the similarity between features learned by
distinct networks. However, feature similarity could be vague in describing the
same feature since equivalent features hardly exist. In this paper, we expand
the concept of equivalent feature and provide the definition of what we call
functionally equivalent features. These features produce equivalent output
under certain transformations. Using this definition, we aim to derive a more
intrinsic metric for the so-called feature complexity regarding the redundancy
of features learned by a neural network at each layer. We offer a formal
interpretation of our approach through the lens of category theory, a
well-developed area in mathematics. To quantify the feature complexity, we
further propose an efficient algorithm named Iterative Feature Merging. Our
experimental results validate our ideas and theories from various perspectives.
We empirically demonstrate that the functionally equivalence widely exists
among different features learned by the same neural network and we could reduce
the number of parameters of the network without affecting the performance.The
IFM shows great potential as a data-agnostic model prune method. We have also
drawn several interesting empirical findings regarding the defined feature
complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07587">Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zikai Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zihan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Songshang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hualiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jin Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Howard Hao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuozhu Liu</a></p>
<p>Data privacy and long-tailed distribution are the norms rather than the
exception in many real-world tasks. This paper investigates a federated
long-tailed learning (Fed-LT) task in which each client holds a locally
heterogeneous dataset; if the datasets can be globally aggregated, they jointly
exhibit a long-tailed distribution. Under such a setting, existing federated
optimization and/or centralized long-tailed learning methods hardly apply due
to challenges in (a) characterizing the global long-tailed distribution under
privacy constraints and (b) adjusting the local learning strategy to cope with
the head-tail imbalance. In response, we propose a method termed
$\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB)
module that re-weights clients' gradients in a closed-loop manner, based on the
feedback of global long-tailed distribution evaluated by a Direct Prior
Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively
alleviate the distribution drift caused by data heterogeneity during the model
training process and obtain a global model with better performance on the
minority classes while maintaining the performance of the majority classes.
Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achieves
state-of-the-art performance on representative datasets such as CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07665">Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kladny_K/0/1/0/all/0/1">Klaus-Rudolf Kladny</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Muehlebach_M/0/1/0/all/0/1">Michael Muehlebach</a></p>
<p>Counterfactuals can offer valuable insights by answering what would have been
observed under altered circumstances, conditional on a factual observation.
Whereas the classical interventional interpretation of counterfactuals has been
studied extensively, backtracking constitutes a less studied alternative the
backtracking principle has emerged as an alternative philosophy where all
causal laws are kept intact. In the present work, we introduce a practical
method for computing backtracking counterfactuals in structural causal models
that consist of deep generative components. To this end, we impose conditions
on the structural assignments that enable the generation of counterfactuals by
solving a tractable constrained optimization problem in the structured latent
space of a causal model. Our formulation also facilitates a comparison with
methods in the field of counterfactual explanations. Compared to these, our
method represents a versatile, modular and causally compliant alternative. We
demonstrate these properties experimentally on a modified version of MNIST and
CelebA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08049">Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Ivan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1">Taylor Berg-Kirkpatrick</a></p>
<p>What is the relationship between model architecture and the ability to
perform in-context learning? In this empirical study, we take the first steps
toward answering this question. We evaluate twelve model architectures capable
of causal language modeling across a suite of synthetic in-context learning
tasks. These selected architectures represent a broad range of paradigms,
including recurrent and convolution-based neural networks, transformers,
state-space model inspired, and other emerging attention alternatives. We
discover that all the considered architectures can perform in-context learning
under a wider range of conditions than previously documented. Additionally, we
observe stark differences in statistical efficiency and consistency by varying
context length and task difficulty. We also measure each architecture's
predisposition towards in-context learning when presented with alternative
routes for task resolution. Finally, and somewhat surprisingly, we find that
several attention alternatives are more robust in-context learners than
transformers. Given that such approaches have constant-sized memory footprints
at inference time, this result opens the possibility of scaling up in-context
learning to accommodate vastly larger numbers of in-context examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08540">Do pretrained Transformers Really Learn In-context by Gradient Descent?. (arXiv:2310.08540v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lingfeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Aayush Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1">Daniel Khashabi</a></p>
<p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?
Several recent works draw analogies between the dynamics of GD and the emergent
behavior of ICL in large language models. However, these works make assumptions
far from the realistic natural language setting in which language models are
trained. Therefore, such discrepancies between theory and practice necessitate
further investigation to validate their applicability.
</p>
<p>We start by highlighting the assumptions in prior works that construct
Transformer weights to simulate gradient descent. Their experiments with
training Transformers on ICL objective, inconsistencies in the order
sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity
to parameter changes are some examples of mismatch from the real-world setting.
</p>
<p>Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural
setting. We conduct comprehensive empirical analyses on language models
pretrained on natural data (LLaMa-7B). Our comparisons on various performance
metrics highlight the inconsistent behavior of ICL and GD as a function of
various factors such as datasets, models, and the number of demonstrations. We
observe that ICL and GD modify the output distribution of language models
differently. These results indicate that the equivalence between ICL and GD is
an open hypothesis, requires nuanced considerations, and calls for further
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09912">Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models. (arXiv:2310.09912v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zijian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Luping Liu. Zhijie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>We propose the first unsupervised and learning-based method to identify
interpretable directions in h-space of pre-trained diffusion models. Our method
is derived from an existing technique that operates on the GAN latent space.
Specifically, we employ a shift control module that works on h-space of
pre-trained diffusion models to manipulate a sample into a shifted version of
itself, followed by a reconstructor to reproduce both the type and the strength
of the manipulation. By jointly optimizing them, the model will spontaneously
discover disentangled and interpretable directions. To prevent the discovery of
meaningless and destructive directions, we employ a discriminator to maintain
the fidelity of shifted sample. Due to the iterative generative process of
diffusion models, our training requires a substantial amount of GPU VRAM to
store numerous intermediate tensors for back-propagating gradient. To address
this issue, we propose a general VRAM-efficient training algorithm based on
gradient checkpointing technique to back-propagate any gradient through the
whole generative process, with acceptable occupancy of VRAM and sacrifice of
training efficiency. Compared with existing related works on diffusion models,
our method inherently identifies global and scalable directions, without
necessitating any other complicated procedures. Extensive experiments on
various datasets demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10092">Label Differential Privacy via Aggregation. (arXiv:2310.10092v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brahmbhatt_A/0/1/0/all/0/1">Anand Brahmbhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Saket_R/0/1/0/all/0/1">Rishi Saket</a>, <a href="http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1">Shreyas Havaldar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1">Anshul Nasery</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1">Aravindan Raghuveer</a></p>
<p>In many real-world applications, due to recent developments in the privacy
landscape, training data may be aggregated to preserve the privacy of sensitive
training labels. In the learning from label proportions (LLP) framework, the
dataset is partitioned into bags of feature-vectors which are available only
with the sum of the labels per bag. A further restriction, which we call
learning from bag aggregates (LBA) is where instead of individual
feature-vectors, only the (possibly weighted) sum of the feature-vectors per
bag is available. We study whether such aggregation techniques can provide
privacy guarantees under the notion of label differential privacy (label-DP)
previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari
et al.'22].
</p>
<p>It is easily seen that naive LBA and LLP do not provide label-DP. Our main
result however, shows that weighted LBA using iid Gaussian weights with $m$
randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon,
\delta)$-label-DP for any $\varepsilon &gt; 0$ with $\delta \approx
\exp(-\Omega(\sqrt{k}))$ assuming a lower bound on the linear-mse regression
loss. Further, the $\ell_2^2$-regressor which minimizes the loss on the
aggregated dataset has a loss within $\left(1 + o(1)\right)$-factor of the
optimum on the original dataset w.p. $\approx 1 - exp(-\Omega(m))$. We
emphasize that no additive label noise is required.
</p>
<p>The analogous weighted-LLP does not however admit label-DP. Nevertheless, we
show that if additive $N(0, 1)$ noise can be added to any constant fraction of
the instance labels, then the noisy weighted-LLP admits similar label-DP
guarantees without assumptions on the dataset, while preserving the utility of
Lipschitz-bounded neural mse-regression tasks.
</p>
<p>Our work is the first to demonstrate that label-DP can be achieved by
randomly weighted aggregation for regression tasks, using no or little additive
noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10520">Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuxiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1">Guanting Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiran Xu</a></p>
<p>Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time-consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods. Our code has been released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10541">AST: Effective Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiyuan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenzhuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1">Kwok-Yan Lam</a></p>
<p>Training large AI models typically requires large-scale datasets in the
machine learning process, making training and parameter-tuning process both
time-consuming and costly. Some researchers address this problem by carefully
synthesizing a very small number of highly representative and informative
samples from real-world datasets. This approach, known as Dataset Distillation
(DD), proposes a perspective for data-efficient learning. Despite recent
progress in this field, the performance of existing methods still cannot meet
expectations, and distilled datasets cannot effectively replace original
datasets. In this paper, unlike previous methods that focus solely on improving
the effectiveness of student distillation, we recognize and leverage the
important mutual influence between expert and student models. We observed that
the smoothness of expert trajectories has a significant impact on subsequent
student parameter alignment. Based on this, we propose an effective DD
framework named AST, standing for Alignment with Smooth and high-quality expert
Trajectories. We devise the integration of clipping loss and gradient penalty
to regulate the rate of parameter changes in expert trajectory generation. To
further refine the student parameter alignment with expert trajectory, we put
forward representative initialization for the synthetic dataset and balanced
inner-loop loss in response to the sensitivity exhibited towards randomly
initialized variables during distillation. We also propose two enhancement
strategies, namely intermediate matching loss and weight perturbation, to
mitigate the potential occurrence of cumulative errors. We conduct extensive
experiments on datasets of different scales, sizes, and resolutions. The
results demonstrate that the proposed method significantly outperforms prior
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11188">Adversarial Bandits with Multi-User Delayed Feedback: Theory and Application. (arXiv:2310.11188v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yandi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianxiong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yupeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1">Weijia Jia</a></p>
<p>The multi-armed bandit (MAB) models have attracted significant research
attention due to their applicability and effectiveness in various real-world
scenarios such as resource allocation, online advertising, and dynamic pricing.
As an important branch, the adversarial MAB problems with delayed feedback have
been proposed and studied by many researchers recently where a conceptual
adversary strategically selects the reward distributions associated with each
arm to challenge the learning algorithm and the agent experiences a delay
between taking an action and receiving the corresponding reward feedback.
However, the existing models restrict the feedback to be generated from only
one user, which makes models inapplicable to the prevailing scenarios of
multiple users (e.g. ad recommendation for a group of users). In this paper, we
consider that the delayed feedback results are from multiple users and are
unrestricted on internal distribution. In contrast, the feedback delay is
arbitrary and unknown to the player in advance. Also, for different users in a
round, the delays in feedback have no assumption of latent correlation. Thus,
we formulate an adversarial MAB problem with multi-user delayed feedback and
design a modified EXP3 algorithm MUD-EXP3, which makes a decision at each round
by considering the importance-weighted estimator of the received feedback from
different users. On the premise of known terminal round index $T$, the number
of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we
prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$.
Furthermore, for the more common case of unknown $T$, an adaptive algorithm
AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally,
extensive experiments are conducted to indicate the correctness and
effectiveness of our algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13121">Understanding Addition in Transformers. (arXiv:2310.13121v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quirke_P/0/1/0/all/0/1">Philip Quirke</a>, <a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1">Fazl Barez</a></p>
<p>Understanding the inner workings of machine learning models like Transformers
is vital for their safe and ethical use. This paper presents an in-depth
analysis of a one-layer Transformer model trained for n-digit integer addition.
We reveal that the model divides the task into parallel, digit-specific streams
and employs distinct algorithms for different digit positions. Our study also
finds that the model starts calculations late but executes them rapidly. A rare
use case with high loss is identified and explained. Overall, the model's
algorithm is explained in detail. These findings are validated through rigorous
testing and mathematical modeling, contributing to the broader works in
Mechanistic Interpretability, AI safety, and alignment. Our approach opens the
door for analyzing more complex tasks and multi-layer Transformer models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13258">ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kedia_K/0/1/0/all/0/1">Kushal Kedia</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_P/0/1/0/all/0/1">Prithwish Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1">Atiksh Bhardwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1">Sanjiban Choudhury</a></p>
<p>Seamless human-robot manipulation in close proximity relies on accurate
forecasts of human motion. While there has been significant progress in
learning forecast models at scale, when applied to manipulation tasks, these
models accrue high errors at critical transition points leading to degradation
in downstream planning performance. Our key insight is that instead of
predicting the most likely human motion, it is sufficient to produce forecasts
that capture how future human motion would affect the cost of a robot's plan.
We present ManiCast, a novel framework that learns cost-aware human forecasts
and feeds them to a model predictive control planner to execute collaborative
manipulation tasks. Our framework enables fluid, real-time interactions between
a human and a 7-DoF robot arm across a number of real-world tasks such as
reactive stirring, object handovers, and collaborative table setting. We
evaluate both the motion forecasts and the end-to-end forecaster-planner system
against a range of learned and heuristic baselines while additionally
contributing new datasets. We release our code and datasets at
https://portal-cornell.github.io/manicast/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15848">On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Surbhi Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakral_K/0/1/0/all/0/1">Kartik Thakral</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Richa Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1">Mayank Vatsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Glaser_T/0/1/0/all/0/1">Tamar Glaser</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1">Cristian Canton Ferrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1">Tal Hassner</a></p>
<p>Artificial Intelligence (AI) has made its way into various scientific fields,
providing astonishing improvements over existing algorithms for a wide variety
of tasks. In recent years, there have been severe concerns over the
trustworthiness of AI technologies. The scientific community has focused on the
development of trustworthy AI algorithms. However, machine and deep learning
algorithms, popular in the AI community today, depend heavily on the data used
during their development. These learning algorithms identify patterns in the
data, learning the behavioral objective. Any flaws in the data have the
potential to translate directly into algorithms. In this study, we discuss the
importance of Responsible Machine Learning Datasets and propose a framework to
evaluate the datasets through a responsible rubric. While existing work focuses
on the post-hoc evaluation of algorithms for their trustworthiness, we provide
a framework that considers the data component separately to understand its role
in the algorithm. We discuss responsible datasets through the lens of fairness,
privacy, and regulatory compliance and provide recommendations for constructing
future datasets. After surveying over 100 datasets, we use 60 datasets for
analysis and demonstrate that none of these datasets is immune to issues of
fairness, privacy preservation, and regulatory compliance. We provide
modifications to the ``datasheets for datasets" with important additions for
improved dataset documentation. With governments around the world regularizing
data protection laws, the method for the creation of datasets in the scientific
community requires revision. We believe this study is timely and relevant in
today's era of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16842">Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chao Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1">Tianheng Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiele_G/0/1/0/all/0/1">Gregor Schiele</a></p>
<p>To process sensor data in the Internet of Things(IoTs), embedded deep
learning for 1-dimensional data is an important technique. In the past, CNNs
were frequently used because they are simple to optimise for special embedded
hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed
at energy-efficient inference on end devices. Using the traffic speed
prediction as a case study, a vanilla LSTM model with the optimised LSTM cell
achieves 17534 inferences per second while consuming only 3.8 $\mu$J per
inference on the FPGA XC7S15 from Spartan-7 family. It achieves at least
5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than
existing approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18285">Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning. (arXiv:2310.18285v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Wenlong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1">Christos Thrampoulidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxiao Li</a></p>
<p>Vision Transformers (ViT) and Visual Prompt Tuning (VPT) achieve
state-of-the-art performance with improved efficiency in various computer
vision tasks. This suggests a promising paradigm shift of adapting pre-trained
ViT models to Federated Learning (FL) settings. However, the challenge of data
heterogeneity among FL clients presents a significant hurdle in effectively
deploying ViT models. Existing Generalized FL (GFL) and Personalized FL (PFL)
methods have limitations in balancing performance across both global and local
data distributions. In this paper, we present a novel algorithm, SGPT, that
integrates GFL and PFL approaches by employing a unique combination of both
shared and group-specific prompts. This design enables SGPT to capture both
common and group-specific features. A key feature of SGPT is its prompt
selection module, which facilitates the training of a single global model
capable of automatically adapting to diverse local client data distributions
without the need for local fine-tuning. To effectively train the prompts, we
utilize block coordinate descent (BCD), learning from common feature
information (shared prompts), and then more specialized knowledge (group
prompts) iteratively. Theoretically, we justify that learning the proposed
prompts can reduce the gap between global and local performance. Empirically,
we conduct experiments on both label and feature heterogeneity settings in
comparison with state-of-the-art baselines, along with extensive ablation
studies, to substantiate the superior performance of SGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20587">Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning. (arXiv:2310.20587v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Ruizhe Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuyao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ze_Y/0/1/0/all/0/1">Yanjie Ze</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon S. Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a></p>
<p>Offline reinforcement learning (RL) aims to find a near-optimal policy using
pre-collected datasets. In real-world scenarios, data collection could be
costly and risky; therefore, offline RL becomes particularly challenging when
the in-domain data is limited. Given recent advances in Large Language Models
(LLMs) and their few-shot learning prowess, this paper introduces
$\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a
general framework based on Decision Transformers to effectively use pre-trained
Language Models (LMs) for offline RL. Our framework highlights four crucial
components: (1) Initializing Decision Transformers with sequentially
pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to
full-weight fine-tuning, to combine the pre-trained knowledge from LMs and
in-domain knowledge effectively, (3) using the non-linear MLP transformation
instead of linear projections, to generate embeddings, and (4) integrating an
auxiliary language prediction loss during fine-tuning to stabilize the LMs and
retain their original abilities on languages. Empirical results indicate
$\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks
and closes the gap between value-based offline RL methods and decision
transformers in dense-reward tasks. In particular, our method demonstrates
superior performance in scenarios with limited data samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00341">The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture. (arXiv:2311.00341v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Sriram_A/0/1/0/all/0/1">Anuroop Sriram</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Choi_S/0/1/0/all/0/1">Sihoon Choi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yu_X/0/1/0/all/0/1">Xiaohan Yu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Brabson_L/0/1/0/all/0/1">Logan M. Brabson</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Das_A/0/1/0/all/0/1">Abhishek Das</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ulissi_Z/0/1/0/all/0/1">Zachary Ulissi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Uyttendaele_M/0/1/0/all/0/1">Matt Uyttendaele</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Medford_A/0/1/0/all/0/1">Andrew J. Medford</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sholl_D/0/1/0/all/0/1">David S. Sholl</a></p>
<p>New methods for carbon dioxide removal are urgently needed to combat global
climate change. Direct air capture (DAC) is an emerging technology to capture
carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have
been widely studied as potentially customizable adsorbents for DAC. However,
discovering promising MOF sorbents for DAC is challenging because of the vast
chemical space to explore and the need to understand materials as functions of
humidity and temperature. We explore a computational approach benefiting from
recent innovations in machine learning (ML) and present a dataset named Open
DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT)
calculations on more than 8,400 MOF materials containing adsorbed $CO_2$ and/or
$H_2O$. ODAC23 is by far the largest dataset of MOF adsorption calculations at
the DFT level of accuracy currently available. In addition to probing
properties of adsorbed molecules, the dataset is a rich source of information
on structural relaxation of MOFs, which will be useful in many contexts beyond
specific applications for DAC. A large number of MOFs with promising properties
for DAC are identified directly in ODAC23. We also trained state-of-the-art ML
models on this dataset to approximate calculations at the DFT level. This
open-source dataset and our initial ML models will provide an important
baseline for future efforts to identify MOFs for a wide range of applications,
including DAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01276">Long-Range Neural Atom Learning for Molecular Graphs. (arXiv:2311.01276v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanke Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiangchao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1">Yu Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a></p>
<p>Graph Neural Networks (GNNs) have been widely adopted for drug discovery with
molecular graphs. Nevertheless, current GNNs are mainly good at leveraging
short-range interactions (SRI) but struggle to capture long-range interactions
(LRI), both of which are crucial for determining molecular properties. To
tackle this issue, we propose a method that implicitly projects all original
atoms into a few Neural Atoms, which abstracts the collective information of
atomic groups within a molecule. Specifically, we explicitly exchange the
information among neural atoms and project them back to the atoms'
representations as an enhancement. With this mechanism, neural atoms establish
the communication channels among distant nodes, effectively reducing the
interaction scope of arbitrary node pairs into a single hop. To provide an
inspection of our method from a physical perspective, we reveal its connection
with the traditional LRI calculation method, Ewald Summation. We conduct
extensive experiments on three long-range graph benchmarks, covering both
graph-level and link-level tasks on molecular graphs. We empirically justify
that our method can be equipped with an arbitrary GNN and help to capture LRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05858">Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation. (arXiv:2311.05858v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Junyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1">Hyeongjun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1">Ilhoon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kwanghoon Sohn</a></p>
<p>Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07590">Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Scheurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1">Mikita Balesni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1">Marius Hobbhahn</a></p>
<p>We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09247">Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. (arXiv:2311.09247v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1">Melanie Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Palmarini_A/0/1/0/all/0/1">Alessandro B. Palmarini</a>, <a href="http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1">Arseny Moskvichev</a></p>
<p>We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09740">Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations. (arXiv:2311.09740v3 [physics.flu-dyn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Sarkar_R/0/1/0/all/0/1">Rajat Kumar Sarkar</a>, <a href="http://arxiv.org/find/physics/1/au:+Majumdar_R/0/1/0/all/0/1">Ritam Majumdar</a>, <a href="http://arxiv.org/find/physics/1/au:+Jadhav_V/0/1/0/all/0/1">Vishal Jadhav</a>, <a href="http://arxiv.org/find/physics/1/au:+Sakhinana_S/0/1/0/all/0/1">Sagar Srinivas Sakhinana</a>, <a href="http://arxiv.org/find/physics/1/au:+Runkana_V/0/1/0/all/0/1">Venkataramana Runkana</a></p>
<p>In Computational Fluid Dynamics (CFD), coarse mesh simulations offer
computational efficiency but often lack precision. Applying conventional
super-resolution to these simulations poses a significant challenge due to the
fundamental contrast between downsampling high-resolution images and
authentically emulating low-resolution physics. The former method conserves
more of the underlying physics, surpassing the usual constraints of real-world
scenarios. We propose a novel definition of super-resolution tailored for
PDE-based problems. Instead of simply downsampling from a high-resolution
dataset, we use coarse-grid simulated data as our input and predict fine-grid
simulated outcomes. Employing a physics-infused UNet upscaling method, we
demonstrate its efficacy across various 2D-CFD problems such as discontinuity
detection in Burger's equation, Methane combustion, and fouling in Industrial
heat exchangers. Our method enables the generation of fine-mesh solutions
bypassing traditional simulation, ensuring considerable computational saving
and fidelity to the original ground truth outcomes. Through diverse boundary
conditions during training, we further establish the robustness of our method,
paving the way for its broad applications in engineering and scientific CFD
solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10093">The Chosen One: Consistent Characters in Text-to-Image Diffusion Models. (arXiv:2311.10093v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1">Omri Avrahami</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1">Yael Vinker</a>, <a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1">Moab Arar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1">Ohad Fried</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1">Dani Lischinski</a></p>
<p>Recent advances in text-to-image generation models have unlocked vast
potential for visual creativity. However, these models struggle with generation
of consistent characters, a crucial aspect for numerous real-world applications
such as story visualization, game development asset design, advertising, and
more. Current methods typically rely on multiple pre-existing images of the
target character or involve labor-intensive manual processes. In this work, we
propose a fully automated solution for consistent character generation, with
the sole input being a text prompt. We introduce an iterative procedure that,
at each stage, identifies a coherent set of images sharing a similar identity
and extracts a more consistent identity from this set. Our quantitative
analysis demonstrates that our method strikes a better balance between prompt
alignment and identity consistency compared to the baseline methods, and these
findings are reinforced by a user study. To conclude, we showcase several
practical applications of our approach. Project page is available at
https://omriavrahami.com/the-chosen-one
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10321">Towards Machine Learning-based Quantitative Hyperspectral Image Guidance for Brain Tumor Resection. (arXiv:2311.10321v2 [q-bio.TO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Black_D/0/1/0/all/0/1">David Black</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Byrne_D/0/1/0/all/0/1">Declan Byrne</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Walke_A/0/1/0/all/0/1">Anna Walke</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_S/0/1/0/all/0/1">Sidong Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+leva_A/0/1/0/all/0/1">Antonio Di leva</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kaneko_S/0/1/0/all/0/1">Sadahiro Kaneko</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stummer_W/0/1/0/all/0/1">Walter Stummer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Salcudean_S/0/1/0/all/0/1">Septimiu Salcudean</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Molina_E/0/1/0/all/0/1">Eric Suero Molina</a></p>
<p>Complete resection of malignant gliomas is hampered by the difficulty in
distinguishing tumor cells at the infiltration zone. Fluorescence guidance with
5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work
characterized five fluorophores' emission spectra in most human brain tumors.
In this paper, the effectiveness of these five spectra was explored for
different tumor and tissue classification tasks in 184 patients (891
hyperspectral measurements) harboring low- (n=30) and high-grade gliomas
(n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2),
miscellaneous (n=10) and metastases (n=8). Four machine learning models were
trained to classify tumor type, grade, glioma margins and IDH mutation. Using
random forests and multi-layer perceptrons, the classifiers achieved average
test accuracies of 84-87%, 96%, 86%, and 93% respectively. All five fluorophore
abundances varied between tumor margin types and tumor grades (p &lt; 0.01). For
tissue type, at least four of the five fluorophore abundances were found to be
significantly different (p &lt; 0.01) between all classes. These results
demonstrate the fluorophores' differing abundances in different tissue classes,
as well as the value of the five fluorophores as potential optical biomarkers,
opening new opportunities for intraoperative classification systems in
fluorescence-guided neurosurgery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11225">TextGuard: Provable Defense against Backdoor Attacks on Text Classification. (arXiv:2311.11225v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hengzhi Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinyuan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wenbo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a></p>
<p>Backdoor attacks have become a major security threat for deploying machine
learning models in security-critical applications. Existing research endeavors
have proposed many defenses against backdoor attacks. Despite demonstrating
certain empirical defense efficacy, none of these techniques could provide a
formal and provable security guarantee against arbitrary attacks. As a result,
they can be easily broken by strong adaptive attacks, as shown in our
evaluation. In this work, we propose TextGuard, the first provable defense
against backdoor attacks on text classification. In particular, TextGuard first
divides the (backdoored) training data into sub-training sets, achieved by
splitting each training sentence into sub-sentences. This partitioning ensures
that a majority of the sub-training sets do not contain the backdoor trigger.
Subsequently, a base classifier is trained from each sub-training set, and
their ensemble provides the final prediction. We theoretically prove that when
the length of the backdoor trigger falls within a certain threshold, TextGuard
guarantees that its prediction will remain unaffected by the presence of the
triggers in training and testing inputs. In our evaluation, we demonstrate the
effectiveness of TextGuard on three benchmark text classification tasks,
surpassing the certification accuracy of existing certified defenses against
backdoor attacks. Furthermore, we propose additional strategies to enhance the
empirical performance of TextGuard. Comparisons with state-of-the-art empirical
defenses validate the superiority of TextGuard in countering multiple backdoor
attacks. Our code and data are available at
https://github.com/AI-secure/TextGuard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11235">Unraveling the &quot;Anomaly&quot; in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution. (arXiv:2311.11235v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuting Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1">Guanhua Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hongzhi Yin</a></p>
<p>The ongoing challenges in time series anomaly detection (TSAD), notably the
scarcity of anomaly labels and the variability in anomaly lengths and shapes,
have led to the need for a more efficient solution. As limited anomaly labels
hinder traditional supervised models in TSAD, various SOTA deep learning
techniques, such as self-supervised learning, have been introduced to tackle
this issue. However, they encounter difficulties handling variations in anomaly
lengths and shapes, limiting their adaptability to diverse anomalies.
Additionally, many benchmark datasets suffer from the problem of having
explicit anomalies that even random functions can detect. This problem is
exacerbated by ill-posed evaluation metrics, known as point adjustment (PA),
which can result in inflated model performance. In this context, we propose a
novel self-supervised learning based Tri-domain Anomaly Detector (TriAD), which
addresses these challenges by modeling features across three data domains -
temporal, frequency, and residual domains - without relying on anomaly labels.
Unlike traditional contrastive learning methods, TriAD employs both
inter-domain and intra-domain contrastive loss to learn common attributes among
normal data and differentiate them from anomalies. Additionally, our approach
can detect anomalies of varying lengths by integrating with a discord discovery
algorithm. It is worth noting that this study is the first to reevaluate the
deep learning potential in TSAD, utilizing both rigorously designed datasets
(i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation).
Through experimental results on the UCR dataset, TriAD achieves an impressive
three-fold increase in PA%K based F1 scores over SOTA deep learning models, and
50% increase of accuracy as compared to SOTA discord discovery algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11509">Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information. (arXiv:2311.11509v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhengmian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Saayan Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1">Viswanathan Swaminathan</a></p>
<p>In recent years, Large Language Models (LLM) have emerged as pivotal tools in
various applications. However, these models are susceptible to adversarial
prompt attacks, where attackers can carefully curate input strings that lead to
undesirable outputs. The inherent vulnerability of LLMs stems from their
input-output mechanisms, especially when presented with intensely
out-of-distribution (OOD) inputs. This paper proposes a token-level detection
method to identify adversarial prompts, leveraging the LLM's capability to
predict the next token's probability. We measure the degree of the model's
perplexity and incorporate neighboring token information to encourage the
detection of contiguous adversarial prompt sequences. As a result, we propose
two methods: one that identifies each token as either being part of an
adversarial prompt or not, and another that estimates the probability of each
token being part of an adversarial prompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11913">Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks. (arXiv:2311.11913v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stillman_N/0/1/0/all/0/1">Namid R. Stillman</a>, <a href="http://arxiv.org/find/cs/1/au:+Baggott_R/0/1/0/all/0/1">Rory Baggott</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyon_J/0/1/0/all/0/1">Justin Lyon</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianfei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dingqiu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vytelingum_P/0/1/0/all/0/1">Perukrishnen Vytelingum</a></p>
<p>The ability to construct a realistic simulator of financial exchanges,
including reproducing the dynamics of the limit order book, can give insight
into many counterfactual scenarios, such as a flash crash, a margin call, or
changes in macroeconomic outlook. In recent years, agent-based models have been
developed that reproduce many features of an exchange, as summarised by a set
of stylised facts and statistics. However, the ability to calibrate simulators
to a specific period of trading remains an open challenge. In this work, we
develop a novel approach to the calibration of market simulators by leveraging
recent advances in deep learning, specifically using neural density estimators
and embedding networks. We demonstrate that our approach is able to correctly
identify high probability parameter sets, both when applied to synthetic and
historical data, and without reliance on manually selected or weighted
ensembles of stylised facts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12530">An efficient likelihood-free Bayesian inference method based on sequential neural posterior estimation. (arXiv:2311.12530v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xiong_Y/0/1/0/all/0/1">Yifei Xiong</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1">Xiliang Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1">Sanguo Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1">Zhijian He</a></p>
<p>Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
Unlike approximate Bayesian computation, SNPE techniques learn the posterior
from sequential simulation using neural network-based conditional density
estimators by minimizing a specific loss function. The SNPE method proposed by
Lueckmann et al. (2017) used a calibration kernel to boost the sample weights
around the observed data, resulting in a concentrated loss function. However,
the use of calibration kernels may increase the variances of both the empirical
loss and its gradient, making the training inefficient. To improve the
stability of SNPE, this paper proposes to use an adaptive calibration kernel
and several variance reduction techniques. The proposed method greatly speeds
up the process of training, and provides a better approximation of the
posterior than the original SNPE method and some existing competitors as
confirmed by numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12570">BEND: Benchmarking DNA Language Models on biologically meaningful tasks. (arXiv:2311.12570v2 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Marin_F/0/1/0/all/0/1">Frederikke Isa Marin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Teufel_F/0/1/0/all/0/1">Felix Teufel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Horlacher_M/0/1/0/all/0/1">Marc Horlacher</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Madsen_D/0/1/0/all/0/1">Dennis Madsen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pultz_D/0/1/0/all/0/1">Dennis Pultz</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Boomsma_W/0/1/0/all/0/1">Wouter Boomsma</a></p>
<p>The genome sequence contains the blueprint for governing cellular processes.
While the availability of genomes has vastly increased over the last decades,
experimental annotation of the various functional, non-coding and regulatory
elements encoded in the DNA sequence remains both expensive and challenging.
This has sparked interest in unsupervised language modeling of genomic DNA, a
paradigm that has seen great success for protein sequence data. Although
various DNA language models have been proposed, evaluation tasks often differ
between individual works, and might not fully recapitulate the fundamental
challenges of genome annotation, including the length, scale and sparsity of
the data. In this study, we introduce BEND, a Benchmark for DNA language
models, featuring a collection of realistic and biologically meaningful
downstream tasks defined on the human genome. We find that embeddings from
current DNA LMs can approach performance of expert methods on some tasks, but
only capture limited information about long-range features. BEND is available
at https://github.com/frederikkemarin/BEND.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12612">A New Type Of Upper And Lower Bounds On Right-Tail Probabilities Of Continuous Random Variables. (arXiv:2311.12612v3 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zlatanov_N/0/1/0/all/0/1">Nikola Zlatanov</a></p>
<p>In this paper, I present a completely new type of upper and lower bounds on
the right-tail probabilities of continuous random variables with unbounded
support and with semi-bounded support from the left. The presented upper and
lower right-tail bounds depend only on the probability density function (PDF),
its first derivative, and two parameters that are used for tightening the
bounds. These tail bounds hold under certain conditions that depend on the PDF,
its first and second derivatives, and the two parameters. The new tail bounds
are shown to be tight for a wide range of continuous random variables via
numerical examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13265">Improved identification accuracy in equation learning via comprehensive $\boldsymbol{R^2}$-elimination and Bayesian model selection. (arXiv:2311.13265v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Nickelsen_D/0/1/0/all/0/1">Daniel Nickelsen</a>, <a href="http://arxiv.org/find/stat/1/au:+Bah_B/0/1/0/all/0/1">Bubacarr Bah</a></p>
<p>In the field of equation learning, exhaustively considering all possible
equations derived from a basis function dictionary is infeasible. Sparse
regression and greedy algorithms have emerged as popular approaches to tackle
this challenge. However, the presence of multicollinearity poses difficulties
for sparse regression techniques, and greedy steps may inadvertently exclude
terms of the true equation, leading to reduced identification accuracy. In this
article, we present an approach that strikes a balance between
comprehensiveness and efficiency in equation learning. Inspired by stepwise
regression, our approach combines the coefficient of determination, $R^2$, and
the Bayesian model evidence, $p(\boldsymbol y|\mathcal M)$, in a novel way. Our
procedure is characterized by a comprehensive search with just a minor
reduction of the model space at each iteration step. With two flavors of our
approach and the adoption of $p(\boldsymbol y|\mathcal M)$ for bi-directional
stepwise regression, we present a total of three new avenues for equation
learning. Through three extensive numerical experiments involving random
polynomials and dynamical systems, we compare our approach against four
state-of-the-art methods and two standard approaches. The results demonstrate
that our comprehensive search approach surpasses all other methods in terms of
identification accuracy. In particular, the second flavor of our approach
establishes an efficient overfitting penalty solely based on $R^2$, which
achieves highest rates of exact equation recovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13580">$\sigma$-PCA: a unified neural model for linear and nonlinear principal component analysis. (arXiv:2311.13580v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1">Fahdi Kanavati</a>, <a href="http://arxiv.org/find/cs/1/au:+Katsnith_L/0/1/0/all/0/1">Lucy Katsnith</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsuneki_M/0/1/0/all/0/1">Masayuki Tsuneki</a></p>
<p>Linear principal component analysis (PCA), nonlinear PCA, and linear
independent component analysis (ICA) -- those are three methods with
single-layer autoencoder formulations for learning linear transformations from
data. Linear PCA learns orthogonal transformations (rotations) that orient axes
to maximise variance, but it suffers from a subspace rotational indeterminacy:
it fails to find a unique rotation for axes that share the same variance. Both
nonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational
to permutational by maximising statistical independence under the assumption of
unit variance. The relationship between all three can be understood by the
singular value decomposition of the linear ICA transformation into a sequence
of rotation, scale, rotation. Linear PCA learns the first rotation; nonlinear
PCA learns the second. The scale is simply the inverse of the standard
deviations. The problem is that, in contrast to linear PCA, conventional
nonlinear PCA cannot be used directly on the data to learn the first rotation,
the first being special as it reduces dimensionality and orders by variances.
In this paper, we have identified the cause, and as a solution we propose
$\sigma$-PCA: a unified neural model for linear and nonlinear PCA as
single-layer autoencoders. One of its key ingredients: modelling not just the
rotation but also the scale -- the variances. This model bridges the disparity
between linear and nonlinear PCA. And so, like linear PCA, it can learn a
semi-orthogonal transformation that reduces dimensionality and orders by
variances, but, unlike linear PCA, it does not suffer from rotational
indeterminacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13959">RankFeat&amp;RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection. (arXiv:2311.13959v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yue Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a></p>
<p>The task of out-of-distribution (OOD) detection is crucial for deploying
machine learning models in real-world settings. In this paper, we observe that
the singular value distributions of the in-distribution (ID) and OOD features
are quite different: the OOD feature matrix tends to have a larger dominant
singular value than the ID feature, and the class predictions of OOD samples
are largely determined by it. This observation motivates us to propose
\texttt{RankFeat}, a simple yet effective \emph{post hoc} approach for OOD
detection by removing the rank-1 matrix composed of the largest singular value
and the associated singular vectors from the high-level feature.
\texttt{RankFeat} achieves \emph{state-of-the-art} performance and reduces the
average false positive rate (FPR95) by 17.90\% compared with the previous best
method. The success of \texttt{RankFeat} motivates us to investigate whether a
similar phenomenon would exist in the parameter matrices of neural networks. We
thus propose \texttt{RankWeight} which removes the rank-1 weight from the
parameter matrices of a single deep layer. Our \texttt{RankWeight}is also
\emph{post hoc} and only requires computing the rank-1 matrix once. As a
standalone approach, \texttt{RankWeight} has very competitive performance
against other methods across various backbones. Moreover, \texttt{RankWeight}
enjoys flexible compatibility with a wide range of OOD detection methods. The
combination of \texttt{RankWeight} and \texttt{RankFeat} refreshes the new
\emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\% on
the ImageNet-1k benchmark. Extensive ablation studies and comprehensive
theoretical analyses are presented to support the empirical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14078">Machine learning-based decentralized TDMA for VLC IoT networks. (arXiv:2311.14078v2 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Makvandi_A/0/1/0/all/0/1">Armin Makvandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kavian_Y/0/1/0/all/0/1">Yousef Seifi Kavian</a></p>
<p>In this paper, a machine learning-based decentralized time division multiple
access (TDMA) algorithm for visible light communication (VLC) Internet of
Things (IoT) networks is proposed. The proposed algorithm is based on
Q-learning, a reinforcement learning algorithm. This paper considers a
decentralized condition in which there is no coordinator node for sending
synchronization frames and assigning transmission time slots to other nodes.
The proposed algorithm uses a decentralized manner for synchronization, and
each node uses the Q-learning algorithm to find the optimal transmission time
slot for sending data without collisions. The proposed algorithm is implemented
on a VLC hardware system, which had been designed and implemented in our
laboratory. Average reward, convergence time, goodput, average delay, and data
packet size are evaluated parameters. The results show that the proposed
algorithm converges quickly and provides collision-free decentralized TDMA for
the network. The proposed algorithm is compared with carrier-sense multiple
access with collision avoidance (CSMA/CA) algorithm as a potential selection
for decentralized VLC IoT networks. The results show that the proposed
algorithm provides up to 61% more goodput and up to 49% less average delay than
CSMA/CA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14412">A Comparison of PDF Projection with Normalizing Flows and SurVAE. (arXiv:2311.14412v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baggenstoss_P/0/1/0/all/0/1">Paul M. Baggenstoss</a>, <a href="http://arxiv.org/find/cs/1/au:+Govaers_F/0/1/0/all/0/1">Felix Govaers</a></p>
<p>Normalizing flows (NF) recently gained attention as a way to construct
generative networks with exact likelihood calculation out of composable layers.
However, NF is restricted to dimension-preserving transformations. Surjection
VAE (SurVAE) has been proposed to extend NF to dimension-altering
transformations. Such networks are desirable because they are expressive and
can be precisely trained. We show that the approaches are a re-invention of PDF
projection, which appeared over twenty years earlier and is much further
developed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14468">Efficient Gradient Estimation via Adaptive Sampling and Importance Sampling. (arXiv:2311.14468v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salaun_C/0/1/0/all/0/1">Corentin Sala&#xfc;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xingchang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgiev_I/0/1/0/all/0/1">Iliyan Georgiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy J. Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1">Gurprit Singh</a></p>
<p>Machine learning problems rely heavily on stochastic gradient descent (SGD)
for optimization. The effectiveness of SGD is contingent upon accurately
estimating gradients from a mini-batch of data samples. Instead of the commonly
used uniform sampling, adaptive or importance sampling reduces noise in
gradient estimation by forming mini-batches that prioritize crucial data
points. Previous research has suggested that data points should be selected
with probabilities proportional to their gradient norm. Nevertheless, existing
algorithms have struggled to efficiently integrate importance sampling into
machine learning frameworks. In this work, we make two contributions. First, we
present an algorithm that can incorporate existing importance functions into
our framework. Second, we propose a simplified importance function that relies
solely on the loss gradient of the output layer. By leveraging our proposed
gradient estimation techniques, we observe improved convergence in
classification and regression tasks with minimal computational overhead. We
validate the effectiveness of our adaptive and importance-sampling approach on
image and point-cloud datasets.
</p>
</p>
</div>

    </div>
    </body>
    