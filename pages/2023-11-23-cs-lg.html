<!DOCTYPE html>
<html>
<head>
<title>2023-11-23-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.12035">Delta Score: Improving the Binding Assessment of Structure-Based Drug Design Methods. (arXiv:2311.12035v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ren_M/0/1/0/all/0/1">Minsi Ren</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gao_B/0/1/0/all/0/1">Bowen Gao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Qiang_B/0/1/0/all/0/1">Bo Qiang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lan_Y/0/1/0/all/0/1">Yanyan Lan</a></p>
<p>Structure-based drug design (SBDD) stands at the forefront of drug discovery,
emphasizing the creation of molecules that target specific binding pockets.
Recent advances in this area have witnessed the adoption of deep generative
models and geometric deep learning techniques, modeling SBDD as a conditional
generation task where the target structure serves as context. Historically,
evaluation of these models centered on docking scores, which quantitatively
depict the predicted binding affinity between a molecule and its target pocket.
Though state-of-the-art models purport that a majority of their generated
ligands exceed the docking score of ground truth ligands in test sets, it begs
the question: Do these scores align with real-world biological needs? In this
paper, we introduce the delta score, a novel evaluation metric grounded in
tangible pharmaceutical requisites. Our experiments reveal that molecules
produced by current deep generative models significantly lag behind ground
truth reference ligands when assessed with the delta score. This novel metric
not only complements existing benchmarks but also provides a pivotal direction
for subsequent research in the domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12040">TransCDR: a deep learning model for enhancing the generalizability of cancer drug response prediction through transfer learning and multimodal data fusion for drug representation. (arXiv:2311.12040v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Xia_X/0/1/0/all/0/1">Xiaoqiong Xia</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhu_C/0/1/0/all/0/1">Chaoyu Zhu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shan_Y/0/1/0/all/0/1">Yuqi Shan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhong_F/0/1/0/all/0/1">Fan Zhong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_L/0/1/0/all/0/1">Lei Liu</a></p>
<p>Accurate and robust drug response prediction is of utmost importance in
precision medicine. Although many models have been developed to utilize the
representations of drugs and cancer cell lines for predicting cancer drug
responses (CDR), their performances can be improved by addressing issues such
as insufficient data modality, suboptimal fusion algorithms, and poor
generalizability for novel drugs or cell lines. We introduce TransCDR, which
uses transfer learning to learn drug representations and fuses multi-modality
features of drugs and cell lines by a self-attention mechanism, to predict the
IC50 values or sensitive states of drugs on cell lines. We are the first to
systematically evaluate the generalization of the CDR prediction model to novel
(i.e., never-before-seen) compound scaffolds and cell line clusters. TransCDR
shows better generalizability than 8 state-of-the-art models. TransCDR
outperforms its 5 variants that train drug encoders (i.e., RNN and AttentiveFP)
from scratch under various scenarios. The most critical contributors among
multiple drug notations and omics profiles are Extended Connectivity
Fingerprint and genetic mutation. Additionally, the attention-based fusion
module further enhances the predictive performance of TransCDR. TransCDR,
trained on the GDSC dataset, demonstrates strong predictive performance on the
external testing set CCLE. It is also utilized to predict missing CDRs on GDSC.
Moreover, we investigate the biological mechanisms underlying drug response by
classifying 7,675 patients from TCGA into drug-sensitive or drug-resistant
groups, followed by a Gene Set Enrichment Analysis. TransCDR emerges as a
potent tool with significant potential in drug response prediction. The source
code and data can be accessed at https://github.com/XiaoqiongXia/TransCDR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12041">Automated Detection of hidden Damages and Impurities in Aluminum Die Casting Materials and Fibre-Metal Laminates using Low-quality X-ray Radiography, Synthetic X-ray Data Augmentation by Simulation, and Machine Learning. (arXiv:2311.12041v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bosse_S/0/1/0/all/0/1">Stefan Bosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmhus_D/0/1/0/all/0/1">Dirk Lehmhus</a></p>
<p>Detection and characterization of hidden defects, impurities, and damages in
layered composites like Fibre laminates, e.g., Fibre Metal Laminates (FML), as
well as in monolithic materials, e.g., aluminum die casting materials, is still
a challenge. This work discusses methods and challenges in data-driven modeling
of automated damage and defect detectors using X-ray single- and
multi-projection (CT) images. Three main issues are identified: Data and
feature variance, data feature labeling (for supervised machine learning), and
the missing ground truth. It will be shown that only simulation of data can
deliver a ground truth data set and accurate labeling. Noise has significant
impact on the feature detection and will be discussed. Data-driven feature
detectors are implemented with semantic pixel- or z-profile Convolutional
Neural Networks and LSTM Auto-encoders. Data is measured with three different
devices: A low-quality and low-cost (Low-Q), a mid- and a high-quality
(micro-CT, Mid-/High-Q) device. The goals of this work are the training of
robust and generalized feature detectors with synthetic data and the transition
from High- and Mid-Q laboratory measuring technologies towards in-field usable
technologies and methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12048">One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning. (arXiv:2311.12048v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Doyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Susik Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1">Dongmin Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Youngjun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hwanjun Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1">Jihwan Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jae-Gil Lee</a></p>
<p>In real-world continual learning scenarios, tasks often exhibit intricate and
unpredictable semantic shifts, posing challenges for fixed prompt management
strategies. We identify the inadequacy of universal and specific prompting in
handling these dynamic shifts. Universal prompting is ineffective for tasks
with abrupt semantic changes, while specific prompting struggles with
overfitting under mild semantic shifts. To overcome these limitations, we
propose an adaptive prompting approach that tailors minimal yet sufficient
prompts based on the task semantics. Our methodology, SemPrompt, incorporates a
two-level semantic grouping process: macroscopic semantic assignment and
microscopic semantic refinement. This process ensures optimal prompt
utilization for varying task semantics, improving the efficiency and
effectiveness of learning in real-world CL settings. Our experimental results
demonstrate that SemPrompt consistently outperforms existing methods in
adapting to diverse semantic shifts in tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12056">Kuro Siwo: 12.1 billion $m^2$ under the water. A global multi-temporal satellite dataset for rapid flood mapping. (arXiv:2311.12056v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1">Nikolaos Ioannis Bountos</a>, <a href="http://arxiv.org/find/cs/1/au:+Sdraka_M/0/1/0/all/0/1">Maria Sdraka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zavras_A/0/1/0/all/0/1">Angelos Zavras</a>, <a href="http://arxiv.org/find/cs/1/au:+Karasante_I/0/1/0/all/0/1">Ilektra Karasante</a>, <a href="http://arxiv.org/find/cs/1/au:+Karavias_A/0/1/0/all/0/1">Andreas Karavias</a>, <a href="http://arxiv.org/find/cs/1/au:+Herekakis_T/0/1/0/all/0/1">Themistocles Herekakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Thanasou_A/0/1/0/all/0/1">Angeliki Thanasou</a>, <a href="http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1">Dimitrios Michail</a>, <a href="http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1">Ioannis Papoutsis</a></p>
<p>Global floods, exacerbated by climate change, pose severe threats to human
life, infrastructure, and the environment. This urgency is highlighted by
recent catastrophic events in Pakistan and New Zealand, underlining the
critical need for precise flood mapping for guiding restoration efforts,
understanding vulnerabilities, and preparing for future events. While Synthetic
Aperture Radar (SAR) offers day-and-night, all-weather imaging capabilities,
harnessing it for deep learning is hindered by the absence of a large annotated
dataset. To bridge this gap, we introduce Kuro Siwo, a meticulously curated
multi-temporal dataset, spanning 32 flood events globally. Our dataset maps
more than 63 billion m2 of land, with 12.1 billion of them being either a
flooded area or a permanent water body. Kuro Siwo stands out for its
unparalleled annotation quality to facilitate rapid flood mapping in a
supervised setting. We also augment learning by including a large unlabeled set
of SAR samples, aimed at self-supervised pretraining. We provide an extensive
benchmark and strong baselines for a diverse set of flood events from Europe,
America, Africa and Australia. Our benchmark demonstrates the quality of Kuro
Siwo annotations, training models that can achieve $\approx$ 85% and $\approx$
87% in F1-score for flooded areas and general water detection respectively.
This work calls on the deep learning community to develop solution-driven
algorithms for rapid flood mapping, with the potential to aid civil protection
and humanitarian agencies amid climate change challenges. Our code and data
will be made available at https://github.com/Orion-AI-Lab/KuroSiwo
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12068">Enhancing Novel Object Detection via Cooperative Foundational Models. (arXiv:2311.12068v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1">Rohit Bharadwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a></p>
<p>In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12071">Enhancing Low-dose CT Image Reconstruction by Integrating Supervised and Unsupervised Learning. (arXiv:2311.12071v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1">Zhishen Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1">Yong Long</a>, <a href="http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1">Saiprasad Ravishankar</a></p>
<p>Traditional model-based image reconstruction (MBIR) methods combine forward
and noise models with simple object priors. Recent application of deep learning
methods for image reconstruction provides a successful data-driven approach to
addressing the challenges when reconstructing images with undersampled
measurements or various types of noise. In this work, we propose a hybrid
supervised-unsupervised learning framework for X-ray computed tomography (CT)
image reconstruction. The proposed learning formulation leverages both sparsity
or unsupervised learning-based priors and neural network reconstructors to
simulate a fixed-point iteration process. Each proposed trained block consists
of a deterministic MBIR solver and a neural network. The information flows in
parallel through these two reconstructors and is then optimally combined.
Multiple such blocks are cascaded to form a reconstruction pipeline. We
demonstrate the efficacy of this learned hybrid model for low-dose CT image
reconstruction with limited training data, where we use the NIH AAPM Mayo
Clinic Low Dose CT Grand Challenge dataset for training and testing. In our
experiments, we study combinations of supervised deep network reconstructors
and MBIR solver with learned sparse representation-based priors or analytical
priors. Our results demonstrate the promising performance of the proposed
framework compared to recent low-dose CT reconstruction methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12074">SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion Detection and Classification. (arXiv:2311.12074v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuemei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Huirong Fu</a></p>
<p>Numerous studies have proved their effective strength in detecting Control
Area Network (CAN) attacks. In the realm of understanding the human semantic
space, transformer-based models have demonstrated remarkable effectiveness.
Leveraging pre-trained transformers has become a common strategy in various
language-related tasks, enabling these models to grasp human semantics more
comprehensively. To delve into the adaptability evaluation on pre-trained
models for CAN intrusion detection, we have developed two distinct models:
CAN-SecureBERT and CAN-LLAMA2. Notably, our CAN-LLAMA2 model surpasses the
state-of-the-art models by achieving an exceptional performance 0.999993 in
terms of balanced accuracy, precision detection rate, F1 score, and a
remarkably low false alarm rate of 3.10e-6. Impressively, the false alarm rate
is 52 times smaller than that of the leading model, MTH-IDS (Multitiered Hybrid
Intrusion Detection System). Our study underscores the promise of employing a
Large Language Model as the foundational model, while incorporating adapters
for other cybersecurity-related tasks and maintaining the model's inherent
language-related capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12078">Fast Controllable Diffusion Models for Undersampled MRI Reconstruction. (arXiv:2311.12078v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1">Zhuang Xiong</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_N/0/1/0/all/0/1">Nan Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1">Hongfu Sun</a></p>
<p>Supervised deep learning methods have shown promise in Magnetic Resonance
Imaging (MRI) undersampling reconstruction, but their requirement for paired
data limits their generalizability to the diverse MRI acquisition parameters.
Recently, unsupervised controllable generative diffusion models have been
applied to MRI undersampling reconstruction, without paired data or model
retraining for different MRI acquisitions. However, diffusion models are
generally slow in sampling and state-of-the-art acceleration techniques can
lead to sub-optimal results when directly applied to the controllable
generation process. This study introduces a new algorithm called
Predictor-Projector-Noisor (PPN), which enhances and accelerates controllable
generation of diffusion models for MRI undersampling reconstruction. Our
results demonstrate that PPN produces high-fidelity MR images that conform to
undersampled k-space measurements with significantly shorter reconstruction
time than other controllable sampling methods. In addition, the unsupervised
PPN accelerated diffusion models are adaptable to different MRI acquisition
parameters, making them more practical for clinical use than supervised
learning techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12081">Leveraging healthy population variability in deep learning unsupervised anomaly detection in brain FDG PET. (arXiv:2311.12081v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Solal_M/0/1/0/all/0/1">Ma&#xeb;lys Solal</a> (ARAMIS), <a href="http://arxiv.org/find/eess/1/au:+Hassanaly_R/0/1/0/all/0/1">Ravi Hassanaly</a> (ARAMIS), <a href="http://arxiv.org/find/eess/1/au:+Burgos_N/0/1/0/all/0/1">Ninon Burgos</a> (ARAMIS)</p>
<p>Unsupervised anomaly detection is a popular approach for the analysis of
neuroimaging data as it allows to identify a wide variety of anomalies from
unlabelled data. It relies on building a subject-specific model of healthy
appearance to which a subject's image can be compared to detect anomalies. In
the literature, it is common for anomaly detection to rely on analysing the
residual image between the subject's image and its pseudo-healthy
reconstruction. This approach however has limitations partly due to the
pseudo-healthy reconstructions being imperfect and to the lack of natural
thresholding mechanism. Our proposed method, inspired by Z-scores, leverages
the healthy population variability to overcome these limitations. Our
experiments conducted on FDG PET scans from the ADNI database demonstrate the
effectiveness of our approach in accurately identifying Alzheimer's disease
related anomalies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12082">Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging. (arXiv:2311.12082v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rahoof_A/0/1/0/all/0/1">Abdul Rahoof</a>, <a href="http://arxiv.org/find/eess/1/au:+Chaturvedi_V/0/1/0/all/0/1">Vivek Chaturvedi</a>, <a href="http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1">Mahesh Raveendranatha Panicker</a>, <a href="http://arxiv.org/find/eess/1/au:+Shafique_M/0/1/0/all/0/1">Muhammad Shafique</a></p>
<p>Accelerating compute intensive non-real-time beam-forming algorithms in
ultrasound imaging using deep learning architectures has been gaining momentum
in the recent past. Nonetheless, the complexity of the state-of-the-art deep
learning techniques poses challenges for deployment on resource-constrained
edge devices. In this work, we propose a novel vision transformer based tiny
beamformer (Tiny-VBF), which works on the raw radio-frequency channel data
acquired through single-angle plane wave insonification. The output of our
Tiny-VBF provides fast envelope detection requiring very low frame rate, i.e.
0.34 GOPs/Frame for a frame size of 368 x 128 in comparison to the
state-of-the-art deep learning models. It also exhibited an 8% increase in
contrast and gains of 5% and 33% in axial and lateral resolution respectively
when compared to Tiny-CNN on in-vitro dataset. Additionally, our model showed a
4.2% increase in contrast and gains of 4% and 20% in axial and lateral
resolution respectively when compared against conventional Delay-and-Sum (DAS)
beamformer. We further propose an accelerator architecture and implement our
Tiny-VBF model on a Zynq UltraScale+ MPSoC ZCU104 FPGA using a hybrid
quantization scheme with 50% less resource consumption compared to the
floating-point implementation, while preserving the image quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12086">Masked Autoencoders Are Robust Neural Architecture Search Learners. (arXiv:2311.12086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yiming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a></p>
<p>Neural Architecture Search (NAS) currently relies heavily on labeled data,
which is both expensive and time-consuming to acquire. In this paper, we
propose a novel NAS framework based on Masked Autoencoders (MAE) that
eliminates the need for labeled data during the search process. By replacing
the supervised learning objective with an image reconstruction task, our
approach enables the robust discovery of network architectures without
compromising performance and generalization ability. Additionally, we address
the problem of performance collapse encountered in the widely-used
Differentiable Architecture Search (DARTS) method in the unsupervised paradigm
by introducing a multi-scale decoder. Through extensive experiments conducted
on various search spaces and datasets, we demonstrate the effectiveness and
robustness of the proposed method, providing empirical evidence of its
superiority over baseline approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12089">Explaining Deep Learning Models for Age-related Gait Classification based on time series acceleration. (arXiv:2311.12089v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoping Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Otten_B/0/1/0/all/0/1">Bert Otten</a>, <a href="http://arxiv.org/find/cs/1/au:+Reneman_M/0/1/0/all/0/1">Michiel F Reneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamoth_C/0/1/0/all/0/1">Claudine JC Lamoth</a></p>
<p>Gait analysis holds significant importance in monitoring daily health,
particularly among older adults. Advancements in sensor technology enable the
capture of movement in real-life environments and generate big data. Machine
learning, notably deep learning (DL), shows promise to use these big data in
gait analysis. However, the inherent black-box nature of these models poses
challenges for their clinical application. This study aims to enhance
transparency in DL-based gait classification for aged-related gait patterns
using Explainable Artificial Intelligence, such as SHAP.
</p>
<p>A total of 244 subjects, comprising 129 adults and 115 older adults (age&gt;65),
were included. They performed a 3-minute walking task while accelerometers were
affixed to the lumbar segment L3. DL models, convolutional neural network (CNN)
and gated recurrent unit (GRU), were trained using 1-stride and 8-stride
accelerations, respectively, to classify adult and older adult groups. SHAP was
employed to explain the models' predictions.
</p>
<p>CNN achieved a satisfactory performance with an accuracy of 81.4% and an AUC
of 0.89, and GRU demonstrated promising results with an accuracy of 84.5% and
an AUC of 0.94. SHAP analysis revealed that both CNN and GRU assigned higher
SHAP values to the data from vertical and walking directions, particularly
emphasizing data around heel contact, spanning from the terminal swing to
loading response phases. Furthermore, SHAP values indicated that GRU did not
treat every stride equally.
</p>
<p>CNN accurately distinguished between adults and older adults based on the
characteristics of a single stride's data. GRU achieved accurate classification
by considering the relationships and subtle differences between strides. In
both models, data around heel contact emerged as most critical, suggesting
differences in acceleration and deceleration patterns during walking between
different age groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12130">Formal Verification of Long Short-Term Memory based Audio Classifiers: A Star based Approach. (arXiv:2311.12130v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pal_N/0/1/0/all/0/1">Neelanjana Pal</a> (Institute for Software Integrated Systems, Vanderbilt University,), <a href="http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1">Taylor T Johnson</a> (Institute for Software Integrated Systems, Vanderbilt University)</p>
<p>Formally verifying audio classification systems is essential to ensure
accurate signal classification across real-world applications like
surveillance, automotive voice commands, and multimedia content management,
preventing potential errors with serious consequences. Drawing from recent
research, this study advances the utilization of star-set-based formal
verification, extended through reachability analysis, tailored explicitly for
Long Short-Term Memory architectures and their Convolutional variations within
the audio classification domain. By conceptualizing the classification process
as a sequence of set operations, the star set-based reachability approach
streamlines the exploration of potential operational states attainable by the
system. The paper serves as an encompassing case study, validating and
verifying sequence audio classification analytics within real-world contexts.
It accentuates the necessity for robustness verification to ensure precise and
dependable predictions, particularly in light of the impact of noise on the
accuracy of output classifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12159">Conditional Modeling Based Automatic Video Summarization. (arXiv:2311.12159v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jia-Hong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao-Han Huck Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Min-Hung Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1">Marcel Worring</a></p>
<p>The aim of video summarization is to shorten videos automatically while
retaining the key information necessary to convey the overall story. Video
summarization methods mainly rely on visual factors, such as visual
consecutiveness and diversity, which may not be sufficient to fully understand
the content of the video. There are other non-visual factors, such as
interestingness, representativeness, and storyline consistency that should also
be considered for generating high-quality video summaries. Current methods do
not adequately take into account these non-visual factors, resulting in
suboptimal performance. In this work, a new approach to video summarization is
proposed based on insights gained from how humans create ground truth video
summaries. The method utilizes a conditional modeling perspective and
introduces multiple meaningful random variables and joint distributions to
characterize the key components of video summarization. Helper distributions
are employed to improve the training of the model. A conditional attention
module is designed to mitigate potential performance degradation in the
presence of multi-modal input. The proposed video summarization method
incorporates the above innovative design choices that aim to narrow the gap
between human-generated and machine-generated video summaries. Extensive
experiments show that the proposed approach outperforms existing methods and
achieves state-of-the-art performance on commonly used video summarization
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12163">Quantum Inception Score. (arXiv:2311.12163v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Sone_A/0/1/0/all/0/1">Akira Sone</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yamamoto_N/0/1/0/all/0/1">Naoki Yamamoto</a></p>
<p>Motivated by the great success of classical generative models in machine
learning, enthusiastic exploration of their quantum version has recently
started. To depart on this journey, it is important to develop a relevant
metric to evaluate the quality of quantum generative models; in the classical
case, one such examples is the inception score. In this paper, we propose the
quantum inception score, which relates the quality to the classical capacity of
the quantum channel that classifies a given dataset. We prove that, under this
proposed measure, the quantum generative models provide better quality than
their classical counterparts because of the presence of quantum coherence and
entanglement. Finally, we harness the quantum fluctuation theorem to
characterize the physical limitation of the quality of quantum generative
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12166">Creating Temporally Correlated High-Resolution Power Injection Profiles Using Physics-Aware GAN. (arXiv:2311.12166v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shah_H/0/1/0/all/0/1">Hritik Gopal Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Azimian_B/0/1/0/all/0/1">Behrouz Azimian</a>, <a href="http://arxiv.org/find/eess/1/au:+Pal_A/0/1/0/all/0/1">Anamitra Pal</a></p>
<p>Traditional smart meter measurements lack the granularity needed for
real-time decision-making. To address this practical problem, we create a
generative adversarial networks (GAN) model that enforces temporal consistency
on its high-resolution outputs via hard inequality constraints using a convex
optimization layer. A unique feature of our GAN model is that it is trained
solely on slow timescale aggregated power information obtained from historical
smart meter data. The results demonstrate that the model can successfully
create minutely interval temporally-correlated instantaneous power injection
profiles from 15-minute average power consumption information. This innovative
approach, emphasizing inter-neuron constraints, offers a promising avenue for
improved high-speed state estimation in distribution systems and enhances the
applicability of data-driven solutions for monitoring such systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12167">Node classification in random trees. (arXiv:2311.12167v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1">Wouter W. L. Nuijten</a>, <a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1">Vlado Menkovski</a></p>
<p>We propose a method for the classification of objects that are structured as
random trees. Our aim is to model a distribution over the node label
assignments in settings where the tree data structure is associated with node
attributes (typically high dimensional embeddings). The tree topology is not
predetermined and none of the label assignments are present during inference.
Other methods that produce a distribution over node label assignment in trees
(or more generally in graphs) either assume conditional independence of the
label assignment, operate on a fixed graph topology, or require part of the
node labels to be observed. Our method defines a Markov Network with the
corresponding topology of the random tree and an associated Gibbs distribution.
We parameterize the Gibbs distribution with a Graph Neural Network that
operates on the random tree and the node embeddings. This allows us to estimate
the likelihood of node assignments for a given random tree and use MCMC to
sample from the distribution of node assignments.
</p>
<p>We evaluate our method on the tasks of node classification in trees on the
Stanford Sentiment Treebank dataset. Our method outperforms the baselines on
this dataset, demonstrating its effectiveness for modeling joint distributions
of node labels in random trees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12198">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. (arXiv:2311.12198v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianyi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zeshun Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxin Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a></p>
<p>We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or
any other geometry embedding, highlighting the principle of "what you see is
what you simulate (WS$^2$)." Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12199">Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation. (arXiv:2311.12199v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chenyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yue Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marsic_I/0/1/0/all/0/1">Ivan Marsic</a></p>
<p>In supervised speech separation, permutation invariant training (PIT) is
widely used to handle label ambiguity by selecting the best permutation to
update the model. Despite its success, previous studies showed that PIT is
plagued by excessive label assignment switching in adjacent epochs, impeding
the model to learn better label assignments. To address this issue, we propose
a novel training strategy, dynamic sample dropout (DSD), which considers
previous best label assignments and evaluation metrics to exclude the samples
that may negatively impact the learned label assignments during training.
Additionally, we include layer-wise optimization (LO) to improve the
performance by solving layer-decoupling. Our experiments showed that combining
DSD and LO outperforms the baseline and solves excessive label assignment
switching and layer-decoupling issues. The proposed DSD and LO approach is easy
to implement, requires no extra training sets or steps, and shows generality to
various speech separation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12214">Random Fourier Signature Features. (arXiv:2311.12214v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Toth_C/0/1/0/all/0/1">Csaba Toth</a>, <a href="http://arxiv.org/find/stat/1/au:+Oberhauser_H/0/1/0/all/0/1">Harald Oberhauser</a>, <a href="http://arxiv.org/find/stat/1/au:+Szabo_Z/0/1/0/all/0/1">Zoltan Szabo</a></p>
<p>Tensor algebras give rise to one of the most powerful measures of similarity
for sequences of arbitrary length called the signature kernel accompanied with
attractive theoretical guarantees from stochastic analysis. Previous algorithms
to compute the signature kernel scale quadratically in terms of the length and
the number of the sequences. To mitigate this severe computational bottleneck,
we develop a random Fourier feature-based acceleration of the signature kernel
acting on the inherently non-Euclidean domain of sequences. We show uniform
approximation guarantees for the proposed unbiased estimator of the signature
kernel, while keeping its computation linear in the sequence length and number.
In addition, combined with recent advances on tensor projections, we derive two
even more scalable time series features with favourable concentration
properties and computational complexity both in time and memory. Our empirical
results show that the reduction in computational cost comes at a negligible
price in terms of accuracy on moderate-sized datasets, and it enables one to
scale to large datasets up to a million time series.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12224">Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators. (arXiv:2311.12224v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pogue_T/0/1/0/all/0/1">Trevor E. Pogue</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolici_N/0/1/0/all/0/1">Nicola Nicolici</a></p>
<p>We introduce a new algorithm called the Free-pipeline Fast Inner Product
(FFIP) and its hardware architecture that improve an under-explored fast
inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the
unrelated Winograd minimal filtering algorithms for convolutional layers, FIP
is applicable to all machine learning (ML) model layers that can mainly
decompose to matrix multiplication, including fully-connected, convolutional,
recurrent, and attention/transformer layers. We implement FIP for the first
time in an ML accelerator then present our FFIP algorithm and generalized
architecture which inherently improve FIP's clock frequency and, as a
consequence, throughput for a similar hardware cost. Finally, we contribute
ML-specific optimizations for the FIP and FFIP algorithms and architectures. We
show that FFIP can be seamlessly incorporated into traditional fixed-point
systolic array ML accelerators to achieve the same throughput with half the
number of multiply-accumulate (MAC) units, or it can double the maximum
systolic array size that can fit onto devices with a fixed hardware budget. Our
FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point
inputs achieves higher throughput and compute efficiency than the best-in-class
prior solutions on the same type of compute platform.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12230">Data-Guided Regulator for Adaptive Nonlinear Control. (arXiv:2311.12230v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rahimi_N/0/1/0/all/0/1">Niyousha Rahimi</a>, <a href="http://arxiv.org/find/eess/1/au:+Mesbahi_M/0/1/0/all/0/1">Mehran Mesbahi</a></p>
<p>This paper addresses the problem of designing a data-driven feedback
controller for complex nonlinear dynamical systems in the presence of
time-varying disturbances with unknown dynamics. Such disturbances are modeled
as the "unknown" part of the system dynamics. The goal is to achieve
finite-time regulation of system states through direct policy updates while
also generating informative data that can subsequently be used for data-driven
stabilization or system identification. First, we expand upon the notion of
"regularizability" and characterize this system characteristic for a linear
time-varying representation of the nonlinear system with locally-bounded
higher-order terms. "Rapid-regularizability" then gauges the extent by which a
system can be regulated in finite time, in contrast to its asymptotic behavior.
We then propose the Data-Guided Regulation for Adaptive Nonlinear Control (
DG-RAN) algorithm, an online iterative synthesis procedure that utilizes
discrete time-series data from a single trajectory for regulating system states
and identifying disturbance dynamics. The effectiveness of our approach is
demonstrated on a 6-DOF power descent guidance problem in the presence of
adverse environmental disturbances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12235">Improvements in Interlayer Pipelining of CNN Accelerators Using Genetic Algorithms. (arXiv:2311.12235v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Horeni_M/0/1/0/all/0/1">Mark Horeni</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Siddharth Joshi</a></p>
<p>Deploying Convolutional Neural Networks (CNNs) on edge platforms necessitates
efficient hardware acceleration. Any unnecessary data movement in such
accelerators can unacceptably degrade performance and efficiency. To address
this, we develop a layer fusion technique targeting CNNs, that reduces off-chip
data communication using a Genetic Algorithm (GA) applied to graph-based
topological sort. Results show a 1.8$\times$ increase in energy efficiency and
1.9$\times$ improvement in energy-delay product (EDP) for MobileNet-v3 on a
SIMBA-like mobile architecture. Our approach consistently improves workload
performance, averaging 1.4$\times$ improvement to EDP for SIMBA and
1.12$\times$ for Eyeriss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12244">Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning. (arXiv:2311.12244v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1">Tongzheng Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chenjun Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1">Dale Schuurmans</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1">Bo Dai</a></p>
<p>In real-world reinforcement learning problems, the state information is often
only partially observable, which breaks the basic assumption in Markov decision
processes, and thus, leads to inferior performances. Partially Observable
Markov Decision Processes have been introduced to explicitly take the issue
into account for learning, exploration, and planning, but presenting
significant computational and statistical challenges. To address these
difficulties, we exploit the representation view, which leads to a coherent
design framework for a practically tractable reinforcement learning algorithm
upon partial observations. We provide a theoretical analysis for justifying the
statistical efficiency of the proposed algorithm. We also empirically
demonstrate the proposed algorithm can surpass state-of-the-art performance
with partial observations across various benchmarks, therefore, pushing
reliable reinforcement learning towards more practical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12253">The limitation of neural nets for approximation and optimization. (arXiv:2311.12253v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giovannelli_T/0/1/0/all/0/1">Tommaso Giovannelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohab_O/0/1/0/all/0/1">Oumaima Sohab</a>, <a href="http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1">Luis Nunes Vicente</a></p>
<p>We are interested in assessing the use of neural networks as surrogate models
to approximate and minimize objective functions in optimization problems. While
neural networks are widely used for machine learning tasks such as
classification and regression, their application in solving optimization
problems has been limited. Our study begins by determining the best activation
function for approximating the objective functions of popular nonlinear
optimization test problems, and the evidence provided shows that~SiLU has the
best performance. We then analyze the accuracy of function value, gradient, and
Hessian approximations for such objective functions obtained through
interpolation/regression models and neural networks. When compared to
interpolation/regression models, neural networks can deliver competitive zero-
and first-order approximations (at a high training cost) but underperform on
second-order approximation. However, it is shown that combining a neural net
activation function with the natural basis for quadratic
interpolation/regression can waive the necessity of including cross terms in
the natural basis, leading to models with fewer parameters to determine.
Lastly, we provide evidence that the performance of a state-of-the-art
derivative-free optimization algorithm can hardly be improved when the gradient
of an objective function is approximated using any of the surrogate models
considered, including neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12255">Exploring Time Granularity on Temporal Graphs for Dynamic Link Prediction in Real-world Networks. (arXiv:2311.12255v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiangjian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yanyi Pu</a></p>
<p>Dynamic Graph Neural Networks (DGNNs) have emerged as the predominant
approach for processing dynamic graph-structured data. However, the influence
of temporal information on model performance and robustness remains
insufficiently explored, particularly regarding how models address prediction
tasks with different time granularities. In this paper, we explore the impact
of time granularity when training DGNNs on dynamic graphs through extensive
experiments. We examine graphs derived from various domains and compare three
different DGNNs to the baseline model across four varied time granularities. We
mainly consider the interplay between time granularities, model architectures,
and negative sampling strategies to obtain general conclusions. Our results
reveal that a sophisticated memory mechanism and proper time granularity are
crucial for a DGNN to deliver competitive and robust performance in the dynamic
link prediction task. We also discuss drawbacks in considered models and
datasets and propose promising directions for future research on the time
granularity of temporal graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12261">Beyond Simulated Drivers: Evaluating the Impact of Real-World Car-Following in Mixed Traffic Control. (arXiv:2311.12261v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1">Bibek Poudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weizi Li</a></p>
<p>Human-driven vehicles can amplify naturally occurring perturbations in
traffic, leading to congestion and consequently increased fuel consumption,
higher collision risks, and reduced capacity utilization. While previous
research has highlighted that a fraction of Robot Vehicles (RVs) can mitigate
these issues, they often rely on simulations with simplistic, model-based
Human-driven Vehicles (HVs) during car-following scenarios. Diverging from this
trend, in this study, we analyze real-world human driving trajectories,
extracting a wide range of acceleration behaviors during car-following. We then
incorporate these behaviors in simulation where RVs from prior studies are
employed to mitigate congestion, and evaluate their safety, efficiency, and
stability. Further, we also introduce a reinforcement learning based RV that
utilizes a congestion stage classifier neural network to optimize either
"safety+stability" or "efficiency" in the presence of the diverse human driving
behaviors. We evaluate the proposed RVs in two different mixed traffic control
environments at various densities, configurations, and penetration rates and
compare with the existing RVs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12264">Resilient Control of Networked Microgrids using Vertical Federated Reinforcement Learning: Designs and Real-Time Test-Bed Validations. (arXiv:2311.12264v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1">Sayak Mukherjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Hossain_R/0/1/0/all/0/1">Ramij R. Hossain</a>, <a href="http://arxiv.org/find/eess/1/au:+Mohiuddin_S/0/1/0/all/0/1">Sheik M. Mohiuddin</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Du_W/0/1/0/all/0/1">Wei Du</a>, <a href="http://arxiv.org/find/eess/1/au:+Adetola_V/0/1/0/all/0/1">Veronica Adetola</a>, <a href="http://arxiv.org/find/eess/1/au:+Jinsiwale_R/0/1/0/all/0/1">Rohit A. Jinsiwale</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1">Qiuhua Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yin_T/0/1/0/all/0/1">Tianzhixi Yin</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_A/0/1/0/all/0/1">Ankit Singhal</a></p>
<p>Improving system-level resiliency of networked microgrids is an important
aspect with increased population of inverter-based resources (IBRs). This paper
(1) presents resilient control design in presence of adversarial cyber-events,
and proposes a novel federated reinforcement learning (Fed-RL) approach to
tackle (a) model complexities, unknown dynamical behaviors of IBR devices, (b)
privacy issues regarding data sharing in multi-party-owned networked grids, and
(2) transfers learned controls from simulation to hardware-in-the-loop
test-bed, thereby bridging the gap between simulation and real world. With
these multi-prong objectives, first, we formulate a reinforcement learning (RL)
training setup generating episodic trajectories with adversaries (attack
signal) injected at the primary controllers of the grid forming (GFM) inverters
where RL agents (or controllers) are being trained to mitigate the injected
attacks. For networked microgrids, the horizontal Fed-RL method involving
distinct independent environments is not appropriate, leading us to develop
vertical variant Federated Soft Actor-Critic (FedSAC) algorithm to grasp the
interconnected dynamics of networked microgrid. Next, utilizing OpenAI Gym
interface, we built a custom simulation set-up in GridLAB-D/HELICS
co-simulation platform, named Resilient RL Co-simulation (ResRLCoSIM), to train
the RL agents with IEEE 123-bus benchmark test systems comprising 3
interconnected microgrids. Finally, the learned policies in simulation world
are transferred to the real-time hardware-in-the-loop test-bed set-up developed
using high-fidelity Hypersim platform. Experiments show that the
simulator-trained RL controllers produce convincing results with the real-time
test-bed set-up, validating the minimization of sim-to-real gap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12267">Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity. (arXiv:2311.12267v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jikai Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1">Vasilis Syrgkanis</a></p>
<p>This paper studies causal representation learning, the task of recovering
high-level latent variables and their causal relationships from low-level data
that we observe, assuming access to observations generated from multiple
environments. While existing works are able to prove full identifiability of
the underlying data generating process, they typically assume access to
single-node, hard interventions which is rather unrealistic in practice. The
main contribution of this paper is characterize a notion of identifiability
which is provably the best one can achieve when hard interventions are not
available. First, for linear causal models, we provide identifiability
guarantee for data observed from general environments without assuming any
similarities between them. While the causal graph is shown to be fully
recovered, the latent variables are only identified up to an effect-domination
ambiguity (EDA). We then propose an algorithm, LiNGCReL which is guaranteed to
recover the ground-truth model up to EDA, and we demonstrate its effectiveness
via numerical experiments. Moving on to general non-parametric causal models,
we prove the same idenfifiability guarantee assuming access to groups of soft
interventions. Finally, we provide counterparts of our identifiability results,
indicating that EDA is basically inevitable in our setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12279">Probabilistic Forecast Reconciliation with Kullback-Leibler Divergence Regularization. (arXiv:2311.12279v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guanyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yanfei Kang</a></p>
<p>As the popularity of hierarchical point forecast reconciliation methods
increases, there is a growing interest in probabilistic forecast
reconciliation. Many studies have utilized machine learning or deep learning
techniques to implement probabilistic forecasting reconciliation and have made
notable progress. However, these methods treat the reconciliation step as a
fixed and hard post-processing step, leading to a trade-off between accuracy
and coherency. In this paper, we propose a new approach for probabilistic
forecast reconciliation. Unlike existing approaches, our proposed approach
fuses the prediction step and reconciliation step into a deep learning
framework, making the reconciliation step more flexible and soft by introducing
the Kullback-Leibler divergence regularization term into the loss function. The
approach is evaluated using three hierarchical time series datasets, which
shows the advantages of our approach over other probabilistic forecast
reconciliation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12282">Orthogonally weighted $\ell_{2,1}$ regularization for rank-aware joint sparse recovery: algorithm and analysis. (arXiv:2311.12282v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Petrosyan_A/0/1/0/all/0/1">Armenak Petrosyan</a>, <a href="http://arxiv.org/find/math/1/au:+Pieper_K/0/1/0/all/0/1">Konstantin Pieper</a>, <a href="http://arxiv.org/find/math/1/au:+Tran_H/0/1/0/all/0/1">Hoang Tran</a></p>
<p>We propose and analyze an efficient algorithm for solving the joint sparse
recovery problem using a new regularization-based method, named orthogonally
weighted $\ell_{2,1}$ ($\mathit{ow}\ell_{2,1}$), which is specifically designed
to take into account the rank of the solution matrix. This method has
applications in feature extraction, matrix column selection, and dictionary
learning, and it is distinct from commonly used $\ell_{2,1}$ regularization and
other existing regularization-based approaches because it can exploit the full
rank of the row-sparse solution matrix, a key feature in many applications. We
provide a proof of the method's rank-awareness, establish the existence of
solutions to the proposed optimization problem, and develop an efficient
algorithm for solving it, whose convergence is analyzed. We also present
numerical experiments to illustrate the theory and demonstrate the
effectiveness of our method on real-life problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12290">A Supervised Contrastive Learning Pretrain-Finetune Approach for Time Series. (arXiv:2311.12290v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Trang H. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_K/0/1/0/all/0/1">Kyongmin Yeo</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nam Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaculin_R/0/1/0/all/0/1">Roman Vaculin</a></p>
<p>Foundation models have recently gained attention within the field of machine
learning thanks to its efficiency in broad data processing. While researchers
had attempted to extend this success to time series models, the main challenge
is effectively extracting representations and transferring knowledge from
pretraining datasets to the target finetuning dataset. To tackle this issue, we
introduce a novel pretraining procedure that leverages supervised contrastive
learning to distinguish features within each pretraining dataset. This
pretraining phase enables a probabilistic similarity metric, which assesses the
likelihood of a univariate sample being closely related to one of the
pretraining datasets. Subsequently, using this similarity metric as a guide, we
propose a fine-tuning procedure designed to enhance the accurate prediction of
the target data by aligning it more closely with the learned dynamics of the
pretraining datasets. Our experiments have shown promising results which
demonstrate the efficacy of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12292">Mapping &quot;Brain Coral&quot; Regions on Mars using Deep Learning. (arXiv:2311.12292v1 [astro-ph.EP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Pearson_K/0/1/0/all/0/1">Kyle A. Pearson</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Noe_E/0/1/0/all/0/1">Eldar Noe</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zhao_D/0/1/0/all/0/1">Daniel Zhao</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Altinok_A/0/1/0/all/0/1">Alphan Altinok</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Morgan_A/0/1/0/all/0/1">Alex Morgan</a></p>
<p>One of the main objectives of the Mars Exploration Program is to search for
evidence of past or current life on the planet. To achieve this, Mars
exploration has been focusing on regions that may have liquid or frozen water.
A set of critical areas may have seen cycles of ice thawing in the relatively
recent past in response to periodic changes in the obliquity of Mars. In this
work, we use convolutional neural networks to detect surface regions containing
"Brain Coral" terrain, a landform on Mars whose similarity in morphology and
scale to sorted stone circles on Earth suggests that it may have formed as a
consequence of freeze/thaw cycles. We use large images (~100-1000 megapixels)
from the Mars Reconnaissance Orbiter to search for these landforms at
resolutions close to a few tens of centimeters per pixel (~25--50 cm). Over
52,000 images (~28 TB) were searched (~5% of the Martian surface) where we
found detections in over 200 images. To expedite the processing we leverage a
classifier network (prior to segmentation) in the Fourier domain that can take
advantage of JPEG compression by leveraging blocks of coefficients from a
discrete cosine transform in lieu of decoding the entire image at the full
spatial resolution. The hybrid pipeline approach maintains ~93% accuracy while
cutting down on ~95% of the total processing time compared to running the
segmentation network at the full resolution on every image. The timely
processing of big data sets helps inform mission operations, geologic surveys
to prioritize candidate landing sites, avoid hazardous areas, or map the
spatial extent of certain terrain. The segmentation masks and source code are
available on Github for the community to explore and build upon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12303">Detecting subtle macroscopic changes in a finite temperature classical scalar field with machine learning. (arXiv:2311.12303v1 [cond-mat.stat-mech])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Yang_J/0/1/0/all/0/1">Jiming Yang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zheng_Y/0/1/0/all/0/1">Yutong Zheng</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhou_J/0/1/0/all/0/1">Jiahong Zhou</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_H/0/1/0/all/0/1">Huiyu Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yin_J/0/1/0/all/0/1">Jun Yin</a></p>
<p>The ability to detect macroscopic changes is important for probing the
behaviors of experimental many-body systems from the classical to the quantum
realm. Although abrupt changes near phase boundaries can easily be detected,
subtle macroscopic changes are much more difficult to detect as the changes can
be obscured by noise. In this study, as a toy model for detecting subtle
macroscopic changes in many-body systems, we try to differentiate scalar field
samples at varying temperatures. We compare different methods for making such
differentiations, from physics method, statistics method, to AI method. Our
finding suggests that the AI method outperforms both the statistical method and
the physics method in its sensitivity. Our result provides a proof-of-concept
that AI can potentially detect macroscopic changes in many-body systems that
elude physical measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12304">Discovering Effective Policies for Land-Use Planning. (arXiv:2311.12304v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1">Risto Miikkulainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Francon_O/0/1/0/all/0/1">Olivier Francon</a>, <a href="http://arxiv.org/find/cs/1/au:+Young_D/0/1/0/all/0/1">Daniel Young</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyerson_E/0/1/0/all/0/1">Elliot Meyerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hodjat_B/0/1/0/all/0/1">Babak Hodjat</a></p>
<p>How areas of land are allocated for different uses, such as forests, urban,
and agriculture, has a large effect on carbon balance, and therefore climate
change. Based on available historical data on changes in land use and a
simulation of carbon emissions/absorption, a surrogate model can be learned
that makes it possible to evaluate the different options available to
decision-makers efficiently. An evolutionary search process can then be used to
discover effective land-use policies for specific locations. Such a system was
built on the Project Resilience platform and evaluated with the Land-Use
Harmonization dataset and the BLUE simulator. It generates Pareto fronts that
trade off carbon impact and amount of change customized to different locations,
thus providing a potentially useful tool for land-use planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12309">Power grid operational risk assessment using graph neural network surrogates. (arXiv:2311.12309v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yadong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Karve_P/0/1/0/all/0/1">Pranav M Karve</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadevan_S/0/1/0/all/0/1">Sankaran Mahadevan</a></p>
<p>We investigate the utility of graph neural networks (GNNs) as proxies of
power grid operational decision-making algorithms (optimal power flow (OPF) and
security-constrained unit commitment (SCUC)) to enable rigorous quantification
of the operational risk. To conduct principled risk analysis, numerous Monte
Carlo (MC) samples are drawn from the (foretasted) probability distributions of
spatio-temporally correlated stochastic grid variables. The corresponding OPF
and SCUC solutions, which are needed to quantify the risk, are generated using
traditional OPF and SCUC solvers to generate data for training GNN model(s).
The GNN model performance is evaluated in terms of the accuracy of predicting
quantities of interests (QoIs) derived from the decision variables in OPF and
SCUC. Specifically, we focus on thermal power generation and load shedding at
system and individual zone level. We also perform reliability and risk
quantification based on GNN predictions and compare with that obtained from
OPF/SCUC solutions. Our results demonstrate that GNNs are capable of providing
fast and accurate prediction of QoIs and thus can be good surrogate models for
OPF and SCUC. The excellent accuracy of GNN-based reliability and risk
assessment further suggests that GNN surrogate has the potential to be applied
in real-time and hours-ahead risk quantification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12310">IEKM: A Model Incorporating External Keyword Matrices. (arXiv:2311.12310v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Cheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1">Mengliang Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yunbo Cao</a></p>
<p>A customer service platform system with a core text semantic similarity (STS)
task faces two urgent challenges: Firstly, one platform system needs to adapt
to different domains of customers, i.e., different domains adaptation (DDA).
Secondly, it is difficult for the model of the platform system to distinguish
sentence pairs that are literally close but semantically different, i.e., hard
negative samples. In this paper, we propose an incorporation external keywords
matrices model (IEKM) to address these challenges. The model uses external
tools or dictionaries to construct external matrices and fuses them to the
self-attention layers of the Transformer structure through gating units, thus
enabling flexible corrections to the model results. We evaluate the method on
multiple datasets and the results show that our method has improved performance
on all datasets. To demonstrate that our method can effectively solve all the
above challenges, we conduct a flexible correction experiment, which results in
an increase in the F1 value from 56.61 to 73.53. Our code will be publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12323">Modeling Political Orientation of Social Media Posts: An Extended Analysis. (arXiv:2311.12323v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamal_S/0/1/0/all/0/1">Sadia Kamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Little_B/0/1/0/all/0/1">Brenner Little</a>, <a href="http://arxiv.org/find/cs/1/au:+Gullic_J/0/1/0/all/0/1">Jade Gullic</a>, <a href="http://arxiv.org/find/cs/1/au:+Harms_T/0/1/0/all/0/1">Trevor Harms</a>, <a href="http://arxiv.org/find/cs/1/au:+Olofsson_K/0/1/0/all/0/1">Kristin Olofsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagavathi_A/0/1/0/all/0/1">Arunkumar Bagavathi</a></p>
<p>Developing machine learning models to characterize political polarization on
online social media presents significant challenges. These challenges mainly
stem from various factors such as the lack of annotated data, presence of noise
in social media datasets, and the sheer volume of data. The common research
practice typically examines the biased structure of online user communities for
a given topic or qualitatively measuring the impacts of polarized topics on
social media. However, there is limited work focusing on analyzing polarization
at the ground-level, specifically in the social media posts themselves. Such
existing analysis heavily relies on annotated data, which often requires
laborious human labeling, offers labels only to specific problems, and lacks
the ability to determine the near-future bias state of a social media
conversations. Understanding the degree of political orientation conveyed in
social media posts is crucial for quantifying the bias of online user
communities and investigating the spread of polarized content. In this work, we
first introduce two heuristic methods that leverage on news media bias and post
content to label social media posts. Next, we compare the efficacy and quality
of heuristically labeled dataset with a randomly sampled human-annotated
dataset. Additionally, we demonstrate that current machine learning models can
exhibit improved performance in predicting political orientation of social
media posts, employing both traditional supervised learning and few-shot
learning setups. We conduct experiments using the proposed heuristic methods
and machine learning approaches to predict the political orientation of posts
collected from two social media forums with diverse political ideologies: Gab
and Twitter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12329">Graph Neural Ordinary Differential Equations-based method for Collaborative Filtering. (arXiv:2311.12329v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Ke Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuanjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weizhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>Graph Convolution Networks (GCNs) are widely considered state-of-the-art for
collaborative filtering. Although several GCN-based methods have been proposed
and achieved state-of-the-art performance in various tasks, they can be
computationally expensive and time-consuming to train if too many layers are
created. However, since the linear GCN model can be interpreted as a
differential equation, it is possible to transfer it to an ODE problem. This
inspired us to address the computational limitations of GCN-based models by
designing a simple and efficient NODE-based model that can skip some GCN layers
to reach the final state, thus avoiding the need to create many layers. In this
work, we propose a Graph Neural Ordinary Differential Equation-based method for
Collaborative Filtering (GODE-CF). This method estimates the final embedding by
utilizing the information captured by one or two GCN layers. To validate our
approach, we conducted experiments on multiple datasets. The results
demonstrate that our model outperforms competitive baselines, including
GCN-based models and other state-of-the-art CF methods. Notably, our proposed
GODE-CF model has several advantages over traditional GCN-based models. It is
simple, efficient, and has a fast training time, making it a practical choice
for real-world situations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12345">Stable Diffusion For Aerial Object Detection. (arXiv:2311.12345v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1">Yanan Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fuxun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Simranjit Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamoulis_D/0/1/0/all/0/1">Dimitrios Stamoulis</a></p>
<p>Aerial object detection is a challenging task, in which one major obstacle
lies in the limitations of large-scale data collection and the long-tail
distribution of certain classes. Synthetic data offers a promising solution,
especially with recent advances in diffusion-based methods like stable
diffusion (SD). However, the direct application of diffusion methods to aerial
domains poses unique challenges: stable diffusion's optimization for rich
ground-level semantics doesn't align with the sparse nature of aerial objects,
and the extraction of post-synthesis object coordinates remains problematic. To
address these challenges, we introduce a synthetic data augmentation framework
tailored for aerial images. It encompasses sparse-to-dense region of interest
(ROI) extraction to bridge the semantic gap, fine-tuning the diffusion model
with low-rank adaptation (LORA) to circumvent exhaustive retraining, and
finally, a Copy-Paste method to compose synthesized objects with backgrounds,
providing a nuanced approach to aerial object detection through synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12351">Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey. (arXiv:2311.12351v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yunpeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zixu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1">Junyu Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zenan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Taolue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lijuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_Z/0/1/0/all/0/1">Zhou Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaoxing Ma</a></p>
<p>With the bomb ignited by ChatGPT, Transformer-based Large Language Models
(LLMs) have paved a revolutionary path toward Artificial General Intelligence
(AGI) and have been applied in diverse areas as knowledge bases, human
interfaces, and dynamic agents. However, a prevailing limitation exists: many
current LLMs, constrained by resources, are primarily pre-trained on shorter
texts, rendering them less effective for longer-context prompts, commonly
encountered in real-world settings. In this paper, we present a comprehensive
survey focusing on the advancement of model architecture in Transformer-based
LLMs to optimize long-context capabilities across all stages from pre-training
to inference. We firstly delineate and analyze the problems of handling
long-context input and output with the current Transformer-based models. Then,
we mainly offer a holistic taxonomy to navigate the landscape of Transformer
upgrades on architecture to solve these problems. Afterward, we provide the
investigation on wildly used evaluation necessities tailored for long-context
LLMs, including datasets, metrics, and baseline models, as well as some amazing
optimization toolkits like libraries, systems, and compilers to augment LLMs'
efficiency and efficacy across different stages. Finally, we further discuss
the predominant challenges and potential avenues for future research in this
domain. Additionally, we have established a repository where we curate relevant
literature with real-time updates at
https://github.com/Strivin0311/long-llms-learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12355">Utilizing Language Models for Tour Itinerary Recommendation. (arXiv:2311.12355v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1">Ngai Lam Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1">Kwan Hui Lim</a></p>
<p>Tour itinerary recommendation involves planning a sequence of relevant
Point-of-Interest (POIs), which combines challenges from the fields of both
Operations Research (OR) and Recommendation Systems (RS). As an OR problem,
there is the need to maximize a certain utility (e.g., popularity of POIs in
the tour) while adhering to some constraints (e.g., maximum time for the tour).
As a RS problem, it is heavily related to problem or filtering or ranking a
subset of POIs that are relevant to a user and recommending it as part of an
itinerary. In this paper, we explore the use of language models for the task of
tour itinerary recommendation and planning. This task has the unique
requirement of recommending personalized POIs relevant to users and planning
these POIs as an itinerary that satisfies various constraints. We discuss some
approaches in this area, such as using word embedding techniques like Word2Vec
and GloVe for learning POI embeddings and transformer-based techniques like
BERT for generating
</p>
<p>itineraries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12356">Random Linear Projections Loss for Hyperplane-Based Optimization in Regression Neural Networks. (arXiv:2311.12356v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatasubramanian_S/0/1/0/all/0/1">Shyam Venkatasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Aloui_A/0/1/0/all/0/1">Ahmed Aloui</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1">Vahid Tarokh</a></p>
<p>Despite their popularity across a wide range of domains, regression neural
networks are prone to overfitting complex datasets. In this work, we propose a
loss function termed Random Linear Projections (RLP) loss, which is empirically
shown to mitigate overfitting. With RLP loss, the distance between sets of
hyperplanes connecting fixed-size subsets of the neural network's
feature-prediction pairs and feature-label pairs is minimized. The intuition
behind this loss derives from the notion that if two functions share the same
hyperplanes connecting all subsets of feature-label pairs, then these functions
must necessarily be equivalent. Our empirical studies, conducted across
benchmark datasets and representative synthetic examples, demonstrate the
improvements of the proposed RLP loss over mean squared error (MSE).
Specifically, neural networks trained with the RLP loss achieve better
performance while requiring fewer data samples and are more robust to additive
noise. We provide theoretical analysis supporting our empirical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12358">Federated Learning via Consensus Mechanism on Heterogeneous Data: A New Perspective on Convergence. (arXiv:2311.12358v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Tiandi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Ming Gao</a></p>
<p>Federated learning (FL) on heterogeneous data (non-IID data) has recently
received great attention. Most existing methods focus on studying the
convergence guarantees for the global objective. While these methods can
guarantee the decrease of the global objective in each communication round,
they fail to ensure risk decrease for each client. In this paper, to address
the problem,we propose FedCOME, which introduces a consensus mechanism to
enforce decreased risk for each client after each training round. In
particular, we allow a slight adjustment to a client's gradient on the server
side, which generates an acute angle between the corrected gradient and the
original ones of other clients. We theoretically show that the consensus
mechanism can guarantee the convergence of the global objective. To generalize
the consensus mechanism to the partial participation FL scenario, we devise a
novel client sampling strategy to select the most representative clients for
the global data distribution. Training on these selected clients with the
consensus mechanism could empirically lead to risk decrease for clients that
are not selected. Finally, we conduct extensive experiments on four benchmark
datasets to show the superiority of FedCOME against other state-of-the-art
methods in terms of effectiveness, efficiency and fairness. For
reproducibility, we make our source code publicly available at:
\url{https://github.com/fedcome/fedcome}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12359">Post-Training Quantization with Low-precision Minifloats and Integers on FPGAs. (arXiv:2311.12359v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_S/0/1/0/all/0/1">Shivam Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1">Alessandro Pappalardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Damsgaard_H/0/1/0/all/0/1">Hans Jakob Damsgaard</a>, <a href="http://arxiv.org/find/cs/1/au:+Franco_G/0/1/0/all/0/1">Giuseppe Franco</a>, <a href="http://arxiv.org/find/cs/1/au:+Preusser_T/0/1/0/all/0/1">Thomas B. Preu&#xdf;er</a>, <a href="http://arxiv.org/find/cs/1/au:+Blott_M/0/1/0/all/0/1">Michaela Blott</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1">Tulika Mitra</a></p>
<p>Post-Training Quantization (PTQ) is a powerful technique for model
compression, reducing the precision of neural networks without additional
training overhead. Recent works have investigated adopting 8-bit floating-point
quantization (FP8) in the context of PTQ for model inference. However, the
exploration of floating-point formats smaller than 8 bits and their comparison
with integer quantization remains relatively limited. In this work, we present
minifloats, which are reduced-precision floating-point formats capable of
further reducing the memory footprint, latency, and energy cost of a model
while approaching full-precision model accuracy. Our work presents a novel PTQ
design-space exploration, comparing minifloat and integer quantization schemes
across a range of 3 to 8 bits for both weights and activations. We examine the
applicability of various PTQ techniques to minifloats, including weight
equalization, bias correction, SmoothQuant, gradient-based learned rounding,
and the GPTQ method. Our experiments validate the effectiveness of
low-precision minifloats when compared to their integer counterparts across a
spectrum of accuracy-precision trade-offs on a set of reference deep learning
vision workloads. Finally, we evaluate our results against an FPGA-based
hardware cost model, showing that integer quantization often remains the
Pareto-optimal option, given its relatively smaller hardware resource
footprint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12379">Infinite forecast combinations based on Dirichlet process. (arXiv:2311.12379v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yinuo Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yanfei Kang</a></p>
<p>Forecast combination integrates information from various sources by
consolidating multiple forecast results from the target time series. Instead of
the need to select a single optimal forecasting model, this paper introduces a
deep learning ensemble forecasting model based on the Dirichlet process.
Initially, the learning rate is sampled with three basis distributions as
hyperparameters to convert the infinite mixture into a finite one. All
checkpoints are collected to establish a deep learning sub-model pool, and
weight adjustment and diversity strategies are developed during the combination
process. The main advantage of this method is its ability to generate the
required base learners through a single training process, utilizing the
decaying strategy to tackle the challenge posed by the stochastic nature of
gradient descent in determining the optimal learning rate. To ensure the
method's generalizability and competitiveness, this paper conducts an empirical
analysis using the weekly dataset from the M4 competition and explores
sensitivity to the number of models to be combined. The results demonstrate
that the ensemble model proposed offers substantial improvements in prediction
accuracy and stability compared to a single benchmark model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12399">A Survey of Graph Meets Large Language Model: Progress and Future Directions. (arXiv:2311.12399v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peisong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangguo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jeffrey Xu Yu</a></p>
<p>Graph plays a significant role in representing and analyzing complex
relationships in real-world applications such as citation networks, social
networks, and biological data. Recently, Large Language Models (LLMs), which
have achieved tremendous success in various domains, have also been leveraged
in graph-related tasks to surpass traditional Graph Neural Networks (GNNs)
based methods and yield state-of-the-art performance. In this survey, we first
present a comprehensive review and analysis of existing methods that integrate
LLMs with graphs. First of all, we propose a new taxonomy, which organizes
existing methods into three categories based on the role (i.e., enhancer,
predictor, and alignment component) played by LLMs in graph-related tasks. Then
we systematically survey the representative methods along the three categories
of the taxonomy. Finally, we discuss the remaining limitations of existing
studies and highlight promising avenues for future research. The relevant
papers are summarized and will be consistently updated at:
https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12410">nach0: Multimodal Natural and Chemical Languages Foundation Model. (arXiv:2311.12410v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Livne_M/0/1/0/all/0/1">Micha Livne</a>, <a href="http://arxiv.org/find/cs/1/au:+Miftahutdinov_Z/0/1/0/all/0/1">Zulfat Miftahutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1">Elena Tutubalina</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1">Maksim Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Polykovskiy_D/0/1/0/all/0/1">Daniil Polykovskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Brundyn_A/0/1/0/all/0/1">Annika Brundyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Jhunjhunwala_A/0/1/0/all/0/1">Aastha Jhunjhunwala</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1">Anthony Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliper_A/0/1/0/all/0/1">Alex Aliper</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhavoronkov_A/0/1/0/all/0/1">Alex Zhavoronkov</a></p>
<p>Large Language Models (LLMs) have substantially driven scientific progress in
various domains, and many papers have demonstrated their ability to tackle
complex problems with creative solutions. Our paper introduces a new foundation
model, nach0, capable of solving various chemical and biological tasks:
biomedical question answering, named entity recognition, molecular generation,
molecular synthesis, attributes prediction, and others. nach0 is a multi-domain
and multi-task encoder-decoder LLM pre-trained on unlabeled text from
scientific literature, patents, and molecule strings to incorporate a range of
chemical and linguistic knowledge. We employed instruction tuning, where
specific task-related instructions are utilized to fine-tune nach0 for the
final set of tasks. To train nach0 effectively, we leverage the NeMo framework,
enabling efficient parallel optimization of both base and large model versions.
Extensive experiments demonstrate that our model outperforms state-of-the-art
baselines on single-domain and cross-domain tasks. Furthermore, it can generate
high-quality outputs in molecular and textual formats, showcasing its
effectiveness in multi-domain setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12419">Board-to-Board: Evaluating Moonboard Grade Prediction Generalization. (arXiv:2311.12419v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petashvili_D/0/1/0/all/0/1">Daniel Petashvili</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodda_M/0/1/0/all/0/1">Matthew Rodda</a></p>
<p>Bouldering is a sport where athletes aim to climb up an obstacle using a set
of defined holds called a route. Typically routes are assigned a grade to
inform climbers of its difficulty and allow them to more easily track their
progression. However, the variation in individual climbers technical and
physical attributes and many nuances of an individual route make grading a
difficult and often biased task. In this work, we apply classical and
deep-learning modelling techniques to the 2016, 2017 and 2019 Moonboard
datasets, achieving state of the art grade prediction performance with 0.87 MAE
and 1.12 RMSE. We achieve this performance on a feature-set that does not
require decomposing routes into individual moves, which is a method common in
literature and introduces bias. We also demonstrate the generalization
capability of this model between editions and introduce a novel vision-based
method of grade prediction. While the generalization performance of these
techniques is below human level performance currently, we propose these methods
as a basis for future work. Such a tool could be implemented in pre-existing
mobile applications and would allow climbers to better track their progress and
assess new routes with reduced bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12424">Looped Transformers are Better at Learning Learning Algorithms. (arXiv:2311.12424v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kangwook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert Nowak</a>, <a href="http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1">Dimitris Papailiopoulos</a></p>
<p>Transformers have demonstrated effectiveness in \emph{in-context solving}
data-fitting problems from various (latent) models, as reported by Garg et al.
However, the absence of an inherent iterative structure in the transformer
architecture presents a challenge in emulating the iterative algorithms, which
are commonly employed in traditional machine learning methods. To address this,
we propose the utilization of \emph{looped} transformer architecture and its
associated training methodology, with the aim of incorporating iterative
characteristics into the transformer architectures. Experimental results
suggest that the looped transformer achieves performance comparable to the
standard transformer in solving various data-fitting problems, while utilizing
less than 10\% of the parameter count.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12435">Fair Enough? A map of the current limitations of the requirements to have &quot;fair&#x27;&#x27; algorithms. (arXiv:2311.12435v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1">Alessandro Castelnovo</a>, <a href="http://arxiv.org/find/cs/1/au:+Inverardi_N/0/1/0/all/0/1">Nicole Inverardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanino_G/0/1/0/all/0/1">Gabriele Nanino</a>, <a href="http://arxiv.org/find/cs/1/au:+Penco_I/0/1/0/all/0/1">Ilaria Giuseppina Penco</a>, <a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1">Daniele Regoli</a></p>
<p>In the recent years, the raise in the usage and efficiency of Artificial
Intelligence and, more in general, of Automated Decision-Making systems has
brought with it an increasing and welcome awareness of the risks associated
with such systems. One of such risks is that of perpetuating or even amplifying
bias and unjust disparities present in the data from which many of these
systems learn to adjust and optimise their decisions. This awareness has on one
side encouraged several scientific communities to come up with more and more
appropriate ways and methods to assess, quantify, and possibly mitigate such
biases and disparities. On the other hand, it has prompted more and more layers
of society, including policy makers, to call for ``fair'' algorithms. We
believe that while a lot of excellent and multidisciplinary research is
currently being conducted, what is still fundamentally missing is the awareness
that having ``fair'' algorithms is per s\'e a nearly meaningless requirement,
that needs to be complemented with a lot of additional societal choices to
become actionable. Namely, there is a hiatus between what the society is
demanding from Automated Decision-Making systems, and what this demand actually
means in real-world scenarios. In this work, we outline the key features of
such a hiatus, and pinpoint a list of fundamental ambiguities and attention
points that we as a society must address in order to give a concrete meaning to
the increasing demand of fairness in Automated Decision-Making systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12436">Classifier Calibration with ROC-Regularized Isotonic Regression. (arXiv:2311.12436v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berta_E/0/1/0/all/0/1">Eugene Berta</a> (SIERRA), <a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1">Francis Bach</a> (SIERRA), <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael Jordan</a> (SIERRA)</p>
<p>Calibration of machine learning classifiers is necessary to obtain reliable
and interpretable predictions, bridging the gap between model confidence and
actual probabilities. One prominent technique, isotonic regression (IR), aims
at calibrating binary classifiers by minimizing the cross entropy on a
calibration set via monotone transformations. IR acts as an adaptive binning
procedure, which allows achieving a calibration error of zero, but leaves open
the issue of the effect on performance. In this paper, we first prove that IR
preserves the convex hull of the ROC curve -- an essential performance metric
for binary classifiers. This ensures that a classifier is calibrated while
controlling for overfitting of the calibration set. We then present a novel
generalization of isotonic regression to accommodate classifiers with K
classes. Our method constructs a multidimensional adaptive binning scheme on
the probability simplex, again achieving a multi-class calibration error equal
to zero. We regularize this algorithm by imposing a form of monotony that
preserves the K-dimensional ROC surface of the classifier. We show empirically
that this general monotony criterion is effective in striking a balance between
reducing cross entropy loss and avoiding overfitting of the calibration set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12439">Harnessing FPGA Technology for Enhanced Biomedical Computation. (arXiv:2311.12439v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alici_N/0/1/0/all/0/1">Nisanur Alici</a>, <a href="http://arxiv.org/find/cs/1/au:+Inadagbo_K/0/1/0/all/0/1">Kayode Inadagbo</a>, <a href="http://arxiv.org/find/cs/1/au:+Isik_M/0/1/0/all/0/1">Murat Isik</a></p>
<p>This research delves into sophisticated neural network frameworks like
Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long
Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for
improved analysis of ECG signals via Field Programmable Gate Arrays (FPGAs).
The MIT-BIH Arrhythmia Database serves as the foundation for training and
evaluating our models, with added Gaussian noise to heighten the algorithms'
resilience. The developed architectures incorporate various layers for specific
processing and categorization functions, employing strategies such as the
EarlyStopping callback and Dropout layer to prevent overfitting. Additionally,
this paper details the creation of a tailored Tensor Compute Unit (TCU)
accelerator for the PYNQ Z1 platform. It provides a thorough methodology for
implementing FPGA-based machine learning, encompassing the configuration of the
Tensil toolchain in Docker, selection of architectures, PS-PL configuration,
and the compilation and deployment of models. By evaluating performance
indicators like latency and throughput, we showcase the efficacy of FPGAs in
advanced biomedical computing. This study ultimately serves as a comprehensive
guide to optimizing neural network operations on FPGAs across various fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12476">MaskFlow: Object-Aware Motion Estimation. (arXiv:2311.12476v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1">Aria Ahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Walton_D/0/1/0/all/0/1">David R. Walton</a>, <a href="http://arxiv.org/find/cs/1/au:+Atherton_T/0/1/0/all/0/1">Tim Atherton</a>, <a href="http://arxiv.org/find/cs/1/au:+Dikici_C/0/1/0/all/0/1">Cagatay Dikici</a></p>
<p>We introduce a novel motion estimation method, MaskFlow, that is capable of
estimating accurate motion fields, even in very challenging cases with small
objects, large displacements and drastic appearance changes. In addition to
lower-level features, that are used in other Deep Neural Network (DNN)-based
motion estimation methods, MaskFlow draws from object-level features and
segmentations. These features and segmentations are used to approximate the
objects' translation motion field. We propose a novel and effective way of
incorporating the incomplete translation motion field into a subsequent motion
estimation network for refinement and completion. We also produced a new
challenging synthetic dataset with motion field ground truth, and also provide
extra ground truth for the object-instance matchings and corresponding
segmentation masks. We demonstrate that MaskFlow outperforms state of the art
methods when evaluated on our new challenging dataset, whilst still producing
comparable results on the popular FlyingThings3D benchmark dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12490">Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields. (arXiv:2311.12490v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yi Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yuan Zeng</a></p>
<p>Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12491">Heuristics for Detecting CoinJoin Transactions on the Bitcoin Blockchain. (arXiv:2311.12491v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schnoering_H/0/1/0/all/0/1">Hugo Schnoering</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a></p>
<p>This research delves into the intricacies of Bitcoin, a decentralized
peer-to-peer network, and its associated blockchain, which records all
transactions since its inception. While this ensures integrity and
transparency, the transparent nature of Bitcoin potentially compromises users'
privacy rights. To address this concern, users have adopted CoinJoin, a method
that amalgamates multiple transaction intents into a single, larger transaction
to bolster transactional privacy. This process complicates individual
transaction tracing and disrupts many established blockchain analysis
heuristics. Despite its significance, limited research has been conducted on
identifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin
implementations such as JoinMarket, Wasabi, and Whirlpool, each presenting
distinct challenges due to their unique transaction structures. This study
delves deeply into the open-source implementations of these protocols, aiming
to develop refined heuristics for identifying their transactions on the
blockchain. Our exhaustive analysis covers transactions up to block 760,000,
offering a comprehensive insight into CoinJoin transactions and their
implications for Bitcoin blockchain analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12495">Multi-Objective Reinforcement Learning based on Decomposition: A taxonomy and framework. (arXiv:2311.12495v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Felten_F/0/1/0/all/0/1">Florian Felten</a>, <a href="http://arxiv.org/find/cs/1/au:+Talbi_E/0/1/0/all/0/1">El-Ghazali Talbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Danoy_G/0/1/0/all/0/1">Gr&#xe9;goire Danoy</a></p>
<p>Multi-objective reinforcement learning (MORL) extends traditional RL by
seeking policies making different compromises among conflicting objectives. The
recent surge of interest in MORL has led to diverse studies and solving
methods, often drawing from existing knowledge in multi-objective optimization
based on decomposition (MOO/D). Yet, a clear categorization based on both RL
and MOO/D is lacking in the existing literature. Consequently, MORL researchers
face difficulties when trying to classify contributions within a broader
context due to the absence of a standardized taxonomy. To tackle such an issue,
this paper introduces Multi-Objective Reinforcement Learning based on
Decomposition (MORL/D), a novel methodology bridging RL and MOO literature. A
comprehensive taxonomy for MORL/D is presented, providing a structured
foundation for categorizing existing and potential MORL works. The introduced
taxonomy is then used to scrutinize MORL research, enhancing clarity and
conciseness through well-defined categorization. Moreover, a flexible framework
derived from the taxonomy is introduced. This framework accommodates diverse
instantiations using tools from both RL and MOO/D. Implementation across
various configurations demonstrates its versatility, assessed against benchmark
problems. Results indicate MORL/D instantiations achieve comparable performance
with significantly greater versatility than current state-of-the-art
approaches. By presenting the taxonomy and framework, this paper offers a
comprehensive perspective and a unified vocabulary for MORL. This not only
facilitates the identification of algorithmic contributions but also lays the
groundwork for novel research avenues in MORL, contributing to the continued
advancement of this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12501">Fair Polylog-Approximate Low-Cost Hierarchical Clustering. (arXiv:2311.12501v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Knittel_M/0/1/0/all/0/1">Marina Knittel</a>, <a href="http://arxiv.org/find/cs/1/au:+Springer_M/0/1/0/all/0/1">Max Springer</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a></p>
<p>Research in fair machine learning, and particularly clustering, has been
crucial in recent years given the many ethical controversies that modern
intelligent systems have posed. Ahmadian et al. [2020] established the study of
fairness in \textit{hierarchical} clustering, a stronger, more structured
variant of its well-known flat counterpart, though their proposed algorithm
that optimizes for Dasgupta's [2016] famous cost function was highly
theoretical. Knittel et al. [2023] then proposed the first practical fair
approximation for cost, however they were unable to break the
polynomial-approximate barrier they posed as a hurdle of interest. We break
this barrier, proposing the first truly polylogarithmic-approximate low-cost
fair hierarchical clustering, thus greatly bridging the gap between the best
fair and vanilla hierarchical clustering approximations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12524">ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models. (arXiv:2311.12524v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiankai Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kegang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hongming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiyuxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peiyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuntao Wang</a></p>
<p>This study concentrates on evaluating the efficacy of Large Language Models
(LLMs) in healthcare, with a specific focus on their application in personal
anomalous health monitoring. Our research primarily investigates the
capabilities of LLMs in interpreting and analyzing physiological data obtained
from FDA-approved devices. We conducted an extensive analysis using anomalous
physiological data gathered in a simulated low-air-pressure plateau
environment. This allowed us to assess the precision and reliability of LLMs in
understanding and evaluating users' health status with notable specificity. Our
findings reveal that LLMs exhibit exceptional performance in determining
medical indicators, including a Mean Absolute Error (MAE) of less than 1 beat
per minute for heart rate and less than 1% for oxygen saturation (SpO2).
Furthermore, the Mean Absolute Percentage Error (MAPE) for these evaluations
remained below 1%, with the overall accuracy of health assessments surpassing
85%. In image analysis tasks, such as interpreting photoplethysmography (PPG)
data, our specially adapted GPT models demonstrated remarkable proficiency,
achieving less than 1 bpm error in cycle count and 7.28 MAE for heart rate
estimation. This study highlights LLMs' dual role as health data analysis tools
and pivotal elements in advanced AI health assistants, offering personalized
health insights and recommendations within the future health assistant
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12526">Neural Network Pruning by Gradient Descent. (arXiv:2311.12526v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ruyi Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiang Zhang</a></p>
<p>The rapid increase in the parameters of deep learning models has led to
significant costs, challenging computational efficiency and model
interpretability. In this paper, we introduce a novel and straightforward
neural network pruning framework that incorporates the Gumbel-Softmax
technique. This framework enables the simultaneous optimization of a network's
weights and topology in an end-to-end process using stochastic gradient
descent. Empirical results demonstrate its exceptional compression capability,
maintaining high accuracy on the MNIST dataset with only 0.15\% of the original
network parameters. Moreover, our framework enhances neural network
interpretability, not only by allowing easy extraction of feature importance
directly from the pruned network but also by enabling visualization of feature
symmetry and the pathways of information propagation from features to outcomes.
Although the pruning strategy is learned through deep learning, it is
surprisingly intuitive and understandable, focusing on selecting key
representative features and exploiting data patterns to achieve extreme sparse
pruning. We believe our method opens a promising new avenue for deep learning
pruning and the creation of interpretable machine learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12528">Inverse Problems with Learned Forward Operators. (arXiv:2311.12528v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Arridge_S/0/1/0/all/0/1">Simon Arridge</a>, <a href="http://arxiv.org/find/math/1/au:+Hauptmann_A/0/1/0/all/0/1">Andreas Hauptmann</a>, <a href="http://arxiv.org/find/math/1/au:+Korolev_Y/0/1/0/all/0/1">Yury Korolev</a></p>
<p>Solving inverse problems requires knowledge of the forward operator, but
accurate models can be computationally expensive and hence cheaper variants are
desired that do not compromise reconstruction quality. This chapter reviews
reconstruction methods in inverse problems with learned forward operators that
follow two different paradigms. The first one is completely agnostic to the
forward operator and learns its restriction to the subspace spanned by the
training data. The framework of regularisation by projection is then used to
find a reconstruction. The second one uses a simplified model of the physics of
the measurement process and only relies on the training data to learn a model
correction. We present the theory of these two approaches and compare them
numerically. A common theme emerges: both methods require, or at least benefit
from, training data not only for the forward operator, but also for its
adjoint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12530">An efficient likelihood-free Bayesian inference method based on sequential neural posterior estimation. (arXiv:2311.12530v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xiong_Y/0/1/0/all/0/1">Yifei Xiong</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1">Xiliang Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1">Sanguo Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1">Zhijian He</a></p>
<p>Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
Unlike approximate Bayesian computation, SNPE techniques learn the posterior
from sequential simulation using neural network-based conditional density
estimators. This paper reclaims SNPE-B proposed by Lueckmann et al. (2017),
which suffers from inefficiency and slow inference due to inefficient
utilization of simulated data and high variance of parameter updates. To
address these issues, we firstly introduce a concentrated loss function based
on an adaptive calibration kernel that reweights the simulated data
appropriately to improve the data efficiency. Moreover, we provide a
theoretical analysis of the variance of associated Monte Carlo estimators.
Based on this analysis, we then propose several variance reduction techniques
to further accelerate the process of learning. Numerical experiments
demonstrate that our method outperforms the original method together with other
existing competitors on certain tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12538">In-Context Learning Functions with Varying Number of Minima. (arXiv:2311.12538v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1">David Oniani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanshan Wang</a></p>
<p>Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12550">Explainable Anomaly Detection using Masked Latent Generative Modeling. (arXiv:2311.12550v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Daesoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Malacarne_S/0/1/0/all/0/1">Sara Malacarne</a>, <a href="http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1">Erlend Aune</a></p>
<p>We present a novel time series anomaly detection method that achieves
excellent detection accuracy while offering a superior level of explainability.
Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted
from the cutting-edge time series generation method known as TimeVQVAE. The
prior model is trained on the discrete latent space of a time-frequency domain.
Notably, the dimensional semantics of the time-frequency domain are preserved
in the latent space, enabling us to compute anomaly scores across different
frequency bands, which provides a better insight into the detected anomalies.
Additionally, the generative nature of the prior model allows for sampling
likely normal states for detected anomalies, enhancing the explainability of
the detected anomalies through counterfactuals. Our experimental evaluation on
the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD
significantly surpasses the existing methods in terms of detection accuracy and
explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12561">Convolutional Neural Networks for Neuroimaging in Parkinson&#x27;s Disease: Is Preprocessing Needed?. (arXiv:2311.12561v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_Murcia_F/0/1/0/all/0/1">Francisco J. Martinez-Murcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1">Juan M. G&#xf3;rriz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1">Javier Ram&#xed;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1">Andr&#xe9;s Ortiz</a></p>
<p>Spatial and intensity normalization are nowadays a prerequisite for
neuroimaging analysis. Influenced by voxel-wise and other univariate
comparisons, where these corrections are key, they are commonly applied to any
type of analysis and imaging modalities. Nuclear imaging modalities such as
PET-FDG or FP-CIT SPECT, a common modality used in Parkinson's Disease
diagnosis, are especially dependent on intensity normalization. However, these
steps are computationally expensive and furthermore, they may introduce
deformations in the images, altering the information contained in them.
Convolutional Neural Networks (CNNs), for their part, introduce position
invariance to pattern recognition, and have been proven to classify objects
regardless of their orientation, size, angle, etc. Therefore, a question
arises: how well can CNNs account for spatial and intensity differences when
analysing nuclear brain imaging? Are spatial and intensity normalization still
needed? To answer this question, we have trained four different CNN models
based on well-established architectures, using or not different spatial and
intensity normalization preprocessing. The results show that a sufficiently
complex model such as our three-dimensional version of the ALEXNET can
effectively account for spatial differences, achieving a diagnosis accuracy of
94.1% with an area under the ROC curve of 0.984. The visualization of the
differences via saliency maps shows that these models are correctly finding
patterns that match those found in the literature, without the need of applying
any complex spatial normalization procedure. However, the intensity
normalization -- and its type -- is revealed as very influential in the results
and accuracy of the trained model, and therefore must be well accounted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12564">Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and LAnguage in Conversational Environments. (arXiv:2311.12564v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Baghel_S/0/1/0/all/0/1">Shikha Baghel</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramoji_S/0/1/0/all/0/1">Shreyas Ramoji</a>, <a href="http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1">Somil Jain</a>, <a href="http://arxiv.org/find/eess/1/au:+Chowdhuri_P/0/1/0/all/0/1">Pratik Roy Chowdhuri</a>, <a href="http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1">Prachi Singh</a>, <a href="http://arxiv.org/find/eess/1/au:+Vijayasenan_D/0/1/0/all/0/1">Deepu Vijayasenan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a></p>
<p>In multi-lingual societies, where multiple languages are spoken in a small
geographic vicinity, informal conversations often involve mix of languages.
Existing speech technologies may be inefficient in extracting information from
such conversations, where the speech data is rich in diversity with multiple
languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in
Conversational Environments) challenge constitutes an open-call for evaluating
and bench-marking the speaker and language diarization technologies on this
challenging condition. The challenge entailed two tracks: Track-1 focused on
speaker diarization (SD) in multilingual situations while, Track-2 addressed
the language diarization (LD) in a multi-speaker scenario. Both the tracks were
evaluated using the same underlying audio data. To facilitate this evaluation,
a real-world dataset featuring multilingual, multi-speaker conversational
far-field speech was recorded and distributed. Furthermore, a baseline system
was made available for both SD and LD task which mimicked the state-of-art in
these tasks. The challenge garnered a total of $42$ world-wide registrations
and received a total of $19$ combined submissions for Track-1 and Track-2. This
paper describes the challenge, details of the datasets, tasks, and the baseline
system. Additionally, the paper provides a concise overview of the submitted
systems in both tracks, with an emphasis given to the top performing systems.
The paper also presents insights and future perspectives for SD and LD tasks,
focusing on the key challenges that the systems need to overcome before
wide-spread commercial deployment on such conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12566">Variational Elliptical Processes. (arXiv:2311.12566v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baankestad_M/0/1/0/all/0/1">Maria B&#xe5;nkestad</a>, <a href="http://arxiv.org/find/cs/1/au:+Sjolund_J/0/1/0/all/0/1">Jens Sj&#xf6;lund</a>, <a href="http://arxiv.org/find/cs/1/au:+Taghia_J/0/1/0/all/0/1">Jalil Taghia</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoon_T/0/1/0/all/0/1">Thomas B. Sch&#xf6;on</a></p>
<p>We present elliptical processes, a family of non-parametric probabilistic
models that subsume Gaussian processes and Student's t processes. This
generalization includes a range of new heavy-tailed behaviors while retaining
computational tractability. Elliptical processes are based on a representation
of elliptical distributions as a continuous mixture of Gaussian distributions.
We parameterize this mixture distribution as a spline normalizing flow, which
we train using variational inference. The proposed form of the variational
posterior enables a sparse variational elliptical process applicable to
large-scale problems. We highlight advantages compared to Gaussian processes
through regression and classification experiments. Elliptical processes can
supersede Gaussian processes in several settings, including cases where the
likelihood is non-Gaussian or when accurate tail modeling is essential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12569">Differentiable Sampling of Categorical Distributions Using the CatLog-Derivative Trick. (arXiv:2311.12569v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smet_L/0/1/0/all/0/1">Lennert De Smet</a>, <a href="http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1">Emanuele Sansone</a>, <a href="http://arxiv.org/find/cs/1/au:+Martires_P/0/1/0/all/0/1">Pedro Zuidberg Dos Martires</a></p>
<p>Categorical random variables can faithfully represent the discrete and
uncertain aspects of data as part of a discrete latent variable model. Learning
in such models necessitates taking gradients with respect to the parameters of
the categorical probability distributions, which is often intractable due to
their combinatorial nature. A popular technique to estimate these otherwise
intractable gradients is the Log-Derivative trick. This trick forms the basis
of the well-known REINFORCE gradient estimator and its many extensions. While
the Log-Derivative trick allows us to differentiate through samples drawn from
categorical distributions, it does not take into account the discrete nature of
the distribution itself. Our first contribution addresses this shortcoming by
introducing the CatLog-Derivative trick - a variation of the Log-Derivative
trick tailored towards categorical distributions. Secondly, we use the
CatLog-Derivative trick to introduce IndeCateR, a novel and unbiased gradient
estimator for the important case of products of independent categorical
distributions with provably lower variance than REINFORCE. Thirdly, we
empirically show that IndeCateR can be efficiently implemented and that its
gradient estimates have significantly lower bias and variance for the same
number of samples compared to the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12570">BEND: Benchmarking DNA Language Models on biologically meaningful tasks. (arXiv:2311.12570v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Marin_F/0/1/0/all/0/1">Frederikke Isa Marin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Teufel_F/0/1/0/all/0/1">Felix Teufel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Horrender_M/0/1/0/all/0/1">Marc Horrender</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Madsen_D/0/1/0/all/0/1">Dennis Madsen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pultz_D/0/1/0/all/0/1">Dennis Pultz</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Boomsma_W/0/1/0/all/0/1">Wouter Boomsma</a></p>
<p>The genome sequence contains the blueprint for governing cellular processes.
While the availability of genomes has vastly increased over the last decades,
experimental annotation of the various functional, non-coding and regulatory
elements encoded in the DNA sequence remains both expensive and challenging.
This has sparked interest in unsupervised language modeling of genomic DNA, a
paradigm that has seen great success for protein sequence data. Although
various DNA language models have been proposed, evaluation tasks often differ
between individual works, and might not fully recapitulate the fundamental
challenges of genome annotation, including the length, scale and sparsity of
the data. In this study, we introduce BEND, a Benchmark for DNA language
models, featuring a collection of realistic and biologically meaningful
downstream tasks defined on the human genome. We find that embeddings from
current DNA LMs can approach performance of expert methods on some tasks, but
only capture limited information about long-range features. BEND is available
at https://github.com/frederikkemarin/BEND.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12573">Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries. (arXiv:2311.12573v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorwa_R/0/1/0/all/0/1">Robert Gorwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Veale_M/0/1/0/all/0/1">Michael Veale</a></p>
<p>The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12579">Machine-Guided Discovery of a Real-World Rogue Wave Model. (arXiv:2311.12579v1 [physics.geo-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Hafner_D/0/1/0/all/0/1">Dion H&#xe4;fner</a>, <a href="http://arxiv.org/find/physics/1/au:+Gemmrich_J/0/1/0/all/0/1">Johannes Gemmrich</a>, <a href="http://arxiv.org/find/physics/1/au:+Jochum_M/0/1/0/all/0/1">Markus Jochum</a></p>
<p>Big data and large-scale machine learning have had a profound impact on
science and engineering, particularly in fields focused on forecasting and
prediction. Yet, it is still not clear how we can use the superior pattern
matching abilities of machine learning models for scientific discovery. This is
because the goals of machine learning and science are generally not aligned. In
addition to being accurate, scientific theories must also be causally
consistent with the underlying physical process and allow for human analysis,
reasoning, and manipulation to advance the field. In this paper, we present a
case study on discovering a new symbolic model for oceanic rogue waves from
data using causal analysis, deep learning, parsimony-guided model selection,
and symbolic regression. We train an artificial neural network on causal
features from an extensive dataset of observations from wave buoys, while
selecting for predictive performance and causal invariance. We apply symbolic
regression to distill this black-box model into a mathematical equation that
retains the neural network's predictive capabilities, while allowing for
interpretation in the context of existing wave theory. The resulting model
reproduces known behavior, generates well-calibrated probabilities, and
achieves better predictive scores on unseen data than current theory. This
showcases how machine learning can facilitate inductive scientific discovery,
and paves the way for more accurate rogue wave forecasting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12589">Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images. (arXiv:2311.12589v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sawhney_G/0/1/0/all/0/1">Gauransh Sawhney</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1">Daksh Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1">Adeel Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiechao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleem_K/0/1/0/all/0/1">Khalid Saleem</a></p>
<p>Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12590">ChronoPscychosis: Temporal Segmentation and Its Impact on Schizophrenia Classification Using Motor Activity Data. (arXiv:2311.12590v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jadhav_P/0/1/0/all/0/1">Pradnya Rajendra Jadhav</a>, <a href="http://arxiv.org/find/cs/1/au:+Aduri_R/0/1/0/all/0/1">Raviprasad Aduri</a></p>
<p>Schizophrenia is a complicated mental illness characterized by a broad
spectrum of symptoms affecting cognition, behavior, and emotion. The task of
identifying reliable biomarkers to classify Schizophrenia accurately continues
to be a challenge in the field of psychiatry. We investigate the temporal
patterns within the motor activity data as a potential key to enhancing the
categorization of individuals with Schizophrenia, using the dataset having
motor activity recordings of 22 Schizophrenia patients and 32 control subjects.
The dataset contains per-minute motor activity measurements collected for an
average of 12.7 days in a row for each participant. We dissect each day into
segments (Twelve, Eight, six, four, three, and two parts) and evaluate their
impact on classification. We employ sixteen statistical features within these
temporal segments and train them on Seven machine learning models to get deeper
insights. LightGBM model outperforms the other six models. Our results indicate
that the temporal segmentation significantly improves the classification, with
AUC-ROC = 0.93, F1 score = 0.84( LightGBM- without any segmentation) and
AUC-ROC = 0.98, F1 score = 0.93( LightGBM- with segmentation). Distinguishing
between diurnal and nocturnal segments amplifies the differences between
Schizophrenia patients and controls. However, further subdivisions into smaller
time segments do not affect the AUC- ROC significantly. Morning, afternoon,
evening, and night partitioning gives similar classification performance to
day-night partitioning. These findings are valuable as they indicate that
extensive temporal classification beyond distinguishing between day and night
does not yield substantial results, offering an efficient approach for further
classification, early diagnosis, and monitoring of Schizophrenia.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12601">Deep learning-based detection of morphological features associated with hypoxia in H&amp;E breast cancer whole slide images. (arXiv:2311.12601v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manescu_P/0/1/0/all/0/1">Petru Manescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Geradts_J/0/1/0/all/0/1">Joseph Geradts</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Reyes_D/0/1/0/all/0/1">Delmiro Fernandez-Reyes</a></p>
<p>Hypoxia occurs when tumour cells outgrow their blood supply, leading to
regions of low oxygen levels within the tumour. Calculating hypoxia levels can
be an important step in understanding the biology of tumours, their clinical
progression and response to treatment. This study demonstrates a novel
application of deep learning to evaluate hypoxia in the context of breast
cancer histomorphology. More precisely, we show that Weakly Supervised Deep
Learning (WSDL) models can accurately detect hypoxia associated features in
routine Hematoxylin and Eosin (H&amp;E) whole slide images (WSI). We trained and
evaluated a deep Multiple Instance Learning model on tiles from WSI H&amp;E tissue
from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on
a left-out test set. We also showed significant differences between features of
hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such
DL hypoxia H&amp;E WSI detection models could potentially be extended to other
tumour types and easily integrated into the pathology workflow without
requiring additional costly assays.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12602">TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using Vision-Based Tactile Sensing. (arXiv:2311.12602v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Comi_M/0/1/0/all/0/1">Mauro Comi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yijiong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Church_A/0/1/0/all/0/1">Alex Church</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1">Alessio Tonioni</a>, <a href="http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1">Laurence Aitchison</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepora_N/0/1/0/all/0/1">Nathan F. Lepora</a></p>
<p>Humans rely on their visual and tactile senses to develop a comprehensive 3D
understanding of their physical environment. Recently, there has been a growing
interest in exploring and manipulating objects using data-driven approaches
that utilise high-resolution vision-based tactile sensors. However, 3D shape
reconstruction using tactile sensing has lagged behind visual shape
reconstruction because of limitations in existing techniques, including the
inability to generalise over unseen shapes, the absence of real-world testing,
and limited expressive capacity imposed by discrete representations. To address
these challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D
shape reconstruction that leverages the rich information provided by a
vision-based tactile sensor and the expressivity of the implicit neural
representation DeepSDF. Our technique consists of two components: (1) a
Convolutional Neural Network that maps tactile images into local meshes
representing the surface at the touch location, and (2) an implicit neural
function that predicts a signed distance function to extract the desired 3D
shape. This combination allows TouchSDF to reconstruct smooth and continuous 3D
shapes from tactile inputs in simulation and real-world settings, opening up
research avenues for robust 3D-aware representations and improved multimodal
perception in robotics. Code and supplementary material are available at:
https://touchsdf.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12612">A New Type Of Upper And Lower Bounds On Right-Tail Probabilities Of Continuous Random Variables. (arXiv:2311.12612v1 [math.PR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zlatanov_N/0/1/0/all/0/1">Nikola Zlatanov</a></p>
<p>In this paper, I present a completely new type of upper and lower bounds on
the right-tail probabilities of continuous random variables with unbounded
support and with semi-bounded support from the left. The presented upper and
lower right-tail bounds depend only on the probability density function (PDF),
its first derivative, and two parameters that are used for tightening the
bounds. These tail bounds hold under certain conditions that depend on the PDF,
its first and second derivatives, and the two parameters. The new tail bounds
are shown to be tight for a wide range of continuous random variables via
numerical examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12613">Decentralised Q-Learning for Multi-Agent Markov Decision Processes with a Satisfiability Criterion. (arXiv:2311.12613v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Keval_K/0/1/0/all/0/1">Keshav P. Keval</a>, <a href="http://arxiv.org/find/eess/1/au:+Borkar_V/0/1/0/all/0/1">Vivek S. Borkar</a></p>
<p>In this paper, we propose a reinforcement learning algorithm to solve a
multi-agent Markov decision process (MMDP). The goal, inspired by Blackwell's
Approachability Theorem, is to lower the time average cost of each agent to
below a pre-specified agent-specific bound. For the MMDP, we assume the state
dynamics to be controlled by the joint actions of agents, but the per-stage
costs to only depend on the individual agent's actions. We combine the
Q-learning algorithm for a weighted combination of the costs of each agent,
obtained by a gossip algorithm with the Metropolis-Hastings or Multiplicative
Weights formalisms to modulate the averaging matrix of the gossip. We use
multiple timescales in our algorithm and prove that under mild conditions, it
approximately achieves the desired bounds for each of the agents. We also
demonstrate the empirical performance of this algorithm in the more general
setting of MMDPs having jointly controlled per-stage costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12615">Koopman Learning with Episodic Memory. (arXiv:2311.12615v1 [math.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Redman_W/0/1/0/all/0/1">William T. Redman</a>, <a href="http://arxiv.org/find/math/1/au:+Huang_D/0/1/0/all/0/1">Dean Huang</a>, <a href="http://arxiv.org/find/math/1/au:+Fonoberova_M/0/1/0/all/0/1">Maria Fonoberova</a>, <a href="http://arxiv.org/find/math/1/au:+Mezic_I/0/1/0/all/0/1">Igor Mezi&#x107;</a></p>
<p>Koopman operator theory, a data-driven dynamical systems framework, has found
significant success in learning models from complex, real-world data sets,
enabling state-of-the-art prediction and control. The greater interpretability
and lower computational costs of these models, compared to traditional machine
learning methodologies, make Koopman learning an especially appealing approach.
Despite this, little work has been performed on endowing Koopman learning with
the ability to learn from its own mistakes. To address this, we equip Koopman
methods - developed for predicting non-stationary time-series - with an
episodic memory mechanism, enabling global recall of (or attention to) periods
in time where similar dynamics previously occurred. We find that a basic
implementation of Koopman learning with episodic memory leads to significant
improvements in prediction on synthetic and real-world data. Our framework has
considerable potential for expansion, allowing for future advances, and opens
exciting new directions for Koopman learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12624">Bridging Algorithmic Information Theory and Machine Learning: A New Approach to Kernel Learning. (arXiv:2311.12624v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hamzi_B/0/1/0/all/0/1">Boumediene Hamzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1">Marcus Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Owhadi_H/0/1/0/all/0/1">Houman Owhadi</a></p>
<p>Machine Learning (ML) and Algorithmic Information Theory (AIT) look at
Complexity from different points of view. We explore the interface between AIT
and Kernel Methods (that are prevalent in ML) by adopting an AIT perspective on
the problem of learning kernels from data, in kernel ridge regression, through
the method of Sparse Kernel Flows. In particular, by looking at the differences
and commonalities between Minimal Description Length (MDL) and Regularization
in Machine Learning (RML), we prove that the method of Sparse Kernel Flows is
the natural approach to adopt to learn kernels from data. This paper shows that
it is not necessary to use the statistical route to derive Sparse Kernel Flows
and that one can directly work with code-lengths and complexities that are
concepts that show up in AIT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12630">Hierarchical Joint Graph Learning and Multivariate Time Series Forecasting. (arXiv:2311.12630v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Juhyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungeun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Seungwon Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1">Ung Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Wooyul Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1">Miseon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kijung Yoon</a></p>
<p>Multivariate time series is prevalent in many scientific and industrial
domains. Modeling multivariate signals is challenging due to their long-range
temporal dependencies and intricate interactions--both direct and indirect. To
confront these complexities, we introduce a method of representing multivariate
signals as nodes in a graph with edges indicating interdependency between them.
Specifically, we leverage graph neural networks (GNN) and attention mechanisms
to efficiently learn the underlying relationships within the time series data.
Moreover, we suggest employing hierarchical signal decompositions running over
the graphs to capture multiple spatial dependencies. The effectiveness of our
proposed model is evaluated across various real-world benchmark datasets
designed for long-term forecasting tasks. The results consistently showcase the
superiority of our model, achieving an average 23\% reduction in mean squared
error (MSE) compared to existing models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12644">Careful Selection and Thoughtful Discarding: Graph Explicit Pooling Utilizing Discarded Nodes. (arXiv:2311.12644v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kuang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xueqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yibing Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Bo Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbin Hu</a></p>
<p>Graph pooling has been increasingly recognized as crucial for Graph Neural
Networks (GNNs) to facilitate hierarchical graph representation learning.
Existing graph pooling methods commonly consist of two stages: selecting
top-ranked nodes and discarding the remaining to construct coarsened graph
representations. However, this paper highlights two key issues with these
methods: 1) The process of selecting nodes to discard frequently employs
additional Graph Convolutional Networks or Multilayer Perceptrons, lacking a
thorough evaluation of each node's impact on the final graph representation and
subsequent prediction tasks. 2) Current graph pooling methods tend to directly
discard the noise segment (dropped) of the graph without accounting for the
latent information contained within these elements. To address the first issue,
we introduce a novel Graph Explicit Pooling (GrePool) method, which selects
nodes by explicitly leveraging the relationships between the nodes and final
representation vectors crucial for classification. The second issue is
addressed using an extended version of GrePool (i.e., GrePool+), which applies
a uniform loss on the discarded nodes. This addition is designed to augment the
training process and improve classification accuracy. Furthermore, we conduct
comprehensive experiments across 12 widely used datasets to validate our
proposed method's effectiveness, including the Open Graph Benchmark datasets.
Our experimental results uniformly demonstrate that GrePool outperforms 14
baseline methods for most datasets. Likewise, implementing GrePool+ enhances
GrePool's performance without incurring additional computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12652">FedDRO: Federated Compositional Optimization for Distributionally Robust Learning. (arXiv:2311.12652v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1">Prashant Khanduri</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengyin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sultan_R/0/1/0/all/0/1">Rafi Ibn Sultan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1">Yao Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1">Joerg Kliewer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dongxiao Zhu</a></p>
<p>Recently, compositional optimization (CO) has gained popularity because of
its applications in distributionally robust optimization (DRO) and many other
machine learning problems. Large-scale and distributed availability of data
demands the development of efficient federated learning (FL) algorithms for
solving CO problems. Developing FL algorithms for CO is particularly
challenging because of the compositional nature of the objective. Moreover,
current state-of-the-art methods to solve such problems rely on large batch
gradients (depending on the solution accuracy) not feasible for most practical
settings. To address these challenges, in this work, we propose efficient
FedAvg-type algorithms for solving non-convex CO in the FL setting. We first
establish that vanilla FedAvg is not suitable to solve distributed CO problems
because of the data heterogeneity in the compositional objective at each client
which leads to the amplification of bias in the local compositional gradient
estimates. To this end, we propose a novel FL framework FedDRO that utilizes
the DRO problem structure to design a communication strategy that allows FedAvg
to control the bias in the estimation of the compositional gradient. A key
novelty of our work is to develop solution accuracy-independent algorithms that
do not require large batch gradients (and function evaluations) for solving
federated CO problems. We establish $\mathcal{O}(\epsilon^{-2})$ sample and
$\mathcal{O}(\epsilon^{-3/2})$ communication complexity in the FL setting while
achieving linear speedup with the number of clients. We corroborate our
theoretical findings with empirical studies on large-scale DRO problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12657">Carbohydrate NMR chemical shift predictions using E(3) equivariant graph neural networks. (arXiv:2311.12657v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baankestad_M/0/1/0/all/0/1">Maria B&#xe5;nkestad</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorst_K/0/1/0/all/0/1">Keven M. Dorst</a>, <a href="http://arxiv.org/find/cs/1/au:+Widmalm_G/0/1/0/all/0/1">G&#xf6;ran Widmalm</a>, <a href="http://arxiv.org/find/cs/1/au:+Ronnols_J/0/1/0/all/0/1">Jerk R&#xf6;nnols</a></p>
<p>Carbohydrates, vital components of biological systems, are well-known for
their structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays
a crucial role in understanding their intricate molecular arrangements and is
essential in assessing and verifying the molecular structure of organic
molecules. An important part of this process is to predict the NMR chemical
shift from the molecular structure. This work introduces a novel approach that
leverages E(3) equivariant graph neural networks to predict carbohydrate NMR
spectra. Notably, our model achieves a substantial reduction in mean absolute
error, up to threefold, compared to traditional models that rely solely on
two-dimensional molecular structure. Even with limited data, the model excels,
highlighting its robustness and generalization capabilities. The implications
are far-reaching and go beyond an advanced understanding of carbohydrate
structures and spectral interpretation. For example, it could accelerate
research in pharmaceutical applications, biochemistry, and structural biology,
offering a faster and more reliable analysis of molecular structures.
Furthermore, our approach is a key step towards a new data-driven era in
spectroscopy, potentially influencing spectroscopic techniques beyond NMR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12666">SSVEP-DAN: A Data Alignment Network for SSVEP-based Brain Computer Interfaces. (arXiv:2311.12666v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sung-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chi-Min Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_K/0/1/0/all/0/1">Kuan-Jung Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chun-Shu Wei</a></p>
<p>Steady-state visual-evoked potential (SSVEP)-based brain-computer interfaces
(BCIs) offer a non-invasive means of communication through high-speed speller
systems. However, their efficiency heavily relies on individual training data
obtained during time-consuming calibration sessions. To address the challenge
of data insufficiency in SSVEP-based BCIs, we present SSVEP-DAN, the first
dedicated neural network model designed for aligning SSVEP data across
different domains, which can encompass various sessions, subjects, or devices.
Our experimental results across multiple cross-domain scenarios demonstrate
SSVEP-DAN's capability to transform existing source SSVEP data into
supplementary calibration data, significantly enhancing SSVEP decoding accuracy
in scenarios with limited calibration data. We envision SSVEP-DAN as a catalyst
for practical SSVEP-based BCI applications with minimal calibration. The source
codes in this work are available at: https://github.com/CECNL/SSVEP-DAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12670">Towards a more inductive world for drug repurposing approaches. (arXiv:2311.12670v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fuente_J/0/1/0/all/0/1">Jesus de la Fuente</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrano_G/0/1/0/all/0/1">Guillermo Serrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Veleiro_U/0/1/0/all/0/1">Ux&#xed;a Veleiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Casals_M/0/1/0/all/0/1">Mikel Casals</a>, <a href="http://arxiv.org/find/cs/1/au:+Vera_L/0/1/0/all/0/1">Laura Vera</a>, <a href="http://arxiv.org/find/cs/1/au:+Pizurica_M/0/1/0/all/0/1">Marija Pizurica</a>, <a href="http://arxiv.org/find/cs/1/au:+Pineda_Lucena_A/0/1/0/all/0/1">Antonio Pineda-Lucena</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochoa_I/0/1/0/all/0/1">Idoia Ochoa</a>, <a href="http://arxiv.org/find/cs/1/au:+Vicent_S/0/1/0/all/0/1">Silve Vicent</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernaez_M/0/1/0/all/0/1">Mikel Hernaez</a></p>
<p>Drug-target interaction (DTI) prediction is a challenging, albeit essential
task in drug repurposing. Learning on graph models have drawn special attention
as they can significantly reduce drug repurposing costs and time commitment.
However, many current approaches require high-demanding additional information
besides DTIs that complicates their evaluation process and usability.
Additionally, structural differences in the learning architecture of current
models hinder their fair benchmarking. In this work, we first perform an
in-depth evaluation of current DTI datasets and prediction models through a
robust benchmarking process, and show that DTI prediction methods based on
transductive models lack generalization and lead to inflated performance when
evaluated as previously done in the literature, hence not being suited for drug
repurposing approaches. We then propose a novel biologically-driven strategy
for negative edge subsampling and show through in vitro validation that newly
discovered interactions are indeed true. We envision this work as the
underpinning for future fair benchmarking and robust model design. All
generated resources and tools are publicly available as a python package.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12674">Contrastive Left-Right Wearable Sensors (IMUs) Consistency Matching for HAR. (arXiv:2311.12674v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nshimyimana_D/0/1/0/all/0/1">Dominique Nshimyimana</a>, <a href="http://arxiv.org/find/cs/1/au:+Rey_V/0/1/0/all/0/1">Vitor Fortes Rey</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukowic_P/0/1/0/all/0/1">Paul Lukowic</a></p>
<p>Machine learning algorithms are improving rapidly, but annotating training
data remains a bottleneck for many applications. In this paper, we show how
real data can be used for self-supervised learning without any transformations
by taking advantage of the symmetry present in the activities. Our approach
involves contrastive matching of two different sensors (left and right wrist or
leg-worn IMUs) to make representations of co-occurring sensor data more similar
and those of non-co-occurring sensor data more different. We test our approach
on the Opportunity and MM-Fit datasets. In MM-Fit we show significant
improvement over the baseline supervised and self-supervised method SimCLR,
while for Opportunity there is significant improvement over the supervised
baseline and slight improvement when compared to SimCLR. Moreover, our method
improves supervised baselines even when using only a small amount of the data
for training. Future work should explore under which conditions our method is
beneficial for human activity recognition systems and other related
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12678">Interpretation of the Transformer and Improvement of the Extractor. (arXiv:2311.12678v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a></p>
<p>It has been over six years since the Transformer architecture was put
forward. Surprisingly, the vanilla Transformer architecture is still widely
used today. One reason is that the lack of deep understanding and comprehensive
interpretation of the Transformer architecture makes it more challenging to
improve the Transformer architecture. In this paper, we first interpret the
Transformer architecture comprehensively in plain words based on our
understanding and experiences. The interpretations are further proved and
verified. These interpretations also cover the Extractor, a family of drop-in
replacements for the multi-head self-attention in the Transformer architecture.
Then, we propose an improvement on a type of the Extractor that outperforms the
self-attention, without introducing additional trainable parameters.
Experimental results demonstrate that the improved Extractor performs even
better, showing a way to improve the Transformer architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12679">BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos. (arXiv:2311.12679v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1">Georgios Albanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1">Nikolaos Zioulis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolomvatsos_K/0/1/0/all/0/1">Kostas Kolomvatsos</a></p>
<p>Capturing smooth motions from videos using markerless techniques typically
involves complex processes such as temporal constraints, multiple stages with
data-driven regression and optimization, and bundle solving over temporal
windows. These processes can be inefficient and require tuning multiple
objectives across stages. In contrast, BundleMoCap introduces a novel and
efficient approach to this problem. It solves the motion capture task in a
single stage, eliminating the need for temporal smoothness objectives while
still delivering smooth motions. BundleMoCap outperforms the state-of-the-art
without increasing complexity. The key concept behind BundleMoCap is manifold
interpolation between latent keyframes. By relying on a local manifold
smoothness assumption, we can efficiently solve a bundle of frames using a
single code. Additionally, the method can be implemented as a sliding window
optimization and requires only the first frame to be properly initialized,
reducing the overall computational burden. BundleMoCap's strength lies in its
ability to achieve high-quality motion capture results with simplicity and
efficiency. More details can be found at https://moverseai.github.io/bundle/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12684">Adversarial Reweighting Guided by Wasserstein Distance for Bias Mitigation. (arXiv:2311.12684v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabbrizzi_S/0/1/0/all/0/1">Simone Fabbrizzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lobo_P/0/1/0/all/0/1">Paula Reyero Lobo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghodsi_S/0/1/0/all/0/1">Siamak Ghodsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1">Klaus Broelemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1">Steffen Staab</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>The unequal representation of different groups in a sample population can
lead to discrimination of minority groups when machine learning models make
automated decisions. To address these issues, fairness-aware machine learning
jointly optimizes two (or more) metrics aiming at predictive effectiveness and
low unfairness. However, the inherent under-representation of minorities in the
data makes the disparate treatment of subpopulations less noticeable and
difficult to deal with during learning. In this paper, we propose a novel
adversarial reweighting method to address such \emph{representation bias}. To
balance the data distribution between the majority and the minority groups, our
approach deemphasizes samples from the majority group. To minimize empirical
risk, our method prefers samples from the majority group that are close to the
minority group as evaluated by the Wasserstein distance. Our theoretical
analysis shows the effectiveness of our adversarial reweighting approach.
Experiments demonstrate that our approach mitigates bias without sacrificing
classification accuracy, outperforming related state-of-the-art methods on
image and tabular benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12686">Managing ML-Based Application Non-Functional Behavior: A Multi-Model Approach. (arXiv:2311.12686v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anisetti_M/0/1/0/all/0/1">Marco Anisetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Ardagna_C/0/1/0/all/0/1">Claudio A. Ardagna</a>, <a href="http://arxiv.org/find/cs/1/au:+Bena_N/0/1/0/all/0/1">Nicola Bena</a>, <a href="http://arxiv.org/find/cs/1/au:+Damiani_E/0/1/0/all/0/1">Ernesto Damiani</a>, <a href="http://arxiv.org/find/cs/1/au:+Panero_P/0/1/0/all/0/1">Paolo G. Panero</a></p>
<p>Modern applications are increasingly driven by Machine Learning (ML) models
whose non-deterministic behavior is affecting the entire application life cycle
from design to operation. The pervasive adoption of ML is urgently calling for
approaches that guarantee a stable non-functional behavior of ML-based
applications over time and across model changes. To this aim, non-functional
properties of ML models, such as privacy, confidentiality, fairness, and
explainability, must be monitored, verified, and maintained. This need is even
more pressing when modern applications operate in the edge-cloud continuum,
increasing their complexity and dynamicity. Existing approaches mostly focus on
i) implementing classifier selection solutions according to the functional
behavior of ML models, ii) finding new algorithmic solutions to this need, such
as continuous re-training. In this paper, we propose a multi-model approach
built on dynamic classifier selection, where multiple ML models showing similar
non-functional properties are made available to the application and one model
is selected over time according to (dynamic and unpredictable) contextual
changes. Our solution goes beyond the state of the art by providing an
architectural and methodological approach that continuously guarantees a stable
non-functional behavior of ML-based applications, is applicable to different ML
models, and is driven by non-functional properties assessed on the models
themselves. It consists of a two-step process working during application
operation, where model assessment verifies non-functional properties of ML
models trained and selected at development time, and model substitution
guarantees a continuous and stable support of non-functional properties. We
experimentally evaluate our solution in a real-world scenario focusing on
non-functional property fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12688">On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning. (arXiv:2311.12688v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scemama_P/0/1/0/all/0/1">Paul Scemama</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapusta_A/0/1/0/all/0/1">Ariel Kapusta</a></p>
<p>Bayesian deep learning and conformal prediction are two methods that have
been used to convey uncertainty and increase safety in machine learning
systems. We focus on combining Bayesian deep learning with split conformal
prediction and how this combination effects out-of-distribution coverage;
particularly in the case of multiclass image classification. We suggest that if
the model is generally underconfident on the calibration set, then the
resultant conformal sets may exhibit worse out-of-distribution coverage
compared to simple predictive credible sets. Conversely, if the model is
overconfident on the calibration set, the use of conformal prediction may
improve out-of-distribution coverage. We evaluate prediction sets as a result
of combining split conformal methods and neural networks trained with (i)
stochastic gradient descent, (ii) deep ensembles, and (iii) mean-field
variational inference. Our results suggest that combining Bayesian deep
learning models with split conformal prediction can, in some cases, cause
unintended consequences such as reducing out-of-distribution coverage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12689">Fair Text Classification with Wasserstein Independence. (arXiv:2311.12689v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leteno_T/0/1/0/all/0/1">Thibaud Leteno</a>, <a href="http://arxiv.org/find/cs/1/au:+Gourru_A/0/1/0/all/0/1">Antoine Gourru</a>, <a href="http://arxiv.org/find/cs/1/au:+Laclau_C/0/1/0/all/0/1">Charlotte Laclau</a>, <a href="http://arxiv.org/find/cs/1/au:+Emonet_R/0/1/0/all/0/1">R&#xe9;mi Emonet</a>, <a href="http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1">Christophe Gravier</a></p>
<p>Group fairness is a central research topic in text classification, where
reaching fair treatment between sensitive groups (e.g. women vs. men) remains
an open challenge. This paper presents a novel method for mitigating biases in
neural text classification, agnostic to the model architecture. Considering the
difficulty to distinguish fair from unfair information in a text encoder, we
take inspiration from adversarial training to induce Wasserstein independence
between representations learned to predict our target label and the ones
learned to predict some sensitive attribute. Our approach provides two
significant advantages. Firstly, it does not require annotations of sensitive
attributes in both testing and training data. This is more suitable for
real-life scenarios compared to existing methods that require annotations of
sensitive attributes at train time. Second, our approach exhibits a comparable
or better fairness-accuracy trade-off compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12711">Regression-Based Analysis of Multimodal Single-Cell Data Integration Strategies. (arXiv:2311.12711v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehta_B/0/1/0/all/0/1">Bhavya Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Deliwala_N/0/1/0/all/0/1">Nirmit Deliwala</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandane_M/0/1/0/all/0/1">Madhav Chandane</a></p>
<p>Multimodal single-cell technologies enable the simultaneous collection of
diverse data types from individual cells, enhancing our understanding of
cellular states. However, the integration of these datatypes and modeling the
interrelationships between modalities presents substantial computational and
analytical challenges in disease biomarker detection and drug discovery.
Established practices rely on isolated methodologies to investigate individual
molecular aspects separately, often resulting in inaccurate analyses. To
address these obstacles, distinct Machine Learning Techniques are leveraged,
each of its own kind to model the co-variation of DNA to RNA, and finally to
surface proteins in single cells during hematopoietic stem cell development,
which simplifies understanding of underlying cellular mechanisms and immune
responses. Experiments conducted on a curated subset of a 300,000-cell time
course dataset, highlights the exceptional performance of Echo State Networks,
boasting a remarkable state-of-the-art correlation score of 0.94 and 0.895 on
Multi-omic and CiteSeq datasets. Beyond the confines of this study, these
findings hold promise for advancing comprehension of cellular differentiation
and function, leveraging the potential of Machine Learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12715">Attacks of fairness in Federated Learning. (arXiv:2311.12715v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rance_J/0/1/0/all/0/1">Joseph Rance</a>, <a href="http://arxiv.org/find/cs/1/au:+Svoboda_F/0/1/0/all/0/1">Filip Svoboda</a></p>
<p>Federated Learning is an important emerging distributed training paradigm
that keeps data private on clients. It is now well understood that by
controlling only a small subset of FL clients, it is possible to introduce a
backdoor to a federated learning model, in the presence of certain attributes.
In this paper, we present a new type of attack that compromises the fairness of
the trained model. Fairness is understood to be the attribute-level performance
distribution of a trained model. It is particularly salient in domains where,
for example, skewed accuracy discrimination between subpopulations could have
disastrous consequences. We find that by employing a threat model similar to
that of a backdoor attack, an attacker is able to influence the aggregated
model to have an unfair performance distribution between any given set of
attributes. Furthermore, we find that this attack is possible by controlling
only a single client. While combating naturally induced unfairness in FL has
previously been discussed in depth, its artificially induced kind has been
neglected. We show that defending against attacks on fairness should be a
critical consideration in any situation where unfairness in a trained model
could benefit a user who participated in its training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12716">minimax: Efficient Baselines for Autocurricula in JAX. (arXiv:2311.12716v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1">Michael Dennis</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a></p>
<p>Unsupervised environment design (UED) is a form of automatic curriculum
learning for training robust decision-making agents to zero-shot transfer into
unseen environments. Such autocurricula have received much interest from the RL
community. However, UED experiments, based on CPU rollouts and GPU model
updates, have often required several weeks of training. This compute
requirement is a major obstacle to rapid innovation for the field. This work
introduces the minimax library for UED training on accelerated hardware. Using
JAX to implement fully-tensorized environments and autocurriculum algorithms,
minimax allows the entire training loop to be compiled for hardware
acceleration. To provide a petri dish for rapid experimentation, minimax
includes a tensorized grid-world based on MiniGrid, in addition to reusable
abstractions for conducting autocurricula in procedurally-generated
environments. With these components, minimax provides strong UED baselines,
including new parallelized variants, which achieve over 120$\times$ speedups in
wall time compared to previous implementations when training with equal batch
sizes. The minimax library is available under the Apache 2.0 license at
https://github.com/facebookresearch/minimax.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12722">Attacking Motion Planners Using Adversarial Perception Errors. (arXiv:2311.12722v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadeghi_J/0/1/0/all/0/1">Jonathan Sadeghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lord_N/0/1/0/all/0/1">Nicholas A. Lord</a>, <a href="http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1">John Redford</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1">Romain Mueller</a></p>
<p>Autonomous driving (AD) systems are often built and tested in a modular
fashion, where the performance of different modules is measured using
task-specific metrics. These metrics should be chosen so as to capture the
downstream impact of each module and the performance of the system as a whole.
For example, high perception quality should enable prediction and planning to
be performed safely. Even though this is true in general, we show here that it
is possible to construct planner inputs that score very highly on various
perception quality metrics but still lead to planning failures. In an analogy
to adversarial attacks on image classifiers, we call such inputs
\textbf{adversarial perception errors} and show they can be systematically
constructed using a simple boundary-attack algorithm. We demonstrate the
effectiveness of this algorithm by finding attacks for two different black-box
planners in several urban and highway driving scenarios using the CARLA
simulator. Finally, we analyse the properties of these attacks and show that
they are isolated in the input space of the planner, and discuss their
implications for AD system deployment and testing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12727">Soft Random Sampling: A Theoretical and Empirical Analysis. (arXiv:2311.12727v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xiaodong Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Ashish Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Songtao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1">George Saon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1">Brian Kingsbury</a></p>
<p>Soft random sampling (SRS) is a simple yet effective approach for efficient
training of large-scale deep neural networks when dealing with massive data.
SRS selects a subset uniformly at random with replacement from the full data
set in each epoch. In this paper, we conduct a theoretical and empirical
analysis of SRS. First, we analyze its sampling dynamics including data
coverage and occupancy. Next, we investigate its convergence with non-convex
objective functions and give the convergence rate. Finally, we provide its
generalization performance. We empirically evaluate SRS for image recognition
on CIFAR10 and automatic speech recognition on Librispeech and an in-house
payload dataset to demonstrate its effectiveness. Compared to existing
coreset-based data selection methods, SRS offers a better accuracy-efficiency
trade-off. Especially on real-world industrial scale data sets, it is shown to
be a powerful training strategy with significant speedup and competitive
performance with almost no additional computing cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12737">Exploring Graph Classification Techniques Under Low Data Constraints: A Comprehensive Study. (arXiv:2311.12737v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kothari_K/0/1/0/all/0/1">Kush Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_B/0/1/0/all/0/1">Bhavya Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Nambiar_R/0/1/0/all/0/1">Reshmika Nambiar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrawne_S/0/1/0/all/0/1">Seema Shrawne</a></p>
<p>This survey paper presents a brief overview of recent research on graph data
augmentation and few-shot learning. It covers various techniques for graph data
augmentation, including node and edge perturbation, graph coarsening, and graph
generation, as well as the latest developments in few-shot learning, such as
meta-learning and model-agnostic meta-learning. The paper explores these areas
in depth and delves into further sub classifications. Rule based approaches and
learning based approaches are surveyed under graph augmentation techniques.
Few-Shot Learning on graphs is also studied in terms of metric learning
techniques and optimization-based techniques. In all, this paper provides an
extensive array of techniques that can be employed in solving graph processing
problems faced in low-data scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12741">Content Augmented Graph Neural Networks. (arXiv:2311.12741v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_F/0/1/0/all/0/1">Fatemeh Gholamzadeh Nasrabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kashani_A/0/1/0/all/0/1">AmirHossein Kashani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zahedi_P/0/1/0/all/0/1">Pegah Zahedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1">Mostafa Haghir Chehreghani</a></p>
<p>In recent years, graph neural networks (GNNs) have become a popular tool for
solving various problems over graphs. In these models, the link structure of
the graph is typically exploited and nodes' embeddings are iteratively updated
based on adjacent nodes. Nodes' contents are used solely in the form of feature
vectors, served as nodes' first-layer embeddings. However, the filters or
convolutions, applied during iterations/layers to these initial embeddings lead
to their impact diminish and contribute insignificantly to the final
embeddings. In order to address this issue, in this paper we propose augmenting
nodes' embeddings by embeddings generating from their content, at higher GNN
layers. More precisely, we propose models wherein a structural embedding using
a GNN and a content embedding are computed for each node. These two are
combined using a combination layer to form the embedding of a node at a given
layer. We suggest methods such as using an auto-encoder or building a content
graph, to generate content embeddings. In the end, by conducting experiments
over several real-world datasets, we demonstrate the high accuracy and
performance of our models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12742">Image Transformation for IoT Time-Series Data: A Review. (arXiv:2311.12742v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Altunkaya_D/0/1/0/all/0/1">Duygu Altunkaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Okay_F/0/1/0/all/0/1">Feyza Yildirim Okay</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozdemir_S/0/1/0/all/0/1">Suat Ozdemir</a></p>
<p>In the era of the Internet of Things (IoT), where smartphones, built-in
systems, wireless sensors, and nearly every smart device connect through local
networks or the internet, billions of smart things communicate with each other
and generate vast amounts of time-series data. As IoT time-series data is
high-dimensional and high-frequency, time-series classification or regression
has been a challenging issue in IoT. Recently, deep learning algorithms have
demonstrated superior performance results in time-series data classification in
many smart and intelligent IoT applications. However, it is hard to explore the
hidden dynamic patterns and trends in time-series. Recent studies show that
transforming IoT data into images improves the performance of the learning
model. In this paper, we present a review of these studies which use image
transformation/encoding techniques in IoT domain. We examine the studies
according to their encoding techniques, data types, and application areas.
Lastly, we emphasize the challenges and future dimensions of image
transformation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12750">Learning to Optimise Wind Farms with Graph Transformers. (arXiv:2311.12750v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Robert_A/0/1/0/all/0/1">Arnaud Robert</a>, <a href="http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1">A. Aldo Faisal</a>, <a href="http://arxiv.org/find/cs/1/au:+Piggott_M/0/1/0/all/0/1">Matthew D. Piggott</a></p>
<p>This work proposes a novel data-driven model capable of providing accurate
predictions for the power generation of all wind turbines in wind farms of
arbitrary layout, yaw angle configurations and wind conditions. The proposed
model functions by encoding a wind farm into a fully-connected graph and
processing the graph representation through a graph transformer. The graph
transformer surrogate is shown to generalise well and is able to uncover latent
structural patterns within the graph representation of wind farms. It is
demonstrated how the resulting surrogate model can be used to optimise yaw
angle configurations using genetic algorithms, achieving similar levels of
accuracy to industrially-standard wind farm simulation tools while only taking
a fraction of the computational cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12754">SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction. (arXiv:2311.12754v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuanhui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wenzhao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Borui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a></p>
<p>3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird's eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12760">High-resolution Image-based Malware Classification using Multiple Instance Learning. (arXiv:2311.12760v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peters_T/0/1/0/all/0/1">Tim Peters</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhat_H/0/1/0/all/0/1">Hikmat Farhat</a></p>
<p>This paper proposes a novel method of classifying malware into families using
high-resolution greyscale images and multiple instance learning to overcome
adversarial binary enlargement. Current methods of visualisation-based malware
classification largely rely on lossy transformations of inputs such as resizing
to handle the large, variable-sized images. Through empirical analysis and
experimentation, it is shown that these approaches cause crucial information
loss that can be exploited. The proposed solution divides the images into
patches and uses embedding-based multiple instance learning with a
convolutional neural network and an attention aggregation function for
classification. The implementation is evaluated on the Microsoft Malware
Classification dataset and achieves accuracies of up to $96.6\%$ on
adversarially enlarged samples compared to the baseline of $22.8\%$. The Python
code is available online at https://github.com/timppeters/MIL-Malware-Images .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12781">Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects. (arXiv:2311.12781v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Boyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1">Aakash Kaku</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangning Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parnandi_A/0/1/0/all/0/1">Avinash Parnandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fokas_E/0/1/0/all/0/1">Emily Fokas</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatesan_A/0/1/0/all/0/1">Anita Venkatesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandit_N/0/1/0/all/0/1">Natasha Pandit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1">Rajesh Ranganath</a>, <a href="http://arxiv.org/find/cs/1/au:+Schambra_H/0/1/0/all/0/1">Heidi Schambra</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1">Carlos Fernandez-Granda</a></p>
<p>Automatic assessment of impairment and disease severity is a key challenge in
data-driven medicine. We propose a novel framework to address this challenge,
which leverages AI models trained exclusively on healthy individuals. The
COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the
decrease in confidence of these models when presented with impaired or diseased
patients to quantify their deviation from the healthy population. We applied
the COBRA score to address a key limitation of current clinical evaluation of
upper-body impairment in stroke patients. The gold-standard Fugl-Meyer
Assessment (FMA) requires in-person administration by a trained assessor for
30-45 minutes, which restricts monitoring frequency and precludes physicians
from adapting rehabilitation protocols to the progress of each patient. The
COBRA score, computed automatically in under one minute, is shown to be
strongly correlated with the FMA on an independent test cohort for two
different data modalities: wearable sensors ($\rho = 0.845$, 95% CI
[0.743,0.908]) and video ($\rho = 0.746$, 95% C.I [0.594, 0.847]). To
demonstrate the generalizability of the approach to other conditions, the COBRA
score was also applied to quantify severity of knee osteoarthritis from
magnetic-resonance imaging scans, again achieving significant correlation with
an independent clinical assessment ($\rho = 0.644$, 95% C.I [0.585,0.696]).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12784">Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian, and Beyond $1+\alpha$ Moments. (arXiv:2311.12784v1 [math.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Dang_T/0/1/0/all/0/1">Trung Dang</a>, <a href="http://arxiv.org/find/math/1/au:+Lee_J/0/1/0/all/0/1">Jasper C.H. Lee</a>, <a href="http://arxiv.org/find/math/1/au:+Song_M/0/1/0/all/0/1">Maoyuan Song</a>, <a href="http://arxiv.org/find/math/1/au:+Valiant_P/0/1/0/all/0/1">Paul Valiant</a></p>
<p>There is growing interest in improving our algorithmic understanding of
fundamental statistical problems such as mean estimation, driven by the goal of
understanding the limits of what we can extract from valuable data. The state
of the art results for mean estimation in $\mathbb{R}$ are 1) the optimal
sub-Gaussian mean estimator by [LV22], with the tight sub-Gaussian constant for
all distributions with finite but unknown variance, and 2) the analysis of the
median-of-means algorithm by [BCL13] and a lower bound by [DLLO16],
characterizing the big-O optimal errors for distributions for which only a
$1+\alpha$ moment exists for $\alpha \in (0,1)$. Both results, however, are
optimal only in the worst case. We initiate the fine-grained study of the mean
estimation problem: Can algorithms leverage useful features of the input
distribution to beat the sub-Gaussian rate, without explicit knowledge of such
features?
</p>
<p>We resolve this question with an unexpectedly nuanced answer: "Yes in limited
regimes, but in general no". For any distribution $p$ with a finite mean, we
construct a distribution $q$ whose mean is well-separated from $p$'s, yet $p$
and $q$ are not distinguishable with high probability, and $q$ further
preserves $p$'s moments up to constants. The main consequence is that no
reasonable estimator can asymptotically achieve better than the sub-Gaussian
error rate for any distribution, matching the worst-case result of [LV22]. More
generally, we introduce a new definitional framework to analyze the
fine-grained optimality of algorithms, which we call "neighborhood optimality",
interpolating between the unattainably strong "instance optimality" and the
trivially weak "admissibility" definitions. Applying the new framework, we show
that median-of-means is neighborhood optimal, up to constant factors. It is
open to find a neighborhood-optimal estimator without constant factor
slackness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12786">Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. (arXiv:2311.12786v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Samyak Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirk_R/0/1/0/all/0/1">Robert Kirk</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1">Ekdeep Singh Lubana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1">Robert P. Dick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Scott Krueger</a></p>
<p>Fine-tuning large pre-trained models has become the de facto strategy for
developing both task-specific and general-purpose machine learning systems,
including developing models that are safe to deploy. Despite its clear
importance, there has been minimal work that explains how fine-tuning alters
the underlying capabilities learned by a model during pretraining: does
fine-tuning yield entirely novel capabilities or does it just modulate existing
ones? We address this question empirically in synthetic, controlled settings
where we can use mechanistic interpretability tools (e.g., network pruning and
probing) to understand how the model's underlying capabilities are changing. We
perform an extensive analysis of the effects of fine-tuning in these settings,
and show that: (i) fine-tuning rarely alters the underlying model capabilities;
(ii) a minimal transformation, which we call a 'wrapper', is typically learned
on top of the underlying model capabilities, creating the illusion that they
have been modified; and (iii) further fine-tuning on a task where such hidden
capabilities are relevant leads to sample-efficient 'revival' of the
capability, i.e., the model begins reusing these capability after only a few
gradient steps. This indicates that practitioners can unintentionally remove a
model's safety wrapper merely by fine-tuning it on a, e.g., superficially
unrelated, downstream task. We additionally perform analysis on language models
trained on the TinyStories dataset to support our claims in a more realistic
setup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2004.07780">Shortcut Learning in Deep Neural Networks. (arXiv:2004.07780v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1">Robert Geirhos</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1">J&#xf6;rn-Henrik Jacobsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Michaelis_C/0/1/0/all/0/1">Claudio Michaelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>, <a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1">Wieland Brendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1">Felix A. Wichmann</a></p>
<p>Deep learning has triggered the current rise of artificial intelligence and
is the workhorse of today's machine intelligence. Numerous success stories have
rapidly spread all over science, industry and society, but its limitations have
only recently come into focus. In this perspective we seek to distill how many
of deep learning's problems can be seen as different symptoms of the same
underlying problem: shortcut learning. Shortcuts are decision rules that
perform well on standard benchmarks but fail to transfer to more challenging
testing conditions, such as real-world scenarios. Related issues are known in
Comparative Psychology, Education and Linguistics, suggesting that shortcut
learning may be a common characteristic of learning systems, biological and
artificial alike. Based on these observations, we develop a set of
recommendations for model interpretation and benchmarking, highlighting recent
advances in machine learning to improve robustness and transferability from the
lab to real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.02470">Online Regularization towards Always-Valid High-Dimensional Dynamic Pricing. (arXiv:2007.02470v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1">Chi-Hua Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zhanyu Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1">Will Wei Sun</a>, <a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1">Guang Cheng</a></p>
<p>Devising dynamic pricing policy with always valid online statistical learning
procedure is an important and as yet unresolved problem. Most existing dynamic
pricing policy, which focus on the faithfulness of adopted customer choice
models, exhibit a limited capability for adapting the online uncertainty of
learned statistical model during pricing process. In this paper, we propose a
novel approach for designing dynamic pricing policy based regularized online
statistical learning with theoretical guarantees. The new approach overcomes
the challenge of continuous monitoring of online Lasso procedure and possesses
several appealing properties. In particular, we make the decisive observation
that the always-validity of pricing decisions builds and thrives on the online
regularization scheme. Our proposed online regularization scheme equips the
proposed optimistic online regularized maximum likelihood pricing (OORMLP)
pricing policy with three major advantages: encode market noise knowledge into
pricing process optimism; empower online statistical learning with
always-validity over all decision points; envelop prediction error process with
time-uniform non-asymptotic oracle inequalities. This type of non-asymptotic
inference results allows us to design more sample-efficient and robust dynamic
pricing algorithms in practice. In theory, the proposed OORMLP algorithm
exploits the sparsity structure of high-dimensional models and secures a
logarithmic regret in a decision horizon. These theoretical advances are made
possible by proposing an optimistic online Lasso procedure that resolves
dynamic pricing problems at the process level, based on a novel use of
non-asymptotic martingale concentration. In experiments, we evaluate OORMLP in
different synthetic and real pricing problem settings, and demonstrate that
OORMLP advances the state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2008.12690">ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single Algorithm. (arXiv:2008.12690v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Li_C/0/1/0/all/0/1">Chris Junchi Li</a>, <a href="http://arxiv.org/find/math/1/au:+Mou_W/0/1/0/all/0/1">Wenlong Mou</a>, <a href="http://arxiv.org/find/math/1/au:+Wainwright_M/0/1/0/all/0/1">Martin J. Wainwright</a>, <a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1">Michael I. Jordan</a></p>
<p>We study the problem of solving strongly convex and smooth unconstrained
optimization problems using stochastic first-order algorithms. We devise a
novel algorithm, referred to as \emph{Recursive One-Over-T SGD} (\ROOTSGD),
based on an easily implementable, recursive averaging of past stochastic
gradients. We prove that it simultaneously achieves state-of-the-art
performance in both a finite-sample, nonasymptotic sense and an asymptotic
sense. On the nonasymptotic side, we prove risk bounds on the last iterate of
\ROOTSGD with leading-order terms that match the optimal statistical risk with
a unity pre-factor, along with a higher-order term that scales at the sharp
rate of $O(n^{-3/2})$ under the Lipschitz condition on the Hessian matrix. On
the asymptotic side, we show that when a mild, one-point Hessian continuity
condition is imposed, the rescaled last iterate of (multi-epoch) \ROOTSGD
converges asymptotically to a Gaussian limit with the Cram\'{e}r-Rao optimal
asymptotic covariance, for a broad range of step-size choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.04923">Topological properties of basins of attraction and expressiveness of width bounded neural networks. (arXiv:2011.04923v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beise_H/0/1/0/all/0/1">Hans-Peter Beise</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1">Steve Dias Da Cruz</a></p>
<p>In Radhakrishnan et al. [2020], the authors empirically show that
autoencoders trained with usual SGD methods shape out basins of attraction
around their training data. We consider network functions of width not
exceeding the input dimension and prove that in this situation basins of
attraction are bounded and their complement cannot have bounded components. Our
conditions in these results are met in several experiments of the latter work
and we thus address a question posed therein. We also show that under some more
restrictive conditions the basins of attraction are path-connected. The
tightness of the conditions in our results is demonstrated by means of several
examples. Finally, the arguments used to prove the above results allow us to
derive a root cause why scalar-valued neural network functions that fulfill our
bounded width condition are not dense in spaces of continuous functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.12311">Influencer Videos: Unboxing the Mystique. (arXiv:2012.12311v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rajaram_P/0/1/0/all/0/1">Prashant Rajaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Manchanda_P/0/1/0/all/0/1">Puneet Manchanda</a></p>
<p>Influencer marketing has become a very popular tool to reach customers.
Despite the rapid growth in influencer videos, there has been little research
on the effectiveness of their constituent features in explaining video
engagement. We study YouTube influencers and analyze their unstructured video
data across text, audio and images using an "interpretable deep learning"
framework that accomplishes both goals of prediction and interpretation. Our
prediction-based approach analyzes unstructured data and finds that "what is
said" in words (text) is more influential than "how it is said" in imagery
(images) or acoustics (audio). Our novel interpretation-based approach is
implemented after completion of model prediction by analyzing the same source
of unstructured data to measure importance attributed to the video features. We
eliminate several spurious relationships in two steps, identifying a subset of
relationships which are confirmed using theory. We uncover novel findings that
establish distinct associations for measures of shallow and deep engagement
based on the dual-system framework of human thinking. Our approach is validated
using simulated data, and we discuss the learnings from our findings for
influencers and brands.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.05879">Feature Engineering with Regularity Structures. (arXiv:2108.05879v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chevyrev_I/0/1/0/all/0/1">Ilya Chevyrev</a>, <a href="http://arxiv.org/find/stat/1/au:+Gerasimovics_A/0/1/0/all/0/1">Andris Gerasimovics</a>, <a href="http://arxiv.org/find/stat/1/au:+Weber_H/0/1/0/all/0/1">Hendrik Weber</a></p>
<p>We investigate the use of models from the theory of regularity structures as
features in machine learning tasks. A model is a polynomial function of a
space-time signal designed to well-approximate solutions to partial
differential equations (PDEs), even in low regularity regimes. Models can be
seen as natural multi-dimensional generalisations of signatures of paths; our
work therefore aims to extend the recent use of signatures in data science
beyond the context of time-ordered data. We provide a flexible definition of a
model feature vector associated to a space-time signal, along with two
algorithms which illustrate ways in which these features can be combined with
linear regression. We apply these algorithms in several numerical experiments
designed to learn solutions to PDEs with a given forcing and boundary data. Our
experiments include semi-linear parabolic and wave equations with forcing, and
Burgers' equation with no forcing. We find an advantage in favour of our
algorithms when compared to several alternative methods. Additionally, in the
experiment with Burgers' equation, we find non-trivial predictive power when
noise is added to the observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.09076">An actor-critic algorithm with policy gradients to solve the job shop scheduling problem using deep double recurrent agents. (arXiv:2110.09076v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Monaci_M/0/1/0/all/0/1">Marta Monaci</a>, <a href="http://arxiv.org/find/math/1/au:+Agasucci_V/0/1/0/all/0/1">Valerio Agasucci</a>, <a href="http://arxiv.org/find/math/1/au:+Grani_G/0/1/0/all/0/1">Giorgio Grani</a></p>
<p>There is a growing interest in integrating machine learning techniques and
optimization to solve challenging optimization problems. In this work, we
propose a deep reinforcement learning methodology for the job shop scheduling
problem (JSSP). The aim is to build up a greedy-like heuristic able to learn on
some distribution of JSSP instances, different in the number of jobs and
machines. The need for fast scheduling methods is well known, and it arises in
many areas, from transportation to healthcare. We model the JSSP as a Markov
Decision Process and then we exploit the efficacy of reinforcement learning to
solve the problem. We adopt an actor-critic scheme, where the action taken by
the agent is influenced by policy considerations on the state-value function.
The procedures are adapted to take into account the challenging nature of JSSP,
where the state and the action space change not only for every instance but
also after each decision. To tackle the variability in the number of jobs and
operations in the input, we modeled the agent using two incident LSTM models, a
special type of deep neural network. Experiments show the algorithm reaches
good solutions in a short time, proving that is possible to generate new greedy
heuristics just from learning-based methodologies. Benchmarks have been
generated in comparison with the commercial solver CPLEX. As expected, the
model can generalize, to some extent, to larger problems or instances
originated by a different distribution from the one used in training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.00955">Source Free Unsupervised Graph Domain Adaptation. (arXiv:2112.00955v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haitao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Lun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yujia Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qiang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zelin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Graph Neural Networks (GNNs) have achieved great success on a variety of
tasks with graph-structural data, among which node classification is an
essential one. Unsupervised Graph Domain Adaptation (UGDA) shows its practical
value of reducing the labeling cost for node classification. It leverages
knowledge from a labeled graph (i.e., source domain) to tackle the same task on
another unlabeled graph (i.e., target domain). Most existing UGDA methods
heavily rely on the labeled graph in the source domain. They utilize labels
from the source domain as the supervision signal and are jointly trained on
both the source graph and the target graph. However, in some real-world
scenarios, the source graph is inaccessible because of privacy issues.
Therefore, we propose a novel scenario named Source Free Unsupervised Graph
Domain Adaptation (SFUGDA). In this scenario, the only information we can
leverage from the source domain is the well-trained source model, without any
exposure to the source graph and its labels. As a result, existing UGDA methods
are not feasible anymore. To address the non-trivial adaptation challenges in
this practical scenario, we propose a model-agnostic algorithm called SOGA for
domain adaptation to fully exploit the discriminative ability of the source
model while preserving the consistency of structural proximity on the target
graph. We prove the effectiveness of the proposed algorithm both theoretically
and empirically. The experimental results on four cross-domain tasks show
consistent improvements in the Macro-F1 score and Macro-AUC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.06054">Towards Data-Algorithm Dependent Generalization: a Case Study on Overparameterized Linear Regression. (arXiv:2202.06054v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1">Jiaye Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Chi-Chih Yao</a></p>
<p>One of the major open problems in machine learning is to characterize
generalization in the overparameterized regime, where most traditional
generalization bounds become inconsistent even for overparameterized linear
regression. In many scenarios, this failure can be attributed to obscuring the
crucial interplay between the training algorithm and the underlying data
distribution. This paper demonstrate that the generalization behavior of
overparameterized model should be analyzed in a both data-relevant and
algorithm-relevant manner. To make a formal characterization, We introduce a
notion called data-algorithm compatibility, which considers the generalization
behavior of the entire data-dependent training trajectory, instead of
traditional last-iterate analysis. We validate our claim by studying the
setting of solving overparameterized linear regression with gradient descent.
Specifically, we perform a data-dependent trajectory analysis and derive a
sufficient condition for compatibility in such a setting. Our theoretical
results demonstrate that if we take early stopping iterates into consideration,
generalization can hold with significantly weaker restrictions on the problem
instance than the previous last-iterate analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10793">PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs. (arXiv:2202.10793v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yixuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xitong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Junjie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1">Benedek Rozemberczki</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1">Mihai Cucuringu</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinert_G/0/1/0/all/0/1">Gesine Reinert</a></p>
<p>Networks are ubiquitous in many real-world applications (e.g., social
networks encoding trust/distrust relationships, correlation networks arising
from time series data). While many networks are signed or directed, or both,
there is a lack of unified software packages on graph neural networks (GNNs)
specially designed for signed and directed networks. In this paper, we present
PyTorch Geometric Signed Directed (PyGSD), a software package which fills this
gap. Along the way, we evaluate the implemented methods with experiments with a
view to providing insights into which method to choose for a given task. The
deep learning framework consists of easy-to-use GNN models, synthetic and
real-world data, as well as task-specific evaluation metrics and loss functions
for signed and directed networks. As an extension library for PyG, our proposed
software is maintained with open-source releases, detailed documentation,
continuous integration, unit tests and code coverage checks. The GitHub
repository of the library is
https://github.com/SherylHYX/pytorch_geometric_signed_directed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.04762">Rockafellian Relaxation and Stochastic Optimization under Perturbations. (arXiv:2204.04762v4 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1">Johannes O. Royset</a>, <a href="http://arxiv.org/find/math/1/au:+Chen_L/0/1/0/all/0/1">Louis L. Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Eckstrand_E/0/1/0/all/0/1">Eric Eckstrand</a></p>
<p>In practice, optimization models are often prone to unavoidable inaccuracies
due to dubious assumptions and corrupted data. Traditionally, this placed
special emphasis on risk-based and robust formulations, and their focus on
``conservative" decisions. We develop, in contrast, an ``optimistic" framework
based on Rockafellian relaxations in which optimization is conducted not only
over the original decision space but also jointly with a choice of model
perturbation. The framework enables us to address challenging problems with
ambiguous probability distributions from the areas of two-stage stochastic
optimization without relatively complete recourse, probability functions
lacking continuity properties, expectation constraints, and outlier analysis.
We are also able to circumvent the fundamental difficulty in stochastic
optimization that convergence of distributions fails to guarantee convergence
of expectations. The framework centers on the novel concepts of exact and
limit-exact Rockafellians, with interpretations of ``negative'' regularization
emerging in certain settings. We illustrate the role of Phi-divergence, examine
rates of convergence under changing distributions, and explore extensions to
first-order optimality conditions. The main development is free of assumptions
about convexity, smoothness, and even continuity of objective functions.
Numerical results in the setting of computer vision and text analytics with
label noise illustrate the framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.09157">Multifidelity Deep Operator Networks For Data-Driven and Physics-Informed Problems. (arXiv:2204.09157v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Howard_A/0/1/0/all/0/1">Amanda A. Howard</a>, <a href="http://arxiv.org/find/math/1/au:+Perego_M/0/1/0/all/0/1">Mauro Perego</a>, <a href="http://arxiv.org/find/math/1/au:+Karniadakis_G/0/1/0/all/0/1">George E. Karniadakis</a>, <a href="http://arxiv.org/find/math/1/au:+Stinis_P/0/1/0/all/0/1">Panos Stinis</a></p>
<p>Operator learning for complex nonlinear systems is increasingly common in
modeling multi-physics and multi-scale systems. However, training such
high-dimensional operators requires a large amount of expensive, high-fidelity
data, either from experiments or simulations. In this work, we present a
composite Deep Operator Network (DeepONet) for learning using two datasets with
different levels of fidelity to accurately learn complex operators when
sufficient high-fidelity data is not available. Additionally, we demonstrate
that the presence of low-fidelity data can improve the predictions of
physics-informed learning with DeepONets. We demonstrate the new multi-fidelity
training in diverse examples, including modeling of the ice-sheet dynamics of
the Humboldt glacier, Greenland, using two different fidelity models and also
using the same physical model at two different resolutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.02645">PyDaddy: A Python package for discovering stochastic dynamical equations from timeseries data. (arXiv:2205.02645v3 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Nabeel_A/0/1/0/all/0/1">Arshed Nabeel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Karichannavar_A/0/1/0/all/0/1">Ashwin Karichannavar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Palathingal_S/0/1/0/all/0/1">Shuaib Palathingal</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jhawar_J/0/1/0/all/0/1">Jitesh Jhawar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bruckner_D/0/1/0/all/0/1">David B. Br&#xfc;ckner</a>, <a href="http://arxiv.org/find/q-bio/1/au:+M%2E_D/0/1/0/all/0/1">Danny Raj M.</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Guttal_V/0/1/0/all/0/1">Vishwesha Guttal</a></p>
<p>Stochastic differential equations (SDEs) are an important framework to model
dynamics with randomness, as is common in most biological systems. The inverse
problem of integrating these models with empirical data remains a major
challenge. Here, we present a software package, PyDaDDy (Python Library for
Data Driven Dynamics) that takes time series data as an input and outputs an
interpretable SDE. We achieve this by combining traditional approaches from
stochastic calculus literature with state-of-the-art equation discovery
techniques. We validate our approach on synthetic datasets, and demonstrate the
generality and applicability of the method on two real-world datasets of vastly
different spatiotemporal scales: (i) collective movement of fish school where
stochasticity plays a crucial role, and (ii) confined migration of a single
cell, primarily following a relaxed oscillation. We make the method available
as an easy-to-use, open-source Python package, PyDaddy (Python Library for Data
Driven Dynamics).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.02956">Low Dimensional Invariant Embeddings for Universal Geometric Learning. (arXiv:2205.02956v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1">Nadav Dym</a>, <a href="http://arxiv.org/find/cs/1/au:+Gortler_S/0/1/0/all/0/1">Steven J. Gortler</a></p>
<p>This paper studies separating invariants: mappings on $D$ dimensional domains
which are invariant to an appropriate group action, and which separate orbits.
The motivation for this study comes from the usefulness of separating
invariants in proving universality of equivariant neural network architectures.
</p>
<p>We observe that in several cases the cardinality of separating invariants
proposed in the machine learning literature is much larger than the dimension
$D$. As a result, the theoretical universal constructions based on these
separating invariants is unrealistically large. Our goal in this paper is to
resolve this issue.
</p>
<p>We show that when a continuous family of semi-algebraic separating invariants
is available, separation can be obtained by randomly selecting $2D+1 $ of these
invariants. We apply this methodology to obtain an efficient scheme for
computing separating invariants for several classical group actions which have
been studied in the invariant learning literature. Examples include matrix
multiplication actions on point clouds by permutations, rotations, and various
other linear groups.
</p>
<p>Often the requirement of invariant separation is relaxed and only generic
separation is required. In this case, we show that only $D+1$ invariants are
required. More importantly, generic invariants are often significantly easier
to compute, as we illustrate by discussing generic and full separation for
weighted graphs. Finally we outline an approach for proving that separating
invariants can be constructed also when the random parameters have finite
precision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.10852">Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1">Zhen Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaozhuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1">Feiyu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.01255">Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v5 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wang_W/0/1/0/all/0/1">Weiqi Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Brugiapaglia_S/0/1/0/all/0/1">Simone Brugiapaglia</a></p>
<p>High-dimensional Partial Differential Equations (PDEs) are a popular
mathematical modelling tool, with applications ranging from finance to
computational chemistry. However, standard numerical techniques for solving
these PDEs are typically affected by the curse of dimensionality. In this work,
we tackle this challenge while focusing on stationary diffusion equations
defined over a high-dimensional domain with periodic boundary conditions.
Inspired by recent progress in sparse function approximation in high
dimensions, we propose a new method called compressive Fourier collocation.
Combining ideas from compressive sensing and spectral collocation, our method
replaces the use of structured collocation grids with Monte Carlo sampling and
employs sparse recovery techniques, such as orthogonal matching pursuit and
$\ell^1$ minimization, to approximate the Fourier coefficients of the PDE
solution. We conduct a rigorous theoretical analysis showing that the
approximation error of the proposed method is comparable with the best $s$-term
approximation (with respect to the Fourier basis) to the solution. Using the
recently introduced framework of random sampling in bounded Riesz systems, our
analysis shows that the compressive Fourier collocation method mitigates the
curse of dimensionality with respect to the number of collocation points under
sufficient conditions on the regularity of the diffusion coefficient. We also
present numerical experiments that illustrate the accuracy and stability of the
method for the approximation of sparse and compressible solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04763">Neural Bregman Divergences for Distance Learning. (arXiv:2206.04763v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1">Fred Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1">Edward Raff</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1">Francis Ferraro</a></p>
<p>Many metric learning tasks, such as triplet learning, nearest neighbor
retrieval, and visualization, are treated primarily as embedding tasks where
the ultimate metric is some variant of the Euclidean distance (e.g., cosine or
Mahalanobis), and the algorithm must learn to embed points into the pre-chosen
space. The study of non-Euclidean geometries is often not explored, which we
believe is due to a lack of tools for learning non-Euclidean measures of
distance. Recent work has shown that Bregman divergences can be learned from
data, opening a promising approach to learning asymmetric distances. We propose
a new approach to learning arbitrary Bergman divergences in a differentiable
manner via input convex neural networks and show that it overcomes significant
limitations of previous works. We also demonstrate that our method more
faithfully learns divergences over a set of both new and previously studied
tasks, including asymmetric regression, ranking, and clustering. Our tests
further extend to known asymmetric, but non-Bregman tasks, where our method
still performs competitively despite misspecification, showing the general
utility of our approach for asymmetric learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05077">Tensor Train for Global Optimization Problems in Robotics. (arXiv:2206.05077v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1">Suhan Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Lembono_T/0/1/0/all/0/1">Teguh Lembono</a>, <a href="http://arxiv.org/find/cs/1/au:+Loew_T/0/1/0/all/0/1">Tobias Loew</a>, <a href="http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1">Sylvain Calinon</a></p>
<p>The convergence of many numerical optimization techniques is highly dependent
on the initial guess given to the solver. To address this issue, we propose a
novel approach that utilizes tensor methods to initialize existing optimization
solvers near global optima. Our method does not require access to a database of
good solutions. We first transform the cost function, which depends on both
task parameters and optimization variables, into a probability density
function. Unlike existing approaches, the joint probability distribution of the
task parameters and optimization variables is approximated using the Tensor
Train model, which enables efficient conditioning and sampling. We treat the
task parameters as random variables, and for a given task, we generate samples
for decision variables from the conditional distribution to initialize the
optimization solver. Our method can produce multiple solutions (when they
exist) faster than existing methods. We first evaluate the approach on
benchmark functions for numerical optimization that are hard to solve using
gradient-based optimization solvers with a naive initialization. The results
show that the proposed method can generate samples close to global optima and
from multiple modes. We then demonstrate the generality and relevance of our
framework to robotics by applying it to inverse kinematics with obstacles and
motion planning problems with a 7-DoF manipulator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.04589">Long-term Causal Effects Estimation via Latent Surrogates Representation Learning. (arXiv:2208.04589v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1">Ruichu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zeqin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1">Shu Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoqing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiecheng Guo</a></p>
<p>Estimating long-term causal effects based on short-term surrogates is a
significant but challenging problem in many real-world applications, e.g.,
marketing and medicine. Despite its success in certain domains, most existing
methods estimate causal effects in an idealistic and simplistic way - ignoring
the causal structure among short-term outcomes and treating all of them as
surrogates. However, such methods cannot be well applied to real-world
scenarios, in which the partially observed surrogates are mixed with their
proxies among short-term outcomes. To this end, we develop our flexible method,
Laser, to estimate long-term causal effects in the more realistic situation
that the surrogates are observed or have observed proxies.Given the
indistinguishability between the surrogates and proxies, we utilize
identifiable variational auto-encoder (iVAE) to recover the whole valid
surrogates on all the surrogates candidates without the need of distinguishing
the observed surrogates or the proxies of latent surrogates. With the help of
the recovered surrogates, we further devise an unbiased estimation of long-term
causal effects. Extensive experimental results on the real-world and
semi-synthetic datasets demonstrate the effectiveness of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.13405">Interpreting Black-box Machine Learning Models for High Dimensional Datasets. (arXiv:2208.13405v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1">Md. Rezaul Karim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1">Md. Shajalal</a>, <a href="http://arxiv.org/find/cs/1/au:+Grass_A/0/1/0/all/0/1">Alex Gra&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohmen_T/0/1/0/all/0/1">Till D&#xf6;hmen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chala_S/0/1/0/all/0/1">Sisay Adugna Chala</a>, <a href="http://arxiv.org/find/cs/1/au:+Boden_A/0/1/0/all/0/1">Alexander Boden</a>, <a href="http://arxiv.org/find/cs/1/au:+Beecks_C/0/1/0/all/0/1">Christian Beecks</a>, <a href="http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1">Stefan Decker</a></p>
<p>Deep neural networks (DNNs) have been shown to outperform traditional machine
learning algorithms in a broad variety of application domains due to their
effectiveness in modeling complex problems and handling high-dimensional
datasets. Many real-life datasets, however, are of increasingly high
dimensionality, where a large number of features may be irrelevant for both
supervised and unsupervised learning tasks. The inclusion of such features
would not only introduce unwanted noise but also increase computational
complexity. Furthermore, due to high non-linearity and dependency among a large
number of features, DNN models tend to be unavoidably opaque and perceived as
black-box methods because of their not well-understood internal functioning.
Their algorithmic complexity is often simply beyond the capacities of humans to
understand the interplay among myriads of hyperparameters. A well-interpretable
model can identify statistically significant features and explain the way they
affect the model's outcome. In this paper, we propose an efficient method to
improve the interpretability of black-box models for classification tasks in
the case of high-dimensional datasets. First, we train a black-box model on a
high-dimensional dataset to learn the embeddings on which the classification is
performed. To decompose the inner working principles of the black-box model and
to identify top-k important features, we employ different probing and
perturbing techniques. We then approximate the behavior of the black-box model
by means of an interpretable surrogate model on the top-k feature space.
Finally, we derive decision rules and local explanations from the surrogate
model to explain individual decisions. Our approach outperforms
state-of-the-art methods like TabNet and XGboost when tested on different
datasets with varying dimensionality between 50 and 20,000 w.r.t metrics and
explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08660">Learn the Time to Learn: Replay Scheduling in Continual Learning. (arXiv:2209.08660v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klasson_M/0/1/0/all/0/1">Marcus Klasson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1">Hedvig Kjellstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a></p>
<p>Replay methods are known to be successful at mitigating catastrophic
forgetting in continual learning scenarios despite having limited access to
historical data. However, storing historical data is cheap in many real-world
settings, yet replaying all historical data is often prohibited due to
processing time constraints. In such settings, we propose that continual
learning systems should learn the time to learn and schedule which tasks to
replay at different time steps. We first demonstrate the benefits of our
proposal by using Monte Carlo tree search to find a proper replay schedule, and
show that the found replay schedules can outperform fixed scheduling policies
when combined with various replay methods in different continual learning
settings. Additionally, we propose a framework for learning replay scheduling
policies with reinforcement learning. We show that the learned policies can
generalize better in new continual learning scenarios compared to equally
replaying all seen tasks, without added computational cost. Our study reveals
the importance of learning the time to learn in continual learning, which
brings current research closer to real-world needs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13193">Langevin dynamics based algorithm e-TH$\varepsilon$O POULA for stochastic optimization problems with discontinuous stochastic gradient. (arXiv:2210.13193v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Lim_D/0/1/0/all/0/1">Dong-Young Lim</a>, <a href="http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1">Ariel Neufeld</a>, <a href="http://arxiv.org/find/math/1/au:+Sabanis_S/0/1/0/all/0/1">Sotirios Sabanis</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1">Ying Zhang</a></p>
<p>We introduce a new Langevin dynamics based algorithm, called
e-TH$\varepsilon$O POULA, to solve optimization problems with discontinuous
stochastic gradients which naturally appear in real-world applications such as
quantile estimation, vector quantization, CVaR minimization, and regularized
optimization problems involving ReLU neural networks. We demonstrate both
theoretically and numerically the applicability of the e-TH$\varepsilon$O POULA
algorithm. More precisely, under the conditions that the stochastic gradient is
locally Lipschitz in average and satisfies a certain convexity at infinity
condition, we establish non-asymptotic error bounds for e-TH$\varepsilon$O
POULA in Wasserstein distances and provide a non-asymptotic estimate for the
expected excess risk, which can be controlled to be arbitrarily small. Three
key applications in finance and insurance are provided, namely, multi-period
portfolio optimization, transfer learning in multi-period portfolio
optimization, and insurance claim prediction, which involve neural networks
with (Leaky)-ReLU activation functions. Numerical experiments conducted using
real-world datasets illustrate the superior empirical performance of
e-TH$\varepsilon$O POULA compared to SGLD, TUSLA, ADAM, and AMSGrad in terms of
model accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06617">Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v3 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Perlaza_S/0/1/0/all/0/1">Samir M. Perlaza</a>, <a href="http://arxiv.org/find/math/1/au:+Bisson_G/0/1/0/all/0/1">Gaetan Bisson</a>, <a href="http://arxiv.org/find/math/1/au:+Esnaola_I/0/1/0/all/0/1">I&#xf1;aki Esnaola</a>, <a href="http://arxiv.org/find/math/1/au:+Jean_Marie_A/0/1/0/all/0/1">Alain Jean-Marie</a>, <a href="http://arxiv.org/find/math/1/au:+Rini_S/0/1/0/all/0/1">Stefano Rini</a></p>
<p>The empirical risk minimization (ERM) problem with relative entropy
regularization (ERM-RER) is investigated under the assumption that the
reference measure is a {\sigma}-finite measure, and not necessarily a
probability measure. Under this assumption, which leads to a generalization of
the ERM-RER problem allowing a larger degree of flexibility for incorporating
prior knowledge, numerous relevant properties are stated. Among these
properties, the solution to this problem, if it exists, is shown to be a unique
probability measure, often mutually absolutely continuous with the reference
measure. Such a solution exhibits a probably-approximately-correct guarantee
for the ERM problem independently of whether the latter possesses a solution.
For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random
variable when the models are sampled from the solution to the ERM-RER problem.
The generalization capabilities of the solution to the ERM-RER problem (the
Gibbs algorithm) are studied via the sensitivity of the expected empirical risk
to deviations from such a solution towards alternative probability measures.
Finally, an interesting connection between sensitivity, generalization error,
and lautum information is established
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07931">Personalized Federated Learning with Multi-branch Architecture. (arXiv:2211.07931v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1">Junki Mori</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshiyama_T/0/1/0/all/0/1">Tomoyuki Yoshiyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryo_F/0/1/0/all/0/1">Furukawa Ryo</a>, <a href="http://arxiv.org/find/cs/1/au:+Teranishi_I/0/1/0/all/0/1">Isamu Teranishi</a></p>
<p>Federated learning (FL) is a decentralized machine learning technique that
enables multiple clients to collaboratively train models without requiring
clients to reveal their raw data to each other. Although traditional FL trains
a single global model with average performance among clients, statistical data
heterogeneity across clients has resulted in the development of personalized FL
(PFL), which trains personalized models with good performance on each client's
data. A key challenge with PFL is how to facilitate clients with similar data
to collaborate more in a situation where each client has data from complex
distribution and cannot determine one another's distribution. In this paper, we
propose a new PFL method (pFedMB) using multi-branch architecture, which
achieves personalization by splitting each layer of a neural network into
multiple branches and assigning client-specific weights to each branch. We also
design an aggregation method to improve the communication efficiency and the
model performance, with which each branch is globally updated with weighted
averaging by client-specific weights assigned to the branch. pFedMB is simple
but effective in facilitating each client to share knowledge with similar
clients by adjusting the weights assigned to each branch. We experimentally
show that pFedMB performs better than the state-of-the-art PFL methods using
the CIFAR10 and CIFAR100 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08942">Differentially Private Optimizers Can Learn Adversarially Robust Models. (arXiv:2211.08942v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1">Zhiqi Bu</a></p>
<p>Machine learning models have shone in a variety of domains and attracted
increasing attention from both the security and the privacy communities. One
important yet worrying question is: Will training models under the differential
privacy (DP) constraint have an unfavorable impact on their adversarial
robustness? While previous works have postulated that privacy comes at the cost
of worse robustness, we give the first theoretical analysis to show that DP
models can indeed be robust and accurate, even sometimes more robust than their
naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP
optimizers are critical; (2) pre-training on public data significantly
mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a
difference. With these factors set properly, we achieve 90\% natural accuracy,
72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$
attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with
pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with
$\epsilon=2$. In fact, we show both theoretically and empirically that DP
models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the
robustness of DP models is consistently observed across various datasets and
models. We believe our encouraging results are a significant step towards
training models that are private as well as robust.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.15513">Composite Score for Anomaly Detection in Imbalanced Real-World Industrial Dataset. (arXiv:2211.15513v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bougaham_A/0/1/0/all/0/1">Arnaud Bougaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Adoui_M/0/1/0/all/0/1">Mohammed El Adoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Linden_I/0/1/0/all/0/1">Isabelle Linden</a>, <a href="http://arxiv.org/find/cs/1/au:+Frenay_B/0/1/0/all/0/1">Beno&#xee;t Fr&#xe9;nay</a></p>
<p>In recent years, the industrial sector has evolved towards its fourth
revolution. The quality control domain is particularly interested in advanced
machine learning for computer vision anomaly detection. Nevertheless, several
challenges have to be faced, including imbalanced datasets, the image
complexity, and the zero-false-negative (ZFN) constraint to guarantee the
high-quality requirement. This paper illustrates a use case for an industrial
partner, where Printed Circuit Board Assembly (PCBA) images are first
reconstructed with a Vector Quantized Generative Adversarial Network (VQGAN)
trained on normal products. Then, several multi-level metrics are extracted on
a few normal and abnormal images, highlighting anomalies through reconstruction
differences. Finally, a classifer is trained to build a composite anomaly score
thanks to the metrics extracted. This three-step approach is performed on the
public MVTec-AD datasets and on the partner PCBA dataset, where it achieves a
regular accuracy of 95.69% and 87.93% under the ZFN constraint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02081">YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection. (arXiv:2212.02081v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zolfi_A/0/1/0/all/0/1">Alon Zolfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Amit_G/0/1/0/all/0/1">Guy Amit</a>, <a href="http://arxiv.org/find/cs/1/au:+Baras_A/0/1/0/all/0/1">Amit Baras</a>, <a href="http://arxiv.org/find/cs/1/au:+Koda_S/0/1/0/all/0/1">Satoru Koda</a>, <a href="http://arxiv.org/find/cs/1/au:+Morikawa_I/0/1/0/all/0/1">Ikuya Morikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1">Yuval Elovici</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1">Asaf Shabtai</a></p>
<p>Out-of-distribution (OOD) detection has attracted a large amount of attention
from the machine learning research community in recent years due to its
importance in deployed systems. Most of the previous studies focused on the
detection of OOD samples in the multi-class classification task. However, OOD
detection in the multi-label classification task, a more common real-world use
case, remains an underexplored domain. In this research, we propose YolOOD - a
method that utilizes concepts from the object detection domain to perform OOD
detection in the multi-label classification task. Object detection models have
an inherent ability to distinguish between objects of interest
(in-distribution) and irrelevant objects (e.g., OOD objects) in images that
contain multiple objects belonging to different class categories. These
abilities allow us to convert a regular object detection model into an image
classifier with inherent OOD detection capabilities with just minor changes. We
compare our approach to state-of-the-art OOD detection methods and demonstrate
YolOOD's ability to outperform these methods on a comprehensive suite of
in-distribution and OOD benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11899">Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers. (arXiv:2301.11899v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1">Shvetank Prakash</a>, <a href="http://arxiv.org/find/cs/1/au:+Stewart_M/0/1/0/all/0/1">Matthew Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1">Colby Banbury</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazumder_M/0/1/0/all/0/1">Mark Mazumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1">Pete Warden</a>, <a href="http://arxiv.org/find/cs/1/au:+Plancher_B/0/1/0/all/0/1">Brian Plancher</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1">Vijay Janapa Reddi</a></p>
<p>The sustained growth of carbon emissions and global waste elicits significant
sustainability concerns for our environment's future. The growing Internet of
Things (IoT) has the potential to exacerbate this issue. However, an emerging
area known as Tiny Machine Learning (TinyML) has the opportunity to help
address these environmental challenges through sustainable computing practices.
TinyML, the deployment of machine learning (ML) algorithms onto low-cost,
low-power microcontroller systems, enables on-device sensor analytics that
unlocks numerous always-on ML applications. This article discusses both the
potential of these TinyML applications to address critical sustainability
challenges, as well as the environmental footprint of this emerging technology.
Through a complete life cycle analysis (LCA), we find that TinyML systems
present opportunities to offset their carbon emissions by enabling applications
that reduce the emissions of other sectors. Nevertheless, when globally scaled,
the carbon footprint of TinyML systems is not negligible, necessitating that
designers factor in environmental impact when formulating new devices. Finally,
we outline research directions to enable further sustainable contributions of
TinyML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03868">A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation. (arXiv:2302.03868v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1">Adrian Celaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1">Beatrice Riviere</a>, <a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1">David Fuentes</a></p>
<p>Within medical imaging segmentation, the Dice coefficient and Hausdorff-based
metrics are standard measures of success for deep learning models. However,
modern loss functions for medical image segmentation often only consider the
Dice coefficient or similar region-based metrics during training. As a result,
segmentation architectures trained over such loss functions run the risk of
achieving high accuracy for the Dice coefficient but low accuracy for
Hausdorff-based metrics. Low accuracy on Hausdorff-based metrics can be
problematic for applications such as tumor segmentation, where such benchmarks
are crucial. For example, high Dice scores accompanied by significant Hausdorff
errors could indicate that the predictions fail to detect small tumors. We
propose the Generalized Surface Loss function, a novel loss function to
minimize Hausdorff-based metrics with more desirable numerical properties than
current methods and with weighting terms for class imbalance. Our loss function
outperforms other losses when tested on the LiTS and BraTS datasets using the
state-of-the-art nnUNet architecture. These results suggest we can improve
medical imaging segmentation accuracy with our novel loss function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04181">Attending to Graph Transformers. (arXiv:2302.04181v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1">Luis M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1">Mikhail Galkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1">Christopher Morris</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1">Ladislav Ramp&#xe1;&#x161;ek</a></p>
<p>Recently, transformer architectures for graphs emerged as an alternative to
established techniques for machine learning with graphs, such as
(message-passing) graph neural networks. So far, they have shown promising
empirical results, e.g., on molecular prediction datasets, often attributed to
their ability to circumvent graph neural networks' shortcomings, such as
over-smoothing and over-squashing. Here, we derive a taxonomy of graph
transformer architectures, bringing some order to this emerging field. We
overview their theoretical properties, survey structural and positional
encodings, and discuss extensions for important graph classes, e.g., 3D
molecular graphs. Empirically, we probe how well graph transformers can recover
various graph properties, how well they can deal with heterophilic graphs, and
to what extent they prevent over-squashing. Further, we outline open challenges
and research direction to stimulate future work. Our code is available at
https://github.com/luis-mueller/probing-graph-transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00970">PAPAL: A Provable PArticle-based Primal-Dual ALgorithm for Mixed Nash Equilibrium. (arXiv:2303.00970v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Ding_S/0/1/0/all/0/1">Shihong Ding</a>, <a href="http://arxiv.org/find/math/1/au:+Dong_H/0/1/0/all/0/1">Hanze Dong</a>, <a href="http://arxiv.org/find/math/1/au:+Fang_C/0/1/0/all/0/1">Cong Fang</a>, <a href="http://arxiv.org/find/math/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>We consider the non-convex non-concave objective function in two-player
zero-sum continuous games. The existence of pure Nash equilibrium requires
stringent conditions, posing a major challenge for this problem. To circumvent
this difficulty, we examine the problem of identifying a mixed Nash
equilibrium, where strategies are randomized and characterized by probability
distributions over continuous domains.To this end, we propose PArticle-based
Primal-dual ALgorithm (PAPAL) tailored for a weakly entropy-regularized min-max
optimization over probability distributions. This algorithm employs the
stochastic movements of particles to represent the updates of random strategies
for the $\epsilon$-mixed Nash equilibrium. We offer a comprehensive convergence
analysis of the proposed algorithm, demonstrating its effectiveness. In
contrast to prior research that attempted to update particle importance without
movements, PAPAL is the first implementable particle-based algorithm
accompanied by non-asymptotic quantitative convergence results, running time,
and sample complexity guarantees. Our framework contributes novel insights into
the particle-based algorithms for continuous min-max optimization in the
general non-convex non-concave setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07647">LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision. (arXiv:2304.07647v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiani Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_M/0/1/0/all/0/1">Mayur Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07927">A Randomized Approach for Tight Privacy Accounting. (arXiv:2304.07927v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1">Saeed Mahloujifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a></p>
<p>Bounding privacy leakage over compositions, i.e., privacy accounting, is a
key challenge in differential privacy (DP). The privacy parameter ($\eps$ or
$\delta$) is often easy to estimate but hard to bound. In this paper, we
propose a new differential privacy paradigm called estimate-verify-release
(EVR), which addresses the challenges of providing a strict upper bound for
privacy parameter in DP compositions by converting an estimate of privacy
parameter into a formal guarantee. The EVR paradigm first estimates the privacy
parameter of a mechanism, then verifies whether it meets this guarantee, and
finally releases the query output based on the verification result. The core
component of the EVR is privacy verification. We develop a randomized privacy
verifier using Monte Carlo (MC) technique. Furthermore, we propose an MC-based
DP accountant that outperforms existing DP accounting techniques in terms of
accuracy and efficiency. Our empirical evaluation shows the newly proposed EVR
paradigm improves the utility-privacy tradeoff for privacy-preserving machine
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07955">Heterogeneous Domain Adaptation with Positive and Unlabeled Data. (arXiv:2304.07955v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1">Junki Mori</a>, <a href="http://arxiv.org/find/cs/1/au:+Furukawa_R/0/1/0/all/0/1">Ryo Furukawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Teranishi_I/0/1/0/all/0/1">Isamu Teranishi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakuma_J/0/1/0/all/0/1">Jun Sakuma</a></p>
<p>Heterogeneous unsupervised domain adaptation (HUDA) is the most challenging
domain adaptation setting where the feature spaces of source and target domains
are heterogeneous, and the target domain has only unlabeled data. Existing HUDA
methods assume that both positive and negative examples are available in the
source domain, which may not be satisfied in some real applications. This paper
addresses a new challenging setting called positive and unlabeled heterogeneous
unsupervised domain adaptation (PU-HUDA), a HUDA setting where the source
domain only has positives. PU-HUDA can also be viewed as an extension of PU
learning where the positive and unlabeled examples are sampled from different
domains. A naive combination of existing HUDA and PU learning methods is
ineffective in PU-HUDA due to the gap in label distribution between the source
and target domains. To overcome this issue, we propose a novel method,
predictive adversarial domain adaptation (PADA), which can predict likely
positive examples from the unlabeled target data and simultaneously align the
feature spaces to reduce the distribution divergence between the whole source
data and the likely positive target data. PADA achieves this by a unified
adversarial training framework for learning a classifier to predict positive
examples and a feature transformer to transform the target feature space to
that of the source. Specifically, they are both trained to fool a common
discriminator that determines whether the likely positive examples are from the
target or source domain. We experimentally show that PADA outperforms several
baseline methods, such as the naive combination of HUDA and PU learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09355">To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shwartz_Ziv_R/0/1/0/all/0/1">Ravid Shwartz-Ziv</a>, <a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1">Yann LeCun</a></p>
<p>Deep neural networks excel in supervised learning tasks but are constrained
by the need for extensive labeled data. Self-supervised learning emerges as a
promising alternative, allowing models to learn without explicit labels.
Information theory, and notably the information bottleneck principle, has been
pivotal in shaping deep neural networks. This principle focuses on optimizing
the trade-off between compression and preserving relevant information,
providing a foundation for efficient network design in supervised contexts.
However, its precise role and adaptation in self-supervised learning remain
unclear. In this work, we scrutinize various self-supervised learning
approaches from an information-theoretic perspective, introducing a unified
framework that encapsulates the \textit{self-supervised information-theoretic
learning problem}. We weave together existing research into a cohesive
narrative, delve into contemporary self-supervised methodologies, and spotlight
potential research avenues and inherent challenges. Additionally, we discuss
the empirical evaluation of information-theoretic quantities and their
estimation methods. Overall, this paper furnishes an exhaustive review of the
intersection of information theory, self-supervised learning, and deep neural
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12023">Multi-channel Speech Separation Using Spatially Selective Deep Non-linear Filters. (arXiv:2304.12023v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tesch_K/0/1/0/all/0/1">Kristina Tesch</a>, <a href="http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1">Timo Gerkmann</a></p>
<p>In a multi-channel separation task with multiple speakers, we aim to recover
all individual speech signals from the mixture. In contrast to single-channel
approaches, which rely on the different spectro-temporal characteristics of the
speech signals, multi-channel approaches should additionally utilize the
different spatial locations of the sources for a more powerful separation
especially when the number of sources increases. To enhance the spatial
processing in a multi-channel source separation scenario, in this work, we
propose a deep neural network (DNN) based spatially selective filter (SSF) that
can be spatially steered to extract the speaker of interest by initializing a
recurrent neural network layer with the target direction. We compare the
proposed SSF with a common end-to-end direct separation (DS) approach trained
using utterance-wise permutation invariant training (PIT), which only
implicitly learns to perform spatial filtering. We show that the SSF has a
clear advantage over a DS approach with the same underlying network
architecture when there are more than two speakers in the mixture, which can be
attributed to a better use of the spatial information. Furthermore, we find
that the SSF generalizes much better to additional noise sources that were not
seen during training and to scenarios with speakers positioned at a similar
angle.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14922">Supervised and Unsupervised Deep Learning Approaches for EEG Seizure Prediction. (arXiv:2304.14922v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Georgis_Yap_Z/0/1/0/all/0/1">Zakary Georgis-Yap</a>, <a href="http://arxiv.org/find/eess/1/au:+Popovic_M/0/1/0/all/0/1">Milos R. Popovic</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1">Shehroz S. Khan</a></p>
<p>Epilepsy affects more than 50 million people worldwide, making it one of the
world's most prevalent neurological diseases. The main symptom of epilepsy is
seizures, which occur abruptly and can cause serious injury or death. The
ability to predict the occurrence of an epileptic seizure could alleviate many
risks and stresses people with epilepsy face. We formulate the problem of
detecting preictal (or pre-seizure) with reference to normal EEG as a precursor
to incoming seizure. To this end, we developed several supervised deep learning
approaches model to identify preictal EEG from normal EEG. We further develop
novel unsupervised deep learning approaches to train the models on only normal
EEG, and detecting pre-seizure EEG as an anomalous event. These deep learning
models were trained and evaluated on two large EEG seizure datasets in a
person-specific manner. We found that both supervised and unsupervised
approaches are feasible; however, their performance varies depending on the
patient, approach and architecture. This new line of research has the potential
to develop therapeutic interventions and save human lives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07251">Machine-learning-accelerated simulations to enable automatic surface reconstruction. (arXiv:2305.07251v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Du_X/0/1/0/all/0/1">Xiaochen Du</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Damewood_J/0/1/0/all/0/1">James K. Damewood</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lunger_J/0/1/0/all/0/1">Jaclyn R. Lunger</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Millan_R/0/1/0/all/0/1">Reisel Millan</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yildiz_B/0/1/0/all/0/1">Bilge Yildiz</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gomez_Bombarelli_R/0/1/0/all/0/1">Rafael G&#xf3;mez-Bombarelli</a></p>
<p>Understanding material surfaces and interfaces is vital in applications like
catalysis or electronics. By combining energies from electronic structure with
statistical mechanics, ab initio simulations can in principle predict the
structure of material surfaces as a function of thermodynamic variables.
However, accurate energy simulations are prohibitive when coupled to the vast
phase space that must be statistically sampled. Here, we present a bi-faceted
computational loop to predict surface phase diagrams of multi-component
materials that accelerates both the energy scoring and statistical sampling
methods. Fast, scalable, and data-efficient machine learning interatomic
potentials are trained on high-throughput density-functional theory
calculations through closed-loop active learning. Markov-chain Monte Carlo
sampling in the semi-grand canonical ensemble is enabled by using virtual
surface sites. The predicted surfaces for GaN(0001), Si(111), and SrTiO3(001)
are in agreement with past work and suggest that the proposed strategy can
model complex material surfaces and discover previously unreported surface
terminations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10229">How does Contrastive Learning Organize Images?. (arXiv:2305.10229v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1">Qi Xuan</a></p>
<p>Contrastive learning, a dominant self-supervised technique, emphasizes
similarity in representations between augmentations of the same input and
dissimilarity for different ones. Although low contrastive loss often
correlates with high classification accuracy, recent studies challenge this
direct relationship, spotlighting the crucial role of inductive biases. We
delve into these biases from a clustering viewpoint, noting that contrastive
learning creates locally dense clusters, contrasting the globally dense
clusters from supervised learning. To capture this discrepancy, we introduce
the "RLD (Relative Local Density)" metric. While this cluster property can
hinder linear classification accuracy, leveraging a Graph Convolutional Network
(GCN) based classifier mitigates this, boosting accuracy and reducing parameter
requirements. The code is available
\href{https://github.com/xsgxlz/How-does-Contrastive-Learning-Organize-Images/tree/main}{here}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10429">DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sang Michael Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hieu Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xuanyi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1">Nan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hanxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yifeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1">Adams Wei Yu</a></p>
<p>The mixture proportions of pretraining data domains (e.g., Wikipedia, books,
web text) greatly affect language model (LM) performance. In this paper, we
propose Domain Reweighting with Minimax Optimization (DoReMi), which first
trains a small proxy model using group distributionally robust optimization
(Group DRO) over domains to produce domain weights (mixture proportions)
without knowledge of downstream tasks. We then resample a dataset with these
domain weights and train a larger, full-sized model. In our experiments, we use
DoReMi on a 280M-parameter proxy model to set the domain weights for training
an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi
improves perplexity across all domains, even when it downweights a domain.
DoReMi improves average few-shot downstream accuracy by 6.5% points over a
baseline model trained using The Pile's default domain weights and reaches the
baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi,
which has no knowledge of downstream tasks, even matches the performance of
using domain weights tuned on downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11774">Multi-Objective Optimization Using the R2 Utility. (arXiv:2305.11774v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Tu_B/0/1/0/all/0/1">Ben Tu</a>, <a href="http://arxiv.org/find/math/1/au:+Kantas_N/0/1/0/all/0/1">Nikolas Kantas</a>, <a href="http://arxiv.org/find/math/1/au:+Lee_R/0/1/0/all/0/1">Robert M. Lee</a>, <a href="http://arxiv.org/find/math/1/au:+Shafei_B/0/1/0/all/0/1">Behrang Shafei</a></p>
<p>The goal of multi-objective optimization is to identify a collection of
points which describe the best possible trade-offs between the multiple
objectives. In order to solve this vector-valued optimization problem,
practitioners often appeal to the use of scalarization functions in order to
transform the multi-objective problem into a collection of single-objective
problems. This set of scalarized problems can then be solved using traditional
single-objective optimization techniques. In this work, we formalise this
convention into a general mathematical framework. We show how this strategy
effectively recasts the original multi-objective optimization problem into a
single-objective optimization problem defined over sets. An appropriate class
of objective functions for this new problem is the R2 utility function, which
is defined as a weighted integral over the scalarized optimization problems. We
show that this utility function is a monotone and submodular set function,
which can be optimised effectively using greedy optimization algorithms. We
analyse the performance of these greedy algorithms both theoretically and
empirically. Our analysis largely focusses on Bayesian optimization, which is a
popular probabilistic framework for black-box optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12827">Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1">Guillermo Ortiz-Jimenez</a>, <a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1">Alessandro Favero</a>, <a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1">Pascal Frossard</a></p>
<p>Task arithmetic has recently emerged as a cost-effective and scalable
approach to edit pre-trained models directly in weight space: By adding the
fine-tuned weights of different tasks, the model's performance can be improved
on these tasks, while negating them leads to task forgetting. Yet, our
understanding of the effectiveness of task arithmetic and its underlying
principles remains limited. We present a comprehensive study of task arithmetic
in vision-language models and show that weight disentanglement is the crucial
factor that makes it effective. This property arises during pre-training and
manifests when distinct directions in weight space govern separate, localized
regions in function space associated with the tasks. Notably, we show that
fine-tuning models in their tangent space by linearizing them amplifies weight
disentanglement. This leads to substantial performance improvements across
multiple task arithmetic benchmarks and diverse models. Building on these
findings, we provide theoretical and empirical analyses of the neural tangent
kernel (NTK) of these models and establish a compelling link between task
arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our
work uncovers novel insights into the fundamental mechanisms of task arithmetic
and offers a more reliable and effective approach to edit pre-trained models
through the NTK linearization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18183">On Counterfactual Data Augmentation Under Confounding. (arXiv:2305.18183v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1">Abbavaram Gowtham Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachu_S/0/1/0/all/0/1">Saketh Bachu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1">Saloni Dash</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1">Charchit Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Amit Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1">Vineeth N Balasubramanian</a></p>
<p>Counterfactual data augmentation has recently emerged as a method to mitigate
confounding biases in the training data. These biases, such as spurious
correlations, arise due to various observed and unobserved confounding
variables in the data generation process. In this paper, we formally analyze
how confounding biases impact downstream classifiers and present a causal
viewpoint to the solutions based on counterfactual data augmentation. We
explore how removing confounding biases serves as a means to learn invariant
features, ultimately aiding in generalization beyond the observed data
distribution. Additionally, we present a straightforward yet powerful algorithm
for generating counterfactual images, which effectively mitigates the influence
of confounding effects on downstream classifiers. Through experiments on MNIST
variants and the CelebA datasets, we demonstrate how our simple augmentation
method helps existing state-of-the-art methods achieve good results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03163">How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Isenko_A/0/1/0/all/0/1">Alexander Isenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayer_R/0/1/0/all/0/1">Ruben Mayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobsen_H/0/1/0/all/0/1">Hans-Arno Jacobsen</a></p>
<p>Training deep learning models in the cloud or on dedicated hardware is
expensive. A more cost-efficient option are hyperscale clouds offering spot
instances, a cheap but ephemeral alternative to on-demand resources. As spot
instance availability can change depending on the time of day, continent, and
cloud provider, it could be more cost-efficient to distribute resources over
the world. Still, it has not been investigated whether geo-distributed,
data-parallel spot deep learning training could be a more cost-efficient
alternative to centralized training.
</p>
<p>This paper aims to answer the question: Can deep learning models be
cost-efficiently trained on a global market of spot VMs spanning different data
centers and cloud providers? To provide guidance, we extensively evaluate the
cost and throughput implications of training in different zones, continents,
and clouds for representative CV, NLP and ASR models. To expand the current
training options further, we compare the scalability potential for hybrid-cloud
scenarios by adding cloud resources to on-premise hardware to improve training
throughput. Finally, we show how leveraging spot instance pricing enables a new
cost-efficient way to train models with multiple cheap VMs, trumping both more
centralized and powerful hardware and even on-demand cloud offerings at
competitive prices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05557">On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v4 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1">Donald Loveland</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heimann_M/0/1/0/all/0/1">Mark Heimann</a>, <a href="http://arxiv.org/find/cs/1/au:+Fish_B/0/1/0/all/0/1">Benjamin Fish</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1">Michael T. Schaub</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1">Danai Koutra</a></p>
<p>Graph Neural Network (GNN) research has highlighted a relationship between
high homophily (i.e., the tendency of nodes of the same class to connect) and
strong predictive performance in node classification. However, recent work has
found the relationship to be more nuanced, demonstrating that simple GNNs can
learn in certain heterophilous settings. To resolve these conflicting findings
and align closer to real-world datasets, we go beyond the assumption of a
global graph homophily level and study the performance of GNNs when the local
homophily level of a node deviates from the global homophily level. Through
theoretical and empirical analysis, we systematically demonstrate how shifts in
local homophily can introduce performance degradation, leading to performance
discrepancies across local homophily levels. We ground the practical
implications of this work through granular analysis on five real-world datasets
with varying global homophily levels, demonstrating that (a) GNNs can fail to
generalize to test nodes that deviate from the global homophily of a graph, and
(b) high local homophily does not necessarily confer high performance for a
node. We further show that GNNs designed for globally heterophilous graphs can
alleviate performance discrepancy by improving performance across local
homophily levels, offering a new perspective on how these GNNs achieve stronger
global performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16772">Learning from Synthetic Human Group Activities. (arXiv:2306.16772v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Che-Jui Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Danrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1">Deep Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1">Parth Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Honglu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seonghyeon Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1">Samuel S. Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sejong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1">Vladimir Pavlovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1">Mubbasir Kapadia</a></p>
<p>The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by the Unity engine, M3Act
features multiple semantic groups, highly diverse and photorealistic images,
and a comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments using various input modalities. First, adding our synthetic data
significantly improves the performance of MOTRv2 on DanceTrack, leading to a
hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking
results on par with MOTRv2*, which is trained with 62.5% more real-world data.
Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on
group activity and atomic action accuracy respectively. Moreover, M3Act opens
new research for controllable 3D group activity generation. We define multiple
metrics and propose a competitive baseline for the novel task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17844">The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. (arXiv:2306.17844v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Ziqian Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1">Max Tegmark</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a></p>
<p>Do neural networks, trained on well-understood algorithmic tasks, reliably
rediscover known algorithms for solving those tasks? Several recent studies, on
tasks ranging from group arithmetic to in-context linear regression, have
suggested that the answer is yes. Using modular addition as a prototypical
problem, we show that algorithm discovery in neural networks is sometimes more
complex. Small changes to model hyperparameters and initializations can induce
the discovery of qualitatively different algorithms from a fixed training set,
and even parallel implementations of multiple such algorithms. Some networks
trained to perform modular addition implement a familiar Clock algorithm;
others implement a previously undescribed, less intuitive, but comprehensible
procedure which we term the Pizza algorithm, or a variety of even more complex
procedures. Our results show that even simple learning problems can admit a
surprising diversity of solutions, motivating the development of new tools for
characterizing the behavior of neural networks across their algorithmic phase
space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00337">Recursive Algorithmic Reasoning. (arXiv:2307.00337v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jurss_J/0/1/0/all/0/1">Jonas J&#xfc;r&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayalath_D/0/1/0/all/0/1">Dulhan Jayalath</a>, <a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1">Petar Veli&#x10d;kovi&#x107;</a></p>
<p>Learning models that execute algorithms can enable us to address a key
problem in deep learning: generalizing to out-of-distribution data. However,
neural networks are currently unable to execute recursive algorithms because
they do not have arbitrarily large memory to store and recall state. To address
this, we (1) propose a way to augment graph neural networks (GNNs) with a
stack, and (2) develop an approach for capturing intermediate algorithm
trajectories that improves algorithmic alignment with recursive algorithms over
previous methods. The stack allows the network to learn to store and recall a
portion of the state of the network at a particular time, analogous to the
action of a call stack in a recursive algorithm. This augmentation permits the
network to reason recursively. We empirically demonstrate that our proposals
significantly improve generalization to larger input graphs over prior work on
depth-first search (DFS).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01452">Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhihong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">Guodong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengqi Zhang</a></p>
<p>Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07081">Kernel t-distributed stochastic neighbor embedding. (arXiv:2307.07081v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1">Denis C. Ilie-Ablachim</a>, <a href="http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1">Bogdan Dumitrescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1">Cristian Rusu</a></p>
<p>This paper presents a kernelized version of the t-SNE algorithm, capable of
mapping high-dimensional data to a low-dimensional space while preserving the
pairwise distances between the data points in a non-Euclidean metric. This can
be achieved using a kernel trick only in the high dimensional space or in both
spaces, leading to an end-to-end kernelized version. The proposed kernelized
version of the t-SNE algorithm can offer new views on the relationships between
data points, which can improve performance and accuracy in particular
applications, such as classification problems involving kernel methods. The
differences between t-SNE and its kernelized version are illustrated for
several datasets, showing a neater clustering of points belonging to different
classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10865">Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Girrbach_L/0/1/0/all/0/1">Leander Girrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1">Anders Christensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a>, <a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1">Zeynep Akata</a>, <a href="http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1">A. Sophia Koepke</a></p>
<p>Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11494">Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1">Marcel Kollovieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1">Abdul Fatir Ansari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohlke_Schneider_M/0/1/0/all/0/1">Michael Bohlke-Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Zschiegner_J/0/1/0/all/0/1">Jasper Zschiegner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a></p>
<p>Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally-trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16189">Stable Adam Optimization for 16-bit Neural Networks Training. (arXiv:2307.16189v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1">Juyoung Yun</a></p>
<p>In this research, we address critical concerns related to the numerical
instability observed in 16-bit computations of machine learning models. Such
instability, particularly when employing popular optimization algorithms like
Adam, often leads to unstable training of deep neural networks. This not only
disrupts the learning process but also poses significant challenges in
deploying dependable models in real-world applications. Our investigation
identifies the epsilon hyperparameter as the primary source of this
instability. A nuanced exploration reveals that subtle adjustments to epsilon
within 16-bit computations can enhance the numerical stability of Adam,
enabling more stable training of 16-bit neural networks. We propose a novel,
dependable approach that leverages updates from the Adam optimizer to bolster
the stability of the learning process. Our contributions provide deeper
insights into optimization challenges in low-precision computations and offer
solutions to ensure the stability of deep neural network training, paving the
way for their dependable use in various applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01981">CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1">Yongcheng Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1">Junru Zhong</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Liping Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1">Sheheryar Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1">Weitian Chen</a></p>
<p>We introduce CartiMorph, a framework for automated knee articular cartilage
morphometrics. It takes an image as input and generates quantitative metrics
for cartilage subregions, including the percentage of full-thickness cartilage
loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the
power of deep learning models for hierarchical image feature representation.
Deep learning models were trained and validated for tissue segmentation,
template construction, and template-to-image registration. We established
methods for surface-normal-based cartilage thickness mapping, FCL estimation,
and rule-based cartilage parcellation. Our cartilage thickness map showed less
error in thin and peripheral regions. We evaluated the effectiveness of the
adopted segmentation model by comparing the quantitative metrics obtained from
model segmentation and those from manual segmentation. The root-mean-squared
deviation of the FCL measurements was less than 8%, and strong correlations
were observed for the mean thickness (Pearson's correlation coefficient $\rho
\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in
[0.89,0.98]$) measurements. We compared our FCL measurements with those from a
previous study and found that our measurements deviated less from the ground
truths. We observed superior performance of the proposed rule-based cartilage
parcellation method compared with the atlas-based approach. CartiMorph has the
potential to promote imaging biomarkers discovery for knee osteoarthritis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07121">Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers. (arXiv:2308.07121v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rauch_L/0/1/0/all/0/1">Lukas Rauch</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwinger_R/0/1/0/all/0/1">Raphael Schwinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Wirth_M/0/1/0/all/0/1">Moritz Wirth</a>, <a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1">Bernhard Sick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomforde_S/0/1/0/all/0/1">Sven Tomforde</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholz_C/0/1/0/all/0/1">Christoph Scholz</a></p>
<p>We propose a shift towards end-to-end learning in bird sound monitoring by
combining self-supervised (SSL) and deep active learning (DAL). Leveraging
transformer models, we aim to bypass traditional spectrogram conversions,
enabling direct raw audio processing. ActiveBird2Vec is set to generate
high-quality bird sound representations through SSL, potentially accelerating
the assessment of environmental changes and decision-making processes for wind
farms. Additionally, we seek to utilize the wide variety of bird vocalizations
through DAL, reducing the reliance on extensively labeled datasets by human
experts. We plan to curate a comprehensive set of tasks through Huggingface
Datasets, enhancing future comparability and reproducibility of bioacoustic
research. A comparative analysis between various transformer models will be
conducted to evaluate their proficiency in bird sound recognition tasks. We aim
to accelerate the progression of avian bioacoustic research and contribute to
more effective conservation strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16113">survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spytek_M/0/1/0/all/0/1">Miko&#x142;aj Spytek</a>, <a href="http://arxiv.org/find/cs/1/au:+Krzyzinski_M/0/1/0/all/0/1">Mateusz Krzyzi&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Langbein_S/0/1/0/all/0/1">Sophie Hanna Langbein</a>, <a href="http://arxiv.org/find/cs/1/au:+Baniecki_H/0/1/0/all/0/1">Hubert Baniecki</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_M/0/1/0/all/0/1">Marvin N. Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1">Przemys&#x142;aw Biecek</a></p>
<p>Due to their flexibility and superior performance, machine learning models
frequently complement and outperform traditional statistical survival models.
However, their widespread adoption is hindered by a lack of user-friendly tools
to explain their internal operations and prediction rationales. To tackle this
issue, we introduce the survex R package, which provides a cohesive framework
for explaining any survival model by applying explainable artificial
intelligence techniques. The capabilities of the proposed software encompass
understanding and diagnosing survival models, which can lead to their
improvement. By revealing insights into the decision-making process, such as
variable effects and importances, survex enables the assessment of model
reliability and the detection of biases. Thus, transparency and responsibility
may be promoted in sensitive areas, such as biomedical research and healthcare
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00018">Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodrigues_C/0/1/0/all/0/1">Caroline Mazini Rodrigues</a> (LIGM, LRDE), <a href="http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1">Nicolas Boutry</a> (LRDE), <a href="http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1">Laurent Najman</a> (LIGM)</p>
<p>Providing interpretability of deep-learning models to non-experts, while
fundamental for a responsible real-world usage, is challenging. Attribution
maps from xAI techniques, such as Integrated Gradients, are a typical example
of a visualization technique containing a high level of information, but with
difficult interpretation. In this paper, we propose two methods, Maximum
Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization
(Ms-IV), to explain the model's decision, enhancing global interpretability.
MAGE finds, for a given CNN, combinations of features which, globally, form a
semantic meaning, that we call concepts. We group these similar feature
patterns by clustering in ``concepts'', that we visualize through Ms-IV. This
last method is inspired by Occlusion and Sensitivity analysis (incorporating
causality), and uses a novel metric, called Class-aware Order Correlation
(CaOC), to globally evaluate the most important image regions according to the
model's decision space. We compare our approach to xAI methods such as LIME and
Integrated Gradients. Experimental results evince the Ms-IV higher localization
and faithfulness values. Finally, qualitative evaluation of combined MAGE and
Ms-IV demonstrates humans' ability to agree, based on the visualization, with
the decision of clusters' concepts; and, to detect, among a given set of
networks, the existence of bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06255">Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yake Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1">Ruoxuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Di Hu</a></p>
<p>One primary topic of multi-modal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multi-modal cooperation, which could not jointly
utilize all modalities well. Some methods are proposed to identify and enhance
the worse learnt modality, but are often hard to provide the fine-grained
observation of multi-modal cooperation at sample-level with theoretical
support. Hence, it is essential to reasonably observe and improve the
fine-grained cooperation between modalities, especially when facing realistic
scenarios where the modality discrepancy could vary across different samples.
To this end, we introduce a fine-grained modality valuation metric to evaluate
the contribution of each modality at sample-level. Via modality valuation, we
regretfully observe that the multi-modal model tends to rely on one specific
modality, resulting in other modalities being low-contributing. We further
analyze this issue and improve cooperation between modalities by enhancing the
discriminative ability of low-contributing modalities in a targeted manner.
Overall, our methods reasonably observe the fine-grained uni-modal contribution
at sample-level and achieve considerable improvement on different multi-modal
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13108">Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Jumade_R/0/1/0/all/0/1">Raghav Jumade</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sawaya_N/0/1/0/all/0/1">Nicolas PD Sawaya</a></p>
<p>Though there has been substantial progress in developing quantum algorithms
to study classical datasets, the cost of simply loading classical data is an
obstacle to quantum advantage. When the amplitude encoding is used, loading an
arbitrary classical vector requires up to exponential circuit depths with
respect to the number of qubits. Here, we address this "input problem" with two
contributions. First, we introduce a circuit compilation method based on tensor
network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader
Exploiting TNs) -- proceeds via careful construction of a specific TN topology
and can be tailored to arbitrary circuit depths. Second, we perform numerical
experiments on real-world classical data from four distinct areas: finance,
images, fluid mechanics, and proteins. To the best of our knowledge, this is
the broadest numerical analysis to date of loading classical data into a
quantum computer. Consistent with other recent work in this area, the required
circuit depths are often several orders of magnitude lower than the
exponentially-scaling general loading algorithm would require. Besides
introducing a more efficient loading algorithm, this work demonstrates that
many classical datasets are loadable in depths that are much shorter than
previously expected, which has positive implications for speeding up classical
workloads on quantum computers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01959">Beyond Labeling Oracles: What does it mean to steal ML models?. (arXiv:2310.01959v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shafran_A/0/1/0/all/0/1">Avital Shafran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1">Ilia Shumailov</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1">Murat A. Erdogdu</a>, <a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1">Nicolas Papernot</a></p>
<p>Model extraction attacks are designed to steal trained models with only query
access, as is often provided through APIs that ML-as-a-Service providers offer.
ML models are expensive to train, in part because data is hard to obtain, and a
primary incentive for model extraction is to acquire a model while incurring
less cost than training from scratch. Literature on model extraction commonly
claims or presumes that the attacker is able to save on both data acquisition
and labeling costs. We show that the attacker often does not. This is because
current attacks implicitly rely on the adversary being able to sample from the
victim model's data distribution. We thoroughly evaluate factors influencing
the success of model extraction. We discover that prior knowledge of the
attacker, i.e. access to in-distribution data, dominates other factors like the
attack policy the adversary follows to choose which queries to make to the
victim model API. Thus, an adversary looking to develop an equally capable
model with a fixed budget has little practical incentive to perform model
extraction, since for the attack to work they need to collect in-distribution
data, saving only on the cost of labeling. With low labeling costs in the
current market, the usefulness of such attacks is questionable. Ultimately, we
demonstrate that the effect of prior knowledge needs to be explicitly decoupled
from the attack policy. To this end, we propose a benchmark to evaluate attack
policy directly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02129">Unveiling the Pitfalls of Knowledge Editing for Large Language Models. (arXiv:2310.02129v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhoubo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yunzhi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code is available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02168">Editing Personality for LLMs. (arXiv:2310.02168v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shengyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yunzhi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pengjun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models'
responses to opinion-related questions on specified topics since an
individual's personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08051">LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jianchao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuzhe Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiaqi Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1">Quan Z. Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xi Zheng</a></p>
<p>Brain-Computer Interfaces (BCIs) are a groundbreaking technology for
interacting with external devices using brain signals. Despite advancements,
electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like
amplitude and phase variability, and complex spatial correlations, with a need
for smaller model size and faster inference. This study introduces the LGL-BCI
framework, employing a Geometric Deep Learning Framework for EEG processing in
non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD)
Manifold space. LGL-BCI offers robust EEG data representation and captures
spatial correlations. We propose an EEG channel selection solution via a
feature decomposition algorithm to reduce SPD matrix dimensionality, with a
lossless transformation boosting inference speed. Extensive experiments show
LGL-BCI's superior accuracy and efficiency compared to current solutions,
highlighting geometric deep learning's potential in MI-BCI applications. The
efficiency, assessed on two public EEG datasets and two real-world EEG devices,
significantly outperforms the state-of-the-art solution in accuracy ($82.54\%$
versus $62.22\%$) with fewer parameters (64.9M compared to 183.7M).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08897">Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Jina Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1">Youngtaek Hong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1">Dawun Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1">Yeonggul Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_S/0/1/0/all/0/1">Sihyeon Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_T/0/1/0/all/0/1">Taekgeun Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1">Yeonyee E. Yoon</a>, <a href="http://arxiv.org/find/eess/1/au:+Moon_I/0/1/0/all/0/1">Inki Moon</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seung-Ah Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1">Hyuk-Jae Chang</a></p>
<p>Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09299">Digital Twin Assisted Deep Reinforcement Learning for Online Admission Control in Sliced Network. (arXiv:2310.09299v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1">Zhenyu Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1">Xiaohu You</a></p>
<p>The proliferation of diverse wireless services in 5G and beyond has led to
the emergence of network slicing technologies. Among these, admission control
plays a crucial role in achieving service-oriented optimization goals through
the selective acceptance of service requests. Although deep reinforcement
learning (DRL) forms the foundation in many admission control approaches thanks
to its effectiveness and flexibility, initial instability with excessive
convergence delay of DRL models hinders their deployment in real-world
networks. We propose a digital twin (DT) accelerated DRL solution to address
this issue. Specifically, we first formulate the admission decision-making
process as a semi-Markov decision process, which is subsequently simplified
into an equivalent discrete-time Markov decision process to facilitate the
implementation of DRL methods. A neural network-based DT is established with a
customized output layer for queuing systems, trained through supervised
learning, and then employed to assist the training phase of the DRL model.
Extensive simulations show that the DT-accelerated DRL improves resource
utilization by over 40% compared to the directly trained state-of-the-art
dueling deep Q-learning model. This improvement is achieved while preserving
the model's capability to optimize the long-term rewards of the admission
process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10837">Approximating Two-Layer Feedforward Networks for Efficient Transformers. (arXiv:2310.10837v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1">R&#xf3;bert Csord&#xe1;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1">Kazuki Irie</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a></p>
<p>How to reduce compute and memory requirements of neural networks (NNs)
without sacrificing performance? Many recent works use sparse Mixtures of
Experts (MoEs) to build resource-efficient large language models (LMs). Here we
introduce several novel perspectives on MoEs, presenting a general framework
that unifies various methods to approximate two-layer NNs (e.g., feedforward
blocks of Transformers), including product-key memories (PKMs). Leveraging
insights from this framework, we propose methods to improve both MoEs and PKMs.
Unlike prior work that compares MoEs with dense baselines under the
compute-equal condition, our evaluation condition is parameter-equal, which is
crucial to properly evaluate LMs. We show that our MoEs are competitive with
the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two
different scales, while being much more resource efficient. This demonstrates
that MoEs are relevant not only to extremely large LMs but also to any-scale
resource-efficient LMs. Our code is public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13019">Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Labib_S/0/1/0/all/0/1">S. M. Fazle Rabby Labib</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1">Joyanta Jyoti Mondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Manab_M/0/1/0/all/0/1">Meem Arafat Manab</a></p>
<p>Deep neural networks (DNNs) have significantly advanced various domains, but
their vulnerability to adversarial attacks poses serious concerns.
Understanding these vulnerabilities and developing effective defense mechanisms
is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016),
finds minimal perturbations to misclassify input images. However, DeepFool
lacks a targeted approach, making it less effective in specific attack
scenarios. Also, in previous related works, researchers primarily focus on
success, not considering how much an image is getting distorted; the integrity
of the image quality, and the confidence level to misclassifying. So, in this
paper, we propose Enhanced Targeted DeepFool, an augmented version of DeepFool
that allows targeting specific classes for misclassification and also introduce
a minimum confidence score requirement hyperparameter to enhance flexibility.
Our experiments demonstrate the effectiveness and efficiency of the proposed
method across different deep neural network architectures while preserving
image integrity as much and perturbation rate as less as possible. By using our
approach, the behavior of models can be manipulated arbitrarily using the
perturbed images, as we can specify both the target class and the associated
confidence score, unlike other DeepFool-derivative works, such as Targeted
DeepFool by Gajjar et al. (2022). Results show that one of the deep
convolutional neural network architectures, AlexNet, and one of the
state-of-the-art model Vision Transformer exhibit high robustness to getting
fooled. This approach can have larger implication, as our tuning of confidence
level can expose the robustness of image recognition models. Our code will be
made public upon acceptance of the paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13230">Absolute Policy Optimization. (arXiv:2310.13230v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weiye Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feihan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Rui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tianhao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changliu Liu</a></p>
<p>In recent years, trust region on-policy reinforcement learning has achieved
impressive results in addressing complex control tasks and gaming scenarios.
However, contemporary state-of-the-art algorithms within this category
primarily emphasize improvement in expected performance, lacking the ability to
control over the worst-case performance outcomes. To address this limitation,
we introduce a novel objective function; by optimizing which, it will lead to
guaranteed monotonic improvement in the lower bound of near-total performance
samples (absolute performance). Considering this groundbreaking theoretical
advancement, we then refine this theoretically grounded algorithm through a
series of approximations, resulting in a practical solution called Absolute
Policy Optimization (APO). Our experiments demonstrate the effectiveness of our
approach across challenging continuous control benchmark tasks and extend its
applicability to mastering Atari games. Our findings reveal that APO
significantly outperforms state-of-the-art policy gradient algorithms,
resulting in substantial improvements in both expected performance and
worst-case performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17355">Exploring the Trie of Rules: a fast data structure for the representation of association rules. (arXiv:2310.17355v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kudriavtsev_M/0/1/0/all/0/1">Mikhail Kudriavtsev</a>, <a href="http://arxiv.org/find/cs/1/au:+Bezbradica_M/0/1/0/all/0/1">Marija Bezbradica</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarren_A/0/1/0/all/0/1">Andrew McCarren</a></p>
<p>Association rule mining techniques can generate a large volume of sequential
data when implemented on transactional databases. Extracting insights from a
large set of association rules has been found to be a challenging process. When
examining a ruleset, the fundamental question is how to summarise and represent
meaningful mined knowledge efficiently. Many algorithms and strategies have
been developed to address issue of knowledge extraction; however, the
effectiveness of this process can be limited by the data structures. A better
data structure can sufficiently affect the speed of the knowledge extraction
process. This paper proposes a novel data structure, called the Trie of rules,
for storing a ruleset that is generated by association rule mining. The
resulting data structure is a prefix-tree graph structure made of pre-mined
rules. This graph stores the rules as paths within the prefix-tree in a way
that similar rules overlay each other. Each node in the tree represents a rule
where a consequent is this node, and an antecedent is a path from this node to
the root of the tree. The evaluation showed that the proposed representation
technique is promising. It compresses a ruleset with almost no data loss and
benefits in terms of time for basic operations such as searching for a specific
rule and sorting, which is the base for many knowledge discovery methods.
Moreover, our method demonstrated a significant improvement in traversing time,
achieving an 8-fold increase compared to traditional data structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18168">Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1">Nitish Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1">Javier Rando</a>, <a href="http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1">Abulhair Saparov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1">Najoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a></p>
<p>Large Language Models (LLMs) are trained on vast amounts of text from the
internet, which contains both factual and misleading information about the
world. Can language models discern truth from falsehood in this contradicting
data? Expanding on the view that LLMs can model different communicative agents,
we present the persona hypothesis: LLMs can cluster agents into personas using
common features of their generations. For instance, a truthful persona is a
group of agents that are likely to produce truthful text and that share similar
features like formal writing styles and scientific references. By modeling this
persona, LLMs can generalize truthfulness beyond the specific contexts in which
each agent generated the training text. For example, the model can infer that
the agent ``Wikipedia'' will behave truthfully on topics that were only
generated by ``Science'' because they both belong to the truthful persona. We
show evidence for the persona hypothesis via two observations: (1) we can probe
whether a model's answer will be truthful before it is generated; (2)
finetuning a model on a set of facts improves its truthfulness on unseen
topics. Next, using arithmetics as a synthetic environment, we show that
language models can separate true and false statements, and generalize
truthfulness across agents; but only if agents in the training data share a
truthful generative process that enables the creation of a truthful persona.
Overall, our findings suggest that models can exploit hierarchical structures
in the data to learn abstract concepts like truthfulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19805">Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1">Xiao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>Offline-to-online RL can make full use of pre-collected offline datasets to
initialize policies, resulting in higher sample efficiency and better
performance compared to only using online algorithms alone for policy training.
However, direct fine-tuning of the pre-trained policy tends to result in
sub-optimal performance. A primary reason is that conservative offline RL
methods diminish the agent's capability of exploration, thereby impacting
online fine-tuning performance. To encourage agent's exploration during online
fine-tuning and enhance the overall online fine-tuning performance, we propose
a generalized reward augmentation method called Sample Efficient Reward
Augmentation (SERA). Specifically, SERA encourages agent to explore by
computing Q conditioned entropy as intrinsic reward. The advantage of SERA is
that it can extensively utilize offline pre-trained Q to encourage agent
uniformly coverage of state space while considering the imbalance between the
distributions of high-value and low-value states. Additionally, SERA can be
effortlessly plugged into various RL algorithms to improve online fine-tuning
and ensure sustained asymptotic improvement. Moreover, extensive experimental
results demonstrate that when conducting offline-to-online problems, SERA
consistently and effectively enhances the performance of various offline
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20567">One-shot backpropagation for multi-step prediction in physics-based system identification -- EXTENDED VERSION. (arXiv:2310.20567v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Donati_C/0/1/0/all/0/1">Cesare Donati</a>, <a href="http://arxiv.org/find/eess/1/au:+Mammarella_M/0/1/0/all/0/1">Martina Mammarella</a>, <a href="http://arxiv.org/find/eess/1/au:+Dabbene_F/0/1/0/all/0/1">Fabrizio Dabbene</a>, <a href="http://arxiv.org/find/eess/1/au:+Novara_C/0/1/0/all/0/1">Carlo Novara</a>, <a href="http://arxiv.org/find/eess/1/au:+Lagoa_C/0/1/0/all/0/1">Constantino Lagoa</a></p>
<p>The aim of this paper is to present a novel physics-based framework for the
identification of dynamical systems, in which the physical and structural
insights are reflected directly into a backpropagation-based learning
algorithm. The main result is a method to compute in closed form the gradient
of a multi-step loss function, while enforcing physical properties and
constraints. The derived algorithm has been exploited to identify the unknown
inertia matrix of a space debris, and the results show the reliability of the
method in capturing the physical adherence of the estimated parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01038">Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks. (arXiv:2311.01038v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiarong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Renhong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuxuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Carl Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a></p>
<p>Pre-training on graph neural networks (GNNs) aims to learn transferable
knowledge for downstream tasks with unlabeled data, and it has recently become
an active research area. The success of graph pre-training models is often
attributed to the massive amount of input data. In this paper, however, we
identify the curse of big data phenomenon in graph pre-training: more training
data do not necessarily lead to better downstream performance. Motivated by
this observation, we propose a better-with-less framework for graph
pre-training: fewer, but carefully chosen data are fed into a GNN model to
enhance pre-training. The proposed pre-training pipeline is called the
data-active graph pre-training (APT) framework, and is composed of a graph
selector and a pre-training model. The graph selector chooses the most
representative and instructive data points based on the inherent properties of
graphs as well as predictive uncertainty. The proposed predictive uncertainty,
as feedback from the pre-training model, measures the confidence level of the
model in the data. When fed with the chosen data, on the other hand, the
pre-training model grasps an initial understanding of the new, unseen data, and
at the same time attempts to remember the knowledge learned from previous data.
Therefore, the integration and interaction between these two components form a
unified framework (APT), in which graph pre-training is performed in a
progressive and iterative way. Experiment results show that the proposed APT is
able to obtain an efficient pre-training model with fewer training data and
better downstream performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01331">Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1">Alexander G. Schwing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-xiong Wang</a></p>
<p>In real-world scenarios, arbitrary interactions with the environment can
often be costly, and actions of expert demonstrations are not always available.
To reduce the need for both, Offline Learning from Observations (LfO) is
extensively studied, where the agent learns to solve a task with only expert
states and \textit{task-agnostic} non-expert state-action pairs. The
state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the
state occupancy divergence between the learner and expert policies. However,
they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein
distance with Rubinstein duality, the latter of which constrains the underlying
distance metric crucial to the performance of Wasserstein-based solutions. To
address this problem, we propose Primal Wasserstein DICE (PW-DICE), which
minimizes the primal Wasserstein distance between the expert and learner state
occupancies with a pessimistic regularizer and leverages a contrastively
learned distance as the underlying metric for the Wasserstein distance.
Theoretically, we prove that our framework is a generalization of the
state-of-the-art, SMODICE, and unifies $f$-divergence and Wasserstein
minimization. Empirically, we find that PW-DICE improves upon several
state-of-the-art methods on multiple testbeds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02143">Pairing-based graph neural network for simulating quantum materials. (arXiv:2311.02143v2 [cond-mat.str-el] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Luo_D/0/1/0/all/0/1">Di Luo</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Dai_D/0/1/0/all/0/1">David D. Dai</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fu_L/0/1/0/all/0/1">Liang Fu</a></p>
<p>We develop a pairing-based graph neural network for simulating quantum
many-body systems. Our architecture augments a BCS-type geminal wavefunction
with a generalized pair amplitude parameterized by a graph neural network.
Variational Monte Carlo with our neural network simultaneously provides an
accurate, flexible, and scalable method for simulating many-electron systems.
We apply this method to two-dimensional semiconductor electron-hole bilayers
and obtain accurate results on a variety of interaction-induced phases,
including the exciton Bose-Einstein condensate, electron-hole superconductor,
and bilayer Wigner crystal. Our study demonstrates the potential of
physically-motivated neural network wavefunctions for quantum materials
simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02198">Imitation Bootstrapped Reinforcement Learning. (arXiv:2311.02198v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hengyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1">Suvir Mirchandani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a></p>
<p>Despite the considerable potential of reinforcement learning (RL), robotics
control tasks predominantly rely on imitation learning (IL) owing to its better
sample efficiency. However, given the high cost of collecting extensive
demonstrations, RL is still appealing if it can utilize limited imitation data
for efficient autonomous self-improvement. Existing RL methods that utilize
demonstrations either initialize the replay buffer with demonstrations and
oversample them during RL training, which does not benefit from the
generalization potential of modern IL methods, or pretrain the RL policy with
IL on the demonstrations, which requires additional mechanisms to prevent
catastrophic forgetting during RL fine-tuning. We propose imitation
bootstrapped reinforcement learning (IBRL), a novel framework that first trains
an IL policy on a limited number of demonstrations and then uses it to propose
alternative actions for both online exploration and target value bootstrapping.
IBRL achieves SoTA performance and sample efficiency on 7 challenging sparse
reward continuous control tasks in simulation while learning directly from
pixels. As a highlight of our method, IBRL achieves $6.4\times$ higher success
rate than RLPD, a strong method that combines the idea of oversampling
demonstrations with modern RL improvements, under the budget of 10 demos and
100K interactions in the challenging PickPlaceCan task in the Robomimic
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02332">Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1">Elisa Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonsang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">William Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1">Tanveer Syeda-Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1">Charles Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Arvind Rao</a></p>
<p>Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of "big data" in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03488">Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems. (arXiv:2311.03488v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lilienthal_D/0/1/0/all/0/1">Derek Lilienthal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1">Paul Mello</a>, <a href="http://arxiv.org/find/cs/1/au:+Eirinaki_M/0/1/0/all/0/1">Magdalini Eirinaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1">Stas Tiomkin</a></p>
<p>While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Module (SDRM), which captures the intricate patterns
of real-world datasets required for training highly accurate recommender
systems. SDRM allows for the generation of synthetic data that can replace
existing datasets to preserve user privacy, or augment existing datasets to
address excessive data sparsity. Our method outperforms competing baselines
such as generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$k$ and
4.65% in NDCG@$k$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03489">Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meech_J/0/1/0/all/0/1">James T. Meech</a></p>
<p>We present a new high-level synthesis methodology for using large language
model tools to generate hardware designs. The methodology uses exclusively
open-source tools excluding the large language model. As a case study, we use
our methodology to generate a permuted congruential random number generator
design with a wishbone interface. We verify the functionality and quality of
the random number generator design using large language model-generated
simulations and the Dieharder randomness test suite. We document all the large
language model chat logs, Python scripts, Verilog scripts, and simulation
results used in the case study. We believe that our method of hardware design
generation coupled with the open source silicon 130 nm design tools will
revolutionize application-specific integrated circuit design. Our methodology
significantly lowers the bar to entry when building domain-specific computing
accelerators for the Internet of Things and proof of concept prototypes for
later fabrication in more modern process nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04158">Computing Approximate $\ell_p$ Sensitivities. (arXiv:2311.04158v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Padmanabhan_S/0/1/0/all/0/1">Swati Padmanabhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiuyi Zhang</a></p>
<p>Recent works in dimensionality reduction for regression tasks have introduced
the notion of sensitivity, an estimate of the importance of a specific
datapoint in a dataset, offering provable guarantees on the quality of the
approximation after removing low-sensitivity datapoints via subsampling.
However, fast algorithms for approximating $\ell_p$ sensitivities, which we
show is equivalent to approximate $\ell_p$ regression, are known for only the
$\ell_2$ setting, in which they are termed leverage scores.
</p>
<p>In this work, we provide efficient algorithms for approximating $\ell_p$
sensitivities and related summary statistics of a given matrix. In particular,
for a given $n \times d$ matrix, we compute $\alpha$-approximation to its
$\ell_1$ sensitivities at the cost of $O(n/\alpha)$ sensitivity computations.
For estimating the total $\ell_p$ sensitivity (i.e. the sum of $\ell_p$
sensitivities), we provide an algorithm based on importance sampling of
$\ell_p$ Lewis weights, which computes a constant factor approximation to the
total sensitivity at the cost of roughly $O(\sqrt{d})$ sensitivity
computations. Furthermore, we estimate the maximum $\ell_1$ sensitivity, up to
a $\sqrt{d}$ factor, using $O(d)$ sensitivity computations. We generalize all
these results to $\ell_p$ norms for $p &gt; 1$. Lastly, we experimentally show
that for a wide class of matrices in real-world datasets, the total sensitivity
can be quickly approximated and is significantly smaller than the theoretical
prediction, demonstrating that real-world datasets have low intrinsic effective
dimensionality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05067">Accelerating Exploration with Unlabeled Prior Data. (arXiv:2311.05067v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jason Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1">Dibya Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Amy Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>Learning to solve tasks from a sparse reward signal is a major challenge for
standard reinforcement learning (RL) algorithms. However, in the real world,
agents rarely need to solve sparse reward tasks entirely from scratch. More
often, we might possess prior experience to draw on that provides considerable
guidance about which actions and outcomes are possible in the world, which we
can use to explore more effectively for new tasks. In this work, we study how
prior data without reward labels may be used to guide and accelerate
exploration for an agent solving a new sparse reward task. We propose a simple
approach that learns a reward model from online experience, labels the
unlabeled prior data with optimistic rewards, and then uses it concurrently
alongside the online data for downstream policy and critic optimization. This
general formula leads to rapid exploration in several challenging sparse-reward
domains where tabula rasa exploration is insufficient, including the AntMaze
domain, Adroit hand manipulation domain, and a visual simulated robotic
manipulation domain. Our results highlight the ease of incorporating unlabeled
prior data into existing online RL algorithms, and the (perhaps surprising)
effectiveness of doing so.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05587">Bayesian Methods for Media Mix Modelling with shape and funnel effects. (arXiv:2311.05587v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marin_J/0/1/0/all/0/1">Javier Marin</a></p>
<p>In recent years, significant progress in generative AI has highlighted the
important role of physics-inspired models that utilize advanced mathematical
concepts based on fundamental physics principles to enhance artificial
intelligence capabilities. Among these models, those based on diffusion
equations have greatly improved image quality. This study aims to explore the
potential uses of Maxwell-Boltzmann equation, which forms the basis of the
kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix
Modelling (MMM) applications. We propose incorporating these equations into
Hierarchical Bayesian models to analyse consumer behaviour in the context of
advertising. These equation sets excel in accurately describing the random
dynamics in complex systems like social interactions and consumer-advertising
interactions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05858">Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation. (arXiv:2311.05858v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Junyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1">Hyeongjun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1">Ilhoon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kwanghoon Sohn</a></p>
<p>Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06185">An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer. (arXiv:2311.06185v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1">Adam J Shephard</a>, <a href="http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1">Mostafa Jahanifar</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Ruoyu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1">Muhammad Dawood</a>, <a href="http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1">Simon Graham</a>, <a href="http://arxiv.org/find/eess/1/au:+Sidlauskas_K/0/1/0/all/0/1">Kastytis Sidlauskas</a>, <a href="http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1">Syed Ali Khurram</a>, <a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1">Nasir M Rajpoot</a>, <a href="http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1">Shan E Ahmed Raza</a></p>
<p>Tumour-infiltrating lymphocytes (TILs) are considered as a valuable
prognostic markers in both triple-negative and human epidermal growth factor
receptor 2 (HER2) positive breast cancer. In this study, we introduce an
innovative deep learning pipeline based on the Efficient-UNet architecture to
predict the TILs score for breast cancer whole-slide images (WSIs). We first
segment tumour and stromal regions in order to compute a tumour bulk mask. We
then detect TILs within the tumour-associated stroma, generating a TILs score
by closely mirroring the pathologist's workflow. Our method exhibits
state-of-the-art performance in segmenting tumour/stroma areas and TILs
detection, as demonstrated by internal cross-validation on the TiGER Challenge
training dataset and evaluation on the final leaderboards. Additionally, our
TILs score proves competitive in predicting survival outcomes within the same
challenge, underscoring the clinical relevance and potential of our automated
TILs scoring pipeline as a breast cancer prognostic tool.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06228">Learning material synthesis-process-structure-property relationship by data fusion: Bayesian Coregionalization N-Dimensional Piecewise Function Learning. (arXiv:2311.06228v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kusne_A/0/1/0/all/0/1">A. Gilad Kusne</a>, <a href="http://arxiv.org/find/cs/1/au:+McDannald_A/0/1/0/all/0/1">Austin McDannald</a>, <a href="http://arxiv.org/find/cs/1/au:+DeCost_B/0/1/0/all/0/1">Brian DeCost</a></p>
<p>Autonomous materials research labs require the ability to combine and learn
from diverse data streams. This is especially true for learning material
synthesis-process-structure-property relationships, key to accelerating
materials optimization and discovery as well as accelerating mechanistic
understanding. We present the Synthesis-process-structure-property relAtionship
coreGionalized lEarner (SAGE) algorithm. A fully Bayesian algorithm that uses
multimodal coregionalization to merge knowledge across data sources to learn
synthesis-process-structure-property relationships. SAGE outputs a
probabilistic posterior for the relationships including the most likely
relationships given the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06233">Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. (arXiv:2311.06233v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06322">Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models. (arXiv:2311.06322v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1">Chaoyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zewen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Diffusion models have achieved great success due to their remarkable
generation ability. However, their high computational overhead is still a
troublesome problem. Recent studies have leveraged post-training quantization
(PTQ) to compress diffusion models. However, most of them only focus on
unconditional models, leaving the quantization of widely used large pretrained
text-to-image models, e.g., Stable Diffusion, largely unexplored. In this
paper, we propose a novel post-training quantization method PCR (Progressive
Calibration and Relaxing) for text-to-image diffusion models, which consists of
a progressive calibration strategy that considers the accumulated quantization
error across timesteps, and an activation relaxing strategy that improves the
performance with negligible cost. Additionally, we demonstrate the previous
metrics for text-to-image diffusion model quantization are not accurate due to
the distribution gap. To tackle the problem, we propose a novel QDiffBench
benchmark, which utilizes data in the same domain for more accurate evaluation.
Besides, QDiffBench also considers the generalization performance of the
quantized model outside the calibration dataset. Extensive experiments on
Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our
method and benchmark. Moreover, we are the first to achieve quantization for
Stable Diffusion XL while maintaining the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06456">Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding. (arXiv:2311.06456v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1">Pengyu Hong</a></p>
<p>The versatility of multimodal deep learning holds tremendous promise for
advancing scientific research and practical applications. As this field
continues to evolve, the collective power of cross-modal analysis promises to
drive transformative innovations, leading us to new frontiers in chemical
understanding and discovery. Hence, we introduce Asymmetric Contrastive
Multimodal Learning (ACML) as a novel approach tailored for molecules,
showcasing its potential to advance the field of chemistry. ACML harnesses the
power of effective asymmetric contrastive learning to seamlessly transfer
information from various chemical modalities to molecular graph
representations. By combining pre-trained chemical unimodal encoders and a
shallow-designed graph encoder, ACML facilitates the assimilation of
coordinated chemical semantics from different modalities, leading to
comprehensive representation learning with efficient training. This innovative
framework enhances the interpretability of learned representations and bolsters
the expressive power of graph neural networks. Through practical tasks such as
isomer discrimination and uncovering crucial chemical properties for drug
discovery, ACML exhibits its capability to revolutionize chemical research and
applications, providing a deeper understanding of chemical semantics of
different modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06483">Stacked networks improve physics-informed training: applications to neural networks and deep operator networks. (arXiv:2311.06483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1">Amanda A Howard</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1">Sarah H Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Shady E Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Stinis_P/0/1/0/all/0/1">Panos Stinis</a></p>
<p>Physics-informed neural networks and operator networks have shown promise for
effectively solving equations modeling physical systems. However, these
networks can be difficult or impossible to train accurately for some systems of
equations. We present a novel multifidelity framework for stacking
physics-informed neural networks and operator networks that facilitates
training. We successively build a chain of networks, where the output at one
step can act as a low-fidelity input for training the next step, gradually
increasing the expressivity of the learned model. The equations imposed at each
step of the iterative process can be the same or different (akin to simulated
annealing). The iterative (stacking) nature of the proposed method allows us to
progressively learn features of a solution that are hard to learn directly.
Through benchmark problems including a nonlinear pendulum, the wave equation,
and the viscous Burgers equation, we show how stacking can be used to improve
the accuracy and reduce the required size of physics-informed neural networks
and operator networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07784">A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks. (arXiv:2311.07784v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Babakniya_S/0/1/0/all/0/1">Sara Babakniya</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1">Zalan Fabian</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Chaoyang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1">Mahdi Soltanolkotabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1">Salman Avestimehr</a></p>
<p>Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
$\textbf{federated class incremental learning}$ that utilizes a generative
model to synthesize samples from past distributions. This data can be later
exploited alongside the training data to mitigate catastrophic forgetting. To
preserve privacy, the generative model is trained on the server using data-free
methods at the end of each task without requesting data from clients. Moreover,
our solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09387">Banach-Tarski Embeddings and Transformers. (arXiv:2311.09387v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maher_J/0/1/0/all/0/1">Joshua Maher</a></p>
<p>We introduce a new construction of embeddings of arbitrary recursive data
structures into high dimensional vectors. These embeddings provide an
interpretable model for the latent state vectors of transformers. We
demonstrate that these embeddings can be decoded to the original data structure
when the embedding dimension is sufficiently large. This decoding algorithm has
a natural implementation as a transformer. We also show that these embedding
vectors can be manipulated directly to perform computations on the underlying
data without decoding. As an example we present an algorithm that constructs
the embedded parse tree of an embedded token sequence using only vector
operations in embedding space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10770">Exponentially Faster Language Modelling. (arXiv:2311.10770v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belcak_P/0/1/0/all/0/1">Peter Belcak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1">Roger Wattenhofer</a></p>
<p>Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10777">A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends. (arXiv:2311.10777v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yan Cathy Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a>, <a href="http://arxiv.org/find/cs/1/au:+Taskova_K/0/1/0/all/0/1">Katerina Taskova</a>, <a href="http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1">J&#xf6;rg Wicker</a></p>
<p>Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment
analysis (SA) that identifies aspects and the associated opinions from a given
text. In the digital era, ABSA gained increasing popularity and applications in
mining opinionated text data to obtain insights and support decisions. ABSA
research employs linguistic, statistical, and machine-learning approaches and
utilises resources such as labelled datasets, aspect and sentiment lexicons and
ontology. By its nature, ABSA is domain-dependent and can be sensitive to the
impact of misalignment between the resource and application domains. However,
to our knowledge, this topic has not been explored by the existing ABSA
literature reviews. In this paper, we present a Systematic Literature Review
(SLR) of ABSA studies with a focus on the research application domain, dataset
domain, and the research methods to examine their relationships and identify
trends over time. Our results suggest a number of potential systemic issues in
the ABSA research literature, including the predominance of the
``product/service review'' dataset domain among the majority of studies that
did not have a specific research application domain, coupled with the
prevalence of dataset-reliant methods such as supervised machine learning. This
review makes a number of unique contributions to the ABSA research field: 1) To
our knowledge, it is the first SLR that links the research domain, dataset
domain, and research method through a systematic perspective; 2) it is one of
the largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191
search results without time constraint; and 3) our review methodology adopted
an innovative automatic filtering process based on PDF-mining, which enhanced
screening quality and reliability. Suggestions and our review limitations are
also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10801">Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools. (arXiv:2311.10801v2 [q-fin.PM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhao_Y/0/1/0/all/0/1">Yilei Zhao</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Sun_S/0/1/0/all/0/1">Shuo Sun</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Ying_J/0/1/0/all/0/1">Jie Ying</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Xie_Y/0/1/0/all/0/1">Yonggang Xie</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Song_Z/0/1/0/all/0/1">Zitao Song</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Wang_X/0/1/0/all/0/1">Xinrun Wang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+An_B/0/1/0/all/0/1">Bo An</a></p>
<p>Portfolio management (PM) is a fundamental financial trading task, which
explores the optimal periodical reallocation of capitals into different stocks
to pursue long-term profits. Reinforcement learning (RL) has recently shown its
potential to train profitable agents for PM through interacting with financial
markets. However, existing work mostly focuses on fixed stock pools, which is
inconsistent with investors' practical demand. Specifically, the target stock
pool of different investors varies dramatically due to their discrepancy on
market states and individual investors may temporally adjust stocks they desire
to trade (e.g., adding one popular stocks), which lead to customizable stock
pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny
change of the stock pool, which leads to high computational cost and unstable
performance. To tackle this challenge, we propose EarnMore, a rEinforcement
leARNing framework with Maskable stOck REpresentation to handle PM with CSPs
through one-shot training in a global stock pool (GSP). Specifically, we first
introduce a mechanism to mask out the representation of the stocks outside the
target pool. Second, we learn meaningful stock representations through a
self-supervised masking and reconstruction process. Third, a re-weighting
mechanism is designed to make the portfolio concentrate on favorable stocks and
neglect the stocks outside the target pool. Through extensive experiments on 8
subset stock pools of the US stock market, we demonstrate that EarnMore
significantly outperforms 14 state-of-the-art baselines in terms of 6 popular
financial metrics with over 40% improvement on profit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10863">Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks. (arXiv:2311.10863v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1">Kaiyuan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zihe Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1">Yiannis Kantaros</a></p>
<p>Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10899">Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning. (arXiv:2311.10899v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Shaunak Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaggar_R/0/1/0/all/0/1">Raghav Gaggar</a></p>
<p>With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline's effectiveness in the end using standard
metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11369">Optimal Locally Private Nonparametric Classification with Public Data. (arXiv:2311.11369v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1">Yuheng Ma</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1">Hanfang Yang</a></p>
<p>In this work, we investigate the problem of public data-assisted
non-interactive LDP (Local Differential Privacy) learning with a focus on
non-parametric classification. Under the posterior drift assumption, we for the
first time derive the mini-max optimal convergence rate with LDP constraint.
Then, we present a novel approach, the locally private classification tree,
which attains the mini-max optimal convergence rate. Furthermore, we design a
data-driven pruning procedure that avoids parameter tuning and produces a fast
converging estimator. Comprehensive experiments conducted on synthetic and real
datasets show the superior performance of our proposed method. Both our
theoretical and experimental findings demonstrate the effectiveness of public
data compared to private data, which leads to practical suggestions for
prioritizing non-private data collection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11446">Weight Norm Control. (arXiv:2311.11446v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loshchilov_I/0/1/0/all/0/1">Ilya Loshchilov</a></p>
<p>We note that decoupled weight decay regularization is a particular case of
weight norm control where the target norm of weights is set to 0. Any
optimization method (e.g., Adam) which uses decoupled weight decay
regularization (respectively, AdamW) can be viewed as a particular case of a
more general algorithm with weight norm control (respectively, AdamWN). We
argue that setting the target norm of weights to 0 can be suboptimal and other
target norm values can be considered. For instance, any training run where
AdamW achieves a particular norm of weights can be challenged by AdamWN
scheduled to achieve a comparable norm of weights. We discuss various
implications of introducing weight norm control instead of weight decay.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11908">Continual Learning: Applications and the Road Forward. (arXiv:2311.11908v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verwimp_E/0/1/0/all/0/1">Eli Verwimp</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1">Rahaf Aljundi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1">Shai Ben-David</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a>, <a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1">Andrea Cossu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1">Alexander Gepperth</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1">Tyler L. Hayes</a>, <a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1">Eyke H&#xfc;llermeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1">Christopher Kanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kudithipudi_D/0/1/0/all/0/1">Dhireesha Kudithipudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1">Christoph H. Lampert</a>, <a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1">Martin Mundt</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1">Razvan Pascanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1">Adrian Popescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1">Andreas S. Tolias</a>, <a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1">Joost van de Weijer</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1">Vincenzo Lomonaco</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a></p>
<p>Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: "Why should one care about continual learning in the
first place?". We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11995">BrainWash: A Poisoning Attack to Forget in Continual Learning. (arXiv:2311.11995v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1">Ali Abbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1">Parsa Nooralinejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1">Hamed Pirsiavash</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1">Soheil Kolouri</a></p>
<p>Continual learning has gained substantial attention within the deep learning
community, offering promising solutions to the challenging problem of
sequential learning. Yet, a largely unexplored facet of this paradigm is its
susceptibility to adversarial attacks, especially with the aim of inducing
forgetting. In this paper, we introduce "BrainWash," a novel data poisoning
method tailored to impose forgetting on a continual learner. By adding the
BrainWash noise to a variety of baselines, we demonstrate how a trained
continual learner can be induced to forget its previously learned tasks
catastrophically, even when using these continual learning baselines. An
important feature of our approach is that the attacker requires no access to
previous tasks' data and is armed merely with the model's current parameters
and the data belonging to the most recent task. Our extensive experiments
highlight the efficacy of BrainWash, showcasing degradation in performance
across various regularization-based continual learning methods.
</p>
</p>
</div>

    </div>
    </body>
    