<!DOCTYPE html>
<html>
<head>
<title>2023-12-26-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.14171">SEOpinion: Summarization and Exploration Opinion of E-Commerce Websites. (arXiv:2312.14171v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mabrouk_A/0/1/0/all/0/1">Alhassan Mabrouk</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Redondo_R/0/1/0/all/0/1">Rebeca P. D&#xed;az-Redondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayed_M/0/1/0/all/0/1">Mohammed Kayed</a></p>
<p>E-Commerce (EC) websites provide a large amount of useful information that
exceed human cognitive processing ability. In order to help customers in
comparing alternatives when buying a product, previous studies designed opinion
summarization systems based on customer reviews. They ignored templates'
information provided by manufacturers, although these descriptive information
have much product aspects or characteristics. Therefore, this paper proposes a
methodology coined as SEOpinion (Summa-rization and Exploration of Opinions)
which provides a summary for the product aspects and spots opinion(s) regarding
them, using a combination of templates' information with the customer reviews
in two main phases. First, the Hierarchical Aspect Extraction (HAE) phase
creates a hierarchy of product aspects from the template. Subsequently, the
Hierarchical Aspect-based Opinion Summarization (HAOS) phase enriches this
hierarchy with customers' opinions; to be shown to other potential buyers. To
test the feasibility of using Deep Learning-based BERT techniques with our
approach, we have created a corpus by gathering information from the top five
EC websites for laptops. The experimental results show that Recurrent Neural
Network (RNN) achieves better results (77.4% and 82.6% in terms of F1-measure
for the first and second phase) than the Convolutional Neural Network (CNN) and
the Support Vector Machine (SVM) technique.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14180">Dynamic Topic Language Model on Heterogeneous Children&#x27;s Mental Health Clinical Notes. (arXiv:2312.14180v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Hanwen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreno_T/0/1/0/all/0/1">Tatiana Moreno</a>, <a href="http://arxiv.org/find/cs/1/au:+Alpern_A/0/1/0/all/0/1">Adrianne Alpern</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehwerhemuepha_L/0/1/0/all/0/1">Louis Ehwerhemuepha</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_A/0/1/0/all/0/1">Annie Qu</a></p>
<p>Mental health diseases affect children's lives and well-beings which have
received increased attention since the COVID-19 pandemic. Analyzing psychiatric
clinical notes with topic models is critical to evaluate children's mental
status over time. However, few topic models are built for longitudinal
settings, and they fail to keep consistent topics and capture temporal
trajectories for each document. To address these challenges, we develop a
longitudinal topic model with time-invariant topics and individualized temporal
dependencies on the evolving document metadata. Our model preserves the
semantic meaning of discovered topics over time and incorporates heterogeneity
among documents. In particular, when documents can be categorized, we propose
an unsupervised topics learning approach to maximize topic heterogeneity across
different document groups. We also present an efficient variational
optimization procedure adapted for the multistage longitudinal setting. In this
case study, we apply our method to the psychiatric clinical notes from a large
tertiary pediatric hospital in Southern California and achieve a 38% increase
in the overall coherence of extracted topics. Our real data analysis reveals
that children tend to express more negative emotions during state shutdowns and
more positive when schools reopen. Furthermore, it suggests that sexual and
gender minority (SGM) children display more pronounced reactions to major
COVID-19 events and a greater sensitivity to vaccine-related news than non-SGM
children. This study examines the progression of children's mental health
during the pandemic and offers clinicians valuable insights to recognize the
disparities in children's mental health related to their sexual and gender
identities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14183">On Early Detection of Hallucinations in Factual Question Answering. (arXiv:2312.14183v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Snyder_B/0/1/0/all/0/1">Ben Snyder</a>, <a href="http://arxiv.org/find/cs/1/au:+Moisescu_M/0/1/0/all/0/1">Marius Moisescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1">Muhammad Bilal Zafar</a></p>
<p>While large language models (LLMs) have taken great strides towards helping
humans with a plethora of tasks like search and summarization, hallucinations
remain a major impediment towards gaining user trust. The fluency and coherence
of model generations even when hallucinating makes it difficult to detect
whether or not a model is hallucinating. In this work, we explore if the
artifacts associated with the model generations can provide hints that the
generation will contain hallucinations. Specifically, we probe LLMs at 1) the
inputs via Integrated Gradients based token attribution, 2) the outputs via the
Softmax probabilities, and 3) the internal state via self-attention and
fully-connected layer activations for signs of hallucinations on open-ended
question answering tasks. Our results show that the distributions of these
artifacts differ between hallucinated and non-hallucinated generations.
Building on this insight, we train binary classifiers that use these artifacts
as input features to classify model generations into hallucinations and
non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC.
We further show that tokens preceding a hallucination can predict the
subsequent hallucination before it occurs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14184">Large Language Models in Medical Term Classification and Unexpected Misalignment Between Response and Reasoning. (arXiv:2312.14184v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaodan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_S/0/1/0/all/0/1">Sandeep Vemulapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_N/0/1/0/all/0/1">Nabasmita Talukdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sumyeong Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiankun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Han Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Murtaza_S/0/1/0/all/0/1">Sardar Mehtab Bin Murtaza</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1">Aakash Ajay Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Leshchiner_D/0/1/0/all/0/1">Dmitry Leshchiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Joseph_D/0/1/0/all/0/1">Dimitri F. Joseph</a>, <a href="http://arxiv.org/find/cs/1/au:+Witteveen_Lane_M/0/1/0/all/0/1">Martin Witteveen-Lane</a>, <a href="http://arxiv.org/find/cs/1/au:+Chesla_D/0/1/0/all/0/1">Dave Chesla</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiayu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bin Chen</a></p>
<p>This study assesses the ability of state-of-the-art large language models
(LLMs) including GPT-3.5, GPT-4, Falcon, and LLaMA 2 to identify patients with
mild cognitive impairment (MCI) from discharge summaries and examines instances
where the models' responses were misaligned with their reasoning. Utilizing the
MIMIC-IV v2.2 database, we focused on a cohort aged 65 and older, verifying MCI
diagnoses against ICD codes and expert evaluations. The data was partitioned
into training, validation, and testing sets in a 7:2:1 ratio for model
fine-tuning and evaluation, with an additional metastatic cancer dataset from
MIMIC III used to further assess reasoning consistency. GPT-4 demonstrated
superior interpretative capabilities, particularly in response to complex
prompts, yet displayed notable response-reasoning inconsistencies. In contrast,
open-source models like Falcon and LLaMA 2 achieved high accuracy but lacked
explanatory reasoning, underscoring the necessity for further research to
optimize both performance and interpretability. The study emphasizes the
significance of prompt engineering and the need for further exploration into
the unexpected reasoning-response misalignment observed in GPT-4. The results
underscore the promise of incorporating LLMs into healthcare diagnostics,
contingent upon methodological advancements to ensure accuracy and clinical
coherence of AI-generated outputs, thereby improving the trustworthiness of
LLMs for medical decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14185">Auto311: A Confidence-guided Automated System for Non-emergency Call. (arXiv:2312.14185v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zirong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xutong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Meiyi Ma</a></p>
<p>Emergency and non-emergency response systems are essential services provided
by local governments and critical to protecting lives, the environment, and
property. The effective handling of (non-)emergency calls is critical for
public safety and well-being. By reducing the burden through non-emergency
callers, residents in critical need of assistance through 911 will receive a
fast and effective response. Collaborating with the Department of Emergency
Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call
recordings and developed Auto311, the first automated system to handle 311
non-emergency calls, which (1) effectively and dynamically predicts ongoing
non-emergency incident types to generate tailored case reports during the call;
(2) itemizes essential information from dialogue contexts to complete the
generated reports; and (3) strategically structures system-caller dialogues
with optimized confidence. We used real-world data to evaluate the system's
effectiveness and deployability. The experimental results indicate that the
system effectively predicts incident type with an average F-1 score of 92.54%.
Moreover, the system successfully itemizes critical information from relevant
contexts to complete reports, evincing a 0.93 average consistency score
compared to the ground truth. Additionally, emulations demonstrate that the
system effectively decreases conversation turns as the utterance size gets more
extensive and categorizes the ongoing call with 94.49% mean accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14187">WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhaojian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1">Ning Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yangyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Can Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yishujie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenxiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Qiufeng Yin</a></p>
<p>Recent work demonstrates that, after being fine-tuned on a high-quality
instruction dataset, the resulting model can obtain impressive capabilities to
address a wide range of tasks. However, existing methods for instruction data
generation often produce duplicate data and are not controllable enough on data
quality. In this paper, we extend the generalization of instruction tuning by
classifying the instruction data to 4 code-related tasks and propose a
LLM-based Generator-Discriminator data process framework to generate diverse,
high-quality instruction data from open source code. Hence, we introduce
CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal
code-related tasks,which is aimed at augmenting the effectiveness of
instruction tuning and improving the generalization ability of fine-tuned
model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with
Widespread And Versatile Enhanced instruction tuning. This model is
specifically designed for enhancing instruction tuning of Code Language Models
(LLMs). Our experiments demonstrate that Wavecoder models outperform other
open-source models in terms of generalization ability across different
code-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder
exhibits high efficiency in previous code generation tasks. This paper thus
offers a significant contribution to the field of instruction data generation
and fine-tuning models, providing new insights and tools for enhancing
performance in code-related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14197">Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. (arXiv:2312.14197v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1">Jingwei Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yueqi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1">Keegan Hines</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1">Emre Kiciman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Guangzhong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fangzhao Wu</a></p>
<p>Recent remarkable advancements in large language models (LLMs) have led to
their widespread adoption in various applications. A key feature of these
applications is the combination of LLMs with external content, where user
instructions and third-party content are combined to create prompts for LLM
processing. These applications, however, are vulnerable to indirect prompt
injection attacks, where malicious instructions embedded within external
content compromise LLM's output, causing their responses to deviate from user
expectations. Despite the discovery of this security issue, no comprehensive
analysis of indirect prompt injection attacks on different LLMs is available
due to the lack of a benchmark. Furthermore, no effective defense has been
proposed.
</p>
<p>In this work, we introduce the first benchmark, BIPIA, to measure the
robustness of various LLMs and defenses against indirect prompt injection
attacks. Our experiments reveal that LLMs with greater capabilities exhibit
more vulnerable to indirect prompt injection attacks for text tasks, resulting
in a higher ASR. We hypothesize that indirect prompt injection attacks are
mainly due to the LLMs' inability to distinguish between instructions and
external content. Based on this conjecture, we propose four black-box methods
based on prompt learning and a white-box defense methods based on fine-tuning
with adversarial training to enable LLMs to distinguish between instructions
and external content and ignore instructions in the external content. Our
experimental results show that our black-box defense methods can effectively
reduce ASR but cannot completely thwart indirect prompt injection attacks,
while our white-box defense method can reduce ASR to nearly zero with little
adverse impact on the LLM's performance on general tasks. We hope that our
benchmark and defenses can inspire future work in this important area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14202">Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models. (arXiv:2312.14202v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jordan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shou-Hsuan Stephen Huang</a></p>
<p>This study explores the idea of AI Personality or AInality suggesting that
Large Language Models (LLMs) exhibit patterns similar to human personalities.
Assuming that LLMs share these patterns with humans, we investigate using
human-centered psychometric tests such as the Myers-Briggs Type Indicator
(MBTI), Big Five Inventory (BFI), and Short Dark Triad (SD3) to identify and
confirm LLM personality types. By introducing role-play prompts, we demonstrate
the adaptability of LLMs, showing their ability to switch dynamically between
different personality types. Using projective tests, such as the Washington
University Sentence Completion Test (WUSCT), we uncover hidden aspects of LLM
personalities that are not easily accessible through direct questioning.
Projective tests allowed for a deep exploration of LLMs cognitive processes and
thought patterns and gave us a multidimensional view of AInality. Our machine
learning analysis revealed that LLMs exhibit distinct AInality traits and
manifest diverse personality types, demonstrating dynamic shifts in response to
external instructions. This study pioneers the application of projective tests
on LLMs, shedding light on their diverse and adaptable AInality traits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14203">Shai: A large language model for asset management. (arXiv:2312.14203v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Guo_Z/0/1/0/all/0/1">Zhongyang Guo</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jiang_G/0/1/0/all/0/1">Guanran Jiang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongdan Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Wang_Z/0/1/0/all/0/1">Zhefeng Wang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Wang_Y/0/1/0/all/0/1">Yinchun Wang</a></p>
<p>This paper introduces "Shai" a 10B level large language model specifically
designed for the asset management industry, built upon an open-source
foundational model. With continuous pre-training and fine-tuning using a
targeted corpus, Shai demonstrates enhanced performance in tasks relevant to
its domain, outperforming baseline models. Our research includes the
development of an innovative evaluation framework, which integrates
professional qualification exams, tailored tasks, open-ended question
answering, and safety assessments, to comprehensively assess Shai's
capabilities. Furthermore, we discuss the challenges and implications of
utilizing large language models like GPT-4 for performance assessment in asset
management, suggesting a combination of automated evaluation and human
judgment. Shai's development, showcasing the potential and versatility of
10B-level large language models in the financial sector with significant
performance and modest computational requirements, hopes to provide practical
insights and methodologies to assist industry peers in their similar endeavors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14211">Experimenting with Large Language Models and vector embeddings in NASA SciX. (arXiv:2312.14211v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blanco_Cuaresma_S/0/1/0/all/0/1">Sergi Blanco-Cuaresma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciuca_I/0/1/0/all/0/1">Ioana Ciuc&#x103;</a>, <a href="http://arxiv.org/find/cs/1/au:+Accomazzi_A/0/1/0/all/0/1">Alberto Accomazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1">Michael J. Kurtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Henneken_E/0/1/0/all/0/1">Edwin A. Henneken</a>, <a href="http://arxiv.org/find/cs/1/au:+Lockhart_K/0/1/0/all/0/1">Kelly E. Lockhart</a>, <a href="http://arxiv.org/find/cs/1/au:+Grezes_F/0/1/0/all/0/1">Felix Grezes</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_T/0/1/0/all/0/1">Thomas Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapurian_G/0/1/0/all/0/1">Golnaz Shapurian</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1">Carolyn S. Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_D/0/1/0/all/0/1">Donna M. Thompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hostetler_T/0/1/0/all/0/1">Timothy W. Hostetler</a>, <a href="http://arxiv.org/find/cs/1/au:+Templeton_M/0/1/0/all/0/1">Matthew R. Templeton</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shinyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1">Jennifer Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacovich_T/0/1/0/all/0/1">Taylor Jacovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Chivvis_D/0/1/0/all/0/1">Daniel Chivvis</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_F/0/1/0/all/0/1">Fernanda de Macedo Alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Paquin_J/0/1/0/all/0/1">Jean-Claude Paquin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartlett_J/0/1/0/all/0/1">Jennifer Bartlett</a>, <a href="http://arxiv.org/find/cs/1/au:+Polimera_M/0/1/0/all/0/1">Mugdha Polimera</a>, <a href="http://arxiv.org/find/cs/1/au:+Jarmak_S/0/1/0/all/0/1">Stephanie Jarmak</a></p>
<p>Open-source Large Language Models enable projects such as NASA SciX (i.e.,
NASA ADS) to think out of the box and try alternative approaches for
information retrieval and data augmentation, while respecting data copyright
and users' privacy. However, when large language models are directly prompted
with questions without any context, they are prone to hallucination. At NASA
SciX we have developed an experiment where we created semantic vectors for our
large collection of abstracts and full-text content, and we designed a prompt
system to ask questions using contextual chunks from our system. Based on a
non-systematic human evaluation, the experiment shows a lower degree of
hallucination and better responses when using Retrieval Augmented Generation.
Further exploration is required to design new features and data augmentation
processes at NASA SciX that leverages this technology while respecting the high
level of trust and quality that the project holds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14215">SimLM: Can Language Models Infer Parameters of Physical Systems?. (arXiv:2312.14215v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Memery_S/0/1/0/all/0/1">Sean Memery</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1">Mirella Lapata</a>, <a href="http://arxiv.org/find/cs/1/au:+Subr_K/0/1/0/all/0/1">Kartic Subr</a></p>
<p>Recent developments in large-scale machine learning models for
general-purpose understanding, translation and generation of language are
driving impact across a variety of sectors including medicine, robotics, and
scientific discovery. The strength of such Large Language Models (LLMs) stems
from the large corpora that they are trained with. While this imbues them with
a breadth of capabilities, they have been found unsuitable for some specific
types of problems such as advanced mathematics. In this paper, we highlight the
inability of LLMs to reason about physics tasks. We demonstrate that their
ability to infer parameters of physical systems can be improved, without
retraining, by augmenting their context with feedback from physical simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14226">Deep de Finetti: Recovering Topic Distributions from Large Language Models. (arXiv:2312.14226v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1">R. Thomas McCoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1">Theodore R. Sumers</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jian-Qiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a></p>
<p>Large language models (LLMs) can produce long, coherent passages of text,
suggesting that LLMs, although trained on next-word prediction, must represent
the latent structure that characterizes a document. Prior work has found that
internal representations of LLMs encode one aspect of latent structure, namely
syntax; here we investigate a complementary aspect, namely the document's topic
structure. We motivate the hypothesis that LLMs capture topic structure by
connecting LLM optimization to implicit Bayesian inference. De Finetti's
theorem shows that exchangeable probability distributions can be represented as
a mixture with respect to a latent generating distribution. Although text is
not exchangeable at the level of syntax, exchangeability is a reasonable
starting assumption for topic structure. We thus hypothesize that predicting
the next token in text will lead LLMs to recover latent topic distributions. We
examine this hypothesis using Latent Dirichlet Allocation (LDA), an
exchangeable probabilistic topic model, as a target, and we show that the
representations formed by LLMs encode both the topics used to generate
synthetic data and those used to explain natural corpus data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14279">Characterizing and Classifying Developer Forum Posts with their Intentions. (arXiv:2312.14279v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingfang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Laufer_E/0/1/0/all/0/1">Eric Laufer</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Santhosh Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jayden Luo</a></p>
<p>With the rapid growth of the developer community, the amount of posts on
online technical forums has been growing rapidly, which poses difficulties for
users to filter useful posts and find important information. Tags provide a
concise feature dimension for users to locate their interested posts and for
search engines to index the most relevant posts according to the queries.
However, most tags are only focused on the technical perspective (e.g., program
language, platform, tool). In most cases, forum posts in online developer
communities reveal the author's intentions to solve a problem, ask for advice,
share information, etc. The modeling of the intentions of posts can provide an
extra dimension to the current tag taxonomy. By referencing previous studies
and learning from industrial perspectives, we create a refined taxonomy for the
intentions of technical forum posts. Through manual labeling and analysis on a
sampled post dataset extracted from online forums, we understand the relevance
between the constitution of posts (code, error messages) and their intentions.
Furthermore, inspired by our manual study, we design a pre-trained
transformer-based model to automatically predict post intentions. The best
variant of our intention prediction framework, which achieves a Micro F1-score
of 0.589, Top 1-3 accuracy of 62.6% to 87.8%, and an average AUC of 0.787,
outperforms the state-of-the-art baseline approach. Our characterization and
automated classification of forum posts regarding their intentions may help
forum maintainers or third-party tool developers improve the organization and
retrieval of posts on technical forums. We have released our annotated dataset
and codes in our supplementary material package.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14302">Exploiting Novel GPT-4 APIs. (arXiv:2312.14302v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1">Kellin Pelrine</a>, <a href="http://arxiv.org/find/cs/1/au:+Taufeeque_M/0/1/0/all/0/1">Mohammad Taufeeque</a>, <a href="http://arxiv.org/find/cs/1/au:+Zajac_M/0/1/0/all/0/1">Micha&#x142; Zaj&#x105;c</a>, <a href="http://arxiv.org/find/cs/1/au:+McLean_E/0/1/0/all/0/1">Euan McLean</a>, <a href="http://arxiv.org/find/cs/1/au:+Gleave_A/0/1/0/all/0/1">Adam Gleave</a></p>
<p>Language model attacks typically assume one of two extreme threat models:
full white-box access to model weights, or black-box access limited to a text
generation API. However, real-world APIs are often more flexible than just text
generation: these APIs expose ``gray-box'' access leading to new threat
vectors. To explore this, we red-team three new functionalities exposed in the
GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that
fine-tuning a model on as few as 15 harmful examples or 100 benign examples can
remove core safeguards from GPT-4, enabling a range of harmful outputs.
Furthermore, we find that GPT-4 Assistants readily divulge the function call
schema and can be made to execute arbitrary function calls. Finally, we find
that knowledge retrieval can be hijacked by injecting instructions into
retrieval documents. These vulnerabilities highlight that any additions to the
functionality exposed by an API can create new vulnerabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14327">Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text Entry: A Case Study on Abbreviation Expansion. (arXiv:2312.14327v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tomanek_K/0/1/0/all/0/1">Katrin Tomanek</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1">Shanqing Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1">Subhashini Venugopalan</a></p>
<p>Abbreviation expansion is a strategy used to speed up communication by
limiting the amount of typing and using a language model to suggest expansions.
Here we look at personalizing a Large Language Model's (LLM) suggestions based
on prior conversations to enhance the relevance of predictions, particularly
when the user data is small (~1000 samples). Specifically, we compare
fine-tuning, prompt-tuning, and retrieval augmented generation of expanded text
suggestions for abbreviated inputs. Our case study with a deployed 8B parameter
LLM on a real user living with ALS, and experiments on movie character
personalization indicates that (1) customization may be necessary in some
scenarios and prompt-tuning generalizes well to those, (2) fine-tuning on
in-domain data (with as few as 600 samples) still shows some gains, however (3)
retrieval augmented few-shot selection also outperforms fine-tuning. (4)
Parameter efficient tuning allows for efficient and scalable personalization.
For prompt-tuning, we also find that initializing the learned "soft-prompts" to
user relevant concept tokens leads to higher accuracy than random
initialization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14335">Context-aware Decoding Reduces Hallucination in Query-focused Summarization. (arXiv:2312.14335v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhichao Xu</a></p>
<p>Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility on one
recently proposed decoding method -- Context-aware Decoding (CAD). In addition
to replicating CAD's experiments on news summarization datasets, we include
experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14345">Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs. (arXiv:2312.14345v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahdari_B/0/1/0/all/0/1">Behnam Rahdari</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Hao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Ziwei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yifei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuotong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1">Anoop Deoras</a>, <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a></p>
<p>The unique capabilities of Large Language Models (LLMs), such as the natural
language text generation ability, position them as strong candidates for
providing explanation for recommendations. However, despite the size of the
LLM, most existing models struggle to produce zero-shot explanations reliably.
To address this issue, we propose a framework called Logic-Scaffolding, that
combines the ideas of aspect-based explanation and chain-of-thought prompting
to generate explanations through intermediate reasoning steps. In this paper,
we share our experience in building the framework and present an interactive
demonstration for exploring our results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14346">Don&#x27;t Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models. (arXiv:2312.14346v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vakharia_P/0/1/0/all/0/1">Priyesh Vakharia</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1">Devavrat Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chavan_M/0/1/0/all/0/1">Meenal Chavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonawane_D/0/1/0/all/0/1">Dhananjay Sonawane</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_B/0/1/0/all/0/1">Bhrigu Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazaheri_P/0/1/0/all/0/1">Parsa Mazaheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_I/0/1/0/all/0/1">Ian Lane</a></p>
<p>Large Language Models (LLMs) are adept at text manipulation -- tasks such as
machine translation and text summarization. However, these models can also be
prone to hallucination, which can be detrimental to the faithfulness of any
answers that the model provides. Recent works in combating hallucinations in
LLMs deal with identifying hallucinated sentences and categorizing the
different ways in which models hallucinate. This paper takes a deep dive into
LLM behavior with respect to hallucinations, defines a token-level approach to
identifying different kinds of hallucinations, and further utilizes this
token-level tagging to improve the interpretability and faithfulness of LLMs in
dialogue summarization tasks. Through this, the paper presents a new, enhanced
dataset and a new training paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14423">Efficacy of Machine-Generated Instructions. (arXiv:2312.14423v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gulati_S/0/1/0/all/0/1">Samaksh Gulati</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1">Anshit Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1">Manoj Parmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_P/0/1/0/all/0/1">Palash Chaudhary</a></p>
<p>Large "instruction-tuned" language models (i.e., finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is often limited in quantity, diversity, and creativity, therefore
hindering the generality of the tuned model. We conducted a quantitative study
to figure out the efficacy of machine-generated annotations, where we compare
the results of a fine-tuned BERT model with human v/s machine-generated
annotations. Applying our methods to the vanilla GPT-3 model, we saw that
machine generated annotations were 78.54% correct and the fine-tuned model
achieved a 96.01% model performance compared to the performance with
human-labelled annotations. This result shows that machine-generated
annotations are a resource and cost effective way to fine-tune down-stream
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14480">MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models. (arXiv:2312.14480v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongyin Zhu</a></p>
<p>Large language models (LLMs) are increasingly being used in Metaverse
environments to generate dynamic and realistic content and to control the
behavior of non-player characters (NPCs). However, the cybersecurity concerns
associated with LLMs have become increasingly prominent. Previous research has
primarily focused on patching system vulnerabilities to enhance cybersecurity,
but these approaches are not well-suited to the Metaverse, where the virtual
space is more complex, LLMs are vulnerable, and ethical user interaction is
critical. Moreover, the scope of cybersecurity in the Metaverse is expected to
expand significantly. This paper proposes a method for enhancing cybersecurity
through the simulation of user interaction with LLMs. Our goal is to educate
users and strengthen their defense capabilities through exposure to a
comprehensive simulation system. This system includes extensive Metaverse
cybersecurity Q&amp;A and attack simulation scenarios. By engaging with these,
users will improve their ability to recognize and withstand risks.
Additionally, to address the ethical implications of user input, we propose
using LLMs as evaluators to assess user content across five dimensions. We
further adapt the models through vocabulary expansion training to better
understand personalized inputs and emoticons. We conduct experiments on
multiple LLMs and find that our approach is effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14488">Language Model is a Branch Predictor for Simultaneous Machine Translation. (arXiv:2312.14488v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1">Aoxiong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Tianyun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>The primary objective of simultaneous machine translation (SiMT) is to
minimize latency while preserving the quality of the final translation. Drawing
inspiration from CPU branch prediction techniques, we propose incorporating
branch prediction techniques in SiMT tasks to reduce translation latency.
Specifically, we utilize a language model as a branch predictor to predict
potential branch directions, namely, future source words. Subsequently, we
utilize the predicted source words to decode the output in advance. When the
actual source word deviates from the predicted source word, we use the real
source word to decode the output again, replacing the predicted output. To
further reduce computational costs, we share the parameters of the encoder and
the branch predictor, and utilize a pre-trained language model for
initialization. Our proposed method can be seamlessly integrated with any SiMT
model. Extensive experimental results demonstrate that our approach can improve
translation quality and latency at the same time. Our code is available at
https://github.com/YinAoXiong/simt_branch_predictor .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14504">Theory of Hallucinations based on Equivariance. (arXiv:2312.14504v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shibata_H/0/1/0/all/0/1">Hisaichi Shibata</a></p>
<p>Equivariance is an important feature in machine learning, including language
models. It ensures that any sequences of phrases with the same meanings are
interpreted consistently. For example, the sentence 'There is a cat on the
table' should be interpreted by language models as it is, regardless of
variations in its token-level expression. Building on this insight, I propose a
new theory suggesting that insufficient equivariance in language models can
lead to hallucinations. According to this theory, which is both intuitive and
novel, language models trained on relatively small datasets tend to
misinterpret input texts and/or generate incorrect texts (i.e.,
hallucinations). To test this theory, I developed a toy model known as 'dancing
men', which is a character-level substitution cipher. Additionally, I propose a
novel technique based on the T5 (Text To Text Transfer Transformer) model to
efficiently decipher these codes without relying on frequency analysis. I have
found that this T5 model can almost completely solve the cipher, demonstrating
its ability to acquire equivariance in this frame. This method could be scaled
up to word-level and sentence-level substitution ciphers, analogous to large
language models without tokenizers or dictionaries. This scalability makes it
suitable for investigating the proposed link between inadequate equivariance
acquisition and the emergence of hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14542">Automatic Data Retrieval for Cross Lingual Summarization. (arXiv:2312.14542v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_N/0/1/0/all/0/1">Nikhilesh Bhatnagar</a>, <a href="http://arxiv.org/find/cs/1/au:+Urlana_A/0/1/0/all/0/1">Ashok Urlana</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujadia_V/0/1/0/all/0/1">Vandan Mujadia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1">Pruthwik Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1">Dipti Misra Sharma</a></p>
<p>Cross-lingual summarization involves the summarization of text written in one
language to a different one. There is a body of research addressing
cross-lingual summarization from English to other European languages. In this
work, we aim to perform cross-lingual summarization from English to Hindi. We
propose pairing up the coverage of newsworthy events in textual and video
format can prove to be helpful for data acquisition for cross lingual
summarization. We analyze the data and propose methods to match articles to
video descriptions that serve as document and summary pairs. We also outline
filtering methods over reasonable thresholds to ensure the correctness of the
summaries. Further, we make available 28,583 mono and cross-lingual
article-summary pairs https://github.com/tingc9/Cross-Sum-News-Aligned. We also
build and analyze multiple baselines on the collected data and report error
analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14557">Aurora:Activating Chinese chat capability for Mistral-8x7B sparse Mixture-of-Experts through Instruction-Tuning. (arXiv:2312.14557v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rongsheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ruizhe Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yaofei Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1">Kunyan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Han Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiaxi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_P/0/1/0/all/0/1">Patrick Cheong-Iao Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yapeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tao Tan</a></p>
<p>Existing research has demonstrated that refining large language models (LLMs)
through the utilization of machine-generated instruction-following data
empowers these models to exhibit impressive zero-shot capabilities for novel
tasks, without requiring human-authored instructions. In this paper, we
systematically investigate, preprocess, and integrate three Chinese
instruction-following datasets with the aim of enhancing the Chinese
conversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.
Through instruction fine-tuning on this carefully processed dataset, we
successfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named
"Aurora." To assess the performance of Aurora, we utilize three widely
recognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate
the effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse
Mixture-of-Experts model. This work is pioneering in the execution of
instruction fine-tuning on a sparse expert-mixed model, marking a significant
breakthrough in enhancing the capabilities of this model architecture. Our
code, data and model are publicly available at:
https://github.com/WangRongsheng/Aurora
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14590">SIG: Speaker Identification in Literature via Prompt-Based Generation. (arXiv:2312.14590v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhenlin Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liyan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huangfu_M/0/1/0/all/0/1">Mingdu Huangfu</a></p>
<p>Identifying speakers of quotations in narratives is an important task in
literary analysis, with challenging scenarios including the out-of-domain
inference for unseen speakers, and non-explicit cases where there are no
speaker mentions in surrounding context. In this work, we propose a simple and
effective approach SIG, a generation-based method that verbalizes the task and
quotation input based on designed prompt templates, which also enables easy
integration of other auxiliary tasks that further bolster the speaker
identification performance. The prediction can either come from direct
generation by the model, or be determined by the highest generation probability
of each speaker candidate. Based on our approach design, SIG supports
out-of-domain evaluation, and achieves open-world classification paradigm that
is able to accept any forms of candidate input. We perform both cross-domain
evaluation and in-domain evaluation on PDNC, the largest dataset of this task,
where empirical results suggest that SIG outperforms previous baselines of
complicated designs, as well as the zero-shot ChatGPT, especially excelling at
those hard non-explicit scenarios by up to 17% improvement. Additional
experiments on another dataset WP further corroborate the efficacy of SIG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14591">Reasons to Reject? Aligning Language Models with Judgments. (arXiv:2312.14591v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Deng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhisong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1">Wai Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a></p>
<p>As humans, we consistently engage in interactions with our peers and receive
feedback in the form of natural language. This language feedback allows us to
reflect on our actions, maintain appropriate behavior, and rectify our errors.
The question arises naturally: can we use language feedback to align large
language models (LLMs)? In contrast to previous research that aligns LLMs with
reward or preference data, we present the first systematic exploration of
alignment through the lens of language feedback (i.e., judgment). We commence
with an in-depth investigation of potential methods that can be adapted for
aligning LLMs with judgments, revealing that these methods are unable to fully
capitalize on the judgments. To facilitate more effective utilization of
judgments, we propose a novel framework, Contrastive Unlikelihood Training
(CUT), that allows for fine-grained inappropriate content detection and
correction based on judgments. Our offline alignment results show that, with
merely 1317 off-the-shelf judgment data, CUT (LLaMA2-13b) can beat the 175B
DaVinci003 and surpass the best baseline by 52.34 points on AlpacaEval. The
online alignment results demonstrate that CUT can align LLMs (LLaMA2-chat-13b)
in an iterative fashion using model-specific judgment data, with a steady
performance improvement from 81.09 to 91.36 points on AlpacaEval. Our analysis
further suggests that judgments exhibit greater potential than rewards for LLM
alignment and warrant future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14609">BLSTM-Based Confidence Estimation for End-to-End Speech Recognition. (arXiv:2312.14609v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ogawa_A/0/1/0/all/0/1">Atsunori Ogawa</a>, <a href="http://arxiv.org/find/eess/1/au:+Tawara_N/0/1/0/all/0/1">Naohiro Tawara</a>, <a href="http://arxiv.org/find/eess/1/au:+Kano_T/0/1/0/all/0/1">Takatomo Kano</a>, <a href="http://arxiv.org/find/eess/1/au:+Delcroix_M/0/1/0/all/0/1">Marc Delcroix</a></p>
<p>Confidence estimation, in which we estimate the reliability of each
recognized token (e.g., word, sub-word, and character) in automatic speech
recognition (ASR) hypotheses and detect incorrectly recognized tokens, is an
important function for developing ASR applications. In this study, we perform
confidence estimation for end-to-end (E2E) ASR hypotheses. Recent E2E ASR
systems show high performance (e.g., around 5% token error rates) for various
ASR tasks. In such situations, confidence estimation becomes difficult since we
need to detect infrequent incorrect tokens from mostly correct token sequences.
To tackle this imbalanced dataset problem, we employ a bidirectional long
short-term memory (BLSTM)-based model as a strong binary-class
(correct/incorrect) sequence labeler that is trained with a class balancing
objective. We experimentally confirmed that, by utilizing several types of ASR
decoding scores as its auxiliary features, the model steadily shows high
confidence estimation performance under highly imbalanced settings. We also
confirmed that the BLSTM-based model outperforms Transformer-based confidence
estimation models, which greatly underestimate incorrect tokens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14646">Collaborative Synthesis of Patient Records through Multi-Visit Health State Inference. (arXiv:2312.14646v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hongda Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hongzhan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Electronic health records (EHRs) have become the foundation of machine
learning applications in healthcare, while the utility of real patient records
is often limited by privacy and security concerns. Synthetic EHR generation
provides an additional perspective to compensate for this limitation. Most
existing methods synthesize new records based on real EHR data, without
consideration of different types of events in EHR data, which cannot control
the event combinations in line with medical common sense. In this paper, we
propose MSIC, a Multi-visit health Status Inference model for Collaborative EHR
synthesis to address these limitations. First, we formulate the synthetic EHR
generation process as a probabilistic graphical model and tightly connect
different types of events by modeling the latent health states. Then, we derive
a health state inference method tailored for the multi-visit scenario to
effectively utilize previous records to synthesize current and future records.
Furthermore, we propose to generate medical reports to add textual descriptions
for each medical event, providing broader applications for synthesized EHR
data. For generating different paragraphs in each visit, we incorporate a
multi-generator deliberation framework to collaborate the message passing of
multiple generators and employ a two-phase decoding strategy to generate
high-quality reports. Our extensive experiments on the widely used benchmarks,
MIMIC-III and MIMIC-IV, demonstrate that MSIC advances state-of-the-art results
on the quality of synthetic data while maintaining low privacy risks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14708">Balancing the Style-Content Trade-Off in Sentiment Transfer Using Polarity-Aware Denoising. (arXiv:2312.14708v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Sourabrata Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasner_Z/0/1/0/all/0/1">Zden&#x11b;k Kasner</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1">Ond&#x159;ej Du&#x161;ek</a></p>
<p>Text sentiment transfer aims to flip the sentiment polarity of a sentence
(positive to negative or vice versa) while preserving its sentiment-independent
content. Although current models show good results at changing the sentiment,
content preservation in transferred sentences is insufficient. In this paper,
we present a sentiment transfer model based on polarity-aware denoising, which
accurately controls the sentiment attributes in generated text, preserving the
content to a great extent and helping to balance the style-content trade-off.
Our proposed model is structured around two key stages in the sentiment
transfer process: better representation learning using a shared encoder and
sentiment-controlled generation using separate sentiment-specific decoders.
Empirical results show that our methods outperforms state-of-the-art baselines
in terms of content preservation while staying competitive in terms of style
transfer accuracy and fluency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14737">Computational Semantics and Evaluation Benchmark for Interrogative Sentences via Combinatory Categorial Grammar. (arXiv:2312.14737v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Funakura_H/0/1/0/all/0/1">Hayate Funakura</a>, <a href="http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1">Koji Mineshima</a></p>
<p>We present a compositional semantics for various types of polar questions and
wh-questions within the framework of Combinatory Categorial Grammar (CCG). To
assess the explanatory power of our proposed analysis, we introduce a
question-answering dataset QSEM specifically designed to evaluate the semantics
of interrogative sentences. We implement our analysis using existing CCG
parsers and conduct evaluations using the dataset. Through the evaluation, we
have obtained annotated data with CCG trees and semantic representations for
about half of the samples included in QSEM. Furthermore, we discuss the
discrepancy between the theoretical capacity of CCG and the capabilities of
existing CCG parsers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14769">Large Language Model (LLM) Bias Index -- LLMBI. (arXiv:2312.14769v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1">Abiodun Finbarrs Oketunji</a>, <a href="http://arxiv.org/find/cs/1/au:+Anas_M/0/1/0/all/0/1">Muhammad Anas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saina_D/0/1/0/all/0/1">Deepthi Saina</a></p>
<p>The Large Language Model Bias Index (LLMBI) is a pioneering approach designed
to quantify and address biases inherent in large language models (LLMs), such
as GPT-4. We recognise the increasing prevalence and impact of LLMs across
diverse sectors. This research introduces a novel metric, LLMBI, to
systematically measure and mitigate biases potentially skewing model responses.
We formulated LLMBI using a composite scoring system incorporating multiple
dimensions of bias, including but not limited to age, gender, and racial
biases.
</p>
<p>To operationalise this metric, we engaged in a multi-step process involving
collecting and annotating LLM responses, applying sophisticated Natural
Language Processing (NLP) techniques for bias detection, and computing the
LLMBI score through a specially crafted mathematical formula. The formula
integrates weighted averages of various bias dimensions, a penalty for dataset
diversity deficiencies, and a correction for sentiment biases. Our empirical
analysis, conducted using responses from OpenAI's API, employs advanced
sentiment analysis as a representative method for bias detection.
</p>
<p>The research reveals LLMs, whilst demonstrating impressive capabilities in
text generation, exhibit varying degrees of bias across different dimensions.
LLMBI provides a quantifiable measure to compare biases across models and over
time, offering a vital tool for systems engineers, researchers and regulators
in enhancing the fairness and reliability of LLMs. It highlights the potential
of LLMs in mimicking unbiased human-like responses. Additionally, it
underscores the necessity of continuously monitoring and recalibrating such
models to align with evolving societal norms and ethical standards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14798">Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases. (arXiv:2312.14798v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eyal_B/0/1/0/all/0/1">Ben Eyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachar_A/0/1/0/all/0/1">Amir Bachar</a>, <a href="http://arxiv.org/find/cs/1/au:+Haroche_O/0/1/0/all/0/1">Ophir Haroche</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1">Michael Elhadad</a></p>
<p>Large Language Models (LLMs) have spurred progress in text-to-SQL, the task
of generating SQL queries from natural language questions based on a given
database schema. Despite the declarative nature of SQL, it continues to be a
complex programming language. In this paper, we investigate the potential of an
alternative query language with simpler syntax and modular specification of
complex queries. The purpose is to create a query language that can be learned
more easily by modern neural semantic parsing architectures while also enabling
non-programmers to better assess the validity of the query plans produced by an
interactive query plan assistant.
</p>
<p>The proposed alternative query language is called Query Plan Language (QPL).
It is designed to be modular and can be translated into a restricted form of
SQL Common Table Expressions (CTEs). The aim of QPL is to make complex data
retrieval accessible to non-programmers by allowing users to express their
questions in natural language while also providing an easier-to-verify target
language. The paper demonstrates how neural LLMs can benefit from QPL's
modularity to generate complex query plans in a compositional manner. This
involves a question decomposition strategy and a planning stage.
</p>
<p>We conduct experiments on a version of the Spider text-to-SQL dataset that
has been converted to QPL. The hierarchical structure of QPL programs enables
us to measure query complexity naturally. Based on this assessment, we identify
the low accuracy of existing text-to-SQL systems on complex compositional
queries. We present ways to address the challenge of complex queries in an
iterative, user-controlled manner, using fine-tuned LLMs and a variety of
prompting strategies in a compositional manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14845">On the Use of Metaphor Translation in Psychiatry. (arXiv:2312.14845v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1">Lois Wong</a></p>
<p>Providing mental healthcare to individuals with limited English proficiency
(LEP) remains a pressing problem within psychiatry. Because the majority of
individuals trained in providing psychiatric care are English speakers, the
quality of mental healthcare given to LEP patients is significantly lower than
that provided for English speakers. The provision of mental healthcare is
contingent on communication and understanding between the patient and
healthcare provider, much more so than in the realm of physical healthcare, and
English speakers are often unable to comprehend figurative language such as
metaphors used by LEPs. Hence, Figurative Language Translation is invaluable to
providing equitable psychiatric care. Now, metaphor has been shown to be
paramount in both identifying individuals struggling with mental problems and
helping those individuals understand and communicate their experiences.
Therefore, this paper aims to survey the potential of Machine Translation for
providing equitable psychiatric healthcare and highlights the need for further
research on the transferability of existing machine and metaphor translation
research in the domain of psychiatry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14862">YAYI 2: Multilingual Open-Source Large Language Models. (arXiv:2312.14862v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1">Qingchao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jia Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1">Bao Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_B/0/1/0/all/0/1">Baoyu Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Donglei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1">Fan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1">Feifei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hailong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hanxuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Haojun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianbin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jiangtao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junfeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liduo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Lifeng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lili Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Minzheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Ping Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qingxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1">Rui Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruiqun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Taiwen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaodong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaofei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xina Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1">Xing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xinglin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yanni Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Ying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yongyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yungan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangsheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhaoxin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Wenji Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Dajun Zeng</a></p>
<p>As the latest advancements in natural language processing, large language
models (LLMs) have achieved human-level language understanding and generation
abilities in many real-world tasks, and even have been regarded as a potential
path to the artificial general intelligence. To better facilitate research on
LLMs, many open-source LLMs, such as Llama 2 and Falcon, have recently been
proposed and gained comparable performances to proprietary models. However,
these models are primarily designed for English scenarios and exhibit poor
performances in Chinese contexts. In this technical report, we propose YAYI 2,
including both base and chat models, with 30 billion parameters. YAYI 2 is
pre-trained from scratch on a multilingual corpus which contains 2.65 trillion
tokens filtered by our pre-training data processing pipeline. The base model is
aligned with human values through supervised fine-tuning with millions of
instructions and reinforcement learning from human feedback. Extensive
experiments on multiple benchmarks, such as MMLU and CMMLU, consistently
demonstrate that the proposed YAYI 2 outperforms other similar sized
open-source models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14867">VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation. (arXiv:2312.14867v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ku_M/0/1/0/all/0/1">Max Ku</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Dongfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Cong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a></p>
<p>In the rapidly advancing field of conditional image generation research,
challenges such as limited explainability lie in effectively evaluating the
performance and capabilities of various models. This paper introduces VIESCORE,
a Visual Instruction-guided Explainable metric for evaluating any conditional
image generation tasks. VIESCORE leverages general knowledge from Multimodal
Large Language Models (MLLMs) as the backbone and does not require training or
fine-tuning. We evaluate VIESCORE on seven prominent tasks in conditional image
tasks and found: (1) VIESCORE (GPT4-v) achieves a high Spearman correlation of
0.3 with human evaluations, while the human-to-human correlation is 0.45. (2)
VIESCORE (with open-source MLLM) is significantly weaker than GPT-4v in
evaluating synthetic images. (3) VIESCORE achieves a correlation on par with
human ratings in the generation tasks but struggles in editing tasks. With
these results, we believe VIESCORE shows its great potential to replace human
judges in evaluating image synthesis tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14870">Numerical Reasoning for Financial Reports. (arXiv:2312.14870v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1">Abhinav Arun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhiman_A/0/1/0/all/0/1">Ashish Dhiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1">Mehul Soni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yibei Hu</a></p>
<p>Financial reports offer critical insights into a company's operations, yet
their extensive length typically spanning 30 40 pages poses challenges for
swift decision making in dynamic markets. To address this, we leveraged
finetuned Large Language Models (LLMs) to distill key indicators and
operational metrics from these reports basis questions from the user. We
devised a method to locate critical data, and leverage the FinQA dataset to
fine-tune both Llama-2 7B and T5 models for customized question answering. We
achieved results comparable to baseline on the final numerical answer, a
competitive accuracy in numerical reasoning and calculation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14877">Robust Knowledge Extraction from Large Language Models using Social Choice Theory. (arXiv:2312.14877v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1">Nico Potyka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuqicheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yunjie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1">Evgeny Kharlamov</a>, <a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1">Steffen Staab</a></p>
<p>Large-language models (LLMs) have the potential to support a wide range of
applications like conversational agents, creative writing, text improvement,
and general query answering. However, they are ill-suited for query answering
in high-stake domains like medicine because they generate answers at random and
their answers are typically not robust - even the same query can result in
different answers when prompted multiple times. In order to improve the
robustness of LLM queries, we propose using ranking queries repeatedly and to
aggregate the queries using methods from social choice theory. We study ranking
queries in diagnostic settings like medical and fault diagnosis and discuss how
the Partial Borda Choice function from the literature can be applied to merge
multiple query results. We discuss some additional interesting properties in
our setting and evaluate the robustness of our approach empirically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14890">NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lizhou Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Haoyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1">Libby Hemphill</a></p>
<p>Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07861">Text normalization for low-resource languages: the case of Ligurian. (arXiv:2206.07861v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lusito_S/0/1/0/all/0/1">Stefano Lusito</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1">Edoardo Ferrante</a>, <a href="http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1">Jean Maillard</a></p>
<p>Text normalization is a crucial technology for low-resource languages which
lack rigid spelling conventions or that have undergone multiple spelling
reforms. Low-resource text normalization has so far relied upon hand-crafted
rules, which are perceived to be more data efficient than neural methods. In
this paper we examine the case of text normalization for Ligurian, an
endangered Romance language. We collect 4,394 Ligurian sentences paired with
their normalized versions, as well as the first open source monolingual corpus
for Ligurian. We show that, in spite of the small amounts of data available, a
compact transformer-based model can be trained to achieve very low error rates
by the use of backtranslation and appropriate tokenization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.07662">NELLIE: A Neuro-Symbolic Inference Engine for Grounded, Compositional, and Explainable Reasoning. (arXiv:2209.07662v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1">Nathaniel Weir</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a></p>
<p>Our goal is a modern approach to answering questions via systematic reasoning
where answers are supported by human interpretable proof trees grounded in an
NL corpus of authoritative facts. Such a system would help alleviate the
challenges of interpretability and hallucination with modern LMs, and the lack
of grounding of current explanation methods (e.g., Chain-of-Thought). This
paper proposes a new take on Prolog-based inference engines, where we replace
handcrafted rules with a combination of neural language modeling, guided
generation, and semiparametric dense retrieval. Our implementation, NELLIE, is
the first system to demonstrate fully interpretable, end-to-end grounded QA as
entailment tree proof search, going beyond earlier work explaining
known-to-be-true facts from text. In experiments, NELLIE outperforms a
similar-sized state-of-the-art reasoner [Tafjord et al., 2022] while producing
knowledge-grounded explanations. We also find NELLIE can exploit both
semi-structured and NL text corpora to guide reasoning. Together these suggest
a new way to jointly reap the benefits of both modern neural methods and
traditional symbolic reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11997">Prompt-Based Editing for Text Style Transfer. (arXiv:2301.11997v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1">Guoqing Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yu Tong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1">Lili Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Firdaus_M/0/1/0/all/0/1">Mauajama Firdaus</a></p>
<p>Prompting approaches have been recently explored in text style transfer,
where a textual prompt is used to query a pretrained language model to generate
style-transferred texts word by word in an autoregressive manner. However, such
a generation process is less controllable and early prediction errors may
affect future word predictions. In this paper, we present a prompt-based
editing approach for text style transfer. Specifically, we prompt a pretrained
language model for style classification and use the classification probability
to compute a style score. Then, we perform discrete search with word-level
editing to maximize a comprehensive scoring function for the style-transfer
task. In this way, we transform a prompt-based generation problem into a
classification one, which is a training-free process and more controllable than
the autoregressive generation of sentences. In our experiments, we performed
both automatic and human evaluation on three style-transfer benchmark datasets,
and show that our approach largely outperforms the state-of-the-art systems
that have 20 times more parameters. Additional empirical analyses further
demonstrate the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13001">Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Mingyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haiyun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Songfang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shilong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huafeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Liping Jing</a></p>
<p>The emergence of ChatGPT has recently garnered significant attention from the
computational linguistics community. To demonstrate its capabilities as a
keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the
keyphrase generation task. We evaluate its performance in various aspects,
including keyphrase generation prompts, keyphrase generation diversity, and
long document understanding. Our evaluation is based on six benchmark datasets,
and we adopt the prompt suggested by OpenAI while extending it to six candidate
prompts. We find that ChatGPT performs exceptionally well on all six candidate
prompts, with minor performance differences observed across the datasets. Based
on our findings, we conclude that ChatGPT has great potential for keyphrase
generation. Moreover, we discover that ChatGPT still faces challenges when it
comes to generating absent keyphrases. Meanwhile, in the final section, we also
present some limitations and future expansions of this report.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14171">In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models. (arXiv:2305.14171v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1">Afra Amini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1">Massimiliano Ciaramita</a></p>
<p>Large language models are able to learn new tasks in context, where they are
provided with instructions and a few annotated examples. However, the
effectiveness of in-context learning is dependent on the provided context, and
the performance on a downstream task can vary considerably, depending on the
instruction. Importantly, such dependency on the context can surface in
unpredictable ways, e.g., a seemingly more informative instruction might lead
to a worse performance. In this paper, we propose an alternative approach,
which we term In-Context Probing (ICP). Similar to in-context learning, we
contextualize the representation of the input with an instruction, but instead
of decoding the output prediction, we probe the contextualized representation
to predict the label. Through a series of experiments on a diverse set of
classification tasks, we show that in-context probing is significantly more
robust to changes in instructions. We further show that ICP performs
competitive or superior to finetuning and can be particularly helpful to build
classifiers on top of smaller models, with less than a hundred training
examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19228">Unsupervised Melody-to-Lyric Generation. (arXiv:2305.19228v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yufei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1">Anjali Narayan-Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1">Shereen Oraby</a>, <a href="http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1">Alessandra Cervone</a>, <a href="http://arxiv.org/find/cs/1/au:+Sigurdsson_G/0/1/0/all/0/1">Gunnar Sigurdsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Chenyang Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenbo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1">Tagyoung Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Automatic melody-to-lyric generation is a task in which song lyrics are
generated to go with a given melody. It is of significant practical interest
and more challenging than unconstrained lyric generation as the music imposes
additional constraints onto the lyrics. The training data is limited as most
songs are copyrighted, resulting in models that underfit the complicated
cross-modal relationship between melody and lyrics. In this work, we propose a
method for generating high-quality lyrics without training on any aligned
melody-lyric data. Specifically, we design a hierarchical lyric generation
framework that first generates a song outline and second the complete lyrics.
The framework enables disentanglement of training (based purely on text) from
inference (melody-guided text generation) to circumvent the shortage of
parallel data.
</p>
<p>We leverage the segmentation and rhythm alignment between melody and lyrics
to compile the given melody into decoding constraints as guidance during
inference. The two-step hierarchical design also enables content control via
the lyric outline, a much-desired feature for democratizing collaborative song
creation. Experimental results show that our model can generate high-quality
lyrics that are more on-topic, singable, intelligible, and coherent than strong
baselines, for example SongMASS, a SOTA model trained on a parallel dataset,
with a 24% relative overall quality improvement based on human ratings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15774">Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang &#x27;Anthony&#x27; Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Burke_J/0/1/0/all/0/1">Jeff Burke</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1">Ruofei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1">Matthew K. Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_J/0/1/0/all/0/1">Jennifer Jacobs</a>, <a href="http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1">Philippe Laban</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dingzeyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1">Karl D. D. Willis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chien-Sheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bolei Zhou</a></p>
<p>Through iterative, cross-disciplinary discussions, we define and propose
next-steps for Human-centered Generative AI (HGAI). We contribute a
comprehensive research agenda that lays out future directions of Generative AI
spanning three levels: aligning with human values; assimilating human intents;
and augmenting human abilities. By identifying these next-steps, we intend to
draw interdisciplinary research teams to pursue a coherent set of emergent
ideas in HGAI, focusing on their interested topics while maintaining a coherent
big picture of the future work landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05707">Guiding Language Model Reasoning with Planning Tokens. (arXiv:2310.05707v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1">Lucas Caccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostapenko_O/0/1/0/all/0/1">Oleksiy Ostapenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xingdi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1">Alessandro Sordoni</a></p>
<p>Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
reasoning. However, most of the existing approaches to enhance this ability
rely heavily on data-driven methods, while neglecting the structural aspects of
the model's reasoning capacity. We find that while LLMs can manage individual
reasoning steps well, they struggle with maintaining consistency across an
entire reasoning chain. To solve this, we introduce 'planning tokens' at the
start of each reasoning step, serving as a guide for the model. These token
embeddings are then fine-tuned along with the rest of the model parameters. Our
approach requires a negligible increase in trainable parameters (just 0.001%)
and can be applied through either full fine-tuning or a more
parameter-efficient scheme. We demonstrate our method's effectiveness by
applying it to three different LLMs, showing notable accuracy improvements
across three math word problem datasets w.r.t. plain chain-of-thought
fine-tuning baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05782">Aligning Language Models with Human Preferences via a Bayesian Approach. (arXiv:2310.05782v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiashuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haozhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjie Li</a></p>
<p>In the quest to advance human-centric natural language generation (NLG)
systems, ensuring alignment between NLG models and human preferences is
crucial. For this alignment, current popular methods leverage a reinforcement
learning (RL) approach with a reward model trained on feedback from humans.
However, inherent disagreements due to the subjective nature of human
preferences pose a significant challenge for training the reward model,
resulting in a deterioration of the NLG performance. To tackle this issue,
previous approaches typically rely on majority voting or averaging to
consolidate multiple inconsistent preferences into a merged one. Although
straightforward to understand and execute, such methods suffer from an
inability to capture the nuanced degrees of disaggregation among humans and may
only represent a specialized subset of individuals, thereby lacking the ability
to quantitatively disclose the universality of human preferences. To address
this challenge, this paper proposes a novel approach, which employs a Bayesian
framework to account for the distribution of disagreements among human
preferences as training a preference model, and names it as d-PM. Besides,
considering the RL strategy's inefficient and complex training process over the
training efficiency, we further propose utilizing the contrastive learning
strategy to train the NLG model with the preference scores derived from the
d-PM model. Extensive experiments on two human-centric NLG tasks, i.e.,
emotional support conversation and integrity "Rule-of-Thumb" generation, show
that our method consistently exceeds previous SOTA models in both automatic and
human evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12794">Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization. (arXiv:2310.12794v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Ningyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jingting Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Menghan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>Large language models (LLMs) have exhibited considerable cross-lingual
generalization abilities, whereby they implicitly transfer knowledge across
languages. However, the transfer is not equally successful for all languages,
especially for low-resource ones, which poses an ongoing challenge. It is
unclear whether we have reached the limits of implicit cross-lingual
generalization and if explicit knowledge transfer is viable. In this paper, we
investigate the potential for explicitly aligning conceptual correspondence
between languages to enhance cross-lingual generalization. Using the syntactic
aspect of language as a testbed, our analyses of 43 languages reveal a high
degree of alignability among the spaces of structural concepts within each
language for both encoder-only and decoder-only LLMs. We then propose a
meta-learning-based method to learn to align conceptual spaces of different
languages, which facilitates zero-shot and few-shot generalization in concept
classification and also offers insights into the cross-lingual in-context
learning phenomenon. Experiments on syntactic analysis tasks show that our
approach achieves competitive results with state-of-the-art methods and narrows
the performance gap between languages, particularly benefiting those with
limited resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12420">How Far Have We Gone in Vulnerability Detection Using Large Language Models. (arXiv:2311.12420v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zeyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>As software becomes increasingly complex and prone to vulnerabilities,
automated vulnerability detection is critically important, yet challenging.
Given the significant successes of large language models (LLMs) in various
tasks, there is growing anticipation of their efficacy in vulnerability
detection. However, a quantitative understanding of their potential in
vulnerability detection is still missing. To bridge this gap, we introduce a
comprehensive vulnerability benchmark VulBench. This benchmark aggregates
high-quality data from a wide range of CTF (Capture-the-Flag) challenges and
real-world applications, with annotations for each vulnerable function
detailing the vulnerability type and its root cause. Through our experiments
encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models
and static analyzers, we find that several LLMs outperform traditional deep
learning approaches in vulnerability detection, revealing an untapped potential
in LLMs. This work contributes to the understanding and utilization of LLMs for
enhanced software security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16502">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. (arXiv:2311.16502v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yuansheng Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruoqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1">Samuel Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Dongfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Weiming Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Cong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Botao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Renliang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1">Ming Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Boyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhenzhu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a></p>
<p>We introduce MMMU: a new benchmark designed to evaluate multimodal models on
massive multi-discipline tasks demanding college-level subject knowledge and
deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal
questions from college exams, quizzes, and textbooks, covering six core
disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp;
Social Science, and Tech &amp; Engineering. These questions span 30 subjects and
183 subfields, comprising 30 highly heterogeneous image types, such as charts,
diagrams, maps, tables, music sheets, and chemical structures. Unlike existing
benchmarks, MMMU focuses on advanced perception and reasoning with
domain-specific knowledge, challenging models to perform tasks akin to those
faced by experts. The evaluation of 14 open-source LMMs as well as the
proprietary GPT-4V(ision) and Gemini highlights the substantial challenges
posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve
accuracies of 56% and 59% respectively, indicating significant room for
improvement. We believe MMMU will stimulate the community to build
next-generation multimodal foundation models towards expert artificial general
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07661">CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor. (arXiv:2312.07661v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shuyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Runjia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiuye Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyang Li</a></p>
<p>Existing open-vocabulary image segmentation methods require a fine-tuning
step on mask annotations and/or image-text datasets. Mask labels are
labor-intensive, which limits the number of categories in segmentation
datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is
severely reduced after fine-tuning. However, without fine-tuning, VLMs trained
under weak image-text supervision tend to make suboptimal mask predictions when
there are text queries referring to non-existing concepts in the image. To
alleviate these issues, we introduce a novel recurrent framework that
progressively filters out irrelevant texts and enhances mask quality without
training efforts. The recurrent unit is a two-stage segmenter built upon a VLM
with frozen weights. Thus, our model retains the VLM's broad vocabulary space
and strengthens its segmentation capability. Experimental results show that our
method outperforms not only the training-free counterparts, but also those
fine-tuned with millions of additional data samples, and sets new
state-of-the-art records for both zero-shot semantic and referring image
segmentation tasks. Specifically, we improve the current record by 28.8, 16.0,
and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13545">Developing Interactive Tourism Planning: A Dialogue Robot System Powered by a Large Language Model. (arXiv:2312.13545v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoshikawa_K/0/1/0/all/0/1">Katsumasa Yoshikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamazaki_T/0/1/0/all/0/1">Takato Yamazaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohagi_M/0/1/0/all/0/1">Masaya Ohagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mizumoto_T/0/1/0/all/0/1">Tomoya Mizumoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1">Keiya Sato</a></p>
<p>In recent years, large language models (LLMs) have rapidly proliferated and
have been utilized in various tasks, including research in dialogue systems. We
aimed to construct a system that not only leverages the flexible conversational
abilities of LLMs but also their advanced planning capabilities to reduce the
speaking load on human interlocutors and efficiently plan trips. Furthermore,
we propose a method that divides the complex task of a travel agency into
multiple subtasks, managing each as a separate phase to effectively accomplish
the task. Our proposed system confirmed a certain level of success by achieving
fourth place in the Dialogue Robot Competition 2023 preliminaries rounds. We
report on the challenges identified through the competition.
</p>
</p>
</div>

    </div>
    </body>
    