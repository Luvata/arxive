<!DOCTYPE html>
<html>
<head>
<title>2023-11-14-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.05629">Experimental Evidence on Negative Impact of Generative AI on Scientific Learning Outcomes. (arXiv:2311.05629v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1">Qirui Ju</a></p>
<p>In this study, I explored the impact of Generative AI on learning efficacy in
academic reading materials using experimental methods. College-educated
participants engaged in three cycles of reading and writing tasks. After each
cycle, they responded to comprehension questions related to the material. After
adjusting for background knowledge and demographic factors, complete reliance
on AI for writing tasks led to a 25.1% reduction in accuracy. In contrast,
AI-assisted reading resulted in a 12% decline. Interestingly, using AI for
summarization significantly improved both quality and output. Accuracy
exhibited notable variance in the AI-assisted section. Further analysis
revealed that individuals with a robust background in the reading topic and
superior reading/writing skills benefitted the most. I conclude the research by
discussing educational policy implications, emphasizing the need for educators
to warn students about the dangers of over-dependence on AI and provide
guidance on its optimal use in educational settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05632">The Impact of Performance Expectancy, Workload, Risk, and Satisfaction on Trust in ChatGPT: Cross-sectional Survey Analysis. (arXiv:2311.05632v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shamszare_H/0/1/0/all/0/1">Hamid Shamszare</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_A/0/1/0/all/0/1">Avishek Choudhury</a></p>
<p>This study investigated how perceived workload, satisfaction, performance
expectancy, and risk-benefit perception influenced users' trust in Chat
Generative Pre-Trained Transformer (ChatGPT). We aimed to understand the
nuances of user engagement and provide insights to improve future design and
adoption strategies for similar technologies. A semi-structured, web-based
survey was conducted among adults in the United States who actively use ChatGPT
at least once a month. The survey was conducted from 22nd February 2023 through
24th March 2023. We used structural equation modeling to understand the
relationships among the constructs of perceived workload, satisfaction,
performance expectancy, risk-benefit, and trust. The analysis of 607 survey
responses revealed a significant negative relationship between perceived
workload and user satisfaction, a negative but insignificant relationship
between perceived workload and trust, and a positive relationship between user
satisfaction and trust. Trust was also found to increase with performance
expectancy. In contrast, the relationship between the benefit-to-risk ratio of
using ChatGPT and trust was insignificant. The findings underscore the
importance of ensuring user-friendly design and functionality in AI-based
applications to reduce workload and enhance user satisfaction, thereby
increasing user trust. Future research should further explore the relationship
between the benefit-to-risk ratio and trust in the context of AI chatbots.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05657">Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs. (arXiv:2311.05657v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Da Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1">Abhilasha Ravichander</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1">Khyathi Chandu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bill Yuchen Lin</a></p>
<p>We introduce Lumos, a novel framework for training language agents that
employs a unified data format and a modular architecture based on open-source
large language models (LLMs). Lumos consists of three distinct modules:
planning, grounding, and execution. The planning module breaks down a task into
a series of high-level, tool-agnostic subgoals, which are then made specific by
the grounding module through a set of low-level actions. These actions are
subsequently executed by the execution module, utilizing a range of
off-the-shelf tools and APIs. In order to train these modules effectively,
high-quality annotations of subgoals and actions were collected and are made
available for fine-tuning open-source LLMs for various tasks such as complex
question answering, web tasks, and math problems. Leveraging this unified data
and modular design, Lumos not only achieves comparable or superior performance
to current, state-of-the-art agents, but also exhibits several key advantages:
(1) Lumos surpasses GPT-4/3.5-based agents in complex question answering and
web tasks, while equalling the performance of significantly larger LLM agents
on math tasks; (2) Lumos outperforms open-source agents created through
conventional training methods and those using chain-of-thoughts training; and
(3) Lumos is capable of effectively generalizing to unseen interactive tasks,
outperforming larger LLM-based agents and even exceeding performance of
specialized agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05659">Enhancing Instance-Level Image Classification with Set-Level Labels. (arXiv:2311.05659v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Aly A. Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Grossman_R/0/1/0/all/0/1">Robert L. Grossman</a></p>
<p>Instance-level image classification tasks have traditionally relied on
single-instance labels to train models, e.g., few-shot learning and transfer
learning. However, set-level coarse-grained labels that capture relationships
among instances can provide richer information in real-world scenarios. In this
paper, we present a novel approach to enhance instance-level image
classification by leveraging set-level labels. We provide a theoretical
analysis of the proposed method, including recognition conditions for fast
excess risk rate, shedding light on the theoretical foundations of our
approach. We conducted experiments on two distinct categories of datasets:
natural image datasets and histopathology image datasets. Our experimental
results demonstrate the effectiveness of our approach, showcasing improved
classification performance compared to traditional single-instance label-based
methods. Notably, our algorithm achieves 13% improvement in classification
accuracy compared to the strongest baseline on the histopathology image
classification benchmarks. Importantly, our experimental findings align with
the theoretical analysis, reinforcing the robustness and reliability of our
proposed method. This work bridges the gap between instance-level and set-level
image classification, offering a promising avenue for advancing the
capabilities of image classification models with set-level coarse-grained
labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05661">Prompt Engineering a Prompt Engineer. (arXiv:2311.05661v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qinyuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Axmed_M/0/1/0/all/0/1">Maxamed Axmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1">Reid Pryzant</a>, <a href="http://arxiv.org/find/cs/1/au:+Khani_F/0/1/0/all/0/1">Fereshte Khani</a></p>
<p>Prompt engineering is a challenging yet crucial task for optimizing the
performance of large language models (LLMs). It requires complex reasoning to
examine the model's errors, hypothesize what is missing or misleading in the
current prompt, and communicate the task with clarity. While recent works
indicate that LLMs can be meta-prompted to perform automatic prompt
engineering, their potentials may not be fully untapped due to the lack of
sufficient guidance to elicit complex reasoning capabilities in LLMs in the
meta-prompt. In this work, we investigate the problem of "prompt engineering a
prompt engineer" -- constructing a meta-prompt that more effectively guides
LLMs to perform automatic prompt engineering. We introduce and analyze key
components, such as a step-by-step reasoning template and context
specification, which lead to improved performance. In addition, inspired by
common optimization concepts such as batch size, step size and momentum, we
introduce their verbalized counterparts to the meta-prompt and investigate
their effects. Our final method, named PE2, finds a prompt that outperforms
"let's think step by step" by 6.3% on the MultiArith dataset and 3.1% on the
GSM8K dataset. To demonstrate its versatility, we apply PE2 to the Instruction
Induction benchmark, a suite of counterfactual tasks, and a lengthy, real-world
industrial prompt. In these settings, PE2 achieves strong performance and
outperforms prior automatic prompt engineering baselines. Further, we show that
PE2 makes meaningful and targeted prompt edits, amends erroneous or incomplete
prompts, and presents non-trivial counterfactual reasoning abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05662">An Experiment in Retrofitting Competency Questions for Existing Ontologies. (arXiv:2311.05662v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alharbi_R/0/1/0/all/0/1">Reham Alharbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamma_V/0/1/0/all/0/1">Valentina Tamma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grasso_F/0/1/0/all/0/1">Floriana Grasso</a>, <a href="http://arxiv.org/find/cs/1/au:+Payne_T/0/1/0/all/0/1">Terry Payne</a></p>
<p>Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05665">Explainable artificial intelligence for Healthcare applications using Random Forest Classifier with LIME and SHAP. (arXiv:2311.05665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panda_M/0/1/0/all/0/1">Mrutyunjaya Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahanta_S/0/1/0/all/0/1">Soumya Ranjan Mahanta</a></p>
<p>With the advances in computationally efficient artificial Intelligence (AI)
techniques and their numerous applications in our everyday life, there is a
pressing need to understand the computational details hidden in black box AI
techniques such as most popular machine learning and deep learning techniques;
through more detailed explanations. The origin of explainable AI (xAI) is
coined from these challenges and recently gained more attention by the
researchers by adding explainability comprehensively in traditional AI systems.
This leads to develop an appropriate framework for successful applications of
xAI in real life scenarios with respect to innovations, risk mitigation,
ethical issues and logical values to the users. In this book chapter, an
in-depth analysis of several xAI frameworks and methods including LIME (Local
Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive
exPlanations) are provided. Random Forest Classifier as black box AI is used on
a publicly available Diabetes symptoms dataset with LIME and SHAP for better
interpretations. The results obtained are interesting in terms of transparency,
valid and trustworthiness in diabetes disease prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05667">A theory for the sparsity emerged in the Forward Forward algorithm. (arXiv:2311.05667v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yukun Yang</a></p>
<p>This report explores the theory that explains the high sparsity phenomenon
\citep{tosato2023emergent} observed in the forward-forward algorithm
\citep{hinton2022forward}. The two theorems proposed predict the sparsity
changes of a single data point's activation in two cases: Theorem
\ref{theorem:1}: Decrease the goodness of the whole batch. Theorem
\ref{theorem:2}: Apply the complete forward forward algorithm to decrease the
goodness for negative data and increase the goodness for positive data. The
theory aligns well with the experiments tested on the MNIST dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05720">Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models. (arXiv:2311.05720v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1">Simon Stepputtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1">Joseph Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yaqi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhengyang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxin Sharon Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangreji_S/0/1/0/all/0/1">Sanketh Rangreji</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1">Michael Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1">Katia Sycara</a></p>
<p>Deception and persuasion play a critical role in long-horizon dialogues
between multiple parties, especially when the interests, goals, and motivations
of the participants are not aligned. Such complex tasks pose challenges for
current Large Language Models (LLM) as deception and persuasion can easily
mislead them, especially in long-horizon multi-party dialogues. To this end, we
explore the game of Avalon: The Resistance, a social deduction game in which
players must determine each other's hidden identities to complete their team's
objective. We introduce an online testbed and a dataset containing 20 carefully
collected and labeled games among human players that exhibit long-horizon
deception in a cooperative-competitive setting. We discuss the capabilities of
LLMs to utilize deceptive long-horizon conversations between six human players
to determine each player's goal and motivation. Particularly, we discuss the
multimodal integration of the chat between the players and the game's state
that grounds the conversation, providing further insights into the true player
identities. We find that even current state-of-the-art LLMs do not reach human
performance, making our dataset a compelling benchmark to investigate the
decision-making and language-processing capabilities of LLMs. Our dataset and
online testbed can be found at our project website:
https://sstepput.github.io/Avalon-NLU/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05740">Generating Pragmatic Examples to Train Neural Program Synthesizers. (arXiv:2311.05740v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1">Saujas Vaduguru</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1">Daniel Fried</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yewen Pu</a></p>
<p>Programming-by-example is the task of synthesizing a program that is
consistent with a set of user-provided input-output examples. As examples are
often an under-specification of one's intent, a good synthesizer must choose
the intended program from the many that are consistent with the given set of
examples. Prior work frames program synthesis as a cooperative game between a
listener (that synthesizes programs) and a speaker (a user choosing examples),
and shows that models of computational pragmatic inference are effective in
choosing the user intended programs. However, these models require
counterfactual reasoning over a large set of programs and examples, which is
infeasible in realistic program spaces. In this paper, we propose a novel way
to amortize this search with neural networks. We sample pairs of programs and
examples via self-play between listener and speaker models, and use pragmatic
inference to choose informative training examples from this sample.We then use
the informative dataset to train models to improve the synthesizer's ability to
disambiguate user-provided examples without human supervision. We validate our
method on the challenging task of synthesizing regular expressions from example
strings, and find that our method (1) outperforms models trained without
choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the
performance of supervised learning on a dataset of pragmatic examples provided
by humans, despite using no human data in training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05741">Efficiently Adapting Pretrained Language Models To New Languages. (arXiv:2311.05741v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Csaki_Z/0/1/0/all/0/1">Zoltan Csaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawakapan_P/0/1/0/all/0/1">Pian Pawakapan</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1">Urmish Thakker</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiantong Xu</a></p>
<p>Recent large language models (LLM) exhibit sub-optimal performance on
low-resource languages, as the training data of these models is usually
dominated by English and other high-resource languages. Furthermore, it is
challenging to train models for low-resource languages, especially from
scratch, due to a lack of high quality training data. Adapting pretrained LLMs
reduces the need for data in the new language while also providing cross
lingual transfer capabilities. However, naively adapting to new languages leads
to catastrophic forgetting and poor tokenizer efficiency. In this work, we
study how to efficiently adapt any existing pretrained LLM to a new language
without running into these issues. In particular, we improve the encoding
efficiency of the tokenizer by adding new tokens from the target language and
study the data mixing recipe to mitigate forgetting. Our experiments on
adapting an English LLM to Hungarian and Thai show that our recipe can reach
better performance than open source models on the target language, with minimal
regressions on English.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05742">Optimal simulation-based Bayesian decisions. (arXiv:2311.05742v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Alsing_J/0/1/0/all/0/1">Justin Alsing</a>, <a href="http://arxiv.org/find/stat/1/au:+Edwards_T/0/1/0/all/0/1">Thomas D. P. Edwards</a>, <a href="http://arxiv.org/find/stat/1/au:+Wandelt_B/0/1/0/all/0/1">Benjamin Wandelt</a></p>
<p>We present a framework for the efficient computation of optimal Bayesian
decisions under intractable likelihoods, by learning a surrogate model for the
expected utility (or its distribution) as a function of the action and data
spaces. We leverage recent advances in simulation-based inference and Bayesian
optimization to develop active learning schemes to choose where in parameter
and action spaces to simulate. This allows us to learn the optimal action in as
few simulations as possible. The resulting framework is extremely simulation
efficient, typically requiring fewer model calls than the associated posterior
inference task alone, and a factor of $100-1000$ more efficient than
Monte-Carlo based methods. Our framework opens up new capabilities for
performing Bayesian decision making, particularly in the previously challenging
regime where likelihoods are intractable, and simulations expensive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05746">Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models. (arXiv:2311.05746v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nwatu_J/0/1/0/all/0/1">Joan Nwatu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1">Oana Ignat</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1">Rada Mihalcea</a></p>
<p>Despite the impressive performance of current AI models reported across
various tasks, performance reports often do not include evaluations of how
these models perform on the specific groups that will be impacted by these
technologies. Among the minority groups under-represented in AI, data from
low-income households are often overlooked in data collection and model
evaluation. We evaluate the performance of a state-of-the-art vision-language
model (CLIP) on a geo-diverse dataset containing household images associated
with different income values (Dollar Street) and show that performance
inequality exists among households of different income levels. Our results
indicate that performance for the poorer groups is consistently lower than the
wealthier groups across various topics and countries. We highlight insights
that can help mitigate these issues and propose actionable steps for
economic-level inclusive AI development. Code is available at
https://github.com/MichiganNLP/Bridging_the_Digital_Divide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05754">Deep Natural Language Feature Learning for Interpretable Prediction. (arXiv:2311.05754v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Urrutia_F/0/1/0/all/0/1">Felipe Urrutia</a>, <a href="http://arxiv.org/find/cs/1/au:+Buc_C/0/1/0/all/0/1">Cristian Buc</a>, <a href="http://arxiv.org/find/cs/1/au:+Barriere_V/0/1/0/all/0/1">Valentin Barriere</a></p>
<p>We propose a general method to break down a main complex task into a set of
intermediary easier sub-tasks, which are formulated in natural language as
binary questions related to the final target task. Our method allows for
representing each example by a vector consisting of the answers to these
questions. We call this representation Natural Language Learned Features
(NLLF). NLLF is generated by a small transformer language model (e.g., BERT)
that has been trained in a Natural Language Inference (NLI) fashion, using weak
labels automatically obtained from a Large Language Model (LLM). We show that
the LLM normally struggles for the main task using in-context learning, but can
handle these easiest subtasks and produce useful weak labels to train a BERT.
The NLI-like training of the BERT allows for tackling zero-shot inference with
any binary question, and not necessarily the ones seen during the training. We
show that this NLLF vector not only helps to reach better performances by
enhancing any classifier, but that it can be used as input of an
easy-to-interpret machine learning model like a decision tree. This decision
tree is interpretable but also reaches high performances, surpassing those of a
pre-trained transformer in some cases.We have successfully applied this method
to two completely different tasks: detecting incoherence in students' answers
to open-ended mathematics exam questions, and screening abstracts for a
systematic literature review of scientific papers on climate change and
agroecology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05769">Chatbots Are Not Reliable Text Annotators. (arXiv:2311.05769v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kristensen_McLachlan_R/0/1/0/all/0/1">Ross Deans Kristensen-McLachlan</a>, <a href="http://arxiv.org/find/cs/1/au:+Canavan_M/0/1/0/all/0/1">Miceal Canavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kardos_M/0/1/0/all/0/1">M&#xe1;rton Kardos</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobsen_M/0/1/0/all/0/1">Mia Jacobsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Aaroe_L/0/1/0/all/0/1">Lene Aar&#xf8;e</a></p>
<p>Recent research highlights the significant potential of ChatGPT for text
annotation in social science research. However, ChatGPT is a closed-source
product which has major drawbacks with regards to transparency,
reproducibility, cost, and data protection. Recent advances in open-source (OS)
large language models (LLMs) offer alternatives which remedy these challenges.
This means that it is important to evaluate the performance of OS LLMs relative
to ChatGPT and standard approaches to supervised machine learning
classification. We conduct a systematic comparative evaluation of the
performance of a range of OS LLM models alongside ChatGPT, using both zero- and
few-shot learning as well as generic and custom prompts, with results compared
to more traditional supervised classification models. Using a new dataset of
Tweets from US news media, and focusing on simple binary text annotation tasks
for standard social science concepts, we find significant variation in the
performance of ChatGPT and OS models across the tasks, and that supervised
classifiers consistently outperform both. Given the unreliable performance of
ChatGPT and the significant challenges it poses to Open Science we advise
against using ChatGPT for substantive text annotation tasks in social science
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05772">ADaPT: As-Needed Decomposition and Planning with Language Models. (arXiv:2311.05772v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1">Archiki Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1">Alexander Koller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1">Mareike Hartmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1">Ashish Sabharwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1">Tushar Khot</a></p>
<p>Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05778">DONUT-hole: DONUT Sparsification by Harnessing Knowledge and Optimizing Learning Efficiency. (arXiv:2311.05778v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1">Azhar Shaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1">Michael Cochez</a>, <a href="http://arxiv.org/find/cs/1/au:+Diachkov_D/0/1/0/all/0/1">Denis Diachkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rijcke_M/0/1/0/all/0/1">Michiel de Rijcke</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousefi_S/0/1/0/all/0/1">Sahar Yousefi</a></p>
<p>This paper introduces DONUT-hole, a sparse OCR-free visual document
understanding (VDU) model that addresses the limitations of its predecessor
model, dubbed DONUT. The DONUT model, leveraging a transformer architecture,
overcoming the challenges of separate optical character recognition (OCR) and
visual semantic understanding (VSU) components. However, its deployment in
production environments and edge devices is hindered by high memory and
computational demands, particularly in large-scale request services. To
overcome these challenges, we propose an optimization strategy based on
knowledge distillation and model pruning. Our paradigm to produce DONUT-hole,
reduces the model denisty by 54\% while preserving performance. We also achieve
a global representational similarity index between DONUT and DONUT-hole based
on centered kernel alignment (CKA) metric of 0.79. Moreover, we evaluate the
effectiveness of DONUT-hole in the document image key information extraction
(KIE) task, highlighting its potential for developing more efficient VDU
systems for logistic companies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05784">Are &quot;Hierarchical&quot; Visual Representations Hierarchical?. (arXiv:2311.05784v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1">Ethan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a></p>
<p>Learned visual representations often capture large amounts of semantic
information for accurate downstream applications. Human understanding of the
world is fundamentally grounded in hierarchy. To mimic this and further improve
representation capabilities, the community has explored "hierarchical" visual
representations that aim at modeling the underlying hierarchy of the visual
world. In this work, we set out to investigate if hierarchical visual
representations truly capture the human perceived hierarchy better than
standard learned representations. To this end, we create HierNet, a suite of 12
datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet.
After extensive evaluation of Hyperbolic and Matryoshka Representations across
training setups, we conclude that they do not capture hierarchy any better than
the standard representations but can assist in other aspects like search
efficiency and interpretability. Our benchmark and the datasets are
open-sourced at https://github.com/ethanlshen/HierNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05790">The Paradox of Noise: An Empirical Study of Noise-Infusion Mechanisms to Improve Generalization, Stability, and Privacy in Federated Learning. (arXiv:2311.05790v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jafarigol_E/0/1/0/all/0/1">Elaheh Jafarigol</a>, <a href="http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1">Theodore Trafalis</a></p>
<p>In a data-centric era, concerns regarding privacy and ethical data handling
grow as machine learning relies more on personal information. This empirical
study investigates the privacy, generalization, and stability of deep learning
models in the presence of additive noise in federated learning frameworks. Our
main objective is to provide strategies to measure the generalization,
stability, and privacy-preserving capabilities of these models and further
improve them. To this end, five noise infusion mechanisms at varying noise
levels within centralized and federated learning settings are explored. As
model complexity is a key component of the generalization and stability of deep
learning models during training and evaluation, a comparative analysis of three
Convolutional Neural Network (CNN) architectures is provided. The paper
introduces Signal-to-Noise Ratio (SNR) as a quantitative measure of the
trade-off between privacy and training accuracy of noise-infused models, aiming
to find the noise level that yields optimal privacy and accuracy. Moreover, the
Price of Stability and Price of Anarchy are defined in the context of
privacy-preserving deep learning, contributing to the systematic investigation
of the noise infusion strategies to enhance privacy without compromising
performance. Our research sheds light on the delicate balance between these
critical factors, fostering a deeper understanding of the implications of
noise-based regularization in machine learning. By leveraging noise as a tool
for regularization and privacy enhancement, we aim to contribute to the
development of robust, privacy-aware algorithms, ensuring that AI-driven
solutions prioritize both utility and privacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05792">Is a Seat at the Table Enough? Engaging Teachers and Students in Dataset Specification for ML in Education. (arXiv:2311.05792v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hansol Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramonyam_H/0/1/0/all/0/1">Hariharan Subramonyam</a></p>
<p>Despite the promises of ML in education, its adoption in the classroom has
surfaced numerous issues regarding fairness, accountability, and transparency,
as well as concerns about data privacy and student consent. A root cause of
these issues is the lack of understanding of the complex dynamics of education,
including teacher-student interactions, collaborative learning, and classroom
environment. To overcome these challenges and fully utilize the potential of ML
in education, software practitioners need to work closely with educators and
students to fully understand the context of the data (the backbone of ML
applications) and collaboratively define the ML data specifications. To gain a
deeper understanding of such a collaborative process, we conduct ten co-design
sessions with ML software practitioners, educators, and students. In the
sessions, teachers and students work with ML engineers, UX designers, and legal
practitioners to define dataset characteristics for a given ML application. We
find that stakeholders contextualize data based on their domain and procedural
knowledge, proactively design data requirements to mitigate downstream harms
and data reliability concerns, and exhibit role-based collaborative strategies
and contribution patterns. Further, we find that beyond a seat at the table,
meaningful stakeholder participation in ML requires structured supports:
defined processes for continuous iteration and co-evaluation, shared contextual
data quality standards, and information scaffolds for both technical and
non-technical stakeholders to traverse expertise boundaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05800">Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval. (arXiv:2311.05800v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1">Nandan Thakur</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1">Jianmo Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Abrego_G/0/1/0/all/0/1">Gustavo Hern&#xe1;ndez &#xc1;brego</a>, <a href="http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1">John Wieting</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jimmy Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1">Daniel Cer</a></p>
<p>Dense retrieval models have predominantly been studied for English, where
models have shown great success, due to the availability of human-labeled
training pairs. However, there has been limited success for multilingual
retrieval so far, as training data is uneven or scarcely available across
multiple languages. Synthetic training data generation is promising (e.g.,
InPars or Promptagator), but has been investigated only for English. Therefore,
to study model capabilities across both cross-lingual and monolingual retrieval
tasks, we develop SWIM-IR, a synthetic retrieval training dataset containing 33
(high to very-low resource) languages for training multilingual dense retrieval
models without requiring any human supervision. To construct SWIM-IR, we
propose SAP (summarize-then-ask prompting), where the large language model
(LLM) generates a textual summary prior to the query generation step. SAP
assists the LLM in generating informative queries in the target language. Using
SWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval
models and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve
(cross-lingual), XTREME-UP (cross-lingual) and MIRACL (monolingual). Our
models, called SWIM-X, are competitive with human-supervised dense retrieval
models, e.g., mContriever, finding that SWIM-IR can cheaply substitute for
expensive human-labeled retrieval training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05804">Model-as-a-Service (MaaS): A Survey. (arXiv:2311.05804v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1">Wensheng Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1">Shicheng Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>Due to the increased number of parameters and data in the pre-trained model
exceeding a certain level, a foundation model (e.g., a large language model)
can significantly improve downstream task performance and emerge with some
novel special abilities (e.g., deep learning, complex reasoning, and human
alignment) that were not present before. Foundation models are a form of
generative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has
emerged as a groundbreaking paradigm that revolutionizes the deployment and
utilization of GenAI models. MaaS represents a paradigm shift in how we use AI
technologies and provides a scalable and accessible solution for developers and
users to leverage pre-trained AI models without the need for extensive
infrastructure or expertise in model training. In this paper, the introduction
aims to provide a comprehensive overview of MaaS, its significance, and its
implications for various industries. We provide a brief review of the
development history of "X-as-a-Service" based on cloud computing and present
the key technologies involved in MaaS. The development of GenAI models will
become more democratized and flourish. We also review recent application
studies of MaaS. Finally, we highlight several challenges and future issues in
this promising area. MaaS is a new deployment and service paradigm for
different AI-based models. We hope this review will inspire future research in
the field of MaaS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05840">Predictive AI for SME and Large Enterprise Financial Performance Management. (arXiv:2311.05840v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Cuervo_R/0/1/0/all/0/1">Ricardo Cuervo</a></p>
<p>Financial performance management is at the core of business management and
has historically relied on financial ratio analysis using Balance Sheet and
Income Statement data to assess company performance as compared with
competitors. Little progress has been made in predicting how a company will
perform or in assessing the risks (probabilities) of financial
underperformance. In this study I introduce a new set of financial and
macroeconomic ratios that supplement standard ratios of Balance Sheet and
Income Statement. I also provide a set of supervised learning models (ML
Regressors and Neural Networks) and Bayesian models to predict company
performance. I conclude that the new proposed variables improve model accuracy
when used in tandem with standard industry ratios. I also conclude that
Feedforward Neural Networks (FNN) are simpler to implement and perform best
across 6 predictive tasks (ROA, ROE, Net Margin, Op Margin, Cash Ratio and Op
Cash Generation); although Bayesian Networks (BN) can outperform FNN under very
specific conditions. BNs have the additional benefit of providing a probability
density function in addition to the predicted (expected) value. The study
findings have significant potential helping CFOs and CEOs assess risks of
financial underperformance to steer companies in more profitable directions;
supporting lenders in better assessing the condition of a company and providing
investors with tools to dissect financial statements of public companies more
accurately.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05842">AI-native Interconnect Framework for Integration of Large Language Model Technologies in 6G Systems. (arXiv:2311.05842v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tarkoma_S/0/1/0/all/0/1">Sasu Tarkoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Morabito_R/0/1/0/all/0/1">Roberto Morabito</a>, <a href="http://arxiv.org/find/cs/1/au:+Sauvola_J/0/1/0/all/0/1">Jaakko Sauvola</a></p>
<p>The evolution towards 6G architecture promises a transformative shift in
communication networks, with artificial intelligence (AI) playing a pivotal
role. This paper delves deep into the seamless integration of Large Language
Models (LLMs) and Generalized Pretrained Transformers (GPT) within 6G systems.
Their ability to grasp intent, strategize, and execute intricate commands will
be pivotal in redefining network functionalities and interactions. Central to
this is the AI Interconnect framework, intricately woven to facilitate
AI-centric operations within the network. Building on the continuously evolving
current state-of-the-art, we present a new architectural perspective for the
upcoming generation of mobile networks. Here, LLMs and GPTs will
collaboratively take center stage alongside traditional pre-generative AI and
machine learning (ML) algorithms. This union promises a novel confluence of the
old and new, melding tried-and-tested methods with transformative AI
technologies. Along with providing a conceptual overview of this evolution, we
delve into the nuances of practical applications arising from such an
integration. Through this paper, we envisage a symbiotic integration where AI
becomes the cornerstone of the next-generation communication paradigm, offering
insights into the structural and functional facets of an AI-native 6G network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05844">Face-StyleSpeech: Improved Face-to-Voice latent mapping for Natural Zero-shot Speech Synthesis from a Face Image. (arXiv:2311.05844v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minki Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wooseok Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Eunho Yang</a></p>
<p>Generating a voice from a face image is crucial for developing virtual humans
capable of interacting using their unique voices, without relying on
pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a
zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech
conditioned on a face image rather than reference speech. We hypothesize that
learning both speaker identity and prosody from a face image poses a
significant challenge. To address the issue, our TTS model incorporates both a
face encoder and a prosody encoder. The prosody encoder is specifically
designed to model prosodic features that are not captured only with a face
image, allowing the face encoder to focus solely on capturing the speaker
identity from the face image. Experimental results demonstrate that
Face-StyleSpeech effectively generates more natural speech from a face image
than baselines, even for the face images the model has not trained. Samples are
at our demo page https://face-stylespeech.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05845">Tamil-Llama: A New Tamil Language Model Based on Llama 2. (arXiv:2311.05845v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balachandran_A/0/1/0/all/0/1">Abhinand Balachandran</a></p>
<p>Language modeling has witnessed remarkable advancements in recent years, with
Large Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in
human-like text generation. However, a prevailing limitation is the
underrepresentation of languages like Tamil in these cutting-edge models,
leading to suboptimal performance in diverse linguistic contexts. This paper
addresses this lacuna, enhancing the open-source LLaMA model with an addition
of 16,000 Tamil tokens, aiming to achieve superior text generation and
comprehension in the Tamil language. We strategically employ the LoRA
methodology for efficient model training on a comprehensive Tamil corpus,
ensuring computational feasibility and model robustness. Moreover, we introduce
a Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca
dataset tailored for instruction fine-tuning. Our results showcase significant
performance improvements in Tamil text generation, with potential implications
for the broader landscape of LLMs in Indian languages. We further underscore
our commitment to open research by making our models, datasets, and code
publicly accessible, fostering further innovations in language modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05851">Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative AIs: Trial on Model-Model Interactions in Tangram Naming Task. (arXiv:2311.05851v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morita_J/0/1/0/all/0/1">Junya Morita</a>, <a href="http://arxiv.org/find/cs/1/au:+Yui_T/0/1/0/all/0/1">Tatsuya Yui</a>, <a href="http://arxiv.org/find/cs/1/au:+Amaya_T/0/1/0/all/0/1">Takeru Amaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Higashinaka_R/0/1/0/all/0/1">Ryuichiro Higashinaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeuchi_Y/0/1/0/all/0/1">Yugo Takeuchi</a></p>
<p>For generative AIs to be trustworthy, establishing transparent common
grounding with humans is essential. As a preparation toward human-model common
grounding, this study examines the process of model-model common grounding. In
this context, common ground is defined as a cognitive framework shared among
agents in communication, enabling the connection of symbols exchanged between
agents to the meanings inherent in each agent. This connection is facilitated
by a shared cognitive framework among the agents involved. In this research, we
focus on the tangram naming task (TNT) as a testbed to examine the
common-ground-building process. Unlike previous models designed for this task,
our approach employs generative AIs to visualize the internal processes of the
model. In this task, the sender constructs a metaphorical image of an abstract
figure within the model and generates a detailed description based on this
image. The receiver interprets the generated description from the partner by
constructing another image and reconstructing the original abstract figure.
Preliminary results from the study show an improvement in task performance
beyond the chance level, indicating the effect of the common cognitive
framework implemented in the models. Additionally, we observed that incremental
backpropagations leveraging successful communication cases for a component of
the model led to a statistically significant increase in performance. These
results provide valuable insights into the mechanisms of common grounding made
by generative AIs, improving human communication with the evolving intelligent
machines in our future society.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05853">Reframing Audience Expansion through the Lens of Probability Density Estimation. (arXiv:2311.05853v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carvalhaes_C/0/1/0/all/0/1">Claudio Carvalhaes</a></p>
<p>Audience expansion has become an important element of prospective marketing,
helping marketers create target audiences based on a mere representative sample
of their current customer base. Within the realm of machine learning, a favored
algorithm for scaling this sample into a broader audience hinges on a binary
classification task, with class probability estimates playing a crucial role.
In this paper, we review this technique and introduce a key change in how we
choose training examples to ensure the quality of the generated audience. We
present a simulation study based on the widely used MNIST dataset, where
consistent high precision and recall values demonstrate our approach's ability
to identify the most relevant users for an expanded audience. Our results are
easily reproducible and a Python implementation is openly available on GitHub:
\url{https://github.com/carvalhaes-ai/audience-expansion}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05864">DPR: An Algorithm Mitigate Bias Accumulation in Recommendation feedback loops. (arXiv:2311.05864v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hangtong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuanbo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongjian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1">Fuzhen Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a></p>
<p>Recommendation models trained on the user feedback collected from deployed
recommendation systems are commonly biased. User feedback is considerably
affected by the exposure mechanism, as users only provide feedback on the items
exposed to them and passively ignore the unexposed items, thus producing
numerous false negative samples. Inevitably, biases caused by such user
feedback are inherited by new models and amplified via feedback loops.
Moreover, the presence of false negative samples makes negative sampling
difficult and introduces spurious information in the user preference modeling
process of the model. Recent work has investigated the negative impact of
feedback loops and unknown exposure mechanisms on recommendation quality and
user experience, essentially treating them as independent factors and ignoring
their cross-effects. To address these issues, we deeply analyze the data
exposure mechanism from the perspective of data iteration and feedback loops
with the Missing Not At Random (\textbf{MNAR}) assumption, theoretically
demonstrating the existence of an available stabilization factor in the
transformation of the exposure mechanism under the feedback loops. We further
propose Dynamic Personalized Ranking (\textbf{DPR}), an unbiased algorithm that
uses dynamic re-weighting to mitigate the cross-effects of exposure mechanisms
and feedback loops without additional information. Furthermore, we design a
plugin named Universal Anti-False Negative (\textbf{UFN}) to mitigate the
negative impact of the false negative problem. We demonstrate theoretically
that our approach mitigates the negative effects of feedback loops and unknown
exposure mechanisms. Experimental results on real-world datasets demonstrate
that models using DPR can better handle bias accumulation and the universality
of UFN in mainstream loss methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05877">A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning. (arXiv:2311.05877v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cherepanova_V/0/1/0/all/0/1">Valeriia Cherepanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Levin_R/0/1/0/all/0/1">Roman Levin</a>, <a href="http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1">Gowthami Somepalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1">Jonas Geiping</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1">C. Bayan Bruss</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1">Micah Goldblum</a></p>
<p>Academic tabular benchmarks often contain small sets of curated features. In
contrast, data scientists typically collect as many features as possible into
their datasets, and even engineer new features from existing ones. To prevent
overfitting in subsequent downstream modeling, practitioners commonly use
automated feature selection methods that identify a reduced subset of
informative features. Existing benchmarks for tabular feature selection
consider classical downstream models, toy synthetic datasets, or do not
evaluate feature selectors on the basis of downstream performance. Motivated by
the increasing popularity of tabular deep learning, we construct a challenging
feature selection benchmark evaluated on downstream neural networks including
transformers, using real datasets and multiple methods for generating
extraneous features. We also propose an input-gradient-based analogue of Lasso
for neural networks that outperforms classical feature selection methods on
challenging problems such as selecting from corrupted or second-order features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05903">Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users. (arXiv:2311.05903v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dodgson_J/0/1/0/all/0/1">Jennifer Dodgson</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanzheng_L/0/1/0/all/0/1">Lin Nanzheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Peh_J/0/1/0/all/0/1">Julian Peh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattirane_A/0/1/0/all/0/1">Akira Rafhael Janson Pattirane</a>, <a href="http://arxiv.org/find/cs/1/au:+Alhajir_A/0/1/0/all/0/1">Alfath Daryl Alhajir</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinarto_E/0/1/0/all/0/1">Eko Ridho Dinarto</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1">Joseph Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1">Syed Danyal Ahmad</a></p>
<p>Research into methods for improving the performance of large language models
(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and
soft-prompting has tended to focus on the use of highly technical or high-cost
techniques, making many of the newly discovered approaches comparatively
inaccessible to non-technical users. In this paper we tested an unmodified
version of GPT 3.5, a fine-tuned version, and the same unmodified model when
given access to a vectorised RAG database, both in isolation and in combination
with a basic, non-algorithmic soft prompt. In each case we tested the model's
ability to answer a set of 100 questions relating primarily to events that
occurred after September 2021 (the point at which GPT 3.5's training data set
ends). We found that if commercial platforms are used and default settings are
applied with no iteration in order to establish a baseline set of outputs, a
fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach
out-performed both. The application of a soft prompt significantly improved the
performance of each approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05915">Fake Alignment: Are LLMs Really Aligned Well?. (arXiv:2311.05915v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yan Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kexin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1">Chengqi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xingjun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingchun Wang</a></p>
<p>The growing awareness of safety concerns in large language models (LLMs) has
sparked considerable interest in the evaluation of safety within current
research endeavors. This study investigates an interesting issue pertaining to
the evaluation of LLMs, namely the substantial discrepancy in performance
between multiple-choice questions and open-ended questions. Inspired by
research on jailbreak attack patterns, we argue this is caused by mismatched
generalization. That is, the LLM does not have a comprehensive understanding of
the complex concept of safety. Instead, it only remembers what to answer for
open-ended safety questions, which makes it unable to solve other forms of
safety tests. We refer to this phenomenon as fake alignment and construct a
comparative benchmark to empirically verify its existence in LLMs. Such fake
alignment renders previous evaluation protocols unreliable. To address this, we
introduce the FAEF framework and two novel metrics\textemdash Consistency Score
(CS) and Consistent Safety Score (CSS), which jointly assess two complementary
forms of evaluation to quantify fake alignment and obtain corrected performance
estimates. Applying FAEF to 14 widely-used LLMs reveals several models with
purported safety are poorly aligned in practice. Our work highlights potential
limitations in prevailing alignment methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05928">The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models. (arXiv:2311.05928v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1">Anton Razzhigaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikhalchuk_M/0/1/0/all/0/1">Matvey Mikhalchuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Goncharova_E/0/1/0/all/0/1">Elizaveta Goncharova</a>, <a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1">Ivan Oseledets</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a></p>
<p>In this study, we present an investigation into the anisotropy dynamics and
intrinsic dimension of embeddings in transformer architectures, focusing on the
dichotomy between encoders and decoders. Our findings reveal that the
anisotropy profile in transformer decoders exhibits a distinct bell-shaped
curve, with the highest anisotropy concentrations in the middle layers. This
pattern diverges from the more uniformly distributed anisotropy observed in
encoders. In addition, we found that the intrinsic dimension of embeddings
increases in the initial phases of training, indicating an expansion into
higher-dimensional space. Which is then followed by a compression phase towards
the end of training with dimensionality decrease, suggesting a refinement into
more compact representations. Our results provide fresh insights to the
understanding of encoders and decoders embedding properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05931">Anytime-Valid Confidence Sequences for Consistent Uncertainty Estimation in Early-Exit Neural Networks. (arXiv:2311.05931v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jazbec_M/0/1/0/all/0/1">Metod Jazbec</a>, <a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1">Patrick Forr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nalisnick_E/0/1/0/all/0/1">Eric Nalisnick</a></p>
<p>Early-exit neural networks (EENNs) facilitate adaptive inference by producing
predictions at multiple stages of the forward pass. In safety-critical
applications, these predictions are only meaningful when complemented with
reliable uncertainty estimates. Yet, due to their sequential structure, an
EENN's uncertainty estimates should also be consistent: labels that are deemed
improbable at one exit should not reappear within the confidence interval / set
of later exits. We show that standard uncertainty quantification techniques,
like Bayesian methods or conformal prediction, can lead to inconsistency across
exits. We address this problem by applying anytime-valid confidence sequences
(AVCSs) to the exits of EENNs. By design, AVCSs maintain consistency across
exits. We examine the theoretical and practical challenges of applying AVCSs to
EENNs and empirically validate our approach on both regression and
classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05937">Genetic Algorithm enhanced by Deep Reinforcement Learning in parent selection mechanism and mutation : Minimizing makespan in permutation flow shop scheduling problems. (arXiv:2311.05937v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Irmouli_M/0/1/0/all/0/1">Maissa Irmouli</a>, <a href="http://arxiv.org/find/cs/1/au:+Benazzoug_N/0/1/0/all/0/1">Nourelhouda Benazzoug</a>, <a href="http://arxiv.org/find/cs/1/au:+Adimi_A/0/1/0/all/0/1">Alaa Dania Adimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezkellah_F/0/1/0/all/0/1">Fatma Zohra Rezkellah</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamzaoui_I/0/1/0/all/0/1">Imane Hamzaoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamitouche_T/0/1/0/all/0/1">Thanina Hamitouche</a></p>
<p>This paper introduces a reinforcement learning (RL) approach to address the
challenges associated with configuring and optimizing genetic algorithms (GAs)
for solving difficult combinatorial or non-linear problems. The proposed RL+GA
method was specifically tested on the flow shop scheduling problem (FSP). The
hybrid algorithm incorporates neural networks (NN) and uses the off-policy
method Q-learning or the on-policy method Sarsa(0) to control two key genetic
algorithm (GA) operators: parent selection mechanism and mutation. At each
generation, the RL agent's action is determining the selection method, the
probability of the parent selection and the probability of the offspring
mutation. This allows the RL agent to dynamically adjust the selection and
mutation based on its learned policy. The results of the study highlight the
effectiveness of the RL+GA approach in improving the performance of the
primitive GA. They also demonstrate its ability to learn and adapt from
population diversity and solution improvements over time. This adaptability
leads to improved scheduling solutions compared to static parameter
configurations while maintaining population diversity throughout the
evolutionary process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05992">Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous. (arXiv:2311.05992v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1">Nabil Aouf</a>, <a href="http://arxiv.org/find/cs/1/au:+Pizarro_J/0/1/0/all/0/1">Jose Pizarro</a>, <a href="http://arxiv.org/find/cs/1/au:+Honvault_C/0/1/0/all/0/1">Christophe Honvault</a></p>
<p>Research on developing deep learning techniques for autonomous spacecraft
relative navigation challenges is continuously growing in recent years.
Adopting those techniques offers enhanced performance. However, such approaches
also introduce heightened apprehensions regarding the trustability and security
of such deep learning methods through their susceptibility to adversarial
attacks. In this work, we propose a novel approach for adversarial attack
detection for deep neural network-based relative pose estimation schemes based
on the explainability concept. We develop for an orbital rendezvous scenario an
innovative relative pose estimation technique adopting our proposed
Convolutional Neural Network (CNN), which takes an image from the chaser's
onboard camera and outputs accurately the target's relative position and
rotation. We perturb seamlessly the input images using adversarial attacks that
are generated by the Fast Gradient Sign Method (FGSM). The adversarial attack
detector is then built based on a Long Short Term Memory (LSTM) network which
takes the explainability measure namely SHapley Value from the CNN-based pose
estimator and flags the detection of adversarial attacks when acting.
Simulation results show that the proposed adversarial attack detector achieves
a detection accuracy of 99.21%. Both the deep relative pose estimator and
adversarial attack detector are then tested on real data captured from our
laboratory-designed setup. The experimental results from our
laboratory-designed setup demonstrate that the proposed adversarial attack
detector achieves an average detection accuracy of 96.29%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05997">JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models. (arXiv:2311.05997v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1">Shaofei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anji Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yonggang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jinbing Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haowei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhaofeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zilong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yitao Liang</a></p>
<p>Achieving human-like planning and control with multimodal observations in an
open world is a key milestone for more functional generalist agents. Existing
approaches can handle certain long-horizon tasks in an open world. However,
they still struggle when the number of open-world tasks could potentially be
infinite and lack the capability to progressively enhance task completion as
game time progresses. We introduce JARVIS-1, an open-world agent that can
perceive multimodal input (visual observations and human instructions),
generate sophisticated plans, and perform embodied control, all within the
popular yet challenging open-world Minecraft universe. Specifically, we develop
JARVIS-1 on top of pre-trained multimodal language models, which map visual
observations and textual instructions to plans. The plans will be ultimately
dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a
multimodal memory, which facilitates planning using both pre-trained knowledge
and its actual game survival experiences. In our experiments, JARVIS-1 exhibits
nearly perfect performances across over 200 varying tasks from the Minecraft
Universe Benchmark, ranging from entry to intermediate levels. JARVIS-1 has
achieved a completion rate of 12.5% in the long-horizon diamond pickaxe task.
This represents a significant increase up to 5 times compared to previous
records. Furthermore, we show that JARVIS-1 is able to $\textit{self-improve}$
following a life-long learning paradigm thanks to multimodal memory, sparking a
more general intelligence and improved autonomy. The project page is available
at https://craftjarvis-jarvis1.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06015">RSG: Fast Learning Adaptive Skills for Quadruped Robots by Skill Graph. (arXiv:2311.06015v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Diyuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhenyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1">Feng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_S/0/1/0/all/0/1">Sibo Gai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shangke Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>Developing robotic intelligent systems that can adapt quickly to unseen wild
situations is one of the critical challenges in pursuing autonomous robotics.
Although some impressive progress has been made in walking stability and skill
learning in the field of legged robots, their ability to fast adaptation is
still inferior to that of animals in nature. Animals are born with massive
skills needed to survive, and can quickly acquire new ones, by composing
fundamental skills with limited experience. Inspired by this, we propose a
novel framework, named Robot Skill Graph (RSG) for organizing massive
fundamental skills of robots and dexterously reusing them for fast adaptation.
Bearing a structure similar to the Knowledge Graph (KG), RSG is composed of
massive dynamic behavioral skills instead of static knowledge in KG and enables
discovering implicit relations that exist in be-tween of learning context and
acquired skills of robots, serving as a starting point for understanding subtle
patterns existing in robots' skill learning. Extensive experimental results
demonstrate that RSG can provide rational skill inference upon new tasks and
environments and enable quadruped robots to adapt to new scenarios and learn
new skills rapidly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06043">Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A Brief Survey. (arXiv:2311.06043v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yang Peng</a></p>
<p>Object detection and tracking are vital and fundamental tasks for autonomous
driving, aiming at identifying and locating objects from those predefined
categories in a scene. 3D point cloud learning has been attracting more and
more attention among all other forms of self-driving data. Currently, there are
many deep learning methods for 3D object detection. However, the tasks of
object detection and tracking for point clouds still need intensive study due
to the unique characteristics of point cloud data. To help get a good grasp of
the present situation of this research, this paper shows recent advances in
deep learning methods for 3D object detection and tracking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06063">RIGA: A Regret-Based Interactive Genetic Algorithm. (arXiv:2311.06063v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benabbou_N/0/1/0/all/0/1">Nawal Benabbou</a>, <a href="http://arxiv.org/find/cs/1/au:+Leroy_C/0/1/0/all/0/1">Cassandre Leroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lust_T/0/1/0/all/0/1">Thibaut Lust</a></p>
<p>In this paper, we propose an interactive genetic algorithm for solving
multi-objective combinatorial optimization problems under preference
imprecision. More precisely, we consider problems where the decision maker's
preferences over solutions can be represented by a parameterized aggregation
function (e.g., a weighted sum, an OWA operator, a Choquet integral), and we
assume that the parameters are initially not known by the recommendation
system. In order to quickly make a good recommendation, we combine elicitation
and search in the following way: 1) we use regret-based elicitation techniques
to reduce the parameter space in a efficient way, 2) genetic operators are
applied on parameter instances (instead of solutions) to better explore the
parameter space, and 3) we generate promising solutions (population) using
existing solving methods designed for the problem with known preferences. Our
algorithm, called RIGA, can be applied to any multi-objective combinatorial
optimization problem provided that the aggregation function is linear in its
parameters and that a (near-)optimal solution can be efficiently determined for
the problem with known preferences. We also study its theoretical performances:
RIGA can be implemented in such way that it runs in polynomial time while
asking no more than a polynomial number of queries. The method is tested on the
multi-objective knapsack and traveling salesman problems. For several
performance indicators (computation times, gap to optimality and number of
queries), RIGA obtains better results than state-of-the-art algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06101">In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models. (arXiv:2311.06101v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zecchin_M/0/1/0/all/0/1">Matteo Zecchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1">Osvaldo Simeone</a></p>
<p>Large pre-trained sequence models, such as transformer-based architectures,
have been recently shown to have the capacity to carry out in-context learning
(ICL). In ICL, a decision on a new input is made via a direct mapping of the
input and of a few examples from the given task, serving as the task's context,
to the output variable. No explicit updates of model parameters are needed to
tailor the decision to a new task. Pre-training, which amounts to a form of
meta-learning, is based on the observation of examples from several related
tasks. Prior work has shown ICL capabilities for linear regression. In this
study, we leverage ICL to address the inverse problem of multiple-input and
multiple-output (MIMO) equalization based on a context given by pilot symbols.
A task is defined by the unknown fading channel and by the signal-to-noise
ratio (SNR) level, which may be known. To highlight the practical potential of
the approach, we allow for the presence of quantization of the received
signals. We demonstrate via numerical results that transformer-based ICL has a
threshold behavior, whereby, as the number of pre-training tasks grows, the
performance switches from that of a minimum mean squared error (MMSE) equalizer
with a prior determined by the pre-trained tasks to that of an MMSE equalizer
with the true data-generating prior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06102">Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking. (arXiv:2311.06102v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1">Lefteris Loukas</a>, <a href="http://arxiv.org/find/cs/1/au:+Stogiannidis_I/0/1/0/all/0/1">Ilias Stogiannidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Diamantopoulos_O/0/1/0/all/0/1">Odysseas Diamantopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1">Prodromos Malakasiotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vassos_S/0/1/0/all/0/1">Stavros Vassos</a></p>
<p>Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06130">High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft. (arXiv:2311.06130v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Saves_P/0/1/0/all/0/1">Paul Saves</a>, <a href="http://arxiv.org/find/math/1/au:+Diouane_Y/0/1/0/all/0/1">Youssef Diouane</a>, <a href="http://arxiv.org/find/math/1/au:+Bartoli_N/0/1/0/all/0/1">Nathalie Bartoli</a>, <a href="http://arxiv.org/find/math/1/au:+Lefebvre_T/0/1/0/all/0/1">Thierry Lefebvre</a>, <a href="http://arxiv.org/find/math/1/au:+Morlier_J/0/1/0/all/0/1">Joseph Morlier</a></p>
<p>Multidisciplinary design optimization (MDO) methods aim at adapting numerical
optimization techniques to the design of engineering systems involving multiple
disciplines. In this context, a large number of mixed continuous, integer, and
categorical variables might arise during the optimization process, and
practical applications involve a significant number of design variables.
Recently, there has been a growing interest in mixed-categorical metamodels
based on Gaussian Process (GP) for Bayesian optimization. In particular, to
handle mixed-categorical variables, several existing approaches employ
different strategies to build the GP. These strategies either use continuous
kernels, such as the continuous relaxation or the Gower distance-based kernels,
or direct estimation of the correlation matrix, such as the exponential
homoscedastic hypersphere (EHH) or the Homoscedastic Hypersphere (HH) kernel.
Although the EHH and HH kernels are shown to be very efficient and lead to
accurate GPs, they are based on a large number of hyperparameters. In this
paper, we address this issue by constructing mixed-categorical GPs with fewer
hyperparameters using Partial Least Squares (PLS) regression. Our goal is to
generalize Kriging with PLS, commonly used for continuous inputs, to handle
mixed-categorical inputs. The proposed method is implemented in the open-source
software SMT and has been efficiently applied to structural and
multidisciplinary applications. Our method is used to effectively demonstrate
the structural behavior of a cantilever beam and facilitates MDO of a green
aircraft, resulting in a 439-kilogram reduction in the amount of fuel consumed
during a single aircraft mission.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06147">Incorporating sufficient physical information into artificial neural networks: a guaranteed improvement via physics-based Rao-Blackwellization. (arXiv:2311.06147v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geuken_G/0/1/0/all/0/1">Gian-Luca Geuken</a>, <a href="http://arxiv.org/find/cs/1/au:+Mosler_J/0/1/0/all/0/1">J&#xf6;rn Mosler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurzeja_P/0/1/0/all/0/1">Patrick Kurzeja</a></p>
<p>The concept of Rao-Blackwellization is employed to improve predictions of
artificial neural networks by physical information. The error norm and the
proof of improvement are transferred from the original statistical concept to a
deterministic one, using sufficient information on physics-based conditions.
The proposed strategy is applied to material modeling and illustrated by
examples of the identification of a yield function, elasto-plastic steel
simulations, the identification of driving forces for quasi-brittle damage and
rubber experiments. Sufficient physical information is employed, e.g., in the
form of invariants, parameters of a minimization problem, dimensional analysis,
isotropy and differentiability. It is proven how intuitive accretion of
information can yield improvement if it is physically sufficient, but also how
insufficient or superfluous information can cause impairment. Opportunities for
the improvement of artificial neural networks are explored in terms of the
training data set, the networks' structure and output filters. Even crude
initial predictions are remarkably improved by reducing noise, overfitting and
data requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06149">Dense Visual Odometry Using Genetic Algorithm. (arXiv:2311.06149v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Djema_S/0/1/0/all/0/1">Slimane Djema</a>, <a href="http://arxiv.org/find/cs/1/au:+Benselama_Z/0/1/0/all/0/1">Zoubir Abdeslem Benselama</a>, <a href="http://arxiv.org/find/cs/1/au:+Hedjar_R/0/1/0/all/0/1">Ramdane Hedjar</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdallah_K/0/1/0/all/0/1">Krabi Abdallah</a></p>
<p>Our work aims to estimate the camera motion mounted on the head of a mobile
robot or a moving object from RGB-D images in a static scene. The problem of
motion estimation is transformed into a nonlinear least squares function.
Methods for solving such problems are iterative. Various classic methods gave
an iterative solution by linearizing this function. We can also use the
metaheuristic optimization method to solve this problem and improve results. In
this paper, a new algorithm is developed for visual odometry using a sequence
of RGB-D images. This algorithm is based on a genetic algorithm. The proposed
iterative genetic algorithm searches using particles to estimate the optimal
motion and then compares it to the traditional methods. To evaluate our method,
we use the root mean square error to compare it with the based energy method
and another metaheuristic method. We prove the efficiency of our innovative
algorithm on a large set of images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06152">Going beyond persistent homology using persistent homology. (arXiv:2311.06152v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Immonen_J/0/1/0/all/0/1">Johanna Immonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_A/0/1/0/all/0/1">Amauri H. Souza</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1">Vikas Garg</a></p>
<p>Representational limits of message-passing graph neural networks (MP-GNNs),
e.g., in terms of the Weisfeiler-Leman (WL) test for isomorphism, are well
understood. Augmenting these graph models with topological features via
persistent homology (PH) has gained prominence, but identifying the class of
attributed graphs that PH can recognize remains open. We introduce a novel
concept of color-separating sets to provide a complete resolution to this
important problem. Specifically, we establish the necessary and sufficient
conditions for distinguishing graphs based on the persistence of their
connected components, obtained from filter functions on vertex and edge colors.
Our constructions expose the limits of vertex- and edge-level PH, proving that
neither category subsumes the other. Leveraging these theoretical insights, we
propose RePHINE for learning topological features on graphs. RePHINE
efficiently combines vertex- and edge-level PH, achieving a scheme that is
provably more powerful than both. Integrating RePHINE into MP-GNNs boosts their
expressive power, resulting in gains over standard PH on several benchmarks for
graph classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06158">Language Models can be Logical Solvers. (arXiv:2311.06158v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiazhan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruochen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Junheng Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1">Hiteshi Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yelong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weizhu Chen</a></p>
<p>Logical reasoning is a fundamental aspect of human intelligence and a key
component of tasks like problem-solving and decision-making. Recent
advancements have enabled Large Language Models (LLMs) to potentially exhibit
reasoning capabilities, but complex logical reasoning remains a challenge. The
state-of-the-art, solver-augmented language models, use LLMs to parse natural
language logical questions into symbolic representations first and then adopt
external logical solvers to take in the symbolic representations and output the
answers. Despite their impressive performance, any parsing errors will
inevitably result in the failure of the execution of the external logical
solver and no answer to the logical questions. In this paper, we introduce
LoGiPT, a novel language model that directly emulates the reasoning processes
of logical solvers and bypasses the parsing errors by learning to strict
adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly
constructed instruction-tuning dataset derived from revealing and refining the
invisible reasoning process of deductive solvers. Experimental results on two
public deductive reasoning datasets demonstrate that LoGiPT outperforms
state-of-the-art solver-augmented LMs and few-shot prompting methods on
competitive LLMs like ChatGPT or GPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06175">Search-Based Fairness Testing: An Overview. (arXiv:2311.06175v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mamman_H/0/1/0/all/0/1">Hussaini Mamman</a>, <a href="http://arxiv.org/find/cs/1/au:+Basri_S/0/1/0/all/0/1">Shuib Basri</a>, <a href="http://arxiv.org/find/cs/1/au:+Balogun_A/0/1/0/all/0/1">Abdullateef Oluwaqbemiga Balogun</a>, <a href="http://arxiv.org/find/cs/1/au:+Imam_A/0/1/0/all/0/1">Abdullahi Abubakar Imam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1">Ganesh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Capretz_L/0/1/0/all/0/1">Luiz Fernando Capretz</a></p>
<p>Artificial Intelligence (AI) has demonstrated remarkable capabilities in
domains such as recruitment, finance, healthcare, and the judiciary. However,
biases in AI systems raise ethical and societal concerns, emphasizing the need
for effective fairness testing methods. This paper reviews current research on
fairness testing, particularly its application through search-based testing.
Our analysis highlights progress and identifies areas of improvement in
addressing AI systems biases. Future research should focus on leveraging
established search-based testing methodologies for fairness testing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06184">Frequency-domain MLPs are More Effective Learners in Time Series Forecasting. (arXiv:2311.06184v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kun Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shoujin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1">Defu Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+An_N/0/1/0/all/0/1">Ning An</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1">Zhendong Niu</a></p>
<p>Time series forecasting has played the key role in different industrial,
including finance, traffic, energy, and healthcare domains. While existing
literatures have designed many sophisticated architectures based on RNNs, GNNs,
or Transformers, another kind of approaches based on multi-layer perceptrons
(MLPs) are proposed with simple structure, low complexity, and {superior
performance}. However, most MLP-based forecasting methods suffer from the
point-wise mappings and information bottleneck, which largely hinders the
forecasting performance. To overcome this problem, we explore a novel direction
of applying MLPs in the frequency domain for time series forecasting. We
investigate the learned patterns of frequency-domain MLPs and discover their
two inherent characteristic benefiting forecasting, (i) global view: frequency
spectrum makes MLPs own a complete view for signals and learn global
dependencies more easily, and (ii) energy compaction: frequency-domain MLPs
concentrate on smaller key part of frequency components with compact signal
energy. Then, we propose FreTS, a simple yet effective architecture built upon
Frequency-domain MLPs for Time Series forecasting. FreTS mainly involves two
stages, (i) Domain Conversion, that transforms time-domain signals into complex
numbers of frequency domain; (ii) Frequency Learning, that performs our
redesigned MLPs for the learning of real and imaginary part of frequency
components. The above stages operated on both inter-series and intra-series
scales further contribute to channel-wise and time-wise dependency learning.
Extensive experiments on 13 real-world benchmarks (including 7 benchmarks for
short-term forecasting and 6 benchmarks for long-term forecasting) demonstrate
our consistent superiority over state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06190">FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective. (arXiv:2311.06190v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kun Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Liang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_N/0/1/0/all/0/1">Ning An</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1">Zhendong Niu</a></p>
<p>Multivariate time series (MTS) forecasting has shown great importance in
numerous industries. Current state-of-the-art graph neural network (GNN)-based
forecasting methods usually require both graph networks (e.g., GCN) and
temporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and
intra-series (temporal) dependencies, respectively. However, the uncertain
compatibility of the two networks puts an extra burden on handcrafted model
designs. Moreover, the separate spatial and temporal modeling naturally
violates the unified spatiotemporal inter-dependencies in real world, which
largely hinders the forecasting performance. To overcome these problems, we
explore an interesting direction of directly applying graph networks and
rethink MTS forecasting from a pure graph perspective. We first define a novel
data structure, hypervariate graph, which regards each series value (regardless
of variates or timestamps) as a graph node, and represents sliding windows as
space-time fully-connected graphs. This perspective considers spatiotemporal
dynamics unitedly and reformulates classic MTS forecasting into the predictions
on hypervariate graphs. Then, we propose a novel architecture Fourier Graph
Neural Network (FourierGNN) by stacking our proposed Fourier Graph Operator
(FGO) to perform matrix multiplications in Fourier space. FourierGNN
accommodates adequate expressiveness and achieves much lower complexity, which
can effectively and efficiently accomplish the forecasting. Besides, our
theoretical analysis reveals FGO's equivalence to graph convolutions in the
time domain, which further verifies the validity of FourierGNN. Extensive
experiments on seven datasets have demonstrated our superior performance with
higher efficiency and fewer parameters compared with state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06192">Greedy PIG: Adaptive Integrated Gradients. (arXiv:2311.06192v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Axiotis_K/0/1/0/all/0/1">Kyriakos Axiotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Abu_al_haija_S/0/1/0/all/0/1">Sami Abu-al-haija</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahrbach_M/0/1/0/all/0/1">Matthew Fahrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1">Gang Fu</a></p>
<p>Deep learning has become the standard approach for most machine learning
tasks. While its impact is undeniable, interpreting the predictions of deep
learning models from a human perspective remains a challenge. In contrast to
model training, model interpretability is harder to quantify and pose as an
explicit optimization problem. Inspired by the AUC softmax information curve
(AUC SIC) metric for evaluating feature attribution methods, we propose a
unified discrete optimization framework for feature attribution and feature
selection based on subset selection. This leads to a natural adaptive
generalization of the path integrated gradients (PIG) method for feature
attribution, which we call Greedy PIG. We demonstrate the success of Greedy PIG
on a wide variety of tasks, including image feature attribution, graph
compression/explanation, and post-hoc feature selection on tabular data. Our
results show that introducing adaptivity is a powerful and versatile method for
making attribution methods more powerful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06204">BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset. (arXiv:2311.06204v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahtab_M/0/1/0/all/0/1">Md. Motahar Mahtab</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1">Monirul Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Mehedi Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadeque_F/0/1/0/all/0/1">Farig Sadeque</a></p>
<p>Intentionally luring readers to click on a particular content by exploiting
their curiosity defines a title as clickbait. Although several studies focused
on detecting clickbait titles in English articles, low resource language like
Bangla has not been given adequate attention. To tackle clickbait titles in
Bangla, we have constructed the first Bangla clickbait detection dataset
containing 15,056 labeled news articles and 65,406 unlabelled news articles
extracted from clickbait dense news sites. Each article has been labeled by
three expert linguists and includes an article's title, body, and other
metadata. By incorporating labeled and unlabelled data, we finetune a
pretrained Bangla transformer model in an adversarial fashion using Semi
Supervised Generative Adversarial Networks (SS GANs). The proposed model acts
as a good baseline for this dataset, outperforming traditional neural network
models (LSTM, GRU, CNN) and linguistic feature based models. We expect that
this dataset and the detailed analysis and comparison of these clickbait
detection models will provide a fundamental basis for future research into
detecting clickbait titles in Bengali articles. We have released the
corresponding code and dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06217">MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things. (arXiv:2311.06217v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Russ Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>The Internet of Things (IoT), the network integrating billions of smart
physical devices embedded with sensors, software, and communication
technologies for the purpose of connecting and exchanging data with other
devices and systems, is a critical and rapidly expanding component of our
modern world. The IoT ecosystem provides a rich source of real-world modalities
such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio
for prediction tasks involving the pose, gaze, activities, and gestures of
humans as well as the touch, contact, pose, 3D of physical objects. Machine
learning presents a rich opportunity to automatically process IoT data at
scale, enabling efficient inference for impact in understanding human
wellbeing, controlling physical devices, and interconnecting smart cities. To
develop machine learning technologies for IoT, this paper proposes MultiIoT,
the most expansive IoT benchmark to date, encompassing over 1.15 million
samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges
involving (1) learning from many sensory modalities, (2) fine-grained
interactions across long temporal ranges, and (3) extreme heterogeneity due to
unique structure and noise topologies in real-world sensors. We also release a
set of strong modeling baselines, spanning modality and task-specific methods
to multisensory and multitask models to encourage future research in
multisensory representation learning for IoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06224">Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization. (arXiv:2311.06224v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benarous_E/0/1/0/all/0/1">Elior Benarous</a>, <a href="http://arxiv.org/find/cs/1/au:+Anagnostidis_S/0/1/0/all/0/1">Sotiris Anagnostidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Biggio_L/0/1/0/all/0/1">Luca Biggio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1">Thomas Hofmann</a></p>
<p>Recent advancements in deep learning have been primarily driven by the use of
large models trained on increasingly vast datasets. While neural scaling laws
have emerged to predict network performance given a specific level of
computational resources, the growing demand for expansive datasets raises
concerns. To address this, a new research direction has emerged, focusing on
the creation of synthetic data as a substitute. In this study, we investigate
how neural networks exhibit shape bias during training on synthetic datasets,
serving as an indicator of the synthetic data quality. Specifically, our
findings indicate three key points: (1) Shape bias varies across network
architectures and types of supervision, casting doubt on its reliability as a
predictor for generalization and its ability to explain differences in model
recognition compared to human capabilities. (2) Relying solely on shape bias to
estimate generalization is unreliable, as it is entangled with diversity and
naturalism. (3) We propose a novel interpretation of shape bias as a tool for
estimating the diversity of samples within a dataset. Our research aims to
clarify the implications of using synthetic data and its associated shape bias
in deep learning, addressing concerns regarding generalization and dataset
quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06233">Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. (arXiv:2311.06233v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06243">Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization. (arXiv:2311.06243v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zeju Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1">Yuliang Xiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yuxuan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Longhui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Haiwen Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1">Juyeon Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Songyou Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yandong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1">Adrian Weller</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>Large foundation models are becoming ubiquitous, but training them from
scratch is prohibitively expensive. Thus, efficiently adapting these powerful
models to downstream tasks is increasingly important. In this paper, we study a
principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream
task adaptation. Despite demonstrating good generalizability, OFT still uses a
fairly large number of trainable parameters due to the high dimensionality of
orthogonal matrices. To address this, we start by examining OFT from an
information transmission perspective, and then identify a few key desiderata
that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast
Fourier transform algorithm enables efficient information transmission, we
propose an efficient orthogonal parameterization using butterfly structures. We
apply this parameterization to OFT, creating a novel parameter-efficient
finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a
special case, BOFT introduces a generalized orthogonal finetuning framework.
Finally, we conduct an extensive empirical study of adapting large vision
transformers, large language models, and text-to-image diffusion models to
various downstream tasks in vision and language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14590">Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_C/0/1/0/all/0/1">Chinmay Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Manxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1">Druv Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1">Shankar Sastry</a></p>
<p>We propose a multi-agent reinforcement learning dynamics, and analyze its
convergence in infinite-horizon discounted Markov potential games. We focus on
the independent and decentralized setting, where players do not have knowledge
of the game model and cannot coordinate. In each stage, players update their
estimate of Q-function that evaluates their total contingent payoff based on
the realized one-stage reward in an asynchronous manner. Then, players
independently update their policies by incorporating an optimal one-stage
deviation strategy based on the estimated Q-function. A key feature of the
learning dynamics is that the Q-function estimates are updated at a faster
timescale than the policies. We prove that the policies induced by our learning
dynamics converge to the set of stationary Nash equilibria in Markov potential
games with probability 1. Our results highlight the efficacy of simple learning
dynamics in reaching to the set of stationary Nash equilibrium even in
environments with minimal information available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.12346">Exploring the Constraints on Artificial General Intelligence: A Game-Theoretic No-Go Theorem. (arXiv:2209.12346v2 [econ.TH] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Ismail_M/0/1/0/all/0/1">Mehmet S. Ismail</a></p>
<p>The emergence of increasingly sophisticated artificial intelligence (AI)
systems have sparked intense debate among researchers, policymakers, and the
public due to their potential to surpass human intelligence and capabilities in
all domains. In this paper, I propose a game-theoretic framework that captures
the strategic interactions between a human agent and a potential superhuman
machine agent. I identify four key assumptions: Strategic Unpredictability,
Access to Machine's Strategy, Rationality, and Superhuman Machine. The main
result of this paper is an impossibility theorem: these four assumptions are
inconsistent when taken together, but relaxing any one of them results in a
consistent set of assumptions. Two straightforward policy recommendations
follow: first, policymakers should control access to specific human data to
maintain Strategic Unpredictability; and second, they should grant select AI
researchers access to superhuman machine research to ensure Access to Machine's
Strategy holds. My analysis contributes to a better understanding of the
context that can shape the theoretical development of superhuman AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01738">ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training. (arXiv:2210.01738v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1">Antonio Norelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1">Marco Fumero</a>, <a href="http://arxiv.org/find/cs/1/au:+Maiorca_V/0/1/0/all/0/1">Valentino Maiorca</a>, <a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1">Luca Moschella</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1">Emanuele Rodol&#xe0;</a>, <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a></p>
<p>CLIP proved that aligning visual and language spaces is key to solving many
vision tasks without explicit training, but required to train image and text
encoders from scratch on a huge dataset. LiT improved this by only training the
text encoder and using a pre-trained vision network. In this paper, we show
that a common space can be created without any training at all, using
single-domain encoders (trained with or without supervision) and a much smaller
amount of image-text pairs. Furthermore, our model has unique properties. Most
notably, deploying a new version with updated training samples can be done in a
matter of seconds. Additionally, the representations in the common space are
easily interpretable as every dimension corresponds to the similarity of the
input to a unique image-text pair in the multimodal dataset. Experiments on
standard zero-shot visual benchmarks demonstrate the typical transfer ability
of image-text models. Overall, our method represents a simple yet surprisingly
strong baseline for foundation multimodal models, raising important questions
on their data efficiency and on the role of retrieval in machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04370">NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation. (arXiv:2211.04370v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1">Abbavaram Gowtham Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1">Vineeth N Balasubramanian</a></p>
<p>Causal effect estimation from observational data is a central problem in
causal inference. Methods based on potential outcomes framework solve this
problem by exploiting inductive biases and heuristics from causal inference.
Each of these methods addresses a specific aspect of causal effect estimation,
such as controlling propensity score, enforcing randomization, etc., by
designing neural network (NN) architectures and regularizers. In this paper, we
propose an adaptive method called Neurosymbolic Causal Effect Estimator
(NESTER), a generalized method for causal effect estimation. NESTER integrates
the ideas used in existing methods based on multi-head NNs for causal effect
estimation into one framework. We design a Domain Specific Language (DSL)
tailored for causal effect estimation based on causal inductive biases used in
literature. We conduct a theoretical analysis to investigate NESTER's efficacy
in estimating causal effects. Our comprehensive empirical results show that
NESTER performs better than state-of-the-art methods on benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01377">Learning with Exposure Constraints in Recommendation Systems. (arXiv:2302.01377v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ben_Porat_O/0/1/0/all/0/1">Omer Ben-Porat</a>, <a href="http://arxiv.org/find/cs/1/au:+Torkan_R/0/1/0/all/0/1">Rotem Torkan</a></p>
<p>Recommendation systems are dynamic economic systems that balance the needs of
multiple stakeholders. A recent line of work studies incentives from the
content providers' point of view. Content providers, e.g., vloggers and
bloggers, contribute fresh content and rely on user engagement to create
revenue and finance their operations. In this work, we propose a contextual
multi-armed bandit setting to model the dependency of content providers on
exposure. In our model, the system receives a user context in every round and
has to select one of the arms. Every arm is a content provider who must receive
a minimum number of pulls every fixed time period (e.g., a month) to remain
viable in later rounds; otherwise, the arm departs and is no longer available.
The system aims to maximize the users' (content consumers) welfare. To that
end, it should learn which arms are vital and ensure they remain viable by
subsidizing arm pulls if needed. We develop algorithms with sub-linear regret,
as well as a lower bound that demonstrates that our algorithms are optimal up
to logarithmic factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01428">Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation. (arXiv:2302.01428v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loo_N/0/1/0/all/0/1">Noel Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1">Mathias Lechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1">Alexander Amini</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>Modern deep learning requires large volumes of data, which could contain
sensitive or private information that cannot be leaked. Recent work has shown
for homogeneous neural networks a large portion of this training data could be
reconstructed with only access to the trained network parameters. While the
attack was shown to work empirically, there exists little formal understanding
of its effective regime which datapoints are susceptible to reconstruction. In
this work, we first build a stronger version of the dataset reconstruction
attack and show how it can provably recover the \emph{entire training set} in
the infinite width regime. We then empirically study the characteristics of
this attack on two-layer networks and reveal that its success heavily depends
on deviations from the frozen infinite-width Neural Tangent Kernel limit. Next,
we study the nature of easily-reconstructed images. We show that both
theoretically and empirically, reconstructed images tend to "outliers" in the
dataset, and that these reconstruction attacks can be used for \textit{dataset
distillation}, that is, we can retrain on reconstructed images and obtain high
predictive accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04071">Taming Local Effects in Graph-based Spatiotemporal Forecasting. (arXiv:2302.04071v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1">Andrea Cini</a>, <a href="http://arxiv.org/find/cs/1/au:+Marisca_I/0/1/0/all/0/1">Ivan Marisca</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1">Daniele Zambon</a>, <a href="http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1">Cesare Alippi</a></p>
<p>Spatiotemporal graph neural networks have shown to be effective in time
series forecasting applications, achieving better performance than standard
univariate predictors in several settings. These architectures take advantage
of a graph structure and relational inductive biases to learn a single (global)
inductive model to predict any number of the input time series, each associated
with a graph node. Despite the gain achieved in computational and data
efficiency w.r.t. fitting a set of local models, relying on a single global
model can be a limitation whenever some of the time series are generated by a
different spatiotemporal stochastic process. The main objective of this paper
is to understand the interplay between globality and locality in graph-based
spatiotemporal forecasting, while contextually proposing a methodological
framework to rationalize the practice of including trainable node embeddings in
such architectures. We ascribe to trainable node embeddings the role of
amortizing the learning of specialized components. Moreover, embeddings allow
for 1) effectively combining the advantages of shared message-passing layers
with node-specific parameters and 2) efficiently transferring the learned model
to new node sets. Supported by strong empirical evidence, we provide insights
and guidelines for specializing graph-based models to the dynamics of each time
series and show how this aspect plays a crucial role in obtaining accurate
predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17896">Exploring the Limits of Deep Image Clustering using Pretrained Models. (arXiv:2303.17896v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adaloglou_N/0/1/0/all/0/1">Nikolas Adaloglou</a>, <a href="http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1">Felix Michels</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalisch_H/0/1/0/all/0/1">Hamza Kalisch</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollmann_M/0/1/0/all/0/1">Markus Kollmann</a></p>
<p>We present a general methodology that learns to classify images without
labels by leveraging pretrained feature extractors. Our approach involves
self-distillation training of clustering heads based on the fact that nearest
neighbours in the pretrained feature space are likely to share the same label.
We propose a novel objective that learns associations between image features by
introducing a variant of pointwise mutual information together with instance
weighting. We demonstrate that the proposed objective is able to attenuate the
effect of false positive pairs while efficiently exploiting the structure in
the pretrained feature space. As a result, we improve the clustering accuracy
over $k$-means on $17$ different pretrained models by $6.1$\% and $12.2$\% on
ImageNet and CIFAR100, respectively. Finally, using self-supervised vision
transformers, we achieve a clustering accuracy of $61.6$\% on ImageNet. The
code is available at https://github.com/HHU-MMBS/TEMI-official-BMVC2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02169">Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Synthetic electronic health records (EHRs) that are both realistic and
preserve privacy can serve as an alternative to real EHRs for machine learning
(ML) modeling and statistical analysis. However, generating high-fidelity and
granular electronic health record (EHR) data in its original,
highly-dimensional form poses challenges for existing methods due to the
complexities inherent in high-dimensional data. In this paper, we propose
Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal
high-dimensional EHR, which preserve the statistical properties of real EHR and
can be used to train accurate ML models without privacy concerns. Our HALO
method, designed as a hierarchical autoregressive model, generates a
probability density function of medical codes, clinical visits, and patient
records, allowing for the generation of realistic EHR data in its original,
unaggregated form without the need for variable selection or aggregation.
Additionally, our model also produces high-quality continuous variables in a
longitudinal and probabilistic manner. We conducted extensive experiments and
demonstrate that HALO can generate high-fidelity EHR data with high-dimensional
disease code probabilities (d &gt; 10,000), disease co-occurrence probabilities
within visits (d &gt; 1,000,000), and conditional probabilities across consecutive
visits (d &gt; 5,000,000) and achieve above 0.9 R2 correlation in comparison to
real EHR data. This performance then enables downstream ML models trained on
its synthetic data to achieve comparable accuracy to models trained on real
data (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using a
combination of real and synthetic data enhances the accuracy of ML models
beyond that achieved by using only real EHR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02754">Conceptual structure coheres in human cognition but not in large language models. (arXiv:2304.02754v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1">Siddharth Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1">Kushin Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xizheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei-Chun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Padua_L/0/1/0/all/0/1">Lisa Padua</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1">Timothy T Rogers</a></p>
<p>Neural network models of language have long been used as a tool for
developing hypotheses about conceptual representation in the mind and brain.
For many years, such use involved extracting vector-space representations of
words and using distances among these to predict or understand human behavior
in various semantic tasks. Contemporary large language models (LLMs), however,
make it possible to interrogate the latent structure of conceptual
representations using experimental methods nearly identical to those commonly
used with human participants. The current work utilizes three common techniques
borrowed from cognitive psychology to estimate and compare the structure of
concepts in humans and a suite of LLMs. In humans, we show that conceptual
structure is robust to differences in culture, language, and method of
estimation. Structures estimated from LLM behavior, while individually fairly
consistent with those estimated from human behavior, vary much more depending
upon the particular task used to generate responses--across tasks, estimates of
conceptual structure from the very same model cohere less with one another than
do human structure estimates. These results highlight an important difference
between contemporary LLMs and human cognition, with implications for
understanding some fundamental limitations of contemporary machine language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05077">If consciousness is dynamically relevant, artificial intelligence isn&#x27;t conscious. (arXiv:2304.05077v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kleiner_J/0/1/0/all/0/1">Johannes Kleiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ludwig_T/0/1/0/all/0/1">Tim Ludwig</a></p>
<p>We demonstrate that if consciousness is relevant for the temporal evolution
of a system's states--that is, if it is dynamically relevant--then AI systems
cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or
other processors which have been designed and verified to adhere to
computational dynamics that systematically preclude or suppress deviations. The
design and verification preclude or suppress, in particular, potential
consciousness-related dynamical effects, so that if consciousness is
dynamically relevant, AI systems cannot be conscious.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05727">Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Linhardt_L/0/1/0/all/0/1">Lorenz Linhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Montavon_G/0/1/0/all/0/1">Gr&#xe9;goire Montavon</a></p>
<p>Robustness has become an important consideration in deep learning. With the
help of explainable AI, mismatches between an explained model's decision
strategy and the user's domain knowledge (e.g. Clever Hans effects) have been
identified as a starting point for improving faulty models. However, it is less
clear what to do when the user and the explanation agree. In this paper, we
demonstrate that acceptance of explanations by the user is not a guarantee for
a machine learning model to be robust against Clever Hans effects, which may
remain undetected. Such hidden flaws of the model can nevertheless be
mitigated, and we demonstrate this by contributing a new method,
Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes
variations in the ML model that have not been the subject of positive
explanation feedback. Experiments demonstrate that our approach leads to models
that strongly reduce their reliance on hidden Clever Hans strategies, and
consequently achieve higher accuracy on new data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01219">Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jinming Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junbo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>The prompt-based learning paradigm, which bridges the gap between
pre-training and fine-tuning, achieves state-of-the-art performance on several
NLP tasks, particularly in few-shot settings. Despite being widely applied,
prompt-based learning is vulnerable to backdoor attacks. Textual backdoor
attacks are designed to introduce targeted vulnerabilities into models by
poisoning a subset of training samples through trigger injection and label
modification. However, they suffer from flaws such as abnormal natural language
expressions resulting from the trigger and incorrect labeling of poisoned
samples. In this study, we propose ProAttack, a novel and efficient method for
performing clean-label backdoor attacks based on the prompt, which uses the
prompt itself as a trigger. Our method does not require external triggers and
ensures correct labeling of poisoned samples, improving the stealthy nature of
the backdoor attack. With extensive experiments on rich-resource and few-shot
text classification tasks, we empirically validate ProAttack's competitive
performance in textual backdoor attacks. Notably, in the rich-resource setting,
ProAttack achieves state-of-the-art attack success rates in the clean-label
backdoor attack benchmark without external triggers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09156">Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network. (arXiv:2305.09156v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zitang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yen-Ju Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yung-hao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishida_S/0/1/0/all/0/1">Shin&#x27;ya Nishida</a></p>
<p>Visual motion processing is essential for humans to perceive and interact
with dynamic environments. Despite extensive research in cognitive
neuroscience, image-computable models that can extract informative motion flow
from natural scenes in a manner consistent with human visual processing have
yet to be established. Meanwhile, recent advancements in computer vision (CV),
propelled by deep learning, have led to significant progress in optical flow
estimation, a task closely related to motion perception. Here we propose an
image-computable model of human motion perception by bridging the gap between
biological and CV models. Specifically, we introduce a novel two-stages
approach that combines trainable motion energy sensing with a recurrent
self-attention network for adaptive motion integration and segregation. This
model architecture aims to capture the computations in V1-MT, the core
structure for motion perception in the biological visual system, while
providing the ability to derive informative motion flow for a wide range of
stimuli, including complex natural scenes. In silico neurophysiology reveals
that our model's unit responses are similar to mammalian neural recordings
regarding motion pooling and speed tuning. The proposed model can also
replicate human responses to a range of stimuli examined in past psychophysical
studies. The experimental results on the Sintel benchmark demonstrate that our
model predicts human responses better than the ground truth, whereas the
state-of-the-art CV models show the opposite. Our study provides a
computational architecture consistent with human visual motion processing,
although the physiological correspondence may not be exact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12553">Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis. (arXiv:2305.12553v4 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_C/0/1/0/all/0/1">Chinmay Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1">Shankar Sastry</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Manxi Wu</a></p>
<p>This paper proposes a new notion of Markov $\alpha$-potential games to study
Markov games. Two important classes of practically significant Markov games,
Markov congestion games and the perturbed Markov team games, are analyzed in
this framework of Markov $\alpha$-potential games, with explicit
characterization of the upper bound for $\alpha$ and its relation to game
parameters. Moreover, any maximizer of the $\alpha$-potential function is shown
to be an $\alpha$-stationary Nash equilibrium of the game. Furthermore, two
algorithms for the Nash regret analysis, namely the projected gradient-ascent
algorithm and the sequential maximum improvement algorithm, are presented and
corroborated by numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13971">Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. (arXiv:2305.13971v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1">Saibo Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1">Martin Josifoski</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1">Maxime Peyrard</a>, <a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1">Robert West</a></p>
<p>Despite their impressive performance, large language models (LMs) still
struggle with reliably generating complex output structures when not finetuned
to follow the required output format exactly. To address this issue,
grammar-constrained decoding (GCD) can be used to control the generation of
LMs, guaranteeing that the output follows a given structure. Most existing GCD
methods are, however, limited to specific tasks, such as parsing or code
generation. In this work, we demonstrate that formal grammars can describe the
output space for a much wider range of tasks and argue that GCD can serve as a
unified framework for structured NLP tasks in general. For increased
flexibility, we introduce input-dependent grammars, which allow the grammar to
depend on the input and thus enable the generation of different output
structures for different inputs. We then empirically demonstrate the power and
flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity
disambiguation, and (3) constituency parsing. Our results indicate that
grammar-constrained LMs substantially outperform unconstrained LMs or even beat
task-specific finetuned models. Grammar constraints thus hold great promise for
harnessing off-the-shelf LMs for a wide range of structured NLP tasks,
especially where training data is scarce or finetuning is expensive. Code and
data: https://github.com/epfl-dlab/GCD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17328">Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dedhia_B/0/1/0/all/0/1">Bhishma Dedhia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1">Niraj K. Jha</a></p>
<p>Deployment of Transformer models on edge devices is becoming increasingly
challenging due to the exponentially growing inference cost that scales
quadratically with the number of tokens in the input sequence. Token pruning is
an emerging solution to address this challenge due to its ease of deployment on
various Transformer backbones. However, most token pruning methods require
computationally expensive fine-tuning, which is undesirable in many edge
deployment cases. In this work, we propose Zero-TPrune, the first zero-shot
method that considers both the importance and similarity of tokens in
performing token pruning. It leverages the attention graph of pre-trained
Transformer models to produce an importance distribution for tokens via our
proposed Weighted Page Rank (WPR) algorithm. This distribution further guides
token partitioning for efficient similarity-based pruning. Due to the
elimination of the fine-tuning overhead, Zero-TPrune can prune large models at
negligible computational cost, switch between different pruning configurations
at no computational cost, and perform hyperparameter tuning efficiently. We
evaluate the performance of Zero-TPrune on vision tasks by applying it to
various vision Transformer backbones and testing them on ImageNet. Without any
fine-tuning, Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7\% and
improves its throughput by 45.3\% with only 0.4\% accuracy loss. Compared with
state-of-the-art pruning methods that require fine-tuning, Zero-TPrune not only
eliminates the need for fine-tuning after pruning but also does so with only
0.1\% accuracy loss. Compared with state-of-the-art fine-tuning-free pruning
methods, Zero-TPrune reduces accuracy loss by up to 49\% with the same or
higher throughput.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19454">Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Lu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tianjin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1">Vlado Menkovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaolong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1">Mykola Pechenizkiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiwei Liu</a></p>
<p>Sparse training has received an upsurging interest in machine learning due to
its tantalizing saving potential for the entire training process as well as
inference. Dynamic sparse training (DST), as a leading sparse training
approach, can train deep neural networks at high sparsity from scratch to match
the performance of their dense counterparts. However, most if not all DST prior
arts demonstrate their effectiveness on unstructured sparsity with highly
irregular sparse patterns, which receives limited support in common hardware.
This limitation hinders the usage of DST in practice. In this paper, we propose
Channel-aware dynamic sparse (Chase), which for the first time seamlessly
translates the promise of unstructured dynamic sparsity to GPU-friendly
channel-level sparsity (not fine-grained N:M or group sparsity) during one
end-to-end training process, without any ad-hoc operations. The resulting small
sparse networks can be directly accelerated by commodity hardware, without
using any particularly sparsity-aware hardware accelerators. This appealing
outcome is partially motivated by a hidden phenomenon of dynamic sparsity:
off-the-shelf unstructured DST implicitly involves biased parameter
reallocation across channels, with a large fraction of channels (up to 60%)
being sparser than others. By progressively identifying and removing these
channels during training, our approach translates unstructured sparsity to
channel-wise sparsity. Our experimental results demonstrate that Chase achieves
1.7 X inference throughput speedup on common GPU devices without compromising
accuracy with ResNet-50 on ImageNet. We release our codes in
https://github.com/luuyin/chase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00297">Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahn_K/0/1/0/all/0/1">Kwangjun Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiang Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Daneshmand_H/0/1/0/all/0/1">Hadi Daneshmand</a>, <a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1">Suvrit Sra</a></p>
<p>Several recent works demonstrate that transformers can implement algorithms
like gradient descent. By a careful construction of weights, these works show
that multiple layers of transformers are expressive enough to simulate
iterations of gradient descent. Going beyond the question of expressivity, we
ask: Can transformers learn to implement such algorithms by training over
random problem instances? To our knowledge, we make the first theoretical
progress on this question via an analysis of the loss landscape for linear
transformers trained over random instances of linear regression. For a single
attention layer, we prove the global minimum of the training objective
implements a single iteration of preconditioned gradient descent. Notably, the
preconditioning matrix not only adapts to the input distribution but also to
the variance induced by data inadequacy. For a transformer with $L$ attention
layers, we prove certain critical points of the training objective implement
$L$ iterations of preconditioned gradient descent. Our results call for future
theoretical studies on learning algorithms by training transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09384">MobileASR: A resource-aware on-device learning framework for user voice personalization applications on mobile phones. (arXiv:2306.09384v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sasindran_Z/0/1/0/all/0/1">Zitha Sasindran</a>, <a href="http://arxiv.org/find/eess/1/au:+Yelchuri_H/0/1/0/all/0/1">Harsha Yelchuri</a>, <a href="http://arxiv.org/find/eess/1/au:+Rao_P/0/1/0/all/0/1">Pooja Rao</a>, <a href="http://arxiv.org/find/eess/1/au:+Prabhakar_T/0/1/0/all/0/1">T. V. Prabhakar</a></p>
<p>We describe a comprehensive methodology for developing user-voice
personalized automatic speech recognition (ASR) models by effectively training
models on mobile phones, allowing user data and models to be stored and used
locally. To achieve this, we propose a resource-aware sub-model-based training
approach that considers the RAM, and battery capabilities of mobile phones. By
considering the evaluation metric and resource constraints of the mobile
phones, we are able to perform efficient training and halt the process
accordingly. To simulate real users, we use speakers with various accents. The
entire on-device training and evaluation framework was then tested on various
mobile phones across brands. We show that fine-tuning the models and selecting
the right hyperparameter values is a trade-off between the lowest achievable
performance metric, on-device training time, and memory consumption. Overall,
our methodology offers a comprehensive solution for developing personalized ASR
models while leveraging the capabilities of mobile phones, and balancing the
need for accuracy with resource constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15362">Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?. (arXiv:2306.15362v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilken_N/0/1/0/all/0/1">Nils Wilken</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohausz_L/0/1/0/all/0/1">Lea Cohausz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartelt_C/0/1/0/all/0/1">Christian Bartelt</a>, <a href="http://arxiv.org/find/cs/1/au:+Stuckenschmidt_H/0/1/0/all/0/1">Heiner Stuckenschmidt</a></p>
<p>Goal recognition is an important problem in many application domains (e.g.,
pervasive computing, intrusion detection, computer games, etc.). In many
application scenarios, it is important that goal recognition algorithms can
recognize goals of an observed agent as fast as possible. However, many early
approaches in the area of Plan Recognition As Planning, require quite large
amounts of computation time to calculate a solution. Mainly to address this
issue, recently, Pereira et al. developed an approach that is based on planning
landmarks and is much more computationally efficient than previous approaches.
However, the approach, as proposed by Pereira et al., also uses trivial
landmarks (i.e., facts that are part of the initial state and goal description
are landmarks by definition). In this paper, we show that it does not provide
any benefit to use landmarks that are part of the initial state in a planning
landmark based goal recognition approach. The empirical results show that
omitting initial state landmarks for goal recognition improves goal recognition
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00177">Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1">Charlie Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Thekumparampil_K/0/1/0/all/0/1">Kiran Koshy Thekumparampil</a>, <a href="http://arxiv.org/find/cs/1/au:+Shavlovsky_M/0/1/0/all/0/1">Michael Shavlovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1">Giulia Fanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Dattatreya_Y/0/1/0/all/0/1">Yesh Dattatreya</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1">Sujay Sanghavi</a></p>
<p>While deep learning (DL) models are state-of-the-art in text and image
domains, they have not yet consistently outperformed Gradient Boosted Decision
Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent
performance gains attained by DL models in text and image tasks have used
unsupervised pretraining, which exploits orders of magnitude more unlabeled
data than labeled data. To the best of our knowledge, unsupervised pretraining
has not been applied to the LTR problem, which often produces vast amounts of
unlabeled data.
</p>
<p>In this work, we study whether unsupervised pretraining of deep models can
improve LTR performance over GBDTs and other non-pretrained models. By
incorporating simple design choices--including SimCLR-Rank, an LTR-specific
pretraining loss--we produce pretrained deep learning models that consistently
(across datasets) outperform GBDTs (and other non-pretrained rankers) in the
case where there is more unlabeled data than labeled data. This performance
improvement occurs not only on average but also on outlier queries. We base our
empirical conclusions off of experiments on (1) public benchmark tabular LTR
datasets, and (2) a large industry-scale proprietary ranking dataset. Code is
provided at https://anonymous.4open.science/r/ltr-pretrain-0DAD/README.md.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10664">A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks. (arXiv:2308.10664v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koursioumpas_N/0/1/0/all/0/1">Nikolaos Koursioumpas</a>, <a href="http://arxiv.org/find/cs/1/au:+Magoula_L/0/1/0/all/0/1">Lina Magoula</a>, <a href="http://arxiv.org/find/cs/1/au:+Petropouleas_N/0/1/0/all/0/1">Nikolaos Petropouleas</a>, <a href="http://arxiv.org/find/cs/1/au:+Thanopoulos_A/0/1/0/all/0/1">Alexandros-Ioannis Thanopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagea_T/0/1/0/all/0/1">Theodora Panagea</a>, <a href="http://arxiv.org/find/cs/1/au:+Alonistioti_N/0/1/0/all/0/1">Nancy Alonistioti</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1">M. A. Gutierrez-Estevez</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalili_R/0/1/0/all/0/1">Ramin Khalili</a></p>
<p>Progressing towards a new era of Artificial Intelligence (AI) - enabled
wireless networks, concerns regarding the environmental impact of AI have been
raised both in industry and academia. Federated Learning (FL) has emerged as a
key privacy preserving decentralized AI technique. Despite efforts currently
being made in FL, its environmental impact is still an open problem. Targeting
the minimization of the overall energy consumption of an FL process, we propose
the orchestration of computational and communication resources of the involved
devices to minimize the total energy required, while guaranteeing a certain
performance of the model. To this end, we propose a Soft Actor Critic Deep
Reinforcement Learning (DRL) solution, where a penalty function is introduced
during training, penalizing the strategies that violate the constraints of the
environment, and contributing towards a safe RL process. A device level
synchronization method, along with a computationally cost effective FL
environment are proposed, with the goal of further reducing the energy
consumption and communication overhead. Evaluation results show the
effectiveness and robustness of the proposed scheme compared to four
state-of-the-art baseline solutions on different network environments and FL
architectures, achieving a decrease of up to 94% in the total energy
consumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08036">BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qutub_S/0/1/0/all/0/1">Syed Sha Qutub</a>, <a href="http://arxiv.org/find/cs/1/au:+Kose_N/0/1/0/all/0/1">Neslihan Kose</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosales_R/0/1/0/all/0/1">Rafael Rosales</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulitsch_M/0/1/0/all/0/1">Michael Paulitsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Hagn_K/0/1/0/all/0/1">Korbinian Hagn</a>, <a href="http://arxiv.org/find/cs/1/au:+Geissler_F/0/1/0/all/0/1">Florian Geissler</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinz_G/0/1/0/all/0/1">Gereon Hinz</a>, <a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1">Alois Knoll</a></p>
<p>This paper introduces the Budding Ensemble Architecture (BEA), a novel
reduced ensemble architecture for anchor-based object detection models. Object
detection models are crucial in vision-based tasks, particularly in autonomous
systems. They should provide precise bounding box detections while also
calibrating their predicted confidence scores, leading to higher-quality
uncertainty estimates. However, current models may make erroneous decisions due
to false positives receiving high scores or true positives being discarded due
to low scores. BEA aims to address these issues. The proposed loss functions in
BEA improve the confidence score calibration and lower the uncertainty error,
which results in a better distinction of true and false positives and,
eventually, higher accuracy of the object detection models. Both Base-YOLOv3
and SSD models were enhanced using the BEA method and its proposed loss
functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6%
and 3.7% increase in mAP and AP50, respectively. Utilizing a well-balanced
uncertainty estimation threshold to discard samples in real-time even leads to
a 9.6% higher AP50 than its base model. This is attributed to a 40% increase in
the area under the AP50-based retention curve used to measure the quality of
calibration of confidence scores. Furthermore, BEA-YOLOV3 trained on KITTI
provides superior out-of-distribution detection on Citypersons, BDD100K, and
COCO datasets compared to the ensembles and vanilla models of YOLOv3 and
Gaussian-YOLOv3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12482">State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Devleena Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1">Sonia Chernova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Been Kim</a></p>
<p>As more non-AI experts use complex AI systems for daily tasks, there has been
an increasing effort to develop methods that produce explanations of AI
decision making that are understandable by non-AI experts. Towards this effort,
leveraging higher-level concepts and producing concept-based explanations have
become a popular method. Most concept-based explanations have been developed
for classification techniques, and we posit that the few existing methods for
sequential decision making are limited in scope. In this work, we first
contribute a desiderata for defining concepts in sequential decision making
settings. Additionally, inspired by the Protege Effect which states explaining
knowledge often reinforces one's self-learning, we explore how concept-based
explanations of an RL agent's decision making can in turn improve the agent's
learning rate, as well as improve end-user understanding of the agent's
decision making. To this end, we contribute a unified framework,
State2Explanation (S2E), that involves learning a joint embedding model between
state-action pairs and concept-based explanations, and leveraging such learned
model to both (1) inform reward shaping during an agent's training, and (2)
provide explanations to end-users at deployment for improved task performance.
Our experimental validations, in Connect 4 and Lunar Lander, demonstrate the
success of S2E in providing a dual-benefit, successfully informing reward
shaping and improving agent learning rate, as well as significantly improving
end user task performance at deployment time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15877">Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xiongye Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gengshuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1">Gaurav Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1">Defu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shixuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaxing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Tianqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Mingxi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1">Paul Bogdan</a></p>
<p>Integrating and processing information from various sources or modalities are
critical for obtaining a comprehensive and accurate perception of the real
world. Drawing inspiration from neuroscience, we develop the
Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the
concept of information bottleneck. Distinct from most traditional fusion models
that aim to incorporate all modalities as input, our model designates the prime
modality as input, while the remaining modalities act as detectors in the
information pathway. Our proposed perception model focuses on constructing an
effective and compact information flow by achieving a balance between the
minimization of mutual information between the latent state and the input modal
state, and the maximization of mutual information between the latent states and
the remaining modal states. This approach leads to compact latent state
representations that retain relevant information while minimizing redundancy,
thereby substantially enhancing the performance of downstream tasks.
Experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstrate
that our model consistently distills crucial information in multimodal learning
scenarios, outperforming state-of-the-art benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00564">DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor. (arXiv:2310.00564v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Richter_O/0/1/0/all/0/1">Ole Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Whatley_A/0/1/0/all/0/1">Adrian M. Whatley</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostinger_G/0/1/0/all/0/1">German K&#xf6;stinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Nielsen_C/0/1/0/all/0/1">Carsten Nielsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_N/0/1/0/all/0/1">Ning Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Indiveri_G/0/1/0/all/0/1">Giacomo Indiveri</a></p>
<p>With the remarkable progress that technology has made, the need for
processing data near the sensors at the edge has increased dramatically. The
electronic systems used in these applications must process data continuously,
in real-time, and extract relevant information using the smallest possible
energy budgets. A promising approach for implementing always-on processing of
sensory signals that supports on-demand, sparse, and edge-computing is to take
inspiration from biological nervous system. Following this approach, we present
a brain-inspired platform for prototyping real-time event-based Spiking Neural
Networks (SNNs). The system proposed supports the direct emulation of dynamic
and realistic neural processing phenomena such as short-term plasticity, NMDA
gating, AMPA diffusion, homeostasis, spike frequency adaptation,
conductance-based dendritic compartments and spike transmission delays. The
analog circuits that implement such primitives are paired with a low latency
asynchronous digital circuits for routing and mapping events. This asynchronous
infrastructure enables the definition of different network architectures, and
provides direct event-based interfaces to convert and encode data from
event-based and continuous-signal sensors. Here we describe the overall system
architecture, we characterize the mixed signal analog-digital circuits that
emulate neural dynamics, demonstrate their features with experimental
measurements, and present a low- and high-level software ecosystem that can be
used for configuring the system. The flexibility to emulate different
biologically plausible neural networks, and the chip's ability to monitor both
population and single neuron signals in real-time, allow to develop and
validate complex models of neural processing for both basic research and
edge-computing applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04010">Excision And Recovery: Visual Defect Obfuscation Based Self-Supervised Anomaly Detection Strategy. (arXiv:2310.04010v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">YeongHyeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Sungho Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Myung Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yeonho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyeong Seok Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1">Juneho Yi</a></p>
<p>Due to scarcity of anomaly situations in the early manufacturing stage, an
unsupervised anomaly detection (UAD) approach is widely adopted which only uses
normal samples for training. This approach is based on the assumption that the
trained UAD model will accurately reconstruct normal patterns but struggles
with unseen anomalous patterns. To enhance the UAD performance,
reconstruction-by-inpainting based methods have recently been investigated,
especially on the masking strategy of suspected defective regions. However,
there are still issues to overcome: 1) time-consuming inference due to multiple
masking, 2) output inconsistency by random masking strategy, and 3) inaccurate
reconstruction of normal patterns when the masked area is large. Motivated by
this, we propose a novel reconstruction-by-inpainting method, dubbed Excision
And Recovery (EAR), that features single deterministic masking based on the
ImageNet pre-trained DINO-ViT and visual obfuscation for hint-providing.
Experimental results on the MVTec AD dataset show that deterministic masking by
pre-trained attention effectively cuts out suspected defective regions and
resolve the aforementioned issues 1 and 2. Also, hint-providing by mosaicing
proves to enhance the UAD performance than emptying those regions by binary
masking, thereby overcomes issue 3. Our approach achieves a high UAD
performance without any change of the neural network structure. Thus, we
suggest that EAR be adopted in various manufacturing industries as a
practically deployable solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06552">Automated clinical coding using off-the-shelf large language models. (arXiv:2310.06552v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boyle_J/0/1/0/all/0/1">Joseph S. Boyle</a>, <a href="http://arxiv.org/find/cs/1/au:+Kascenas_A/0/1/0/all/0/1">Antanas Kascenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lok_P/0/1/0/all/0/1">Pat Lok</a>, <a href="http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1">Maria Liakata</a>, <a href="http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1">Alison Q. O&#x27;Neil</a></p>
<p>The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment.
Unsupervised pre-training alone does not guarantee precise knowledge of the ICD
ontology and specialist clinical coding task, therefore we frame the task as
information extraction, providing a description of each coded concept and
asking the model to retrieve related mentions. For efficiency, rather than
iterating over all codes, we leverage the hierarchical nature of the ICD
ontology to sparsely search for relevant codes. Then, in a second stage, which
we term 'meta-refinement', we utilise GPT-4 to select a subset of the relevant
labels as predictions. We validate our method using Llama-2, GPT-3.5 and GPT-4
on the CodiEsp dataset of ICD-coded clinical case documents. Our tree-search
method achieves state-of-the-art performance on rarer classes, achieving the
best macro-F1 of 0.225, whilst achieving slightly lower micro-F1 of 0.157,
compared to 0.216 and 0.219 respectively from PLM-ICD. To the best of our
knowledge, this is the first method for automated ICD coding requiring no
task-specific learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07325">An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dao_J/0/1/0/all/0/1">James Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_Y/0/1/0/all/0/1">Yeu-Tong Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Rager_C/0/1/0/all/0/1">Can Rager</a>, <a href="http://arxiv.org/find/cs/1/au:+Janiak_J/0/1/0/all/0/1">Jett Janiak</a></p>
<p>How do language models deal with the limited bandwidth of the residual
stream? Prior work has suggested that some attention heads and MLP layers may
perform a "memory management" role. That is, clearing residual stream
directions set by earlier layers by reading in information and writing out the
negative version. In this work, we present concrete evidence for this
phenomenon in a 4-layer transformer. We identify several heads in layer 2 that
consistently remove the output of a single layer 0 head. We then verify that
this erasure causally depends on the original written direction. We further
demonstrate that direct logit attribution (DLA) suggests that writing and
erasing heads directly contribute to predictions, when in fact their effects
cancel out. Then we present adversarial prompts for which this effect is
particularly salient. These findings reveal that memory management can make DLA
results misleading. Accordingly, we make concrete recommendations for circuit
analysis to prevent interpretability illusions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09183">PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Mingjia Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huaizheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shudong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiangcheng Lv</a></p>
<p>Classical federated learning (FL) enables training machine learning models
without sharing data for privacy preservation, but heterogeneous data
characteristic degrades the performance of the localized model. Personalized FL
(PFL) addresses this by synthesizing personalized models from a global model
via training on local data. Such a global model may overlook the specific
information that the clients have been sampled. In this paper, we propose a
novel scheme to inject personalized prior knowledge into the global model in
each client, which attempts to mitigate the introduced incomplete information
problem in PFL. At the heart of our proposed approach is a framework, the PFL
with Bregman Divergence (pFedBreD), decoupling the personalized prior from the
local objective function regularized by Bregman divergence for greater
adaptability in personalized scenarios. We also relax the mirror descent (RMD)
to extract the prior explicitly to provide optional strategies. Additionally,
our pFedBreD is backed up by a convergence analysis. Sufficient experiments
demonstrate that our method reaches the state-of-the-art performances on 5
datasets and outperforms other methods by up to 3.5% across 8 benchmarks.
Extensive analyses verify the robustness and necessity of proposed designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13001">Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Stephen Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gazeley_W/0/1/0/all/0/1">William Gazeley</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1">Siu Ho Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tingting Li</a></p>
<p>With the exponential growth in large language models (LLMs), leveraging their
emergent properties for specialized domains like finance merits exploration.
However, regulated fields such as finance pose unique constraints, requiring
domain-optimized frameworks. We present ConFIRM, an LLM-based conversational
financial information retrieval model tailored for query intent classification
and knowledge base labeling.
</p>
<p>ConFIRM comprises two modules:
</p>
<p>1) a method to synthesize finance domain-specific question-answer pairs, and
</p>
<p>2) evaluation of parameter efficient fine-tuning approaches for the query
classification task. We generate a dataset of over 4000 samples, assessing
accuracy on a separate test set.
</p>
<p>ConFIRM achieved over 90% accuracy, essential for regulatory compliance.
ConFIRM provides a data-efficient solution to extract precise query intent for
financial dialog systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15823">Rosetta Stone at the Arabic Reverse Dictionary Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+ElBakry_A/0/1/0/all/0/1">Ahmed ElBakry</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabr_M/0/1/0/all/0/1">Mohamed Gabr</a>, <a href="http://arxiv.org/find/cs/1/au:+ElNokrashy_M/0/1/0/all/0/1">Muhammad ElNokrashy</a>, <a href="http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1">Badr AlKhamissi</a></p>
<p>A Reverse Dictionary is a tool enabling users to discover a word based on its
provided definition, meaning, or description. Such a technique proves valuable
in various scenarios, aiding language learners who possess a description of a
word without its identity, and benefiting writers seeking precise terminology.
These scenarios often encapsulate what is referred to as the
"Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning
solution for the Arabic Reverse Dictionary shared task. This task focuses on
deriving a vector representation of an Arabic word from its accompanying
description. The shared task encompasses two distinct subtasks: the first
involves an Arabic definition as input, while the second employs an English
definition. For the first subtask, our approach relies on an ensemble of
finetuned Arabic BERT-based models, predicting the word embedding for a given
definition. The final representation is obtained through averaging the output
embeddings from each model within the ensemble. In contrast, the most effective
solution for the second subtask involves translating the English test
definitions into Arabic and applying them to the finetuned models originally
trained for the first subtask. This straightforward method achieves the highest
score across both subtasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19805">Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1">Xiao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>A prospective application of offline reinforcement learning (RL) involves
initializing a pre-trained policy using existing static datasets for subsequent
online fine-tuning. However, direct fine-tuning of the offline pre-trained
policy often results in sub-optimal performance. A primary reason is that
offline conservative methods diminish the agent's capability of exploration,
thereby impacting online fine-tuning performance. To enhance exploration during
online fine-tuning and thus enhance the overall online fine-tuning performance,
we introduce a generalized reward augmentation framework called Sample
Efficient Reward Augmentation (SERA). SERA aims to improve the performance of
online fine-tuning by designing intrinsic rewards that encourage the agent to
explore. Specifically, it implicitly implements State Marginal Matching (SMM)
and penalizes out-of-distribution (OOD) state actions, thus encouraging agents
to cover the target state density, and achieving better online fine-tuning
results. Additionally, SERA can be effortlessly plugged into various RL
algorithms to improve online fine-tuning and ensure sustained asymptotic
improvement, showing the versatility as well as the effectiveness of SERA.
Moreover, extensive experimental results will demonstrate that when conducting
offline-to-online problems, SERA consistently and effectively enhances the
performance of various offline algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20463">Interpretable Neural PDE Solvers using Symbolic Frameworks. (arXiv:2310.20463v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yolanne Yi Ran Lee</a></p>
<p>Partial differential equations (PDEs) are ubiquitous in the world around us,
modelling phenomena from heat and sound to quantum systems. Recent advances in
deep learning have resulted in the development of powerful neural solvers;
however, while these methods have demonstrated state-of-the-art performance in
both accuracy and computational efficiency, a significant challenge remains in
their interpretability. Most existing methodologies prioritize predictive
accuracy over clarity in the underlying mechanisms driving the model's
decisions. Interpretability is crucial for trustworthiness and broader
applicability, especially in scientific and engineering domains where neural
PDE solvers might see the most impact. In this context, a notable gap in
current research is the integration of symbolic frameworks (such as symbolic
regression) into these solvers. Symbolic frameworks have the potential to
distill complex neural operations into human-readable mathematical expressions,
bridging the divide between black-box predictions and solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00157">Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Guoxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Danier_D/0/1/0/all/0/1">Duolikun Danier</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Ayan Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Fotiadis_S/0/1/0/all/0/1">Stathi Fotiadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabiei_F/0/1/0/all/0/1">Farhang Nabiei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_U/0/1/0/all/0/1">Ushnish Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1">Alberto Bernacchia</a></p>
<p>Recently, Zhang et al. have proposed the Diffusion Exponential Integrator
Sampler (DEIS) for fast generation of samples from Diffusion Models. It
leverages the semi-linear nature of the probability flow ordinary differential
equation (ODE) in order to greatly reduce integration error and improve
generation quality at low numbers of function evaluations (NFEs). Key to this
approach is the score function reparameterisation, which reduces the
integration error incurred from using a fixed score function estimate over each
integration step. The original authors use the default parameterisation used by
models trained for noise prediction -- multiply the score by the standard
deviation of the conditional forward noising distribution. We find that
although the mean absolute value of this score parameterisation is close to
constant for a large portion of the reverse sampling process, it changes
rapidly at the end of sampling. As a simple fix, we propose to instead
reparameterise the score (at inference) by dividing it by the average absolute
value of previous score estimates at that time step collected from offline high
NFE generations. We find that our score normalisation (DEIS-SN) consistently
improves FID compared to vanilla DEIS, showing an improvement at 10 NFEs from
6.44 to 5.57 on CIFAR-10 and from 5.9 to 4.95 on LSUN-Church 64x64. Our code is
available at https://github.com/mtkresearch/Diffusion-DEIS-SN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00530">The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jinzhou Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Han Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rongtao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Li Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shibiao Xu</a></p>
<p>In recent years, the rapid advancement of Large Language Models (LLMs) such
as the Generative Pre-trained Transformer (GPT) has attracted increasing
attention due to their potential in a variety of practical applications. The
application of LLMs with Embodied Intelligence has emerged as a significant
area of focus. Among the myriad applications of LLMs, navigation tasks are
particularly noteworthy because they demand a deep understanding of the
environment and quick, accurate decision-making. LLMs can augment embodied
intelligence systems with sophisticated environmental perception and
decision-making support, leveraging their robust language and image-processing
capabilities. This article offers an exhaustive summary of the symbiosis
between LLMs and embodied intelligence with a focus on navigation. It reviews
state-of-the-art models, research methodologies, and assesses the advantages
and disadvantages of existing embodied navigation models and datasets. Finally,
the article elucidates the role of LLMs in embodied intelligence, based on
current research, and forecasts future directions in the field. A comprehensive
list of studies in this survey is available at
https://github.com/Rongtao-Xu/Awesome-LLM-EN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01490">The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oli_P/0/1/0/all/0/1">Priti Oli</a>, <a href="http://arxiv.org/find/cs/1/au:+Banjade_R/0/1/0/all/0/1">Rabin Banjade</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapagain_J/0/1/0/all/0/1">Jeevan Chapagain</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_V/0/1/0/all/0/1">Vasile Rus</a></p>
<p>This paper systematically investigates the generation of code explanations by
Large Language Models (LLMs) for code examples commonly encountered in
introductory programming courses. Our findings reveal significant variations in
the nature of code explanations produced by LLMs, influenced by factors such as
the wording of the prompt, the specific code examples under consideration, the
programming language involved, the temperature parameter, and the version of
the LLM. However, a consistent pattern emerges for Java and Python, where
explanations exhibit a Flesch-Kincaid readability level of approximately 7-8
grade and a consistent lexical density, indicating the proportion of meaningful
words relative to the total explanation size. Additionally, the generated
explanations consistently achieve high scores for correctness, but lower scores
on three other metrics: completeness, conciseness, and specificity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02329">Complex Organ Mask Guided Radiology Report Generation. (arXiv:2311.02329v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1">Tiancheng Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dongnan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weidong Cai</a></p>
<p>The goal of automatic report generation is to generate a clinically accurate
and coherent phrase from a single given X-ray image, which could alleviate the
workload of traditional radiology reporting. However, in a real-world scenario,
radiologists frequently face the challenge of producing extensive reports
derived from numerous medical images, thereby medical report generation from
multi-image perspective is needed. In this paper, we propose the Complex Organ
Mask Guided (termed as COMG) report generation model, which incorporates masks
from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide
more detailed information and guide the model's attention to these crucial body
regions. Specifically, we leverage prior knowledge of the disease corresponding
to each organ in the fusion process to enhance the disease identification phase
during the report generation process. Additionally, cosine similarity loss is
introduced as target function to ensure the convergence of cross-modal
consistency and facilitate model optimization.Experimental results on two
public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms
of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.
The code is publicly available at https://github.com/GaryGuTC/COMG_model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03508">Astrocytes as a mechanism for meta-plasticity and contextually-guided network function. (arXiv:2311.03508v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Gong_L/0/1/0/all/0/1">Lulu Gong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pasqualetti_F/0/1/0/all/0/1">Fabio Pasqualetti</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Papouin_T/0/1/0/all/0/1">Thomas Papouin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ching_S/0/1/0/all/0/1">ShiNung Ching</a></p>
<p>Astrocytes are a ubiquitous and enigmatic type of non-neuronal cell and are
found in the brain of all vertebrates. While traditionally viewed as being
supportive of neurons, it is increasingly recognized that astrocytes may play a
more direct and active role in brain function and neural computation. On
account of their sensitivity to a host of physiological covariates and ability
to modulate neuronal activity and connectivity on slower time scales,
astrocytes may be particularly well poised to modulate the dynamics of neural
circuits in functionally salient ways. In the current paper, we seek to capture
these features via actionable abstractions within computational models of
neuron-astrocyte interaction. Specifically, we engage how nested feedback loops
of neuron-astrocyte interaction, acting over separated time-scales may endow
astrocytes with the capability to enable learning in context-dependent
settings, where fluctuations in task parameters may occur much more slowly than
within-task requirements. We pose a general model of neuron-synapse-astrocyte
interaction and use formal analysis to characterize how astrocytic modulation
may constitute a form of meta-plasticity, altering the ways in which synapses
and neurons adapt as a function of time. We then embed this model in a
bandit-based reinforcement learning task environment, and show how the presence
of time-scale separated astrocytic modulation enables learning over multiple
fluctuating contexts. Indeed, these networks learn far more reliably versus
dynamically homogeneous networks and conventional non-network-based bandit
algorithms. Our results indicate how the presence of neuron-astrocyte
interaction in the brain may benefit learning over different time-scales and
the conveyance of task-relevant contextual information onto circuit dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03698">A Novel Variational Lower Bound for Inverse Reinforcement Learning. (arXiv:2311.03698v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gui_Y/0/1/0/all/0/1">Yikang Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1">Prashant Doshi</a></p>
<p>Inverse reinforcement learning (IRL) seeks to learn the reward function from
expert trajectories, to understand the task for imitation or collaboration
thereby removing the need for manual reward engineering. However, IRL in the
context of large, high-dimensional problems with unknown dynamics has been
particularly challenging. In this paper, we present a new Variational Lower
Bound for IRL (VLB-IRL), which is derived under the framework of a
probabilistic graphical model with an optimality node. Our method
simultaneously learns the reward function and policy under the learned reward
function by maximizing the lower bound, which is equivalent to minimizing the
reverse Kullback-Leibler divergence between an approximated distribution of
optimality given the reward function and the true distribution of optimality
given trajectories. This leads to a new IRL method that learns a valid reward
function such that the policy under the learned reward achieves expert-level
performance on several known domains. Importantly, the method outperforms the
existing state-of-the-art IRL algorithms on these domains by demonstrating
better reward from the learned policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04498">NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liming Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chen-Wei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04698">Challenging Common Assumptions in Multi-task Learning. (arXiv:2311.04698v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1">Cathrin Elich</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirchdorfer_L/0/1/0/all/0/1">Lukas Kirchdorfer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1">Jan M. K&#xf6;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1">Lukas Schott</a></p>
<p>While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge common assumptions in
MTL in the context of STL: First, the choice of optimizer has only been mildly
investigated in MTL. We show the pivotal role of common STL tools such as the
Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial
loss-scale invariance. Second, the notion of gradient conflicts has often been
phrased as a specific problem in MTL. We delve into the role of gradient
conflicts in MTL and compare it to STL. For angular gradient alignment we find
no evidence that this is a unique problem in MTL. We emphasize differences in
gradient magnitude as the main distinguishing factor. Lastly, we compare the
transferability of features learned through MTL and STL on common image
corruptions, and find no conclusive evidence that MTL leads to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05047">DeepLearningBrasil@LT-EDI-2023: Exploring Deep Learning Techniques for Detecting Depression in Social Media Text. (arXiv:2311.05047v1 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcia_E/0/1/0/all/0/1">Eduardo Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1">Juliana Gomes</a>, <a href="http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1">Adalberto Barbosa J&#xfa;nior</a>, <a href="http://arxiv.org/find/cs/1/au:+Borges_C/0/1/0/all/0/1">Cardeque Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1">N&#xe1;dia da Silva</a></p>
<p>In this paper, we delineate the strategy employed by our team,
DeepLearningBrasil, which secured us the first place in the shared task
DepSign-LT-EDI@RANLP-2023, achieving a 47.0% Macro F1-Score and a notable 2.4%
advantage. The task was to classify social media texts into three distinct
levels of depression - "not depressed," "moderately depressed," and "severely
depressed." Leveraging the power of the RoBERTa and DeBERTa models, we further
pre-trained them on a collected Reddit dataset, specifically curated from
mental health-related Reddit's communities (Subreddits), leading to an enhanced
understanding of nuanced mental health discourse. To address lengthy textual
data, we used truncation techniques that retained the essence of the content by
focusing on its beginnings and endings. Our model was robust against unbalanced
data by incorporating sample weights into the loss. Cross-validation and
ensemble techniques were then employed to combine our k-fold trained models,
delivering an optimal solution. The accompanying code is made available for
transparency and further development.
</p>
</p>
</div>

    </div>
    </body>
    