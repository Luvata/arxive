<!DOCTYPE html>
<html>
<head>
<title>2023-07-12-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.04804">S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weijie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Desai_J/0/1/0/all/0/1">Jay Desai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1">Srinivasan Sengamedu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaoyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1">Francis Iannacci</a></p>
<p>Language model based methods are powerful techniques for text classification.
However, the models have several shortcomings. (1) It is difficult to integrate
human knowledge such as keywords. (2) It needs a lot of resources to train the
models. (3) It relied on large text data to pretrain. In this paper, we propose
Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these
difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM
leverages the pattern of keywords to identify potential topics, as well as
optimize the quality of topics' keywords sets. Across a variety of datasets,
S2vNTM outperforms existing semi-supervised topic modeling methods in
classification accuracy with limited keywords provided. S2vNTM is at least
twice as fast as baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04821">Amplifying Limitations, Harms and Risks of Large Language Models. (arXiv:2307.04821v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+ONeill_M/0/1/0/all/0/1">Michael O&#x27;Neill</a>, <a href="http://arxiv.org/find/cs/1/au:+Connor_M/0/1/0/all/0/1">Mark Connor</a></p>
<p>We present this article as a small gesture in an attempt to counter what
appears to be exponentially growing hype around Artificial Intelligence (AI)
and its capabilities, and the distraction provided by the associated talk of
science-fiction scenarios that might arise if AI should become sentient and
super-intelligent. It may also help those outside of the field to become more
informed about some of the limitations of AI technology. In the current context
of popular discourse AI defaults to mean foundation and large language models
(LLMs) such as those used to create ChatGPT. This in itself is a
misrepresentation of the diversity, depth and volume of research, researchers,
and technology that truly represents the field of AI. AI being a field of
research that has existed in software artefacts since at least the 1950's. We
set out to highlight a number of limitations of LLMs, and in so doing highlight
that harms have already arisen and will continue to arise due to these
limitations. Along the way we also highlight some of the associated risks for
individuals and organisations in using this technology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04841">Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1">Blake Bordelon</a>, <a href="http://arxiv.org/find/stat/1/au:+Masset_P/0/1/0/all/0/1">Paul Masset</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuo_H/0/1/0/all/0/1">Henry Kuo</a>, <a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a></p>
<p>Reinforcement learning has been successful across several applications in
which agents have to learn to act in environments with sparse feedback.
However, despite this empirical success there is still a lack of theoretical
understanding of how the parameters of reinforcement learning models and the
features used to represent states interact to control the dynamics of learning.
In this work, we use concepts from statistical physics, to study the typical
case learning curves for temporal difference learning of a value function with
linear function approximators. Our theory is derived under a Gaussian
equivalence hypothesis where averages over the random trajectories are replaced
with temporally correlated Gaussian feature averages and we validate our
assumptions on small scale Markov Decision Processes. We find that the
stochastic semi-gradient noise due to subsampling the space of possible
episodes leads to significant plateaus in the value error, unlike in
traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and
reward function. We then analyze how strategies like learning rate annealing
and reward shaping can favorably alter learning dynamics and plateaus. To
conclude, our work introduces new tools to open a new direction towards
developing a theory of learning dynamics in reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04849">SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees. (arXiv:2307.04849v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sorokin_A/0/1/0/all/0/1">Aleksei Sorokin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xinran Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eric Hans Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1">Bolong Cheng</a></p>
<p>Gradient boosted trees (GBTs) are ubiquitous models used by researchers,
machine learning (ML) practitioners, and data scientists because of their
robust performance, interpretable behavior, and ease-of-use. One critical
challenge in training GBTs is the tuning of their hyperparameters. In practice,
selecting these hyperparameters is often done manually. Recently, the ML
community has advocated for tuning hyperparameters through black-box
optimization and developed state-of-the-art systems to do so. However, applying
such systems to tune GBTs suffers from two drawbacks. First, these systems are
not \textit{model-aware}, rather they are designed to apply to a
\textit{generic} model; this leaves significant optimization performance on the
table. Second, using these systems requires \textit{domain knowledge} such as
the choice of hyperparameter search space, which is an antithesis to the
automatic experimentation that black-box optimization aims to provide. In this
paper, we present SigOpt Mulch, a model-aware hyperparameter tuning system
specifically designed for automated tuning of GBTs that provides two
improvements over existing systems. First, Mulch leverages powerful techniques
in metalearning and multifidelity optimization to perform model-aware
hyperparameter optimization. Second, it automates the process of learning
performant hyperparameters by making intelligent decisions about the
optimization search space, thus reducing the need for user domain knowledge.
These innovations allow Mulch to identify good GBT hyperparameters far more
efficiently -- and in a more seamless and user-friendly way -- than existing
black-box hyperparameter tuning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04850">SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features. (arXiv:2307.04850v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kariyappa_S/0/1/0/all/0/1">Sanjay Kariyappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsepenekas_L/0/1/0/all/0/1">Leonidas Tsepenekas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecue_F/0/1/0/all/0/1">Freddy L&#xe9;cu&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1">Daniele Magazzeni</a></p>
<p>The SHAP framework provides a principled method to explain the predictions of
a model by computing feature importance. Motivated by applications in finance,
we introduce the Top-k Identification Problem (TkIP), where the objective is to
identify the k features with the highest SHAP values. While any method to
compute SHAP values with uncertainty estimates (such as KernelSHAP and
SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample
inefficient. The goal of our work is to improve the sample efficiency of
existing methods in the context of solving TkIP. Our key insight is that TkIP
can be framed as an Explore-m problem--a well-studied problem related to
multi-armed bandits (MAB). This connection enables us to improve sample
efficiency by leveraging two techniques from the MAB literature: (1) a better
stopping-condition (to stop sampling) that identifies when PAC (Probably
Approximately Correct) guarantees have been met and (2) a greedy sampling
scheme that judiciously allocates samples between different features. By
adopting these methods we develop KernelSHAP@k and SamplingSHAP@k to
efficiently solve TkIP, offering an average improvement of $5\times$ in
sample-efficiency and runtime across most common credit related datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04866">Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1">Albara Ah Ramli</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Berndt_K/0/1/0/all/0/1">Kelly Berndt</a>, <a href="http://arxiv.org/find/eess/1/au:+Chuah_C/0/1/0/all/0/1">Chen-Nee Chuah</a>, <a href="http://arxiv.org/find/eess/1/au:+Goude_E/0/1/0/all/0/1">Erica Goude</a>, <a href="http://arxiv.org/find/eess/1/au:+Kaethler_L/0/1/0/all/0/1">Lynea B. Kaethler</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_A/0/1/0/all/0/1">Amanda Lopez</a>, <a href="http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1">Alina Nicorici</a>, <a href="http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1">Corey Owens</a>, <a href="http://arxiv.org/find/eess/1/au:+Rodriguez_D/0/1/0/all/0/1">David Rodriguez</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jane Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1">Daniel Aranki</a>, <a href="http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1">Craig M. McDonald</a>, <a href="http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1">Erik K. Henricson</a></p>
<p>Background: Estimation of temporospatial clinical features of gait (CFs),
such as step count and length, step duration, step frequency, gait speed and
distance traveled is an important component of community-based mobility
evaluation using wearable accelerometers. However, challenges arising from
device complexity and availability, cost and analytical methodology have
limited widespread application of such tools. Research Question: Can
accelerometer data from commercially-available smartphones be used to extract
gait CFs across a broad range of attainable gait velocities in children with
Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using
machine learning (ML)-based methods Methods: Fifteen children with DMD and 15
TDs underwent supervised clinical testing across a range of gait speeds using
10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT)
and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer
at the waist near the body's center of mass. Gait CFs were extracted from the
accelerometer data using a multi-step machine learning-based process and
results were compared to ground-truth observation data. Results: Model
predictions vs. observed values for step counts, distance traveled, and step
length showed a strong correlation (Pearson's r = -0.9929 to 0.9986, p&lt;0.0001).
The estimates demonstrated a mean (SD) percentage error of 1.49% (7.04%) for
step counts, 1.18% (9.91%) for distance traveled, and 0.37% (7.52%) for step
length compared to ground truth observations for the combined 6MWT, 100MRW, and
FW tasks. Significance: The study findings indicate that a single accelerometer
placed near the body's center of mass can accurately measure CFs across
different gait speeds in both TD and DMD peers, suggesting that there is
potential for accurately measuring CFs in the community with consumer-level
smartphones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04869">Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagwe_G/0/1/0/all/0/1">Gaurav Bagwe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoyong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1">Miao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lan Zhang</a></p>
<p>Federated continual learning (FCL) learns incremental tasks over time from
confidential datasets distributed across clients. This paper focuses on
rehearsal-free FCL, which has severe forgetting issues when learning new tasks
due to the lack of access to historical task data. To address this issue, we
propose Fed-CPrompt based on prompt learning techniques to obtain task-specific
prompts in a communication-efficient way. Fed-CPrompt introduces two key
components, asynchronous prompt learning, and contrastive continual loss, to
handle asynchronous task arrival and heterogeneous data distributions in FCL,
respectively. Extensive experiments demonstrate the effectiveness of
Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04887">Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1">Vincent Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ruo Yu Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Javed_K/0/1/0/all/0/1">Khurram Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1">Adam White</a>, <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Martha White</a></p>
<p>Catastrophic interference is common in many network-based learning systems,
and many proposals exist for mitigating it. Before overcoming interference we
must understand it better. In this work, we provide a definition and novel
measure of interference for value-based reinforcement learning methods such as
Fitted Q-Iteration and DQN. We systematically evaluate our measure of
interference, showing that it correlates with instability in control
performance, across a variety of network architectures. Our new interference
measure allows us to ask novel scientific questions about commonly used deep
learning architectures and study learning algorithms which mitigate
interference. Lastly, we outline a class of algorithms which we call
online-aware that are designed to mitigate interference, and show they do
reduce interference according to our measure and that they improve stability
and performance in several classic control environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04893">Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moraes_R/0/1/0/all/0/1">Rubens O. Moraes</a>, <a href="http://arxiv.org/find/cs/1/au:+Aleixo_D/0/1/0/all/0/1">David S. Aleixo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferreira_L/0/1/0/all/0/1">Lucas N. Ferreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Lelis_L/0/1/0/all/0/1">Levi H. S. Lelis</a></p>
<p>This paper introduces Local Learner (2L), an algorithm for providing a set of
reference strategies to guide the search for programmatic strategies in
two-player zero-sum games. Previous learning algorithms, such as Iterated Best
Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be
computationally expensive or miss important information for guiding search
algorithms. 2L actively selects a set of reference strategies to improve the
search signal. We empirically demonstrate the advantages of our approach while
guiding a local search algorithm for synthesizing strategies in three games,
including MicroRTS, a challenging real-time strategy game. Results show that 2L
learns reference strategies that provide a stronger search signal than IBR, FP,
and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L
outperformed the winners of the two latest MicroRTS competitions, which were
programmatic strategies written by human programmers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04895">Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer. (arXiv:2307.04895v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1">Adam Ishay</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohyung Lee</a></p>
<p>Constraint satisfaction problems (CSPs) are about finding values of variables
that satisfy the given constraints. We show that Transformer extended with
recurrence is a viable approach to learning to solve CSPs in an end-to-end
manner, having clear advantages over state-of-the-art methods such as Graph
Neural Networks, SATNet, and some neuro-symbolic models. With the ability of
Transformer to handle visual input, the proposed Recurrent Transformer can
straightforwardly be applied to visual constraint reasoning problems while
successfully addressing the symbol grounding problem. We also show how to
leverage deductive knowledge of discrete constraints in the Transformer's
inductive learning to achieve sample-efficient learning and semi-supervised
learning for CSPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04957">Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Wei Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wei Yu</a></p>
<p>In reinforcement learning, the objective is almost always defined as a
\emph{cumulative} function over the rewards along the process. However, there
are many optimal control and reinforcement learning problems in various
application fields, especially in communications and networking, where the
objectives are not naturally expressed as summations of the rewards. In this
paper, we recognize the prevalence of non-cumulative objectives in various
problems, and propose a modification to existing algorithms for optimizing such
objectives. Specifically, we dive into the fundamental building block for many
optimal control and reinforcement learning algorithms: the Bellman optimality
equation. To optimize a non-cumulative objective, we replace the original
summation operation in the Bellman update rule with a generalized operation
corresponding to the objective. Furthermore, we provide sufficient conditions
on the form of the generalized operation as well as assumptions on the Markov
decision process under which the globally optimal convergence of the
generalized Bellman updates can be guaranteed. We demonstrate the idea
experimentally with the bottleneck objective, i.e., the objectives determined
by the minimum reward along the process, on classical optimal control and
reinforcement learning tasks, as well as on two network routing problems on
maximizing the flow rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04962">Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1">Shubhankar P. Patankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1">Mathieu Ouellet</a>, <a href="http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1">Juan Cervino</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1">Alejandro Ribeiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1">Kieran A. Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1">Dani S. Bassett</a></p>
<p>Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by the visited nodes in the environment. We use these
proposed features as rewards for graph neural-network-based reinforcement
learning. On multiple classes of synthetically generated graphs, we find that
trained agents generalize to larger environments and to longer exploratory
walks than are seen during training. Our method computes more efficiently than
the greedy evaluation of the relevant topological properties. The proposed
intrinsic motivations bear particular relevance for recommender systems. We
demonstrate that curiosity-based recommendations are more predictive of human
behavior than PageRank centrality for several real-world graph datasets,
including MovieLens, Amazon Books, and Wikispeedia.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04964">Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Rui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1">Shihan Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Songyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Binghai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Senjie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Limao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zhiheng Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1">Wenbin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minghao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1">Rongxiang Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wensen Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Cheng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yuan Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haoran Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04986">Epidemic Modeling with Generative Agents. (arXiv:2307.04986v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Williams_R/0/1/0/all/0/1">Ross Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseinichimeh_N/0/1/0/all/0/1">Niyousha Hosseinichimeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1">Aritra Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_N/0/1/0/all/0/1">Navid Ghaffarzadegan</a></p>
<p>This study offers a new paradigm of individual-level modeling to address the
grand challenge of incorporating human behavior in epidemic models. Using
generative artificial intelligence in an agent-based epidemic model, each agent
is empowered to make its own reasonings and decisions via connecting to a large
language model such as ChatGPT. Through various simulation experiments, we
present compelling evidence that generative agents mimic real-world behaviors
such as quarantining when sick and self-isolation when cases rise.
Collectively, the agents demonstrate patterns akin to multiple waves observed
in recent pandemics followed by an endemic period. Moreover, the agents
successfully flatten the epidemic curve. This study creates potential to
improve dynamic system modeling by offering a way to represent human brain,
reasoning, and decision making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04990">Monotone deep Boltzmann machines. (arXiv:2307.04990v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhili Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Winston_E/0/1/0/all/0/1">Ezra Winston</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a></p>
<p>Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods
ever studied, are multi-layered probabilistic models governed by a pairwise
energy function that describes the likelihood of all variables/nodes in the
network. In practice, DBMs are often constrained, i.e., via the
\emph{restricted} Boltzmann machine (RBM) architecture (which does not permit
intra-layer connections), in order to allow for more efficient inference. In
this work, we revisit the generic DBM approach, and ask the question: are there
other possible restrictions to their design that would enable efficient
(approximate) inference? In particular, we develop a new class of restricted
model, the monotone DBM, which allows for arbitrary self-connection in each
layer, but restricts the \emph{weights} in a manner that guarantees the
existence and global uniqueness of a mean-field fixed point. To do this, we
leverage tools from the recently-proposed monotone Deep Equilibrium model and
show that a particular choice of activation results in a fixed-point iteration
that gives a variational mean-field solution. While this approach is still
largely conceptual, it is the first architecture that allows for efficient
approximate inference in fully-general weight structures for DBMs. We apply
this approach to simple deep convolutional Boltzmann architectures and
demonstrate that it allows for tasks such as the joint completion and
classification of images, within a single deep probabilistic setting, while
avoiding the pitfalls of mean-field inference in traditional RBMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04996">Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1">Ghanshyam Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1">Shovon Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Simanta_S/0/1/0/all/0/1">Simon Simanta</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Perge_J/0/1/0/all/0/1">Janos A. Perge</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillai_D/0/1/0/all/0/1">Devishree Pillai</a>, <a href="http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1">John P. McCrae</a>, <a href="http://arxiv.org/find/cs/1/au:+Buitelaar_P/0/1/0/all/0/1">Paul Buitelaar</a></p>
<p>Personalized recommendations have a growing importance in direct marketing,
which motivates research to enhance customer experiences by knowledge graph
(KG) applications. For example, in financial services, companies may benefit
from providing relevant financial articles to their customers to cultivate
relationships, foster client engagement and promote informed financial
decisions. While several approaches center on KG-based recommender systems for
improved content, in this study we focus on interpretable KG-based recommender
systems for decision making.To this end, we present two knowledge graph-based
approaches for personalized article recommendations for a set of customers of a
large multinational financial services company. The first approach employs
Reinforcement Learning and the second approach uses the XGBoost algorithm for
recommending articles to the customers. Both approaches make use of a KG
generated from both structured (tabular data) and unstructured data (a large
body of text data).Using the Reinforcement Learning-based recommender system we
could leverage the graph traversal path leading to the recommendation as a way
to generate interpretations (Path Directed Reasoning (PDR)). In the
XGBoost-based approach, one can also provide explainable results using post-hoc
methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I
am Five).Importantly, our approach offers explainable results, promoting better
decision-making. This study underscores the potential of combining advanced
machine learning techniques with KG-driven insights to bolster experience in
customer relationship management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04998">Selective Sampling and Imitation Learning via Online Regression. (arXiv:2307.04998v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1">Ayush Sekhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1">Karthik Sridharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Runzhe Wu</a></p>
<p>We consider the problem of Imitation Learning (IL) by actively querying noisy
expert for feedback. While imitation learning has been empirically successful,
much of prior work assumes access to noiseless expert feedback which is not
practical in many applications. In fact, when one only has access to noisy
expert feedback, algorithms that rely on purely offline data (non-interactive
IL) can be shown to need a prohibitively large number of samples to be
successful. In contrast, in this work, we provide an interactive algorithm for
IL that uses selective sampling to actively query the noisy expert for
feedback. Our contributions are twofold: First, we provide a new selective
sampling algorithm that works with general function classes and multiple
actions, and obtains the best-known bounds for the regret and the number of
queries. Next, we extend this analysis to the problem of IL with noisy expert
feedback and provide a new IL algorithm that makes limited queries.
</p>
<p>Our algorithm for selective sampling leverages function approximation, and
relies on an online regression oracle w.r.t.~the given model class to predict
actions, and to decide whether to query the expert for its label. On the
theoretical side, the regret bound of our algorithm is upper bounded by the
regret of the online regression oracle, while the query complexity additionally
depends on the eluder dimension of the model class. We complement this with a
lower bound that demonstrates that our results are tight. We extend our
selective sampling algorithm for IL with general function approximation and
provide bounds on both the regret and the number of queries made to the noisy
expert. A key novelty here is that our regret and query complexity bounds only
depend on the number of times the optimal policy (and not the noisy expert, or
the learner) go to states that have a small margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05004">Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning. (arXiv:2307.05004v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakamura_T/0/1/0/all/0/1">Tomoaki Nakamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1">Akira Taniguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1">Tadahiro Taniguchi</a></p>
<p>This paper proposes a generative probabilistic model integrating emergent
communication and multi-agent reinforcement learning. The agents plan their
actions by probabilistic inference, called control as inference, and
communicate using messages that are latent variables and estimated based on the
planned actions. Through these messages, each agent can send information about
its actions and know information about the actions of another agent. Therefore,
the agents change their actions according to the estimated messages to achieve
cooperative tasks. This inference of messages can be considered as
communication, and this procedure can be formulated by the Metropolis-Hasting
naming game. Through experiments in the grid world environment, we show that
the proposed PGM can infer meaningful messages to achieve the cooperative task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05017">Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification. (arXiv:2307.05017v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yongsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weichuan Zhang</a></p>
<p>Decisions made by convolutional neural networks(CNN) can be understood and
explained by visualizing discriminative regions on images. To this end, Class
Activation Map (CAM) based methods were proposed as powerful interpretation
tools, making the prediction of deep learning models more explainable,
transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,
Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with
fully-connected (FC) layers as a classifier. It is worth noting that many deep
learning models classify images without FC layers, e.g., few-shot learning
image classification, contrastive learning image classification, and image
retrieval tasks. In this work, a post-hoc interpretation tool named feature
activation map (FAM) is proposed, which can interpret deep learning models
without FC layers as a classifier. In the proposed FAM algorithm, the
channel-wise contribution weights are derived from the similarity scores
between two image embeddings. The activation maps are linearly combined with
the corresponding normalized contribution weights, forming the explanation map
for visualization. The quantitative and qualitative experiments conducted on
ten deep learning models for few-shot image classification, contrastive
learning image classification and image retrieval tasks demonstrate the
effectiveness of the proposed FAM algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05025">Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hui Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huaxi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dadong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a></p>
<p>In recent years, research on learning with noisy labels has focused on
devising novel algorithms that can achieve robustness to noisy training labels
while generalizing to clean data. These algorithms often incorporate
sophisticated techniques, such as noise modeling, label correction, and
co-training. In this study, we demonstrate that a simple baseline using
cross-entropy loss, combined with widely used regularization strategies like
learning rate decay, model weights average, and data augmentations, can
outperform state-of-the-art methods. Our findings suggest that employing a
combination of regularization strategies can be more effective than intricate
algorithms in tackling the challenges of learning with noisy labels. While some
of these regularization strategies have been utilized in previous noisy label
learning research, their full potential has not been thoroughly explored. Our
results encourage a reevaluation of benchmarks for learning with noisy labels
and prompt reconsideration of the role of specialized learning algorithms
designed for training with noisy labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05036">Neural-Symbolic Recommendation with Graph-Enhanced Information. (arXiv:2307.05036v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Maonian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Bo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shaojun Zhu</a></p>
<p>The recommendation system is not only a problem of inductive statistics from
data but also a cognitive task that requires reasoning ability. The most
advanced graph neural networks have been widely used in recommendation systems
because they can capture implicit structured information from graph-structured
data. However, like most neural network algorithms, they only learn matching
patterns from a perception perspective. Some researchers use user behavior for
logic reasoning to achieve recommendation prediction from the perspective of
cognitive reasoning, but this kind of reasoning is a local one and ignores
implicit information on a global scale. In this work, we combine the advantages
of graph neural networks and propositional logic operations to construct a
neuro-symbolic recommendation model with both global implicit reasoning ability
and local explicit logic reasoning ability. We first build an item-item graph
based on the principle of adjacent interaction and use graph neural networks to
capture implicit information in global data. Then we transform user behavior
into propositional logic expressions to achieve recommendations from the
perspective of cognitive reasoning. Extensive experiments on five public
datasets show that our proposed model outperforms several state-of-the-art
methods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05043">Epistemic Syllogistic: First Steps. (arXiv:2307.05043v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yipu Li</a> (Peking University), <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanjing Wang</a> (Peking University)</p>
<p>Aristotle's discussions on modal syllogistic have often been viewed as
error-prone and have garnered significant attention in the literature due to
historical and philosophical interests. However, from a contemporary
standpoint, they also introduced natural fragments of first-order modal logic,
warranting a comprehensive technical analysis. In this paper, drawing
inspiration from the natural logic program, we propose and examine several
variants of modal syllogistic within the epistemic context, thereby coining the
term Epistemic Syllogistic. Specifically, we concentrate on the de re
interpretation of epistemic syllogisms containing non-trivial yet natural
expressions such as "all things known to be A are also known to be not B." We
explore the epistemic apodeictic syllogistic and its extensions, which
accommodate more complex terms. Our main contributions include several
axiomatizations of these logics, with completeness proofs that may be of
independent interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05052">Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Paiheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fuxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hyemi Song</a></p>
<p>We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it's more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05053">Strengthening Consistency Results in Modal Logic. (arXiv:2307.05053v1 [math.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Alexander_S/0/1/0/all/0/1">Samuel Allen Alexander</a> (US Securities and Exchange Commission), <a href="http://arxiv.org/find/math/1/au:+Pedersen_A/0/1/0/all/0/1">Arthur Paul Pedersen</a> (City University of New York)</p>
<p>A fundamental question asked in modal logic is whether a given theory is
consistent. But consistent with what? A typical way to address this question
identifies a choice of background knowledge axioms (say, S4, D, etc.) and then
shows the assumptions codified by the theory in question to be consistent with
those background axioms. But determining the specific choice and division of
background axioms is, at least sometimes, little more than tradition. This
paper introduces **generic theories** for propositional modal logic to address
consistency results in a more robust way. As building blocks for background
knowledge, generic theories provide a standard for categorical determinations
of consistency. We argue that the results and methods of this paper help to
elucidate problems in epistemology and enjoy sufficient scope and power to have
purchase on problems bearing on modalities in judgement, inference, and
decision making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05059">On Imperfect Recall in Multi-Agent Influence Diagrams. (arXiv:2307.05059v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1">James Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDermott_M/0/1/0/all/0/1">Matt MacDermott</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammond_L/0/1/0/all/0/1">Lewis Hammond</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrenstein_P/0/1/0/all/0/1">Paul Harrenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1">Alessandro Abate</a>, <a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1">Michael Wooldridge</a></p>
<p>Multi-agent influence diagrams (MAIDs) are a popular game-theoretic model
based on Bayesian networks. In some settings, MAIDs offer significant
advantages over extensive-form game representations. Previous work on MAIDs has
assumed that agents employ behavioural policies, which set independent
conditional probability distributions over actions for each of their decisions.
In settings with imperfect recall, however, a Nash equilibrium in behavioural
policies may not exist. We overcome this by showing how to solve MAIDs with
forgetful and absent-minded agents using mixed policies and two types of
correlated equilibrium. We also analyse the computational complexity of key
decision problems in MAIDs, and explore tractable cases. Finally, we describe
applications of MAIDs to Markov games and team situations, where imperfect
recall is often unavoidable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05062">System of Spheres-based Two Level Credibility-limited Revisions. (arXiv:2307.05062v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garapa_M/0/1/0/all/0/1">Marco Garapa</a> (University of Madeira), <a href="http://arxiv.org/find/cs/1/au:+Ferme_E/0/1/0/all/0/1">Eduardo Ferme</a> (University of Madeira), <a href="http://arxiv.org/find/cs/1/au:+Reis_M/0/1/0/all/0/1">Maur&#xed;cio D.L. Reis</a> (University of Madeira)</p>
<p>Two level credibility-limited revision is a non-prioritized revision
operation. When revising by a two level credibility-limited revision, two
levels of credibility and one level of incredibility are considered. When
revising by a sentence at the highest level of credibility, the operator
behaves as a standard revision, if the sentence is at the second level of
credibility, then the outcome of the revision process coincides with a standard
contraction by the negation of that sentence. If the sentence is not credible,
then the original belief set remains unchanged. In this paper, we propose a
construction for two level credibility-limited revision operators based on
Grove's systems of spheres and present an axiomatic characterization for these
operators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05066">Tableaux for the Logic of Strategically Knowing How. (arXiv:2307.05066v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjun Li</a></p>
<p>The logic of goal-directed knowing-how extends the standard epistemic logic
with an operator of knowing-how. The knowing-how operator is interpreted as
that there exists a strategy such that the agent knows that the strategy can
make sure that p. This paper presents a tableau procedure for the multi-agent
version of the logic of strategically knowing-how and shows the soundness and
completeness of this tableau procedure. This paper also shows that the
satisfiability problem of the logic can be decided in PSPACE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05067">Exploiting Asymmetry in Logic Puzzles: Using ZDDs for Symbolic Model Checking Dynamic Epistemic Logic. (arXiv:2307.05067v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miedema_D/0/1/0/all/0/1">Daniel Miedema</a> (Bernoulli Institute, University of Groningen), <a href="http://arxiv.org/find/cs/1/au:+Gattinger_M/0/1/0/all/0/1">Malvin Gattinger</a> (ILLC, University of Amsterdam)</p>
<p>Binary decision diagrams (BDDs) are widely used to mitigate the
state-explosion problem in model checking. A variation of BDDs are
Zero-suppressed Decision Diagrams (ZDDs) which omit variables that must be
false, instead of omitting variables that do not matter. We use ZDDs to
symbolically encode Kripke models used in Dynamic Epistemic Logic, a framework
to reason about knowledge and information dynamics in multi-agent systems. We
compare the memory usage of different ZDD variants for three well-known
examples from the literature: the Muddy Children, the Sum and Product puzzle
and the Dining Cryptographers. Our implementation is based on the existing
model checker SMCDEL and the CUDD library. Our results show that replacing BDDs
with the right variant of ZDDs can significantly reduce memory usage. This
suggests that ZDDs are a useful tool for model checking multi-agent systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05068">A Theory of Bounded Inductive Rationality. (arXiv:2307.05068v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1">Caspar Oesterheld</a> (Carnegie Mellon University), <a href="http://arxiv.org/find/cs/1/au:+Demski_A/0/1/0/all/0/1">Abram Demski</a> (Machine Intelligence Research Institute), <a href="http://arxiv.org/find/cs/1/au:+Conitzer_V/0/1/0/all/0/1">Vincent Conitzer</a> (Carnegie Mellon University)</p>
<p>The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05069">Cognitive Bias and Belief Revision. (arXiv:2307.05069v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papadamos_P/0/1/0/all/0/1">Panagiotis Papadamos</a> (Technical University of Denmark), <a href="http://arxiv.org/find/cs/1/au:+Gierasimczuk_N/0/1/0/all/0/1">Nina Gierasimczuk</a> (Technical University of Denmark)</p>
<p>In this paper we formalise three types of cognitive bias within the framework
of belief revision: confirmation bias, framing bias, and anchoring bias. We
interpret them generally, as restrictions on the process of iterated revision,
and we apply them to three well-known belief revision methods: conditioning,
lexicographic revision, and minimal revision. We investigate the reliability of
biased belief revision methods in truth tracking. We also run computer
simulations to assess the performance of biased belief revision in random
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05071">Mining for Unknown Unknowns. (arXiv:2307.05071v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinclair_Desgagne_B/0/1/0/all/0/1">Bernard Sinclair-Desgagn&#xe9;</a></p>
<p>Unknown unknowns are future relevant contingencies that lack an ex ante
description. While there are numerous retrospective accounts showing that
significant gains or losses might have been achieved or avoided had such
contingencies been previously uncovered, getting hold of unknown unknowns still
remains elusive, both in practice and conceptually. Using Formal Concept
Analysis (FCA) - a subfield of lattice theory which is increasingly applied for
mining and organizing data - this paper introduces a simple framework to
systematically think out of the box and direct the search for unknown unknowns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05072">Aggregating Credences into Beliefs: Agenda Conditions for Impossibility Results. (arXiv:2307.05072v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Minkyung Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chisu Kim</a></p>
<p>Binarizing belief aggregation addresses how to rationally aggregate
individual probabilistic beliefs into collective binary beliefs. Similar to the
development of judgment aggregation theory, formulating axiomatic requirements,
proving impossibility theorems, and identifying exact agenda conditions of
impossibility theorems are natural and important research topics in binarizing
belief aggregation. Building on our previous research on impossibility
theorems, we use an agenda-theoretic approach to generalize the results and to
determine the necessary and sufficient level of logical interconnection between
the issues in an agenda for the impossibility theorems to arise. We demonstrate
that (1) path-connectedness and even-negatability constitute the exact agenda
condition for the oligarchy result stating that binarizing belief aggregation
satisfying proposition-wise independence and deductive closure of collective
beliefs yields the oligarchies under minor conditions; (2)
negation-connectedness is the condition for the triviality result obtained by
adding anonymity to the oligarchy result; and (3) blockedness is the condition
for the impossibility result, which follows by adding completeness and
consistency of collective beliefs. Moreover, we compare these novel findings
with existing agenda-theoretic characterization theorems in judgment
aggregation and belief binarization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05074">Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain. (arXiv:2307.05074v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chunxi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhiliang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jintao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shasha Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zhihua Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kaixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Ting Wang</a></p>
<p>Text-to-SQL aims at generating SQL queries for the given natural language
questions and thus helping users to query databases. Prompt learning with large
language models (LLMs) has emerged as a recent approach, which designs prompts
to lead LLMs to understand the input question and generate the corresponding
SQL. However, it faces challenges with strict SQL syntax requirements. Existing
work prompts the LLMs with a list of demonstration examples (i.e. question-SQL
pairs) to generate SQL, but the fixed prompts can hardly handle the scenario
where the semantic gap between the retrieved demonstration and the input
question is large. In this paper, we propose a retrieval-augmented prompting
method for a LLM-based Text-to-SQL framework, involving sample-aware prompting
and a dynamic revision chain. Our approach incorporates sample-aware
demonstrations, which include the composition of SQL operators and fine-grained
information related to the given question. To retrieve questions sharing
similar intents with input questions, we propose two strategies for assisting
retrieval. Firstly, we leverage LLMs to simplify the original questions,
unifying the syntax and thereby clarifying the users' intentions. To generate
executable and accurate SQLs without human intervention, we design a dynamic
revision chain which iteratively adapts fine-grained feedback from the
previously generated SQL. Experimental results on three Text-to-SQL benchmarks
demonstrate the superiority of our method over strong baseline models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05075">Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images. (arXiv:2307.05075v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1">Danfeng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuanqiang Cai</a></p>
<p>Removing multiple degradations, such as haze, rain, and blur, from real-world
images poses a challenging and illposed problem. Recently, unified models that
can handle different degradations have been proposed and yield promising
results. However, these approaches focus on synthetic images and experience a
significant performance drop when applied to realworld images. In this paper,
we introduce Uni-Removal, a twostage semi-supervised framework for addressing
the removal of multiple degradations in real-world images using a unified model
and parameters. In the knowledge transfer stage, Uni-Removal leverages a
supervised multi-teacher and student architecture in the knowledge transfer
stage to facilitate learning from pretrained teacher networks specialized in
different degradation types. A multi-grained contrastive loss is introduced to
enhance learning from feature and image spaces. In the domain adaptation stage,
unsupervised fine-tuning is performed by incorporating an adversarial
discriminator on real-world images. The integration of an extended
multi-grained contrastive loss and generative adversarial loss enables the
adaptation of the student network from synthetic to real-world domains.
Extensive experiments on real-world degraded datasets demonstrate the
effectiveness of our proposed method. We compare our Uni-Removal framework with
state-of-the-art supervised and unsupervised methods, showcasing its promising
results in real-world image dehazing, deraining, and deblurring simultaneously.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05082">OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning. (arXiv:2307.05082v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1">Oleksandr Palagin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaverinskiy_V/0/1/0/all/0/1">Vladislav Kaverinskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Litvin_A/0/1/0/all/0/1">Anna Litvin</a>, <a href="http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1">Kyrylo Malakhov</a></p>
<p>This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05095">ATWM: Defense against adversarial malware based on adversarial training. (arXiv:2307.05095v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wei Guo</a></p>
<p>Deep learning technology has made great achievements in the field of image.
In order to defend against malware attacks, researchers have proposed many
Windows malware detection models based on deep learning. However, deep learning
models are vulnerable to adversarial example attacks. Malware can generate
adversarial malware with the same malicious function to attack the malware
detection model and evade detection of the model. Currently, many adversarial
defense studies have been proposed, but existing adversarial defense studies
are based on image sample and cannot be directly applied to malware sample.
Therefore, this paper proposes an adversarial malware defense method based on
adversarial training. This method uses preprocessing to defend simple
adversarial examples to reduce the difficulty of adversarial training.
Moreover, this method improves the adversarial defense capability of the model
through adversarial training. We experimented with three attack methods in two
sets of datasets, and the results show that the method in this paper can
improve the adversarial defense capability of the model without reducing the
accuracy of the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05104">A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI. (arXiv:2307.05104v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schlegel_U/0/1/0/all/0/1">Udo Schlegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Keim_D/0/1/0/all/0/1">Daniel A. Keim</a></p>
<p>Explainable Artificial Intelligence (XAI) has gained significant attention
recently as the demand for transparency and interpretability of machine
learning models has increased. In particular, XAI for time series data has
become increasingly important in finance, healthcare, and climate science.
However, evaluating the quality of explanations, such as attributions provided
by XAI techniques, remains challenging. This paper provides an in-depth
analysis of using perturbations to evaluate attributions extracted from time
series models. A perturbation analysis involves systematically modifying the
input data and evaluating the impact on the attributions generated by the XAI
method. We apply this approach to several state-of-the-art XAI techniques and
evaluate their performance on three time series classification datasets. Our
results demonstrate that the perturbation analysis approach can effectively
evaluate the quality of attributions and provide insights into the strengths
and limitations of XAI techniques. Such an approach can guide the selection of
XAI methods for time series data, e.g., focusing on return time rather than
precision, and facilitate the development of more reliable and interpretable
machine learning models for time series analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05134">TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1">Paul Grimal</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1">Herv&#xe9; Le Borgne</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1">Olivier Ferret</a>, <a href="http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1">Julien Tourille</a></p>
<p>The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05150">A Modal Logic for Explaining some Graph Neural Networks. (arXiv:2307.05150v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nunn_P/0/1/0/all/0/1">Pierre Nunn</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzentruber_F/0/1/0/all/0/1">Fran&#xe7;ois Schwarzentruber</a></p>
<p>In this paper, we propose a modal logic in which counting modalities appear
in linear inequalities. We show that each formula can be transformed into an
equivalent graph neural network (GNN). We also show that each GNN can be
transformed into a formula. We show that the satisfiability problem is
decidable. We also discuss some variants that are in PSPACE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05156">Stable Normative Explanations: From Argumentation to Deontic Logic. (arXiv:2307.05156v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Florio_C/0/1/0/all/0/1">Cecilia Di Florio</a>, <a href="http://arxiv.org/find/cs/1/au:+Governatori_G/0/1/0/all/0/1">Guido Governatori</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotolo_A/0/1/0/all/0/1">Antonino Rotolo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sartor_G/0/1/0/all/0/1">Giovanni Sartor</a></p>
<p>This paper examines how a notion of stable explanation developed elsewhere in
Defeasible Logic can be expressed in the context of formal argumentation. With
this done, we discuss the deontic meaning of this reconstruction and show how
to build from argumentation neighborhood structures for deontic logic where
this notion of explanation can be characterised. Some direct complexity results
are offered.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05161">On the Effectiveness of Speech Self-supervised Learning for Music. (arXiv:2307.05161v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yinghao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xingran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hanzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1">Emmanouil Benetos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1">Anton Ragni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1">Norbert Gyenge</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Gus Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1">Roger Dannenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>Self-supervised learning (SSL) has shown promising results in various speech
and natural language processing applications. However, its efficacy in music
information retrieval (MIR) still remains largely unexplored. While previous
SSL models pre-trained on music recordings may have been mostly closed-sourced,
recent speech models such as wav2vec2.0 have shown promise in music modelling.
Nevertheless, research exploring the effectiveness of applying speech SSL
models to music recordings has been limited. We explore the music adaption of
SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and
refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL
models with 95M parameters under various pre-training configurations and
systematically evaluate the MIR task performances with 13 different MIR tasks.
Our findings suggest that training with music data can generally improve
performance on MIR tasks, even when models are trained using paradigms designed
for speech. However, we identify the limitations of such existing
speech-oriented designs, especially in modelling polyphonic information. Based
on the experimental results, empirical suggestions are also given for designing
future musical SSL strategies and paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05162">SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suri_K/0/1/0/all/0/1">Kunal Suri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1">Prakhar Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Saumajit Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Atul Singh</a></p>
<p>Finetuning Large Language Models helps improve the results for
domain-specific use cases. End-to-end finetuning of large language models is
time and resource intensive and has high storage requirements to store the
finetuned version of the large language model. Parameter Efficient Fine Tuning
(PEFT) methods address the time and resource challenges by keeping the large
language model as a fixed base and add additional layers, which the PEFT
methods finetune. This paper demonstrates the evaluation results for one such
PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.
The evaluation results show that LoRA works at par with end-to-end finetuning
for a large language model. The paper presents the evaluations done for solving
both the Subtask A and B from ImageCLEFmedical
{https://www.imageclef.org/2023/medical}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05170">Neural Quantile Optimization for Edge-Cloud Computing. (arXiv:2307.05170v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Bin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiangle Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>We seek the best traffic allocation scheme for the edge-cloud computing
network that satisfies constraints and minimizes the cost based on burstable
billing. First, for a fixed network topology, we formulate a family of integer
programming problems with random parameters describing the various traffic
demands. Then, to overcome the difficulty caused by the discrete feature of the
problem, we generalize the Gumbel-softmax reparameterization method to induce
an unconstrained continuous optimization problem as a regularized continuation
of the discrete problem. Finally, we introduce the Gumbel-softmax sampling
network to solve the optimization problems via unsupervised learning. The
network structure reflects the edge-cloud computing topology and is trained to
minimize the expectation of the cost function for unconstrained continuous
optimization problems. The trained network works as an efficient traffic
allocation scheme sampler, remarkably outperforming the random strategy in
feasibility and cost function value. Besides testing the quality of the output
allocation scheme, we examine the generalization property of the network by
increasing the time steps and the number of users. We also feed the solution to
existing integer optimization solvers as initial conditions and verify the
warm-starts can accelerate the short-time iteration process. The framework is
general with solid performance, and the decoupled feature of the random neural
networks is adequate for practical implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05171">Enriching Verbal Feedback from Usability Testing: Automatic Linking of Thinking-Aloud Recordings and Stimulus using Eye Tracking and Mouse Data. (arXiv:2307.05171v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murali_S/0/1/0/all/0/1">Supriya Murali</a>, <a href="http://arxiv.org/find/cs/1/au:+Walber_T/0/1/0/all/0/1">Tina Walber</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1">Christoph Schaefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Sezen Lim</a></p>
<p>The think aloud method is an important and commonly used tool for usability
optimization. However, analyzing think aloud data could be time consuming. In
this paper, we put forth an automatic analysis of verbal protocols and test the
link between spoken feedback and the stimulus using eye tracking and mouse
tracking. The gained data - user feedback linked to a specific area of the
stimulus - could be used to let an expert review the feedback on specific web
page elements or to visualize on which parts of the web page the feedback was
given. Specifically, we test if participants fixate on or point with the mouse
to the content of the webpage that they are verbalizing. During the testing,
participants were shown three websites and asked to verbally give their
opinion. The verbal responses, along with the eye and cursor movements were
recorded. We compared the hit rate, defined as the percentage of verbally
mentioned areas of interest (AOIs) that were fixated with gaze or pointed to
with the mouse. The results revealed a significantly higher hit rate for the
gaze compared to the mouse data. Further investigation revealed that, while the
mouse was mostly used passively to scroll, the gaze was often directed towards
relevant AOIs, thus establishing a strong association between spoken words and
stimuli. Therefore, eye tracking data possibly provides more detailed
information and more valuable insights about the verbalizations compared to the
mouse data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05182">Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1">Long Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Mobarakol Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Hongliang Ren</a></p>
<p>Medical students and junior surgeons often rely on senior surgeons and
specialists to answer their questions when learning surgery. However, experts
are often busy with clinical and academic work, and have little time to give
guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question
Answering (VQA) systems can only provide simple answers without the location of
the answers. In addition, vision-language (ViL) embedding is still a less
explored research in these kinds of tasks. Therefore, a surgical Visual
Question Localized-Answering (VQLA) system would be helpful for medical
students and junior surgeons to learn and understand from recorded surgical
videos. We propose an end-to-end Transformer with Co-Attention gaTed
Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not
require feature extraction through detection models. The CAT-ViL embedding
module is designed to fuse heterogeneous features from visual and textual
sources. The fused embedding will feed a standard Data-Efficient Image
Transformer (DeiT) module, before the parallel classifier and detector for
joint prediction. We conduct the experimental validation on public surgical
videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results
highlight the superior performance and robustness of our proposed model
compared to the state-of-the-art approaches. Ablation studies further prove the
outstanding performance of all the proposed components. The proposed method
provides a promising solution for surgical scene understanding, and opens up a
primary step in the Artificial Intelligence (AI)-based VQLA system for surgical
training. Our code is publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05194">Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Jewson_J/0/1/0/all/0/1">Jack Jewson</a>, <a href="http://arxiv.org/find/stat/1/au:+Ghalebikesabi_S/0/1/0/all/0/1">Sahra Ghalebikesabi</a>, <a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1">Chris Holmes</a></p>
<p>Differential privacy guarantees allow the results of a statistical analysis
involving sensitive data to be released without compromising the privacy of any
individual taking part. Achieving such guarantees generally requires the
injection of noise, either directly into parameter estimates or into the
estimation process. Instead of artificially introducing perturbations, sampling
from Bayesian posterior distributions has been shown to be a special case of
the exponential mechanism, producing consistent, and efficient private
estimates without altering the data generative process. The application of
current approaches has, however, been limited by their strong bounding
assumptions which do not hold for basic models, such as simple linear
regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling
scheme from a generalised posterior targeting the minimisation of the
$\beta$-divergence between the model and the data generating process. This
provides private estimation that is generally applicable without requiring
changes to the underlying model and consistently learns the data generating
parameter. We show that $\beta$D-Bayes produces more precise inference
estimation for the same privacy guarantees, and further facilitates
differentially private estimation via posterior sampling for complex
classifiers and continuous regression models such as neural networks for the
first time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05209">Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azran_G/0/1/0/all/0/1">Guy Azran</a>, <a href="http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1">Mohamad H. Danesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Keren_S/0/1/0/all/0/1">Sarah Keren</a></p>
<p>Recent studies show that deep reinforcement learning (DRL) agents tend to
overfit to the task on which they were trained and fail to adapt to minor
environment changes. To expedite learning when transferring to unseen tasks, we
propose a novel approach to representing the current task using reward machines
(RM), state machine abstractions that induce subtasks based on the current
task's rewards and dynamics. Our method provides agents with symbolic
representations of optimal transitions from their current abstract state and
rewards them for achieving these transitions. These representations are shared
across tasks, allowing agents to exploit knowledge of previously encountered
symbols and transitions, thus enhancing transfer. Our empirical evaluation
shows that our representations improve sample efficiency and few-shot transfer
in a variety of domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05213">Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silvestri_M/0/1/0/all/0/1">Mattia Silvestri</a>, <a href="http://arxiv.org/find/cs/1/au:+Berden_S/0/1/0/all/0/1">Senne Berden</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1">Jayanta Mandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmutogullari_A/0/1/0/all/0/1">Ali &#x130;rfan Mahmuto&#x11f;ullar&#x131;</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulamba_M/0/1/0/all/0/1">Maxime Mulamba</a>, <a href="http://arxiv.org/find/cs/1/au:+Filippo_A/0/1/0/all/0/1">Allegra De Filippo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1">Tias Guns</a>, <a href="http://arxiv.org/find/cs/1/au:+Lombardi_M/0/1/0/all/0/1">Michele Lombardi</a></p>
<p>Many real-world optimization problems contain unknown parameters that must be
predicted prior to solving. To train the predictive machine learning (ML)
models involved, the commonly adopted approach focuses on maximizing predictive
accuracy. However, this approach does not always lead to the minimization of
the downstream task loss. Decision-focused learning (DFL) is a recently
proposed paradigm whose goal is to train the ML model by directly minimizing
the task loss. However, state-of-the-art DFL methods are limited by the
assumptions they make about the structure of the optimization problem (e.g.,
that the problem is linear) and by the fact that can only predict parameters
that appear in the objective function. In this work, we address these
limitations by instead predicting \textit{distributions} over parameters and
adopting score function gradient estimation (SFGE) to compute decision-focused
updates to the predictive model, thereby widening the applicability of DFL. Our
experiments show that by using SFGE we can: (1) deal with predictions that
occur both in the objective function and in the constraints; and (2)
effectively tackle two-stage stochastic optimization problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05256">Towards exploring adversarial learning for anomaly detection in complex driving scenes. (arXiv:2307.05256v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Habib_N/0/1/0/all/0/1">Nour Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Yunsu Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Buragohain_A/0/1/0/all/0/1">Abhishek Buragohain</a>, <a href="http://arxiv.org/find/cs/1/au:+Rausch_A/0/1/0/all/0/1">Andreas Rausch</a></p>
<p>One of the many Autonomous Systems (ASs), such as autonomous driving cars,
performs various safety-critical functions. Many of these autonomous systems
take advantage of Artificial Intelligence (AI) techniques to perceive their
environment. But these perceiving components could not be formally verified,
since, the accuracy of such AI-based components has a high dependency on the
quality of training data. So Machine learning (ML) based anomaly detection, a
technique to identify data that does not belong to the training data could be
used as a safety measuring indicator during the development and operational
time of such AI-based components. Adversarial learning, a sub-field of machine
learning has proven its ability to detect anomalies in images and videos with
impressive results on simple data sets. Therefore, in this work, we investigate
and provide insight into the performance of such techniques on a highly complex
driving scenes dataset called Berkeley DeepDrive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05258">Integrated Planning in Hospitals: A Review. (arXiv:2307.05258v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rachuba_S/0/1/0/all/0/1">Sebastian Rachuba</a>, <a href="http://arxiv.org/find/cs/1/au:+Reuter_Oppermann_M/0/1/0/all/0/1">Melanie Reuter-Oppermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Thielen_C/0/1/0/all/0/1">Clemens Thielen</a></p>
<p>Efficient planning of scarce resources in hospitals is a challenging task for
which a large variety of Operations Research and Management Science approaches
have been developed since the 1950s. While efficient planning of single
resources such as operating rooms, beds, or specific types of staff can already
lead to enormous efficiency gains, integrated planning of several resources has
been shown to hold even greater potential, and a large number of integrated
planning approaches have been presented in the literature over the past
decades.
</p>
<p>This paper provides the first literature review that focuses specifically on
the Operations Research and Management Science literature related to integrated
planning of different resources in hospitals. We collect the relevant
literature and analyze it regarding different aspects such as uncertainty
modeling and the use of real-life data. Several cross comparisons reveal
interesting insights concerning, e.g., relations between the modeling and
solution methods used and the practical implementation of the approaches
developed. Moreover, we provide a high-level taxonomy for classifying different
resource-focused integration approaches and point out gaps in the literature as
well as promising directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05260">U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Abhinav Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Akshat Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanikella_S/0/1/0/all/0/1">Sai Kiran Tanikella</a>, <a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1">Ashutosh Modi</a></p>
<p>The task of Prior Case Retrieval (PCR) in the legal domain is about
automatically citing relevant (based on facts and precedence) prior legal cases
in a given query case. To further promote research in PCR, in this paper, we
propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian
Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance
and the long size of legal documents, BM25 remains a strong baseline for
ranking the cited prior documents. In this work, we explore the role of events
in legal case retrieval and propose an unsupervised retrieval method-based
pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find
that the proposed unsupervised retrieval method significantly increases
performance compared to BM25 and makes retrieval faster by a considerable
margin, making it applicable to real-time case retrieval systems. Our proposed
system is generic, we show that it generalizes across two different legal
systems (Indian and Canadian), and it shows state-of-the-art performance on the
benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05284">On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiashuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1">Hongseok Namkoong</a></p>
<p>Different distribution shifts require different algorithmic and operational
interventions. Methodological research must be grounded by the specific shifts
they address. Although nascent benchmarks provide a promising empirical
foundation, they implicitly focus on covariate shifts, and the validity of
empirical findings depends on the type of shift, e.g., previous observations on
algorithmic performance can fail to be valid when the $Y|X$ distribution
changes. We conduct a thorough investigation of natural shifts in 5 tabular
datasets over 86,000 model configurations, and find that $Y|X$-shifts are most
prevalent. To encourage researchers to develop a refined language for
distribution shifts, we build WhyShift, an empirical testbed of curated
real-world shifts where we characterize the type of shift we benchmark
performance over. Since $Y|X$-shifts are prevalent in tabular settings, we
identify covariate regions that suffer the biggest $Y|X$-shifts and discuss
implications for algorithmic and data-based interventions. Our testbed
highlights the importance of future research that builds an understanding of
how distributions differ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05300">Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenhailong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenshan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>Human intelligence thrives on the concept of cognitive synergy, where
collaboration and information integration among different cognitive processes
yield superior outcomes compared to individual cognitive processes in
isolation. Although Large Language Models (LLMs) have demonstrated promising
performance as general task-solving agents, they still struggle with tasks that
require intensive domain knowledge and complex reasoning. In this work, we
propose Solo Performance Prompting (SPP), which transforms a single LLM into a
cognitive synergist by engaging in multi-turn self-collaboration with multiple
personas. A cognitive synergist refers to an intelligent agent that
collaborates with multiple minds, combining their individual strengths and
knowledge, to enhance problem-solving and overall performance in complex tasks.
By dynamically identifying and simulating different personas based on task
inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have
discovered that assigning multiple, fine-grained personas in LLMs elicits
better problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP
effectively elicits internal knowledge acquisition abilities, reduces
hallucination, and maintains strong reasoning capabilities. Code, data, and
prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05314">Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering. (arXiv:2307.05314v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinlong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zixu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shenjun Zhong</a></p>
<p>Medical visual question answering (VQA) is a challenging task that requires
answering clinical questions of a given medical image, by taking consider of
both visual and language information. However, due to the small scale of
training data for medical VQA, pre-training fine-tuning paradigms have been a
commonly used solution to improve model generalization performance. In this
paper, we present a novel self-supervised approach that learns unimodal and
multimodal feature representations of input images and text using medical image
caption datasets, by leveraging both unimodal and multimodal contrastive
losses, along with masked language modeling and image text matching as
pretraining objectives. The pre-trained model is then transferred to downstream
medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA)
performance on three publicly available medical VQA datasets with significant
accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we
conduct a comprehensive analysis to validate the effectiveness of different
components of the approach and study different pre-training settings. Our codes
and models are available at https://github.com/pengfeiliHEU/MUMC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05317">Automatic Generation of Semantic Parts for Face Image Synthesis. (arXiv:2307.05317v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fontanini_T/0/1/0/all/0/1">Tomaso Fontanini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1">Claudio Ferrari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1">Massimo Bertozzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1">Andrea Prati</a></p>
<p>Semantic image synthesis (SIS) refers to the problem of generating realistic
imagery given a semantic segmentation mask that defines the spatial layout of
object classes. Most of the approaches in the literature, other than the
quality of the generated images, put effort in finding solutions to increase
the generation diversity in terms of style i.e. texture. However, they all
neglect a different feature, which is the possibility of manipulating the
layout provided by the mask. Currently, the only way to do so is manually by
means of graphical users interfaces. In this paper, we describe a network
architecture to address the problem of automatically manipulating or generating
the shape of object classes in semantic segmentation masks, with specific focus
on human faces. Our proposed model allows embedding the mask class-wise into a
latent space where each class embedding can be independently edited. Then, a
bi-directional LSTM block and a convolutional decoder output a new, locally
manipulated mask. We report quantitative and qualitative results on the
CelebMask-HQ dataset, which show our model can both faithfully reconstruct and
modify a segmentation mask at the class level. Also, we show our model can be
put before a SIS generator, opening the way to a fully automatic generation
control of both shape and texture. Code available at
https://github.com/TFonta/Semantic-VAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05328">ProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal Production. (arXiv:2307.05328v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loth_J/0/1/0/all/0/1">Jackson Loth</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarmento_P/0/1/0/all/0/1">Pedro Sarmento</a>, <a href="http://arxiv.org/find/cs/1/au:+Carr_C/0/1/0/all/0/1">CJ Carr</a>, <a href="http://arxiv.org/find/cs/1/au:+Zukowski_Z/0/1/0/all/0/1">Zack Zukowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1">Mathieu Barthet</a></p>
<p>Recent work in the field of symbolic music generation has shown value in
using a tokenization based on the GuitarPro format, a symbolic representation
supporting guitar expressive attributes, as an input and output representation.
We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a
custom dataset of 173 progressive metal songs, for the purposes of creating
compositions from that genre through a human-AI partnership. Our model is able
to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We
examine the validity of the generated music using a mixed methods approach by
combining quantitative analyses following a computational musicology paradigm
and qualitative analyses following a practice-based research paradigm. Finally,
we demonstrate the value of the model by using it as a tool to create a
progressive metal song, fully produced and mixed by a human metal producer
based on AI-generated music.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05330">The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aditya Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Maharaj_S/0/1/0/all/0/1">Shiva Maharaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Polson_N/0/1/0/all/0/1">Nicholas Polson</a>, <a href="http://arxiv.org/find/cs/1/au:+Sokolov_V/0/1/0/all/0/1">Vadim Sokolov</a></p>
<p>Valuing chess squares and determining the placement of pieces on the board
are the main objectives of our study. With the emergence of chess AI, it has
become possible to accurately assess the worth of positions in a game of chess.
The conventional approach assigns fixed values to pieces $(\symking=\infty,
\symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance
this analysis by introducing marginal valuations for both pieces and squares.
We demonstrate our method by examining the positioning of Knights and Bishops,
and also provide valuable insights into the valuation of pawns. Notably,
Nimzowitsch was among the pioneers in advocating for the significance of Pawn
structure and valuation. Finally, we conclude by suggesting potential avenues
for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05333">Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sultana_S/0/1/0/all/0/1">Sharmin Sultana</a>, <a href="http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1">Md Mahmudur Rahman</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahi_A/0/1/0/all/0/1">Atqiya Munawara Mahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shao-Hsien Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1">Mohammad Arif Ul Alam</a></p>
<p>The combination of diverse health data (IoT, EHR, and clinical surveys) and
scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of
physical, behavioral, and psycho-social indicators of pain status. Despite the
hype and promise to fundamentally alter the healthcare system with
technological advancements, much AI adoption in clinical pain evaluation has
been hampered by the heterogeneity of the problem itself and other challenges,
such as personalization and fairness. Studies have revealed that many AI (i.e.,
machine learning or deep learning) models display biases and discriminate
against specific population segments (such as those based on gender or
ethnicity), which breeds skepticism among medical professionals about AI
adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL)
based CNN model that aims to account for any sensitive attributes included in
the data and fairly predict patients' pain status while attempting to minimize
the discrepancies between privileged and unprivileged groups. In order to
determine whether the trade-off between accuracy and fairness can be satisfied,
we compare the proposed model with well-known existing mitigation procedures,
and studies reveal that the implemented model performs favorably in contrast to
state-of-the-art methods. Utilizing NIH All-Of-US data, where a cohort of 868
distinct individuals with wearables and EHR data gathered over 1500 days has
been taken into consideration to analyze our suggested fair pain assessment
system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05357">Over-the-Air Computation in OFDM Systems with Imperfect Channel State Information. (arXiv:2307.05357v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yilong Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Xing_H/0/1/0/all/0/1">Huijun Xing</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1">Lexi Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a></p>
<p>This paper studies the over-the-air computation (AirComp) in an orthogonal
frequency division multiplexing (OFDM) system with imperfect channel state
information (CSI), in which multiple single-antenna wireless devices (WDs)
simultaneously send uncoded signals to a multi-antenna access point (AP) for
distributed functional computation over multiple subcarriers. In particular, we
consider two scenarios with best-effort and error-constrained computation
tasks, with the objectives of minimizing the average computation mean squared
error (MSE) and the computation outage probability over the multiple
subcarriers, respectively. Towards this end, we jointly optimize the transmit
coefficients at the WDs and the receive beamforming vectors at the AP over
subcarriers, subject to the maximum transmit power constraints at individual
WDs. First, for the special case with a single receive antenna at the AP, we
propose the semi-closed-form globally optimal solutions to the two problems
using the Lagrange-duality method. It is shown that at each subcarrier, the
WDs' optimized power control policy for average MSE minimization follows a
regularized channel inversion structure, while that for computation outage
probability minimization follows an on-off regularized channel inversion, with
the regularization dependent on the transmit power budget and channel
estimation error. Next, for the general case with multiple receive antennas at
the AP, we present efficient algorithms based on alternating optimization and
convex optimization to find converged solutions to both problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05358">Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Sikai Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuaicheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1">Weiming Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kunlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jun Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1">Shuai Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Song Guo</a></p>
<p>Federated learning has become a popular method to learn from decentralized
heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train
models from a small fraction of labeled data due to label scarcity on
decentralized clients. Existing FSSL methods assume independent and identically
distributed (IID) labeled data across clients and consistent class distribution
between labeled and unlabeled data within a client. This work studies a more
practical and challenging scenario of FSSL, where data distribution is
different not only across clients but also within a client between labeled and
unlabeled data. To address this challenge, we propose a novel FSSL framework
with dual regulators, FedDure.} FedDure lifts the previous assumption with a
coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg
regularizes the updating of the local model by tracking the learning effect on
labeled data distribution; F-reg learns an adaptive weighting scheme tailored
for unlabeled instances in each client. We further formulate the client model
training as bi-level optimization that adaptively optimizes the model in the
client with two regulators. Theoretically, we show the convergence guarantee of
the dual regulators. Empirically, we demonstrate that FedDure is superior to
the existing methods across a wide range of settings, notably by more than 11%
on CIFAR-10 and CINIC-10 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05360">Unmasking the giant: A comprehensive evaluation of ChatGPT&#x27;s proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arefin_S/0/1/0/all/0/1">Sayed Erfan Arefin</a>, <a href="http://arxiv.org/find/cs/1/au:+Heya_T/0/1/0/all/0/1">Tasnia Ashrafi Heya</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Qudah_H/0/1/0/all/0/1">Hasan Al-Qudah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ineza_Y/0/1/0/all/0/1">Ynes Ineza</a>, <a href="http://arxiv.org/find/cs/1/au:+Serwadda_A/0/1/0/all/0/1">Abdul Serwadda</a></p>
<p>The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05361">A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics. (arXiv:2307.05361v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1">Yue Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1">Shuhao Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1">Yihui Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiqiang Zhang</a></p>
<p>Muscle force and joint kinematics estimation from surface electromyography
(sEMG) are essential for real-time biomechanical analysis of the dynamic
interplay among neural muscle stimulation, muscle dynamics, and kinetics.
Recent advances in deep neural networks (DNNs) have shown the potential to
improve biomechanical analysis in a fully automated and reproducible manner.
However, the small sample nature and physical interpretability of biomechanical
analysis limit the applications of DNNs. This paper presents a novel
physics-informed low-shot learning method for sEMG-based estimation of muscle
force and joint kinematics. This method seamlessly integrates Lagrange's
equation of motion and inverse dynamic muscle model into the generative
adversarial network (GAN) framework for structured feature decoding and
extrapolated estimation from the small sample data. Specifically, Lagrange's
equation of motion is introduced into the generative model to restrain the
structured decoding of the high-level features following the laws of physics.
And a physics-informed policy gradient is designed to improve the adversarial
learning efficiency by rewarding the consistent physical representation of the
extrapolated estimations and the physical references. Experimental validations
are conducted on two scenarios (i.e. the walking trials and wrist motion
trials). Results indicate that the estimations of the muscle forces and joint
kinematics are unbiased compared to the physics-based inverse dynamics, which
outperforms the selected benchmark methods, including physics-informed
convolution neural network (PI-CNN), vallina generative adversarial network
(GAN), and multi-layer extreme learning machine (ML-ELM).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05373">Classification of sleep stages from EEG, EOG and EMG signals by SSNet. (arXiv:2307.05373v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Almutairi_H/0/1/0/all/0/1">Haifa Almutairi</a>, <a href="http://arxiv.org/find/eess/1/au:+Hassan_G/0/1/0/all/0/1">Ghulam Mubashar Hassan</a>, <a href="http://arxiv.org/find/eess/1/au:+Datta_A/0/1/0/all/0/1">Amitava Datta</a></p>
<p>Classification of sleep stages plays an essential role in diagnosing
sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In
this study, we propose an end-to-end deep learning architecture, named SSNet,
which comprises of two deep learning networks based on Convolutional Neuron
Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks
extract features from the combination of Electrooculogram (EOG),
Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal
has distinct features that help in the classification of sleep stages. The
features produced by the two-deep learning networks are concatenated to pass to
the fully connected layer for the classification. The performance of our
proposed model is evaluated by using two public datasets Sleep-EDF Expanded
dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36%
and 93.40% respectively, for classifying three classes of sleep stages using
Sleep-EDF Expanded dataset. Whereas, the accuracy and Kappa coefficient are
96.57% and 83.05% respectively for five classes of sleep stages using Sleep-EDF
Expanded dataset. Our model achieves the best performance in classifying sleep
stages when compared with the state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05375">Emotion Analysis on EEG Signal Using Machine Learning and Neural Network. (arXiv:2307.05375v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1">S. M. Masrur Ahmed</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Sabur_E/0/1/0/all/0/1">Eshaan Tanzim Sabur</a> (2) ((1) bKash Limited, (2) BRAC University)</p>
<p>Emotion has a significant influence on how one thinks and interacts with
others. It serves as a link between how a person feels and the actions one
takes, or it could be said that it influences one's life decisions on occasion.
Since the patterns of emotions and their reflections vary from person to
person, their inquiry must be based on approaches that are effective over a
wide range of population regions. To extract features and enhance accuracy,
emotion recognition using brain waves or EEG signals requires the
implementation of efficient signal processing techniques. Various approaches to
human-machine interaction technologies have been ongoing for a long time, and
in recent years, researchers have had great success in automatically
understanding emotion using brain signals. In our research, several emotional
states were classified and tested on EEG signals collected from a well-known
publicly available dataset, the DEAP Dataset, using SVM (Support Vector
Machine), KNN (K-Nearest Neighbor), and an advanced neural network model, RNN
(Recurrent Neural Network), trained with LSTM (Long Short Term Memory). The
main purpose of this study is to improve ways to improve emotion recognition
performance using brain signals. Emotions, on the other hand, can change with
time. As a result, the changes in emotion over time are also examined in our
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05382">Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling. (arXiv:2307.05382v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Ziyue Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1">Yuchen Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">You Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ren_K/0/1/0/all/0/1">Kan Ren</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yansen Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1">Xufang Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1">Juanyong Duan</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1">Congrui Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Qiu_L/0/1/0/all/0/1">Lili Qiu</a></p>
<p>A timely detection of seizures for newborn infants with electroencephalogram
(EEG) has been a common yet life-saving practice in the Neonatal Intensive Care
Unit (NICU). However, it requires great human efforts for real-time monitoring,
which calls for automated solutions to neonatal seizure detection. Moreover,
the current automated methods focusing on adult epilepsy monitoring often fail
due to (i) dynamic seizure onset location in human brains; (ii) different
montages on neonates and (iii) huge distribution shift among different
subjects. In this paper, we propose a deep learning framework, namely STATENet,
to address the exclusive challenges with exquisite designs at the temporal,
spatial and model levels. The experiments over the real-world large-scale
neonatal EEG dataset illustrate that our framework achieves significantly
better seizure detection performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05385">Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Sully F. Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1">Zhicheng Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1">Cheng Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a></p>
<p>Photoplethysmography (PPG) provides a low-cost, non-invasive method to
continuously monitor various cardiovascular parameters. PPG signals are
generated by wearable devices and frequently contain large artifacts caused by
external factors, such as motion of the human subject. In order to ensure
robust and accurate extraction of physiological parameters, corrupted areas of
the signal need to be identified and handled appropriately. Previous
methodology relied either on handcrafted feature detectors or signal metrics
which yield sub-optimal performance, or relied on machine learning techniques
such as deep neural networks (DNN) which lack interpretability and are
computationally and memory intensive. In this work, we present a novel method
to learn a small set of interpretable convolutional kernels that has
performance similar to -- and often better than -- the state-of-the-art DNN
approach with several orders of magnitude fewer parameters. This work allows
for efficient, robust, and interpretable signal quality assessment and artifact
segmentation on low-power devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05396">Handwritten Text Recognition Using Convolutional Neural Network. (arXiv:2307.05396v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Atman Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ram_A/0/1/0/all/0/1">A. Sharath Ram</a>, <a href="http://arxiv.org/find/cs/1/au:+C_K/0/1/0/all/0/1">Kavyashree C</a></p>
<p>OCR (Optical Character Recognition) is a technology that offers comprehensive
alphanumeric recognition of handwritten and printed characters at electronic
speed by merely scanning the document. Recently, the understanding of visual
data has been termed Intelligent Character Recognition (ICR). Intelligent
Character Recognition (ICR) is the OCR module that can convert scans of
handwritten or printed characters into ASCII text. ASCII data is the standard
format for data encoding in electronic communication. ASCII assigns standard
numeric values to letters, numeral, symbols, white-spaces and other characters.
In more technical terms, OCR is the process of using an electronic device to
transform 2-Dimensional textual information into machine-encoded text. Anything
that contains text both machine written or handwritten can be scanned either
through a scanner or just simply a picture of the text is enough for the
recognition system to distinguish the text. The goal of this papers is to show
the results of a Convolutional Neural Network model which has been trained on
National Institute of Science and Technology (NIST) dataset containing over a
100,000 images. The network learns from the features extracted from the images
and use it to generate the probability of each class to which the picture
belongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05399">Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform. (arXiv:2307.05399v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1">Mateusz W&#xf3;jcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosciukiewicz_W/0/1/0/all/0/1">Witold Ko&#x15b;ciukiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1">Mateusz Baran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1">Tomasz Kajdanowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonczarek_A/0/1/0/all/0/1">Adam Gonczarek</a></p>
<p>Production deployments in complex systems require ML architectures to be
highly efficient and usable against multiple tasks. Particularly demanding are
classification problems in which data arrives in a streaming fashion and each
class is presented separately. Recent methods with stochastic gradient learning
have been shown to struggle in such setups or have limitations like memory
buffers, and being restricted to specific domains that disable its usage in
real-world scenarios. For this reason, we present a fully differentiable
architecture based on the Mixture of Experts model, that enables the training
of high-performance classifiers when examples from each class are presented
separately. We conducted exhaustive experiments that proved its applicability
in various domains and ability to learn online in production environments. The
proposed technique achieves SOTA results without a memory buffer and clearly
outperforms the reference methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05409">3D detection of roof sections from a single satellite image and application to LOD2-building reconstruction. (arXiv:2307.05409v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lussange_J/0/1/0/all/0/1">Johann Lussange</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1">Mulin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarabalka_Y/0/1/0/all/0/1">Yuliya Tarabalka</a>, <a href="http://arxiv.org/find/cs/1/au:+Lafarge_F/0/1/0/all/0/1">Florent Lafarge</a></p>
<p>Reconstructing urban areas in 3D out of satellite raster images has been a
long-standing and challenging goal of both academical and industrial research.
The rare methods today achieving this objective at a Level Of Details $2$ rely
on procedural approaches based on geometry, and need stereo images and/or LIDAR
data as input. We here propose a method for urban 3D reconstruction named
KIBS(\textit{Keypoints Inference By Segmentation}), which comprises two novel
features: i) a full deep learning approach for the 3D detection of the roof
sections, and ii) only one single (non-orthogonal) satellite raster image as
model input. This is achieved in two steps: i) by a Mask R-CNN model performing
a 2D segmentation of the buildings' roof sections, and after blending these
latter segmented pixels within the RGB satellite raster image, ii) by another
identical Mask R-CNN model inferring the heights-to-ground of the roof
sections' corners via panoptic segmentation, unto full 3D reconstruction of the
buildings and city. We demonstrate the potential of the KIBS method by
reconstructing different urban areas in a few minutes, with a Jaccard index for
the 2D segmentation of individual roof sections of $88.55\%$ and $75.21\%$ on
our two data sets resp., and a height's mean error of such correctly segmented
pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets
resp., hence within the LOD2 precision range.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05426">Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort. (arXiv:2307.05426v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Addeh_A/0/1/0/all/0/1">Abdoljalil Addeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Vega_F/0/1/0/all/0/1">Fernando Vega</a>, <a href="http://arxiv.org/find/eess/1/au:+Williams_R/0/1/0/all/0/1">Rebecca J Williams</a>, <a href="http://arxiv.org/find/eess/1/au:+Golestani_A/0/1/0/all/0/1">Ali Golestani</a>, <a href="http://arxiv.org/find/eess/1/au:+Pike_G/0/1/0/all/0/1">G. Bruce Pike</a>, <a href="http://arxiv.org/find/eess/1/au:+MacDonald_M/0/1/0/all/0/1">M. Ethan MacDonald</a></p>
<p>In many fMRI studies, respiratory signals are unavailable or do not have
acceptable quality. Consequently, the direct removal of low-frequency
respiratory variations from BOLD signals is not possible. This study proposes a
one-dimensional CNN model for reconstruction of two respiratory measures, RV
and RVT. Results show that a CNN can capture informative features from resting
BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected
that application of the proposed method will lower the cost of fMRI studies,
reduce complexity, and decrease the burden on participants as they will not be
required to wear a respiratory bellows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05440">ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Abhinav Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Susmit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1">Ashutosh Modi</a></p>
<p>Sign languages are the primary means of communication for many
hard-of-hearing people worldwide. Recently, to bridge the communication gap
between the hard-of-hearing community and the rest of the population, several
sign language translation datasets have been proposed to enable the development
of statistical sign language translation systems. However, there is a dearth of
sign language resources for the Indian sign language. This resource paper
introduces ISLTranslate, a translation dataset for continuous Indian Sign
Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best
of our knowledge, it is the largest translation dataset for continuous Indian
Sign Language. We provide a detailed analysis of the dataset. To validate the
performance of existing end-to-end Sign language to spoken language translation
systems, we benchmark the created dataset with a transformer-based model for
ISL translation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05447">Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and Denoising. (arXiv:2307.05447v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xinyi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Priyanka_S/0/1/0/all/0/1">Steffi Agino Priyanka</a>, <a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1">Hsiao-Jung Tung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuankai Wang</a></p>
<p>Due to the low accuracy of object detection and recognition in many
intelligent surveillance systems at nighttime, the quality of night images is
crucial. Compared with the corresponding daytime image, nighttime image is
characterized as low brightness, low contrast and high noise. In this paper, a
bio-inspired image enhancement algorithm is proposed to convert a low
illuminance image to a brighter and clear one. Different from existing
bio-inspired algorithm, the proposed method doesn't use any training sequences,
we depend on a novel chain of contrast enhancement and denoising algorithms
without using any forms of recursive functions. Our method can largely improve
the brightness and contrast of night images, besides, suppress noise. Then we
implement on real experiment, and simulation experiment to test our algorithms.
Both results show the advantages of proposed algorithm over contrast pair,
Meylan and Retinex.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05476">Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation. (arXiv:2307.05476v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1">Jung Hyun Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1">Jaeheyoung Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jewoong Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+1_M/0/1/0/all/0/1">Myungjoo Kang 1</a></p>
<p>Along with the exponential growth of online platforms and services,
recommendation systems have become essential for identifying relevant items
based on user preferences. The domain of sequential recommendation aims to
capture evolving user preferences over time. To address dynamic preference,
various contrastive learning methods have been proposed to target data
sparsity, a challenge in recommendation systems due to the limited user-item
interactions. In this paper, we are the first to apply the Fisher-Merging
method to Sequential Recommendation, addressing and resolving practical
challenges associated with it. This approach ensures robust fine-tuning by
merging the parameters of multiple models, resulting in improved overall
performance. Through extensive experiments, we demonstrate the effectiveness of
our proposed methods, highlighting their potential to advance the
state-of-the-art in sequential learning and recommendation systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.12689">The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?. (arXiv:2012.12689v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Fioretti_G/0/1/0/all/0/1">Guido Fioretti</a>, <a href="http://arxiv.org/find/eess/1/au:+Policarpi_A/0/1/0/all/0/1">Andrea Policarpi</a></p>
<p>We explore a Leviathan analogy between neurons in a brain and human beings in
society, asking ourselves whether individual intelligence is necessary for
collective intelligence to emerge and, most importantly, what sort of
individual intelligence is conducive of greater collective intelligence. We
first review disparate insights from connectionist cognitive science,
agent-based modeling, group psychology, economics and physics. Subsequently, we
apply these insights to the sort and degrees of intelligence that in the
Lotka-Volterra model lead to either co-existence or global extinction of
predators and preys.
</p>
<p>We find several individual behaviors -- particularly of predators -- that are
conducive to co-existence, eventually with oscillations around an equilibrium.
However, we also find that if both preys and predators are sufficiently
intelligent to extrapolate one other's behavior, co-existence comes along with
indefinite growth of both populations. Since the Lotka-Volterra model is also
interpreted to represent the business cycle, we understand this finding as a
condition for economic growth around oscillations. Specifically, we hypothesize
that pre-modern societies may not have exhibited limitless growth also because
capitalistic future-oriented thinking based on saving and investing concerned
at most a fraction of the population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08933">Responsive parallelized architecture for deploying deep learning models in production environments. (arXiv:2112.08933v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1">Nikhil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_K/0/1/0/all/0/1">Krishna Prasad</a></p>
<p>Recruiters can easily shortlist candidates for jobs via viewing their
curriculum vitae (CV) document. Unstructured document CV beholds candidate's
portfolio and named entities listing details. The main aim of this study is to
design and propose a web oriented, highly responsive, computational pipeline
that systematically predicts CV entities using hierarchically-refined label
attention networks. Deep learning models specialized for named entity
recognition were trained on large dataset to predict relevant fields. The
article suggests an optimal strategy to use a number of deep learning models in
parallel and predict in real time. We demonstrate selection of light weight
micro web framework using Analytical Hierarchy Processing algorithm and focus
on an approach useful to deploy large deep learning model-based pipelines in
production ready environments using microservices. Deployed models and
architecture proposed helped in parsing normal CV in less than 700 milliseconds
for sequential flow of requests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.14586">Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Orseau_L/0/1/0/all/0/1">Laurent Orseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1">Marcus Hutter</a></p>
<p>We extend and combine several tools of the literature to design fast,
adaptive, anytime and scale-free online learning algorithms. Scale-free regret
bounds must scale linearly with the maximum loss, both toward large losses and
toward very small losses. Adaptive regret bounds demonstrate that an algorithm
can take advantage of easy data and potentially have constant regret. We seek
to develop fast algorithms that depend on as few parameters as possible, in
particular they should be anytime and thus not depend on the time horizon. Our
first and main tool, isotuning, is a generalization of the idea of balancing
the trade-off of the regret. We develop a set of tools to design and analyze
such learning rates easily and show that they adapts automatically to the rate
of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a
factor 2 of the optimal learning rate in hindsight for the same observed
quantities. The second tool is an online correction, which allows us to obtain
centered bounds for many algorithms, to prevent the regret bounds from being
vacuous when the domain is overly large or only partially constrained. The last
tool, null updates, prevents the algorithm from performing overly large
updates, which could result in unbounded regret, or even invalid updates. We
develop a general theory using these tools and apply it to several standard
algorithms. In particular, we (almost entirely) restore the adaptivity to small
losses of FTRL for unbounded domains, design and prove scale-free adaptive
guarantees for a variant of Mirror Descent (at least when the Bregman
divergence is convex in its second argument), extend Adapt-ML-Prod to
scale-free guarantees, and provide several other minor contributions about
Prod, AdaHedge, BOA and Soft-Bayes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.09523">BTPK-based interpretable method for NER tasks based on Talmudic Public Announcement Logic. (arXiv:2201.09523v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yulin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1">Beishui Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bentzen_B/0/1/0/all/0/1">Bruno Bentzen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Bo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zelai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Haixiao Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabbay_D/0/1/0/all/0/1">Dov Gabbay</a></p>
<p>As one of the basic tasks in natural language processing (NLP), named entity
recognition (NER) is an important basic tool for downstream tasks of NLP, such
as information extraction, syntactic analysis, machine translation and so on.
The internal operation logic of current name entity recognition model is
black-box to the user, so the user has no basis to determine which name entity
makes more sense. Therefore, a user-friendly explainable recognition process
would be very useful for many people. In this paper, we propose a novel
interpretable method, BTPK (Binary Talmudic Public Announcement Logic model),
to help users understand the internal recognition logic of the name entity
recognition tasks based on Talmudic Public Announcement Logic. BTPK model can
also capture the semantic information in the input sentences, that is, the
context dependency of the sentence. We observed the public announcement of BTPK
presents the inner decision logic of BRNNs, and the explanations obtained from
a BTPK model show us how BRNNs essentially handle NER tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06960">Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1">Nilay Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1">Jeffrey Flanigan</a></p>
<p>Human language is known to exhibit a nested, hierarchical structure, allowing
us to form complex sentences out of smaller pieces. However, many
state-of-the-art neural networks models such as Transformers have no explicit
hierarchical structure in its architecture -- that is, they don't have an
inductive bias toward hierarchical structure. Additionally, Transformers are
known to perform poorly on compositional generalization tasks which require
such structures. In this paper, we introduce Treeformer, a general-purpose
encoder module inspired by the CKY algorithm which learns a composition
operator and pooling function to construct hierarchical encodings for phrases
and sentences. Our extensive experiments demonstrate the benefits of
incorporating hierarchical structure into the Transformer and show significant
improvements in compositional generalization as well as in downstream tasks
such as machine translation, abstractive summarization, and various natural
language understanding tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11030">Adversarial Cheap Talk. (arXiv:2211.11030v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1">Timon Willi</a>, <a href="http://arxiv.org/find/cs/1/au:+Letcher_A/0/1/0/all/0/1">Alistair Letcher</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a></p>
<p>Adversarial attacks in reinforcement learning (RL) often assume
highly-privileged access to the victim's parameters, environment, or data.
Instead, this paper proposes a novel adversarial setting called a Cheap Talk
MDP in which an Adversary can merely append deterministic messages to the
Victim's observation, resulting in a minimal range of influence. The Adversary
cannot occlude ground truth, influence underlying environment dynamics or
reward signals, introduce non-stationarity, add stochasticity, see the Victim's
actions, or access their parameters. Additionally, we present a simple
meta-learning algorithm called Adversarial Cheap Talk (ACT) to train
Adversaries in this setting. We demonstrate that an Adversary trained with ACT
still significantly influences the Victim's training and testing performance,
despite the highly constrained setting. Affecting train-time performance
reveals a new attack vector and provides insight into the success and failure
modes of existing RL algorithms. More specifically, we show that an ACT
Adversary is capable of harming performance by interfering with the learner's
function approximation, or instead helping the Victim's performance by
outputting useful features. Finally, we show that an ACT Adversary can
manipulate messages during train-time to directly and arbitrarily control the
Victim at test-time. Project video and code are available at
https://sites.google.com/view/adversarial-cheap-talk
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01977">Distributed Pruning Towards Tiny Neural Networks in Federated Learning. (arXiv:2212.01977v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chaoyue Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Ruogu Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoyong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dapeng Wu</a></p>
<p>Neural network pruning is an essential technique for reducing the size and
complexity of deep neural networks, enabling large-scale models on devices with
limited resources. However, existing pruning approaches heavily rely on
training data for guiding the pruning strategies, making them ineffective for
federated learning over distributed and confidential datasets. Additionally,
the memory- and computation-intensive pruning process becomes infeasible for
recourse-constrained devices in federated learning. To address these
challenges, we propose FedTiny, a distributed pruning framework for federated
learning that generates specialized tiny models for memory- and
computing-constrained devices. We introduce two key modules in FedTiny to
adaptively search coarse- and finer-pruned specialized models to fit deployment
scenarios with sparse and cheap local computation. First, an adaptive batch
normalization selection module is designed to mitigate biases in pruning caused
by the heterogeneity of local data. Second, a lightweight progressive pruning
module aims to finer prune the models under strict memory and computational
budgets, allowing the pruning policy for each layer to be gradually determined
rather than evaluating the overall model structure. The experimental results
demonstrate the effectiveness of FedTiny, which outperforms state-of-the-art
approaches, particularly when compressing deep models to extremely sparse tiny
models. FedTiny achieves an accuracy improvement of 2.61% while significantly
reducing the computational cost by 95.91% and the memory footprint by 94.01%
compared to state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05995">Collective Privacy Recovery: Data-sharing Coordination via Decentralized Artificial Intelligence. (arXiv:2301.05995v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1">Evangelos Pournaras</a>, <a href="http://arxiv.org/find/cs/1/au:+Ballandies_M/0/1/0/all/0/1">Mark Christopher Ballandies</a>, <a href="http://arxiv.org/find/cs/1/au:+Bennati_S/0/1/0/all/0/1">Stefano Bennati</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chien-fei Chen</a></p>
<p>Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving &gt;27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10343">ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1">Johannes Brandstetter</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1">Ashish Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jayesh K. Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12313">Adapting Neural Link Predictors for Data-Efficient Complex Query Answering. (arXiv:2301.12313v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arakelyan_E/0/1/0/all/0/1">Erik Arakelyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Daza_D/0/1/0/all/0/1">Daniel Daza</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1">Michael Cochez</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a></p>
<p>Answering complex queries on incomplete knowledge graphs is a challenging
task where a model needs to answer complex logical queries in the presence of
missing knowledge. Prior work in the literature has proposed to address this
problem by designing architectures trained end-to-end for the complex query
answering task with a reasoning process that is hard to interpret while
requiring data and resource-intensive training. Other lines of research have
proposed re-using simple neural link predictors to answer complex queries,
reducing the amount of training data by orders of magnitude while providing
interpretable answers. The neural link predictor used in such approaches is not
explicitly optimised for the complex query answering task, implying that its
scores are not calibrated to interact together. We propose to address these
problems via CQD$^{\mathcal{A}}$, a parameter-efficient score \emph{adaptation}
model optimised to re-calibrate neural link prediction scores for the complex
query answering task. While the neural link predictor is frozen, the adaptation
component -- which only increases the number of model parameters by $0.03\%$ --
is trained on the downstream complex query answering task. Furthermore, the
calibration component enables us to support reasoning over queries that include
atomic negations, which was previously impossible with link predictors. In our
experiments, CQD$^{\mathcal{A}}$ produces significantly more accurate results
than current state-of-the-art methods, improving from $34.4$ to $35.1$ Mean
Reciprocal Rank values averaged across all datasets and query types while using
$\leq 30\%$ of the available training query types. We further show that
CQD$^{\mathcal{A}}$ is data-efficient, achieving competitive results with only
$1\%$ of the training complex queries, and robust in out-of-domain evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12636">Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays. (arXiv:2301.12636v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sluijs_R/0/1/0/all/0/1">Rogier van der Sluijs</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhaskhar_N/0/1/0/all/0/1">Nandita Bhaskhar</a>, <a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1">Daniel Rubin</a>, <a href="http://arxiv.org/find/eess/1/au:+Langlotz_C/0/1/0/all/0/1">Curtis Langlotz</a>, <a href="http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1">Akshay Chaudhari</a></p>
<p>Image augmentations are quintessential for effective visual representation
learning across self-supervised learning techniques. While augmentation
strategies for natural imaging have been studied extensively, medical images
are vastly different from their natural counterparts. Thus, it is unknown
whether common augmentation strategies employed in Siamese representation
learning generalize to medical images and to what extent. To address this
challenge, in this study, we systematically assess the effect of various
augmentations on the quality and robustness of the learned representations. We
train and evaluate Siamese Networks for abnormality detection on chest X-Rays
across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate
the efficacy of the learned representations through experiments involving
linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally,
we identify a set of augmentations that yield robust representations that
generalize well to both out-of-distribution data and diseases, while
outperforming supervised baselines using just zero-shot transfer and linear
probes by up to 20%. Our code is available at
https://github.com/StanfordMIMI/siaug.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00390">Hierarchical Classification of Research Fields in the &quot;Web of Science&quot; Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1">Susie Xi Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Egger_P/0/1/0/all/0/1">Peter H. Egger</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a></p>
<p>This paper presents a hierarchical classification system that automatically
categorizes a scholarly publication using its abstract into a three-tier
hierarchical label set (discipline, field, subfield) in a multi-class setting.
This system enables a holistic categorization of research activities in the
mentioned hierarchy in terms of knowledge production through articles and
impact through citations, permitting those activities to fall into multiple
categories. The classification system distinguishes 44 disciplines, 718 fields
and 1,485 subfields among 160 million abstract snippets in Microsoft Academic
Graph (version 2018-05-17). We used batch training in a modularized and
distributed fashion to address and allow for interdisciplinary and interfield
classifications in single-label and multi-label settings. In total, we have
conducted 3,140 experiments in all considered models (Convolutional Neural
Networks, Recurrent Neural Networks, Transformers). The classification accuracy
is &gt; 90% in 77.13% and 78.19% of the single-label and multi-label
classifications, respectively. We examine the advantages of our classification
by its ability to better align research texts and output with disciplines, to
adequately classify them in an automated way, and to capture the degree of
interdisciplinarity. The proposed system (a set of pre-trained models) can
serve as a backbone to an interactive system for indexing scientific
publications in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14468">SAINE: Scientific Annotation and Inference Engine of Scientific Research. (arXiv:2302.14468v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1">Susie Xi Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1">Yilei Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Egger_P/0/1/0/all/0/1">Peter H. Egger</a></p>
<p>We present SAINE, an Scientific Annotation and Inference ENgine based on a
set of standard open-source software, such as Label Studio and MLflow. We show
that our annotation engine can benefit the further development of a more
accurate classification. Based on our previous work on hierarchical discipline
classifications, we demonstrate its application using SAINE in understanding
the space for scholarly publications. The user study of our annotation results
shows that user input collected with the help of our system can help us better
understand the classification process. We believe that our work will help to
foster greater transparency and better understand scientific research. Our
annotation and inference engine can further support the downstream meta-science
projects. We welcome collaboration and feedback from the scientific community
on these projects. The demonstration video can be accessed from
https://youtu.be/yToO-G9YQK4. A live demo website is available at
https://app.heartex.com/user/signup/?token=e2435a2f97449fa1 upon free
registration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05066">Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Sifan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jionglong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a></p>
<p>Self-supervised learning is well known for its remarkable performance in
representation learning and various downstream computer vision tasks. Recently,
Positive-pair-Only Contrastive Learning (POCL) has achieved reliable
performance without the need to construct positive-negative training sets. It
reduces memory requirements by lessening the dependency on the batch size. The
POCL method typically uses a single loss function to extract the distortion
invariant representation (DIR) which describes the proximity of positive-pair
representations affected by different distortions. This loss function
implicitly enables the model to filter out or ignore the distortion variant
representation (DVR) affected by different distortions. However, existing POCL
methods do not explicitly enforce the disentanglement and exploitation of the
actually valuable DVR. In addition, these POCL methods have been observed to be
sensitive to augmentation strategies. To address these limitations, we propose
a novel POCL framework named Distortion-Disentangled Contrastive Learning
(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to
explicitly disentangle and exploit the DVR inside the model and feature stream
to improve the overall representation utilization efficiency, robustness and
representation ability. Experiments carried out demonstrate the superiority of
our framework to Barlow Twins and Simsiam in terms of convergence,
representation quality, and robustness on several benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07185">Joint Behavior and Common Belief. (arXiv:2303.07185v2 [cs.MA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Friedenberg_M/0/1/0/all/0/1">Meir Friedenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Halpern_J/0/1/0/all/0/1">Joseph Y. Halpern</a></p>
<p>For over 25 years, common belief has been widely viewed as necessary for
joint behavior. But this is not quite correct. We show by example that what can
naturally be thought of as joint behavior can occur without common belief. We
then present two variants of common belief that can lead to joint behavior,
even without standard common belief ever being achieved, and show that one of
them, action-stamped common belief, is in a sense necessary and sufficient for
joint behavior. These observations are significant because, as is well known,
common belief is quite difficult to achieve in practice, whereas these variants
are more easily achievable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02168">I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1">Tejas Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1">Furong Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06470">Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1">Ali Borji</a></p>
<p>The ability of image and video generation models to create photorealistic
images has reached unprecedented heights, making it difficult to distinguish
between real and fake images in many cases. However, despite this progress, a
gap remains between the quality of generated images and those found in the real
world. To address this, we have reviewed a vast body of literature from both
academic publications and social media to identify qualitative shortcomings in
image generation models, which we have classified into five categories. By
understanding these failures, we can identify areas where these models need
improvement, as well as develop strategies for detecting deep fakes. The
prevalence of deep fakes in today's society is a serious concern, and our
findings can help mitigate their negative impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03017">Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1">Sajjad Rahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1">AmirHossein Naghshzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerrouj_L/0/1/0/all/0/1">Latifa Guerrouj</a></p>
<p>Our research investigates the recommendation of code examples to aid software
developers, a practice that saves developers significant time by providing
ready-to-use code snippets. The focus of our study is Stack Overflow, a
commonly used resource for coding discussions and solutions, particularly in
the context of the Java programming language.
</p>
<p>We applied BERT, a powerful Large Language Model (LLM) that enables us to
transform code examples into numerical vectors by extracting their semantic
information. Once these numerical representations are prepared, we identify
Approximate Nearest Neighbors (ANN) using Locality-Sensitive Hashing (LSH). Our
research employed two variants of LSH: Random Hyperplane-based LSH and
Query-Aware LSH. We rigorously compared these two approaches across four
parameters: HitRate, Mean Reciprocal Rank (MRR), Average Execution Time, and
Relevance.
</p>
<p>Our study revealed that the Query-Aware (QA) approach showed superior
performance over the Random Hyperplane-based (RH) method. Specifically, it
exhibited a notable improvement of 20% to 35% in HitRate for query pairs
compared to the RH approach. Furthermore, the QA approach proved significantly
more time-efficient, with its speed in creating hashing tables and assigning
data samples to buckets being at least four times faster. It can return code
examples within milliseconds, whereas the RH approach typically requires
several seconds to recommend code examples. Due to the superior performance of
the QA approach, we tested it against PostFinder and FaCoY, the
state-of-the-art baselines. Our QA method showed comparable efficiency proving
its potential for effective code recommendation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15066">GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. (arXiv:2305.15066v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiayan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Lun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hengyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mengyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xinyi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shi Han</a></p>
<p>Large language models~(LLM) like ChatGPT have become indispensable to
artificial general intelligence~(AGI), demonstrating excellent performance in
various natural language processing tasks. In the real world, graph data is
ubiquitous and an essential part of AGI and prevails in domains like social
network analysis, bioinformatics and recommender systems. The training corpus
of large language models often includes some algorithmic components, which
allows them to achieve certain effects on some graph data-related problems.
However, there is still little research on their performance on a broader range
of graph-structured data. In this study, we conduct an extensive investigation
to assess the proficiency of LLMs in comprehending graph data, employing a
diverse range of structural and semantic-related tasks. Our analysis
encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph
understanding. Through our study, we not only uncover the current limitations
of language models in comprehending graph structures and performing associated
reasoning tasks but also emphasize the necessity for further advancements and
novel approaches to enhance their graph processing capabilities. Our findings
contribute valuable insights towards bridging the gap between language models
and graph understanding, paving the way for more effective graph mining and
knowledge extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01147">Smooth Monotonic Networks. (arXiv:2306.01147v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1">Christian Igel</a></p>
<p>Monotonicity constraints are powerful regularizers in statistical modelling.
They can support fairness in computer supported decision making and increase
plausibility in data-driven scientific models. The seminal min-max (MM) neural
network architecture ensures monotonicity, but often gets stuck in undesired
local optima during training because of vanishing gradients. We propose a
simple modification of the MM network using strictly-increasing smooth
non-linearities that alleviates this problem. The resulting smooth min-max
(SMM) network module inherits the asymptotic approximation properties from the
MM architecture. It can be used within larger deep learning systems trained
end-to-end. The SMM module is considerably simpler and less computationally
demanding than state-of-the-art neural networks for monotonic modelling. Still,
in our experiments, it compared favorably to alternative neural and non-neural
approaches in terms of generalization performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09841">Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond. (arXiv:2306.09841v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Fangzhi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qika Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a></p>
<p>Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16015">BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1">Stefan T Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1">Marvin Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schumacher_L/0/1/0/all/0/1">Lukas Schumacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsemuller_L/0/1/0/all/0/1">Lasse Elsem&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratz_V/0/1/0/all/0/1">Valentin Pratz</a>, <a href="http://arxiv.org/find/cs/1/au:+Schalte_Y/0/1/0/all/0/1">Yannik Sch&#xe4;lte</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1">Ullrich K&#xf6;the</a>, <a href="http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1">Paul-Christian B&#xfc;rkner</a></p>
<p>Modern Bayesian inference involves a mixture of computational techniques for
estimating, validating, and drawing conclusions from probabilistic models as
part of principled workflows for data analysis. Typical problems in Bayesian
workflows are the approximation of intractable posterior distributions for
diverse model types and the comparison of competing models of the same process
in terms of their complexity and predictive performance. This manuscript
introduces the Python library BayesFlow for simulation-based training of
established neural network architectures for amortized data compression and
inference. Amortized Bayesian inference, as implemented in BayesFlow, enables
users to train custom neural networks on model simulations and re-use these
networks for any subsequent application of the models. Since the trained
networks can perform inference almost instantaneously, the upfront neural
network training is quickly amortized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16177">Defining data science: a new field of inquiry. (arXiv:2306.16177v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brodie_M/0/1/0/all/0/1">Michael L Brodie</a></p>
<p>Data science is not a science. It is a research paradigm. Its power, scope,
and scale will surpass science, our most powerful research paradigm, to enable
knowledge discovery and change our world. We have yet to understand and define
it, vital to realizing its potential and managing its risks. Modern data
science is in its infancy. Emerging slowly since 1962 and rapidly since 2000,
it is a fundamentally new field of inquiry, one of the most active, powerful,
and rapidly evolving 21st century innovations. Due to its value, power, and
applicability, it is emerging in 40+ disciplines, hundreds of research areas,
and thousands of applications. Millions of data science publications contain
myriad definitions of data science and data science problem solving. Due to its
infancy, many definitions are independent, application-specific, mutually
incomplete, redundant, or inconsistent, hence so is data science. This research
addresses this data science multiple definitions challenge by proposing the
development of coherent, unified definition based on a data science reference
framework using a data science journal for the data science community to
achieve such a definition. This paper provides candidate definitions for
essential data science artifacts that are required to discuss such a
definition. They are based on the classical research paradigm concept
consisting of a philosophy of data science, the data science problem solving
paradigm, and the six component data science reference framework (axiology,
ontology, epistemology, methodology, methods, technology) that is a frequently
called for unifying framework with which to define, unify, and evolve data
science. It presents challenges for defining data science, solution approaches,
i.e., means for defining data science, and their requirements and benefits as
the basis of a comprehensive solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02738">RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1">Brandon Kynoch</a>, <a href="http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1">Hugo Latapie</a></p>
<p>The ideal long-term memory mechanism for Large Language Model (LLM) based
chatbots, would lay the foundation for continual learning, complex reasoning
and allow sequential and temporal dependencies to be learnt. Creating this type
of memory mechanism is an extremely challenging problem. In this paper we
explore different methods of achieving the effect of long-term memory. We
propose a new architecture focused on creating adaptable and updatable
long-term memory for AGI systems. We demonstrate through various experiments
the benefits of the RecallM architecture, particularly the improved temporal
understanding of knowledge it provides.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03109">A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yupeng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Linyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1">Xiaoyuan Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cunxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wei Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03913">Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zaifeng Gao</a></p>
<p>Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
</p>
<p>Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13339">TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1">Jiahe Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1">Elisa Bertino</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a></p>
<p>Trust evaluation assesses trust relationships between entities and
facilitates decision-making. Machine Learning (ML) shows great potential for
trust evaluation owing to its learning capabilities. In recent years, Graph
Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in
dealing with graph data. This has motivated researchers to explore their use in
trust evaluation, as trust relationships among entities can be modeled as a
graph. However, current trust evaluation methods that employ GNNs fail to fully
satisfy the dynamicity nature of trust, overlook the adverse effects of attacks
on trust evaluation, and cannot provide convincing explanations on evaluation
results. To address these problems, in this paper, we propose TrustGuard, a
GNN-based accurate trust evaluation model that supports trust dynamicity, is
robust against typical attacks, and provides explanations through
visualization. Specifically, TrustGuard is designed with a layered architecture
that contains a snapshot input layer, a spatial aggregation layer, a temporal
aggregation layer, and a prediction layer. Among them, the spatial aggregation
layer can be plugged into a defense mechanism for a robust aggregation of local
trust relationships, and the temporal aggregation layer applies an attention
mechanism for effective learning of temporal patterns. Extensive experiments on
two real-world datasets show that TrustGuard outperforms state-of-the-art
GNN-based trust evaluation models with respect to trust prediction across
single-timeslot and multi-timeslot, even in the presence of attacks. In
particular, TrustGuard can explain its evaluation results by visualizing both
spatial and temporal views.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03419">QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v2 [cs.CY] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geerkens_S/0/1/0/all/0/1">Simon Geerkens</a>, <a href="http://arxiv.org/find/cs/1/au:+Sieberichs_C/0/1/0/all/0/1">Christian Sieberichs</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1">Alexander Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1">Thomas Waschulzik</a></p>
<p>The importance of high data quality is increasing with the growing impact and
distribution of ML systems and big data. Also the planned AI Act from the
European commission defines challenging legal requirements for data quality
especially for the market introduction of safety relevant ML systems. In this
paper we introduce a novel approach that supports the data quality assurance
process of multiple data quality aspects. This approach enables the
verification of quantitative data quality requirements. The concept and
benefits are introduced and explained on small example data sets. How the
method is applied is demonstrated on the well known MNIST data set based an
handwritten digits.
</p>
</p>
</div>

    </div>
    </body>
    