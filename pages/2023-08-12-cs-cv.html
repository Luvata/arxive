<!DOCTYPE html>
<html>
<head>
<title>2023-08-12-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.05106">Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures. (arXiv:2308.05106v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quentin_P/0/1/0/all/0/1">Pajon Quentin</a>, <a href="http://arxiv.org/find/cs/1/au:+Swan_S/0/1/0/all/0/1">Serre Swan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hugo_W/0/1/0/all/0/1">Wissocq Hugo</a>, <a href="http://arxiv.org/find/cs/1/au:+Leo_R/0/1/0/all/0/1">Rabaud L&#xe9;o</a>, <a href="http://arxiv.org/find/cs/1/au:+Siba_H/0/1/0/all/0/1">Haidar Siba</a>, <a href="http://arxiv.org/find/cs/1/au:+Antoun_Y/0/1/0/all/0/1">Yaacoub Antoun</a></p>
<p>This paper presents an investigation into machine learning techniques for
violence detection in videos and their adaptation to a federated learning
context. The study includes experiments with spatio-temporal features extracted
from benchmark video datasets, comparison of different methods, and proposal of
a modified version of the "Flow-Gated" architecture called "Diff-Gated."
Additionally, various machine learning techniques, including super-convergence
and transfer learning, are explored, and a method for adapting centralized
datasets to a federated learning context is developed. The research achieves
better accuracy results compared to state-of-the-art models by training the
best violence detection model in a federated learning context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05112">Explicifying Neural Implicit Fields for Efficient Dynamic Human Avatar Modeling via a Neural Explicit Surface. (arXiv:2308.05112v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiang Wang</a></p>
<p>This paper proposes a technique for efficiently modeling dynamic humans by
explicifying the implicit neural fields via a Neural Explicit Surface (NES).
Implicit neural fields have advantages over traditional explicit
representations in modeling dynamic 3D content from sparse observations and
effectively representing complex geometries and appearances. Implicit neural
fields defined in 3D space, however, are expensive to render due to the need
for dense sampling during volumetric rendering. Moreover, their memory
efficiency can be further optimized when modeling sparse 3D space. To overcome
these issues, the paper proposes utilizing Neural Explicit Surface (NES) to
explicitly represent implicit neural fields, facilitating memory and
computational efficiency. To achieve this, the paper creates a fully
differentiable conversion between the implicit neural fields and the explicit
rendering interface of NES, leveraging the strengths of both implicit and
explicit approaches. This conversion enables effective training of the hybrid
representation using implicit methods and efficient rendering by integrating
the explicit rendering interface with a newly proposed rasterization-based
neural renderer that only incurs a texture color query once for the initial ray
interaction with the explicit surface, resulting in improved inference
efficiency. NES describes dynamic human geometries with pose-dependent neural
implicit surface deformation fields and their dynamic neural textures both in
2D space, which is a more memory-efficient alternative to traditional 3D
methods, reducing redundancy and computational load. The comprehensive
experiments show that NES performs similarly to previous 3D approaches, with
greatly improved rendering speed and reduced memory cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05122">Copy Number Variation Informs fMRI-based Prediction of Autism Spectrum Disorder. (arXiv:2308.05122v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Dvornek_N/0/1/0/all/0/1">Nicha C. Dvornek</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sullivan_C/0/1/0/all/0/1">Catherine Sullivan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1">Abha R. Gupta</a></p>
<p>The multifactorial etiology of autism spectrum disorder (ASD) suggests that
its study would benefit greatly from multimodal approaches that combine data
from widely varying platforms, e.g., neuroimaging, genetics, and clinical
characterization. Prior neuroimaging-genetic analyses often apply naive feature
concatenation approaches in data-driven work or use the findings from one
modality to guide posthoc analysis of another, missing the opportunity to
analyze the paired multimodal data in a truly unified approach. In this paper,
we develop a more integrative model for combining genetic, demographic, and
neuroimaging data. Inspired by the influence of genotype on phenotype, we
propose using an attention-based approach where the genetic data guides
attention to neuroimaging features of importance for model prediction. The
genetic data is derived from copy number variation parameters, while the
neuroimaging data is from functional magnetic resonance imaging. We evaluate
the proposed approach on ASD classification and severity prediction tasks,
using a sex-balanced dataset of 228 ASD and typically developing subjects in a
10-fold cross-validation framework. We demonstrate that our attention-based
model combining genetic information, demographic data, and functional magnetic
resonance imaging results in superior prediction performance compared to other
multimodal approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05123">Towards Automatic Scoring of Spinal X-ray for Ankylosing Spondylitis. (arXiv:2308.05123v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1">Yuanhan Mo</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Readie_A/0/1/0/all/0/1">Aimee Readie</a>, <a href="http://arxiv.org/find/eess/1/au:+Ligozio_G/0/1/0/all/0/1">Gregory Ligozio</a>, <a href="http://arxiv.org/find/eess/1/au:+Coroller_T/0/1/0/all/0/1">Thibaud Coroller</a>, <a href="http://arxiv.org/find/eess/1/au:+Papiez_B/0/1/0/all/0/1">Bart&#x142;omiej W. Papie&#x17c;</a></p>
<p>Manually grading structural changes with the modified Stoke Ankylosing
Spondylitis Spinal Score (mSASSS) on spinal X-ray imaging is costly and
time-consuming due to bone shape complexity and image quality variations. In
this study, we address this challenge by prototyping a 2-step auto-grading
pipeline, called VertXGradeNet, to automatically predict mSASSS scores for the
cervical and lumbar vertebral units (VUs) in X-ray spinal imaging. The
VertXGradeNet utilizes VUs generated by our previously developed VU extraction
pipeline (VertXNet) as input and predicts mSASSS based on those VUs.
VertXGradeNet was evaluated on an in-house dataset of lateral cervical and
lumbar X-ray images for axial spondylarthritis patients. Our results show that
VertXGradeNet can predict the mSASSS score for each VU when the data is limited
in quantity and imbalanced. Overall, it can achieve a balanced accuracy of 0.56
and 0.51 for 4 different mSASSS scores (i.e., a score of 0, 1, 2, 3) on two
test datasets. The accuracy of the presented method shows the potential to
streamline the spinal radiograph readings and therefore reduce the cost of
future clinical trials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05127">Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1">Harshit Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+G_A/0/1/0/all/0/1">Aravindhan G</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1">Pavan Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Govidarajulu_Y/0/1/0/all/0/1">Yuvaraj Govidarajulu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1">Manojkumar Parmar</a></p>
<p>A significant number of machine learning models are vulnerable to model
extraction attacks, which focus on stealing the models by using specially
curated queries against the target model. This task is well accomplished by
using part of the training data or a surrogate dataset to train a new model
that mimics a target model in a white-box environment. In pragmatic situations,
however, the target models are trained on private datasets that are
inaccessible to the adversary. The data-free model extraction technique
replaces this problem when it comes to using queries artificially curated by a
generator similar to that used in Generative Adversarial Nets. We propose for
the first time, to the best of our knowledge, an adversary black box attack
extending to a regression problem for predicting bounding box coordinates in
object detection. As part of our study, we found that defining a loss function
and using a novel generator setup is one of the key aspects in extracting the
target model. We find that the proposed model extraction method achieves
significant results by using reasonable queries. The discovery of this object
detection vulnerability will support future prospects for securing such models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05128">High-Level Features Parallelization for Inference Cost Reduction Through Selective Attention. (arXiv:2308.05128v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kelm_A/0/1/0/all/0/1">Andr&#xe9; Peter Kelm</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1">Lucas Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolff_T/0/1/0/all/0/1">Tim Rolff</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1">Christian Wilms</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaghoubi_E/0/1/0/all/0/1">Ehsan Yaghoubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1">Simone Frintrop</a></p>
<p>In this work, we parallelize high-level features in deep networks to
selectively skip or select class-specific features to reduce inference costs.
This challenges most deep learning methods due to their limited ability to
efficiently and effectively focus on selected class-specific features without
retraining. We propose a serial-parallel hybrid architecture with serial
generic low-level features and parallel high-level features. This accounts for
the fact that many high-level features are class-specific rather than generic,
and has connections to recent neuroscientific findings that observe spatially
and contextually separated neural activations in the human brain. Our approach
provides the unique functionality of cutouts: selecting parts of the network to
focus on only relevant subsets of classes without requiring retraining. High
performance is maintained, but the cost of inference can be significantly
reduced. In some of our examples, up to $75\,\%$ of parameters are skipped and
$35\,\%$ fewer GMACs (Giga multiply-accumulate) operations are used as the
approach adapts to a change in task complexity. This is important for mobile,
industrial, and robotic applications where reducing the number of parameters,
the computational complexity, and thus the power consumption can be paramount.
Another unique functionality is that it allows processing to be directly
influenced by enhancing or inhibiting high-level class-specific features,
similar to the mechanism of selective attention in the human brain. This can be
relevant for cross-modal applications, the use of semantic prior knowledge,
and/or context-aware processing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05129">Are Sex-based Physiological Differences the Cause of Gender Bias for Chest X-ray Diagnosis?. (arXiv:2308.05129v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Weng_N/0/1/0/all/0/1">Nina Weng</a>, <a href="http://arxiv.org/find/eess/1/au:+Bigdeli_S/0/1/0/all/0/1">Siavash Bigdeli</a>, <a href="http://arxiv.org/find/eess/1/au:+Petersen_E/0/1/0/all/0/1">Eike Petersen</a>, <a href="http://arxiv.org/find/eess/1/au:+Feragen_A/0/1/0/all/0/1">Aasa Feragen</a></p>
<p>While many studies have assessed the fairness of AI algorithms in the medical
field, the causes of differences in prediction performance are often unknown.
This lack of knowledge about the causes of bias hampers the efficacy of bias
mitigation, as evidenced by the fact that simple dataset balancing still often
performs best in reducing performance gaps but is unable to resolve all
performance differences. In this work, we investigate the causes of gender bias
in machine learning-based chest X-ray diagnosis. In particular, we explore the
hypothesis that breast tissue leads to underexposure of the lungs and causes
lower model performance. Methodologically, we propose a new sampling method
which addresses the highly skewed distribution of recordings per patient in two
widely used public datasets, while at the same time reducing the impact of
label errors. Our comprehensive analysis of gender differences across diseases,
datasets, and gender representations in the training set shows that dataset
imbalance is not the sole cause of performance differences. Moreover, relative
group performance differs strongly between datasets, indicating important
dataset-specific factors influencing male/female group performance. Finally, we
investigate the effect of breast tissue more specifically, by cropping out the
breasts from recordings, finding that this does not resolve the observed
performance gaps. In conclusion, our results indicate that dataset-specific
factors, not fundamental physiological differences, are the main drivers of
male--female performance gaps in chest X-ray analyses on widely used NIH and
CheXpert Dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05137">Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images. (arXiv:2308.05137v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1">Fan Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xiaohan Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yutian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Han Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1">Max Q.-H. Meng</a></p>
<p>Weakly supervised methods, such as class activation maps (CAM) based, have
been applied to achieve bleeding segmentation with low annotation efforts in
Wireless Capsule Endoscopy (WCE) images. However, the CAM labels tend to be
extremely noisy, and there is an irreparable gap between CAM labels and ground
truths for medical images. This paper proposes a new Discrepancy-basEd Active
Learning (DEAL) approach to bridge the gap between CAMs and ground truths with
a few annotations. Specifically, to liberate labor, we design a novel
discrepancy decoder model and a CAMPUS (CAM, Pseudo-label and groUnd-truth
Selection) criterion to replace the noisy CAMs with accurate model predictions
and a few human labels. The discrepancy decoder model is trained with a unique
scheme to generate standard, coarse and fine predictions. And the CAMPUS
criterion is proposed to predict the gaps between CAMs and ground truths based
on model divergence and CAM divergence. We evaluate our method on the WCE
dataset and results show that our method outperforms the state-of-the-art
active learning methods and reaches comparable performance to those trained
with full annotated datasets with only 10% of the training data labeled.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05140">Robust Object Modeling for Visual Tracking. (arXiv:2308.05140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yidong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gangshan Wu</a></p>
<p>Object modeling has become a core part of recent tracking frameworks. Current
popular tackers use Transformer attention to extract the template feature
separately or interactively with the search region. However, separate template
learning lacks communication between the template and search regions, which
brings difficulty in extracting discriminative target-oriented features. On the
other hand, interactive template learning produces hybrid template features,
which may introduce potential distractors to the template via the cluttered
search regions. To enjoy the merits of both methods, we propose a robust object
modeling framework for visual tracking (ROMTrack), which simultaneously models
the inherent template and the hybrid template features. As a result, harmful
distractors can be suppressed by combining the inherent features of target
objects with search regions' guidance. Target-related features can also be
extracted using the hybrid template, thus resulting in a more robust object
modeling framework. To further enhance robustness, we present novel variation
tokens to depict the ever-changing appearance of target objects. Variation
tokens are adaptable to object deformation and appearance variations, which can
boost overall performance with negligible computation. Experiments show that
our ROMTrack sets a new state-of-the-art on multiple benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05166">Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels. (arXiv:2308.05166v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Gupta_N/0/1/0/all/0/1">Nikhel Gupta</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hayder_Z/0/1/0/all/0/1">Zeeshan Hayder</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Norris_R/0/1/0/all/0/1">Ray P. Norris</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Huynh_M/0/1/0/all/0/1">Minh Huynh</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Petersson_L/0/1/0/all/0/1">Lars Petersson</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Wang_X/0/1/0/all/0/1">X. Rosalind Wang</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Andernach_H/0/1/0/all/0/1">Heinz Andernach</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Koribalski_B/0/1/0/all/0/1">B&#xe4;rbel S. Koribalski</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Yew_M/0/1/0/all/0/1">Miranda Yew</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Crawford_E/0/1/0/all/0/1">Evan J. Crawford</a></p>
<p>The present work discusses the use of a weakly-supervised deep learning
algorithm that reduces the cost of labelling pixel-level masks for complex
radio galaxies with multiple components. The algorithm is trained on weak
class-level labels of radio galaxies to get class activation maps (CAMs). The
CAMs are further refined using an inter-pixel relations network (IRNet) to get
instance segmentation masks over radio galaxies and the positions of their
infrared hosts. We use data from the Australian Square Kilometre Array
Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe
(EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS
sensitivity of 25-35 $\mu$Jy/beam. We demonstrate that weakly-supervised deep
learning algorithms can achieve high accuracy in predicting pixel-level
information, including masks for the extended radio emission encapsulating all
galaxy components and the positions of the infrared host galaxies. We evaluate
the performance of our method using mean Average Precision (mAP) across
multiple classes at a standard intersection over union (IoU) threshold of 0.5.
We show that the model achieves a mAP$_{50}$ of 67.5\% and 76.8\% for radio
masks and infrared host positions, respectively. The network architecture can
be found at the following link: https://github.com/Nikhel1/Gal-CAM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05168">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision. (arXiv:2308.05168v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changjian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yukai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1">Fengyuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shilong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Weikai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1">Hanspeter Pfister</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shixia Liu</a></p>
<p>Existing model evaluation tools mainly focus on evaluating classification
models, leaving a gap in evaluating more complex models, such as object
detection. In this paper, we develop an open-source visual analysis tool,
Uni-Evaluator, to support a unified model evaluation for classification, object
detection, and instance segmentation in computer vision. The key idea behind
our method is to formulate both discrete and continuous predictions in
different tasks as unified probability distributions. Based on these
distributions, we develop 1) a matrix-based visualization to provide an
overview of model performance; 2) a table visualization to identify the
problematic data subsets where the model performs poorly; 3) a grid
visualization to display the samples of interest. These visualizations work
together to facilitate the model evaluation from a global overview to
individual samples. Two case studies demonstrate the effectiveness of
Uni-Evaluator in evaluating model performance and making informed improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05178">An Improved Model for Diabetic Retinopathy Detection by using Transfer Learning and Ensemble Learning. (arXiv:2308.05178v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Talukder_M/0/1/0/all/0/1">Md. Simul Hasan Talukder</a>, <a href="http://arxiv.org/find/eess/1/au:+Sarkar_A/0/1/0/all/0/1">Ajay Kirshno Sarkar</a>, <a href="http://arxiv.org/find/eess/1/au:+Akter_S/0/1/0/all/0/1">Sharmin Akter</a>, <a href="http://arxiv.org/find/eess/1/au:+Nuhi_Alamin_M/0/1/0/all/0/1">Md. Nuhi-Alamin</a></p>
<p>Diabetic Retinopathy (DR) is an ocular condition caused by a sustained high
level of sugar in the blood, which causes the retinal capillaries to block and
bleed, causing retinal tissue damage. It usually results in blindness. Early
detection can help in lowering the risk of DR and its severity. The robust and
accurate prediction and detection of diabetic retinopathy is a challenging
task. This paper develops a machine learning model for detecting Diabetic
Retinopathy that is entirely accurate. Pre-trained models such as ResNet50,
InceptionV3, Xception, DenseNet121, VGG19, NASNetMobile, MobileNetV2,
DensNet169, and DenseNet201 with pooling layer, dense layer, and appropriate
dropout layer at the bottom of them were carried out in transfer learning (TL)
approach. Data augmentation and regularization was performed to reduce
overfitting. Transfer Learning model of DenseNet121, Average and weighted
ensemble of DenseNet169 and DenseNet201 TL architectures contribute
individually the highest accuracy of 100%, the highest precision, recall, F-1
score of 100%, 100%, and 100%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05179">JutePestDetect: An Intelligent Approach for Jute Pest Identification Using Fine-Tuned Transfer Learning. (arXiv:2308.05179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Talukder_M/0/1/0/all/0/1">Md. Simul Hasan Talukder</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1">Mohammad Raziuddin Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Sourav_M/0/1/0/all/0/1">Md Sakib Ullah Sourav</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakin_A/0/1/0/all/0/1">Abdullah Al Rakin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shuvo_S/0/1/0/all/0/1">Shabbir Ahmed Shuvo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sulaiman_R/0/1/0/all/0/1">Rejwan Bin Sulaiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Nipun_M/0/1/0/all/0/1">Musarrat Saberin Nipun</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Muntarin Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Mst Rumpa Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Aminul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_Z/0/1/0/all/0/1">Zubaer Haque</a></p>
<p>In certain Asian countries, Jute is one of the primary sources of income and
Gross Domestic Product (GDP) for the agricultural sector. Like many other
crops, Jute is prone to pest infestations, and its identification is typically
made visually in countries like Bangladesh, India, Myanmar, and China. In
addition, this method is time-consuming, challenging, and somewhat imprecise,
which poses a substantial financial risk. To address this issue, the study
proposes a high-performing and resilient transfer learning (TL) based
JutePestDetect model to identify jute pests at the early stage. Firstly, we
prepared jute pest dataset containing 17 classes and around 380 photos per pest
class, which were evaluated after manual and automatic pre-processing and
cleaning, such as background removal and resizing. Subsequently, five prominent
pre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50
were selected from a previous study to design the JutePestDetect model. Each
model was revised by replacing the classification layer with a global average
pooling layer and incorporating a dropout layer for regularization. To evaluate
the models performance, various metrics such as precision, recall, F1 score,
ROC curve, and confusion matrix were employed. These analyses provided
additional insights for determining the efficacy of the models. Among them, the
customized regularized DenseNet201-based proposed JutePestDetect model
outperformed the others, achieving an impressive accuracy of 99%. As a result,
our proposed method and strategy offer an enhanced approach to pest
identification in the case of Jute, which can significantly benefit farmers
worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05189">Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding. (arXiv:2308.05189v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Torres_M/0/1/0/all/0/1">Miguel-&#xc1;ngel Fern&#xe1;ndez-Torres</a></p>
<p>This PhD. Thesis concerns the study and development of hierarchical
representations for spatio-temporal visual attention modeling and understanding
in video sequences. More specifically, we propose two computational models for
visual attention. First, we present a generative probabilistic model for
context-aware visual attention modeling and understanding. Secondly, we develop
a deep network architecture for visual attention modeling, which first
estimates top-down spatio-temporal visual attention, and ultimately serves for
modeling attention in the temporal domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05232">SegMatch: A semi-supervised learning method for surgical instrument segmentation. (arXiv:2308.05232v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1">Meng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Budd_C/0/1/0/all/0/1">Charlie Budd</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_Peraza_Herrera_L/0/1/0/all/0/1">Luis C. Garcia-Peraza-Herrera</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1">Reuben Dorent</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Miaojing Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1">Tom Vercauteren</a></p>
<p>Surgical instrument segmentation is recognised as a key enabler to provide
advanced surgical assistance and improve computer assisted interventions. In
this work, we propose SegMatch, a semi supervised learning method to reduce the
need for expensive annotation for laparoscopic and robotic surgical images.
SegMatch builds on FixMatch, a widespread semi supervised classification
pipeline combining consistency regularization and pseudo labelling, and adapts
it for the purpose of segmentation. In our proposed SegMatch, the unlabelled
images are weakly augmented and fed into the segmentation model to generate a
pseudo-label to enforce the unsupervised loss against the output of the model
for the adversarial augmented image on the pixels with a high confidence score.
Our adaptation for segmentation tasks includes carefully considering the
equivariance and invariance properties of the augmentation functions we rely
on. To increase the relevance of our augmentations, we depart from using only
handcrafted augmentations and introduce a trainable adversarial augmentation
strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation
Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate
that adding unlabelled data for training purposes allows us to surpass the
performance of fully supervised approaches which are limited by the
availability of training data in these challenges. SegMatch also outperforms a
range of state-of-the-art semi-supervised learning semantic segmentation models
in different labelled to unlabelled data ratios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05234">Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving. (arXiv:2308.05234v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hawlader_F/0/1/0/all/0/1">Faisal Hawlader</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinet_F/0/1/0/all/0/1">Fran&#xe7;ois Robinet</a>, <a href="http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1">Rapha&#xeb;l Frank</a></p>
<p>Environmental perception is a key element of autonomous driving because the
information received from the perception module influences core driving
decisions. An outstanding challenge in real-time perception for autonomous
driving lies in finding the best trade-off between detection quality and
latency. Major constraints on both computation and power have to be taken into
account for real-time perception in autonomous vehicles. Larger object
detection models tend to produce the best results, but are also slower at
runtime. Since the most accurate detectors cannot run in real-time locally, we
investigate the possibility of offloading computation to edge and cloud
platforms, which are less resource-constrained. We create a synthetic dataset
to train object detection models and evaluate different offloading strategies.
Using real hardware and network simulations, we compare different trade-offs
between prediction quality and end-to-end delay. Since sending raw frames over
the network implies additional transmission delays, we also explore the use of
JPEG and H.265 compression at varying qualities and measure their impact on
prediction metrics. We show that models with adequate compression can be run in
real-time on the cloud while outperforming local detection performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05235">Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping. (arXiv:2308.05235v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jamali_A/0/1/0/all/0/1">Ali Jamali</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Swalpa Kumar Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1">Danfeng Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1">Peter M Atkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1">Pedram Ghamisi</a></p>
<p>Convolutional Neural Networks (CNNs) are models that are utilized extensively
for the hierarchical extraction of features. Vision transformers (ViTs),
through the use of a self-attention mechanism, have recently achieved superior
modeling of global contextual information compared to CNNs. However, to realize
their image classification strength, ViTs require substantial training
datasets. Where the available training data are limited, current advanced
multi-layer perceptrons (MLPs) can provide viable alternatives to both deep
CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm
that effectively uses both MLPs and spatial gating units (SGUs) for precise
land use land cover (LULC) mapping. Results illustrated the superiority of the
developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based
models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The
proposed SGU-MLP algorithm was tested through three experiments in Houston,
USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model
was found to consistently outperform the benchmark CNN and CNN-ViT-based
algorithms. For example, for the Houston experiment, SGU-MLP significantly
outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by
approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average
accuracy. The code will be made publicly available at
https://github.com/aj1365/SGUMLP
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05242">Vector quantization loss analysis in VQGANs: a single-GPU ablation study for image-to-image synthesis. (arXiv:2308.05242v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_L/0/1/0/all/0/1">Luv Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1">Varun Mohan</a></p>
<p>This study performs an ablation analysis of Vector Quantized Generative
Adversarial Networks (VQGANs), concentrating on image-to-image synthesis
utilizing a single NVIDIA A100 GPU. The current work explores the nuanced
effects of varying critical parameters including the number of epochs, image
count, and attributes of codebook vectors and latent dimensions, specifically
within the constraint of limited resources. Notably, our focus is pinpointed on
the vector quantization loss, keeping other hyperparameters and loss components
(GAN loss) fixed. This was done to delve into a deeper understanding of the
discrete latent space, and to explore how varying its size affects the
reconstruction. Though, our results do not surpass the existing benchmarks,
however, our findings shed significant light on VQGAN's behaviour for a smaller
dataset, particularly concerning artifacts, codebook size optimization, and
comparative analysis with Principal Component Analysis (PCA). The study also
uncovers the promising direction by introducing 2D positional encodings,
revealing a marked reduction in artifacts and insights into balancing clarity
and overfitting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05257">Advancing Early Detection of Virus Yellows: Developing a Hybrid Convolutional Neural Network for Automatic Aphid Counting in Sugar Beet Fields. (arXiv:2308.05257v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xumin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wenxin Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Lennox_C/0/1/0/all/0/1">Callum Lennox</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_M/0/1/0/all/0/1">Mark Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junfeng Gao</a></p>
<p>Aphids are efficient vectors to transmit virus yellows in sugar beet fields.
Timely monitoring and control of their populations are thus critical to prevent
the large-scale outbreak of virus yellows. However, the manual counting of
aphids, which is the most common practice, is labor-intensive and
time-consuming. Additionally, two of the biggest challenges in aphid counting
are that aphids are small objects and their density distributions are varied in
different areas of the field. To address these challenges, we proposed a hybrid
automatic aphid counting network architecture which integrates the detection
network and the density map estimation network. When the distribution density
of aphids is low, it utilizes an improved Yolov5 to count aphids. Conversely,
when the distribution density of aphids is high, its witches to CSRNet to count
aphids. To the best of our knowledge, this is the first framework integrating
the detection network and the density map estimation network for counting
tasks. Through comparison experiments of counting aphids, it verified that our
proposed approach outperforms all other methods in counting aphids. It achieved
the lowest MAE and RMSE values for both the standard and high-density aphid
datasets: 2.93 and 4.01 (standard), and 34.19 and 38.66 (high-density),
respectively. Moreover, the AP of the improved Yolov5 is 5% higher than that of
the original Yolov5. Especially for extremely small aphids and densely
distributed aphids, the detection performance of the improved Yolov5 is
significantly better than the original Yolov5. This work provides an effective
early warning for the virus yellows risk caused by aphids in sugar beet fields,
offering protection for sugar beet growth and ensuring sugar beet yield. The
datasets and project code are released at:
https://github.com/JunfengGaolab/Counting-Aphids.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05264">TrainFors: A Large Benchmark Training Dataset for Image Manipulation Detection and Localization. (arXiv:2308.05264v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1">Soumyaroop Nandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1">Prem Natarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Abd_Almageed_W/0/1/0/all/0/1">Wael Abd-Almageed</a></p>
<p>The evaluation datasets and metrics for image manipulation detection and
localization (IMDL) research have been standardized. But the training dataset
for such a task is still nonstandard. Previous researchers have used
unconventional and deviating datasets to train neural networks for detecting
image forgeries and localizing pixel maps of manipulated regions. For a fair
comparison, the training set, test set, and evaluation metrics should be
persistent. Hence, comparing the existing methods may not seem fair as the
results depend heavily on the training datasets as well as the model
architecture. Moreover, none of the previous works release the synthetic
training dataset used for the IMDL task. We propose a standardized benchmark
training dataset for image splicing, copy-move forgery, removal forgery, and
image enhancement forgery. Furthermore, we identify the problems with the
existing IMDL datasets and propose the required modifications. We also train
the state-of-the-art IMDL methods on our proposed TrainFors1 dataset for a fair
evaluation and report the actual performance of these methods under similar
conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05274">Local-Global Information Interaction Debiasing for Dynamic Scene Graph Generation. (arXiv:2308.05274v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xinyu Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lianli Gao</a></p>
<p>The task of dynamic scene graph generation (DynSGG) aims to generate scene
graphs for given videos, which involves modeling the spatial-temporal
information in the video. However, due to the long-tailed distribution of
samples in the dataset, previous DynSGG models fail to predict the tail
predicates. We argue that this phenomenon is due to previous methods that only
pay attention to the local spatial-temporal information and neglect the
consistency of multiple frames. To solve this problem, we propose a novel
DynSGG model based on multi-task learning, DynSGG-MTL, which introduces the
local interaction information and global human-action interaction information.
The interaction between objects and frame features makes the model more fully
understand the visual context of the single image. Long-temporal human actions
supervise the model to generate multiple scene graphs that conform to the
global constraints and avoid the model being unable to learn the tail
predicates. Extensive experiments on Action Genome dataset demonstrate the
efficacy of our proposed framework, which not only improves the dynamic scene
graph generation but also alleviates the long-tail problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05286">Informative Scene Graph Generation via Debiasing. (arXiv:2308.05286v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lianli Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xinyu Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yuxuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan-Fang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a></p>
<p>Scene graph generation aims to detect visual relationship triplets, (subject,
predicate, object). Due to biases in data, current models tend to predict
common predicates, e.g. "on" and "at", instead of informative ones, e.g.
"standing on" and "looking at". This tendency results in the loss of precise
information and overall performance. If a model only uses "stone on road"
rather than "stone blocking road" to describe an image, it may be a grave
misunderstanding. We argue that this phenomenon is caused by two imbalances:
semantic space level imbalance and training sample level imbalance. For this
problem, we propose DB-SGG, an effective framework based on debiasing but not
the conventional distribution fitting. It integrates two components: Semantic
Debiasing (SD) and Balanced Predicate Learning (BPL), for these imbalances. SD
utilizes a confusion matrix and a bipartite graph to construct predicate
relationships. BPL adopts a random undersampling strategy and an ambiguity
removing strategy to focus on informative predicates. Benefiting from the
model-agnostic process, our method can be easily applied to SGG models and
outperforms Transformer by 136.3%, 119.5%, and 122.6% on mR@20 at three SGG
sub-tasks on the SGG-VG dataset. Our method is further verified on another
complex SGG dataset (SGG-GQA) and two downstream tasks (sentence-to-graph
retrieval and image captioning).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05298">Double-chain Constraints for 3D Human Pose Estimation in Images and Videos. (arXiv:2308.05298v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hongbo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Doudou Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenming Yang</a></p>
<p>Reconstructing 3D poses from 2D poses lacking depth information is
particularly challenging due to the complexity and diversity of human motion.
The key is to effectively model the spatial constraints between joints to
leverage their inherent dependencies. Thus, we propose a novel model, called
Double-chain Graph Convolutional Transformer (DC-GCT), to constrain the pose
through a double-chain design consisting of local-to-global and global-to-local
chains to obtain a complex representation more suitable for the current human
pose. Specifically, we combine the advantages of GCN and Transformer and design
a Local Constraint Module (LCM) based on GCN and a Global Constraint Module
(GCM) based on self-attention mechanism as well as a Feature Interaction Module
(FIM). The proposed method fully captures the multi-level dependencies between
human body joints to optimize the modeling capability of the model. Moreover,
we propose a method to use temporal information into the single-frame model by
guiding the video sequence embedding through the joint embedding of the target
frame, with negligible increase in computational cost. Experimental results
demonstrate that DC-GCT achieves state-of-the-art performance on two
challenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achieves
state-of-the-art performance on all action categories in the Human3.6M dataset
using detected 2D poses from CPN, and our code is available at:
https://github.com/KHB1698/DC-GCT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05303">Multi-Visual-Inertial System: Analysis,Calibration and Estimation. (arXiv:2308.05303v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yulin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geneva_P/0/1/0/all/0/1">Patrick Geneva</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guoquan Huang</a></p>
<p>In this paper, we study state estimation of multi-visual-inertial systems
(MVIS) and develop sensor fusion algorithms to optimally fuse an arbitrary
number of asynchronous inertial measurement units (IMUs) or gyroscopes and
global and(or) rolling shutter cameras. We are especially interested in the
full calibration of the associated visual-inertial sensors, including the IMU
or camera intrinsics and the IMU-IMU(or camera) spatiotemporal extrinsics as
well as the image readout time of rolling-shutter cameras (if used). To this
end, we develop a new analytic combined IMU integration with intrinsics-termed
ACI3-to preintegrate IMU measurements, which is leveraged to fuse auxiliary
IMUs and(or) gyroscopes alongside a base IMU. We model the multi-inertial
measurements to include all the necessary inertial intrinsic and IMU-IMU
spatiotemporal extrinsic parameters, while leveraging IMU-IMU rigid-body
constraints to eliminate the necessity of auxiliary inertial poses and thus
reducing computational complexity. By performing observability analysis of
MVIS, we prove that the standard four unobservable directions remain - no
matter how many inertial sensors are used, and also identify, for the first
time, degenerate motions for IMU-IMU spatiotemporal extrinsics and auxiliary
inertial intrinsics. In addition to the extensive simulations that validate our
analysis and algorithms, we have built our own MVIS sensor rig and collected
over 25 real-world datasets to experimentally verify the proposed calibration
against the state-of-the-art calibration method such as Kalibr. We show that
the proposed MVIS calibration is able to achieve competing accuracy with
improved convergence and repeatability, which is open sourced to better benefit
the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05305">From CNN to Transformer: A Review of Medical Image Segmentation Models. (arXiv:2308.05305v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_W/0/1/0/all/0/1">Wenjian Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_J/0/1/0/all/0/1">Jiajun Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1">Wei Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yuheng Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1">Mengjuan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1">Yao Xie</a></p>
<p>Medical image segmentation is an important step in medical image analysis,
especially as a crucial prerequisite for efficient disease diagnosis and
treatment. The use of deep learning for image segmentation has become a
prevalent trend. The widely adopted approach currently is U-Net and its
variants. Additionally, with the remarkable success of pre-trained models in
natural language processing tasks, transformer-based models like TransUNet have
achieved desirable performance on multiple medical image segmentation datasets.
In this paper, we conduct a survey of the most representative four medical
image segmentation models in recent years. We theoretically analyze the
characteristics of these models and quantitatively evaluate their performance
on two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors).
Finally, we discuss the main challenges and future trends in medical image
segmentation. Our work can assist researchers in the related field to quickly
establish medical segmentation models tailored to specific regions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05311">DAOT: Domain-Agnostically Aligned Optimal Transport for Domain-Adaptive Crowd Counting. (arXiv:2308.05311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Huilin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jingling Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1">Xian Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shengfeng He</a></p>
<p>Domain adaptation is commonly employed in crowd counting to bridge the domain
gaps between different datasets. However, existing domain adaptation methods
tend to focus on inter-dataset differences while overlooking the
intra-differences within the same dataset, leading to additional learning
ambiguities. These domain-agnostic factors, e.g., density, surveillance
perspective, and scale, can cause significant in-domain variations, and the
misalignment of these factors across domains can lead to a drop in performance
in cross-domain crowd counting. To address this issue, we propose a
Domain-agnostically Aligned Optimal Transport (DAOT) strategy that aligns
domain-agnostic factors between domains. The DAOT consists of three steps.
First, individual-level differences in domain-agnostic factors are measured
using structural similarity (SSIM). Second, the optimal transfer (OT) strategy
is employed to smooth out these differences and find the optimal
domain-to-domain misalignment, with outlier individuals removed via a virtual
"dustbin" column. Third, knowledge is transferred based on the aligned
domain-agnostic factors, and the model is retrained for domain adaptation to
bridge the gap across domains. We conduct extensive experiments on five
standard crowd-counting benchmarks and demonstrate that the proposed method has
strong generalizability across diverse datasets. Our code will be available at:
https://github.com/HopooLinZ/DAOT/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05314">Deep Semantic Graph Matching for Large-scale Outdoor Point Clouds Registration. (arXiv:2308.05314v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shaocong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ruqin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1">Chenguang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongsheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanyun Wang</a></p>
<p>The current point cloud registration methods are mainly based on geometric
information and usually ignore the semantic information in the point clouds. In
this paper, we treat the point cloud registration problem as semantic instance
matching and registration task, and propose a deep semantic graph matching
method for large-scale outdoor point cloud registration. Firstly, the semantic
category labels of 3D point clouds are obtained by utilizing large-scale point
cloud semantic segmentation network. The adjacent points with the same category
labels are then clustered together by using Euclidean clustering algorithm to
obtain the semantic instances. Secondly, the semantic adjacency graph is
constructed based on the spatial adjacency relation of semantic instances.
Three kinds of high-dimensional features including geometric shape features,
semantic categorical features and spatial distribution features are learned
through graph convolutional network, and enhanced based on attention mechanism.
Thirdly, the semantic instance matching problem is modeled as an optimal
transport problem, and solved through an optimal matching layer. Finally,
according to the matched semantic instances, the geometric transformation
matrix between two point clouds is first obtained by SVD algorithm and then
refined by ICP algorithm. The experiments are cconducted on the KITTI Odometry
dataset, and the average relative translation error and average relative
rotation error of the proposed method are 6.6cm and 0.229{\deg} respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05318">RLSAC: Reinforcement Learning enhanced Sample Consensus for End-to-End Robust Estimation. (arXiv:2308.05318v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1">Chang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavalli_L/0/1/0/all/0/1">Luca Cavalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>Robust estimation is a crucial and still challenging task, which involves
estimating model parameters in noisy environments. Although conventional
sampling consensus-based algorithms sample several times to achieve robustness,
these algorithms cannot use data features and historical information
effectively. In this paper, we propose RLSAC, a novel Reinforcement Learning
enhanced SAmple Consensus framework for end-to-end robust estimation. RLSAC
employs a graph neural network to utilize both data and memory features to
guide exploring directions for sampling the next minimum set. The feedback of
downstream tasks serves as the reward for unsupervised training. Therefore,
RLSAC can avoid differentiating to learn the features and the feedback of
downstream tasks for end-to-end robust estimation. In addition, RLSAC
integrates a state transition module that encodes both data and memory
features. Our experimental results demonstrate that RLSAC can learn from
features to gradually explore a better hypothesis. Through analysis, it is
apparent that RLSAC can be easily transferred to other sampling consensus-based
robust estimation tasks. To the best of our knowledge, RLSAC is also the first
method that uses reinforcement learning to sample consensus for end-to-end
robust estimation. We release our codes at https://github.com/IRMVLab/RLSAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05320">Adv-Inpainting: Generating Natural and Transferable Adversarial Patch via Attention-guided Feature Fusion. (arXiv:2308.05320v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1">Mingxing Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1">Bin Xiao</a></p>
<p>The rudimentary adversarial attacks utilize additive noise to attack facial
recognition (FR) models. However, because manipulating the total face is
impractical in the physical setting, most real-world FR attacks are based on
adversarial patches, which limit perturbations to a small area. Previous
adversarial patch attacks often resulted in unnatural patterns and clear
boundaries that were easily noticeable. In this paper, we argue that generating
adversarial patches with plausible content can result in stronger
transferability than using additive noise or directly sampling from the latent
space. To generate natural-looking and highly transferable adversarial patches,
we propose an innovative two-stage coarse-to-fine attack framework called
Adv-Inpainting. In the first stage, we propose an attention-guided StyleGAN
(Att-StyleGAN) that adaptively combines texture and identity features based on
the attention map to generate high-transferable and natural adversarial
patches. In the second stage, we design a refinement network with a new
boundary variance loss to further improve the coherence between the patch and
its surrounding area. Experiment results demonstrate that Adv-Inpainting is
stealthy and can produce adversarial patches with stronger transferability and
improved visual quality than previous adversarial patch attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05344">Prostate Age Gap (PAG): An MRI surrogate marker of aging for prostate cancer detection. (arXiv:2308.05344v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Quilez_A/0/1/0/all/0/1">Alvaro Fernandez-Quilez</a>, <a href="http://arxiv.org/find/cs/1/au:+Nordstrom_T/0/1/0/all/0/1">Tobias Nordstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaderling_F/0/1/0/all/0/1">Fredrik J&#xe4;derling</a>, <a href="http://arxiv.org/find/cs/1/au:+Kjosavik_S/0/1/0/all/0/1">Svein Reidar Kjosavik</a>, <a href="http://arxiv.org/find/cs/1/au:+Eklund_M/0/1/0/all/0/1">Martin Eklund</a></p>
<p>Background: Prostate cancer (PC) MRI-based risk calculators are commonly
based on biological (e.g. PSA), MRI markers (e.g. volume), and patient age.
Whilst patient age measures the amount of years an individual has existed,
biological age (BA) might better reflect the physiology of an individual.
However, surrogates from prostate MRI and linkage with clinically significant
PC (csPC) remain to be explored. Purpose: To obtain and evaluate Prostate Age
Gap (PAG) as an MRI marker tool for csPC risk. Study type: Retrospective.
Population: A total of 7243 prostate MRI slices from 468 participants who had
undergone prostate biopsies. A deep learning model was trained on 3223 MRI
slices cropped around the gland from 81 low-grade PC (ncsPC, Gleason score &lt;=6)
and 131 negative cases and tested on the remaining 256 participants.
Assessment: Chronological age was defined as the age of the participant at the
time of the visit and used to train the deep learning model to predict the age
of the patient. Following, we obtained PAG, defined as the model predicted age
minus the patient's chronological age. Multivariate logistic regression models
were used to estimate the association through odds ratio (OR) and predictive
value of PAG and compared against PSA levels and PI-RADS&gt;=3. Statistical tests:
T-test, Mann-Whitney U test, Permutation test and ROC curve analysis. Results:
The multivariate adjusted model showed a significant difference in the odds of
clinically significant PC (csPC, Gleason score &gt;=7) (OR =3.78, 95% confidence
interval (CI):2.32-6.16, P &lt;.001). PAG showed a better predictive ability when
compared to PI-RADS&gt;=3 and adjusted by other risk factors, including PSA
levels: AUC =0.981 vs AUC =0.704, p&lt;.001. Conclusion: PAG was significantly
associated with the risk of clinically significant PC and outperformed other
well-established PC risk factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05346">Towards General and Fast Video Derain via Knowledge Distillation. (arXiv:2308.05346v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Defang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1">Pan Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1">Sixian Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhanpeng Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1">Cong Bai</a></p>
<p>As a common natural weather condition, rain can obscure video frames and thus
affect the performance of the visual system, so video derain receives a lot of
attention. In natural environments, rain has a wide variety of streak types,
which increases the difficulty of the rain removal task. In this paper, we
propose a Rain Review-based General video derain Network via knowledge
distillation (named RRGNet) that handles different rain streak types with one
pre-training weight. Specifically, we design a frame grouping-based
encoder-decoder network that makes full use of the temporal information of the
video. Further, we use the old task model to guide the current model in
learning new rain streak types while avoiding forgetting. To consolidate the
network's ability to derain, we design a rain review module to play back data
from old tasks for the current model. The experimental results show that our
developed general method achieves the best results in terms of running speed
and derain effect.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05355">TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction. (arXiv:2308.05355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinquan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jinheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuechen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuguang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Linlin Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yongqiang Deng</a></p>
<p>In implant prosthesis treatment, the surgical guide of implant is used to
ensure accurate implantation. However, such design heavily relies on the manual
location of the implant position. When deep neural network has been proposed to
assist the dentist in locating the implant position, most of them take a single
slice as input, which do not fully explore 3D contextual information and
ignoring the influence of implant slope. In this paper, we design a Text Guided
3D Context and Slope Aware Triple Network (TCSloT) which enables the perception
of contextual information from multiple adjacent slices and awareness of
variation of implant slopes. A Texture Variation Perception (TVP) module is
correspondingly elaborated to process the multiple slices and capture the
texture variation among slices and a Slope-Aware Loss (SAL) is proposed to
dynamically assign varying weights for the regression head. Additionally, we
design a conditional text guidance (CTG) module to integrate the text condition
(i.e., left, middle and right) from the CLIP for assisting the implant position
prediction. Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed TCSloT achieves superior
performance than existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05358">Fine-grained building roof instance segmentation based on domain adapted pretraining and composite dual-backbone. (arXiv:2308.05358v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guozhang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baochai Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mengke Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chaoran Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1">Ningning Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Simin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a></p>
<p>The diversity of building architecture styles of global cities situated on
various landforms, the degraded optical imagery affected by clouds and shadows,
and the significant inter-class imbalance of roof types pose challenges for
designing a robust and accurate building roof instance segmentor. To address
these issues, we propose an effective framework to fulfill semantic
interpretation of individual buildings with high-resolution optical satellite
imagery. Specifically, the leveraged domain adapted pretraining strategy and
composite dual-backbone greatly facilitates the discriminative feature
learning. Moreover, new data augmentation pipeline, stochastic weight averaging
(SWA) training and instance segmentation based model ensemble in testing are
utilized to acquire additional performance boost. Experiment results show that
our approach ranks in the first place of the 2023 IEEE GRSS Data Fusion Contest
(DFC) Track 1 test phase ($mAP_{50}$:50.6\%). Note-worthily, we have also
explored the potential of multimodal data fusion with both optical satellite
imagery and SAR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05359">Pseudo-label Alignment for Semi-supervised Instance Segmentation. (arXiv:2308.05359v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Liujuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_A/0/1/0/all/0/1">Annan Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1">Guannan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>Pseudo-labeling is significant for semi-supervised instance segmentation,
which generates instance masks and classes from unannotated images for
subsequent training. However, in existing pipelines, pseudo-labels that contain
valuable information may be directly filtered out due to mismatches in class
and mask quality. To address this issue, we propose a novel framework, called
pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we
devise a dynamic aligning loss (DALoss) that adjusts the weights of
semi-supervised loss terms with varying class and mask score pairs. Through
extensive experiments conducted on the COCO and Cityscapes datasets, we
demonstrate that PAIS is a promising framework for semi-supervised instance
segmentation, particularly in cases where labeled data is severely limited.
Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based on
Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the
current state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin of
over 12 points. Code is available at: \url{https://github.com/hujiecpp/PAIS}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05365">TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms. (arXiv:2308.05365v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1">Jiaqi Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Zeng_P/0/1/0/all/0/1">Pinxian Zeng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1">Xinyi Zeng</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xi Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jiliu Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1">Dinggang Shen</a></p>
<p>To obtain high-quality positron emission tomography (PET) images while
minimizing radiation exposure, various methods have been proposed for
reconstructing standard-dose PET (SPET) images from low-dose PET (LPET)
sinograms directly. However, current methods often neglect boundaries during
sinogram-to-image reconstruction, resulting in high-frequency distortion in the
frequency domain and diminished or fuzzy edges in the reconstructed images.
Furthermore, the convolutional architectures, which are commonly used, lack the
ability to model long-range non-local interactions, potentially leading to
inaccurate representations of global structures. To alleviate these problems,
we propose a transformer-based model that unites triple domains of sinogram,
image, and frequency for direct PET reconstruction, namely TriDo-Former.
Specifically, the TriDo-Former consists of two cascaded networks, i.e., a
sinogram enhancement transformer (SE-Former) for denoising the input LPET
sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for
reconstructing SPET images from the denoised sinograms. Different from the
vanilla transformer that splits an image into 2D patches, based specifically on
the PET imaging mechanism, our SE-Former divides the sinogram into 1D
projection view angles to maintain its inner-structure while denoising,
preventing the noise in the sinogram from prorogating into the image domain.
Moreover, to mitigate high-frequency distortion and improve reconstruction
details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP
serves as a learnable frequency filter that globally adjusts the frequency
components in the frequency domain, enforcing the network to restore
high-frequency details resembling real SPET images. Validations on a clinical
dataset demonstrate that our TriDo-Former outperforms the state-of-the-art
methods qualitatively and quantitatively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05371">Flexible Isosurface Extraction for Gradient-Based Mesh Optimization. (arXiv:2308.05371v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1">Tianchang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Munkberg_J/0/1/0/all/0/1">Jacob Munkberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasselgren_J/0/1/0/all/0/1">Jon Hasselgren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Kangxue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenzheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1">Zan Gojcic</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1">Sanja Fidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1">Nicholas Sharp</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jun Gao</a></p>
<p>This work considers gradient-based mesh optimization, where we iteratively
optimize for a 3D surface mesh by representing it as the isosurface of a scalar
field, an increasingly common paradigm in applications including
photogrammetry, generative modeling, and inverse physics. Existing
implementations adapt classic isosurface extraction algorithms like Marching
Cubes or Dual Contouring; these techniques were designed to extract meshes from
fixed, known fields, and in the optimization setting they lack the degrees of
freedom to represent high-quality feature-preserving meshes, or suffer from
numerical instabilities. We introduce FlexiCubes, an isosurface representation
specifically designed for optimizing an unknown mesh with respect to geometric,
visual, or even physical objectives. Our main insight is to introduce
additional carefully-chosen parameters into the representation, which allow
local flexible adjustments to the extracted mesh geometry and connectivity.
These parameters are updated along with the underlying scalar field via
automatic differentiation when optimizing for a downstream task. We base our
extraction scheme on Dual Marching Cubes for improved topological properties,
and present extensions to optionally generate tetrahedral and
hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on
both synthetic benchmarks and real-world applications, showing that it offers
significant improvements in mesh quality and geometric fidelity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05382">Interaction-aware Joint Attention Estimation Using People Attributes. (arXiv:2308.05382v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakatani_C/0/1/0/all/0/1">Chihiro Nakatani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawashima_H/0/1/0/all/0/1">Hiroaki Kawashima</a>, <a href="http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1">Norimichi Ukita</a></p>
<p>This paper proposes joint attention estimation in a single image. Different
from related work in which only the gaze-related attributes of people are
independently employed, (I) their locations and actions are also employed as
contextual cues for weighting their attributes, and (ii) interactions among all
of these attributes are explicitly modeled in our method. For the interaction
modeling, we propose a novel Transformer-based attention network to encode
joint attention as low-dimensional features. We introduce a specialized MLP
head with positional embedding to the Transformer so that it predicts pixelwise
confidence of joint attention for generating the confidence heatmap. This
pixelwise prediction improves the heatmap accuracy by avoiding the ill-posed
problem in which the high-dimensional heatmap is predicted from the
low-dimensional features. The estimated joint attention is further improved by
being integrated with general image-based attention estimation. Our method
outperforms SOTA methods quantitatively in comparative experiments. Code:
https://anonymous.4open.science/r/anonymized_codes-ECA4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05387">HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View Building Extraction and Height Estimation. (arXiv:2308.05387v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chaoran Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1">Ningning Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baochai Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guozhang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mengke Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Simin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a></p>
<p>Unifying the correlative single-view satellite image building extraction and
height estimation tasks indicates a promising way to share representations and
acquire generalist model for large-scale urban 3D reconstruction. However, the
common spatial misalignment between building footprints and
stereo-reconstructed nDSM height labels incurs degraded performance on both
tasks. To address this issue, we propose a Height-hierarchy Guided Dual-decoder
Network (HGDNet) to estimate building height. Under the guidance of synthesized
discrete height-hierarchy nDSM, auxiliary height-hierarchical building
extraction branch enhance the height estimation branch with implicit
constraints, yielding an accuracy improvement of more than 6% on the DFC 2023
track2 dataset. Additional two-stage cascade architecture is adopted to achieve
more accurate building extraction. Experiments on the DFC 2023 Track 2 dataset
shows the superiority of the proposed method in building height estimation
({\delta}1:0.8012), instance extraction (AP50:0.7730), and the final average
score 0.7871 ranks in the first place in test phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05390">Product Review Image Ranking for Fashion E-commerce. (arXiv:2308.05390v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1">Sangeet Jaiswal</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1">Dhruv Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Vempati_S/0/1/0/all/0/1">Sreekanth Vempati</a>, <a href="http://arxiv.org/find/cs/1/au:+Saiswaroop_K/0/1/0/all/0/1">Konduru Saiswaroop</a></p>
<p>In a fashion e-commerce platform where customers can't physically examine the
products on their own, being able to see other customers' text and image
reviews of the product is critical while making purchase decisions. Given the
high reliance on these reviews, over the years we have observed customers
proactively sharing their reviews. With an increase in the coverage of User
Generated Content (UGC), there has been a corresponding increase in the number
of customer images. It is thus imperative to display the most relevant images
on top as it may influence users' online shopping choices and behavior. In this
paper, we propose a simple yet effective training procedure for ranking
customer images. We created a dataset consisting of Myntra (A Major Indian
Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes)
UGC images as our starting point and used selected distortion techniques on the
images of the above dataset to bring their quality at par with those of bad UGC
images. We train our network to rank bad-quality images lower than high-quality
ones. Our proposed method outperforms the baseline models on two metrics,
namely correlation coefficient, and accuracy, by substantial margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05394">Robust Localization with Visual-Inertial Odometry Constraints for Markerless Mobile AR. (arXiv:2308.05394v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changkun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yukun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Braud_T/0/1/0/all/0/1">Tristan Braud</a></p>
<p>Visual Inertial Odometry (VIO) is an essential component of modern Augmented
Reality (AR) applications. However, VIO only tracks the relative pose of the
device, leading to drift over time. Absolute pose estimation methods infer the
device's absolute pose, but their accuracy depends on the input quality. This
paper introduces VIO-APR, a new framework for markerless mobile AR that
combines an absolute pose regressor (APR) with a local VIO tracking system.
VIO-APR uses VIO to assess the reliability of the APR and the APR to identify
and compensate for VIO drift. This feedback loop results in more accurate
positioning and more stable AR experiences. To evaluate VIO-APR, we created a
dataset that combines camera images with ARKit's VIO system output for six
indoor and outdoor scenes of various scales. Over this dataset, VIO-APR
improves the median accuracy of popular APR by up to 36\% in position and 29\%
in orientation, increases the percentage of frames in the high ($0.25 m,
2^{\circ}$) accuracy level by up to 112\% and reduces the percentage of frames
predicted below the low ($5 m, 10^\circ$) accuracy greatly. We implement
VIO-APR into a mobile AR application using Unity to demonstrate its
capabilities. VIO-APR results in noticeably more accurate localization and a
more stable overall experience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05396">Learning Gabor Texture Features for Fine-Grained Recognition. (arXiv:2308.05396v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lanyun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianrun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jianxiong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1">Simon See</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>Extracting and using class-discriminative features is critical for
fine-grained recognition. Existing works have demonstrated the possibility of
applying deep CNNs to exploit features that distinguish similar classes.
However, CNNs suffer from problems including frequency bias and loss of
detailed local information, which restricts the performance of recognizing
fine-grained categories. To address the challenge, we propose a novel texture
branch as complimentary to the CNN branch for feature extraction. We
innovatively utilize Gabor filters as a powerful extractor to exploit texture
features, motivated by the capability of Gabor filters in effectively capturing
multi-frequency features and detailed local information. We implement several
designs to enhance the effectiveness of Gabor filters, including imposing
constraints on parameter values and developing a learning method to determine
the optimal parameters. Moreover, we introduce a statistical feature extractor
to utilize informative statistical information from the signals captured by
Gabor filters, and a gate selection mechanism to enable efficient computation
by only considering qualified regions as input for texture extraction. Through
the integration of features from the Gabor-filter-based texture branch and
CNN-based semantic branch, we achieve comprehensive information extraction. We
demonstrate the efficacy of our method on multiple datasets, including
CUB-200-2011, NA-bird, Stanford Dogs, and GTOS-mobile. State-of-the-art
performance is achieved using our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05404">Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network. (arXiv:2308.05404v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xianqiang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a></p>
<p>This paper presents a novel and interpretable end-to-end learning framework,
called the deep compensation unfolding network (DCUNet), for restoring light
field (LF) images captured under low-light conditions. DCUNet is designed with
a multi-stage architecture that mimics the optimization process of solving an
inverse imaging problem in a data-driven fashion. The framework uses the
intermediate enhanced result to estimate the illumination map, which is then
employed in the unfolding process to produce a new enhanced result.
Additionally, DCUNet includes a content-associated deep compensation module at
each optimization stage to suppress noise and illumination map estimation
errors. To properly mine and leverage the unique characteristics of LF images,
this paper proposes a pseudo-explicit feature interaction module that
comprehensively exploits redundant information in LF images. The experimental
results on both simulated and real datasets demonstrate the superiority of our
DCUNet over state-of-the-art methods, both qualitatively and quantitatively.
Moreover, DCUNet preserves the essential geometric structure of enhanced LF
images much better. The code will be publicly available at
https://github.com/lyuxianqiang/LFLL-DCU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05407">A Comparative Assessment of Multi-view fusion learning for Crop Classification. (arXiv:2308.05407v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mena_F/0/1/0/all/0/1">Francisco Mena</a>, <a href="http://arxiv.org/find/cs/1/au:+Arenas_D/0/1/0/all/0/1">Diego Arenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Nuske_M/0/1/0/all/0/1">Marlon Nuske</a>, <a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1">Andreas Dengel</a></p>
<p>With a rapidly increasing amount and diversity of remote sensing (RS) data
sources, there is a strong need for multi-view learning modeling. This is a
complex task when considering the differences in resolution, magnitude, and
noise of RS data. The typical approach for merging multiple RS sources has been
input-level fusion, but other - more advanced - fusion strategies may
outperform this traditional approach. This work assesses different fusion
strategies for crop classification in the CropHarvest dataset. The fusion
methods proposed in this work outperform models based on individual views and
previous fusion methods. We do not find one single fusion method that
consistently outperforms all other approaches. Instead, we present a comparison
of multi-view fusion methods for three different datasets and show that,
depending on the test region, different methods obtain the best performance.
Despite this, we suggest a preliminary criterion for the selection of fusion
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05410">SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data. (arXiv:2308.05410v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zohaib_M/0/1/0/all/0/1">Mohammad Zohaib</a>, <a href="http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1">Alessio Del Bue</a></p>
<p>This paper proposes a new method to infer keypoints from arbitrary object
categories in practical scenarios where point cloud data (PCD) are noisy,
down-sampled and arbitrarily rotated. Our proposed model adheres to the
following principles: i) keypoints inference is fully unsupervised (no
annotation given), ii) keypoints position error should be low and resilient to
PCD perturbations (robustness), iii) keypoints should not change their indexes
for the intra-class objects (semantic coherence), iv) keypoints should be close
to or proximal to PCD surface (compactness). We achieve these desiderata by
proposing a new self-supervised training strategy for keypoints estimation that
does not assume any a priori knowledge of the object class, and a model
architecture with coupled auxiliary losses that promotes the desired keypoints
properties. We compare the keypoints estimated by the proposed approach with
those of the state-of-the-art unsupervised approaches. The experiments show
that our approach outperforms by estimating keypoints with improved coverage
(+9.41%) while being semantically consistent (+4.66%) that best characterizes
the object's 3D shape for downstream tasks. Code and data are available at:
https://github.com/IITPAVIS/SC3K
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05421">Progressive Spatio-temporal Perception for Audio-Visual Question Answering. (arXiv:2308.05421v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guangyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wenxuan Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Di Hu</a></p>
<p>Audio-Visual Question Answering (AVQA) task aims to answer questions about
different visual objects, sounds, and their associations in videos. Such
naturally multi-modal videos are composed of rich and complex dynamic
audio-visual components, where most of which could be unrelated to the given
questions, or even play as interference in answering the content of interest.
Oppositely, only focusing on the question-aware audio-visual content could get
rid of influence, meanwhile enabling the model to answer more efficiently. In
this paper, we propose a Progressive Spatio-Temporal Perception Network
(PSTP-Net), which contains three modules that progressively identify key
spatio-temporal regions w.r.t. questions. Specifically, a temporal segment
selection module is first introduced to select the most relevant audio-visual
segments related to the given question. Then, a spatial region selection module
is utilized to choose the most relevant regions associated with the question
from the selected temporal segments. To further refine the selection of
features, an audio-guided visual attention module is employed to perceive the
association between auido and selected spatial regions. Finally, the
spatio-temporal features from these modules are integrated for answering the
question. Extensive experimental results on the public MUSIC-AVQA and AVQA
datasets provide compelling evidence of the effectiveness and efficiency of
PSTP-Net. Code is available at:
\href{https://github.com/GeWu-Lab/PSTP-Net}{https://github.com/GeWu-Lab/PSTP-Net}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05426">Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection. (arXiv:2308.05426v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1">Ruikai Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Siyuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Shi Qiu</a></p>
<p>Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and
Google's PaLM2, have revolutionized the field of artificial intelligence. A
notable paradigm shift has been the advent of the Segment Anything Model (SAM),
which has exhibited a remarkable capability to segment real-world objects,
trained on 1 billion masks and 11 million images. Although SAM excels in
general object segmentation, it lacks the intrinsic ability to detect salient
objects, resulting in suboptimal performance in this domain. To address this
challenge, we present the Segment Salient Object Model (SSOM), an innovative
approach that adaptively fine-tunes SAM for salient object detection by
harnessing the low-rank structure inherent in deep learning. Comprehensive
qualitative and quantitative evaluations across five challenging RGB benchmark
datasets demonstrate the superior performance of our approach, surpassing
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05428">Speech-Driven 3D Face Animation with Composite and Regional Facial Movements. (arXiv:2308.05428v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haozhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Songtao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jia Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1">Junliang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xiang Wen</a></p>
<p>Speech-driven 3D face animation poses significant challenges due to the
intricacy and variability inherent in human facial movements. This paper
emphasizes the importance of considering both the composite and regional
natures of facial movements in speech-driven 3D face animation. The composite
nature pertains to how speech-independent factors globally modulate
speech-driven facial movements along the temporal dimension. Meanwhile, the
regional nature alludes to the notion that facial movements are not globally
correlated but are actuated by local musculature along the spatial dimension.
It is thus indispensable to incorporate both natures for engendering vivid
animation. To address the composite nature, we introduce an adaptive modulation
module that employs arbitrary facial movements to dynamically adjust
speech-driven facial movements across frames on a global scale. To accommodate
the regional nature, our approach ensures that each constituent of the facial
features for every frame focuses on the local spatial movements of 3D faces.
Moreover, we present a non-autoregressive backbone for translating audio to 3D
facial movements, which maintains high-frequency nuances of facial movements
and facilitates efficient inference. Comprehensive experiments and user studies
demonstrate that our method surpasses contemporary state-of-the-art approaches
both qualitatively and quantitatively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05430">Ensemble Modeling for Multimodal Visual Action Recognition. (arXiv:2308.05430v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1">Jyoti Kini</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleischer_S/0/1/0/all/0/1">Sarah Fleischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_I/0/1/0/all/0/1">Ishan Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a></p>
<p>In this work, we propose an ensemble modeling approach for multimodal action
recognition. We independently train individual modality models using a variant
of focal loss tailored to handle the long-tailed distribution of the MECCANO
[21] dataset. Based on the underlying principle of focal loss, which captures
the relationship between tail (scarce) classes and their prediction
difficulties, we propose an exponentially decaying variant of focal loss for
our current task. It initially emphasizes learning from the hard misclassified
examples and gradually adapts to the entire range of examples in the dataset.
This annealing process encourages the model to strike a balance between
focusing on the sparse set of hard samples, while still leveraging the
information provided by the easier ones. Additionally, we opt for the late
fusion strategy to combine the resultant probability distributions from RGB and
Depth modalities for final action prediction. Experimental evaluations on the
MECCANO dataset demonstrate the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05438">Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation. (arXiv:2308.05438v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Linlin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1">Qi Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jing Qin</a></p>
<p>One critical challenge in 6D object pose estimation from a single RGBD image
is efficient integration of two different modalities, i.e., color and depth. In
this work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)
block that can aggregate cross-modality features for improving pose estimation.
Unlike existing fusion methods, the proposed DFTr can better model
cross-modality semantic correlation by leveraging their semantic similarity,
such that globally enhanced features from different modalities can be better
integrated for improved information extraction. Moreover, to further improve
robustness and efficiency, we introduce a novel weighted vector-wise voting
algorithm that employs a non-iterative global optimization strategy for precise
3D keypoint localization while achieving near real-time inference. Extensive
experiments show the effectiveness and strong generalization capability of our
proposed 3D keypoint voting algorithm. Results on four widely used benchmarks
also demonstrate that our method outperforms the state-of-the-art methods by
large margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05441">Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation. (arXiv:2308.05441v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1">Hao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1">Pietro Perona</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1">Guha Balakrishnan</a></p>
<p>We propose an experimental method for measuring bias in face recognition
systems. Existing methods to measure bias depend on benchmark datasets that are
collected in the wild and annotated for protected (e.g., race, gender) and
non-protected (e.g., pose, lighting) attributes. Such observational datasets
only permit correlational conclusions, e.g., "Algorithm A's accuracy is
different on female and male faces in dataset X.". By contrast, experimental
methods manipulate attributes individually and thus permit causal conclusions,
e.g., "Algorithm A's accuracy is affected by gender and skin color."
</p>
<p>Our method is based on generating synthetic faces using a neural face
generator, where each attribute of interest is modified independently while
leaving all other attributes constant. Human observers crucially provide the
ground truth on perceptual identity similarity between synthetic image pairs.
We validate our method quantitatively by evaluating race and gender biases of
three research-grade face recognition models. Our synthetic pipeline reveals
that for these algorithms, accuracy is lower for Black and East Asian
population subgroups. Our method can also quantify how perceptual changes in
attributes affect face identity distances reported by these models. Our large
synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200
unique synthetic faces) and 555,000 human annotations (individual attributes
and pairwise identity comparisons) is available to researchers in this
important area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05447">A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement. (arXiv:2308.05447v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1">Pan Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanning Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zheyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1">Sixian Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1">Cong Bai</a></p>
<p>Underwater images often suffer from color distortion and low contrast
resulting in various image types, due to the scattering and absorption of light
by water. While it is difficult to obtain high-quality paired training samples
with a generalized model. To tackle these challenges, we design a Generalized
Underwater image enhancement method via a Physical-knowledge-guided Dynamic
Model (short for GUPDM), consisting of three parts: Atmosphere-based Dynamic
Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based
Multi-scale Structure (PMS). In particular, to cover complex underwater scenes,
this study changes the global atmosphere light and the transmission to simulate
various underwater image types (e.g., the underwater image color ranging from
yellow to blue) through the formation model. We then design ADS and TDS that
use dynamic convolutions to adaptively extract prior information from
underwater images and generate parameters for PMS. These two modules enable the
network to select appropriate parameters for various water types adaptively.
Besides, the multi-scale feature extraction module in PMS uses convolution
blocks with different kernel sizes and obtains weights for each feature map via
channel attention block and fuses them to boost the receptive field of the
network. The source code will be available at
\href{https://github.com/shiningZZ/GUPDM}{https://github.com/shiningZZ/GUPDM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05449">Transforming Breast Cancer Diagnosis: Towards Real-Time Ultrasound to Mammogram Conversion for Cost-Effective Diagnosis. (arXiv:2308.05449v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Nasser_S/0/1/0/all/0/1">Sahar Almahfouz Nasser</a>, <a href="http://arxiv.org/find/eess/1/au:+Sharma_A/0/1/0/all/0/1">Ashutosh Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Saraf_A/0/1/0/all/0/1">Anmol Saraf</a>, <a href="http://arxiv.org/find/eess/1/au:+Parulekar_A/0/1/0/all/0/1">Amruta Mahendra Parulekar</a>, <a href="http://arxiv.org/find/eess/1/au:+Haria_P/0/1/0/all/0/1">Purvi Haria</a>, <a href="http://arxiv.org/find/eess/1/au:+Sethi_A/0/1/0/all/0/1">Amit Sethi</a></p>
<p>Ultrasound (US) imaging is better suited for intraoperative settings because
it is real-time and more portable than other imaging techniques, such as
mammography. However, US images are characterized by lower spatial resolution
noise-like artifacts. This research aims to address these limitations by
providing surgeons with mammogram-like image quality in real-time from noisy US
images. Unlike previous approaches for improving US image quality that aim to
reduce artifacts by treating them as (speckle noise), we recognize their value
as informative wave interference pattern (WIP). To achieve this, we utilize the
Stride software to numerically solve the forward model, generating ultrasound
images from mammograms images by solving wave-equations. Additionally, we
leverage the power of domain adaptation to enhance the realism of the simulated
ultrasound images. Then, we utilize generative adversarial networks (GANs) to
tackle the inverse problem of generating mammogram-quality images from
ultrasound images. The resultant images have considerably more discernible
details than the original US images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05459">KS-APR: Keyframe Selection for Robust Absolute Pose Regression. (arXiv:2308.05459v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changkun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yukun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Braud_T/0/1/0/all/0/1">Tristan Braud</a></p>
<p>Markerless Mobile Augmented Reality (AR) aims to anchor digital content in
the physical world without using specific 2D or 3D objects. Absolute Pose
Regressors (APR) are end-to-end machine learning solutions that infer the
device's pose from a single monocular image. Thanks to their low computation
cost, they can be directly executed on the constrained hardware of mobile AR
devices. However, APR methods tend to yield significant inaccuracies for input
images that are too distant from the training set. This paper introduces
KS-APR, a pipeline that assesses the reliability of an estimated pose with
minimal overhead by combining the inference results of the APR and the prior
images in the training set. Mobile AR systems tend to rely upon visual-inertial
odometry to track the relative pose of the device during the experience. As
such, KS-APR favours reliability over frequency, discarding unreliable poses.
This pipeline can integrate most existing APR methods to improve accuracy by
filtering unreliable images with their pose estimates. We implement the
pipeline on three types of APR models on indoor and outdoor datasets. The
median error on position and orientation is reduced for all models, and the
proportion of large errors is minimized across datasets. Our method enables
state-of-the-art APRs such as DFNetdm to outperform single-image and sequential
APR methods. These results demonstrate the scalability and effectiveness of
KS-APR for visual localization tasks that do not require one-shot decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05474">Surface Masked AutoEncoder: Self-Supervision for Cortical Imaging Data. (arXiv:2308.05474v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dahan_S/0/1/0/all/0/1">Simon Dahan</a>, <a href="http://arxiv.org/find/eess/1/au:+Silva_M/0/1/0/all/0/1">Mariana da Silva</a>, <a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/eess/1/au:+Robinson_E/0/1/0/all/0/1">Emma C Robinson</a></p>
<p>Self-supervision has been widely explored as a means of addressing the lack
of inductive biases in vision transformer architectures, which limits
generalisation when networks are trained on small datasets. This is crucial in
the context of cortical imaging, where phenotypes are complex and
heterogeneous, but the available datasets are limited in size. This paper
builds upon recent advancements in translating vision transformers to surface
meshes and investigates the potential of Masked AutoEncoder (MAE)
self-supervision for cortical surface learning. By reconstructing surface data
from a masked version of the input, the proposed method effectively models
cortical structure to learn strong representations that translate to improved
performance in downstream tasks. We evaluate our approach on cortical phenotype
regression using the developing Human Connectome Project (dHCP) and demonstrate
that pre-training leads to a 26\% improvement in performance, with an 80\%
faster convergence, compared to models trained from scratch. Furthermore, we
establish that pre-training vision transformer models on large datasets, such
as the UK Biobank (UKB), enables the acquisition of robust representations for
finetuning in low-data scenarios. Our code and pre-trained models are publicly
available at \url{https://github.com/metrics-lab/surface-vision-transformers}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05478">Reviewing 3D Object Detectors in the Context of High-Resolution 3+1D Radar. (arXiv:2308.05478v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Palmer_P/0/1/0/all/0/1">Patrick Palmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_M/0/1/0/all/0/1">Martin Krueger</a>, <a href="http://arxiv.org/find/cs/1/au:+Altendorfer_R/0/1/0/all/0/1">Richard Altendorfer</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_G/0/1/0/all/0/1">Ganesh Adam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1">Torsten Bertram</a></p>
<p>Recent developments and the beginning market introduction of high-resolution
imaging 4D (3+1D) radar sensors have initialized deep learning-based radar
perception research. We investigate deep learning-based models operating on
radar point clouds for 3D object detection. 3D object detection on lidar point
cloud data is a mature area of 3D vision. Many different architectures have
been proposed, each with strengths and weaknesses. Due to similarities between
3D lidar point clouds and 3+1D radar point clouds, those existing 3D object
detectors are a natural basis to start deep learning-based 3D object detection
on radar data. Thus, the first step is to analyze the detection performance of
the existing models on the new data modality and evaluate them in depth. In
order to apply existing 3D point cloud object detectors developed for lidar
point clouds to the radar domain, they need to be adapted first. While some
detectors, such as PointPillars, have already been adapted to be applicable to
radar data, we have adapted others, e.g., Voxel R-CNN, SECOND, PointRCNN, and
PV-RCNN. To this end, we conduct a cross-model validation (evaluating a set of
models on one particular data set) as well as a cross-data set validation
(evaluating all models in the model set on several data sets). The
high-resolution radar data used are the View-of-Delft and Astyx data sets.
Finally, we evaluate several adaptations of the models and their training
procedures. We also discuss major factors influencing the detection performance
on radar data and propose possible solutions indicating potential future
research avenues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05480">YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection. (arXiv:2308.05480v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xinbin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruiqi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiabao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qibin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a></p>
<p>We aim at providing the object detection community with an efficient and
performant object detector, termed YOLO-MS. The core design is based on a
series of investigations on how convolutions with different kernel sizes affect
the detection performance of objects at different scales. The outcome is a new
strategy that can strongly enhance multi-scale feature representations of
real-time object detectors. To verify the effectiveness of our strategy, we
build a network architecture, termed YOLO-MS. We train our YOLO-MS on the MS
COCO dataset from scratch without relying on any other large-scale datasets,
like ImageNet, or pre-trained weights. Without bells and whistles, our YOLO-MS
outperforms the recent state-of-the-art real-time object detectors, including
YOLO-v7 and RTMDet, when using a comparable number of parameters and FLOPs.
Taking the XS version of YOLO-MS as an example, with only 4.5M learnable
parameters and 8.7G FLOPs, it can achieve an AP score of 43%+ on MS COCO, which
is about 2%+ higher than RTMDet with the same model size. Moreover, our work
can also be used as a plug-and-play module for other YOLO models. Typically,
our method significantly improves the AP of YOLOv8 from 37%+ to 40%+ with even
fewer parameters and FLOPs. Code is available at
https://github.com/FishAndWasabi/YOLO-MS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05493">Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation. (arXiv:2308.05493v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1">Tianbo Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yunhao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lin Wang</a></p>
<p>Endeavors have been recently made to transfer knowledge from the labeled
pinhole image domain to the unlabeled panoramic image domain via Unsupervised
Domain Adaptation (UDA). The aim is to tackle the domain gaps caused by the
style disparities and distortion problem from the non-uniformly distributed
pixels of equirectangular projection (ERP). Previous works typically focus on
transferring knowledge based on geometric priors with specially designed
multi-branch network architectures. As a result, considerable computational
costs are induced, and meanwhile, their generalization abilities are profoundly
hindered by the variation of distortion among pixels. In this paper, we find
that the pixels' neighborhood regions of the ERP indeed introduce less
distortion. Intuitively, we propose a novel UDA framework that can effectively
address the distortion problems for panoramic semantic segmentation. In
comparison, our method is simpler, easier to implement, and more
computationally efficient. Specifically, we propose distortion-aware attention
(DA) capturing the neighboring pixel distribution without using any geometric
constraints. Moreover, we propose a class-wise feature aggregation (CFA) module
to iteratively update the feature representations with a memory bank. As such,
the feature similarity between two domains can be consistently optimized.
Extensive experiments show that our method achieves new state-of-the-art
performance while remarkably reducing 80% parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05525">Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1">Meir Yossef Levi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1">Guy Gilboa</a></p>
<p>The ability to cope accurately and fast with Out-Of-Distribution (OOD)
samples is crucial in real-world safety demanding applications. In this work we
first study the interplay between critical points of 3D point clouds and OOD
samples. Our findings are that common corruptions and outliers are often
interpreted as critical points. We generalize the notion of critical points
into importance measures. We show that training a classification network based
only on less important points dramatically improves robustness, at a cost of
minor performance loss on the clean set. We observe that normalized entropy is
highly informative for corruption analysis. An adaptive threshold based on
normalized entropy is suggested for selecting the set of uncritical points. Our
proposed importance measure is extremely fast to compute. We show it can be
used for a variety of applications, such as Explainable AI (XAI), Outlier
Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense.
We reach SOTA results on the two latter tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05533">Is there progress in activity progress prediction?. (arXiv:2308.05533v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boer_F/0/1/0/all/0/1">Frans de Boer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1">Jan C. van Gemert</a>, <a href="http://arxiv.org/find/cs/1/au:+Dijkstra_J/0/1/0/all/0/1">Jouke Dijkstra</a>, <a href="http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1">Silvia L. Pintea</a></p>
<p>Activity progress prediction aims to estimate what percentage of an activity
has been completed. Currently this is done with machine learning approaches,
trained and evaluated on complicated and realistic video datasets. The videos
in these datasets vary drastically in length and appearance. And some of the
activities have unanticipated developments, making activity progression
difficult to estimate. In this work, we examine the results obtained by
existing progress prediction methods on these datasets. We find that current
progress prediction methods seem not to extract useful visual information for
the progress prediction task. Therefore, these methods fail to exceed simple
frame-counting baselines. We design a precisely controlled dataset for activity
progress prediction and on this synthetic dataset we show that the considered
methods can make use of the visual information, when this directly relates to
the progress prediction. We conclude that the progress prediction task is
ill-posed on the currently used real-world datasets. Moreover, to fairly
measure activity progression we advise to consider a, simple but effective,
frame-counting baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05542">Robust Asymmetric Loss for Multi-Label Long-Tailed Learning. (arXiv:2308.05542v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1">Wongi Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1">Inhyuk Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1">Jongbin Ryu</a></p>
<p>In real medical data, training samples typically show long-tailed
distributions with multiple labels. Class distribution of the medical data has
a long-tailed shape, in which the incidence of different diseases is quite
varied, and at the same time, it is not unusual for images taken from
symptomatic patients to be multi-label diseases. Therefore, in this paper, we
concurrently address these two issues by putting forth a robust asymmetric loss
on the polynomial function. Since our loss tackles both long-tailed and
multi-label classification problems simultaneously, it leads to a complex
design of the loss function with a large number of hyper-parameters. Although a
model can be highly fine-tuned due to a large number of hyper-parameters, it is
difficult to optimize all hyper-parameters at the same time, and there might be
a risk of overfitting a model. Therefore, we regularize the loss function using
the Hill loss approach, which is beneficial to be less sensitive against the
numerous hyper-parameters so that it reduces the risk of overfitting the model.
For this reason, the proposed loss is a generic method that can be applied to
most medical image classification tasks and does not make the training process
more time-consuming. We demonstrate that the proposed robust asymmetric loss
performs favorably against the long-tailed with multi-label medical image
classification in addition to the various long-tailed single-label datasets.
Notably, our method achieves Top-5 results on the CXR-LT dataset of the ICCV
CVAMD 2023 competition. We opensource our implementation of the robust
asymmetric loss in the public repository: https://github.com/kalelpark/RAL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05543">Deep Richardson-Lucy Deconvolution for Low-Light Image Deblurring. (arXiv:2308.05543v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenhua Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunxuan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1">Faming Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jimmy Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jinshan Pan</a></p>
<p>Images taken under the low-light condition often contain blur and saturated
pixels at the same time. Deblurring images with saturated pixels is quite
challenging. Because of the limited dynamic range, the saturated pixels are
usually clipped in the imaging process and thus cannot be modeled by the linear
blur model. Previous methods use manually designed smooth functions to
approximate the clipping procedure. Their deblurring processes often require
empirically defined parameters, which may not be the optimal choices for
different images. In this paper, we develop a data-driven approach to model the
saturated pixels by a learned latent map. Based on the new model, the non-blind
deblurring task can be formulated into a maximum a posterior (MAP) problem,
which can be effectively solved by iteratively computing the latent map and the
latent image. Specifically, the latent map is computed by learning from a map
estimation network (MEN), and the latent image estimation process is
implemented by a Richardson-Lucy (RL)-based updating scheme. To estimate
high-quality deblurred images without amplified artifacts, we develop a prior
estimation network (PEN) to obtain prior information, which is further
integrated into the RL scheme. Experimental results demonstrate that the
proposed method performs favorably against state-of-the-art algorithms both
quantitatively and qualitatively on synthetic and real-world images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05550">Cross-Domain Product Representation Learning for Rich-Content E-Commerce. (arXiv:2308.05550v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xuehan Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yanhua Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenjie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Quan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Han Li</a></p>
<p>The proliferation of short video and live-streaming platforms has
revolutionized how consumers engage in online shopping. Instead of browsing
product pages, consumers are now turning to rich-content e-commerce, where they
can purchase products through dynamic and interactive media like short videos
and live streams. This emerging form of online shopping has introduced
technical challenges, as products may be presented differently across various
media domains. Therefore, a unified product representation is essential for
achieving cross-domain product recognition to ensure an optimal user search
experience and effective product recommendations. Despite the urgent industrial
need for a unified cross-domain product representation, previous studies have
predominantly focused only on product pages without taking into account short
videos and live streams. To fill the gap in the rich-content e-commerce area,
in this paper, we introduce a large-scale cRoss-dOmain Product Ecognition
dataset, called ROPE. ROPE covers a wide range of product categories and
contains over 180,000 products, corresponding to millions of short videos and
live streams. It is the first dataset to cover product pages, short videos, and
live streams simultaneously, providing the basis for establishing a unified
product representation across different media domains. Furthermore, we propose
a Cross-dOmain Product rEpresentation framework, namely COPE, which unifies
product representations in different domains through multimodal learning
including text and vision. Extensive experiments on downstream tasks
demonstrate the effectiveness of COPE in learning a joint feature space for all
product domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05574">Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation of Dravidian Languages. (arXiv:2308.05574v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebadulla_D/0/1/0/all/0/1">Danish Ebadulla</a>, <a href="http://arxiv.org/find/cs/1/au:+Raman_R/0/1/0/all/0/1">Rahul Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1">S. Natarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_H/0/1/0/all/0/1">Hridhay Kiran Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1">Ashish Harish Shenoy</a></p>
<p>Current research in zero-shot translation is plagued by several issues such
as high compute requirements, increased training time and off target
translations. Proposed remedies often come at the cost of additional data or
compute requirements. Pivot based neural machine translation is preferred over
a single-encoder model for most settings despite the increased training and
evaluation time. In this work, we overcome the shortcomings of zero-shot
translation by taking advantage of transliteration and linguistic similarity.
We build a single encoder-decoder neural machine translation system for
Dravidian-Dravidian multilingual translation and perform zero-shot translation.
We compare the data vs zero-shot accuracy tradeoff and evaluate the performance
of our vanilla method against the current state of the art pivot based method.
We also test the theory that morphologically rich languages require large
vocabularies by restricting the vocabulary using an optimal transport based
technique. Our model manages to achieves scores within 3 BLEU of large-scale
pivot-based models when it is trained on 50\% of the language directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05581">Category Feature Transformer for Semantic Segmentation. (arXiv:2308.05581v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Quan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuanjian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fagui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yifan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bowen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a></p>
<p>Aggregation of multi-stage features has been revealed to play a significant
role in semantic segmentation. Unlike previous methods employing point-wise
summation or concatenation for feature aggregation, this study proposes the
Category Feature Transformer (CFT) that explores the flow of category embedding
and transformation among multi-stage features through the prevalent multi-head
attention mechanism. CFT learns unified feature embeddings for individual
semantic categories from high-level features during each aggregation process
and dynamically broadcasts them to high-resolution features. Integrating the
proposed CFT into a typical feature pyramid structure exhibits superior
performance over a broad range of backbone networks. We conduct extensive
experiments on popular semantic segmentation benchmarks. Specifically, the
proposed CFT obtains a compelling 55.1% mIoU with greatly reduced model
parameters and computations on the challenging ADE20K dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05595">Test-Time Selection for Robust Skin Lesion Analysis. (arXiv:2308.05595v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1">Alceu Bissoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Barata_C/0/1/0/all/0/1">Catarina Barata</a>, <a href="http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1">Eduardo Valle</a>, <a href="http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1">Sandra Avila</a></p>
<p>Skin lesion analysis models are biased by artifacts placed during image
acquisition, which influence model predictions despite carrying no clinical
information. Solutions that address this problem by regularizing models to
prevent learning those spurious features achieve only partial success, and
existing test-time debiasing techniques are inappropriate for skin lesion
analysis due to either making unrealistic assumptions on the distribution of
test data or requiring laborious annotation from medical practitioners. We
propose TTS (Test-Time Selection), a human-in-the-loop method that leverages
positive (e.g., lesion area) and negative (e.g., artifacts) keypoints in test
samples. TTS effectively steers models away from exploiting spurious
artifact-related correlations without retraining, and with less annotation
requirements. Our solution is robust to a varying availability of annotations,
and different levels of bias. We showcase on the ISIC2019 dataset (for which we
release a subset of annotated images) how our model could be deployed in the
real-world for mitigating bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05600">NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search. (arXiv:2308.05600v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1">Edouard Yvinec</a>, <a href="http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1">Arnaud Dapogny</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1">Kevin Bailly</a></p>
<p>Deep neural network (DNN) deployment has been confined to larger hardware
devices due to their expensive computational requirements. This challenge has
recently reached another scale with the emergence of large language models
(LLMs). In order to reduce both their memory footprint and latency, a promising
technique is quantization. It consists in converting floating point
representations to low bit-width fixed point representations, usually by
assuming a uniform mapping onto a regular grid. This process, referred to in
the literature as uniform quantization, may however be ill-suited as most DNN
weights and activations follow a bell-shaped distribution. This is even worse
on LLMs whose weight distributions are known to exhibit large, high impact,
outlier values. In this work, we propose an improvement over the most commonly
adopted way to tackle this limitation in deep learning models quantization,
namely, non-uniform quantization. NUPES leverages automorphisms to preserve the
scalar multiplications. Such transformations are derived from power functions.
However, the optimization of the exponent parameter and weight values remains a
challenging and novel problem which could not be solved with previous post
training optimization techniques which only learn to round up or down weight
values in order to preserve the predictive function. We circumvent this
limitation with a new paradigm: learning new quantized weights over the entire
quantized space. Similarly, we enable the optimization of the power exponent,
i.e. the optimization of the quantization operator itself during training by
alleviating all the numerical instabilities. The resulting predictive function
is compatible with integer-only low-bit inference. We show the ability of the
method to achieve state-of-the-art compression rates in both, data-free and
data-driven configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05602">Object Goal Navigation with Recursive Implicit Maps. (arXiv:2308.05602v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shizhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chabal_T/0/1/0/all/0/1">Thomas Chabal</a>, <a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1">Ivan Laptev</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a></p>
<p>Object goal navigation aims to navigate an agent to locations of a given
object category in unseen environments. Classical methods explicitly build maps
of environments and require extensive engineering while lacking semantic
information for object-oriented exploration. On the other hand, end-to-end
learning methods alleviate manual map design and predict actions using implicit
representations. Such methods, however, lack an explicit notion of geometry and
may have limited ability to encode navigation history. In this work, we propose
an implicit spatial map for object goal navigation. Our implicit map is
recursively updated with new observations at each step using a transformer. To
encourage spatial reasoning, we introduce auxiliary tasks and train our model
to reconstruct explicit maps as well as to predict visual features, semantic
labels and actions. Our method significantly outperforms the state of the art
on the challenging MP3D dataset and generalizes well to the HM3D dataset. We
successfully deploy our model on a real robot and achieve encouraging object
goal navigation results in real scenes using only a few real-world
demonstrations. Code, trained models and videos are available at
\url{https://www.di.ens.fr/willow/research/onav_rim/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05605">Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network. (arXiv:2308.05605v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wencheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Junbo Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jianbing Shen</a></p>
<p>Monocular depth estimation is known as an ill-posed task in which objects in
a 2D image usually do not contain sufficient information to predict their
depth. Thus, it acts differently from other tasks (e.g., classification and
segmentation) in many ways. In this paper, we find that self-supervised
monocular depth estimation shows a direction sensitivity and environmental
dependency in the feature representation. But the current backbones borrowed
from other tasks pay less attention to handling different types of
environmental information, limiting the overall depth accuracy. To bridge this
gap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN),
which improves the depth feature representation in two aspects. First, we
propose a direction-aware module, which can learn to adjust the feature
extraction in each direction, facilitating the encoding of different types of
information. Secondly, we design a new cumulative convolution to improve the
efficiency for aggregating important environmental information. Experiments
show that our method achieves significant improvements on three widely used
benchmarks, KITTI, Cityscapes, and Make3D, setting a new state-of-the-art
performance on the popular benchmarks with all three types of self-supervision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05633">IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer. (arXiv:2308.05633v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1">Keqiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiaohao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1">Mahesan Niranjan</a></p>
<p>Automated medical report generation has become increasingly important in
medical analysis. It can produce computer-aided diagnosis descriptions and thus
significantly alleviate the doctors' work. Inspired by the huge success of
neural machine translation and image captioning, various deep learning methods
have been proposed for medical report generation. However, due to the inherent
properties of medical data, including data imbalance and the length and
correlation between report sequences, the generated reports by existing methods
may exhibit linguistic fluency but lack adequate clinical accuracy. In this
work, we propose an image-to-indicator hierarchical transformer (IIHT)
framework for medical report generation. It consists of three modules, i.e., a
classifier module, an indicator expansion module and a generator module. The
classifier module first extracts image features from the input medical images
and produces disease-related indicators with their corresponding states. The
disease-related indicators are subsequently utilised as input for the indicator
expansion module, incorporating the "data-text-data" strategy. The
transformer-based generator then leverages these extracted features along with
image features as auxiliary information to generate final reports. Furthermore,
the proposed IIHT method is feasible for radiologists to modify disease
indicators in real-world scenarios and integrate the operations into the
indicator expansion module for fluent and accurate medical report generation.
Extensive experiments and comparisons with state-of-the-art methods under
various evaluation metrics demonstrate the great performance of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05648">Counterfactual Cross-modality Reasoning for Weakly Supervised Video Moment Localization. (arXiv:2308.05648v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1">Zezhong Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1">Bing Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Video moment localization aims to retrieve the target segment of an untrimmed
video according to the natural language query. Weakly supervised methods gains
attention recently, as the precise temporal location of the target segment is
not always available. However, one of the greatest challenges encountered by
the weakly supervised method is implied in the mismatch between the video and
language induced by the coarse temporal annotations. To refine the
vision-language alignment, recent works contrast the cross-modality
similarities driven by reconstructing masked queries between positive and
negative video proposals. However, the reconstruction may be influenced by the
latent spurious correlation between the unmasked and the masked parts, which
distorts the restoring process and further degrades the efficacy of contrastive
learning since the masked words are not completely reconstructed from the
cross-modality knowledge. In this paper, we discover and mitigate this spurious
correlation through a novel proposed counterfactual cross-modality reasoning
method. Specifically, we first formulate query reconstruction as an aggregated
causal effect of cross-modality and query knowledge. Then by introducing
counterfactual cross-modality knowledge into this aggregation, the spurious
impact of the unmasked part contributing to the reconstruction is explicitly
modeled. Finally, by suppressing the unimodal effect of masked query, we can
rectify the reconstructions of video proposals to perform reasonable
contrastive learning. Extensive experimental evaluations demonstrate the
effectiveness of our proposed method. The code is available at
\href{https://github.com/sLdZ0306/CCR}{https://github.com/sLdZ0306/CCR}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05655">Attention-based 3D CNN with Multi-layer Features for Alzheimer&#x27;s Disease Diagnosis using Brain Images. (arXiv:2308.05655v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yanteng Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Teng_Q/0/1/0/all/0/1">Qizhi Teng</a>, <a href="http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1">Xiaohai He</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_T/0/1/0/all/0/1">Tong Niu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lipei Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ren_C/0/1/0/all/0/1">Chao Ren</a></p>
<p>Structural MRI and PET imaging play an important role in the diagnosis of
Alzheimer's disease (AD), showing the morphological changes and glucose
metabolism changes in the brain respectively. The manifestations in the brain
image of some cognitive impairment patients are relatively inconspicuous, for
example, it still has difficulties in achieving accurate diagnosis through sMRI
in clinical practice. With the emergence of deep learning, convolutional neural
network (CNN) has become a valuable method in AD-aided diagnosis, but some CNN
methods cannot effectively learn the features of brain image, making the
diagnosis of AD still presents some challenges. In this work, we propose an
end-to-end 3D CNN framework for AD diagnosis based on ResNet, which integrates
multi-layer features obtained under the effect of the attention mechanism to
better capture subtle differences in brain images. The attention maps showed
our model can focus on key brain regions related to the disease diagnosis. Our
method was verified in ablation experiments with two modality images on 792
subjects from the ADNI database, where AD diagnostic accuracies of 89.71% and
91.18% were achieved based on sMRI and PET respectively, and also outperformed
some state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05659">AD-CLIP: Adapting Domains in Prompt Space Using CLIP. (arXiv:2308.05659v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singha_M/0/1/0/all/0/1">Mainak Singha</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_H/0/1/0/all/0/1">Harsh Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1">Ankit Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1">Biplab Banerjee</a></p>
<p>Although deep learning models have shown impressive performance on supervised
learning tasks, they often struggle to generalize well when the training
(source) and test (target) domains differ. Unsupervised domain adaptation (DA)
has emerged as a popular solution to this problem. However, current DA
techniques rely on visual backbones, which may lack semantic richness. Despite
the potential of large-scale vision-language foundation models like CLIP, their
effectiveness for DA has yet to be fully explored. To address this gap, we
introduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP that
aims to solve the DA problem in the prompt space. We leverage the frozen vision
backbone of CLIP to extract both image style (domain) and content information,
which we apply to learn prompt tokens. Our prompts are designed to be
domain-invariant and class-generalizable, by conditioning prompt learning on
image style and content features simultaneously. We use standard supervised
contrastive learning in the source domain, while proposing an entropy
minimization strategy to align domains in the embedding space given the target
domain data. We also consider a scenario where only target domain samples are
available during testing, without any source domain data, and propose a
cross-domain style mapping network to hallucinate domain-agnostic tokens. Our
extensive experiments on three benchmark DA datasets demonstrate the
effectiveness of AD-CLIP compared to existing literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05667">2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds. (arXiv:2308.05667v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhirui Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1">Renjiao Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chengyang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a></p>
<p>The commonly adopted detect-then-match approach to registration finds
difficulties in the cross-modality cases due to the incompatible keypoint
detection and inconsistent feature description. We propose, 2D3D-MATR, a
detection-free method for accurate and robust registration between images and
point clouds. Our method adopts a coarse-to-fine pipeline where it first
computes coarse correspondences between downsampled patches of the input image
and the point cloud and then extends them to form dense correspondences between
pixels and points within the patch region. The coarse-level patch matching is
based on transformer which jointly learns global contextual constraints with
self-attention and cross-modality correlations with cross-attention. To resolve
the scale ambiguity in patch matching, we construct a multi-scale pyramid for
each image patch and learn to find for each point patch the best matching image
patch at a proper resolution level. Extensive experiments on two public
benchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-art
P2-Net by around $20$ percentage points on inlier ratio and over $10$ points on
registration recall. Our code and models are available at
\url{https://github.com/minhaolee/2D3DMATR}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05681">Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhengzhi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">He Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1">Ziyi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guoan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1">Hubert P. H. Shum</a></p>
<p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05695">Masked Diffusion as Self-supervised Representation Learner. (arXiv:2308.05695v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zixuan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianxu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yiyu Shi</a></p>
<p>Denoising diffusion probabilistic models have recently demonstrated
state-of-the-art generative performance and been used as strong pixel-level
representation learners. This paper decomposes the interrelation between the
generative capability and representation learning ability inherent in diffusion
models. We present masked diffusion model (MDM), a scalable self-supervised
representation learner that substitutes the conventional additive Gaussian
noise of traditional diffusion with a masking mechanism. Our proposed approach
convincingly surpasses prior benchmarks, demonstrating remarkable advancements
in both medical and natural image semantic segmentation tasks, particularly
within the context of few-shot scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05707">Shadow Datasets, New challenging datasets for Causal Representation Learning. (arXiv:2308.05707v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiageng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hanchen Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jianhua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiazhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1">Mahyar Khayatkhoei</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussein_M/0/1/0/all/0/1">Mohamed E. Hussein</a>, <a href="http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1">Wael AbdAlmageed</a></p>
<p>Discovering causal relations among semantic factors is an emergent topic in
representation learning. Most causal representation learning (CRL) methods are
fully supervised, which is impractical due to costly labeling. To resolve this
restriction, weakly supervised CRL methods were introduced. To evaluate CRL
performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and
CelebA(SMILE), are utilized. However, existing CRL datasets are limited to
simple graphs with few generative factors. Thus we propose two new datasets
with a larger number of diverse generative factors and more sophisticated
causal graphs. In addition, current real datasets, CelebA(BEARD) and
CelebA(SMILE), the originally proposed causal graphs are not aligned with the
dataset distributions. Thus, we propose modifications to them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05721">Deformable Mixer Transformer with Gating for Multi-Task Learning of Dense Prediction. (arXiv:2308.05721v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yangyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yibo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanemm_B/0/1/0/all/0/1">Bernard Ghanemm</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lefei Zhang</a></p>
<p>CNNs and Transformers have their own advantages and both have been widely
used for dense prediction in multi-task learning (MTL). Most of the current
studies on MTL solely rely on CNN or Transformer. In this work, we present a
novel MTL model by combining both merits of deformable CNN and query-based
Transformer with shared gating for multi-task learning of dense prediction.
This combination may offer a simple and efficient solution owing to its
powerful and flexible task-specific learning and advantages of lower cost, less
complexity and smaller parameters than the traditional MTL methods. We
introduce deformable mixer Transformer with gating (DeMTG), a simple and
effective encoder-decoder architecture up-to-date that incorporates the
convolution and attention mechanism in a unified network for MTL. It is
exquisitely designed to use advantages of each block, and provide deformable
and comprehensive features for all tasks from local and global perspective.
First, the deformable mixer encoder contains two types of operators: the
channel-aware mixing operator leveraged to allow communication among different
channels, and the spatial-aware deformable operator with deformable convolution
applied to efficiently sample more informative spatial locations. Second, the
task-aware gating transformer decoder is used to perform the task-specific
predictions, in which task interaction block integrated with self-attention is
applied to capture task interaction features, and the task query block
integrated with gating attention is leveraged to select corresponding
task-specific features. Further, the experiment results demonstrate that the
proposed DeMTG uses fewer GFLOPs and significantly outperforms current
Transformer-based and CNN-based competitive models on a variety of metrics on
three dense prediction datasets. Our code and models are available at
https://github.com/yangyangxu0/DeMTG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05731">Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hagedorn_S/0/1/0/all/0/1">Steffen Hagedorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Hallgarten_M/0/1/0/all/0/1">Marcel Hallgarten</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1">Martin Stoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1">Alexandru Condurache</a></p>
<p>Automated driving has the potential to revolutionize personal, public, and
freight mobility. Besides the enormous challenge of perception, i.e. accurately
perceiving the environment using available sensor data, automated driving
comprises planning a safe, comfortable, and efficient motion trajectory. To
promote safety and progress, many works rely on modules that predict the future
motion of surrounding traffic. Modular automated driving systems commonly
handle prediction and planning as sequential separate tasks. While this
accounts for the influence of surrounding traffic on the ego-vehicle, it fails
to anticipate the reactions of traffic participants to the ego-vehicle's
behavior. Recent works suggest that integrating prediction and planning in an
interdependent joint step is necessary to achieve safe, efficient, and
comfortable driving. While various models implement such integrated systems, a
comprehensive overview and theoretical understanding of different principles
are lacking. We systematically review state-of-the-art deep learning-based
prediction, planning, and integrated prediction and planning models. Different
facets of the integration ranging from model architecture and model design to
behavioral aspects are considered and related to each other. Moreover, we
discuss the implications, strengths, and limitations of different integration
methods. By pointing out research gaps, describing relevant future challenges,
and highlighting trends in the research field, we identify promising directions
for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05733">FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models. (arXiv:2308.05733v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Guangkai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1">Wei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kai Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1">Feng Zhao</a></p>
<p>3D scene reconstruction is a long-standing vision task. Existing approaches
can be categorized into geometry-based and learning-based methods. The former
leverages multi-view geometry but can face catastrophic failures due to the
reliance on accurate pixel correspondence across views. The latter was
proffered to mitigate these issues by learning 2D or 3D representation
directly. However, without a large-scale video or 3D training data, it can
hardly generalize to diverse real-world scenarios due to the presence of tens
of millions or even billions of optimization parameters in the deep network.
Recently, robust monocular depth estimation models trained with large-scale
datasets have been proven to possess weak 3D geometry prior, but they are
insufficient for reconstruction due to the unknown camera parameters, the
affine-invariant property, and inter-frame inconsistency. Here, we propose a
novel test-time optimization approach that can transfer the robustness of
affine-invariant depth models such as LeReS to challenging diverse scenes while
ensuring inter-frame consistency, with only dozens of parameters to optimize
per video frame. Specifically, our approach involves freezing the pre-trained
affine-invariant depth model's depth predictions, rectifying them by optimizing
the unknown scale-shift values with a geometric consistency alignment module,
and employing the resulting scale-consistent depth maps to robustly obtain
camera poses and achieve dense scene reconstruction, even in low-texture
regions. Experiments show that our method achieves state-of-the-art
cross-dataset reconstruction on five zero-shot testing datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05736">MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction. (arXiv:2308.05736v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1">Bencheng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunchi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a></p>
<p>High-definition (HD) map provides abundant and precise static environmental
information of the driving scene, serving as a fundamental and indispensable
component for planning in autonomous driving system. In this paper, we present
\textbf{Map} \textbf{TR}ansformer, an end-to-end framework for online
vectorized HD map construction. We propose a unified permutation-equivalent
modeling approach, \ie, modeling map element as a point set with a group of
equivalent permutations, which accurately describes the shape of map element
and stabilizes the learning process. We design a hierarchical query embedding
scheme to flexibly encode structured map information and perform hierarchical
bipartite matching for map element learning. To speed up convergence, we
further introduce auxiliary one-to-many matching and dense supervision. The
proposed method well copes with various map elements with arbitrary shapes. It
runs at real-time inference speed and achieves state-of-the-art performance on
both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable
and robust map construction quality in complex and various driving scenes. Code
and more demos are available at \url{https://github.com/hustvl/MapTR} for
facilitating further studies and applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05737">Follow Anything: Open-set detection, tracking, and following in real-time. (arXiv:2308.05737v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1">Alaa Maalouf</a>, <a href="http://arxiv.org/find/cs/1/au:+Jadhav_N/0/1/0/all/0/1">Ninad Jadhav</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1">Krishna Murthy Jatavallabhula</a>, <a href="http://arxiv.org/find/cs/1/au:+Chahine_M/0/1/0/all/0/1">Makram Chahine</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_D/0/1/0/all/0/1">Daniel M.Vogt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_R/0/1/0/all/0/1">Robert J. Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>Tracking and following objects of interest is critical to several robotics
use cases, ranging from industrial automation to logistics and warehousing, to
healthcare and security. In this paper, we present a robotic system to detect,
track, and follow any object in real-time. Our approach, dubbed ``follow
anything'' (FAn), is an open-vocabulary and multimodal model -- it is not
restricted to concepts seen at training time and can be applied to novel
classes at inference time using text, images, or click queries. Leveraging rich
visual descriptors from large-scale pre-trained models (foundation models), FAn
can detect and segment objects by matching multimodal queries (text, images,
clicks) against an input image sequence. These detected and segmented objects
are tracked across image frames, all while accounting for occlusion and object
re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial
vehicle) and report its ability to seamlessly follow the objects of interest in
a real-time control loop. FAn can be deployed on a laptop with a lightweight
(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To
enable rapid adoption, deployment, and extensibility, we open-source all our
code on our project webpage at https://github.com/alaamaalouf/FollowAnything .
We also encourage the reader the watch our 5-minutes explainer video in this
https://www.youtube.com/watch?v=6Mgt3EPytrw .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05739">Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics. (arXiv:2308.05739v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Michael Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1">Tobias Ritschel</a></p>
<p>Gradient-based optimization is now ubiquitous across graphics, but
unfortunately can not be applied to problems with undefined or zero gradients.
To circumvent this issue, the loss function can be manually replaced by a
"surrogate" that has similar minima but is differentiable. Our proposed
framework, ZeroGrads, automates this process by learning a neural approximation
of the objective function, the surrogate, which in turn can be used to
differentiate through arbitrary black-box graphics pipelines. We train the
surrogate on an actively smoothed version of the objective and encourage
locality, focusing the surrogate's capacity on what matters at the current
training episode. The fitting is performed online, alongside the parameter
optimization, and self-supervised, without pre-computed data or pre-trained
models. As sampling the objective is expensive (it requires a full rendering or
simulator run), we devise an efficient sampling scheme that allows for
tractable run-times and competitive performance at little overhead. We
demonstrate optimizing diverse non-convex, non-differentiable black-box
problems in graphics, such as visibility in rendering, discrete parameter
spaces in procedural modelling or optimal control in physics-driven animation.
In contrast to more traditional algorithms, our approach scales well to higher
dimensions, which we demonstrate on problems with up to 35k interlinked
variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05741">Neural Progressive Meshes. (arXiv:2308.05741v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yun-Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1">Vladimir G. Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1">Noam Aigerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1">Alec Jacobson</a></p>
<p>The recent proliferation of 3D content that can be consumed on hand-held
devices necessitates efficient tools for transmitting large geometric data,
e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a
challenge to storage as well as transmission bandwidth, and level-of-detail
techniques are often used to transmit an asset using an appropriate bandwidth
budget. It is especially desirable for these methods to transmit data
progressively, improving the quality of the geometry with more data. Our key
insight is that the geometric details of 3D meshes often exhibit similar local
patterns even across different shapes, and thus can be effectively represented
with a shared learned generative space. We learn this space using a
subdivision-based encoder-decoder architecture trained in advance on a large
collection of surfaces. We further observe that additional residual features
can be transmitted progressively between intermediate levels of subdivision
that enable the client to control the tradeoff between bandwidth cost and
quality of reconstruction, providing a neural progressive mesh representation.
We evaluate our method on a diverse set of complex 3D shapes and demonstrate
that it outperforms baselines in terms of compression ratio and reconstruction
quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05744">PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs. (arXiv:2308.05744v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wentao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jia Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaojun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jian Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zihan Zhou</a></p>
<p>In this paper, we develop a new method to automatically convert 2D line
drawings from three orthographic views into 3D CAD models. Existing methods for
this problem reconstruct 3D models by back-projecting the 2D observations into
3D space while maintaining explicit correspondence between the input and
output. Such methods are sensitive to errors and noises in the input, thus
often fail in practice where the input drawings created by human designers are
imperfect. To overcome this difficulty, we leverage the attention mechanism in
a Transformer-based sequence generation model to learn flexible mappings
between the input and output. Further, we design shape programs which are
suitable for generating the objects of interest to boost the reconstruction
accuracy and facilitate CAD modeling applications. Experiments on a new
benchmark dataset show that our method significantly outperforms existing ones
when the inputs are noisy or incomplete.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05745">Iterative Reweighted Least Squares Networks With Convergence Guarantees for Solving Inverse Imaging Problems. (arXiv:2308.05745v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koshelev_I/0/1/0/all/0/1">Iaroslav Koshelev</a>, <a href="http://arxiv.org/find/cs/1/au:+Lefkimmiatis_S/0/1/0/all/0/1">Stamatios Lefkimmiatis</a></p>
<p>In this work we present a novel optimization strategy for image
reconstruction tasks under analysis-based image regularization, which promotes
sparse and/or low-rank solutions in some learned transform domain. We
parameterize such regularizers using potential functions that correspond to
weighted extensions of the $\ell_p^p$-vector and $\mathcal{S}_p^p$
Schatten-matrix quasi-norms with $0 &lt; p \le 1$. Our proposed minimization
strategy extends the Iteratively Reweighted Least Squares (IRLS) method,
typically used for synthesis-based $\ell_p$ and $\mathcal{S}_p$ norm and
analysis-based $\ell_1$ and nuclear norm regularization. We prove that under
mild conditions our minimization algorithm converges linearly to a stationary
point, and we provide an upper bound for its convergence rate. Further, to
select the parameters of the regularizers that deliver the best results for the
problem at hand, we propose to learn them from training data by formulating the
supervised learning process as a stochastic bilevel optimization problem. We
show that thanks to the convergence guarantees of our proposed minimization
strategy, such optimization can be successfully performed with a
memory-efficient implicit back-propagation scheme. We implement our learned
IRLS variants as recurrent networks and assess their performance on the
challenging image reconstruction tasks of non-blind deblurring,
super-resolution and demosaicking. The comparisons against other existing
learned reconstruction approaches demonstrate that our overall method is very
competitive and in many cases outperforms existing unrolled networks, whose
number of parameters is orders of magnitude higher than in our case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1901.01381">Brain segmentation based on multi-atlas guided 3D fully convolutional network ensembles. (arXiv:1901.01381v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiaoying Tang</a></p>
<p>In this study, we proposed and validated a multi-atlas guided 3D fully
convolutional network (FCN) ensemble model (M-FCN) for segmenting brain regions
of interest (ROIs) from structural magnetic resonance images (MRIs). One major
limitation of existing state-of-the-art 3D FCN segmentation models is that they
often apply image patches of fixed size throughout training and testing, which
may miss some complex tissue appearance patterns of different brain ROIs. To
address this limitation, we trained a 3D FCN model for each ROI using patches
of adaptive size and embedded outputs of the convolutional layers in the
deconvolutional layers to further capture the local and global context
patterns. In addition, with an introduction of multi-atlas based guidance in
M-FCN, our segmentation was generated by combining the information of images
and labels, which is highly robust. To reduce over-fitting of the FCN model on
the training data, we adopted an ensemble strategy in the learning procedure.
Evaluation was performed on two brain MRI datasets, aiming respectively at
segmenting 14 subcortical and ventricular structures and 54 brain ROIs. The
segmentation results of the proposed method were compared with those of a
state-of-the-art multi-atlas based segmentation method and an existing 3D FCN
segmentation model. Our results suggested that the proposed method had a
superior segmentation performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.02399">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Longtian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Ziyao Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zilu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yafeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guangnan Zhang</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention
recently for its transferable visual representation learning. However, due to
the semantic gap within datasets, CLIP's pre-trained image-text alignment
becomes sub-optimal on downstream tasks, which severely harms its transferring
performance. To better adapt the cross-modality embedding space, we propose to
enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide
textual features of different categories to adaptively explore informative
regions on the image and aggregate visual features by attention mechanisms. In
this way, the texts become visual-guided, namely, more semantically correlated
with downstream images, which greatly benefits the category-wise matching
process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known
classification datasets to demonstrate its effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.06193">GUNNEL: Guided Mixup Augmentation and Multi-View Fusion for Aquatic Animal Segmentation. (arXiv:2112.06193v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">Minh-Quan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Trung-Nghia Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tam V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1">Isao Echizen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a></p>
<p>Recent years have witnessed great advances in object segmentation research.
In addition to generic objects, aquatic animals have attracted research
attention. Deep learning-based methods are widely used for aquatic animal
segmentation and have achieved promising performance. However, there is a lack
of challenging datasets for benchmarking. In this work, we build a new dataset
dubbed Aquatic Animal Species. We also devise a novel GUided mixup augmeNtatioN
and multi-modEl fusion for aquatic animaL segmentation (GUNNEL) that leverages
the advantages of multiple segmentation models to effectively segment aquatic
animals and improves the training performance by synthesizing hard samples.
Extensive experiments demonstrated the superiority of our proposed framework
over existing state-of-the-art instance segmentation methods. The code is
available at https://github.com/lmquan2000/mask-mixup. The dataset is available
at https://doi.org/10.5281/zenodo.8208877 .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.03026">Context Autoencoder for Self-Supervised Representation Learning. (arXiv:2202.03026v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaokang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaodi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1">Ying Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shumin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1">Gang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a></p>
<p>We present a novel masked image modeling (MIM) approach, context autoencoder
(CAE), for self-supervised representation pretraining. We pretrain an encoder
by making predictions in the encoded representation space. The pretraining
tasks include two tasks: masked representation prediction - predict the
representations for the masked patches, and masked patch reconstruction -
reconstruct the masked patches. The network is an encoder-regressor-decoder
architecture: the encoder takes the visible patches as input; the regressor
predicts the representations of the masked patches, which are expected to be
aligned with the representations computed from the encoder, using the
representations of visible patches and the positions of visible and masked
patches; the decoder reconstructs the masked patches from the predicted encoded
representations. The CAE design encourages the separation of learning the
encoder (representation) from completing the pertaining tasks: masked
representation prediction and masked patch reconstruction tasks, and making
predictions in the encoded representation space empirically shows the benefit
to representation learning. We demonstrate the effectiveness of our CAE through
superior transfer performance in downstream tasks: semantic segmentation,
object detection and instance segmentation, and classification. The code will
be available at https://github.com/Atten4Vis/CAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.13341">Overlooked Implications of the Reconstruction Loss for VAE Disentanglement. (arXiv:2202.13341v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Michlo_N/0/1/0/all/0/1">Nathan Michlo</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1">Richard Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1">Steven James</a></p>
<p>Learning disentangled representations with variational autoencoders (VAEs) is
often attributed to the regularisation component of the loss. In this work, we
highlight the interaction between data and the reconstruction term of the loss
as the main contributor to disentanglement in VAEs. We show that standard
benchmark datasets have unintended correlations between their subjective
ground-truth factors and perceived axes in the data according to typical VAE
reconstruction losses. Our work exploits this relationship to provide a theory
for what constitutes an adversarial dataset under a given reconstruction loss.
We verify this by constructing an example dataset that prevents disentanglement
in state-of-the-art frameworks while maintaining human-intuitive ground-truth
factors. Finally, we re-enable disentanglement by designing an example
reconstruction loss that is once again able to perceive the ground-truth
factors. Our findings demonstrate the subjective nature of disentanglement and
the importance of considering the interaction between the ground-truth factors,
data and notably, the reconstruction loss, which is under-recognised in the
literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03190">Learning Music-Dance Representations through Explicit-Implicit Rhythm Synchronization. (arXiv:2207.03190v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiashuo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1">Junfu Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Ying Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1">Rui Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Although audio-visual representation has been proved to be applicable in many
downstream tasks, the representation of dancing videos, which is more specific
and always accompanied by music with complex auditory contents, remains
challenging and uninvestigated. Considering the intrinsic alignment between the
cadent movement of dancer and music rhythm, we introduce MuDaR, a novel
Music-Dance Representation learning framework to perform the synchronization of
music and dance rhythms both in explicit and implicit ways. Specifically, we
derive the dance rhythms based on visual appearance and motion cues inspired by
the music rhythm analysis. Then the visual rhythms are temporally aligned with
the music counterparts, which are extracted by the amplitude of sound
intensity. Meanwhile, we exploit the implicit coherence of rhythms implied in
audio and visual streams by contrastive learning. The model learns the joint
embedding by predicting the temporal consistency between audio-visual pairs.
The music-dance representation, together with the capability of detecting audio
and visual rhythms, can further be applied to three downstream tasks: (a) dance
classification, (b) music-dance retrieval, and (c) music-dance retargeting.
Extensive experiments demonstrate that our proposed framework outperforms other
self-supervised methods by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.04278">Deep learning-based Crop Row Detection for Infield Navigation of Agri-Robots. (arXiv:2209.04278v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1">Rajitha de Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1">Grzegorz Cielniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Gang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junfeng Gao</a></p>
<p>Autonomous navigation in agricultural environments is challenged by varying
field conditions that arise in arable fields. State-of-the-art solutions for
autonomous navigation in such environments require expensive hardware such as
RTK-GNSS. This paper presents a robust crop row detection algorithm that
withstands such field variations using inexpensive cameras. Existing datasets
for crop row detection does not represent all the possible field variations. A
dataset of sugar beet images was created representing 11 field variations
comprised of multiple grow stages, light levels, varying weed densities, curved
crop rows and discontinuous crop rows. The proposed pipeline segments the crop
rows using a deep learning-based method and employs the predicted segmentation
mask for extraction of the central crop using a novel central crop row
selection algorithm. The novel crop row detection algorithm was tested for crop
row detection performance and the capability of visual servoing along a crop
row. The visual servoing-based navigation was tested on a realistic simulation
scenario with the real ground and plant textures. Our algorithm demonstrated
robust vision-based crop row detection in challenging field conditions
outperforming the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.07902">MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangmeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1">Wenyi Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1">Bing Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a></p>
<p>As a successful approach to self-supervised learning, contrastive learning
aims to learn invariant information shared among distortions of the input
sample. While contrastive learning has yielded continuous advancements in
sampling strategy and architecture design, it still remains two persistent
defects: the interference of task-irrelevant information and sample
inefficiency, which are related to the recurring existence of trivial constant
solutions. From the perspective of dimensional analysis, we find out that the
dimensional redundancy and dimensional confounder are the intrinsic issues
behind the phenomena, and provide experimental evidence to support our
viewpoint. We further propose a simple yet effective approach MetaMask, short
for the dimensional Mask learned by Meta-learning, to learn representations
against dimensional redundancy and confounder. MetaMask adopts the
redundancy-reduction technique to tackle the dimensional redundancy issue and
innovatively introduces a dimensional mask to reduce the gradient effects of
specific dimensions containing the confounder, which is trained by employing a
meta-learning paradigm with the objective of improving the performance of
masked representations on a typical self-supervised task. We provide solid
theoretical analyses to prove MetaMask can obtain tighter risk bounds for
downstream classification compared to typical contrastive methods. Empirically,
our method achieves state-of-the-art performance on various benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.14408">RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1">Eddy Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_A/0/1/0/all/0/1">Alex Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Budhwani_A/0/1/0/all/0/1">Alikasim Budhwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dempster_R/0/1/0/all/0/1">Rowan Dempster</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanquan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Sharman_M/0/1/0/all/0/1">Mohammad Al-Sharman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rayside_D/0/1/0/all/0/1">Derek Rayside</a>, <a href="http://arxiv.org/find/cs/1/au:+Melek_W/0/1/0/all/0/1">William Melek</a></p>
<p>When applied to autonomous vehicle (AV) settings, action recognition can
enhance an environment model's situational awareness. This is especially
prevalent in scenarios where traditional geometric descriptions and heuristics
in AVs are insufficient. However, action recognition has traditionally been
studied for humans, and its limited adaptability to noisy, un-clipped,
un-pampered, raw RGB data has limited its application in other fields. To push
for the advancement and adoption of action recognition into AVs, this work
proposes a novel two-stage action recognition system, termed RALACs. RALACs
formulates the problem of action recognition for road scenes, and bridges the
gap between it and the established field of human action recognition. This work
shows how attention layers can be useful for encoding the relations across
agents, and stresses how such a scheme can be class-agnostic. Furthermore, to
address the dynamic nature of agents on the road, RALACs constructs a novel
approach to adapting Region of Interest (ROI) Alignment to agent tracks for
downstream action classification. Finally, our scheme also considers the
problem of active agent detection, and utilizes a novel application of fusing
optical flow maps to discern relevant agents in a road scene. We show that our
proposed scheme can outperform the baseline on the ICCV2021 Road Challenge
dataset and by deploying it on a real vehicle platform, we provide preliminary
insight to the usefulness of action recognition in decision making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04087">Symmetry Defense Against CNN Adversarial Perturbation Attacks. (arXiv:2210.04087v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lindqvist_B/0/1/0/all/0/1">Blerta Lindqvist</a></p>
<p>This paper uses symmetry to make Convolutional Neural Network classifiers
(CNNs) robust against adversarial perturbation attacks. Such attacks add
perturbation to original images to generate adversarial images that fool
classifiers such as road sign classifiers of autonomous vehicles. Although
symmetry is a pervasive aspect of the natural world, CNNs are unable to handle
symmetry well. For example, a CNN can classify an image differently from its
mirror image. For an adversarial image that misclassifies with a wrong label
$l_w$, CNN inability to handle symmetry means that a symmetric adversarial
image can classify differently from the wrong label $l_w$. Further than that,
we find that the classification of a symmetric adversarial image reverts to the
correct label. To classify an image when adversaries are unaware of the
defense, we apply symmetry to the image and use the classification label of the
symmetric image. To classify an image when adversaries are aware of the
defense, we use mirror symmetry and pixel inversion symmetry to form a symmetry
group. We apply all the group symmetries to the image and decide on the output
label based on the agreement of any two of the classification labels of the
symmetry images. Adaptive attacks fail because they need to rely on loss
functions that use conflicting CNN output values for symmetric images. Without
attack knowledge, the proposed symmetry defense succeeds against both
gradient-based and random-search attacks, with up to near-default accuracies
for ImageNet. The defense even improves the classification accuracy of original
images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07764">Intel Labs at Ego4D Challenge 2022: A Better Baseline for Audio-Visual Diarization. (arXiv:2210.07764v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1">Kyle Min</a></p>
<p>This report describes our approach for the Audio-Visual Diarization (AVD)
task of the Ego4D Challenge 2022. Specifically, we present multiple technical
improvements over the official baselines. First, we improve the detection
performance of the camera wearer's voice activity by modifying the training
scheme of its model. Second, we discover that an off-the-shelf voice activity
detection model can effectively remove false positives when it is applied
solely to the camera wearer's voice activities. Lastly, we show that better
active speaker detection leads to a better AVD outcome. Our final method
obtains 65.9% DER on the test set of Ego4D, which significantly outperforms all
the baselines. Our submission achieved 1st place in the Ego4D Challenge 2022.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08095">Will Large-scale Generative Models Corrupt Future Datasets?. (arXiv:2211.08095v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hataya_R/0/1/0/all/0/1">Ryuichiro Hataya</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1">Han Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1">Hiromi Arai</a></p>
<p>Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently, a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: "\textbf{will such generated images impact the quality of future
datasets and the performance of computer vision models positively or
negatively?}" This paper empirically answers this question by simulating
contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using
a state-of-the-art generative model and evaluate models trained with
"contaminated" datasets on various tasks, including image classification and
image generation. Throughout experiments, we conclude that generated images
negatively affect downstream performance, while the significance depends on
tasks and the amount of generated images. The generated datasets and the codes
for experiments will be publicly released for future research. Generated
datasets and source codes are available from
\url{https://github.com/moskomule/dataset-contamination}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10705">TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer. (arXiv:2211.10705v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhiyang Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Cheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zeyu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiangqiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Weilin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>In this paper, we introduce a set of simple yet effective TOken REduction
(TORE) strategies for Transformer-based Human Mesh Recovery from monocular
images. Current SOTA performance is achieved by Transformer-based structures.
However, they suffer from high model complexity and computation cost caused by
redundant tokens. We propose token reduction strategies based on two important
aspects, i.e., the 3D geometry structure and 2D image feature, where we
hierarchically recover the mesh geometry with priors from body structure and
conduct token clustering to pass fewer but more discriminative image feature
tokens to the Transformer. Our method massively reduces the number of tokens
involved in high-complexity interactions in the Transformer. This leads to a
significantly reduced computational cost while still achieving competitive or
even higher accuracy in shape recovery. Extensive experiments across a wide
range of benchmarks validate the superior effectiveness of the proposed method.
We further demonstrate the generalizability of our method on hand mesh
recovery. Visit our project page at
https://frank-zy-dou.github.io/projects/Tore/index.html.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05221">Open-vocabulary Object Segmentation with Diffusion Models. (arXiv:2301.05221v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qinye Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a></p>
<p>The goal of this paper is to extract the visual-language correspondence from
a pre-trained text-to-image diffusion model, in the form of segmentation map,
i.e., simultaneously generating images and segmentation masks for the
corresponding visual entities described in the text prompt. We make the
following contributions: (i) we pair the existing Stable Diffusion model with a
novel grounding module, that can be trained to align the visual and textual
embedding space of the diffusion model with only a small number of object
categories; (ii) we establish an automatic pipeline for constructing a dataset,
that consists of {image, segmentation mask, text prompt} triplets, to train the
proposed grounding module; (iii) we evaluate the performance of open-vocabulary
grounding on images generated from the text-to-image diffusion model and show
that the module can well segment the objects of categories beyond seen ones at
training time; (iv) we adopt the augmented diffusion model to build a synthetic
semantic segmentation dataset, and show that, training a standard segmentation
model on such dataset demonstrates competitive performance on the zero-shot
segmentation(ZS3) benchmark, which opens up new opportunities for adopting the
powerful diffusion model for discriminative tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08365">On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction. (arXiv:2301.08365v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yiasemis_G/0/1/0/all/0/1">George Yiasemis</a>, <a href="http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1">Clara I. S&#xe1;nchez</a>, <a href="http://arxiv.org/find/eess/1/au:+Sonke_J/0/1/0/all/0/1">Jan-Jakob Sonke</a>, <a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1">Jonas Teuwen</a></p>
<p>Acquiring fully-sampled MRI $k$-space data is time-consuming, and collecting
accelerated data can reduce the acquisition time. Employing 2D
Cartesian-rectilinear subsampling schemes is a conventional approach for
accelerated acquisitions; however, this often results in imprecise
reconstructions, even with the use of Deep Learning (DL), especially at high
acceleration factors. Non-rectilinear or non-Cartesian trajectories can be
implemented in MRI scanners as alternative subsampling options. This work
investigates the impact of the $k$-space subsampling scheme on the quality of
reconstructed accelerated MRI measurements produced by trained DL models. The
Recurrent Variational Network (RecurrentVarNet) was used as the DL-based
MRI-reconstruction architecture. Cartesian, fully-sampled multi-coil $k$-space
measurements from three datasets were retrospectively subsampled with different
accelerations using eight distinct subsampling schemes: four
Cartesian-rectilinear, two Cartesian non-rectilinear, and two non-Cartesian.
Experiments were conducted in two frameworks: scheme-specific, where a distinct
model was trained and evaluated for each dataset-subsampling scheme pair, and
multi-scheme, where for each dataset a single model was trained on data
randomly subsampled by any of the eight schemes and evaluated on data
subsampled by all schemes. In both frameworks, RecurrentVarNets trained and
evaluated on non-rectilinearly subsampled data demonstrated superior
performance, particularly for high accelerations. In the multi-scheme setting,
reconstruction performance on rectilinearly subsampled data improved when
compared to the scheme-specific experiments. Our findings demonstrate the
potential for using DL-based methods, trained on non-rectilinearly subsampled
measurements, to optimize scan time and image quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05194">Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation. (arXiv:2303.05194v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1">David Bruggemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1">Christos Sakaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Brodermann_T/0/1/0/all/0/1">Tim Br&#xf6;dermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>Standard unsupervised domain adaptation methods adapt models from a source to
a target domain using labeled source data and unlabeled target data jointly. In
model adaptation, on the other hand, access to the labeled source data is
prohibited, i.e., only the source-trained model and unlabeled target data are
available. We investigate normal-to-adverse condition model adaptation for
semantic segmentation, whereby image-level correspondences are available in the
target domain. The target set consists of unlabeled pairs of adverse- and
normal-condition street images taken at GPS-matched locations. Our method --
CMA -- leverages such image pairs to learn condition-invariant features via
contrastive learning. In particular, CMA encourages features in the embedding
space to be grouped according to their condition-invariant semantic content and
not according to the condition under which respective inputs are captured. To
obtain accurate cross-domain semantic correspondences, we warp the normal image
to the viewpoint of the adverse image and leverage warp-confidence scores to
create robust, aggregated features. With this approach, we achieve
state-of-the-art semantic segmentation performance for model adaptation on
several normal-to-adverse adaptation benchmarks, such as ACDC and Dark Zurich.
We also evaluate CMA on a newly procured adverse-condition generalization
benchmark and report favorable results compared to standard unsupervised domain
adaptation methods, despite the comparative handicap of CMA due to source data
inaccessibility. Code is available at https://github.com/brdav/cma.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06601">Multi-metrics adaptively identifies backdoors in Federated learning. (arXiv:2303.06601v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siquan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yijiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Leyu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Ying Gao</a></p>
<p>The decentralized and privacy-preserving nature of federated learning (FL)
makes it vulnerable to backdoor attacks aiming to manipulate the behavior of
the resulting model on specific adversary-chosen inputs. However, most existing
defenses based on statistical differences take effect only against specific
attacks, especially when the malicious gradients are similar to benign ones or
the data are highly non-independent and identically distributed (non-IID). In
this paper, we revisit the distance-based defense methods and discover that i)
Euclidean distance becomes meaningless in high dimensions and ii) malicious
gradients with diverse characteristics cannot be identified by a single metric.
To this end, we present a simple yet effective defense strategy with
multi-metrics and dynamic weighting to identify backdoors adaptively.
Furthermore, our novel defense has no reliance on predefined assumptions over
attack settings or data distributions and little impact on benign performance.
To evaluate the effectiveness of our approach, we conduct comprehensive
experiments on different datasets under various attack settings, where our
method achieves the best defensive performance. For instance, we achieve the
lowest backdoor accuracy of 3.06% under the difficult Edge-case PGD, showing
significant superiority over previous defenses. The results also demonstrate
that our method can be well-adapted to a wide range of non-IID degrees without
sacrificing the benign performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11316">Generative Semantic Segmentation. (arXiv:2303.11316v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiachen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a></p>
<p>We present Generative Semantic Segmentation (GSS), a generative learning
approach for semantic segmentation. Uniquely, we cast semantic segmentation as
an image-conditioned mask generation problem. This is achieved by replacing the
conventional per-pixel discriminative learning with a latent prior learning
process. Specifically, we model the variational posterior distribution of
latent variables given the segmentation mask. To that end, the segmentation
mask is expressed with a special type of image (dubbed as maskige). This
posterior distribution allows to generate segmentation masks unconditionally.
To achieve semantic segmentation on a given image, we further introduce a
conditioning network. It is optimized by minimizing the divergence between the
posterior distribution of maskige (i.e., segmentation masks) and the latent
prior distribution of input training images. Extensive experiments on standard
benchmarks show that our GSS can perform competitively to prior art
alternatives in the standard semantic segmentation setting, whilst achieving a
new state of the art in the more challenging cross-domain setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12384">RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration. (arXiv:2303.12384v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiuming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chaokang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>Although point cloud registration has achieved remarkable advances in
object-level and indoor scenes, large-scale registration methods are rarely
explored. Challenges mainly arise from the huge point number, complex
distribution, and outliers of outdoor LiDAR scans. In addition, most existing
registration works generally adopt a two-stage paradigm: They first find
correspondences by extracting discriminative local features and then leverage
estimators (eg. RANSAC) to filter outliers, which are highly dependent on
well-designed descriptors and post-processing choices. To address these
problems, we propose an end-to-end transformer network (RegFormer) for
large-scale point cloud alignment without any further post-processing.
Specifically, a projection-aware hierarchical transformer is proposed to
capture long-range dependencies and filter outliers by extracting point
features globally. Our transformer has linear complexity, which guarantees high
efficiency even for large-scale scenes. Furthermore, to effectively reduce
mismatches, a bijective association transformer is designed for regressing the
initial transformation. Extensive experiments on KITTI and NuScenes datasets
demonstrate that our RegFormer achieves competitive performance in terms of
both accuracy and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14961">Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franco_N/0/1/0/all/0/1">Nicola Franco</a>, <a href="http://arxiv.org/find/cs/1/au:+Korth_D/0/1/0/all/0/1">Daniel Korth</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1">Jeanette Miriam Lorenz</a>, <a href="http://arxiv.org/find/cs/1/au:+Roscher_K/0/1/0/all/0/1">Karsten Roscher</a>, <a href="http://arxiv.org/find/cs/1/au:+Guennemann_S/0/1/0/all/0/1">Stephan Guennemann</a></p>
<p>As the use of machine learning continues to expand, the importance of
ensuring its safety cannot be overstated. A key concern in this regard is the
ability to identify whether a given sample is from the training distribution,
or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can
manipulate OOD samples in ways that lead a classifier to make a confident
prediction. In this study, we present a novel approach for certifying the
robustness of OOD detection within a $\ell_2$-norm around the input, regardless
of network architecture and without the need for specific components or
additional training. Further, we improve current techniques for detecting
adversarial attacks on OOD samples, while providing high levels of certified
and adversarial robustness on in-distribution samples. The average of all OOD
detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$
relative to previous approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17005">Tightly-coupled Visual-DVL-Inertial Odometry for Robot-based Ice-water Boundary Exploration. (arXiv:2303.17005v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingxi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Loose_B/0/1/0/all/0/1">Brice Loose</a></p>
<p>Robotic underwater systems, e.g., Autonomous Underwater Vehicles (AUVs) and
Remotely Operated Vehicles (ROVs), are promising tools for collecting
biogeochemical data at the ice-water interface for scientific advancements.
However, state estimation, i.e., localization, is a well-known problem for
robotic systems, especially, for the ones that travel underwater. In this
paper, we present a tightly-coupled multi-sensors fusion framework to increase
localization accuracy that is robust to sensor failure. Visual images, Doppler
Velocity Log (DVL), Inertial Measurement Unit (IMU) and Pressure sensor are
integrated into the state-of-art Multi-State Constraint Kalman Filter (MSCKF)
for state estimation. Besides that a new keyframe-based state clone mechanism
and a new DVL-aided feature enhancement are presented to further improve the
localization performance. The proposed method is validated with a data set
collected in the field under frozen ice, and the result is compared with 6
other different sensor fusion setups. Overall, the result with the keyframe
enabled and DVL-aided feature enhancement yields the best performance with a
Root-mean-square error of less than 2 m compared to the ground truth path with
a total traveling distance of about 200 m.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02970">A Closer Look at Audio-Visual Semantic Segmentation. (arXiv:2304.02970v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuanhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fengbei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1">Gustavo Carneiro</a></p>
<p>Audio-visual segmentation (AVS) is a complex task that involves accurately
segmenting the corresponding sounding object based on audio-visual queries.
Successful audio-visual learning requires two essential components: 1) an
unbiased dataset with high-quality pixel-level multi-class labels, and 2) a
model capable of effectively linking audio information with its corresponding
visual object. However, these two requirements are only partially addressed by
current methods, with training sets containing biased audio-visual data, and
models that generalise poorly beyond this biased training set. In this work, we
propose a new strategy to build cost-effective and relatively unbiased
audio-visual semantic segmentation benchmarks. Our strategy, called Visual
Post-production (VPO), explores the observation that it is not necessary to
have explicit audio-visual pairs extracted from single video sources to build
such benchmarks. We also refine the previously proposed AVSBench to transform
it into the audio-visual semantic segmentation benchmark AVSBench-Single+.
Furthermore, this paper introduces a new pixel-wise audio-visual contrastive
learning method to enable a better generalisation of the model beyond the
training set. We verify the validity of the VPO strategy by showing that
state-of-the-art (SOTA) models trained with datasets built by matching audio
and visual data from different sources or with datasets containing audio and
visual data from the same video source produce almost the same accuracy. Then,
using the proposed VPO benchmarks and AVSBench-Single+, we show that our method
produces more accurate audio-visual semantic segmentation than SOTA models.
Code and dataset will be available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03246">Inst-Inpaint: Instructing to Remove Objects with Diffusion Models. (arXiv:2304.03246v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yildirim_A/0/1/0/all/0/1">Ahmet Burak Yildirim</a>, <a href="http://arxiv.org/find/cs/1/au:+Baday_V/0/1/0/all/0/1">Vedat Baday</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1">Erkut Erdem</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1">Aykut Erdem</a>, <a href="http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1">Aysegul Dundar</a></p>
<p>Image inpainting task refers to erasing unwanted pixels from images and
filling them in a semantically consistent and realistic way. Traditionally, the
pixels that are wished to be erased are defined with binary masks. From the
application point of view, a user needs to generate the masks for the objects
they would like to remove which can be time-consuming and prone to errors. In
this work, we are interested in an image inpainting algorithm that estimates
which object to be removed based on natural language input and removes it,
simultaneously. For this purpose, first, we construct a dataset named
GQA-Inpaint for this task. Second, we present a novel inpainting framework,
Inst-Inpaint, that can remove objects from images based on the instructions
given as text prompts. We set various GAN and diffusion-based baselines and run
experiments on synthetic and real image datasets. We compare methods with
different evaluation metrics that measure the quality and accuracy of the
models and show significant quantitative and qualitative improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10769">Deep Multiview Clustering by Contrasting Cluster Assignments. (arXiv:2304.10769v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Hua Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1">Wai Lok Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xi Peng</a></p>
<p>Multiview clustering (MVC) aims to reveal the underlying structure of
multiview data by categorizing data samples into clusters. Deep learning-based
methods exhibit strong feature learning capabilities on large-scale datasets.
For most existing deep MVC methods, exploring the invariant representations of
multiple views is still an intractable problem. In this paper, we propose a
cross-view contrastive learning (CVCL) method that learns view-invariant
representations and produces clustering results by contrasting the cluster
assignments among multiple views. Specifically, we first employ deep
autoencoders to extract view-dependent features in the pretraining stage. Then,
a cluster-level CVCL strategy is presented to explore consistent semantic label
information among the multiple views in the fine-tuning stage. Thus, the
proposed CVCL method is able to produce more discriminative cluster assignments
by virtue of this learning strategy. Moreover, we provide a theoretical
analysis of soft cluster assignment alignment. Extensive experimental results
obtained on several datasets demonstrate that the proposed CVCL method
outperforms several state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07304">CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1">Ruixiang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingbo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changwen Chen</a></p>
<p>Recent advances in visual-language models have shown remarkable zero-shot
text-image matching ability that is transferable to downstream tasks such as
object detection and segmentation. Adapting these models for object counting,
however, remains a formidable challenge. In this study, we first investigate
transferring vision-language models (VLMs) for class-agnostic object counting.
Specifically, we propose CLIP-Count, the first end-to-end pipeline that
estimates density maps for open-vocabulary objects with text guidance in a
zero-shot manner. To align the text embedding with dense visual features, we
introduce a patch-text contrastive loss that guides the model to learn
informative patch-level visual representations for dense prediction. Moreover,
we design a hierarchical patch-text interaction module to propagate semantic
information across different resolution levels of visual features. Benefiting
from the full exploitation of the rich image-text alignment knowledge of
pretrained VLMs, our method effectively generates high-quality density maps for
objects-of-interest. Extensive experiments on FSC-147, CARPK, and ShanghaiTech
crowd counting datasets demonstrate state-of-the-art accuracy and
generalizability of the proposed method. Code is available:
https://github.com/songrise/CLIP-Count.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10608">STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced Audio-Visual Diarization. (arXiv:2306.10608v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1">Kyle Min</a></p>
<p>This report introduces our novel method named STHG for the Audio-Visual
Diarization task of the Ego4D Challenge 2023. Our key innovation is that we
model all the speakers in a video using a single, unified heterogeneous graph
learning framework. Unlike previous approaches that require a separate
component solely for the camera wearer, STHG can jointly detect the speech
activities of all people including the camera wearer. Our final method obtains
61.1% DER on the test set of Ego4D, which significantly outperforms all the
baselines as well as last year's winner. Our submission achieved 1st place in
the Ego4D Challenge 2023. We additionally demonstrate that applying the
off-the-shelf speech recognition system to the diarized speech segments by STHG
produces a competitive performance on the Speech Transcription task of this
challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11029">RemoteCLIP: A Vision Language Foundation Model for Remote Sensing. (arXiv:2306.11029v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Delong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1">Zhangqingyun Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaocong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiale Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a></p>
<p>General-purpose foundation models have become increasingly important in the
field of artificial intelligence. While self-supervised learning (SSL) and
Masked Image Modeling (MIM) have led to promising results in building such
foundation models for remote sensing, these models primarily learn low-level
features, require annotated data for fine-tuning, and not applicable for
retrieval and zero-shot applications due to the lack of language understanding.
In response to these limitations, we propose RemoteCLIP, the first
vision-language foundation model for remote sensing that aims to learn robust
visual features with rich semantics, as well as aligned text embeddings for
seamless downstream application. To address the scarcity of pre-training data,
we leverage data scaling, converting heterogeneous annotations based on
Box-to-Caption (B2C) and Mask-to-Box (M2B) conversion, and further
incorporating UAV imagery, resulting a 12xlarger pretraining dataset.
RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot
image classification, linear probing, k-NN classification, few-shot
classification, image-text retrieval, and object counting. Evaluations on 16
datasets, including a newly introduced RemoteCount benchmark to test the object
counting ability, show that RemoteCLIP consistently outperforms baseline
foundation models across different model scales. Impressively, RemoteCLIP
outperform previous SoTA by 9.14% mean recall on RSICD dataset and by 8.92% on
RSICD dataset. For zero-shot classification, our RemoteCLIP outperform CLIP
baseline by up to 6.39% average accuracy on 12 downstream datasets.Pretrained
models is available at https://github.com/ChenDelong1999/RemoteCLIP .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13720">Decoupled Diffusion Models with Explicit Transition Probability. (arXiv:2306.13720v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuhang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinwang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a></p>
<p>Recent diffusion probabilistic models (DPMs) have shown remarkable abilities
of generated content, however, they often suffer from complex forward
processes, resulting in inefficient solutions for the reversed process and
prolonged sampling times. In this paper, we aim to address the aforementioned
challenges by focusing on the diffusion process itself that we propose to
decouple the intricate diffusion process into two comparatively simpler process
to improve the generative efficacy and speed. In particular, we present a novel
diffusion paradigm named DDM (Decoupled Diffusion Models) based on the Ito
diffusion process, in which the image distribution is approximated by an
explicit transition probability while the noise path is controlled by the
standard Wiener process. We find that decoupling the diffusion process reduces
the learning difficulty and the explicit transition probability improves the
generative speed significantly. We prove a new training objective for DPM,
which enables the model to learn to predict the noise and image components
separately. Moreover, given the novel forward diffusion equation, we derive the
reverse denoising formula of DDM that naturally supports fewer steps of
generation without ordinary differential equation (ODE) based accelerators. Our
experiments demonstrate that DDM outperforms previous DPMs by a large margin in
fewer function evaluations setting and gets comparable performances in long
function evaluations setting. We also show that our framework can be applied to
image-conditioned generation and high-resolution image synthesis, and that it
can generate high-quality images with only 10 function evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01097">MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion. (arXiv:2307.01097v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shitao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiacheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1">Yasutaka Furukawa</a></p>
<p>This paper introduces MVDiffusion, a simple yet effective method for
generating consistent multi-view images from text prompts given pixel-to-pixel
correspondences (e.g., perspective crops from a panorama or multi-view images
given depth maps and poses). Unlike prior methods that rely on iterative image
warping and inpainting, MVDiffusion simultaneously generates all images with a
global awareness, effectively addressing the prevalent error accumulation
issue. At its core, MVDiffusion processes perspective images in parallel with a
pre-trained text-to-image diffusion model, while integrating novel
correspondence-aware attention layers to facilitate cross-view interactions.
For panorama generation, while only trained with 10k panoramas, MVDiffusion is
able to generate high-resolution photorealistic images for arbitrary texts or
extrapolate one perspective image to a 360-degree view. For multi-view
depth-to-image generation, MVDiffusion demonstrates state-of-the-art
performance for texturing a scene mesh. The project page is at
https://mvdiffusion.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07944">Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuoxiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yadan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1">Mahsa Baktashmotlagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zi Huang</a></p>
<p>Unsupervised domain adaptation (DA) with the aid of pseudo labeling
techniques has emerged as a crucial approach for domain-adaptive 3D object
detection. While effective, existing DA methods suffer from a substantial drop
in performance when applied to a multi-class training setting, due to the
co-existence of low-quality pseudo labels and class imbalance issues. In this
paper, we address this challenge by proposing a novel ReDB framework tailored
for learning to detect all classes at once. Our approach produces Reliable,
Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the
self-training on a distributionally different target domain. To alleviate
disruptions caused by the environmental discrepancy (e.g., beam numbers), the
proposed cross-domain examination (CDE) assesses the correctness of pseudo
labels by copy-pasting target instances into a source environment and measuring
the prediction consistency. To reduce computational overhead and mitigate the
object shift (e.g., scales and point densities), we design an overlapped boxes
counting (OBC) metric that allows to uniformly downsample pseudo-labeled
objects across different geometric characteristics. To confront the issue of
inter-class imbalance, we progressively augment the target point clouds with a
class-balanced set of pseudo-labeled target instances and source objects, which
boosts recognition accuracies on both frequently appearing and rare classes.
Experimental results on three benchmark datasets using both voxel-based (i.e.,
SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our
proposed ReDB approach outperforms existing 3D domain adaptation methods by a
large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.
The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08106">Polarization Multi-Image Synthesis with Birefringent Metasurfaces. (arXiv:2307.08106v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hazineh_D/0/1/0/all/0/1">Dean Hazineh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Soon Wei Daniel Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Capasso_F/0/1/0/all/0/1">Federico Capasso</a>, <a href="http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1">Todd Zickler</a></p>
<p>Optical metasurfaces composed of precisely engineered nanostructures have
gained significant attention for their ability to manipulate light and
implement distinct functionalities based on the properties of the incident
field. Computational imaging systems have started harnessing this capability to
produce sets of coded measurements that benefit certain tasks when paired with
digital post-processing. Inspired by these works, we introduce a new system
that uses a birefringent metasurface with a polarizer-mosaicked photosensor to
capture four optically-coded measurements in a single exposure. We apply this
system to the task of incoherent opto-electronic filtering, where digital
spatial-filtering operations are replaced by simpler, per-pixel sums across the
four polarization channels, independent of the spatial filter size. In contrast
to previous work on incoherent opto-electronic filtering that can realize only
one spatial filter, our approach can realize a continuous family of filters
from a single capture, with filters being selected from the family by adjusting
the post-capture digital summation weights. To find a metasurface that can
realize a set of user-specified spatial filters, we introduce a form of
gradient descent with a novel regularizer that encourages light efficiency and
a high signal-to-noise ratio. We demonstrate several examples in simulation and
with fabricated prototypes, including some with spatial filters that have
prescribed variations with respect to depth and wavelength.
</p>
<p>Visit the Project Page at
https://deanhazineh.github.io/publications/Multi_Image_Synthesis/MIS_Home.html
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08695">Neural Video Depth Stabilizer. (arXiv:2307.08695v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Min Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhiguo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_K/0/1/0/all/0/1">Ke Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a></p>
<p>Video depth estimation aims to infer temporally consistent depth. Some
methods achieve temporal consistency by finetuning a single-image depth model
during test time using geometry and re-projection constraints, which is
inefficient and not robust. An alternative approach is to learn how to enforce
temporal consistency from data, but this requires well-designed models and
sufficient video depth data. To address these challenges, we propose a
plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that
stabilizes inconsistent depth estimations and can be applied to different
single-image depth models without extra effort. We also introduce a large-scale
dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with
over two million frames, making it the largest natural-scene video depth
dataset to our knowledge. We evaluate our method on the VDW dataset as well as
two public benchmarks and demonstrate significant improvements in consistency,
accuracy, and efficiency compared to previous approaches. Our work serves as a
solid baseline and provides a data foundation for learning-based video depth
models. We will release our dataset and code for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10816">BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion. (arXiv:2307.10816v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jinheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuexiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yawen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haozhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>Recent text-to-image diffusion models have demonstrated an astonishing
capacity to generate high-quality images. However, researchers mainly studied
the way of synthesizing images with only text prompts. While some works have
explored using other modalities as conditions, considerable paired data, e.g.,
box/mask-image pairs, and fine-tuning time are required for nurturing models.
As such paired data is time-consuming and labor-intensive to acquire and
restricted to a closed set, this potentially becomes the bottleneck for
applications in an open world. This paper focuses on the simplest form of
user-provided conditions, e.g., box or scribble. To mitigate the aforementioned
problem, we propose a training-free method to control objects and contexts in
the synthesized images adhering to the given spatial conditions. Specifically,
three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,
are designed and seamlessly integrated into the denoising step of diffusion
models, requiring no additional training and massive annotated layout data.
Extensive results show that the proposed constraints can control what and where
to present in the images while retaining the ability of the Stable Diffusion
model to synthesize with high fidelity and diverse concept coverage. The code
is publicly available at https://github.com/Sierkinhane/BoxDiff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11074">Learning Dense UV Completion for Human Mesh Recovery. (arXiv:2307.11074v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanjun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qingping Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1">Jun Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Rong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Li Song</a></p>
<p>Human mesh reconstruction from a single image is challenging in the presence
of occlusion, which can be caused by self, objects, or other humans. Existing
methods either fail to separate human features accurately or lack proper
supervision for feature completion. In this paper, we propose Dense Inpainting
Human Mesh Recovery (DIMR), a two-stage method that leverages dense
correspondence maps to handle occlusion. Our method utilizes a dense
correspondence map to separate visible human features and completes human
features on a structured UV map dense human with an attention-based feature
completion module. We also design a feature inpainting training procedure that
guides the network to learn from unoccluded features. We evaluate our method on
several datasets and demonstrate its superior performance under heavily
occluded scenarios compared to other methods. Extensive experiments show that
our method obviously outperforms prior SOTA methods on heavily occluded images
and achieves comparable results on the standard benchmarks (3DPW).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14527">Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1">Thomas Manzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1">Robin Murphy</a></p>
<p>This paper details the challenges in applying two computer vision systems, an
EfficientDET supervised learning model and the unsupervised RX spectral
classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and
rescue (WSAR) effort in Japan and identifies 3 directions for future research.
There have been at least 19 proposed approaches and 3 datasets aimed at
locating missing persons in drone imagery, but only 3 approaches (2
unsupervised and 1 of an unknown structure) are referenced in the literature as
having been used in an actual WSAR operation. Of these proposed approaches, the
EfficientDET architecture and the unsupervised spectral RX classifier were
selected as the most appropriate for this setting. The EfficientDET model was
applied to the HERIDAL dataset and despite achieving performance that is
statistically equivalent to the state-of-the-art, the model fails to translate
to the real world in terms of false positives (e.g., identifying tree limbs and
rocks as people), and false negatives (e.g., failing to identify members of the
search team). The poor results in practice for algorithms that showed good
results on datasets suggest 3 areas of future research: more realistic datasets
for wilderness SAR, computer vision models that are capable of seamlessly
handling the variety of imagery that can be collected during actual WSAR
operations, and better alignment on performance measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15644">Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jialu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yicong Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1">Hao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Recent research in language-guided visual navigation has demonstrated a
significant demand for the diversity of traversable environments and the
quantity of supervision for training generalizable agents. To tackle the common
data scarcity issue in existing vision-and-language navigation datasets, we
propose an effective paradigm for generating large-scale data for learning,
which applies 1200+ photo-realistic environments from HM3D and Gibson datasets
and synthesizes 4.9 million instruction trajectory pairs using fully-accessible
resources on the web. Importantly, we investigate the influence of each
component in this paradigm on the agent's performance and study how to
adequately apply the augmented data to pre-train and fine-tune an agent. Thanks
to our large-scale dataset, the performance of an existing agent can be pushed
up (+11% absolute with regard to previous SoTA) to a significantly new best of
80% single-run success rate on the R2R test split by simple imitation learning.
The long-lasting generalization gap between navigating in seen and unseen
environments is also reduced to less than 1% (versus 8% in the previous best
method). Moreover, our paradigm also facilitates different models to achieve
new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous
environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16361">Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples. (arXiv:2307.16361v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1">Qiufan Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Cong Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shengshan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a></p>
<p>Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to
adversarial examples, threatening their practical deployment. Despite the many
research endeavors have been made to tackle this issue in recent years, the
diversity of adversarial examples on 3D point clouds makes them more
challenging to defend against than those on 2D images. For examples, attackers
can generate adversarial examples by adding, shifting, or removing points.
Consequently, existing defense strategies are hard to counter unseen point
cloud adversarial examples. In this paper, we first establish a comprehensive,
and rigorous point cloud adversarial robustness benchmark to evaluate
adversarial robustness, which can provide a detailed understanding of the
effects of the defense and attack methods. We then collect existing defense
tricks in point cloud adversarial defenses and then perform extensive and
systematic experiments to identify an effective combination of these tricks.
Furthermore, we propose a hybrid training augmentation methods that consider
various types of point cloud adversarial examples to adversarial training,
significantly improving the adversarial robustness. By combining these tricks,
we construct a more robust defense framework achieving an average accuracy of
83.45\% against various attacks, demonstrating its capability to enabling
robust learners. Our codebase are open-sourced on:
\url{https://github.com/qiufan319/benchmark_pc_attack.git}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00135">InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing. (arXiv:2308.00135v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1">Anant Khandelwal</a></p>
<p>Large text-to-image diffusion models have achieved remarkable success in
generating diverse, high-quality images. Additionally, these models have been
successfully leveraged to edit input images by just changing the text prompt.
But when these models are applied to videos, the main challenge is to ensure
temporal consistency and coherence across frames. In this paper, we propose
InFusion, a framework for zero-shot text-based video editing leveraging large
pre-trained image diffusion models. Our framework specifically supports editing
of multiple concepts with pixel-level control over diverse concepts mentioned
in the editing prompt. Specifically, we inject the difference in features
obtained with source and edit prompts from U-Net residual blocks of decoder
layers. When these are combined with injected attention features, it becomes
feasible to query the source contents and scale edited concepts along with the
injection of unedited parts. The editing is further controlled in a
fine-grained manner with mask extraction and attention fusion, which cut the
edited part from the source and paste it into the denoising pipeline for the
editing prompt. Our framework is a low-cost alternative to one-shot tuned
models for editing since it does not require training. We demonstrated complex
concept editing with a generalised image model (Stable Diffusion v1.5) using
LoRA. Adaptation is compatible with all the existing image diffusion
techniques. Extensive experimental results demonstrate the effectiveness of
existing methods in rendering high-quality and temporally consistent videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03382">Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network. (arXiv:2308.03382v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Junzhou Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1">Qian Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yulin Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1">Linyi Qian</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1">Chengyuan Yu</a></p>
<p>Nucleus image segmentation is a crucial step in the analysis, pathological
diagnosis, and classification, which heavily relies on the quality of nucleus
segmentation. However, the complexity of issues such as variations in nucleus
size, blurred nucleus contours, uneven staining, cell clustering, and
overlapping cells poses significant challenges. Current methods for nucleus
segmentation primarily rely on nuclear morphology or contour-based approaches.
Nuclear morphology-based methods exhibit limited generalization ability and
struggle to effectively predict irregular-shaped nuclei, while contour-based
extraction methods face challenges in accurately segmenting overlapping nuclei.
To address the aforementioned issues, we propose a dual-branch network using
hybrid attention based residual U-blocks for nucleus instance segmentation. The
network simultaneously predicts target information and target contours.
Additionally, we introduce a post-processing method that combines the target
information and target contours to distinguish overlapping nuclei and generate
an instance segmentation image. Within the network, we propose a context fusion
block (CF-block) that effectively extracts and merges contextual information
from the network. Extensive quantitative evaluations are conducted to assess
the performance of our method. Experimental results demonstrate the superior
performance of the proposed method compared to state-of-the-art approaches on
the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03463">DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis. (arXiv:2308.03463v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1">Zhongjie Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1">Lizhou You</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1">Weining Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a></p>
<p>In recent years, diffusion models have emerged as the most powerful approach
in image synthesis. However, applying these models directly to video synthesis
presents challenges, as it often leads to noticeable flickering contents.
Although recently proposed zero-shot methods can alleviate flicker to some
extent, we still struggle to generate coherent videos. In this paper, we
propose DiffSynth, a novel approach that aims to convert image synthesis
pipelines to video synthesis pipelines. DiffSynth consists of two key
components: a latent in-iteration deflickering framework and a video
deflickering algorithm. The latent in-iteration deflickering framework applies
video deflickering to the latent space of diffusion models, effectively
preventing flicker accumulation in intermediate steps. Additionally, we propose
a video deflickering algorithm, named patch blending algorithm, that remaps
objects in different frames and blends them together to enhance video
consistency. One of the notable advantages of DiffSynth is its general
applicability to various video synthesis tasks, including text-guided video
stylization, fashion video synthesis, image-guided video stylization, video
restoring, and 3D rendering. In the task of text-guided video stylization, we
make it possible to synthesize high-quality videos without cherry-picking. The
experimental results demonstrate the effectiveness of DiffSynth. All videos can
be viewed on our project page. Source codes will also be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03712">Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience. (arXiv:2308.03712v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1">A. Emin Orhan</a></p>
<p>This paper asks whether current self-supervised learning methods, if
sufficiently scaled up, would be able to reach human-level visual object
recognition capabilities with the same type and amount of visual experience
humans learn from. Previous work on this question only considered the scaling
of data size. Here, we consider the simultaneous scaling of data size, model
size, and image resolution. We perform a scaling experiment with vision
transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K
hours of human-like video data (long, continuous, mostly egocentric videos)
with image resolutions of up to 476x476 pixels. The efficiency of masked
autoencoders (MAEs) as a self-supervised learning algorithm makes it possible
to run this scaling experiment on an unassuming academic budget. We find that
it is feasible to reach human-level object recognition capacity at sub-human
scales of model size, data size, and image size, if these factors are scaled up
simultaneously. To give a concrete example, we estimate that a 2.5B parameter
ViT model trained with 20K hours (2.3 years) of human-like video data with a
spatial resolution of 952x952 pixels should be able to reach roughly
human-level accuracy on ImageNet. Human-level competence is thus achievable for
a fundamental perceptual capability from human-like perceptual experience
(human-like in both amount and type) with extremely generic learning algorithms
and architectures and without any substantive inductive biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04152">Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions. (arXiv:2308.04152v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1">Kaihang Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1">Zhiqi Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Minghe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanwang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Multimodal Large Language Models (MLLMs) have recently sparked significant
interest, which demonstrates emergent capabilities to serve as a
general-purpose model for various vision-language tasks. However, existing
methods mainly focus on limited types of instructions with a single image as
visual context, which hinders the widespread availability of MLLMs. In this
paper, we introduce the I4 benchmark to comprehensively evaluate the
instruction following ability on complicated interleaved vision-language
instructions, which involve intricate image-text sequential context, covering a
diverse range of scenarios (e.g., visually-rich webpages/textbooks, lecture
slides, embodied dialogue). Systematic evaluation on our I4 benchmark reveals a
common defect of existing methods: the Visual Prompt Generator (VPG) trained on
image-captioning alignment objective tends to attend to common foreground
information for captioning but struggles to extract specific information
required by particular tasks. To address this issue, we propose a generic and
lightweight controllable knowledge re-injection module, which utilizes the
sophisticated reasoning ability of LLMs to control the VPG to conditionally
extract instruction-specific visual information and re-inject it into the LLM.
Further, we introduce an annotation-free cross-attention guided counterfactual
image training strategy to methodically learn the proposed module by
collaborating a cascade of foundation models. Enhanced by the proposed module
and training strategy, we present Cheetor, a Transformer-based MLLM that can
effectively handle a wide variety of interleaved vision-language instructions
and achieves state-of-the-art zero-shot performance across all tasks of I4,
without high-quality multimodal instruction tuning data. Cheetor also exhibits
competitive performance compared with state-of-the-art instruction tuned models
on MME benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04733">TextPainter: Multimodal Text Image Generation withVisual-harmony and Text-comprehension for Poster Design. (arXiv:2308.04733v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yifan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jinpeng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Min Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuanbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hongtao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tiezheng Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuning Jiang</a></p>
<p>Text design is one of the most critical procedures in poster design, as it
relies heavily on the creativity and expertise of humans to design text images
considering the visual harmony and text-semantic. This study introduces
TextPainter, a novel multimodal approach that leverages contextual visual
information and corresponding text semantics to generate text images.
Specifically, TextPainter takes the global-local background image as a hint of
style and guides the text image generation with visual harmony. Furthermore, we
leverage the language model and introduce a text comprehension module to
achieve both sentence-level and word-level style variations. Besides, we
construct the PosterT80K dataset, consisting of about 80K posters annotated
with sentence-level bounding boxes and text contents. We hope this dataset will
pave the way for further research on multimodal text image generation.
Extensive quantitative and qualitative experiments demonstrate that TextPainter
can generate visually-and-semantically-harmonious text images for posters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04868">InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering. (arXiv:2308.04868v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Canela_A/0/1/0/all/0/1">Antonio Canela</a>, <a href="http://arxiv.org/find/cs/1/au:+Caselles_P/0/1/0/all/0/1">Pol Caselles</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_I/0/1/0/all/0/1">Ibrar Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1">Eduard Ramon</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1">Jaime Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_Riera_J/0/1/0/all/0/1">Jordi S&#xe1;nchez-Riera</a>, <a href="http://arxiv.org/find/cs/1/au:+Triginer_G/0/1/0/all/0/1">Gil Triginer</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1">Francesc Moreno-Noguer</a></p>
<p>Recent advances in full-head reconstruction have been obtained by optimizing
a neural field through differentiable surface or volume rendering to represent
a single scene. While these techniques achieve an unprecedented accuracy, they
take several minutes, or even hours, due to the expensive optimization process
required. In this work, we introduce InstantAvatar, a method that recovers
full-head avatars from few images (down to just one) in a few seconds on
commodity hardware. In order to speed up the reconstruction process, we propose
a system that combines, for the first time, a voxel-grid neural field
representation with a surface renderer. Notably, a naive combination of these
two techniques leads to unstable optimizations that do not converge to valid
solutions. In order to overcome this limitation, we present a novel statistical
model that learns a prior distribution over 3D head signed distance functions
using a voxel-grid based architecture. The use of this prior model, in
combination with other design choices, results into a system that achieves 3D
head reconstructions with comparable accuracy as the state-of-the-art with a
100x speed-up.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04904">StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability. (arXiv:2308.04904v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kou_T/0/1/0/all/0/1">Tengchuan Kou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaohong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a></p>
<p>Video shakiness is an unpleasant distortion of User Generated Content (UGC)
videos, which is usually caused by the unstable hold of cameras. In recent
years, many video stabilization algorithms have been proposed, yet no specific
and accurate metric enables comprehensively evaluating the stability of videos.
Indeed, most existing quality assessment models evaluate video quality as a
whole without specifically taking the subjective experience of video stability
into consideration. Therefore, these models cannot measure the video stability
explicitly and precisely when severe shakes are present. In addition, there is
no large-scale video database in public that includes various degrees of shaky
videos with the corresponding subjective scores available, which hinders the
development of Video Quality Assessment for Stability (VQA-S). To this end, we
build a new database named StableDB that contains 1,952 diversely-shaky UGC
videos, where each video has a Mean Opinion Score (MOS) on the degree of video
stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S
model named StableVQA, which consists of three feature extractors to acquire
the optical flow, semantic, and blur features respectively, and a regression
layer to predict the final stability score. Extensive experiments demonstrate
that the StableVQA achieves a higher correlation with subjective opinions than
the existing VQA-S models and generic VQA models. The database and codes are
available at https://github.com/QMME/StableVQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04952">Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kai Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feigege Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1">Ye Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yutao Gao</a></p>
<p>Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic
Segmentation (FSS) to simultaneously segment unseen classes and seen classes
during evaluation. Previous works leverage additional branch or prototypical
aggregation to eliminate the constrained setting of FSS. However,
representation division and embedding prejudice, which heavily results in poor
performance of GFSS, have not been synthetical considered. We address the
aforementioned problems by jointing the prototypical kernel learning and
open-set foreground perception. Specifically, a group of learnable kernels is
proposed to perform segmentation with each kernel in charge of a stuff class.
Then, we explore to merge the prototypical learning to the update of base-class
kernels, which is consistent with the prototype knowledge aggregation of
few-shot novel classes. In addition, a foreground contextual perception module
cooperating with conditional bias based inference is adopted to perform
class-agnostic as well as open-set foreground detection, thus to mitigate the
embedding prejudice and prevent novel targets from being misclassified as
background. Moreover, we also adjust our method to the Class Incremental
Few-shot Semantic Segmentation (CIFSS) which takes the knowledge of novel
classes in a incremental stream. Extensive experiments on PASCAL-5i and
COCO-20i datasets demonstrate that our method performs better than previous
state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04995">IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models. (arXiv:2308.04995v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1">Fadi Boutros</a>, <a href="http://arxiv.org/find/cs/1/au:+Grebe_J/0/1/0/all/0/1">Jonas Henry Grebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1">Arjan Kuijper</a>, <a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1">Naser Damer</a></p>
<p>The availability of large-scale authentic face databases has been crucial to
the significant advances made in face recognition research over the past
decade. However, legal and ethical concerns led to the recent retraction of
many of these databases by their creators, raising questions about the
continuity of future face recognition research without one of its key
resources. Synthetic datasets have emerged as a promising alternative to
privacy-sensitive authentic data for face recognition development. However,
recent synthetic datasets that are used to train face recognition models suffer
either from limitations in intra-class diversity or cross-class (identity)
discrimination, leading to less optimal accuracies, far away from the
accuracies achieved by models trained on authentic data. This paper targets
this issue by proposing IDiff-Face, a novel approach based on conditional
latent diffusion models for synthetic identity generation with realistic
identity variations for face recognition training. Through extensive
evaluations, our proposed synthetic-based face recognition approach pushed the
limits of state-of-the-art performances, achieving, for example, 98.00%
accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the
recent synthetic-based face recognition solutions with 95.40% and bridging the
gap to authentic-based face recognition with 99.82% accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05068">Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors. (arXiv:2308.05068v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+C_S/0/1/0/all/0/1">Sneha Sree C</a>, <a href="http://arxiv.org/find/eess/1/au:+Fahim_M/0/1/0/all/0/1">Mohammad Al Fahim</a>, <a href="http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1">Keerthi Ram</a>, <a href="http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1">Mohanasankar Sivaprakasam</a></p>
<p>Many segmentation networks have been proposed for 3D volumetric segmentation
of tumors and organs at risk. Hospitals and clinical institutions seek to
accelerate and minimize the efforts of specialists in image segmentation.
Still, in case of errors generated by these networks, clinicians would have to
manually edit the generated segmentation maps. Given a 3D volume and its
putative segmentation map, we propose an approach to identify and measure
erroneous regions in the segmentation map. Our method can estimate error at any
point or node in a 3D mesh generated from a possibly erroneous volumetric
segmentation map, serving as a Quality Assurance tool. We propose a graph
neural network-based transformer based on the Nodeformer architecture to
measure and classify the segmentation errors at any point. We have evaluated
our network on a high-resolution micro-CT dataset of the human inner-ear bony
labyrinth structure by simulating erroneous 3D segmentation maps. Our network
incorporates a convolutional encoder to compute node-centric features from the
input micro-CT data, the Nodeformer to learn the latent graph embeddings, and a
Multi-Layer Perceptron (MLP) to compute and classify the node-wise errors. Our
network achieves a mean absolute error of ~0.042 over other Graph Neural
Networks (GNN) and an accuracy of 79.53% over other GNNs in estimating and
classifying the node-wise errors, respectively. We also put forth vertex-normal
prediction as a custom pretext task for pre-training the CNN encoder to improve
the network's overall performance. Qualitative analysis shows the efficiency of
our network in correctly classifying errors and reducing misclassifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19069">Multi-source adversarial transfer learning for ultrasound image segmentation with limited similarity. (arXiv:2305.19069v1 [eess.IV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yifu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hongru Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1">Rui Tao</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">Zhengyuan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1">Shimeng Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jiansong Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_N/0/1/0/all/0/1">Ning Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1">Wujin Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1">Zhanhu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a></p>
<p>Lesion segmentation of ultrasound medical images based on deep learning
techniques is a widely used method for diagnosing diseases. Although there is a
large amount of ultrasound image data in medical centers and other places,
labeled ultrasound datasets are a scarce resource, and it is likely that no
datasets are available for new tissues/organs. Transfer learning provides the
possibility to solve this problem, but there are too many features in natural
images that are not related to the target domain. As a source domain, redundant
features that are not conducive to the task will be extracted. Migration
between ultrasound images can avoid this problem, but there are few types of
public datasets, and it is difficult to find sufficiently similar source
domains. Compared with natural images, ultrasound images have less information,
and there are fewer transferable features between different ultrasound images,
which may cause negative transfer. To this end, a multi-source adversarial
transfer learning network for ultrasound image segmentation is proposed.
Specifically, to address the lack of annotations, the idea of adversarial
transfer learning is used to adaptively extract common features between a
certain pair of source and target domains, which provides the possibility to
utilize unlabeled ultrasound data. To alleviate the lack of knowledge in a
single source domain, multi-source transfer learning is adopted to fuse
knowledge from multiple source domains. In order to ensure the effectiveness of
the fusion and maximize the use of precious data, a multi-source domain
independent strategy is also proposed to improve the estimation of the target
domain data distribution, which further increases the learning ability of the
multi-source adversarial migration learning network in multiple domains.
</p>
</p>
</div>

    </div>
    </body>
    