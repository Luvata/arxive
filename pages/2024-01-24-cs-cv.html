<!DOCTYPE html>
<html>
<head>
<title>2024-01-24-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.10926">A VR Serious Game to Increase Empathy towards Students with Phonological Dyslexia. (arXiv:2401.10926v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alcalde_Llergo_J/0/1/0/all/0/1">Jos&#xe9; M. Alcalde-Llergo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeguas_Bolivar_E/0/1/0/all/0/1">Enrique Yeguas-Bol&#xed;var</a>, <a href="http://arxiv.org/find/cs/1/au:+Aparicio_Martinez_P/0/1/0/all/0/1">Pilar Aparicio-Mart&#xed;nez</a>, <a href="http://arxiv.org/find/cs/1/au:+Zingoni_A/0/1/0/all/0/1">Andrea Zingoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Taborri_J/0/1/0/all/0/1">Juri Taborri</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinzi_S/0/1/0/all/0/1">Sara Pinzi</a></p>
<p>Dyslexia is a neurodevelopmental disorder that is estimated to affect about
5-10% of the population. In particular, phonological dyslexia causes problems
in connecting the sounds of words with their written forms. This results in
difficulties such as slow reading speed, inaccurate reading, and difficulty
decoding unfamiliar words. Moreover, dyslexia can also be a challenging and
frustrating experience for students as they may feel misunderstood or
stigmatized by their peers or educators. For these reasons, the use of
compensatory tools and strategies is of crucial importance for dyslexic
students to have the same opportunities as non-dyslexic ones. However,
generally, people underestimate the problem and are not aware of the importance
of support methodologies. In the light of this, the main purpose of this paper
is to propose a virtual reality (VR) serious game through which teachers,
students and, in general, non-dyslexic people could understand which are some
of the issues of student with dyslexia and the fundamental utility of offering
support to them. In the game, players must create a potion by following a
recipe written in an alphabet that is specifically designed to replicate the
reading difficulties experienced by individuals with dyslexia. The task must be
solved first without any help and then by receiving supporting tools and
strategies with the idea that the player can put himself in the place of the
dyslexic person and understand the real need for support methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10962">One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaolong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiankun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xueran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xuesong Gao</a></p>
<p>Visual fine-tuning has garnered significant attention with the rise of
pre-trained vision models. The current prevailing method, full fine-tuning,
suffers from the issue of knowledge forgetting as it focuses solely on fitting
the downstream training set. In this paper, we propose a novel weight
rollback-based fine-tuning method called OLOR (One step Learning, One step
Review). OLOR combines fine-tuning with optimizers, incorporating a weight
rollback term into the weight update term at each step. This ensures
consistency in the weight range of upstream and downstream models, effectively
mitigating knowledge forgetting and enhancing fine-tuning performance. In
addition, a layer-wise penalty is presented to employ penalty decay and the
diversified decay rate to adjust the weight rollback levels of layers for
adapting varying downstream tasks. Through extensive experiments on various
tasks such as image classification, object detection, semantic segmentation,
and instance segmentation, we demonstrate the general applicability and
state-of-the-art performance of our proposed OLOR. Code is available at
https://github.com/rainbow-xiao/OLOR-AAAI-2024.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10967">HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations. (arXiv:2401.10967v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Serrano_D/0/1/0/all/0/1">Danzel Serrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Szymkowiak_J/0/1/0/all/0/1">Jakub Szymkowiak</a>, <a href="http://arxiv.org/find/cs/1/au:+Musialski_P/0/1/0/all/0/1">Przemyslaw Musialski</a></p>
<p>Recently proposed methods for implicitly representing signals such as images,
scenes, or geometries using coordinate-based neural network architectures often
do not leverage the choice of activation functions, or do so only to a limited
extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC),
a novel activation function with a controllable sharpness parameter. Unlike any
previous activations, HOSC has been specifically designed to better capture
sudden changes in the input signal, and hence sharp or acute features of the
underlying data, as well as smooth low-frequency transitions. Due to its
simplicity and modularity, HOSC offers a plug-and-play functionality that can
be easily incorporated into any existing method employing a neural network as a
way of implicitly representing a signal. We benchmark HOSC against other
popular activations in an array of general tasks, empirically showing an
improvement in the quality of obtained representations, provide the
mathematical motivation behind the efficacy of HOSC, and discuss its
limitations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11002">Fast Registration of Photorealistic Avatars for VR Facial Animation. (arXiv:2401.11002v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_C/0/1/0/all/0/1">Chaitanya Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Shaojie Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Te-Li Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1">Jason Saragih</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Shih-En Wei</a></p>
<p>Virtual Reality (VR) bares promise of social interactions that can feel more
immersive than other media. Key to this is the ability to accurately animate a
photorealistic avatar of one's likeness while wearing a VR headset. Although
high quality registration of person-specific avatars to headset-mounted camera
(HMC) images is possible in an offline setting, the performance of generic
realtime models are significantly degraded. Online registration is also
challenging due to oblique camera views and differences in modality. In this
work, we first show that the domain gap between the avatar and headset-camera
images is one of the primary sources of difficulty, where a transformer-based
architecture achieves high accuracy on domain-consistent data, but degrades
when the domain-gap is re-introduced. Building on this finding, we develop a
system design that decouples the problem into two parts: 1) an iterative
refinement module that takes in-domain inputs, and 2) a generic avatar-guided
image-to-image style transfer module that is conditioned on current estimation
of expression and head pose. These two modules reinforce each other, as image
style transfer becomes easier when close-to-ground-truth examples are shown,
and better domain-gap removal helps registration. Our system produces
high-quality results efficiently, obviating the need for costly offline
registration to generate personalized labels. We validate the accuracy and
efficiency of our approach through extensive experiments on a commodity
headset, demonstrating significant improvements over direct regression methods
as well as offline registration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11008">Helmholtz-Decomposition and Optical Flow: A new method to characterize GCamP recordings. (arXiv:2401.11008v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gerstenberger_M/0/1/0/all/0/1">Michael Gerstenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Juestel_D/0/1/0/all/0/1">Dominic Juestel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bodea_S/0/1/0/all/0/1">Silviu Bodea</a></p>
<p>During deep sleep and under anaesthesia spontaneous patterns of cortical
activation frequently take the form of slow travelling waves. Slow wave sleep
is an important cognitive state especially because of its relevance for memory
consolidation. However, despite extensive research the exact mechanisms are
still ill-understood. Novel methods such as high speed widefield imaging of
GCamP activity offer new potentials. Here we show how data recorded from
transgenic mice under anesthesia can be processed to analyze sources, sinks and
patterns of flow. To make the best possible use of the data novel means of data
processing are necessary. Therefore, we (1) give a an brief account on
processes that play a role in generating slow waves and demonstrate (2) a novel
approach to characterize its patterns in GCamP recordings. While slow waves are
highly variable, it shows that some are surprisingly similar. To enable
quantitative means of analysis and examine the structure of such prototypical
events we propose a novel approach for the characterization of slow waves: The
Helmholtz-Decomposition of gradient-based Dense Optical Flow of the pixeldense
GCamP contrast (df/f). It allows to detect the sources and sinks of activation
and discern them from global patterns of neural flow. Aggregated features can
be analyzed with variational autoencoders. The results unravel regularities
between slow waves and shows how they relate to the experimental conditions.
The approach reveals a complex topology of different features in latent slow
wave space and identifies prototypical examples for each stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11035">Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content Counterfactually. (arXiv:2401.11035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bethany_M/0/1/0/all/0/1">Mazal Bethany</a>, <a href="http://arxiv.org/find/cs/1/au:+Wherry_B/0/1/0/all/0/1">Brandon Wherry</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishwamitra_N/0/1/0/all/0/1">Nishant Vishwamitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1">Peyman Najafirad</a></p>
<p>Social media platforms are being increasingly used by malicious actors to
share unsafe content, such as images depicting sexual activity, cyberbullying,
and self-harm. Consequently, major platforms use artificial intelligence (AI)
and human moderation to obfuscate such images to make them safer. Two critical
needs for obfuscating unsafe images is that an accurate rationale for
obfuscating image regions must be provided, and the sensitive regions should be
obfuscated (\textit{e.g.} blurring) for users' safety. This process involves
addressing two key problems: (1) the reason for obfuscating unsafe images
demands the platform to provide an accurate rationale that must be grounded in
unsafe image-specific attributes, and (2) the unsafe regions in the image must
be minimally obfuscated while still depicting the safe regions. In this work,
we address these key issues by first performing visual reasoning by designing a
visual reasoning model (VLM) conditioned on pre-trained unsafe image
classifiers to provide an accurate rationale grounded in unsafe image
attributes, and then proposing a counterfactual explanation algorithm that
minimally identifies and obfuscates unsafe regions for safe viewing, by first
utilizing an unsafe image classifier attribution matrix to guide segmentation
for a more optimal subregion segmentation followed by an informed greedy search
to determine the minimum number of subregions required to modify the
classifier's output based on attribution score. Extensive experiments on
uncurated data from social networks emphasize the efficacy of our proposed
method. We make our code available at:
https://github.com/SecureAIAutonomyLab/ConditionalVLM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11061">PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Limoyo_O/0/1/0/all/0/1">Oliver Limoyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jimmy Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivkin_D/0/1/0/all/0/1">Dmitriy Rivkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1">Jonathan Kelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Gregory Dudek</a></p>
<p>We introduce PhotoBot, a framework for automated photo acquisition based on
an interplay between high-level human language guidance and a robot
photographer. We propose to communicate photography suggestions to the user via
a reference picture that is retrieved from a curated gallery. We exploit a
visual language model (VLM) and an object detector to characterize reference
pictures via textual descriptions and use a large language model (LLM) to
retrieve relevant reference pictures based on a user's language query through
text-based reasoning. To correspond the reference picture and the observed
scene, we exploit pre-trained features from a vision transformer capable of
capturing semantic similarity across significantly varying images. Using these
features, we compute pose adjustments for an RGB-D camera by solving a
Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world
manipulator equipped with a wrist camera. Our user studies show that photos
taken by PhotoBot are often more aesthetically pleasing than those taken by
users themselves, as measured by human feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11062">Learned Image resizing with efficient training (LRET) facilitates improved performance of large-scale digital histopathology image classification models. (arXiv:2401.11062v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alom_M/0/1/0/all/0/1">Md Zahangir Alom</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quynh T. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Orr_B/0/1/0/all/0/1">Brent A. Orr</a></p>
<p>Histologic examination plays a crucial role in oncology research and
diagnostics. The adoption of digital scanning of whole slide images (WSI) has
created an opportunity to leverage deep learning-based image classification
methods to enhance diagnosis and risk stratification. Technical limitations of
current approaches to training deep convolutional neural networks (DCNN) result
in suboptimal model performance and make training and deployment of
comprehensive classification models unobtainable. In this study, we introduce a
novel approach that addresses the main limitations of traditional
histopathology classification model training. Our method, termed Learned
Resizing with Efficient Training (LRET), couples efficient training techniques
with image resizing to facilitate seamless integration of larger histology
image patches into state-of-the-art classification models while preserving
important structural information.
</p>
<p>We used the LRET method coupled with two distinct resizing techniques to
train three diverse histology image datasets using multiple diverse DCNN
architectures. Our findings demonstrate a significant enhancement in
classification performance and training efficiency. Across the spectrum of
experiments, LRET consistently outperforms existing methods, yielding a
substantial improvement of 15-28% in accuracy for a large-scale, multiclass
tumor classification task consisting of 74 distinct brain tumor types. LRET not
only elevates classification accuracy but also substantially reduces training
times, unlocking the potential for faster model development and iteration. The
implications of this work extend to broader applications within medical imaging
and beyond, where efficient integration of high-resolution images into deep
learning pipelines is paramount for driving advancements in research and
clinical practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11067">Make-A-Shape: a Ten-Million-scale 3D Shape Model. (arXiv:2401.11067v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1">Ka-Hei Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1">Aditya Sanghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampini_A/0/1/0/all/0/1">Arianna Rampini</a>, <a href="http://arxiv.org/find/cs/1/au:+Malekshan_K/0/1/0/all/0/1">Kamal Rahimi Malekshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengzhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1">Hooman Shayani</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chi-Wing Fu</a></p>
<p>Significant progress has been made in training large generative models for
natural language and images. Yet, the advancement of 3D generative models is
hindered by their substantial resource demands for training, along with
inefficient, non-compact, and less expressive representations. This paper
introduces Make-A-Shape, a new 3D generative model designed for efficient
training on a vast scale, capable of utilizing 10 millions publicly-available
shapes. Technical-wise, we first innovate a wavelet-tree representation to
compactly encode shapes by formulating the subband coefficient filtering scheme
to efficiently exploit coefficient relations. We then make the representation
generatable by a diffusion model by devising the subband coefficients packing
scheme to layout the representation in a low-resolution grid. Further, we
derive the subband adaptive training strategy to train our model to effectively
learn to generate coarse and detail wavelet coefficients. Last, we extend our
framework to be controlled by additional input conditions to enable it to
generate shapes from assorted modalities, e.g., single/multi-view images, point
clouds, and low-resolution voxels. In our extensive set of experiments, we
demonstrate various applications, such as unconditional generation, shape
completion, and conditional generation on a wide range of modalities. Our
approach not only surpasses the state of the art in delivering high-quality
results but also efficiently generates shapes within a few seconds, often
achieving this in just 2 seconds for most conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11078">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures. (arXiv:2401.11078v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hyder_R/0/1/0/all/0/1">Rakib Hyder</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_Z/0/1/0/all/0/1">Ziwei Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guojun Qi</a></p>
<p>Recent advances in 3D avatar generation have gained significant attentions.
These breakthroughs aim to produce more realistic animatable avatars, narrowing
the gap between virtual and real-world experiences. Most of existing works
employ Score Distillation Sampling (SDS) loss, combined with a differentiable
renderer and text condition, to guide a diffusion model in generating 3D
avatars. However, SDS often generates oversmoothed results with few facial
details, thereby lacking the diversity compared with ancestral sampling. On the
other hand, other works generate 3D avatar from a single image, where the
challenges of unwanted lighting effects, perspective views, and inferior image
quality make them difficult to reliably reconstruct the 3D face meshes with the
aligned complete textures. In this paper, we propose a novel 3D avatar
generation approach termed UltrAvatar with enhanced fidelity of geometry, and
superior quality of physically based rendering (PBR) textures without unwanted
lighting. To this end, the proposed approach presents a diffuse color
extraction model and an authenticity guided texture diffusion model. The former
removes the unwanted lighting effects to reveal true diffuse colors so that the
generated avatars can be rendered under various lighting conditions. The latter
follows two gradient-based guidances for generating PBR textures to render
diverse face-identity features and details better aligning with 3D mesh
geometry. We demonstrate the effectiveness and robustness of the proposed
method, outperforming the state-of-the-art methods by a large margin in the
experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11085">Adaptive Global-Local Representation Learning and Selection for Cross-Domain Facial Expression Recognition. (arXiv:2401.11085v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuefang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuhao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zeke Zexi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianshui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Domain shift poses a significant challenge in Cross-Domain Facial Expression
Recognition (CD-FER) due to the distribution variation across different
domains. Current works mainly focus on learning domain-invariant features
through global feature adaptation, while neglecting the transferability of
local features. Additionally, these methods lack discriminative supervision
during training on target datasets, resulting in deteriorated feature
representation in target domain. To address these limitations, we propose an
Adaptive Global-Local Representation Learning and Selection (AGLRLS) framework.
The framework incorporates global-local adversarial adaptation and
semantic-aware pseudo label generation to enhance the learning of
domain-invariant and discriminative feature during training. Meanwhile, a
global-local prediction consistency learning is introduced to improve
classification results during inference. Specifically, the framework consists
of separate global-local adversarial learning modules that learn
domain-invariant global and local features independently. We also design a
semantic-aware pseudo label generation module, which computes semantic labels
based on global and local features. Moreover, a novel dynamic threshold
strategy is employed to learn the optimal thresholds by leveraging independent
prediction of global and local features, ensuring filtering out the unreliable
pseudo labels while retaining reliable ones. These labels are utilized for
model optimization through the adversarial learning process in an end-to-end
manner. During inference, a global-local prediction consistency module is
developed to automatically learn an optimal result from multiple predictions.
We conduct comprehensive experiments and analysis based on a fair evaluation
benchmark. The results demonstrate that the proposed framework outperforms the
current competing methods by a substantial margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11110">VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE. (arXiv:2401.11110v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haonan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>Unsupervised video object learning seeks to decompose video scenes into
structural object representations without any supervision from depth, optical
flow, or segmentation. We present VONet, an innovative approach that is
inspired by MONet. While utilizing a U-Net architecture, VONet employs an
efficient and effective parallel attention inference process, generating
attention masks for all slots simultaneously. Additionally, to enhance the
temporal consistency of each mask across consecutive video frames, VONet
develops an object-wise sequential VAE framework. The integration of these
innovative encoder-side techniques, in conjunction with an expressive
transformer-based decoder, establishes VONet as the leading unsupervised method
for object learning across five MOVI datasets, encompassing videos of diverse
complexities. Code is available at https://github.com/hnyu/vonet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11114">DengueNet: Dengue Prediction using Spatiotemporal Satellite Imagery for Resource-Limited Countries. (arXiv:2401.11114v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuo_K/0/1/0/all/0/1">Kuan-Ting Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1">Dana Moukheiber</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordonez_S/0/1/0/all/0/1">Sebastian Cajas Ordonez</a>, <a href="http://arxiv.org/find/cs/1/au:+Restrepo_D/0/1/0/all/0/1">David Restrepo</a>, <a href="http://arxiv.org/find/cs/1/au:+Paddo_A/0/1/0/all/0/1">Atika Rahman Paddo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tsung-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Moukheiber_L/0/1/0/all/0/1">Lama Moukheiber</a>, <a href="http://arxiv.org/find/cs/1/au:+Moukheiber_M/0/1/0/all/0/1">Mira Moukheiber</a>, <a href="http://arxiv.org/find/cs/1/au:+Moukheiber_S/0/1/0/all/0/1">Sulaiman Moukheiber</a>, <a href="http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1">Saptarshi Purkayastha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1">Po-Chih Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1">Leo Anthony Celi</a></p>
<p>Dengue fever presents a substantial challenge in developing countries where
sanitation infrastructure is inadequate. The absence of comprehensive
healthcare systems exacerbates the severity of dengue infections, potentially
leading to life-threatening circumstances. Rapid response to dengue outbreaks
is also challenging due to limited information exchange and integration. While
timely dengue outbreak forecasts have the potential to prevent such outbreaks,
the majority of dengue prediction studies have predominantly relied on data
that impose significant burdens on individual countries for collection. In this
study, our aim is to improve health equity in resource-constrained countries by
exploring the effectiveness of high-resolution satellite imagery as a
nontraditional and readily accessible data source. By leveraging the wealth of
publicly available and easily obtainable satellite imagery, we present a
scalable satellite extraction framework based on Sentinel Hub, a cloud-based
computing platform. Furthermore, we introduce DengueNet, an innovative
architecture that combines Vision Transformer, Radiomics, and Long Short-term
Memory to extract and integrate spatiotemporal features from satellite images.
This enables dengue predictions on an epi-week basis. To evaluate the
effectiveness of our proposed method, we conducted experiments on five
municipalities in Colombia. We utilized a dataset comprising 780
high-resolution Sentinel-2 satellite images for training and evaluation. The
performance of DengueNet was assessed using the mean absolute error (MAE)
metric. Across the five municipalities, DengueNet achieved an average MAE of
43.92. Our findings strongly support the efficacy of satellite imagery as a
valuable resource for dengue prediction, particularly in informing public
health policies within countries where manually collected data is scarce and
dengue virus prevalence is severe.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11115">MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation. (arXiv:2401.11115v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1">Nhat M. Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1">Kehong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1">Michael Bi Mi</a></p>
<p>Controllable generation of 3D human motions becomes an important topic as the
world embraces digital transformation. Existing works, though making promising
progress with the advent of diffusion models, heavily rely on meticulously
captured and annotated (e.g., text) high-quality motion corpus, a
resource-intensive endeavor in the real world. This motivates our proposed
MotionMix, a simple yet effective weakly-supervised diffusion model that
leverages both noisy and unannotated motion sequences. Specifically, we
separate the denoising objectives of a diffusion model into two stages:
obtaining conditional rough motion approximations in the initial $T-T^*$ steps
by learning the noisy annotated motions, followed by the unconditional
refinement of these preliminary motions during the last $T^*$ steps using
unannotated motions. Notably, though learning from two sources of imperfect
data, our model does not compromise motion generation quality compared to fully
supervised approaches that access gold data. Extensive experiments on several
benchmarks demonstrate that our MotionMix, as a versatile framework,
consistently achieves state-of-the-art performances on text-to-motion,
action-to-motion, and music-to-dance tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11122">Spatial Structure Constraints for Weakly Supervised Semantic Segmentation. (arXiv:2401.11122v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yazhou Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xingguo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zechao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1">Liqiang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jinhui Tang</a></p>
<p>The image-level label has prevailed in weakly supervised semantic
segmentation tasks due to its easy availability. Since image-level labels can
only indicate the existence or absence of specific categories of objects,
visualization-based techniques have been widely adopted to provide object
location clues. Considering class activation maps (CAMs) can only locate the
most discriminative part of objects, recent approaches usually adopt an
expansion strategy to enlarge the activation area for more integral object
localization. However, without proper constraints, the expanded activation will
easily intrude into the background region. In this paper, we propose spatial
structure constraints (SSC) for weakly supervised semantic segmentation to
alleviate the unwanted object over-activation of attention expansion.
Specifically, we propose a CAM-driven reconstruction module to directly
reconstruct the input image from deep CAM features, which constrains the
diffusion of last-layer object attention by preserving the coarse spatial
structure of the image content. Moreover, we propose an activation
self-modulation module to refine CAMs with finer spatial structure details by
enhancing regional consistency. Without external saliency models to provide
background clues, our approach achieves 72.7\% and 47.0\% mIoU on the PASCAL
VOC 2012 and COCO datasets, respectively, demonstrating the superiority of our
proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11123">Uncertainty-aware Bridge based Mobile-Former Network for Event-based Pattern Recognition. (arXiv:2401.11123v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haoxiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chengguo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yabin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jin Tang</a></p>
<p>The mainstream human activity recognition (HAR) algorithms are developed
based on RGB cameras, which are easily influenced by low-quality images (e.g.,
low illumination, motion blur). Meanwhile, the privacy protection issue caused
by ultra-high definition (HD) RGB cameras aroused more and more people's
attention. Inspired by the success of event cameras which perform better on
high dynamic range, no motion blur, and low energy consumption, we propose to
recognize human actions based on the event stream. We propose a lightweight
uncertainty-aware information propagation based Mobile-Former network for
efficient pattern recognition, which aggregates the MobileNet and Transformer
network effectively. Specifically, we first embed the event images using a stem
network into feature representations, then, feed them into uncertainty-aware
Mobile-Former blocks for local and global feature learning and fusion. Finally,
the features from MobileNet and Transformer branches are concatenated for
pattern recognition. Extensive experiments on multiple event-based recognition
datasets fully validated the effectiveness of our model. The source code of
this work will be released at
https://github.com/Event-AHU/Uncertainty_aware_MobileFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11124">EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions. (arXiv:2401.11124v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinodinos_D/0/1/0/all/0/1">Dimitrios Sinodinos</a>, <a href="http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1">Narges Armanfard</a></p>
<p>Multitask learning (MTL) has gained prominence for its ability to jointly
predict multiple tasks, achieving better per-task performance while using fewer
per-task model parameters than single-task learning. More recently,
decoder-focused architectures have considerably improved multitask performance
by refining task predictions using the features of other related tasks.
However, most of these refinement methods fail to simultaneously capture local
and global task-specific representations, as well as cross-task patterns in a
parameter-efficient manner. In this paper, we introduce the Efficient Multitask
Affinity Learning Network (EMA-Net), which is a lightweight framework that
enhances the task refinement capabilities of multitask networks. EMA-Net
adeptly captures local, global, and cross-task interactions using our novel
Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in
its ability to manipulate task affinity matrices in a manner that is optimally
suited to apply parameter-efficient grouped convolutions without worrying about
information loss. Our results show that we achieve state-of-the-art MTL
performance for CNN-based decoder-focused models while using substantially
fewer model parameters. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11140">Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end Object Detection. (arXiv:2401.11140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yuantao Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1">Ping Yin</a></p>
<p>Few-shot object detection(FSOD) aims to design methods to adapt object
detectors efficiently with only few annotated samples. Fine-tuning has been
shown to be an effective and practical approach. However, previous works often
take the classical base-novel two stage fine-tuning procedure but ignore the
implicit stability-plasticity contradiction among different modules.
Specifically, the random re-initialized classifiers need more plasticity to
adapt to novel samples. The other modules inheriting pre-trained weights demand
more stability to reserve their class-agnostic knowledge. Regular fine-tuning
which couples the optimization of these two parts hurts the model
generalization in FSOD scenarios. In this paper, we find that this problem is
prominent in the end-to-end object detector Sparse R-CNN for its
multi-classifier cascaded architecture. We propose to mitigate this
contradiction by a new three-stage fine-tuning procedure by introducing an
addtional plasticity classifier fine-tuning(PCF) stage. We further design the
multi-source ensemble(ME) technique to enhance the generalization of the model
in the final fine-tuning stage. Extensive experiments verify that our method is
effective in regularizing Sparse R-CNN, outperforming previous methods in the
FSOD benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11143">Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ioannides_G/0/1/0/all/0/1">Georgios Ioannides</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1">Aman Chadha</a>, <a href="http://arxiv.org/find/cs/1/au:+Elkins_A/0/1/0/all/0/1">Aaron Elkins</a></p>
<p>We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM's
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11144">Towards Open-World Gesture Recognition. (arXiv:2401.11144v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junxiao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_M/0/1/0/all/0/1">Matthias De Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai &quot;Orson&quot; Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1">Enmin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1">Ran Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Suda_N/0/1/0/all/0/1">Naveen Suda</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazarewicz_M/0/1/0/all/0/1">Maciej Lazarewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kristensson_P/0/1/0/all/0/1">Per Ola Kristensson</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlson_A/0/1/0/all/0/1">Amy Karlson</a>, <a href="http://arxiv.org/find/cs/1/au:+Strasnick_E/0/1/0/all/0/1">Evan Strasnick</a></p>
<p>Static machine learning methods in gesture recognition assume that training
and test data come from the same underlying distribution. However, in
real-world applications involving gesture recognition on wrist-worn devices,
data distribution may change over time. We formulate this problem of adapting
recognition models to new tasks, where new data patterns emerge, as open-world
gesture recognition (OWGR). We propose leveraging continual learning to make
machine learning models adaptive to new tasks without degrading performance on
previously learned tasks. However, the exploration of parameters for questions
around when and how to train and deploy recognition models requires
time-consuming user studies and is sometimes impractical. To address this
challenge, we propose a design engineering approach that enables offline
analysis on a collected large-scale dataset with various parameters and
compares different continual learning methods. Finally, design guidelines are
provided to enhance the development of an open-world wrist-worn gesture
recognition process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11150">Simultaneous Gesture Classification and Localization with an Automatic Gesture Annotation Model. (arXiv:2401.11150v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junxiao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1">Ran Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlson_A/0/1/0/all/0/1">Amy Karlson</a>, <a href="http://arxiv.org/find/cs/1/au:+Strasnick_E/0/1/0/all/0/1">Evan Strasnick</a></p>
<p>Training a real-time gesture recognition model heavily relies on annotated
data. However, manual data annotation is costly and demands substantial human
effort. In order to address this challenge, we propose a novel annotation model
that can automatically annotate gesture classes and identify their temporal
ranges. Our ablation study demonstrates that our annotation model design
surpasses the baseline in terms of both gesture classification accuracy (3-4\%
improvement) and localization accuracy (71-75\% improvement). We believe that
this annotation model has immense potential to improve the training of
downstream gesture recognition models using unlabeled datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11170">Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images. (arXiv:2401.11170v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kuofeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yang Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>Large vision-language models (VLMs) such as GPT-4 have achieved exceptional
performance across various multi-modal tasks. However, the deployment of VLMs
necessitates substantial energy consumption and computational resources. Once
attackers maliciously induce high energy consumption and latency time
(energy-latency cost) during inference of VLMs, it will exhaust computational
resources. In this paper, we explore this attack surface about availability of
VLMs and aim to induce high energy-latency cost during inference of VLMs. We
find that high energy-latency cost during inference of VLMs can be manipulated
by maximizing the length of generated sequences. To this end, we propose
verbose images, with the goal of crafting an imperceptible perturbation to
induce VLMs to generate long sentences during inference. Concretely, we design
three loss objectives. First, a loss is proposed to delay the occurrence of
end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop
generating further tokens. Moreover, an uncertainty loss and a token diversity
loss are proposed to increase the uncertainty over each generated token and the
diversity among all tokens of the whole generated sequence, respectively, which
can break output dependency at token-level and sequence-level. Furthermore, a
temporal weight adjustment algorithm is proposed, which can effectively balance
these losses. Extensive experiments demonstrate that our verbose images can
increase the length of generated sequences by 7.87 times and 8.56 times
compared to original images on MS-COCO and ImageNet datasets, which presents
potential challenges for various applications. Our code is available at
https://github.com/KuofengGao/Verbose_Images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11174">Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1">Nicol&#xe1;s Ayobi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1">Santiago Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1">Alejandra P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1">Isabela Hern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Aparicio_N/0/1/0/all/0/1">Nicol&#xe1;s Aparicio</a>, <a href="http://arxiv.org/find/cs/1/au:+Dessevres_E/0/1/0/all/0/1">Eug&#xe9;nie Dessevres</a>, <a href="http://arxiv.org/find/cs/1/au:+Pena_S/0/1/0/all/0/1">Sebasti&#xe1;n Pe&#xf1;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Santander_J/0/1/0/all/0/1">Jessica Santander</a>, <a href="http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1">Juan Ignacio Caicedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1">Nicol&#xe1;s Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1">Pablo Arbel&#xe1;ez</a></p>
<p>This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach enables a multi-level comprehension
of surgical activities, encompassing long-term tasks such as surgical phases
and steps recognition and short-term tasks including surgical instrument
segmentation and atomic visual actions detection. To exploit our proposed
benchmark, we introduce the Transformers for Actions, Phases, Steps, and
Instrument Segmentation (TAPIS) model, a general architecture that combines a
global video feature extractor with localized region proposals from an
instrument segmentation model to tackle the multi-granularity of our benchmark.
Through extensive experimentation, we demonstrate the impact of including
segmentation annotations in short-term recognition tasks, highlight the varying
granularity requirements of each task, and establish TAPIS's superiority over
previously proposed baselines and conventional CNN-based models. Additionally,
we validate the robustness of our method across multiple public benchmarks,
confirming the reliability and applicability of our dataset. This work
represents a significant step forward in Endoscopic Vision, offering a novel
and comprehensive framework for future research towards a holistic
understanding of surgical procedures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11204">Towards Category Unification of 3D Single Object Tracking on Point Clouds. (arXiv:2401.11204v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jiahao Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1">Xudong Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xueyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chae_D/0/1/0/all/0/1">Dong-Kyu Chae</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1">Fei Xie</a></p>
<p>Category-specific models are provenly valuable methods in 3D single object
tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such
over-specialized model designs incur redundant parameters, thus limiting the
broader applicability of 3D SOT task. This paper first introduces unified
models that can simultaneously track objects across all categories using a
single network with shared model parameters. Specifically, we propose to
explicitly encode distinct attributes associated to different object
categories, enabling the model to adapt to cross-category data. We find that
the attribute variances of point cloud objects primarily occur from the varying
size and shape (e.g., large and square vehicles v.s. small and slender humans).
Based on this observation, we design a novel point set representation learning
network inheriting transformer architecture, termed AdaFormer, which adaptively
encodes the dynamically varying shape and size information from cross-category
data in a unified manner. We further incorporate the size and shape prior
derived from the known template targets into the model's inputs and learning
objective, facilitating the learning of unified representation. Equipped with
such designs, we construct two category-unified models SiamCUT and
MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong
generalization and training stability. Furthermore, our category-unified models
outperform the category-specific counterparts by a significant margin (e.g., on
KITTI dataset, 12% and 3% performance gains on the Siamese and motion
paradigms). Our code will be available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11224">Susceptibility of Adversarial Attack on Medical Image Segmentation Models. (arXiv:2401.11224v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zhongxuan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1">Leo Xu</a></p>
<p>The nature of deep neural networks has given rise to a variety of attacks,
but little work has been done to address the effect of adversarial attacks on
segmentation models trained on MRI datasets. In light of the grave consequences
that such attacks could cause, we explore four models from the U-Net family and
examine their responses to the Fast Gradient Sign Method (FGSM) attack. We
conduct FGSM attacks on each of them and experiment with various schemes to
conduct the attacks. In this paper, we find that medical imaging segmentation
models are indeed vulnerable to adversarial attacks and that there is a
negligible correlation between parameter size and adversarial attack success.
Furthermore, we show that using a different loss function than the one used for
training yields higher adversarial attack success, contrary to what the FGSM
authors suggested. In future efforts, we will conduct the experiments detailed
in this paper with more segmentation models and different attacks. We will also
attempt to find ways to counteract the attacks by using model ensembles or
special data augmentations. Our code is available at
https://github.com/ZhongxuanWang/adv_attk
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11228">Unifying Visual and Vision-Language Tracking via Contrastive Learning. (arXiv:2401.11228v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yinchao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yuyang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenfei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianzhu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Mengxue Kang</a></p>
<p>Single object tracking aims to locate the target object in a video sequence
according to the state specified by different modal references, including the
initial bounding box (BBOX), natural language (NL), or both (NL+BBOX). Due to
the gap between different modalities, most existing trackers are designed for
single or partial of these reference settings and overspecialize on the
specific modality. Differently, we present a unified tracker called UVLTrack,
which can simultaneously handle all three reference settings (BBOX, NL,
NL+BBOX) with the same parameters. The proposed UVLTrack enjoys several merits.
First, we design a modality-unified feature extractor for joint visual and
language feature learning and propose a multi-modal contrastive loss to align
the visual and language features into a unified semantic space. Second, a
modality-adaptive box head is proposed, which makes full use of the target
reference to mine ever-changing scenario features dynamically from video
contexts and distinguish the target in a contrastive way, enabling robust
performance in different reference settings. Extensive experimental results
demonstrate that UVLTrack achieves promising performance on seven visual
tracking datasets, three vision-language tracking datasets, and three visual
grounding datasets. Codes and models will be open-sourced at
https://github.com/OpenSpaceAI/UVLTrack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11239">Product-Level Try-on: Characteristics-preserving Try-on with Realistic Clothes Shading and Wrinkles. (arXiv:2401.11239v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yanlong Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Han Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1">Jiaxu Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>Image-based virtual try-on systems,which fit new garments onto human
portraits,are gaining research attention.An ideal pipeline should preserve the
static features of clothes(like textures and logos)while also generating
dynamic elements(e.g.shadows,folds)that adapt to the model's pose and
environment.Previous works fail specifically in generating dynamic features,as
they preserve the warped in-shop clothes trivially with predicted an alpha mask
by composition.To break the dilemma of over-preserving and textures losses,we
propose a novel diffusion-based Product-level virtual try-on pipeline,\ie
PLTON, which can preserve the fine details of logos and embroideries while
producing realistic clothes shading and wrinkles.The main insights are in three
folds:1)Adaptive Dynamic Rendering:We take a pre-trained diffusion model as a
generative prior and tame it with image features,training a dynamic extractor
from scratch to generate dynamic tokens that preserve high-fidelity semantic
information. Due to the strong generative power of the diffusion prior,we can
generate realistic clothes shadows and wrinkles.2)Static Characteristics
Transformation: High-frequency Map(HF-Map)is our fundamental insight for static
representation.PLTON first warps in-shop clothes to the target model pose by a
traditional warping network,and uses a high-pass filter to extract an HF-Map
for preserving static cloth features.The HF-Map is used to generate modulation
maps through our static extractor,which are injected into a fixed U-net to
synthesize the final result.To enhance retention,a Two-stage Blended Denoising
method is proposed to guide the diffusion process for correct spatial layout
and color.PLTON is finetuned only with our collected small-size try-on
dataset.Extensive quantitative and qualitative experiments on 1024 768 datasets
demonstrate the superiority of our framework in mimicking real clothes
dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11243">LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation. (arXiv:2401.11243v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranjan_N/0/1/0/all/0/1">Navin Ranjan</a>, <a href="http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1">Andreas Savakis</a></p>
<p>Vision transformers (ViTs) have demonstrated remarkable performance across
various visual tasks. However, ViT models suffer from substantial computational
and memory requirements, making it challenging to deploy them on
resource-constrained platforms. Quantization is a popular approach for reducing
model size, but most studies mainly focus on equal bit-width quantization for
the entire network, resulting in sub-optimal solutions. While there are few
works on mixed precision quantization (MPQ) for ViTs, they typically rely on
search space-based methods or employ mixed precision arbitrarily. In this
paper, we introduce LRP-QViT, an explainability-based method for assigning
mixed-precision bit allocations to different layers based on their importance
during classification. Specifically, to measure the contribution score of each
layer in predicting the target class, we employ the Layer-wise Relevance
Propagation (LRP) method. LRP assigns local relevance at the output layer and
propagates it through all layers, distributing the relevance until it reaches
the input layers. These relevance scores serve as indicators for computing the
layer contribution score. Additionally, we have introduced a clipped
channel-wise quantization aimed at eliminating outliers from post-LayerNorm
activations to alleviate severe inter-channel variations. To validate and
assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer
models on various datasets. Our experimental findings demonstrate that both our
fixed-bit and mixed-bit post-training quantization methods surpass existing
models in the context of 4-bit and 6-bit quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11256">Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT. (arXiv:2401.11256v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Moriakov_N/0/1/0/all/0/1">Nikita Moriakov</a>, <a href="http://arxiv.org/find/physics/1/au:+Sonke_J/0/1/0/all/0/1">Jan-Jakob Sonke</a>, <a href="http://arxiv.org/find/physics/1/au:+Teuwen_J/0/1/0/all/0/1">Jonas Teuwen</a></p>
<p>Cone Beam CT (CBCT) is an essential imaging modality nowadays, but the image
quality of CBCT still lags behind the high quality standards established by the
conventional Computed Tomography. We propose LIRE+, a learned iterative scheme
for fast and memory-efficient CBCT reconstruction, which is a substantially
faster and more parameter-efficient alternative to the recently proposed LIRE
method. LIRE+ is a rotationally-equivariant multiscale learned invertible
primal-dual iterative scheme for CBCT reconstruction. Memory usage is optimized
by relying on simple reversible residual networks in primal/dual cells and
patch-wise computations inside the cells during forward and backward passes,
while increased inference speed is achieved by making the primal-dual scheme
multiscale so that the reconstruction process starts at low resolution and with
low resolution primal/dual latent vectors. A LIRE+ model was trained and
validated on a set of 260 + 22 thorax CT scans and tested using a set of 142
thorax CT scans with additional evaluation with and without finetuning on an
out-of-distribution set of 79 Head and Neck (HN) CT scans. Our method surpasses
classical and deep learning baselines, including LIRE, on the thorax test set.
For a similar inference time and with only 37 % of the parameter budget, LIRE+
achieves a +0.2 dB PSNR improvement over LIRE, while being able to match the
performance of LIRE in 45 % less inference time and with 28 % of the parameter
budget. Rotational equivariance ensures robustness of LIRE+ to patient
orientation, while LIRE and other deep learning baselines suffer from
substantial performance degradation when patient orientation is unusual. On the
HN dataset in the absence of finetuning, LIRE+ is generally comparable to LIRE
in performance apart from a few outlier cases, whereas after identical
finetuning LIRE+ demonstates a +1.02 dB PSNR improvement over LIRE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11261">Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weiguo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1">Deng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jinqiao Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1">Jirong Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1">Gangnan Yuan</a></p>
<p>Diffusion models (DMs) are a type of generative model that has a huge impact
on image synthesis and beyond. They achieve state-of-the-art generation results
in various generative tasks. A great diversity of conditioning inputs, such as
text or bounding boxes, are accessible to control the generation. In this work,
we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as
feature conditioning to guide the denoising process. Based on set theory, we
provide a comprehensive theoretical analysis that shows that conditional latent
distribution based on features and classes is significantly different, so that
conditional latent distribution on features produces fewer defect generations
than conditioning on classes. Two diffusion models conditioned on the Gaussian
mixture model are trained separately for comparison. Experiments support our
findings. A novel gradient function called the negative Gaussian mixture
gradient (NGMG) is proposed and applied in diffusion model training with an
additional classifier. Training stability has improved. We also theoretically
prove that NGMG shares the same benefit as the Earth Mover distance
(Wasserstein) as a more sensible cost function when learning distributions
supported by low-dimensional manifolds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11284">Evaluating Driver Readiness in Conditionally Automated Vehicles from Eye-Tracking Data and Head Pose. (arXiv:2401.11284v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1">Mostafa Kazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1">Mahdi Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Azarmi_M/0/1/0/all/0/1">Mohsen Azarmi</a></p>
<p>As automated driving technology advances, the role of the driver to resume
control of the vehicle in conditionally automated vehicles becomes increasingly
critical. In the SAE Level 3 or partly automated vehicles, the driver needs to
be available and ready to intervene when necessary. This makes it essential to
evaluate their readiness accurately. This article presents a comprehensive
analysis of driver readiness assessment by combining head pose features and
eye-tracking data. The study explores the effectiveness of predictive models in
evaluating driver readiness, addressing the challenges of dataset limitations
and limited ground truth labels. Machine learning techniques, including LSTM
architectures, are utilised to model driver readiness based on the
Spatio-temporal status of the driver's head pose and eye gaze. The experiments
in this article revealed that a Bidirectional LSTM architecture, combining both
feature sets, achieves a mean absolute error of 0.363 on the DMD dataset,
demonstrating superior performance in assessing driver readiness. The modular
architecture of the proposed model also allows the integration of additional
driver-specific features, such as steering wheel activity, enhancing its
adaptability and real-world applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.04852">Deep Tiny Network for Recognition-Oriented Face Image Quality Assessment. (arXiv:2106.04852v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baoyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Min Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a></p>
<p>Face recognition has made significant progress in recent years due to deep
convolutional neural networks (CNN). In many face recognition (FR) scenarios,
face images are acquired from a sequence with huge intra-variations. These
intra-variations, which are mainly affected by the low-quality face images,
cause instability of recognition performance. Previous works have focused on
ad-hoc methods to select frames from a video or use face image quality
assessment (FIQA) methods, which consider only a particular or combination of
several distortions.
</p>
<p>In this work, we present an efficient non-reference image quality assessment
for FR that directly links image quality assessment (IQA) and FR. More
specifically, we propose a new measurement to evaluate image quality without
any reference. Based on the proposed quality measurement, we propose a deep
Tiny Face Quality network (tinyFQnet) to learn a quality prediction function
from data.
</p>
<p>We evaluate the proposed method for different powerful FR models on two
classical video-based (or template-based) benchmark: IJB-B and YTF. Extensive
experiments show that, although the tinyFQnet is much smaller than the others,
the proposed method outperforms state-of-the-art quality assessment methods in
terms of effectiveness and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.13718">Digital Fingerprinting of Microstructures. (arXiv:2203.13718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Michael D. White</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarakanov_A/0/1/0/all/0/1">Alexander Tarakanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Race_C/0/1/0/all/0/1">Christopher P. Race</a>, <a href="http://arxiv.org/find/cs/1/au:+Withers_P/0/1/0/all/0/1">Philip J. Withers</a>, <a href="http://arxiv.org/find/cs/1/au:+Law_K/0/1/0/all/0/1">Kody J.H. Law</a></p>
<p>Finding efficient means of fingerprinting microstructural information is a
critical step towards harnessing data-centric machine learning approaches. A
statistical framework is systematically developed for compressed
characterisation of a population of images, which includes some classical
computer vision methods as special cases. The focus is on materials
microstructure. The ultimate purpose is to rapidly fingerprint sample images in
the context of various high-throughput design/make/test scenarios. This
includes, but is not limited to, quantification of the disparity between
microstructures for quality control, classifying microstructures, predicting
materials properties from image data and identifying potential processing
routes to engineer new materials with specific properties. Here, we consider
microstructure classification and utilise the resulting features over a range
of related machine learning tasks, namely supervised, semi-supervised, and
unsupervised learning.
</p>
<p>The approach is applied to two distinct datasets to illustrate various
aspects and some recommendations are made based on the findings. In particular,
methods that leverage transfer learning with convolutional neural networks
(CNNs), pretrained on the ImageNet dataset, are generally shown to outperform
other methods. Additionally, dimensionality reduction of these CNN-based
fingerprints is shown to have negligible impact on classification accuracy for
the supervised learning approaches considered. In situations where there is a
large dataset with only a handful of images labelled, graph-based label
propagation to unlabelled data is shown to be favourable over discarding
unlabelled data and performing supervised learning. In particular, label
propagation by Poisson learning is shown to be highly effective at low label
rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.03842">From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion. (arXiv:2204.03842v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weiguang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chaolong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jianan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuyao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1">Amir Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>While weakly supervised multi-view face reconstruction (MVR) is garnering
increased attention, one critical issue still remains open: how to effectively
fuse multiple image information to reconstruct high-precision 3D models. In
this regard, we propose a novel model called Deep Fusion MVR (DF-MVR) to
reconstruct high-precision 3D facial shapes from multi-view images.
Specifically, we introduce MulEn-Unet, a multi-view encoding to single decoding
framework with skip connections and attention. This design allows for the
extraction, integration, and compensation of deep features with attention from
multi-view images. Furthermore, we adopt the involution kernel to enrich deep
fusion features with channel features. In addition, we develop the face parse
network to learn, identify, and emphasize the critical common face area within
multi-view images. Experiments on Pixel-Face and Bosphorus datasets indicate
the superiority of our model. Without 3D annotation, DF-MVR achieves 5.2% and
3.0% RMSE improvement over the existing weakly supervised MVRs respectively on
Pixel-Face and Bosphorus dataset. Code will be available publicly at
https://github.com/weiguangzhao/DF_MVR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14540">SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1">Feng Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangguang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1">Diana Marculescu</a></p>
<p>Recently, self-supervised Masked Autoencoders (MAE) have attracted
unprecedented attention for their impressive representation learning ability.
However, the pretext task, Masked Image Modeling (MIM), reconstructs the
missing local patches, lacking the global understanding of the image. This
paper extends MAE to a fully supervised setting by adding a supervised
classification branch, thereby enabling MAE to learn global features from
golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a
visible subset of image patches for classification, unlike the standard
supervised pre-training where all image patches are used. Through experiments,
we demonstrate that SupMAE is not only more training efficient but it also
learns more robust and transferable features. Specifically, SupMAE achieves
comparable performance with MAE using only 30% of compute when evaluated on
ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and
transfer learning performance outperforms MAE and standard supervised
pre-training counterparts. Codes are available at
https://github.com/enyac-group/supmae.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.13082">PST: Plant segmentation transformer for 3D point clouds of rapeseed plants at the podding stage. (arXiv:2206.13082v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1">Ruiming Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhihong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pengyao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cen_H/0/1/0/all/0/1">Haiyan Cen</a></p>
<p>Segmentation of plant point clouds to obtain high-precise morphological
traits is essential for plant phenotyping. Although the fast development of
deep learning has boosted much research on segmentation of plant point clouds,
previous studies mainly focus on the hard voxelization-based or
down-sampling-based methods, which are limited to segmenting simple plant
organs. Segmentation of complex plant point clouds with a high spatial
resolution still remains challenging. In this study, we proposed a deep
learning network plant segmentation transformer (PST) to achieve the semantic
and instance segmentation of rapeseed plants point clouds acquired by handheld
laser scanning (HLS) with the high spatial resolution, which can characterize
the tiny siliques as the main traits targeted. PST is composed of: (i) a
dynamic voxel feature encoder (DVFE) to aggregate the point features with the
raw spatial resolution; (ii) the dual window sets attention blocks to capture
the contextual information; and (iii) a dense feature propagation module to
obtain the final dense point feature map. The results proved that PST and
PST-PointGroup (PG) achieved superior performance in semantic and instance
segmentation tasks. For the semantic segmentation, the mean IoU, mean
Precision, mean Recall, mean F1-score, and overall accuracy of PST were 93.96%,
97.29%, 96.52%, 96.88%, and 97.07%, achieving an improvement of 7.62%, 3.28%,
4.8%, 4.25%, and 3.88% compared to the second-best state-of-the-art network
PAConv. For instance segmentation, PST-PG reached 89.51%, 89.85%, 88.83% and
82.53% in mCov, mWCov, mPerc90, and mRec90, achieving an improvement of 2.93%,
2.21%, 1.99%, and 5.9% compared to the original PG. This study proves that the
deep-learning-based point cloud segmentation method has a great potential for
resolving dense plant point clouds with complex morphological traits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.00067">Rethinking Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2207.00067v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1">Masanori Suganuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1">Takayuki Okatani</a></p>
<p>Unsupervised domain adaptation (UDA) adapts a model trained on one domain
(called source) to a novel domain (called target) using only unlabeled data.
Due to its high annotation cost, researchers have developed many UDA methods
for semantic segmentation, which assume no labeled sample is available in the
target domain. We question the practicality of this assumption for two reasons.
First, after training a model with a UDA method, we must somehow verify the
model before deployment. Second, UDA methods have at least a few
hyper-parameters that need to be determined. The surest solution to these is to
evaluate the model using validation data, i.e., a certain amount of labeled
target-domain samples. This question about the basic assumption of UDA leads us
to rethink UDA from a data-centric point of view. Specifically, we assume we
have access to a minimum level of labeled data. Then, we ask how much is
necessary to find good hyper-parameters of existing UDA methods. We then
consider what if we use the same data for supervised training of the same
model, e.g., finetuning. We conducted experiments to answer these questions
with popular scenarios, {GTA5, SYNTHIA}$\rightarrow$Cityscapes. We found that
i) choosing good hyper-parameters needs only a few labeled images for some UDA
methods whereas a lot more for others; and ii) simple finetuning works
surprisingly well; it outperforms many UDA methods if only several dozens of
labeled images are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.05481">High-Frequency Space Diffusion Models for Accelerated MRI. (arXiv:2208.05481v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cao_C/0/1/0/all/0/1">Chentao Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1">Zhuo-Xu Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shaonan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1">Taijin Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1">Hairong Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1">Dong Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1">Yanjie Zhu</a></p>
<p>Diffusion models with continuous stochastic differential equations (SDEs)
have shown superior performances in image generation. It can serve as a deep
generative prior to solving the inverse problem in magnetic resonance (MR)
reconstruction. However, low-frequency regions of $k$-space data are typically
fully sampled in fast MR imaging, while existing diffusion models are performed
throughout the entire image or $k$-space, inevitably introducing uncertainty in
the reconstruction of low-frequency regions. Additionally, existing diffusion
models often demand substantial iterations to converge, resulting in
time-consuming reconstructions. To address these challenges, we propose a novel
SDE tailored specifically for MR reconstruction with the diffusion process in
high-frequency space (referred to as HFS-SDE). This approach ensures
determinism in the fully sampled low-frequency regions and accelerates the
sampling procedure of reverse diffusion. Experiments conducted on the publicly
available fastMRI dataset demonstrate that the proposed HFS-SDE method
outperforms traditional parallel imaging methods, supervised deep learning, and
existing diffusion models in terms of reconstruction accuracy and stability.
The fast convergence properties are also confirmed through theoretical and
experimental validation. Our code and weights are available at
https://github.com/Aboriginer/HFS-SDE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14571">Towards the Detection of Diffusion Model Deepfakes. (arXiv:2210.14571v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ricker_J/0/1/0/all/0/1">Jonas Ricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Damm_S/0/1/0/all/0/1">Simon Damm</a>, <a href="http://arxiv.org/find/cs/1/au:+Holz_T/0/1/0/all/0/1">Thorsten Holz</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1">Asja Fischer</a></p>
<p>In the course of the past few years, diffusion models (DMs) have reached an
unprecedented level of visual quality. However, relatively little attention has
been paid to the detection of DM-generated images, which is critical to prevent
adverse impacts on our society. In contrast, generative adversarial networks
(GANs), have been extensively studied from a forensic perspective. In this
work, we therefore take the natural next step to evaluate whether previous
methods can be used to detect images generated by DMs. Our experiments yield
two key findings: (1) state-of-the-art GAN detectors are unable to reliably
distinguish real from DM-generated images, but (2) re-training them on
DM-generated images allows for almost perfect detection, which remarkably even
generalizes to GANs. Together with a feature space analysis, our results lead
to the hypothesis that DMs produce fewer detectable artifacts and are thus more
difficult to detect compared to GANs. One possible reason for this is the
absence of grid-like frequency artifacts in DM-generated images, which are a
known weakness of GANs. However, we make the interesting observation that
diffusion models tend to underestimate high frequencies, which we attribute to
the learning objective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.02915">ESKNet-An enhanced adaptive selection kernel convolution for breast tumors segmentation. (arXiv:2211.02915v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1">Gongping Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1">Lu Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jianxun Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yin_X/0/1/0/all/0/1">Xiaotao Yin</a>, <a href="http://arxiv.org/find/eess/1/au:+Cui_L/0/1/0/all/0/1">Liang Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1">Yu Dai</a></p>
<p>Breast cancer is one of the common cancers that endanger the health of women
globally. Accurate target lesion segmentation is essential for early clinical
intervention and postoperative follow-up. Recently, many convolutional neural
networks (CNNs) have been proposed to segment breast tumors from ultrasound
images. However, the complex ultrasound pattern and the variable tumor shape
and size bring challenges to the accurate segmentation of the breast lesion.
Motivated by the selective kernel convolution, we introduce an enhanced
selective kernel convolution for breast tumor segmentation, which integrates
multiple feature map region representations and adaptively recalibrates the
weights of these feature map regions from the channel and spatial dimensions.
This region recalibration strategy enables the network to focus more on
high-contributing region features and mitigate the perturbation of less useful
regions. Finally, the enhanced selective kernel convolution is integrated into
U-net with deep supervision constraints to adaptively capture the robust
representation of breast tumors. Extensive experiments with twelve
state-of-the-art deep learning segmentation methods on three public breast
ultrasound datasets demonstrate that our method has a more competitive
segmentation performance in breast ultrasound images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07394">Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization. (arXiv:2211.07394v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhedong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Leigang Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>We investigate composed image retrieval with text feedback. Users gradually
look for the target of interest by moving from coarse to fine-grained feedback.
However, existing methods merely focus on the latter, i.e., fine-grained
search, by harnessing positive and negative pairs during training. This
pair-based paradigm only considers the one-to-one distance between a pair of
specific points, which is not aligned with the one-to-many coarse-grained
retrieval process and compromises the recall rate. In an attempt to fill this
gap, we introduce a unified learning approach to simultaneously modeling the
coarse- and fine-grained retrieval by considering the multi-grained
uncertainty. The key idea underpinning the proposed method is to integrate
fine- and coarse-grained retrieval as matching data points with small and large
fluctuations, respectively. Specifically, our method contains two modules:
uncertainty modeling and uncertainty regularization. (1) The uncertainty
modeling simulates the multi-grained queries by introducing identically
distributed fluctuations in the feature space. (2) Based on the uncertainty
modeling, we further introduce uncertainty regularization to adapt the matching
objective according to the fluctuation range. Compared with existing methods,
the proposed strategy explicitly prevents the model from pushing away potential
candidates in the early stage, and thus improves the recall rate. On the three
public datasets, i.e., FashionIQ, Fashion200k, and Shoes, the proposed method
has achieved +4.03%, +3.38%, and +2.40% Recall@50 accuracy over a strong
baseline, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08824">SMILEtrack: SiMIlarity LEarning for Occlusion-Aware Multiple Object Tracking. (arXiv:2211.08824v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Hsiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1">Jun-Wei Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Ping-Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Ching Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+So_H/0/1/0/all/0/1">Hung Hin So</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a></p>
<p>Despite recent progress in Multiple Object Tracking (MOT), several obstacles
such as occlusions, similar objects, and complex scenes remain an open
challenge. Meanwhile, a systematic study of the cost-performance tradeoff for
the popular tracking-by-detection paradigm is still lacking. This paper
introduces SMILEtrack, an innovative object tracker that effectively addresses
these challenges by integrating an efficient object detector with a Siamese
network-based Similarity Learning Module (SLM). The technical contributions of
SMILETrack are twofold. First, we propose an SLM that calculates the appearance
similarity between two objects, overcoming the limitations of feature
descriptors in Separate Detection and Embedding (SDE) models. The SLM
incorporates a Patch Self-Attention (PSA) block inspired by the vision
Transformer, which generates reliable features for accurate similarity
matching. Second, we develop a Similarity Matching Cascade (SMC) module with a
novel GATE function for robust object matching across consecutive video frames,
further enhancing MOT performance. Together, these innovations help SMILETrack
achieve an improved trade-off between the cost ({\em e.g.}, running speed) and
performance (e.g., tracking accuracy) over several existing state-of-the-art
benchmarks, including the popular BYTETrack method. SMILETrack outperforms
BYTETrack by 0.4-0.8 MOTA and 2.1-2.2 HOTA points on MOT17 and MOT20 datasets.
Code is available at https://github.com/pingyang1117/SMILEtrack_Official
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.09480">ArcAid: Analysis of Archaeological Artifacts using Drawings. (arXiv:2211.09480v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayon_O/0/1/0/all/0/1">Offry Hayon</a>, <a href="http://arxiv.org/find/cs/1/au:+Munger_S/0/1/0/all/0/1">Stefan M&#xfc;nger</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimshoni_I/0/1/0/all/0/1">Ilan Shimshoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Tal_A/0/1/0/all/0/1">Ayellet Tal</a></p>
<p>Archaeology is an intriguing domain for computer vision. It suffers not only
from shortage in (labeled) data, but also from highly-challenging data, which
is often extremely abraded and damaged. This paper proposes a novel
semi-supervised model for classification and retrieval of images of
archaeological artifacts. This model utilizes unique data that exists in the
domain -- manual drawings made by special artists. These are used during
training to implicitly transfer the domain knowledge from the drawings to their
corresponding images, improving their classification results. We show that
while learning how to classify, our model also learns how to generate drawings
of the artifacts, an important documentation task, which is currently performed
manually. Last but not least, we collected a new dataset of stamp-seals of the
Southern Levant. Our code and dataset are publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11174">Unveiling the Tapestry: the Interplay of Generalization and Forgetting in Continual Learning. (arXiv:2211.11174v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zenglin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jie_J/0/1/0/all/0/1">Jing Jie</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Ying Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1">Joo Hwee Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengmi Zhang</a></p>
<p>In AI, generalization refers to a model's ability to perform well on
out-of-distribution data related to the given task, beyond the data it was
trained on. For an AI agent to excel, it must also possess the continual
learning capability, whereby an agent incrementally learns to perform a
sequence of tasks without forgetting the previously acquired knowledge to solve
the old tasks. Intuitively, generalization within a task allows the model to
learn underlying features that can readily be applied to novel tasks,
facilitating quicker learning and enhanced performance in subsequent tasks
within a continual learning framework. Conversely, continual learning methods
often include mechanisms to mitigate catastrophic forgetting, ensuring that
knowledge from earlier tasks is retained. This preservation of knowledge over
tasks plays a role in enhancing generalization for the ongoing task at hand.
Despite the intuitive appeal of the interplay of both abilities, existing
literature on continual learning and generalization has proceeded separately.
In the preliminary effort to promote studies that bridge both fields, we first
present empirical evidence showing that each of these fields has a mutually
positive effect on the other. Next, building upon this finding, we introduce a
simple and effective technique known as Shape-Texture Consistency
Regularization (STCR), which caters to continual learning. STCR learns both
shape and texture representations for each task, consequently enhancing
generalization and thereby mitigating forgetting. Remarkably, extensive
experiments validate that our STCR, can be seamlessly integrated with existing
continual learning methods, where its performance surpasses these continual
learning methods in isolation or when combined with established generalization
techniques by a large margin. Our data and source code will be made publicly
available upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14835">CLID: Controlled-Length Image Descriptions with Limited Data. (arXiv:2211.14835v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1">Elad Hirsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Tal_A/0/1/0/all/0/1">Ayellet Tal</a></p>
<p>Controllable image captioning models generate human-like image descriptions,
enabling some kind of control over the generated captions. This paper focuses
on controlling the caption length, i.e. a short and concise description or a
long and detailed one. Since existing image captioning datasets contain mostly
short captions, generating long captions is challenging. To address the
shortage of long training examples, we propose to enrich the dataset with
varying-length self-generated captions. These, however, might be of varying
quality and are thus unsuitable for conventional training. We introduce a novel
training strategy that selects the data points to be used at different times
during the training. Our method dramatically improves the length-control
abilities, while exhibiting SoTA performance in terms of caption quality. Our
approach is general and is shown to be applicable also to paragraph generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05758">BEV-MAE: Bird&#x27;s Eye View Masked Autoencoders for Point Cloud Pre-training in Autonomous Driving Scenarios. (arXiv:2212.05758v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongtao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1">Shengxiang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1">Nan Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Existing LiDAR-based 3D object detection methods for autonomous driving
scenarios mainly adopt the training-from-scratch paradigm. Unfortunately, this
paradigm heavily relies on large-scale labeled data, whose collection can be
expensive and time-consuming. Self-supervised pre-training is an effective and
desirable way to alleviate this dependence on extensive annotated data. In this
work, we present BEV-MAE, an efficient masked autoencoder pre-training
framework for LiDAR-based 3D object detection in autonomous driving.
Specifically, we propose a bird's eye view (BEV) guided masking strategy to
guide the 3D encoder learning feature representation in a BEV perspective and
avoid complex decoder design during pre-training. Furthermore, we introduce a
learnable point token to maintain a consistent receptive field size of the 3D
encoder with fine-tuning for masked point cloud inputs. Based on the property
of outdoor point clouds in autonomous driving scenarios, i.e., the point clouds
of distant objects are more sparse, we propose point density prediction to
enable the 3D encoder to learn location information, which is essential for
object detection. Experimental results show that BEV-MAE surpasses prior
state-of-the-art self-supervised methods and achieves a favorably pre-training
efficiency. Furthermore, based on TransFusion-L, BEV-MAE achieves new
state-of-the-art LiDAR-based 3D object detection results, with 73.6 NDS and
69.6 mAP on the nuScenes benchmark. The source code will be released at
https://github.com/VDIGPKU/BEV-MAE
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08898">Recurrent Generic Contour-based Instance Segmentation with Progressive Learning. (arXiv:2301.08898v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Keyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wengang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yufei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jiajun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houqiang Li</a></p>
<p>Contour-based instance segmentation has been actively studied, thanks to its
flexibility and elegance in processing visual objects within complex
backgrounds. In this work, we propose a novel deep network architecture, i.e.,
PolySnake, for generic contour-based instance segmentation. Motivated by the
classic Snake algorithm, the proposed PolySnake achieves superior and robust
segmentation performance with an iterative and progressive contour refinement
strategy. Technically, PolySnake introduces a recurrent update operator to
estimate the object contour iteratively. It maintains a single estimate of the
contour that is progressively deformed toward the object boundary. At each
iteration, PolySnake builds a semantic-rich representation for the current
contour and feeds it to the recurrent operator for further contour adjustment.
Through the iterative refinements, the contour progressively converges to a
stable status that tightly encloses the object instance. Beyond the scope of
general instance segmentation, extensive experiments are conducted to validate
the effectiveness and generalizability of our PolySnake in two additional
specific task scenarios, including scene text detection and lane detection. The
results demonstrate that the proposed PolySnake outperforms the existing
advanced methods on several multiple prevalent benchmarks across the three
tasks. The codes and pre-trained models are available at
https://github.com/fh2019ustc/PolySnake
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05105">MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation. (arXiv:2303.05105v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">Minh-Quan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tam V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Trung-Nghia Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1">Thanh-Toan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh N. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a></p>
<p>Few-shot instance segmentation extends the few-shot learning paradigm to the
instance segmentation task, which tries to segment instance objects from a
query image with a few annotated examples of novel categories. Conventional
approaches have attempted to address the task via prototype learning, known as
point estimation. However, this mechanism depends on prototypes (\eg mean of
$K-$shot) for prediction, leading to performance instability. To overcome the
disadvantage of the point estimation mechanism, we propose a novel approach,
dubbed MaskDiff, which models the underlying conditional distribution of a
binary mask, which is conditioned on an object region and $K-$shot information.
Inspired by augmentation approaches that perturb data with Gaussian noise for
populating low data density regions, we model the mask distribution with a
diffusion probabilistic model. We also propose to utilize classifier-free
guided mask sampling to integrate category information into the binary mask
generation process. Without bells and whistles, our proposed method
consistently outperforms state-of-the-art methods on both base and novel
classes of the COCO dataset while simultaneously being more stable than
existing methods. The source code is available at:
https://github.com/minhquanlecs/MaskDiff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05123">Dominating Set Database Selection for Visual Place Recognition. (arXiv:2303.05123v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1">Anastasiia Kornilova</a>, <a href="http://arxiv.org/find/cs/1/au:+Moskalenko_I/0/1/0/all/0/1">Ivan Moskalenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Pushkin_T/0/1/0/all/0/1">Timofei Pushkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tojiboev_F/0/1/0/all/0/1">Fakhriddin Tojiboev</a>, <a href="http://arxiv.org/find/cs/1/au:+Tariverdizadeh_R/0/1/0/all/0/1">Rahim Tariverdizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1">Gonzalo Ferrer</a></p>
<p>This paper presents an approach for creating a visual place recognition (VPR)
database for localization in indoor environments from RGBD scanning sequences.
The proposed approach is formulated as a minimization problem in terms of
dominating set algorithm for graph, constructed from spatial information, and
referred as DominatingSet. Our algorithm shows better scene coverage in
comparison to other methodologies that are used for database creation. Also, we
demonstrate that using DominatingSet, a database size could be up to 250-1400
times smaller than the original scanning sequence while maintaining a recall
rate of more than 80% on testing sequences. We evaluated our algorithm on
7-scenes and BundleFusion datasets and an additionally recorded sequence in a
highly repetitive office setting. In addition, the database selection can
produce weakly-supervised labels for fine-tuning neural place recognition
algorithms to particular settings, improving even more their accuracy. The
paper also presents a fully automated pipeline for VPR database creation from
RGBD scanning sequences, as well as a set of metrics for VPR database
evaluation. The code and released data are available on our web-page~ --
https://prime-slam.github.io/place-recognition-db/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07064">A Generalized Multi-Modal Fusion Detection Framework. (arXiv:2303.07064v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Leichao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiuxian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1">Min Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1">Xiaoyu Mo</a></p>
<p>LiDAR point clouds have become the most common data source in autonomous
driving. However, due to the sparsity of point clouds, accurate and reliable
detection cannot be achieved in specific scenarios. Because of their
complementarity with point clouds, images are getting increasing attention.
Although with some success, existing fusion methods either perform hard fusion
or do not fuse in a direct manner. In this paper, we propose a generic 3D
detection framework called MMFusion, using multi-modal features. The framework
aims to achieve accurate fusion between LiDAR and images to improve 3D
detection in complex scenes. Our framework consists of two separate streams:
the LiDAR stream and the camera stream, which can be compatible with any
single-modal feature extraction network. The Voxel Local Perception Module in
the LiDAR stream enhances local feature representation, and then the
Multi-modal Feature Fusion Module selectively combines feature output from
different streams to achieve better fusion. Extensive experiments have shown
that our framework not only outperforms existing benchmarks but also improves
their detection, especially for detecting cyclists and pedestrians on KITTI
benchmarks, with strong robustness and generalization capabilities. Hopefully,
our work will stimulate more research into multi-modal fusion for autonomous
driving tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11681">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models. (arXiv:2303.11681v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a></p>
<p>Collecting and annotating images with pixel-wise labels is time-consuming and
laborious. In contrast, synthetic data can be freely available using a
generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that
it is possible to automatically obtain accurate semantic masks of synthetic
images generated by the Off-the-shelf Stable Diffusion model, which uses only
text-image pairs during training. Our approach, called DiffuMask, exploits the
potential of the cross-attention map between text and image, which is natural
and seamless to extend the text-driven image synthesis to semantic mask
generation. DiffuMask uses text-guided cross-attention information to localize
class/word-specific regions, which are combined with practical techniques to
create a novel high-resolution and class-discriminative pixel-wise mask. The
methods help to reduce data collection and annotation costs obviously.
Experiments demonstrate that the existing segmentation methods trained on
synthetic data of DiffuMask can achieve a competitive performance over the
counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),
DiffuMask presents promising performance, close to the stateof-the-art result
of real data (within 3% mIoU gap). Moreover, in the open-vocabulary
segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on
Unseen class of VOC 2012. The project website can be found at
https://weijiawu.github.io/DiffusionMask/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13472">Promptable Game Models: Text-Guided Game Simulation via Masked Diffusion Models. (arXiv:2303.13472v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Menapace_W/0/1/0/all/0/1">Willi Menapace</a>, <a href="http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1">Aliaksandr Siarohin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1">St&#xe9;phane Lathuili&#xe8;re</a>, <a href="http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1">Panos Achlioptas</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1">Elisa Ricci</a></p>
<p>Neural video game simulators emerged as powerful tools to generate and edit
videos. Their idea is to represent games as the evolution of an environment's
state driven by the actions of its agents. While such a paradigm enables users
to play a game action-by-action, its rigidity precludes more semantic forms of
control. To overcome this limitation, we augment game models with prompts
specified as a set of natural language actions and desired states. The result-a
Promptable Game Model (PGM)-makes it possible for a user to play the game by
prompting it with high- and low-level action sequences. Most captivatingly, our
PGM unlocks the director's mode, where the game is played by specifying goals
for the agents in the form of a prompt. This requires learning "game AI",
encapsulated by our animation model, to navigate the scene using high-level
constraints, play against an adversary, and devise a strategy to win a point.
To render the resulting state, we use a compositional NeRF representation
encapsulated in our synthesis model. To foster future research, we present
newly collected, annotated and calibrated Tennis and Minecraft datasets. Our
method significantly outperforms existing neural video game simulators in terms
of rendering quality and unlocks applications beyond the capabilities of the
current state of the art. Our framework, data, and models are available at
https://snap-research.github.io/promptable-game-models/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03047">ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1">Dong An</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenguan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Keji He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a></p>
<p>Vision-language navigation is a task that requires an agent to follow
instructions to navigate in environments. It becomes increasingly crucial in
the field of embodied AI, with potential applications in autonomous navigation,
search and rescue, and human-robot interaction. In this paper, we propose to
address a more practical yet challenging counterpart setting - vision-language
navigation in continuous environments (VLN-CE). To develop a robust VLN-CE
agent, we propose a new navigation framework, ETPNav, which focuses on two
critical skills: 1) the capability to abstract environments and generate
long-range navigation plans, and 2) the ability of obstacle-avoiding control in
continuous environments. ETPNav performs online topological mapping of
environments by self-organizing predicted waypoints along a traversed path,
without prior environmental experience. It privileges the agent to break down
the navigation procedure into high-level planning and low-level control.
Concurrently, ETPNav utilizes a transformer-based cross-modal planner to
generate navigation plans based on topological maps and instructions. The plan
is then performed through an obstacle-avoiding controller that leverages a
trial-and-error heuristic to prevent navigation from getting stuck in
obstacles. Experimental results demonstrate the effectiveness of the proposed
method. ETPNav yields more than 10% and 20% improvements over prior
state-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is
available at https://github.com/MarSaKi/ETPNav.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05482">Computational Pathology: A Survey Review and The Way Forward. (arXiv:2304.05482v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hosseini_M/0/1/0/all/0/1">Mahdi S. Hosseini</a>, <a href="http://arxiv.org/find/eess/1/au:+Bejnordi_B/0/1/0/all/0/1">Babak Ehteshami Bejnordi</a>, <a href="http://arxiv.org/find/eess/1/au:+Trinh_V/0/1/0/all/0/1">Vincent Quoc-Huy Trinh</a>, <a href="http://arxiv.org/find/eess/1/au:+Hasan_D/0/1/0/all/0/1">Danial Hasan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xingwen Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_T/0/1/0/all/0/1">Taehyo Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Haochen Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1">Theodore Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chinniah_K/0/1/0/all/0/1">Kajanan Chinniah</a>, <a href="http://arxiv.org/find/eess/1/au:+Maghsoudlou_S/0/1/0/all/0/1">Sina Maghsoudlou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1">Ryan Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1">Stephen Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1">Jiadai Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chan_L/0/1/0/all/0/1">Lyndon Chan</a>, <a href="http://arxiv.org/find/eess/1/au:+Khaki_S/0/1/0/all/0/1">Samir Khaki</a>, <a href="http://arxiv.org/find/eess/1/au:+Buin_A/0/1/0/all/0/1">Andrei Buin</a>, <a href="http://arxiv.org/find/eess/1/au:+Chaji_F/0/1/0/all/0/1">Fatemeh Chaji</a>, <a href="http://arxiv.org/find/eess/1/au:+Salehi_A/0/1/0/all/0/1">Ala Salehi</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_B/0/1/0/all/0/1">Bich Ngoc Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1">Dimitris Samaras</a>, <a href="http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1">Konstantinos N. Plataniotis</a></p>
<p>Computational Pathology CPath is an interdisciplinary science that augments
developments of computational approaches to analyze and model medical
histopathology images. The main objective for CPath is to develop
infrastructure and workflows of digital diagnostics as an assistive CAD system
for clinical pathology, facilitating transformational changes in the diagnosis
and treatment of cancer that are mainly address by CPath tools. With
evergrowing developments in deep learning and computer vision algorithms, and
the ease of the data flow from digital pathology, currently CPath is witnessing
a paradigm shift. Despite the sheer volume of engineering and scientific works
being introduced for cancer image analysis, there is still a considerable gap
of adopting and integrating these algorithms in clinical practice. This raises
a significant question regarding the direction and trends that are undertaken
in CPath. In this article we provide a comprehensive review of more than 800
papers to address the challenges faced in problem design all-the-way to the
application and implementation viewpoints. We have catalogued each paper into a
model-card by examining the key works and challenges faced to layout the
current landscape in CPath. We hope this helps the community to locate relevant
works and facilitate understanding of the field's future directions. In a
nutshell, we oversee the CPath developments in cycle of stages which are
required to be cohesively linked together to address the challenges associated
with such multidisciplinary science. We overview this cycle from different
perspectives of data-centric, model-centric, and application-centric problems.
We finally sketch remaining challenges and provide directions for future
technical developments and clinical integration of CPath
(https://github.com/AtlasAnalyticsLab/CPath_Survey).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06662">Deep Learning in Breast Cancer Imaging: A Decade of Progress and Future Directions. (arXiv:2304.06662v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1">Luyang Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1">Xiaoqi Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_A/0/1/0/all/0/1">Andong Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Chan_R/0/1/0/all/0/1">Ronald Chan</a>, <a href="http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1">Varut Vardhanabhuti</a>, <a href="http://arxiv.org/find/eess/1/au:+Chu_W/0/1/0/all/0/1">Winnie CW Chu</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1">Kwang-Ting Cheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Breast cancer has reached the highest incidence rate worldwide among all
malignancies since 2020. Breast imaging plays a significant role in early
diagnosis and intervention to improve the outcome of breast cancer patients. In
the past decade, deep learning has shown remarkable progress in breast cancer
imaging analysis, holding great promise in interpreting the rich information
and complex context of breast imaging modalities. Considering the rapid
improvement in deep learning technology and the increasing severity of breast
cancer, it is critical to summarize past progress and identify future
challenges to be addressed. This paper provides an extensive review of deep
learning-based breast cancer imaging research, covering studies on mammogram,
ultrasound, magnetic resonance imaging, and digital pathology images over the
past decade. The major deep learning methods and applications on imaging-based
screening, diagnosis, treatment response prediction, and prognosis are
elaborated and discussed. Drawn from the findings of this survey, we present a
comprehensive discussion of the challenges and potential avenues for future
research in deep learning-based breast cancer imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07444">The Art of Camouflage: Few-shot Learning for Animal Detection and Segmentation. (arXiv:2304.07444v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh-Danh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_A/0/1/0/all/0/1">Anh-Khoa Nguyen Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhat-Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Vinh-Tiep Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1">Thanh Duc Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1">Thanh-Toan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tam V. Nguyen</a></p>
<p>Camouflaged object detection and segmentation is a new and challenging
research topic in computer vision. There is a serious issue of lacking data of
camouflaged objects such as camouflaged animals in natural scenes. In this
paper, we address the problem of few-shot learning for camouflaged object
detection and segmentation. To this end, we first collect a new dataset,
CAMO-FS, for the benchmark. We then propose a novel method to efficiently
detect and segment the camouflaged objects in the images. In particular, we
introduce the instance triplet loss and the instance memory storage. The
extensive experiments demonstrated that our proposed method achieves
state-of-the-art performance on the newly collected dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14811">NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields. (arXiv:2304.14811v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Feihu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_S/0/1/0/all/0/1">Shaochen Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a></p>
<p>Labeling LiDAR point clouds for training autonomous driving is extremely
expensive and difficult. LiDAR simulation aims at generating realistic LiDAR
data with labels for training and verifying self-driving algorithms more
efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for
novel view synthesis using implicit reconstruction of 3D scenes. Inspired by
this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages
real-world information to generate realistic LIDAR point clouds. Different from
existing LiDAR simulators, we use real images and point cloud data collected by
self-driving cars to learn the 3D scene representation, point cloud generation
and label rendering. We verify the effectiveness of our NeRF-LiDAR by training
different 3D segmentation models on the generated LiDAR point clouds. It
reveals that the trained models are able to achieve similar accuracy when
compared with the same model trained on the real LiDAR data. Besides, the
generated data is capable of boosting the accuracy through pre-training which
helps reduce the requirements of the real labeled data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08891">Common Diffusion Noise Schedules and Sample Steps are Flawed. (arXiv:2305.08891v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shanchuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiashi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a></p>
<p>We discover that common diffusion noise schedules do not enforce the last
timestep to have zero signal-to-noise ratio (SNR), and some implementations of
diffusion samplers do not start from the last timestep. Such designs are flawed
and do not reflect the fact that the model is given pure Gaussian noise at
inference, creating a discrepancy between training and inference. We show that
the flawed design causes real problems in existing implementations. In Stable
Diffusion, it severely limits the model to only generate images with medium
brightness and prevents it from generating very bright and dark samples. We
propose a few simple fixes: (1) rescale the noise schedule to enforce zero
terminal SNR; (2) train the model with v prediction; (3) change the sampler to
always start from the last timestep; (4) rescale classifier-free guidance to
prevent over-exposure. These simple changes ensure the diffusion process is
congruent between training and inference and allow the model to generate
samples more faithful to the original data distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10124">Principal Uncertainty Quantification with Spatial Correlation for Image Restoration Problems. (arXiv:2305.10124v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belhasin_O/0/1/0/all/0/1">Omer Belhasin</a>, <a href="http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1">Yaniv Romano</a>, <a href="http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1">Daniel Freedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1">Ehud Rivlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1">Michael Elad</a></p>
<p>Uncertainty quantification for inverse problems in imaging has drawn much
attention lately. Existing approaches towards this task define uncertainty
regions based on probable values per pixel, while ignoring spatial correlations
within the image, resulting in an exaggerated volume of uncertainty. In this
paper, we propose PUQ (Principal Uncertainty Quantification) -- a novel
definition and corresponding analysis of uncertainty regions that takes into
account spatial relationships within the image, thus providing reduced volume
regions. Using recent advancements in generative models, we derive uncertainty
intervals around principal components of the empirical posterior distribution,
forming an ambiguity region that guarantees the inclusion of true unseen values
with a user-defined confidence probability. To improve computational efficiency
and interpretability, we also guarantee the recovery of true unseen values
using only a few principal directions, resulting in more informative
uncertainty regions. Our approach is verified through experiments on image
colorization, super-resolution, and inpainting; its effectiveness is shown
through comparison to baseline methods, demonstrating significantly tighter
uncertainty regions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16789">Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1">Xi Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yunhao Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tengwei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1">Rao Muhammad Anwer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lei Huang</a></p>
<p>Whitening loss offers a theoretical guarantee against feature collapse in
self-supervised learning (SSL) with joint embedding architectures. Typically,
it involves a hard whitening approach, transforming the embedding and applying
loss to the whitened output. In this work, we introduce Spectral Transformation
(ST), a framework to modulate the spectrum of embedding and to seek for
functions beyond whitening that can avoid dimensional collapse. We show that
whitening is a special instance of ST by definition, and our empirical
investigations unveil other ST instances capable of preventing collapse.
Additionally, we propose a novel ST instance named IterNorm with trace loss
(INTL). Theoretical analysis confirms INTL's efficacy in preventing collapse
and modulating the spectrum of embedding toward equal-eigenvalues during
optimization. Our experiments on ImageNet classification and COCO object
detection demonstrate INTL's potential in learning superior representations.
The code is available at https://github.com/winci-ai/INTL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06071">Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (arXiv:2306.06071v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanyam Jain</a></p>
<p>This paper implements and investigates popular adversarial attacks on the
YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the
YOLOv5 to adversarial attacks in the context of traffic and road sign
detection. The paper investigates the impact of different types of attacks,
including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the
Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&amp;W) attack,
the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD)
attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on
the accuracy of YOLOv5 in detecting traffic and road signs. The results show
that YOLOv5 is susceptible to these attacks, with misclassification rates
increasing as the magnitude of the perturbations increases. We also explain the
results using saliency maps. The findings of this paper have important
implications for the safety and reliability of object detection algorithms used
in traffic and transportation systems, highlighting the need for more robust
and secure models to ensure their effectiveness in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13301">Deep Omni-supervised Learning for Rib Fracture Detection from Chest Radiology Images. (arXiv:2306.13301v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Zhizhong Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Luyang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Huangjing Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng-Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Deep learning (DL)-based rib fracture detection has shown promise of playing
an important role in preventing mortality and improving patient outcome.
Normally, developing DL-based object detection models requires a huge amount of
bounding box annotation. However, annotating medical data is time-consuming and
expertise-demanding, making obtaining a large amount of fine-grained
annotations extremely infeasible. This poses a pressing need {for} developing
label-efficient detection models to alleviate radiologists' labeling burden. To
tackle this challenge, the literature on object detection has witnessed an
increase of weakly-supervised and semi-supervised approaches, yet still lacks a
unified framework that leverages various forms of fully-labeled,
weakly-labeled, and unlabeled data. In this paper, we present a novel
omni-supervised object detection network, ORF-Netv2, to leverage as much
available supervision as possible. Specifically, a multi-branch omni-supervised
detection head is introduced with each branch trained with a specific type of
supervision. A co-training-based dynamic label assignment strategy is then
proposed to enable flexible and robust learning from the weakly-labeled and
unlabeled data. Extensive evaluation was conducted for the proposed framework
with three rib fracture datasets on both chest CT and X-ray. By leveraging all
forms of supervision, ORF-Netv2 achieves mAPs of 34.7, 44.7, and 19.4 on the
three datasets, respectively, surpassing the baseline detector which uses only
box annotations by mAP gains of 3.8, 4.8, and 5.0, respectively. Furthermore,
ORF-Netv2 consistently outperforms other competitive label-efficient methods
over various scenarios, showing a promising framework for label-efficient
fracture detection. The code is available at:
https://github.com/zhizhongchai/ORF-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17778">Look, Remember and Reason: Grounded reasoning in videos with language models. (arXiv:2306.17778v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1">Apratim Bhattacharyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchal_S/0/1/0/all/0/1">Sunny Panchal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Mingu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1">Reza Pourreza</a>, <a href="http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1">Pulkit Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1">Roland Memisevic</a></p>
<p>Multi-modal language models (LM) have recently shown promising performance in
high-level reasoning tasks on videos. However, existing methods still fall
short in tasks like causal or compositional spatiotemporal reasoning over
actions, in which model predictions need to be grounded in fine-grained
low-level details, such as object motions and object interactions. In this
work, we propose training an LM end-to-end on low-level surrogate tasks,
including object detection, re-identification, and tracking, to endow the model
with the required low-level visual capabilities. We show that a two-stream
video encoder with spatiotemporal attention is effective at capturing the
required static and motion-based cues in the video. By leveraging the LM's
ability to perform the low-level surrogate tasks, we can cast reasoning in
videos as the three-step process of Look, Remember, Reason wherein visual
information is extracted using low-level visual skills step-by-step and then
integrated to arrive at a final answer. We demonstrate the effectiveness of our
framework on diverse visual reasoning tasks from the ACRE, CATER,
Something-Else and STAR datasets. Our approach is trainable end-to-end and
surpasses state-of-the-art task-specific methods across these tasks by a large
margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02273">Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression. (arXiv:2307.02273v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghorbel_A/0/1/0/all/0/1">Ahmed Ghorbel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1">Wassim Hamidouche</a>, <a href="http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1">Luce Morin</a></p>
<p>Recently, the performance of neural image compression (NIC) has steadily
improved thanks to the last line of study, reaching or outperforming
state-of-the-art conventional codecs. Despite significant progress, current NIC
methods still rely on ConvNet-based entropy coding, limited in modeling
long-range dependencies due to their local connectivity and the increasing
number of architectural biases and priors, resulting in complex underperforming
models with high decoding latency. Motivated by the efficiency investigation of
the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose
to enhance the latter, as first, with a more straightforward yet effective
Tranformer-based channel-wise auto-regressive prior model, resulting in an
absolute image compression transformer (ICT). Through the proposed ICT, we can
capture both global and local contexts from the latent representations and
better parameterize the distribution of the quantized latents. Further, we
leverage a learnable scaling module with a sandwich ConvNeXt-based
pre-/post-processor to accurately extract more compact latent codes while
reconstructing higher-quality images. Extensive experimental results on
benchmark datasets showed that the proposed framework significantly improves
the trade-off between coding efficiency and decoder complexity over the
versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM. Moreover, we provide model scaling studies to verify the
computational efficiency of our approach and conduct several objective and
subjective analyses to bring to the fore the performance gap between the
adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08279">Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation. (arXiv:2307.08279v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1">Wen Yan</a>, <a href="http://arxiv.org/find/eess/1/au:+Chiu_B/0/1/0/all/0/1">Bernard Chiu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_Z/0/1/0/all/0/1">Ziyi Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1">Qianye Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Syer_T/0/1/0/all/0/1">Tom Syer</a>, <a href="http://arxiv.org/find/eess/1/au:+Min_Z/0/1/0/all/0/1">Zhe Min</a>, <a href="http://arxiv.org/find/eess/1/au:+Punwani_S/0/1/0/all/0/1">Shonit Punwani</a>, <a href="http://arxiv.org/find/eess/1/au:+Emberton_M/0/1/0/all/0/1">Mark Emberton</a>, <a href="http://arxiv.org/find/eess/1/au:+Atkinson_D/0/1/0/all/0/1">David Atkinson</a>, <a href="http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1">Dean C. Barratt</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yipeng Hu</a></p>
<p>One of the distinct characteristics in radiologists' reading of
multiparametric prostate MR scans, using reporting systems such as PI-RADS
v2.1, is to score individual types of MR modalities, T2-weighted,
diffusion-weighted, and dynamic contrast-enhanced, and then combine these
image-modality-specific scores using standardised decision rules to predict the
likelihood of clinically significant cancer. This work aims to demonstrate that
it is feasible for low-dimensional parametric models to model such decision
rules in the proposed Combiner networks, without compromising the accuracy of
predicting radiologic labels: First, it is shown that either a linear mixture
model or a nonlinear stacking model is sufficient to model PI-RADS decision
rules for localising prostate cancer. Second, parameters of these (generalised)
linear models are proposed as hyperparameters, to weigh multiple networks that
independently represent individual image modalities in the Combiner network
training, as opposed to end-to-end modality ensemble. A HyperCombiner network
is developed to train a single image segmentation network that can be
conditioned on these hyperparameters during inference, for much improved
efficiency. Experimental results based on data from 850 patients, for the
application of automating radiologist labelling multi-parametric MR, compare
the proposed combiner networks with other commonly-adopted end-to-end networks.
Using the added advantages of obtaining and interpreting the modality combining
rules, in terms of the linear weights or odds-ratios on individual image
modalities, three clinical applications are presented for prostate cancer
segmentation, including modality availability assessment, importance
quantification and rule discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09997">TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition. (arXiv:2307.09997v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Funke_I/0/1/0/all/0/1">Isabel Funke</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1">Dominik Rivoir</a>, <a href="http://arxiv.org/find/cs/1/au:+Krell_S/0/1/0/all/0/1">Stefanie Krell</a>, <a href="http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1">Stefanie Speidel</a></p>
<p>To enable context-aware computer assistance in the operating room of the
future, cognitive systems need to understand automatically which surgical phase
is being performed by the medical team. The primary source of information for
surgical phase recognition is typically video, which presents two challenges:
extracting meaningful features from the video stream and effectively modeling
temporal information in the sequence of visual features. For temporal modeling,
attention mechanisms have gained popularity due to their ability to capture
long-range dependencies. In this paper, we explore design choices for attention
in existing temporal models for surgical phase recognition and propose a novel
approach that uses attention more effectively: TUNeS, an efficient and simple
temporal model that incorporates self-attention at the core of a convolutional
U-Net structure. In addition, we propose to train the feature extractor, a
standard CNN, together with an LSTM on preferably long video segments, i.e.,
with long temporal context. In our experiments, all temporal models performed
better on top of feature extractors that were trained with longer temporal
context. On these contextualized features, TUNeS achieves state-of-the-art
results on the Cholec80 dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15567">Panoptic Scene Graph Generation with Semantics-Prototype Learning. (arXiv:2307.15567v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yiming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">You Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lina Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1">Roger Zimmermann</a></p>
<p>Panoptic Scene Graph Generation (PSG) parses objects and predicts their
relationships (predicate) to connect human language and visual scenes. However,
different language preferences of annotators and semantic overlaps between
predicates lead to biased predicate annotations in the dataset, i.e. different
predicates for same object pairs. Biased predicate annotations make PSG models
struggle in constructing a clear decision plane among predicates, which greatly
hinders the real application of PSG models. To address the intrinsic bias
above, we propose a novel framework named ADTrans to adaptively transfer biased
predicate annotations to informative and unified ones. To promise consistency
and accuracy during the transfer process, we propose to measure the invariance
of representations in each predicate class, and learn unbiased prototypes of
predicates with different intensities. Meanwhile, we continuously measure the
distribution changes between each presentation and its prototype, and
constantly screen potential biased data. Finally, with the unbiased
predicate-prototype representation embedding space, biased annotations are
easily identified. Experiments show that ADTrans significantly improves the
performance of benchmark models, achieving a new state-of-the-art performance,
and shows great generalization and effectiveness on multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00783">Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking. (arXiv:2308.00783v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mingzhan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1">Guangxin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bin Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenhua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jinqing Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Huchuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a></p>
<p>Multi-Object Tracking (MOT) aims to detect and associate all desired objects
across frames. Most methods accomplish the task by explicitly or implicitly
leveraging strong cues (i.e., spatial and appearance information), which
exhibit powerful instance-level discrimination. However, when object occlusion
and clustering occur, spatial and appearance information will become ambiguous
simultaneously due to the high overlap among objects. In this paper, we
demonstrate this long-standing challenge in MOT can be efficiently and
effectively resolved by incorporating weak cues to compensate for strong cues.
Along with velocity direction, we introduce the confidence and height state as
potential weak cues. With superior performance, our method still maintains
Simple, Online and Real-Time (SORT) characteristics. Also, our method shows
strong generalization for diverse trackers and scenarios in a plug-and-play and
training-free manner. Significant and consistent improvements are observed when
applying our method to 5 different representative trackers. Further, with both
strong and weak cues, our method Hybrid-SORT achieves superior performance on
diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where
interaction and severe occlusion frequently happen with complex motions. The
code and models are available at https://github.com/ymzis69/HybridSORT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01738">Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution. (arXiv:2308.01738v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yeying Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Beibei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1">Wending Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wei Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1">Robby T. Tan</a></p>
<p>Visibility in hazy nighttime scenes is frequently reduced by multiple
factors, including low light, intense glow, light scattering, and the presence
of multicolored light sources. Existing nighttime dehazing methods often
struggle with handling glow or low-light conditions, resulting in either
excessively dark visuals or unsuppressed glow outputs. In this paper, we
enhance the visibility from a single nighttime haze image by suppressing glow
and enhancing low-light regions. To handle glow effects, our framework learns
from the rendered glow pairs. Specifically, a light source aware network is
proposed to detect light sources of night images, followed by the APSF
(Atmospheric Point Spread Function)-guided glow rendering. Our framework is
then trained on the rendered images, resulting in glow suppression. Moreover,
we utilize gradient-adaptive convolution, to capture edges and textures in hazy
scenes. By leveraging extracted edges and textures, we enhance the contrast of
the scene without losing important structural details. To boost low-light
intensity, our network learns an attention map, then adjusted by gamma
correction. This attention has high values on low-light regions and low values
on haze and glow regions. Extensive evaluation on real nighttime haze images,
demonstrates the effectiveness of our method. Our experiments demonstrate that
our method achieves a PSNR of 30.38dB, outperforming state-of-the-art methods
by 13% on GTA5 nighttime haze dataset. Our data and code is available at
https://github.com/jinyeying/nighttime_dehaze.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07314">Dual Associated Encoder for Face Restoration. (arXiv:2308.07314v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yu-Ju Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Lun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kelvin C.K. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Restoring facial details from low-quality (LQ) images has remained a
challenging problem due to its ill-posedness induced by various degradations in
the wild. The existing codebook prior mitigates the ill-posedness by leveraging
an autoencoder and learned codebook of high-quality (HQ) features, achieving
remarkable quality. However, existing approaches in this paradigm frequently
depend on a single encoder pre-trained on HQ data for restoring HQ images,
disregarding the domain gap between LQ and HQ images. As a result, the encoding
of LQ inputs may be insufficient, resulting in suboptimal performance. To
tackle this problem, we propose a novel dual-branch framework named DAEFR. Our
method introduces an auxiliary LQ branch that extracts crucial information from
the LQ inputs. Additionally, we incorporate association training to promote
effective synergy between the two branches, enhancing code prediction and
output quality. We evaluate the effectiveness of DAEFR on both synthetic and
real-world datasets, demonstrating its superior performance in restoring facial
details. Project page: https://liagm.github.io/DAEFR/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10610">Ultrafast and Ultralight Network-Based Intelligent System for Real-time Diagnosis of Ear diseases in Any Devices. (arXiv:2308.10610v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yubiao Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xinyu Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoqiang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Meiping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1">Haihua Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanmei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zefeng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenrui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenzhang Li</a></p>
<p>Traditional ear disease diagnosis heavily depends on experienced specialists
and specialized equipment, frequently resulting in misdiagnoses, treatment
delays, and financial burdens for some patients. Utilizing deep learning models
for efficient ear disease diagnosis has proven effective and affordable.
However, existing research overlooked model inference speed and parameter size
required for deployment. To tackle these challenges, we constructed a
large-scale dataset comprising eight ear disease categories and normal ear
canal samples from two hospitals. Inspired by ShuffleNetV2, we developed
Best-EarNet, an ultrafast and ultralight network enabling real-time ear disease
diagnosis. Best-EarNet incorporates the novel Local-Global Spatial Feature
Fusion Module which can capture global and local spatial information
simultaneously and guide the network to focus on crucial regions within feature
maps at various levels, mitigating low accuracy issues. Moreover, our network
uses multiple auxiliary classification heads for efficient parameter
optimization. With 0.77M parameters, Best-EarNet achieves an average frames per
second of 80 on CPU. Employing transfer learning and five-fold cross-validation
with 22,581 images from Hospital-1, the model achieves an impressive 95.23%
accuracy. External testing on 1,652 images from Hospital-2 validates its
performance, yielding 92.14% accuracy. Compared to state-of-the-art networks,
Best-EarNet establishes a new state-of-the-art (SOTA) in practical
applications. Most importantly, we developed an intelligent diagnosis system
called Ear Keeper, which can be deployed on common electronic devices. By
manipulating a compact electronic otoscope, users can perform comprehensive
scanning and diagnosis of the ear canal using real-time video. This study
provides a novel paradigm for ear endoscopy and other medical endoscopic image
recognition applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11737">Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape. (arXiv:2308.11737v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiacong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jiawei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wufei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jesslen_A/0/1/0/all/0/1">Artur Jesslen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1">Pengliang Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qixin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiehua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qihao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoding Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaushik_P/0/1/0/all/0/1">Prakhar Kaushik</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guofeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yushan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yawen Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1">Adam Kortylewski</a></p>
<p>Accurately estimating the 3D pose and shape is an essential step towards
understanding animal behavior, and can potentially benefit many downstream
applications, such as wildlife conservation. However, research in this area is
held back by the lack of a comprehensive and diverse dataset with high-quality
3D pose and shape annotations. In this paper, we propose Animal3D, the first
comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D
consists of 3379 images collected from 40 mammal species, high-quality
annotations of 26 keypoints, and importantly the pose and shape parameters of
the SMAL model. All annotations were labeled and checked manually in a
multi-stage process to ensure highest quality results. Based on the Animal3D
dataset, we benchmark representative shape and pose estimation models at: (1)
supervised learning from only the Animal3D data, (2) synthetic to real transfer
from synthetically generated images, and (3) fine-tuning human pose and shape
estimation models. Our experimental results demonstrate that predicting the 3D
shape and pose of animals across species remains a very challenging task,
despite significant advances in human pose estimation. Our results further
demonstrate that synthetic pre-training is a viable strategy to boost the model
performance. Overall, Animal3D opens new directions for facilitating future
research in animal 3D pose and shape estimation, and is publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00733">TExplain: Explaining Learned Visual Features via Pre-trained (Frozen) Language Models. (arXiv:2309.00733v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1">Saeid Asgari Taghanaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Khani_A/0/1/0/all/0/1">Aliasghar Khani</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1">Amir Khasahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1">Aditya Sanghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1">Karl D.D. Willis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1">Ali Mahdavi-Amiri</a></p>
<p>Interpreting the learned features of vision models has posed a longstanding
challenge in the field of machine learning. To address this issue, we propose a
novel method that leverages the capabilities of language models to interpret
the learned features of pre-trained image classifiers. Our method, called
TExplain, tackles this task by training a neural network to establish a
connection between the feature space of image classifiers and language models.
Then, during inference, our approach generates a vast number of sentences to
explain the features learned by the classifier for a given image. These
sentences are then used to extract the most frequent words, providing a
comprehensive understanding of the learned features and patterns within the
classifier. Our method, for the first time, utilizes these frequent words
corresponding to a visual representation to provide insights into the
decision-making process of the independently trained classifier, enabling the
detection of spurious correlations, biases, and a deeper comprehension of its
behavior. To validate the effectiveness of our approach, we conduct experiments
on diverse datasets, including ImageNet-9L and Waterbirds. The results
demonstrate the potential of our method to enhance the interpretability and
robustness of image classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01409">Implicit Neural Image Stitching. (arXiv:2309.01409v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaewon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byeonghun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sunghoon Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1">Kyong Hwan Jin</a></p>
<p>Existing frameworks for image stitching often provide visually reasonable
stitchings. However, they suffer from blurry artifacts and disparities in
illumination, depth level, etc. Although the recent learning-based stitchings
relax such disparities, the required methods impose sacrifice of image
qualities failing to capture high-frequency details for stitched images. To
address the problem, we propose a novel approach, implicit Neural Image
Stitching (NIS) that extends arbitrary-scale super-resolution. Our method
estimates Fourier coefficients of images for quality-enhancing warps. Then, the
suggested model blends color mismatches and misalignment in the latent space
and decodes the features into RGB values of stitched images. Our experiments
show that our approach achieves improvement in resolving the low-definition
imaging of the previous deep image stitching with favorable accelerated
image-enhancing methods. Our source code is available at
https://github.com/minshu-kim/NIS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02773">Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter. (arXiv:2309.02773v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinglong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qingyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1">Lu Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a></p>
<p>The pre-trained text-image discriminative models, such as CLIP, has been
explored for open-vocabulary semantic segmentation with unsatisfactory results
due to the loss of crucial localization information and awareness of object
shapes. Recently, there has been a growing interest in expanding the
application of generative models from generation tasks to semantic
segmentation. These approaches utilize generative models either for generating
annotated data or extracting features to facilitate semantic segmentation. This
typically involves generating a considerable amount of synthetic data or
requiring additional mask annotations. To this end, we uncover the potential of
generative text-to-image diffusion models (e.g., Stable Diffusion) as highly
efficient open-vocabulary semantic segmenters, and introduce a novel
training-free approach named DiffSegmenter. The insight is that to generate
realistic objects that are semantically faithful to the input text, both the
complete object shapes and the corresponding semantics are implicitly learned
by diffusion models. We discover that the object shapes are characterized by
the self-attention maps while the semantics are indicated through the
cross-attention maps produced by the denoising U-Net, forming the basis of our
segmentation results.Additionally, we carefully design effective textual
prompts and a category filtering mechanism to further enhance the segmentation
results. Extensive experiments on three benchmark datasets show that the
proposed DiffSegmenter achieves impressive results for open-vocabulary semantic
segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03020">SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution. (arXiv:2309.03020v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenlong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaohui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiao-Ming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chao Dong</a></p>
<p>Real-world Super-Resolution (Real-SR) methods focus on dealing with diverse
real-world images and have attracted increasing attention in recent years. The
key idea is to use a complex and high-order degradation model to mimic
real-world degradations. Although they have achieved impressive results in
various scenarios, they are faced with the obstacle of evaluation. Currently,
these methods are only assessed by their average performance on a small set of
degradation cases randomly selected from a large space, which fails to provide
a comprehensive understanding of their overall performance and often yields
inconsistent and potentially misleading results. To overcome the limitation in
evaluation, we propose SEAL, a framework for systematic evaluation of real-SR.
In particular, we cluster the extensive degradation space to create a set of
representative degradation cases, which serves as a comprehensive test set.
Next, we propose a coarse-to-fine evaluation protocol to measure the
distributed and relative performance of real-SR methods on the test set. The
protocol incorporates two new metrics: acceptance rate (AR) and relative
performance ratio (RPR), derived from acceptance and excellence lines. Under
SEAL, we benchmark existing real-SR methods, obtain new observations and
insights into their performance, and develop a new strong baseline. We consider
SEAL as the first step towards creating a comprehensive real-SR evaluation
platform, which can promote the development of real-SR. The source code is
available at https://github.com/XPixelGroup/SEAL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04089">Toward Sufficient Spatial-Frequency Interaction for Gradient-aware Underwater Image Enhancement. (arXiv:2309.04089v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weiling Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chenyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Ziqi Zeng</a></p>
<p>Underwater images suffer from complex and diverse degradation, which
inevitably affects the performance of underwater visual tasks. However, most
existing learning-based Underwater image enhancement (UIE) methods mainly
restore such degradations in the spatial domain, and rarely pay attention to
the fourier frequency information. In this paper, we develop a novel UIE
framework based on spatial-frequency interaction and gradient maps, namely
SFGNet, which consists of two stages. Specifically, in the first stage, we
propose a dense spatial-frequency fusion network (DSFFNet), mainly including
our designed dense fourier fusion block and dense spatial fusion block,
achieving sufficient spatial-frequency interaction by cross connections between
these two blocks. In the second stage, we propose a gradient-aware corrector
(GAC) to further enhance perceptual details and geometric structures of images
by gradient map. Experimental results on two real-world underwater image
datasets show that our approach can successfully enhance underwater images, and
achieves competitive performance in visual quality improvement. The code is
available at https://github.com/zhihefang/SFGNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengxiang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1">Aldo Lipani</a></p>
<p>Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer's quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving substantial memory and time costs compared to vanilla
PT and its variants, without changing trainable parameter sizes. Through
extensive experiments on 23 natural language processing (NLP) and
vision-language (VL) tasks, we demonstrate that DePT outperforms
state-of-the-art PEFT approaches, including the full fine-tuning baseline, in
some scenarios. Additionally, we empirically show that DEPT grows more
efficient as the model size increases. Our further study reveals that DePT
integrates seamlessly with parameter-efficient transfer learning in the
few-shot learning setting and highlights its adaptability to various model
architectures and sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06207">SGNet: Salient Geometric Network for Point Cloud Registration. (arXiv:2309.06207v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qianliang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yaqing Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Lei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shuo Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chuanwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a></p>
<p>Point Cloud Registration (PCR) is a critical and challenging task in computer
vision. One of the primary difficulties in PCR is identifying salient and
meaningful points that exhibit consistent semantic and geometric properties
across different scans. Previous methods have encountered challenges with
ambiguous matching due to the similarity among patch blocks throughout the
entire point cloud and the lack of consideration for efficient global geometric
consistency. To address these issues, we propose a new framework that includes
several novel techniques. Firstly, we introduce a semantic-aware geometric
encoder that combines object-level and patch-level semantic information. This
encoder significantly improves registration recall by reducing ambiguity in
patch-level superpoint matching. Additionally, we incorporate a prior knowledge
approach that utilizes an intrinsic shape signature to identify salient points.
This enables us to extract the most salient super points and meaningful dense
points in the scene. Secondly, we introduce an innovative transformer that
encodes High-Order (HO) geometric features. These features are crucial for
identifying salient points within initial overlap regions while considering
global high-order geometric consistency. To optimize this high-order
transformer further, we introduce an anchor node selection strategy. By
encoding inter-frame triangle or polyhedron consistency features based on these
anchor nodes, we can effectively learn high-order geometric features of salient
super points. These high-order features are then propagated to dense points and
utilized by a Sinkhorn matching module to identify key correspondences for
successful registration. In our experiments conducted on well-known datasets
such as 3DMatch/3DLoMatch and KITTI, our approach has shown promising results,
highlighting the effectiveness of our novel method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16301">Gated Cross-Attention Network for Depth Completion. (arXiv:2309.16301v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaogang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1">Songlei Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yusong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_Y/0/1/0/all/0/1">Yonggang Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhengfa Liang</a></p>
<p>Depth completion is a popular research direction in the field of depth
estimation. The fusion of color and depth features is the current critical
challenge in this task, mainly due to the asymmetry between the rich scene
details in color images and the sparse pixels in depth maps. To tackle this
issue, we design an efficient Gated Cross-Attention Network that propagates
confidence via a gating mechanism, simultaneously extracting and refining key
information in both color and depth branches to achieve local spatial feature
fusion. Additionally, we employ an attention network based on the Transformer
in low-dimensional space to effectively fuse global features and increase the
network's receptive field. With a simple yet efficient gating mechanism, our
proposed method achieves fast and accurate depth completion without the need
for additional branches or post-processing steps. At the same time, we use the
Ray Tune mechanism with the AsyncHyperBandScheduler scheduler and the
HyperOptSearch algorithm to automatically search for the optimal number of
module iterations, which also allows us to achieve performance comparable to
state-of-the-art methods. We conduct experiments on both indoor and outdoor
scene datasets. Our fast network achieves Pareto-optimal solutions in terms of
time and accuracy, and at the time of submission, our accurate network ranks
first among all published papers on the KITTI official website in terms of
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00106">FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery. (arXiv:2310.00106v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1">Tasin Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Miron_A/0/1/0/all/0/1">Alina Miron</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">XiaoHui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongmin Li</a></p>
<p>Our study introduces a new image-to-video generator called FashionFlow to
generate fashion videos. By utilising a diffusion model, we are able to create
short videos from still fashion images. Our approach involves developing and
connecting relevant components with the diffusion model, which results in the
creation of high-fidelity videos that are aligned with the conditional image.
The components include the use of pseudo-3D convolutional layers to generate
videos efficiently. VAE and CLIP encoders capture vital characteristics from
still images to condition the diffusion model at a global level. Our research
demonstrates a successful synthesis of fashion videos featuring models posing
from various angles, showcasing the fit and appearance of the garment. Our
findings hold great promise for improving and enhancing the shopping experience
for the online fashion industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00454">UniLVSeg: Unified Left Ventricular Segmentation with Sparsely Annotated Echocardiogram Videos through Self-Supervised Temporal Masking and Weakly Supervised Training. (arXiv:2310.00454v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maani_F/0/1/0/all/0/1">Fadillah Maani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ukaye_A/0/1/0/all/0/1">Asim Ukaye</a>, <a href="http://arxiv.org/find/cs/1/au:+Saadi_N/0/1/0/all/0/1">Nada Saadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeed_N/0/1/0/all/0/1">Numan Saeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1">Mohammad Yaqub</a></p>
<p>Echocardiography has become an indispensable clinical imaging modality for
general heart health assessment. From calculating biomarkers such as ejection
fraction to the probability of a patient's heart failure, accurate segmentation
of the heart and its structures allows doctors to plan and execute treatments
with greater precision and accuracy. However, achieving accurate and robust
left ventricle segmentation is time-consuming and challenging due to different
reasons. This work introduces a novel approach for consistent left ventricular
(LV) segmentation from sparsely annotated echocardiogram videos. We achieve
this through (1) self-supervised learning (SSL) using temporal masking followed
by (2) weakly supervised training. We investigate two different segmentation
approaches: 3D segmentation and a novel 2D superimage (SI). We demonstrate how
our proposed method outperforms the state-of-the-art solutions by achieving a
93.32% (95%CI 93.21-93.43%) dice score on a large-scale dataset
(EchoNet-Dynamic) while being more efficient. To show the effectiveness of our
approach, we provide extensive ablation studies, including pre-training
settings and various deep learning backbones. Additionally, we discuss how our
proposed methodology achieves high data utility by incorporating unlabeled
frames in the training process. To help support the AI in medicine community,
the complete solution with the source code will be made publicly available upon
acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00647">Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning. (arXiv:2310.00647v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1">Mustafa Shukor</a>, <a href="http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1">Alexandre Rame</a>, <a href="http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1">Corentin Dancette</a>, <a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1">Matthieu Cord</a></p>
<p>Following the success of Large Language Models (LLMs), Large Multimodal
Models (LMMs), such as the Flamingo model and its subsequent competitors, have
started to emerge as natural steps towards generalist agents. However,
interacting with recent LMMs reveals major limitations that are hardly captured
by the current evaluation benchmarks. Indeed, task performances (e.g., VQA
accuracy) alone do not provide enough clues to understand their real
capabilities, limitations, and to which extent such models are aligned to human
expectations. To refine our understanding of those flaws, we deviate from the
current evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from
3B up to 80B parameter scale, on 5 different axes; hallucinations, abstention,
compositionality, explainability and instruction following. Our evaluation on
these axes reveals major flaws in LMMs. While the current go-to solution to
align these models is based on training, such as instruction tuning or RLHF, we
rather (2) explore the training-free in-context learning (ICL) as a solution,
and study how it affects these limitations. Based on our ICL study, (3) we push
ICL further and propose new multimodal ICL variants such as; Multitask-ICL,
Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows.
(1) Despite their success, LMMs have flaws that remain unsolved with scaling
alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its
effectiveness for improved explainability, answer abstention, ICL only slightly
improves instruction following, does not improve compositional abilities, and
actually even amplifies hallucinations. (3) The proposed ICL variants are
promising as post-hoc approaches to efficiently tackle some of those flaws. The
code is available here: https://github.com/mshukor/EvALign-ICL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01361">GenSim: Generating Robotic Simulation Tasks via Large Language Models. (arXiv:2310.01361v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1">Yiyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zhecheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1">Mohit Shridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1">Chen Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yuzhe Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>Collecting large amounts of real-world interaction data to train general
robotic policies is often prohibitively expensive, thus motivating the use of
simulation data. However, existing methods for data generation have generally
focused on scene-level diversity (e.g., object instances and poses) rather than
task-level diversity, due to the human effort required to come up with and
verify novel tasks. This has made it challenging for policies trained on
simulation data to demonstrate significant task-level generalization. In this
paper, we propose to automatically generate rich simulation environments and
expert demonstrations by exploiting a large language models' (LLM) grounding
and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed
generation, wherein a target task is given to the LLM and the LLM proposes a
task curriculum to solve the target task, and exploratory generation, wherein
the LLM bootstraps from previous tasks and iteratively proposes novel tasks
that would be helpful in solving more complex tasks. We use GPT4 to expand the
existing benchmark by ten times to over 100 tasks, on which we conduct
supervised finetuning and evaluate several LLMs including finetuned GPTs and
Code Llama on code generation for robotic simulation tasks. Furthermore, we
observe that LLMs-generated simulation programs can enhance task-level
generalization significantly when used for multitask policy training. We
further find that with minimal sim-to-real adaptation, the multitask policies
pretrained on GPT4-generated simulation tasks exhibit stronger transfer to
unseen long-horizon tasks in the real world and outperform baselines by 25%.
See the project website (https://liruiw.github.io/gensim) for code, demos, and
videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01852">LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1">Munan Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiaxi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">HongFa Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">Yatian Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wenhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junwu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wancai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. LanguageBind has achieved superior performance on a wide range of
15 benchmarks covering video, audio, depth, and infrared. Moreover, multiple
experiments have provided evidence for the effectiveness of LanguageBind in
achieving indirect alignment and complementarity among diverse modalities. Code
address: https://github.com/PKU-YuanGroup/LanguageBind
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02255">MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. (arXiv:2310.02255v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tony Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiacheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1">Michel Galley</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05916">Interpreting CLIP&#x27;s Image Representation via Text-Based Decomposition. (arXiv:2310.05916v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1">Yossi Gandelsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1">Alexei A. Efros</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1">Jacob Steinhardt</a></p>
<p>We investigate the CLIP image encoder by analyzing how individual model
components affect the final representation. We decompose the image
representation as a sum across individual image patches, model layers, and
attention heads, and use CLIP's text representation to interpret the summands.
Interpreting the attention heads, we characterize each head's role by
automatically finding text representations that span its output space, which
reveals property-specific roles for many heads (e.g. location or shape). Next,
interpreting the image patches, we uncover an emergent spatial localization
within CLIP. Finally, we use this understanding to remove spurious features
from CLIP and to create a strong zero-shot image segmenter. Our results
indicate that a scalable understanding of transformer models is attainable and
can be used to repair and improve models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09126">Physics-guided Noise Neural Proxy for Practical Low-light Raw Image Denoising. (arXiv:2310.09126v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1">Hansen Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1">Lizhi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yiqi Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yuzhi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1">Lin Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1">Hua Huang</a></p>
<p>Recently, the mainstream practice for training low-light raw image denoising
methods has shifted towards employing synthetic data. Noise modeling, which
focuses on characterizing the noise distribution of real-world sensors,
profoundly influences the effectiveness and practicality of synthetic data.
Currently, physics-based noise modeling struggles to characterize the entire
real noise distribution, while learning-based noise modeling impractically
depends on paired real data. In this paper, we propose a novel strategy:
learning the noise model from dark frames instead of paired real data, to break
down the data dependency. Based on this strategy, we introduce an efficient
physics-guided noise neural proxy (PNNP) to approximate the real-world sensor
noise model. Specifically, we integrate physical priors into neural proxies and
introduce three efficient techniques: physics-guided noise decoupling (PND),
physics-guided proxy model (PPM), and differentiable distribution loss (DDL).
PND decouples the dark frame into different components and handles different
levels of noise flexibly, which reduces the complexity of noise modeling. PPM
incorporates physical priors to constrain the generated noise, which promotes
the accuracy of noise modeling. DDL provides explicit and reliable supervision
for noise distribution, which promotes the precision of noise modeling. PNNP
exhibits powerful potential in characterizing the real noise distribution.
Extensive experiments on public datasets demonstrate superior performance in
practical low-light raw image denoising. The code will be available at
\url{https://github.com/fenghansen/PNNP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09221">Ultrasound Image Segmentation of Thyroid Nodule via Latent Semantic Feature Co-Registration. (arXiv:2310.09221v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xuewei Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1">Yaqiao Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1">Jie Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1">Xi Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1">Ruixuan Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">ZhiQiang Liu</a></p>
<p>Segmentation of nodules in thyroid ultrasound imaging plays a crucial role in
the detection and treatment of thyroid cancer. However, owing to the diversity
of scanner vendors and imaging protocols in different hospitals, the automatic
segmentation model, which has already demonstrated expert-level accuracy in the
field of medical image segmentation, finds its accuracy reduced as the result
of its weak generalization performance when being applied in clinically
realistic environments. To address this issue, the present paper proposes ASTN,
a framework for thyroid nodule segmentation achieved through a new type
co-registration network. By extracting latent semantic information from the
atlas and target images and utilizing in-depth features to accomplish the
co-registration of nodules in thyroid ultrasound images, this framework can
ensure the integrity of anatomical structure and reduce the impact on
segmentation as the result of overall differences in image caused by different
devices. In addition, this paper also provides an atlas selection algorithm to
mitigate the difficulty of co-registration. As shown by the evaluation results
collected from the datasets of different devices, thanks to the method we
proposed, the model generalization has been greatly improved while maintaining
a high level of segmentation accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12817">2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng-Kun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Min-Hung Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yung-Yu Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yen-Yu Lin</a></p>
<p>We present a Multimodal Interlaced Transformer (MIT) that jointly considers
2D and 3D data for weakly supervised point cloud segmentation. Research studies
have shown that 2D and 3D features are complementary for point cloud
segmentation. However, existing methods require extra 2D annotations to achieve
2D-3D information fusion. Considering the high annotation cost of point clouds,
effective 2D and 3D feature fusion based on weakly supervised learning is in
great demand. To this end, we propose a transformer model with two encoders and
one decoder for weakly supervised point cloud segmentation using only
scene-level class tags. Specifically, the two encoders compute the
self-attended features for 3D point clouds and 2D multi-view images,
respectively. The decoder implements interlaced 2D-3D cross-attention and
carries out implicit 2D and 3D feature fusion. We alternately switch the roles
of queries and key-value pairs in the decoder layers. It turns out that the 2D
and 3D features are iteratively enriched by each other. Experiments show that
it performs favorably against existing weakly supervised point cloud
segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The
project page will be available at https://jimmy15923.github.io/mit_web/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20381">A Systematic Evaluation of GPT-4V&#x27;s Multimodal Capability for Medical Image Analysis. (arXiv:2310.20381v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhanyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xinyu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingqiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Leyang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luping Zhou</a></p>
<p>This work conducts an evaluation of GPT-4V's multimodal capability for
medical image analysis, with a focus on three representative tasks of radiology
report generation, medical visual question answering, and medical visual
grounding. For the evaluation, a set of prompts is designed for each task to
induce the corresponding capability of GPT-4V to produce sufficiently good
outputs. Three evaluation ways including quantitative analysis, human
evaluation, and case study are employed to achieve an in-depth and extensive
evaluation. Our evaluation shows that GPT-4V excels in understanding medical
images and is able to generate high-quality radiology reports and effectively
answer questions about medical images. Meanwhile, it is found that its
performance for medical visual grounding needs to be substantially improved. In
addition, we observe the discrepancy between the evaluation outcome from
quantitative analysis and that from human evaluation. This discrepancy suggests
the limitations of conventional metrics in assessing the performance of large
language models like GPT-4V and the necessity of developing new metrics for
automatic quantitative analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02189">Harvard FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling. (arXiv:2311.02189v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Min Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouhana_A/0/1/0/all/0/1">Ava Kouhana</a>, <a href="http://arxiv.org/find/cs/1/au:+Elze_T/0/1/0/all/0/1">Tobias Elze</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengyu Wang</a></p>
<p>Fairness in artificial intelligence models has gained significantly more
attention in recent years, especially in the area of medicine, as fairness in
medical models is critical to people's well-being and lives. High-quality
medical fairness datasets are needed to promote fairness learning research.
Existing medical fairness datasets are all for classification tasks, and no
fairness datasets are available for medical segmentation, while medical
segmentation is an equally important clinical task as classifications, which
can provide detailed spatial information on organ abnormalities ready to be
assessed by clinicians. In this paper, we propose the first fairness dataset
for medical segmentation named Harvard-FairSeg with 10,000 subject samples. In
addition, we propose a fair error-bound scaling approach to reweight the loss
function with the upper error-bound in each identity group, using the segment
anything model (SAM). We anticipate that the segmentation performance equity
can be improved by explicitly tackling the hard cases with high training errors
in each identity group. To facilitate fair comparisons, we utilize a novel
equity-scaled segmentation performance metric to compare segmentation metrics
in the context of fairness, such as the equity-scaled Dice coefficient. Through
comprehensive experiments, we demonstrate that our fair error-bound scaling
approach either has superior or comparable fairness performance to the
state-of-the-art fairness learning models. The dataset and code are publicly
accessible via https://ophai.hms.harvard.edu/harvard-fairseg10k.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02332">Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1">Elisa Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonsang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">William Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1">Tanveer Syeda-Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1">Charles Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Arvind Rao</a></p>
<p>Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also highlights the need for
principled assessments and practical implementation of such models, bringing
attention to the dynamics between decision support systems and healthcare
providers and personnel. Despite advancements, challenges such as data biases
and the scarcity of "big data" in many biomedical domains persist. We conclude
with a discussion on principled innovation and collaborative efforts to further
the mission of seamless integration of multimodal ML models into biomedical
practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03500">Predicting Age from White Matter Diffusivity with Residual Learning. (arXiv:2311.03500v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1">Chenyu Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1">Michael E. Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1">Ho Hin Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1">Qi Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Khairi_N/0/1/0/all/0/1">Nazirah Mohd Khairi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanakaraj_P/0/1/0/all/0/1">Praitayini Kanakaraj</a>, <a href="http://arxiv.org/find/eess/1/au:+Newlin_N/0/1/0/all/0/1">Nancy R. Newlin</a>, <a href="http://arxiv.org/find/eess/1/au:+Archer_D/0/1/0/all/0/1">Derek B. Archer</a>, <a href="http://arxiv.org/find/eess/1/au:+Jefferson_A/0/1/0/all/0/1">Angela L. Jefferson</a>, <a href="http://arxiv.org/find/eess/1/au:+Taylor_W/0/1/0/all/0/1">Warren D. Taylor</a>, <a href="http://arxiv.org/find/eess/1/au:+Boyd_B/0/1/0/all/0/1">Brian D. Boyd</a>, <a href="http://arxiv.org/find/eess/1/au:+Beason_Held_L/0/1/0/all/0/1">Lori L. Beason-Held</a>, <a href="http://arxiv.org/find/eess/1/au:+Resnick_S/0/1/0/all/0/1">Susan M. Resnick</a>, The <a href="http://arxiv.org/find/eess/1/au:+Team_BIOCARD_Study/0/1/0/all/0/1">BIOCARD Study Team</a>, <a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1">Yuankai Huo</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaik_K/0/1/0/all/0/1">Katherine D. Van Schaik</a>, <a href="http://arxiv.org/find/eess/1/au:+Schilling_K/0/1/0/all/0/1">Kurt G. Schilling</a>, <a href="http://arxiv.org/find/eess/1/au:+Moyer_D/0/1/0/all/0/1">Daniel Moyer</a>, <a href="http://arxiv.org/find/eess/1/au:+Isgum_I/0/1/0/all/0/1">Ivana I&#x161;gum</a>, <a href="http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a></p>
<p>Imaging findings inconsistent with those expected at specific chronological
age ranges may serve as early indicators of neurological disorders and
increased mortality risk. Estimation of chronological age, and deviations from
expected results, from structural MRI data has become an important task for
developing biomarkers that are sensitive to such deviations. Complementary to
structural analysis, diffusion tensor imaging (DTI) has proven effective in
identifying age-related microstructural changes within the brain white matter,
thereby presenting itself as a promising additional modality for brain age
prediction. Although early studies have sought to harness DTI's advantages for
age estimation, there is no evidence that the success of this prediction is
owed to the unique microstructural and diffusivity features that DTI provides,
rather than the macrostructural features that are also available in DTI data.
Therefore, we seek to develop white-matter-specific age estimation to capture
deviations from normal white matter aging. Specifically, we deliberately
disregard the macrostructural information when predicting age from DTI scalar
images, using two distinct methods. The first method relies on extracting only
microstructural features from regions of interest. The second applies 3D
residual neural networks (ResNets) to learn features directly from the images,
which are non-linearly registered and warped to a template to minimize
macrostructural variations. When tested on unseen data, the first method yields
mean absolute error (MAE) of 6.11 years for cognitively normal participants and
MAE of 6.62 years for cognitively impaired participants, while the second
method achieves MAE of 4.69 years for cognitively normal participants and MAE
of 4.96 years for cognitively impaired participants. We find that the ResNet
model captures subtler, non-macrostructural features for brain age prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03782">CapST: An Enhanced and Lightweight Model Attribution Approach for Synthetic Videos. (arXiv:2311.03782v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1">Wasim Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yan-Tsung Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yuan-Hao Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganfure_G/0/1/0/all/0/1">Gaddisa Olani Ganfure</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Sarwar Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahzad_S/0/1/0/all/0/1">Sahibzada Adil Shahzad</a></p>
<p>Deepfake videos, generated through AI faceswapping techniques, have garnered
considerable attention due to their potential for powerful impersonation
attacks. While existing research primarily focuses on binary classification to
discern between real and fake videos, however determining the specific
generation model for a fake video is crucial for forensic investigation.
Addressing this gap, this paper investigates the model attribution problem of
Deepfake videos from a recently proposed dataset, Deepfakes from Different
Models (DFDM), derived from various Autoencoder models. The dataset comprises
6,450 Deepfake videos generated by five distinct models with variations in
encoder, decoder, intermediate layer, input resolution, and compression ratio.
This study formulates Deepfakes model attribution as a multiclass
classification task, proposing a segment of VGG19 as a feature extraction
backbone, known for its effectiveness in imagerelated tasks, while integrated a
Capsule Network with a Spatio-Temporal attention mechanism. The Capsule module
captures intricate hierarchies among features for robust identification of
deepfake attributes. Additionally, the video-level fusion technique leverages
temporal attention mechanisms to handle concatenated feature vectors,
capitalizing on inherent temporal dependencies in deepfake videos. By
aggregating insights across frames, our model gains a comprehensive
understanding of video content, resulting in more precise predictions.
Experimental results on the deepfake benchmark dataset (DFDM) demonstrate the
efficacy of our proposed method, achieving up to a 4% improvement in accurately
categorizing deepfake videos compared to baseline models while demanding fewer
computational resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11205">Shape-Sensitive Loss for Catheter and Guidewire Segmentation. (arXiv:2311.11205v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kongtongvattana_C/0/1/0/all/0/1">Chayun Kongtongvattana</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1">Baoru Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kang_J/0/1/0/all/0/1">Jingxuan Kang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1">Hoan Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Olufemi_O/0/1/0/all/0/1">Olajide Olufemi</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a></p>
<p>We introduce a shape-sensitive loss function for catheter and guidewire
segmentation and utilize it in a vision transformer network to establish a new
state-of-the-art result on a large-scale X-ray images dataset. We transform
network-derived predictions and their corresponding ground truths into signed
distance maps, thereby enabling any networks to concentrate on the essential
boundaries rather than merely the overall contours. These SDMs are subjected to
the vision transformer, efficiently producing high-dimensional feature vectors
encapsulating critical image attributes. By computing the cosine similarity
between these feature vectors, we gain a nuanced understanding of image
similarity that goes beyond the limitations of traditional overlap-based
measures. The advantages of our approach are manifold, ranging from scale and
translation invariance to superior detection of subtle differences, thus
ensuring precise localization and delineation of the medical instruments within
the images. Comprehensive quantitative and qualitative analyses substantiate
the significant enhancement in performance over existing baselines,
demonstrating the promise held by our new shape-sensitive loss function for
improving catheter and guidewire segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16141">Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Boxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1">Haihang You</a></p>
<p>Spiking Neural Networks (SNNs) have gained considerable attention due to the
energy-efficient and multiplication-free characteristics. The continuous growth
in scale of deep SNNs poses challenges for model deployment. Network pruning
reduces hardware resource requirements of model deployment by compressing the
network scale. However, existing SNN pruning methods cause high pruning costs
and performance loss because the pruning iterations amplify the training
difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in
neuroscience, we propose a regeneration mechanism based on the neuron
criticality for SNN pruning to enhance feature extraction and accelerate the
pruning process. Firstly, we propose a low-cost metric for the criticality in
SNNs. Then, we re-rank the pruned structures after pruning and regenerate those
with higher criticality to obtain the critical network. Our method achieves
higher performance than the current state-of-the-art (SOTA) method with up to
95.26% reduction of pruning cost. Moreover, we investigate the underlying
mechanism of our method and find that it efficiently selects potential
structures and learns the consistent feature representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17005">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark. (arXiv:2311.17005v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunchang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yali Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yinan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jilan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>With the rapid development of Multi-modal Large Language Models (MLLMs), a
number of diagnostic benchmarks have recently emerged to evaluate the
comprehension capabilities of these models. However, most benchmarks
predominantly assess spatial understanding in the static image tasks, while
overlooking temporal understanding in the dynamic video tasks. To alleviate
this issue, we introduce a comprehensive Multi-modal Video understanding
Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot
be effectively solved with a single frame. Specifically, we first introduce a
novel static-to-dynamic method to define these temporal-related tasks. By
transforming various static tasks into dynamic ones, we enable the systematic
generation of video tasks that require a broad spectrum of temporal skills,
ranging from perception to cognition. Then, guided by the task definition, we
automatically convert public video annotations into multiple-choice QA to
evaluate each task. On one hand, such a distinct paradigm allows us to build
MVBench efficiently, without much manual intervention. On the other hand, it
guarantees evaluation fairness with ground-truth video annotations, avoiding
the biased scoring of LLMs. Moreover, we further develop a robust video MLLM
baseline, i.e., VideoChat2, by progressive multi-modal training with diverse
instruction-tuning data. The extensive results on our MVBench reveal that, the
existing MLLMs are far from satisfactory in temporal understanding, while our
VideoChat2 largely surpasses these leading models by over 15% on MVBench. All
models and data are available at https://github.com/OpenGVLab/Ask-Anything.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00157">Universal Backdoor Attacks. (arXiv:2312.00157v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schneider_B/0/1/0/all/0/1">Benjamin Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukas_N/0/1/0/all/0/1">Nils Lukas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1">Florian Kerschbaum</a></p>
<p>Web-scraped datasets are vulnerable to data poisoning, which can be used for
backdooring deep image classifiers during training. Since training on large
datasets is expensive, a model is trained once and re-used many times. Unlike
adversarial examples, backdoor attacks often target specific classes rather
than any class learned by the model. One might expect that targeting many
classes through a naive composition of attacks vastly increases the number of
poison samples. We show this is not necessarily true and more efficient,
universal data poisoning attacks exist that allow controlling
misclassifications from any source class into any target class with a small
increase in poison samples. Our idea is to generate triggers with salient
characteristics that the model can learn. The triggers we craft exploit a
phenomenon we call inter-class poison transferability, where learning a trigger
from one class makes the model more vulnerable to learning triggers for other
classes. We demonstrate the effectiveness and robustness of our universal
backdoor attacks by controlling models with up to 6,000 classes while poisoning
only 0.15% of the training dataset. Our source code is available at
https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01632">GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation. (arXiv:2312.01632v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiu-Cheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianyan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1">Chi-Man Pun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hao Gao</a></p>
<p>Constructing vivid 3D head avatars for given subjects and realizing a series
of animations on them is valuable yet challenging. This paper presents
GaussianHead, which models the actional human head with anisotropic 3D
Gaussians. In our framework, a motion deformation field and multi-resolution
tri-plane are constructed respectively to deal with the head's dynamic geometry
and complex texture. Notably, we impose an exclusive derivation scheme on each
Gaussian, which generates its multiple doppelgangers through a set of learnable
parameters for position transformation. With this design, we can compactly and
accurately encode the appearance information of Gaussians, even those fitting
the head's particular components with sophisticated structures. In addition, an
inherited derivation strategy for newly added Gaussians is adopted to
facilitate training acceleration. Extensive experiments show that our method
can produce high-fidelity renderings, outperforming state-of-the-art approaches
in reconstruction, cross-identity reenactment, and novel view synthesis tasks.
Our code is available at: https://github.com/chiehwangs/gaussian-head.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10105">Forging Tokens for Improved Storage-efficient Training. (arXiv:2312.10105v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minhyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Song Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1">Byeongho Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dongyoon Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1">Hyunjung Shim</a></p>
<p>Recent advancements in Deep Neural Network (DNN) models have significantly
improved performance across computer vision tasks. However, achieving highly
generalizable and high-performing vision models requires extensive datasets,
leading to large storage requirements. This storage challenge poses a critical
bottleneck for scaling up vision models. Motivated by the success of discrete
representations, SeiT proposes to use Vector-Quantized (VQ) feature vectors
(i.e., tokens) as network inputs for vision classification. However, applying
traditional data augmentations to tokens faces challenges due to input domain
shift. To address this issue, we introduce TokenAdapt and ColorAdapt, simple
yet effective token-based augmentation strategies. TokenAdapt realigns token
embedding space for compatibility with spatial augmentations, preserving the
model's efficiency without requiring fine-tuning. Additionally, ColorAdapt
addresses color-based augmentations for tokens inspired by Adaptive Instance
Normalization (AdaIN). We evaluate our approach across various scenarios,
including storage-efficient ImageNet-1k classification, fine-grained
classification, robustness benchmarks, and ADE-20k semantic segmentation.
Experimental results demonstrate consistent performance improvement in diverse
experiments. Code is available at https://github.com/naver-ai/tokenadapt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11841">MixRT: Mixed Neural Representations For Real-Time NeRF Rendering. (arXiv:2312.11841v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaojian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Yingyan/0/1/0/all/0/1">Yingyan</a> (Celine)Lin</p>
<p>Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16244">Modality-missing RGBT Tracking via Invertible Prompt Learning and A High-quality Data Simulation Method. (arXiv:2312.16244v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1">Andong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiacong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Current RGBT tracking researches mainly focus on the modality-complete
scenarios, overlooking the modality-missing challenge in real-world scenes. In
this work, we comprehensively investigate the impact of modality-missing
challenge in RGBT tracking and propose a novel invertible prompt learning
approach, which integrates the content-preserving prompts into a well-trained
tracking model to adapt to various modality-missing scenarios, for
modality-missing RGBT tracking. In particular, given one modality-missing
scenario, we propose to utilize the available modality to generate the prompt
of the missing modality to adapt to RGBT tracking model. However, the
cross-modality gap between available and missing modalities usually causes
semantic distortion and information loss in prompt generation. To handle this
issue, we propose the invertible prompt learning scheme by incorporating the
full reconstruction of the input available modality from the prompt in prompt
generation model. Considering that there lacks a modality-missing RGBT tracking
dataset and many modality-missing scenarios are difficult to capture, we design
a high-quality data simulation method based on hierarchical combination schemes
to generate real-world modality-missing data. Extensive experiments on three
modality-missing datasets show that our method achieves significant performance
improvements compared with state-of-the-art methods. We will release the code
and simulation dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17004">Continual Learning in Medical Image Analysis: A Comprehensive Review of Recent Advancements and Future Prospects. (arXiv:2312.17004v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kumari_P/0/1/0/all/0/1">Pratibha Kumari</a>, <a href="http://arxiv.org/find/eess/1/au:+Chauhan_J/0/1/0/all/0/1">Joohi Chauhan</a>, <a href="http://arxiv.org/find/eess/1/au:+Bozorgpour_A/0/1/0/all/0/1">Afshin Bozorgpour</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1">Boqiang Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Azad_R/0/1/0/all/0/1">Reza Azad</a>, <a href="http://arxiv.org/find/eess/1/au:+Merhof_D/0/1/0/all/0/1">Dorit Merhof</a></p>
<p>Medical imaging analysis has witnessed remarkable advancements even
surpassing human-level performance in recent years, driven by the rapid
development of advanced deep-learning algorithms. However, when the inference
dataset slightly differs from what the model has seen during one-time training,
the model performance is greatly compromised. The situation requires restarting
the training process using both the old and the new data which is
computationally costly, does not align with the human learning process, and
imposes storage constraints and privacy concerns. Alternatively, continual
learning has emerged as a crucial approach for developing unified and
sustainable deep models to deal with new classes, tasks, and the drifting
nature of data in non-stationary environments for various application areas.
Continual learning techniques enable models to adapt and accumulate knowledge
over time, which is essential for maintaining performance on evolving datasets
and novel tasks. This systematic review paper provides a comprehensive overview
of the state-of-the-art in continual learning techniques applied to medical
imaging analysis. We present an extensive survey of existing research, covering
topics including catastrophic forgetting, data drifts, stability, and
plasticity requirements. Further, an in-depth discussion of key components of a
continual learning framework such as continual learning scenarios, techniques,
evaluation schemes, and metrics is provided. Continual learning techniques
encompass various categories, including rehearsal, regularization,
architectural, and hybrid strategies. We assess the popularity and
applicability of continual learning categories in various medical sub-fields
like radiology and histopathology...
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17240">LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model. (arXiv:2312.17240v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Senqiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_T/0/1/0/all/0/1">Tianyuan Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1">Xin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhuotao Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bohao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jiaya Jia</a></p>
<p>While LISA effectively bridges the gap between segmentation and large
language models to enable reasoning segmentation, it poses certain limitations:
unable to distinguish different instances of the target region, and constrained
by the pre-defined textual response formats. In this work, we introduce LISA++,
an update to the existing LISA model, focusing on improving core
functionalities while keeping the base architecture intact. The main
enhancements in LISA++ include: \textbf{1) Enhanced Segmentation}: The instance
segmentation ability has been added, providing a more detailed scene analysis
along with the existing multi-region semantic segmentation. \textbf{2) More
Natural Conversation}: Improved capability for multi-turn dialogue, with the
ability to incorporate segmentation results directly into text responses, i.e.,
Segmentation in Dialogue (SiD). These improvements are achieved by curating the
existing samples of generic segmentation datasets, aimed specifically at
enhancing the segmentation and conversational skills without structural change
and additional data sources. Comparative analysis with the original LISA model
shows significant advancements in these areas, positioning LISA++ as a notable
upgrade in visual understanding and interaction. LISA++'s adaptability and
improved features highlight the versatility of the mask-as-embedding paradigm
proposed by LISA, and the potential as a foundational model for diverse
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00334">Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Echim_S/0/1/0/all/0/1">Sebastian-Vasile Echim</a>, <a href="http://arxiv.org/find/cs/1/au:+Taiatu_I/0/1/0/all/0/1">Iulian-Marius T&#x103;iatu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1">Dumitru-Clementin Cercel</a>, <a href="http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1">Florin Pop</a></p>
<p>This work focuses on plant leaf disease classification and explores three
crucial aspects: adversarial training, model explainability, and model
compression. The models' robustness against adversarial attacks is enhanced
through adversarial training, ensuring accurate classification even in the
presence of threats. Leveraging explainability techniques, we gain insights
into the model's decision-making process, improving trust and transparency.
Additionally, we explore model compression techniques to optimize computational
efficiency while maintaining classification performance. Through our
experiments, we determine that on a benchmark dataset, the robustness can be
the price of the classification accuracy with performance reductions of 3%-20%
for regular tests and gains of 50%-70% for adversarial attack tests. We also
demonstrate that a student model can be 15-25 times more computationally
efficient for a slight performance reduction, distilling the knowledge of more
complex models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02436">Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis. (arXiv:2401.02436v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niedermayr_S/0/1/0/all/0/1">Simon Niedermayr</a>, <a href="http://arxiv.org/find/cs/1/au:+Stumpfegger_J/0/1/0/all/0/1">Josef Stumpfegger</a>, <a href="http://arxiv.org/find/cs/1/au:+Westermann_R/0/1/0/all/0/1">R&#xfc;diger Westermann</a></p>
<p>Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02610">DHGCN: Dynamic Hop Graph Convolution Network for Self-Supervised Point Cloud Learning. (arXiv:2401.02610v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jincen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lizhi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xuequan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1">Imran Razzak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meili Wang</a></p>
<p>Recent works attempt to extend Graph Convolution Networks (GCNs) to point
clouds for classification and segmentation tasks. These works tend to sample
and group points to create smaller point sets locally and mainly focus on
extracting local features through GCNs, while ignoring the relationship between
point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network
(DHGCN) for explicitly learning the contextual relationships between the
voxelized point parts, which are treated as graph nodes. Motivated by the
intuition that the contextual information between point parts lies in the
pairwise adjacent relationship, which can be depicted by the hop distance of
the graph quantitatively, we devise a novel self-supervised part-level hop
distance reconstruction task and design a novel loss function accordingly to
facilitate training. In addition, we propose the Hop Graph Attention (HGA),
which takes the learned hop distance as input for producing attention weights
to allow edge features to contribute distinctively in aggregation. Eventually,
the proposed DHGCN is a plug-and-play module that is compatible with
point-based backbone networks. Comprehensive experiments on different backbones
and tasks demonstrate that our self-supervised method achieves state-of-the-art
performance. Our source code is available at: https://github.com/Jinec98/DHGCN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04614">Generic Knowledge Boosted Pre-training For Remote Sensing Images. (arXiv:2401.04614v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yuan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhong Wang</a></p>
<p>Deep learning models are essential for scene classification, change
detection, land cover segmentation, and other remote sensing image
understanding tasks. Most backbones of existing remote sensing deep learning
models are typically initialized by pre-trained weights obtained from ImageNet
pre-training (IMP). However, domain gaps exist between remote sensing images
and natural images (e.g., ImageNet), making deep learning models initialized by
pre-trained weights of IMP perform poorly for remote sensing image
understanding. Although some pre-training methods are studied in the remote
sensing community, current remote sensing pre-training methods face the problem
of vague generalization by only using remote sensing images. In this paper, we
propose a novel remote sensing pre-training framework, Generic Knowledge
Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations
from remote sensing and natural images for remote sensing understanding tasks.
GeRSP contains two pre-training branches: (1) A self-supervised pre-training
branch is adopted to learn domain-related representations from unlabeled remote
sensing images. (2) A supervised pre-training branch is integrated into GeRSP
for general knowledge learning from labeled natural images. Moreover, GeRSP
combines two pre-training branches using a teacher-student architecture to
simultaneously learn representations with general and special knowledge, which
generates a powerful pre-trained model for deep learning model initialization.
Finally, we evaluate GeRSP and other remote sensing pre-training methods on
three downstream tasks, i.e., object detection, semantic segmentation, and
scene classification. The extensive experimental results consistently
demonstrate that GeRSP can effectively learn robust representations in a
unified manner, improving the performance of remote sensing downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06144">DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1">Alex Havrilla</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojas_K/0/1/0/all/0/1">Kevin Rojas</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wenjing Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1">Molei Tao</a></p>
<p>Diffusion generative models have achieved remarkable success in generating
images with a fixed resolution. However, existing models have limited ability
to generalize to different resolutions when training data at those resolutions
are not available. Leveraging techniques from operator learning, we present a
novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the
score operator by combining both spatial and spectral information at multiple
resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1)
simultaneously training on multiple resolutions improves FID over training at
any single fixed resolution; 2) DFU generalizes beyond its training
resolutions, allowing for coherent, high-fidelity generation at
higher-resolutions with the same model, i.e. zero-shot super-resolution
image-generation; 3) we propose a fine-tuning strategy to further enhance the
zero-shot super-resolution image-generation capability of our model, leading to
a FID of 11.3 at 1.66 times the maximum training resolution on FFHQ, which no
other method can come close to achieving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.07278">Semi-supervised Semantic Segmentation using Redesigned Self-Training for White Blood Cell. (arXiv:2401.07278v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luu_V/0/1/0/all/0/1">Vinh Quoc Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1">Duy Khanh Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1">Minh Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thinh Tien Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinh_V/0/1/0/all/0/1">Vinh Quang Dinh</a></p>
<p>Artificial Intelligence (AI) in healthcare, especially in white blood cell
cancer diagnosis, is hindered by two primary challenges: the lack of
large-scale labeled datasets for white blood cell (WBC) segmentation and
outdated segmentation methods. To address the first challenge, a
semi-supervised learning framework should be brought to efficiently annotate
the large dataset. In this work, we address this issue by proposing a novel
self-training pipeline with the incorporation of FixMatch. We discover that by
incorporating FixMatch in the self-training pipeline, the performance improves
in the majority of cases. Our performance achieved the best performance with
the self-training scheme with consistency on DeepLab-V3 architecture and
ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC
datasets, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.07378">Efficient approximation of Earth Mover&#x27;s Distance Based on Nearest Neighbor Search. (arXiv:2401.07378v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1">Guangyu Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ruyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Peixian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danny Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Niemier_M/0/1/0/all/0/1">Michael Niemier</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">X.Sharon Hu</a></p>
<p>Earth Mover's Distance (EMD) is an important similarity measure between two
distributions, used in computer vision and many other application domains.
However, its exact calculation is computationally and memory intensive, which
hinders its scalability and applicability for large-scale problems. Various
approximate EMD algorithms have been proposed to reduce computational costs,
but they suffer lower accuracy and may require additional memory usage or
manual parameter tuning. In this paper, we present a novel approach, NNS-EMD,
to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve
high accuracy, low time complexity, and high memory efficiency. The NNS
operation reduces the number of data points compared in each NNS iteration and
offers opportunities for parallel processing. We further accelerate NNS-EMD via
vectorization on GPU, which is especially beneficial for large datasets. We
compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD
algorithms on image classification and retrieval tasks. We also apply NNS-EMD
to calculate transport mapping and realize color transfer between images.
NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and
achieves superior accuracy, speedup, and memory efficiency over existing
approximate EMD methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.07450">Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhifeng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Huiming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengtian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Ying Cao</a></p>
<p>Cross-modal fashion synthesis and editing offer intelligent support to
fashion designers by enabling the automatic generation and local modification
of design drafts.While current diffusion models demonstrate commendable
stability and controllability in image synthesis,they still face significant
challenges in generating fashion design from abstract design elements and
fine-grained editing.Abstract sensory expressions, \eg office, business, and
party, form the high-level design concepts, while measurable aspects like
sleeve length, collar type, and pant length are considered the low-level
attributes of clothing.Controlling and editing fashion images using lengthy
text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a
novel fashion design method using the shared multi-stage diffusion model
encompassing high-level design concepts and low-level clothing attributes in a
hierarchical structure.Specifically, we categorized the input text into
different levels and fed them in different time step to the diffusion model
according to the criteria of professional clothing designers.HieraFashDiff
allows designers to add low-level attributes after high-level prompts for
interactive editing incrementally.In addition, we design a differentiable loss
function in the sampling process with a mask to keep non-edit
areas.Comprehensive experiments performed on our newly conducted Hierarchical
fashion dataset,demonstrate that our proposed method outperforms other
state-of-the-art competitors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08503">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis. (arXiv:2401.08503v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhenhui Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Tianyun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yi Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiaqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weichuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiawei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Ziyue Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinzheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Rongjie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xiang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zejun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from
an unseen image, and then animate it with a reference video or audio to
generate a talking portrait video. The existing methods fail to simultaneously
achieve the goals of accurate 3D avatar reconstruction and stable talking face
animation. Besides, while the existing works mainly focus on synthesizing the
head part, it is also vital to generate natural torso and background segments
to obtain a realistic talking portrait video. To address these limitations, we
present Real3D-Potrait, a framework that (1) improves the one-shot 3D
reconstruction power with a large image-to-plane model that distills 3D prior
knowledge from a 3D face generative model; (2) facilitates accurate
motion-conditioned animation with an efficient motion adapter; (3) synthesizes
realistic video with natural torso movement and switchable background using a
head-torso-background super-resolution model; and (4) supports one-shot
audio-driven talking face generation with a generalizable audio-to-motion
model. Extensive experiments show that Real3D-Portrait generalizes well to
unseen identities and generates more realistic talking portrait videos compared
to previous methods. Video samples and source code are available at
https://real3dportrait.github.io .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08573">Benchmarking the Robustness of Image Watermarks. (arXiv:2401.08573v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bang An</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mucong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabbani_T/0/1/0/all/0/1">Tahseen Rabbani</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1">Aakriti Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuancheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chenghao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Sicheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1">Abdirisak Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yuxin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a></p>
<p>This paper investigates the weaknesses of image watermarking techniques. We
present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel
benchmark for assessing watermark robustness, overcoming the limitations of
current evaluation methods.WAVES integrates detection and identification tasks,
and establishes a standardized evaluation protocol comprised of a diverse range
of stress tests. The attacks in WAVES range from traditional image distortions
to advanced and novel variations of diffusive, and adversarial attacks. Our
evaluation examines two pivotal dimensions: the degree of image quality
degradation and the efficacy of watermark detection after attacks. We develop a
series of Performance vs. Quality 2D plots, varying over several prominent
image similarity metrics, which are then aggregated in a heuristically novel
manner to paint an overall picture of watermark robustness and attack potency.
Our comprehensive evaluation reveals previously undetected vulnerabilities of
several modern watermarking algorithms. We envision WAVES as a toolkit for the
future development of robust watermarking systems. The project is available at
https://wavesbench.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08865">The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1">Maciej A. Mazurowski</a></p>
<p>This paper investigates discrepancies in how neural networks learn from
different imaging domains, which are commonly overlooked when adopting computer
vision techniques from the domain of natural images to other specialized
domains such as medical images. Recent works have found that the generalization
error of a trained network typically increases with the intrinsic dimension
($d_{data}$) of its training set. Yet, the steepness of this relationship
varies significantly between medical (radiological) and natural imaging
domains, with no existing theoretical explanation. We address this gap in
knowledge by establishing and empirically validating a generalization scaling
law with respect to $d_{data}$, and propose that the substantial scaling
discrepancy between the two considered domains may be at least partially
attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging
datasets, a metric which we propose. Next, we demonstrate an additional benefit
of measuring the label sharpness of a training set: it is negatively correlated
with the trained model's adversarial robustness, which notably leads to models
for medical images having a substantially higher vulnerability to adversarial
attack. Finally, we extend our $d_{data}$ formalism to the related metric of
learned representation intrinsic dimension ($d_{repr}$), derive a
generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$
serves as an upper bound for $d_{repr}$. Our theoretical results are supported
by thorough experiments with six models and eleven natural and medical imaging
datasets over a range of training set sizes. Our findings offer insights into
the influence of intrinsic dataset properties on generalization, representation
learning, and robustness in deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09059">Autonomous Catheterization with Open-source Simulator and Expert Trajectory. (arXiv:2401.09059v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jianu_T/0/1/0/all/0/1">Tudor Jianu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Baoru Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1">Tuan Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1">Minh Nhat Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jingxuan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Omisore_O/0/1/0/all/0/1">Olatunji Omisore</a>, <a href="http://arxiv.org/find/cs/1/au:+Berthet_Rayne_P/0/1/0/all/0/1">Pierre Berthet-Rayne</a>, <a href="http://arxiv.org/find/cs/1/au:+Fichera_S/0/1/0/all/0/1">Sebastiano Fichera</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a></p>
<p>Endovascular robots have been actively developed in both academia and
industry. However, progress toward autonomous catheterization is often hampered
by the widespread use of closed-source simulators and physical phantoms.
Additionally, the acquisition of large-scale datasets for training machine
learning algorithms with endovascular robots is usually infeasible due to
expensive medical procedures. In this chapter, we introduce CathSim, the first
open-source simulator for endovascular intervention to address these
limitations. CathSim emphasizes real-time performance to enable rapid
development and testing of learning algorithms. We validate CathSim against the
real robot and show that our simulator can successfully mimic the behavior of
the real robot. Based on CathSim, we develop a multimodal expert navigation
network and demonstrate its effectiveness in downstream endovascular navigation
tasks. The intensive experimental results suggest that CathSim has the
potential to significantly accelerate research in the autonomous
catheterization field. Our project is publicly available at
https://github.com/airvlab/cathsim.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09495">IPR-NeRF: Ownership Verification meets Neural Radiance Field. (arXiv:2401.09495v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1">Win Kent Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1">Kam Woh Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1">Chee Seng Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yi Zhe Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a></p>
<p>Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09496">Learning to Generalize over Subpartitions for Heterogeneity-aware Domain Adaptive Nuclei Segmentation. (arXiv:2401.09496v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jianan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dongnan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Hang Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weidong Cai</a></p>
<p>Annotation scarcity and cross-modality/stain data distribution shifts are two
major obstacles hindering the application of deep learning models for nuclei
analysis, which holds a broad spectrum of potential applications in digital
pathology. Recently, unsupervised domain adaptation (UDA) methods have been
proposed to mitigate the distributional gap between different imaging
modalities for unsupervised nuclei segmentation in histopathology images.
However, existing UDA methods are built upon the assumption that data
distributions within each domain should be uniform. Based on the
over-simplified supposition, they propose to align the histopathology target
domain with the source domain integrally, neglecting severe intra-domain
discrepancy over subpartitions incurred by mixed cancer types and sampling
organs. In this paper, for the first time, we propose to explicitly consider
the heterogeneity within the histopathology domain and introduce open compound
domain adaptation (OCDA) to resolve the crux. In specific, a two-stage
disentanglement framework is proposed to acquire domain-invariant feature
representations at both image and instance levels. The holistic design
addresses the limitations of existing OCDA approaches which struggle to capture
instance-wise variations. Two regularization strategies are specifically
devised herein to leverage the rich subpartition-specific characteristics in
histopathology images and facilitate subdomain decomposition. Moreover, we
propose a dual-branch nucleus shape and structure preserving module to prevent
nucleus over-generation and deformation in the synthesized images. Experimental
results on both cross-modality and cross-stain scenarios over a broad range of
diverse datasets demonstrate the superiority of our method compared with
state-of-the-art UDA and OCDA methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09671">Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1">Sagar Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xiao Fu</a></p>
<p>Unsupervised domain translation (UDT) aims to find functions that convert
samples from one domain (e.g., sketches) to another domain (e.g., photos)
without changing the high-level semantic meaning (also referred to as
``content''). The translation functions are often sought by probability
distribution matching of the transformed source domain and target domain.
CycleGAN stands as arguably the most representative approach among this line of
work. However, it was noticed in the literature that CycleGAN and variants
could fail to identify the desired translation functions and produce
content-misaligned translations. This limitation arises due to the presence of
multiple translation functions -- referred to as ``measure-preserving
automorphism" (MPA) -- in the solution space of the learning criteria. Despite
awareness of such identifiability issues, solutions have remained elusive. This
study delves into the core identifiability inquiry and introduces an MPA
elimination theory. Our analysis shows that MPA is unlikely to exist, if
multiple pairs of diverse cross-domain conditional distributions are matched by
the learning function. Our theory leads to a UDT learner using distribution
matching over auxiliary variable-induced subsets of the domains -- other than
over the entire data domains as in the classical approaches. The proposed
framework is the first to rigorously establish translation identifiability
under reasonable UDT settings, to our best knowledge. Experiments corroborate
with our theoretical claims.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09826">Boosting Few-Shot Semantic Segmentation Via Segment Anything Model. (arXiv:2401.09826v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chen-Bin Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1">Qi Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangdao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Houcheng Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Vong_C/0/1/0/all/0/1">Chi-Man Vong</a></p>
<p>In semantic segmentation, accurate prediction masks are crucial for
downstream tasks such as medical image analysis and image editing. Due to the
lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in
predicting masks with precise contours. Recently, we have noticed that the
large foundation model segment anything model (SAM) performs well in processing
detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by
addressing the issue of inaccurate contour. The FSS-SAM is training-free. It
works as a post-processing tool for any FSS methods and can improve the
accuracy of predicted masks. Specifically, we use predicted masks from FSS
methods to generate prompts and then use SAM to predict new masks. To avoid
predicting wrong masks with SAM, we propose a prediction result selection (PRS)
algorithm. The algorithm can remarkably decrease wrong predictions. Experiment
results on public datasets show that our method is superior to base FSS methods
in both quantitative and qualitative aspects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10150">Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation. (arXiv:2401.10150v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changgu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1">Junwei Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lianggangxu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1">Gaoqi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a></p>
<p>Recent large-scale pre-trained diffusion models have demonstrated a powerful
generative ability to produce high-quality videos from detailed text
descriptions. However, exerting control over the motion of objects in videos
generated by any video diffusion model is a challenging problem. In this paper,
we propose a novel zero-shot moving object trajectory control framework,
Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video
diffusion model. To this end, an initial noise prior module is designed to
provide a position-based prior to improve the stability of the appearance of
the moving object and the accuracy of position. In addition, based on the
attention map of the U-net, spatial constraints are directly applied to the
denoising process of diffusion models, which further ensures the positional and
spatial consistency of moving objects during the inference. Furthermore,
temporal consistency is guaranteed with a proposed shift temporal attention
mechanism. Our method can be flexibly applied to various state-of-the-art video
diffusion models without any training process. Extensive experiments
demonstrate our proposed method can control the motion trajectories of objects
and generate high-quality videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02015">Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Ling Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Shenda Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhilong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhilin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zheming Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1">Bin Cui</a></p>
<p>Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06344">Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weizheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1">Le Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Baijian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guohua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1">Byung-Cheol Min</a></p>
<p>Predicting crowded intents and trajectories is crucial in varouls real-world
applications, including service robots and autonomous vehicles. Understanding
environmental dynamics is challenging, not only due to the complexities of
modeling pair-wise spatial and temporal interactions but also the diverse
influence of group-wise interactions. To decode the comprehensive pair-wise and
group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a
Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory
prediction. In Hyper-STTN, crowded group-wise correlations are constructed
using a set of multi-scale hypergraphs with varying group sizes, captured
through random-walk robability-based hypergraph spectral convolution.
Additionally, a spatial-temporal transformer is adapted to capture pedestrians'
pair-wise latent interactions in spatial-temporal dimensions. These
heterogeneous group-wise and pair-wise are then fused and aligned though a
multimodal transformer network. Hyper-STTN outperformes other state-of-the-art
baselines and ablation models on 5 real-world pedestrian motion datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10586">PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Ping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingchuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingfu Zhang</a></p>
<p>Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model's architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.
</p>
</p>
</div>

    </div>
    </body>
    