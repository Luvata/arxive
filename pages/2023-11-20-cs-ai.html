<!DOCTYPE html>
<html>
<head>
<title>2023-11-20-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.09224">The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks. (arXiv:2311.09224v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsamados_A/0/1/0/all/0/1">Andreas Tsamados</a>, <a href="http://arxiv.org/find/cs/1/au:+Floridi_L/0/1/0/all/0/1">Luciano Floridi</a>, <a href="http://arxiv.org/find/cs/1/au:+Taddeo_M/0/1/0/all/0/1">Mariarosaria Taddeo</a></p>
<p>The widespread integration of autoregressive-large language models (AR-LLMs),
such as ChatGPT, across established applications, like search engines, has
introduced critical vulnerabilities with uniquely scalable characteristics. In
this commentary, we analyse these vulnerabilities, their dependence on natural
language as a vector of attack, and their challenges to cybersecurity best
practices. We offer recommendations designed to mitigate these challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09227">Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives. (arXiv:2311.09227v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seger_E/0/1/0/all/0/1">Elizabeth Seger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dreksler_N/0/1/0/all/0/1">Noemi Dreksler</a>, <a href="http://arxiv.org/find/cs/1/au:+Moulange_R/0/1/0/all/0/1">Richard Moulange</a>, <a href="http://arxiv.org/find/cs/1/au:+Dardaman_E/0/1/0/all/0/1">Emily Dardaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuett_J/0/1/0/all/0/1">Jonas Schuett</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1">K. Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1">Christoph Winter</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1">Mackenzie Arnold</a>, <a href="http://arxiv.org/find/cs/1/au:+hEigeartaigh_S/0/1/0/all/0/1">Se&#xe1;n &#xd3; h&#xc9;igeartaigh</a>, <a href="http://arxiv.org/find/cs/1/au:+Korinek_A/0/1/0/all/0/1">Anton Korinek</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderljung_M/0/1/0/all/0/1">Markus Anderljung</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucknall_B/0/1/0/all/0/1">Ben Bucknall</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1">Alan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Stafford_E/0/1/0/all/0/1">Eoghan Stafford</a>, <a href="http://arxiv.org/find/cs/1/au:+Koessler_L/0/1/0/all/0/1">Leonie Koessler</a>, <a href="http://arxiv.org/find/cs/1/au:+Ovadya_A/0/1/0/all/0/1">Aviv Ovadya</a>, <a href="http://arxiv.org/find/cs/1/au:+Garfinkel_B/0/1/0/all/0/1">Ben Garfinkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bluemke_E/0/1/0/all/0/1">Emma Bluemke</a>, <a href="http://arxiv.org/find/cs/1/au:+Aird_M/0/1/0/all/0/1">Michael Aird</a>, <a href="http://arxiv.org/find/cs/1/au:+Levermore_P/0/1/0/all/0/1">Patrick Levermore</a>, <a href="http://arxiv.org/find/cs/1/au:+Hazell_J/0/1/0/all/0/1">Julian Hazell</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhishek Gupta</a></p>
<p>Recent decisions by leading AI labs to either open-source their models or to
restrict access to their models has sparked debate about whether, and how,
increasingly capable AI models should be shared. Open-sourcing in AI typically
refers to making model architecture and weights freely and publicly accessible
for anyone to modify, study, build on, and use. This offers advantages such as
enabling external oversight, accelerating progress, and decentralizing control
over AI development and use. However, it also presents a growing potential for
misuse and unintended consequences. This paper offers an examination of the
risks and benefits of open-sourcing highly capable foundation models. While
open-sourcing has historically provided substantial net benefits for most
software and AI development processes, we argue that for some highly capable
foundation models likely to be developed in the near future, open-sourcing may
pose sufficiently extreme risks to outweigh the benefits. In such a case,
highly capable foundation models should not be open-sourced, at least not
initially. Alternative strategies, including non-open-source model sharing
options, are explored. The paper concludes with recommendations for developers,
standard-setting bodies, and governments for establishing safe and responsible
model sharing practices and preserving open-source benefits where safe.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09230">Evaluating and Improving Value Judgments in AI: A Scenario-Based Study on Large Language Models&#x27; Depiction of Social Conventions. (arXiv:2311.09230v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1">Jaeyoun You</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_B/0/1/0/all/0/1">Bongwon Suh</a></p>
<p>The adoption of generative AI technologies is swiftly expanding. Services
employing both linguistic and mul-timodal models are evolving, offering users
increasingly precise responses. Consequently, human reliance on these
technologies is expected to grow rapidly. With the premise that people will be
impacted by the output of AI, we explored approaches to help AI output produce
better results. Initially, we evaluated how contemporary AI services
competitively meet user needs, then examined society's depiction as mirrored by
Large Language Models (LLMs). We did a query experiment, querying about social
conventions in various countries and eliciting a one-word response. We compared
the LLMs' value judgments with public data and suggested an model of
decision-making in value-conflicting scenarios which could be adopted for
future machine value judgments. This paper advocates for a practical approach
to using AI as a tool for investigating other remote worlds. This re-search has
significance in implicitly rejecting the notion of AI making value judgments
and instead arguing a more critical perspective on the environment that defers
judgmental capabilities to individuals. We anticipate this study will empower
anyone, regardless of their capacity, to receive safe and accurate value
judgment-based out-puts effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09231">Key Factors Affecting European Reactions to AI in European Full and Flawed Democracies. (arXiv:2311.09231v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_L/0/1/0/all/0/1">Long Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+OSullivan_B/0/1/0/all/0/1">Barry O&#x27;Sullivan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1">Tai Tan Mai</a></p>
<p>This study examines the key factors that affect European reactions to
artificial intelligence (AI) in the context of both full and flawed democracies
in Europe. Analysing a dataset of 4,006 respondents, categorised into full
democracies and flawed democracies based on the Democracy Index developed by
the Economist Intelligence Unit (EIU), this research identifies crucial factors
that shape European attitudes toward AI in these two types of democracies. The
analysis reveals noteworthy findings. Firstly, it is observed that flawed
democracies tend to exhibit higher levels of trust in government entities
compared to their counterparts in full democracies. Additionally, individuals
residing in flawed democracies demonstrate a more positive attitude toward AI
when compared to respondents from full democracies. However, the study finds no
significant difference in AI awareness between the two types of democracies,
indicating a similar level of general knowledge about AI technologies among
European citizens. Moreover, the study reveals that trust in AI measures,
specifically "Trust AI Solution", does not significantly vary between full and
flawed democracies. This suggests that despite the differences in democratic
quality, both types of democracies have similar levels of confidence in AI
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09235">Scalable Diffusion for Materials Generation. (arXiv:2311.09235v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mengjiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">KwangHwan Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Merchant_A/0/1/0/all/0/1">Amil Merchant</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1">Dale Schuurmans</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1">Ekin Dogus Cubuk</a></p>
<p>Generative models trained on internet-scale data are capable of generating
novel and realistic texts, images, and videos. A natural next question is
whether these models can advance science, for example by generating novel
stable materials. Traditionally, models with explicit structures (e.g., graphs)
have been used in modeling structural relationships in scientific data (e.g.,
atoms and bonds in crystals), but generating structures can be difficult to
scale to large and complex systems. Another challenge in generating materials
is the mismatch between standard generative modeling metrics and downstream
applications. For instance, common metrics such as the reconstruction error do
not correlate well with the downstream goal of discovering stable materials. In
this work, we tackle the scalability challenge by developing a unified crystal
representation that can represent any crystal structure (UniMat), followed by
training a diffusion probabilistic model on these UniMat representations. Our
empirical results suggest that despite the lack of explicit structure modeling,
UniMat can generate high fidelity crystal structures from larger and more
complex chemical systems, outperforming previous graph-based approaches under
various generative modeling metrics. To better connect the generation quality
of materials to downstream applications, such as discovering novel stable
materials, we propose additional metrics for evaluating generative models of
materials, including per-composition formation energy and stability with
respect to convex hulls through decomposition energy from Density Function
Theory (DFT). Lastly, we show that conditional generation with UniMat can scale
to previously established crystal datasets with up to millions of crystals
structures, outperforming random structure search (the current leading method
for structure discovery) in discovering new stable materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09240">Devil in the Landscapes: Inferring Epidemic Exposure Risks from Street View Imagery. (arXiv:2311.09240v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhenyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1">Yanxin Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a></p>
<p>Built environment supports all the daily activities and shapes our health.
Leveraging informative street view imagery, previous research has established
the profound correlation between the built environment and chronic,
non-communicable diseases; however, predicting the exposure risk of infectious
diseases remains largely unexplored. The person-to-person contacts and
interactions contribute to the complexity of infectious disease, which is
inherently different from non-communicable diseases. Besides, the complex
relationships between street view imagery and epidemic exposure also hinder
accurate predictions. To address these problems, we construct a regional
mobility graph informed by the gravity model, based on which we propose a
transmission-aware graph convolutional network (GCN) to capture disease
transmission patterns arising from human mobility. Experiments show that the
proposed model significantly outperforms baseline models by 8.54% in weighted
F1, shedding light on a low-cost, scalable approach to assess epidemic exposure
risks from street view imagery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09241">Chain of Images for Intuitively Reasoning. (arXiv:2311.09241v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanxu Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haotong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a></p>
<p>The human brain is naturally equipped to comprehend and interpret visual
information rapidly. When confronted with complex problems or concepts, we use
flowcharts, sketches, and diagrams to aid our thought process. Leveraging this
inherent ability can significantly enhance logical reasoning. However, current
Large Language Models (LLMs) do not utilize such visual intuition to help their
thinking. Even the most advanced version language models (e.g., GPT-4V and
LLaVA) merely align images into textual space, which means their reasoning
processes remain purely verbal. To mitigate such limitations, we present a
Chain of Images (CoI) approach, which can convert complex language reasoning
problems to simple pattern recognition by generating a series of images as
intermediate representations. Furthermore, we have developed a CoI evaluation
dataset encompassing 15 distinct domains where images can intuitively aid
problem-solving. Based on this dataset, we aim to construct a benchmark to
assess the capability of future multimodal large-scale models to leverage
images for reasoning. In supporting our CoI reasoning, we introduce a symbolic
multimodal large language model (SyMLLM) that generates images strictly based
on language instructions and accepts both text and image as input. Experiments
on Geometry, Chess and Common Sense tasks sourced from the CoI evaluation
dataset show that CoI improves performance significantly over the pure-language
Chain of Thoughts (CoT) baselines. The code is available at
https://github.com/GraphPKU/CoI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09243">Evaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling. (arXiv:2311.09243v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Yujin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Mingeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seojin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_O/0/1/0/all/0/1">Oyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_R/0/1/0/all/0/1">Ryan Donghan Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yoonha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1">Dohyun Lim</a></p>
<p>This study investigates the efficacy of Large Language Models (LLMs) in
interactive language therapy for high-functioning autistic adolescents. With
the rapid advancement of artificial intelligence, particularly in natural
language processing, LLMs present a novel opportunity to augment traditional
psychological counseling methods. This research primarily focuses on evaluating
the LLM's ability to engage in empathetic, adaptable, and contextually
appropriate interactions within a therapeutic setting. A comprehensive
evaluation was conducted by a panel of clinical psychologists and psychiatrists
using a specially developed scorecard. The assessment covered various aspects
of the LLM's performance, including empathy, communication skills,
adaptability, engagement, and the ability to establish a therapeutic alliance.
The study avoided direct testing with patients, prioritizing privacy and
ethical considerations, and instead relied on simulated scenarios to gauge the
LLM's effectiveness. The results indicate that LLMs hold significant promise as
supportive tools in therapy, demonstrating strengths in empathetic engagement
and adaptability in conversation. However, challenges in achieving the depth of
personalization and emotional understanding characteristic of human therapists
were noted. The study also highlights the importance of ethical considerations
in the application of AI in therapeutic contexts. This research provides
valuable insights into the potential and limitations of using LLMs in
psychological counseling for autistic adolescents. It lays the groundwork for
future explorations into AI's role in mental health care, emphasizing the need
for ongoing development to enhance the capabilities of these models in
therapeutic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09247">Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. (arXiv:2311.09247v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1">Melanie Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Palmarini_A/0/1/0/all/0/1">Alessandro B. Palmarini</a>, <a href="http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1">Arseny Moskvichev</a></p>
<p>We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09248">Smart Home Goal Feature Model -- A guide to support Smart Homes for Ageing in Place. (arXiv:2311.09248v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Logothetis_I/0/1/0/all/0/1">Irini Logothetis</a>, <a href="http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1">Priya Rani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivasothy_S/0/1/0/all/0/1">Shangeetha Sivasothy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasa_R/0/1/0/all/0/1">Rajesh Vasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouzakis_K/0/1/0/all/0/1">Kon Mouzakis</a></p>
<p>Smart technologies are significant in supporting ageing in place for elderly.
Leveraging Artificial Intelligence (AI) and Machine Learning (ML), it provides
peace of mind, enabling the elderly to continue living independently. Elderly
use smart technologies for entertainment and social interactions, this can be
extended to provide safety and monitor health and environmental conditions,
detect emergencies and notify informal and formal caregivers when care is
needed. This paper provides an overview of the smart home technologies
commercially available to support ageing in place, the advantages and
challenges of smart home technologies, and their usability from elderlys
perspective. Synthesizing prior knowledge, we created a structured Smart Home
Goal Feature Model (SHGFM) to resolve heuristic approaches used by the Subject
Matter Experts (SMEs) at aged care facilities and healthcare researchers in
adapting smart homes. The SHGFM provides SMEs the ability to (i) establish
goals and (ii) identify features to set up strategies to design, develop and
deploy smart homes for the elderly based on personalised needs. Our model
provides guidance to healthcare researchers and aged care industries to set up
smart homes based on the needs of elderly, by defining a set of goals at
different levels mapped to a different set of features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09255">Artificial intelligence and the skill premium. (arXiv:2311.09255v1 [econ.TH])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Bloom_D/0/1/0/all/0/1">David E. Bloom</a>, <a href="http://arxiv.org/find/econ/1/au:+Prettner_K/0/1/0/all/0/1">Klaus Prettner</a>, <a href="http://arxiv.org/find/econ/1/au:+Saadaoui_J/0/1/0/all/0/1">Jamel Saadaoui</a>, <a href="http://arxiv.org/find/econ/1/au:+Veruete_M/0/1/0/all/0/1">Mario Veruete</a></p>
<p>What will likely be the effect of the emergence of ChatGPT and other forms of
artificial intelligence (AI) on the skill premium? To address this question, we
develop a nested constant elasticity of substitution production function that
distinguishes between industrial robots and AI. Industrial robots predominantly
substitute for low-skill workers, whereas AI mainly helps to perform the tasks
of high-skill workers. We show that AI reduces the skill premium as long as it
is more substitutable for high-skill workers than low-skill workers are for
high-skill workers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09261">Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network. (arXiv:2311.09261v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1">Yongqi Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yao_Q/0/1/0/all/0/1">Quanming Yao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yue_L/0/1/0/all/0/1">Ling Yue</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Z/0/1/0/all/0/1">Ziheng Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lin_Z/0/1/0/all/0/1">Zhenxi Lin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a></p>
<p>Accurately predicting drug-drug interactions (DDI) for emerging drugs, which
offer possibilities for treating and alleviating diseases, with computational
methods can improve patient care and contribute to efficient drug development.
However, many existing computational methods require large amounts of known DDI
information, which is scarce for emerging drugs. In this paper, we propose
EmerGNN, a graph neural network (GNN) that can effectively predict interactions
for emerging drugs by leveraging the rich information in biomedical networks.
EmerGNN learns pairwise representations of drugs by extracting the paths
between drug pairs, propagating information from one drug to the other, and
incorporating the relevant biomedical concepts on the paths. The different
edges on the biomedical network are weighted to indicate the relevance for the
target DDI prediction. Overall, EmerGNN has higher accuracy than existing
approaches in predicting interactions for emerging drugs and can identify the
most relevant information on the biomedical network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09262">Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values. (arXiv:2311.09262v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zhikai Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1">Guoxiu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhuoren Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yangyang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Star Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wei Lu</a></p>
<p>The potential impact of an academic paper is determined by various factors,
including its popularity and contribution. Existing models usually estimate
original citation counts based on static graphs and fail to differentiate
values from nuanced perspectives. In this study, we propose a novel graph
neural network to Disentangle the Potential impacts of Papers into Diffusion,
Conformity, and Contribution values (called DPPDCC). Given a target paper,
DPPDCC encodes temporal and structural features within the constructed dynamic
heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize
the importance of comparative and co-cited/citing information between papers
and aggregate snapshots evolutionarily. To unravel popularity, we contrast
augmented graphs to extract the essence of diffusion and predict the
accumulated citation binning to model conformity. We further apply orthogonal
constraints to encourage distinct modeling of each perspective and preserve the
inherent value of contribution. To evaluate models' generalization for papers
published at various times, we reformulate the problem by partitioning data
based on specific time points to mirror real-world conditions. Extensive
experimental results on three datasets demonstrate that DPPDCC significantly
outperforms baselines for previously, freshly, and immediately published
papers. Further analyses confirm its robust capabilities. We will make our
datasets and codes publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09263">Auto-ICL: In-Context Learning without Human Supervision. (arXiv:2311.09263v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinghan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>In the era of Large Language Models (LLMs), human-computer interaction has
evolved towards natural language, offering unprecedented flexibility. Despite
this, LLMs are heavily reliant on well-structured prompts to function
efficiently within the realm of In-Context Learning. Vanilla In-Context
Learning relies on human-provided contexts, such as labeled examples, explicit
instructions, or other guiding mechanisms that shape the model's outputs. To
address this challenge, our study presents a universal framework named
Automatic In-Context Learning. Upon receiving a user's request, we ask the
model to independently generate examples, including labels, instructions, or
reasoning pathways. The model then leverages this self-produced context to
tackle the given problem. Our approach is universally adaptable and can be
implemented in any setting where vanilla In-Context Learning is applicable. We
demonstrate that our method yields strong performance across a range of tasks,
standing up well when compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09264">Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response. (arXiv:2311.09264v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1">Jia Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a></p>
<p>High-throughput screening technology has facilitated the generation of
large-scale drug responses across hundreds of cancer cell lines. However, there
exists significant discrepancy between in vitro cell lines and actual tumors in
vivo in terms of their response to drug treatments, because of tumors comprise
of complex cellular compositions and histopathology structure, known as tumor
microenvironment (TME), which greatly influences the drug cytotoxicity against
tumor cells. To date, no study has focused on modeling the impact of the TME on
clinical drug response. This paper proposed a domain adaptation network for
feature disentanglement to separate representations of cancer cells and TME of
a tumor in patients. Two denoising autoencoders were separately used to extract
features from cell lines (source domain) and tumors (target domain) for partial
domain alignment and feature decoupling. The specific encoder was enforced to
extract information only about TME. Moreover, to ensure generalizability to
novel drugs, we applied a graph attention network to learn the latent
representation of drugs, allowing us to linearly model the drug perturbation on
cellular state in latent space. We calibrated our model on a benchmark dataset
and demonstrated its superior performance in predicting clinical drug response
and dissecting the influence of the TME on drug efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09266">Adversarially Robust Spiking Neural Networks Through Conversion. (arXiv:2311.09266v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ozdenizci_O/0/1/0/all/0/1">Ozan &#xd6;zdenizci</a>, <a href="http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1">Robert Legenstein</a></p>
<p>Spiking neural networks (SNNs) provide an energy-efficient alternative to a
variety of artificial neural network (ANN) based AI applications. As the
progress in neuromorphic computing with SNNs expands their use in applications,
the problem of adversarial robustness of SNNs becomes more pronounced. To the
contrary of the widely explored end-to-end adversarial training based
solutions, we address the limited progress in scalable robust SNN training
methods by proposing an adversarially robust ANN-to-SNN conversion algorithm.
Our method provides an efficient approach to embrace various computationally
demanding robust learning objectives that have been proposed for ANNs. During a
post-conversion robust finetuning phase, our method adversarially optimizes
both layer-wise firing thresholds and synaptic connectivity weights of the SNN
to maintain transferred robustness gains from the pre-trained ANN. We perform
experimental evaluations in numerous adaptive adversarial settings that account
for the spike-based operation dynamics of SNNs, and show that our approach
yields a scalable state-of-the-art solution for adversarially robust deep SNNs
with low-latency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09269">NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios. (arXiv:2311.09269v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1">En-Te Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1">Wei-Jie Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Ding-Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Long Zeng</a></p>
<p>Existing Object Pose Estimation (OPE) methods for stacked scenarios are not
robust to changes in object scale. This paper proposes a new 6DoF OPE network
(NormNet) for different scale objects in stacked scenarios. Specifically, each
object's scale is first learned with point-wise regression. Then, all objects
in the stacked scenario are normalized into the same scale through semantic
segmentation and affine transformation. Finally, they are fed into a shared
pose estimator to recover their 6D poses. In addition, we introduce a new
Sim-to-Real transfer pipeline, combining style transfer and domain
randomization. This improves the NormNet's performance on real data even if we
only train it on synthetic data. Extensive experiments demonstrate that the
proposed method achieves state-of-the-art performance on public benchmarks and
the MultiScale dataset we constructed. The real-world experiments show that our
method can robustly estimate the 6D pose of objects at different scales.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09271">An Empathetic User-Centric Chatbot for Emotional Support. (arXiv:2311.09271v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yanting Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yixuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yuchen Niu</a></p>
<p>This paper explores the intersection of Otome Culture and artificial
intelligence, particularly focusing on how Otome-oriented games fulfill the
emotional needs of young women. These games, which are deeply rooted in a
subcultural understanding of love, provide players with feelings of
satisfaction, companionship, and protection through carefully crafted narrative
structures and character development. With the proliferation of Large Language
Models (LLMs), there is an opportunity to transcend traditional static game
narratives and create dynamic, emotionally responsive interactions. We present
a case study of Tears of Themis, where we have integrated LLM technology to
enhance the interactive experience. Our approach involves augmenting existing
game narratives with a Question and Answer (QA) system, enriched through data
augmentation and emotional enhancement techniques, resulting in a chatbot that
offers realistic and supportive companionship.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09272">Linear time Evidence Accumulation Clustering with KMeans. (arXiv:2311.09272v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Candel_G/0/1/0/all/0/1">Ga&#xeb;lle Candel</a></p>
<p>Among ensemble clustering methods, Evidence Accumulation Clustering is one of
the simplest technics. In this approach, a co-association (CA) matrix
representing the co-clustering frequency is built and then clustered to extract
consensus clusters. Compared to other approaches, this one is simple as there
is no need to find matches between clusters obtained from two different
partitionings. Nevertheless, this method suffers from computational issues, as
it requires to compute and store a matrix of size n x n, where n is the number
of items. Due to the quadratic cost, this approach is reserved for small
datasets. This work describes a trick which mimic the behavior of average
linkage clustering. We found a way of computing efficiently the density of a
partitioning, reducing the cost from a quadratic to linear complexity.
Additionally, we proved that the k-means maximizes naturally the density. We
performed experiments on several benchmark datasets where we compared the
k-means and the bisecting version to other state-of-the-art consensus
algorithms. The k-means results are comparable to the best state of the art in
terms of NMI while keeping the computational cost low. Additionally, the
k-means led to the best results in terms of density. These results provide
evidence that consensus clustering can be solved with simple algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09273">In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment. (arXiv:2311.09273v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moshfeghi_S/0/1/0/all/0/1">Sonia Moshfeghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jan_M/0/1/0/all/0/1">Muhammad Tanveer Jan</a>, <a href="http://arxiv.org/find/cs/1/au:+Conniff_J/0/1/0/all/0/1">Joshua Conniff</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghoreishi_S/0/1/0/all/0/1">Seyedeh Gol Ara Ghoreishi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Jinwoo Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Furht_B/0/1/0/all/0/1">Borko Furht</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kwangsoo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosselli_M/0/1/0/all/0/1">Monica Rosselli</a>, <a href="http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1">David Newman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tappen_R/0/1/0/all/0/1">Ruth Tappen</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1">Dana Smith</a></p>
<p>Driving is a complex daily activity indicating age and disease related
cognitive declines. Therefore, deficits in driving performance compared with
ones without mild cognitive impairment (MCI) can reflect changes in cognitive
functioning. There is increasing evidence that unobtrusive monitoring of older
adults driving performance in a daily-life setting may allow us to detect
subtle early changes in cognition. The objectives of this paper include
designing low-cost in-vehicle sensing hardware capable of obtaining
high-precision positioning and telematics data, identifying important
indicators for early changes in cognition, and detecting early-warning signs of
cognitive impairment in a truly normal, day-to-day driving condition with
machine learning approaches. Our statistical analysis comparing drivers with
MCI to those without reveals that those with MCI exhibit smoother and safer
driving patterns. This suggests that drivers with MCI are cognizant of their
condition and tend to avoid erratic driving behaviors. Furthermore, our Random
Forest models identified the number of night trips, number of trips, and
education as the most influential factors in our data evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09278">Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models. (arXiv:2311.09278v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Fangzhi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qiushi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Siyu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Fei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shuai Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qika Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>Large Language Models (LLMs) have greatly propelled the progress in natural
language(NL)-centric tasks based on NL interface. However, the NL form is not
enough for world knowledge. Current works focus on this question by injecting
specific symbolic knowledge into LLM, which ignore two critical challenges: the
interrelations between various symbols and the balance between symbolic-centric
and NL-centric capabilities. In this work, we tackle these challenges from both
a data and framework perspective and introduce Symbol-LLM series models. First,
we collect 34 symbolic tasks, covering ~20 different forms, which are unified
to capture symbol interrelations. Then, a two-stage tuning framework succeeds
in injecting symbolic knowledge without loss of the generality ability.
Extensive experiments on both symbol- and NL-centric tasks demonstrate the
balanced and superior performances of Symbol-LLM series models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09308">Divergences between Language Models and Human Brains. (arXiv:2311.09308v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1">Emmy Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Wehbe_L/0/1/0/all/0/1">Leila Wehbe</a></p>
<p>Do machines and humans process language in similar ways? A recent line of
research has hinted in the affirmative, demonstrating that human brain signals
can be effectively predicted using the internal representations of language
models (LMs). This is thought to reflect shared computational principles
between LMs and human language processing. However, there are also clear
differences in how LMs and humans acquire and use language, even if the final
task they are performing is the same. Despite this, there is little work
exploring systematic differences between human and machine language processing
using brain data. To address this question, we examine the differences between
LM representations and the human brain's responses to language, specifically by
examining a dataset of Magnetoencephalography (MEG) responses to a written
narrative. In doing so we identify three phenomena that, in prior work, LMs
have been found to not capture well: emotional understanding, figurative
language processing, and physical commonsense. By fine-tuning LMs on datasets
related to these phenomena, we observe that fine-tuned LMs show improved
alignment with human brain responses across these tasks. Our study implies that
the observed divergences between LMs and human brains may stem from LMs'
inadequate representation of these specific types of knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09312">H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing. (arXiv:2311.09312v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Visani_G/0/1/0/all/0/1">Gian Marco Visani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Galvin_W/0/1/0/all/0/1">William Galvin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pun_M/0/1/0/all/0/1">Michael Neal Pun</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nourmohammad_A/0/1/0/all/0/1">Armita Nourmohammad</a></p>
<p>Accurately modeling protein 3D structure is essential for the design of
functional proteins. An important sub-task of structure modeling is protein
side-chain packing: predicting the conformation of side-chains (rotamers) given
the protein's backbone structure and amino-acid sequence. Conventional
approaches for this task rely on expensive sampling procedures over
hand-crafted energy functions and rotamer libraries. Recently, several deep
learning methods have been developed to tackle the problem in a data-driven
way, albeit with vastly different formulations (from image-to-image translation
to directly predicting atomic coordinates). Here, we frame the problem as a
joint regression over the side-chains' true degrees of freedom: the dihedral
$\chi$ angles. We carefully study possible objective functions for this task,
while accounting for the underlying symmetries of the task. We propose
Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain
packing built on top of two light-weight rotationally equivariant neural
networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is
computationally efficient and shows favorable performance against conventional
physics-based algorithms and is competitive against alternative deep learning
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09319">Spoken Word2Vec: A Perspective And Some Techniques. (arXiv:2311.09319v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sayeed_M/0/1/0/all/0/1">Mohammad Amaan Sayeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1">Hanan Aldarmaki</a></p>
<p>Text word embeddings that encode distributional semantic features work by
modeling contextual similarities of frequently occurring words. Acoustic word
embeddings, on the other hand, typically encode low-level phonetic
similarities. Semantic embeddings for spoken words have been previously
explored using similar algorithms to Word2Vec, but the resulting vectors still
mainly encoded phonetic rather than semantic features. In this paper, we
examine the assumptions and architectures used in previous works and show
experimentally how Word2Vec algorithms fail to encode distributional semantics
when the input units are acoustically correlated. In addition, previous works
relied on the simplifying assumptions of perfect word segmentation and
clustering by word type. Given these conditions, a trivial solution identical
to text-based embeddings has been overlooked. We follow this simpler path using
automatic word type clustering and examine the effects on the resulting
embeddings, highlighting the true challenges in this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09325">Improving fit to human reading times via temperature-scaled surprisal. (arXiv:2311.09325v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Skrjanec_I/0/1/0/all/0/1">Iza &#x160;krjanec</a>, <a href="http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1">Vera Demberg</a></p>
<p>Past studies have provided broad support for that words with lower
predictability (i.e., higher surprisal) require more time for comprehension by
using large language models (LLMs) to simulate humans' cognitive load. In
general, these studies have implicitly assumed that the probability scores from
LLMs are accurate, ignoring the discrepancies between human cognition and LLMs
from this standpoint. Inspired by the concept of probability calibration, we
are the first work to focus on the probability distribution for human reading
simulation. We propose to use temperature-scaled surprisal, a surprisal
calculated by shaped probability, to be the predictor of human reading times.
Our results across three corpora consistently revealed that such a surprisal
can drastically improve the prediction of reading times. Setting the
temperature to be approximately 2.5 across all models and datasets can yield up
to an 89% of increase in delta log-likelihood in our setting. We also propose a
calibration metric to quantify the possible human-likeness bias. Further
analysis was done and provided insights into this phenomenon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09333">Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production. (arXiv:2311.09333v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khosravi_H/0/1/0/all/0/1">Hamed Khosravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadpour_S/0/1/0/all/0/1">Sarah Farhadpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Grandhi_M/0/1/0/all/0/1">Manikanta Grandhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Raihan_A/0/1/0/all/0/1">Ahmed Shoyeb Raihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Srinjoy Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1">Imtiaz Ahmed</a></p>
<p>A significant challenge for predictive maintenance in the pulp-and-paper
industry is the infrequency of paper breaks during the production process. In
this article, operational data is analyzed from a paper manufacturing machine
in which paper breaks are relatively rare but have a high economic impact.
Utilizing a dataset comprising 18,398 instances derived from a quality
assurance protocol, we address the scarcity of break events (124 cases) that
pose a challenge for machine learning predictive models. With the help of
Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority
Oversampling Technique (SMOTE), we implement a novel data augmentation
framework. This method ensures that the synthetic data mirrors the distribution
of the real operational data but also seeks to enhance the performance metrics
of predictive modeling. Before and after the data augmentation, we evaluate
three different machine learning algorithms-Decision Trees (DT), Random Forest
(RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our
study achieved significant improvements in predictive maintenance performance
metrics. The efficacy of CTGAN in addressing data scarcity was evident, with
the models' detection of machine breaks (Class 1) improving by over 30% for
Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.
With this methodological advancement, this study contributes to industrial
quality control and maintenance scheduling by addressing rare event prediction
in manufacturing processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09335">Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1">George Chrysostomou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhixue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Miles Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Despite their remarkable performance on abstractive summarization, large
language models (LLMs) face two significant challenges: their considerable size
and tendency to hallucinate. Hallucinations are concerning because they erode
the reliability of LLMs and raise safety issues. Pruning is a technique that
reduces model size by removing redundant weights to create sparse models that
enable more efficient inference. Pruned models yield comparable performance to
their counterpart full-sized models, making them ideal alternatives when
operating on a limited budget. However, the effect that pruning has upon
hallucinations in abstractive summarization with LLMs has yet to be explored.
In this paper, we provide an extensive empirical study on the hallucinations
produced by pruned models across three standard summarization tasks, two
pruning approaches, three instruction-tuned LLMs, and three hallucination
evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less
compared to their full-sized counterparts. Our follow-up analysis suggests that
pruned models tend to depend more on the source input and less on their
parametric knowledge from pre-training for generation. This greater dependency
on the source input leads to a higher lexical overlap between generated content
and the source input, which can be a reason for the reduction in
hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09349">Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models. (arXiv:2311.09349v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Letafati_M/0/1/0/all/0/1">Mehdi Letafati</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Samad Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Latva_aho_M/0/1/0/all/0/1">Matti Latva-aho</a></p>
<p>Diffusion models are at the vanguard of generative AI research with renowned
solutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI.
Nevertheless, the potential merits of diffusion models for communication
engineering applications are not fully understood yet. In this paper, we aim to
unleash the power of generative AI for PHY design of constellation symbols in
communication systems. Although the geometry of constellations is predetermined
according to networking standards, e.g., quadrature amplitude modulation (QAM),
probabilistic shaping can design the probability of occurrence (generation) of
constellation symbols. This can help improve the information rate and decoding
performance of communication systems. We exploit the ``denoise-and-generate''
characteristics of denoising diffusion probabilistic models (DDPM) for
probabilistic constellation shaping. The key idea is to learn generating
constellation symbols out of noise, ``mimicking'' the way the receiver performs
symbol reconstruction. This way, we make the constellation symbols sent by the
transmitter, and what is inferred (reconstructed) at the receiver become as
similar as possible, resulting in as few mismatches as possible. Our results
show that the generative AI-based scheme outperforms deep neural network
(DNN)-based benchmark and uniform shaping, while providing network resilience
as well as robust out-of-distribution performance under low-SNR regimes and
non-Gaussian assumptions. Numerical evaluations highlight 30% improvement in
terms of cosine similarity and a threefold improvement in terms of mutual
information compared to DNN-based approach for 64-QAM geometry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09350">Generalizable Imitation Learning Through Pre-Trained Representations. (arXiv:2311.09350v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1">Wei-Di Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1">Francois Hogan</a>, <a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1">David Meger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Gregory Dudek</a></p>
<p>In this paper we leverage self-supervised vision transformer models and their
emergent semantic abilities to improve the generalization abilities of
imitation learning policies. We introduce BC-ViT, an imitation learning
algorithm that leverages rich DINO pre-trained Visual Transformer (ViT)
patch-level embeddings to obtain better generalization when learning through
demonstrations. Our learner sees the world by clustering appearance features
into semantic concepts, forming stable keypoints that generalize across a wide
range of appearance variations and object types. We show that this
representation enables generalized behaviour by evaluating imitation learning
across a diverse dataset of object manipulation tasks. Our method, data and
evaluation approach are made available to facilitate further study of
generalization in Imitation Learners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09355">Privacy Threats in Stable Diffusion Models. (arXiv:2311.09355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cilloni_T/0/1/0/all/0/1">Thomas Cilloni</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_C/0/1/0/all/0/1">Charles Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_C/0/1/0/all/0/1">Charles Walter</a></p>
<p>This paper introduces a novel approach to membership inference attacks (MIA)
targeting stable diffusion computer vision models, specifically focusing on the
highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract
sensitive information about a model's training data, posing significant privacy
concerns. Despite its advancements in image synthesis, our research reveals
privacy vulnerabilities in the stable diffusion models' outputs. Exploiting
this information, we devise a black-box MIA that only needs to query the victim
model repeatedly. Our methodology involves observing the output of a stable
diffusion model at different generative epochs and training a classification
model to distinguish when a series of intermediates originated from a training
sample or not. We propose numerous ways to measure the membership features and
discuss what works best. The attack's efficacy is assessed using the ROC AUC
method, demonstrating a 60\% success rate in inferring membership information.
This paper contributes to the growing body of research on privacy and security
in machine learning, highlighting the need for robust defenses against MIAs.
Our findings prompt a reevaluation of the privacy implications of stable
diffusion models, urging practitioners and developers to implement enhanced
security measures to safeguard against such attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09358">Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science. (arXiv:2311.09358v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1">Sridevi Wagle</a>, <a href="http://arxiv.org/find/cs/1/au:+Munikoti_S/0/1/0/all/0/1">Sai Munikoti</a>, <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Anurag Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1">Sara Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1">Sameera Horawalavithana</a></p>
<p>Large language models (LLMs) have shown remarkable achievements in natural
language processing tasks, producing high-quality outputs. However, LLMs still
exhibit limitations, including the generation of factually incorrect
information. In safety-critical applications, it is important to assess the
confidence of LLM-generated content to make informed decisions. Retrieval
Augmented Language Models (RALMs) is relatively a new area of research in NLP.
RALMs offer potential benefits for scientific NLP tasks, as retrieved
documents, can serve as evidence to support model-generated content. This
inclusion of evidence enhances trustworthiness, as users can verify and explore
the retrieved documents to validate model outputs. Quantifying uncertainty in
RALM generations further improves trustworthiness, with retrieved text and
confidence scores contributing to a comprehensive and reliable model for
scientific applications. However, there is limited to no research on UQ for
RALMs, particularly in scientific contexts. This study aims to address this gap
by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific
tasks. This research investigates how uncertainty scores vary when scientific
knowledge is incorporated as pretraining and retrieval data and explores the
relationship between uncertainty scores and the accuracy of model-generated
outputs. We observe that an existing RALM finetuned with scientific knowledge
as the retrieval data tends to be more confident in generating predictions
compared to the model pretrained only with scientific knowledge. We also found
that RALMs are overconfident in their predictions, making inaccurate
predictions more confidently than accurate ones. Scientific knowledge provided
either as pretraining or retrieval corpus does not help alleviate this issue.
We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09366">LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction. (arXiv:2311.09366v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McCusker_J/0/1/0/all/0/1">Jamie McCusker</a></p>
<p>While the potential of Open Information Extraction (Open IE) for Knowledge
Graph Construction (KGC) may seem promising, we find that the alignment of Open
IE extraction results with existing knowledge graphs to be inadequate. The
advent of Large Language Models (LLMs), especially the commercially available
OpenAI models, have reset expectations for what is possible with deep learning
models and have created a new field called prompt engineering. We investigate
the use of GPT models and prompt engineering for knowledge graph construction
with the Wikidata knowledge graph to address a similar problem to Open IE,
which we call Open Knowledge Extraction (OKE) using an approach we call the
Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the
entity linking task essential to construction of real world knowledge graphs.
We merge the CaRB benchmark scoring approach with data from the TekGen dataset
for the LOKE task. We then show that a well engineered prompt, paired with a
naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's
OpenIE 4 implementation on the OKE task, although it over-generates triples
compared to the reference set due to overall triple scarcity in the TekGen set.
Through an analysis of entity linkability in the CaRB dataset, as well as
outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver"
TekGen triples show that the task is significantly different in content from
OIE, if not structure. Through this analysis and a qualitative analysis of
sentence extractions via all methods, we found that LOKE-GPT extractions are of
high utility for the KGC task and suitable for use in semi-automated extraction
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09410">When Large Language Models contradict humans? Large Language Models&#x27; Sycophantic Behaviour. (arXiv:2311.09410v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1">Leonardo Ranaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pucci_G/0/1/0/all/0/1">Giulia Pucci</a></p>
<p>Large Language Models (LLMs) have been demonstrating the ability to solve
complex tasks by delivering answers that are positively evaluated by humans due
in part to the intensive use of human feedback that refines responses. However,
the suggestibility transmitted through human feedback increases the inclination
to produce responses that correspond to the user's beliefs or misleading
prompts as opposed to true facts, a behaviour known as sycophancy. This
phenomenon decreases the bias, robustness, and, consequently, their
reliability.
</p>
<p>In this paper, we shed light on the suggestibility of LLMs to sycophantic
behaviour, demonstrating these tendencies via human-influenced prompts over
different tasks. Our investigation reveals that LLMs show sycophantic
tendencies when responding to queries involving subjective opinions and
statements that should elicit a contrary response based on facts, demonstrating
a lack of robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09428">Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models. (arXiv:2311.09428v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yueqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1">Ali Payani</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored
outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09433">Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment. (arXiv:2311.09433v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>To ensure AI safety, instruction-tuned Large Language Models (LLMs) are
specifically trained to ensure alignment, which refers to making models behave
in accordance with human intentions. While these models have demonstrated
commendable results on various safety benchmarks, the vulnerability of their
safety alignment has not been extensively studied. This is particularly
troubling given the potential harm that LLMs can inflict. Existing attack
methods on LLMs often rely on poisoned training data or the injection of
malicious prompts. These approaches compromise the stealthiness and
generalizability of the attacks, making them susceptible to detection.
Additionally, these models often demand substantial computational resources for
implementation, making them less practical for real-world applications. In this
work, we introduce a novel attack framework, called Backdoor Activation Attack,
which injects trojan steering vectors into the activation layers of LLMs. These
malicious steering vectors can be triggered at inference time to steer the
models toward attacker-desired behaviors by manipulating their activations. In
particular, the steering vectors are generated by taking the difference between
benign and malicious activations. Then, the most effective steering vector is
selected and added to the forward passes of the LLMs. Our experiment results on
four primary alignment tasks show that our proposed method is highly effective
and adds little or no overhead to attack efficiency. Additionally, we discuss
potential countermeasures against such activation attacks. Our code and data
are available at https://email-haoran-for-link. Warning: this paper contains
content that can be offensive or upsetting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09441">Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohyung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Seif_M/0/1/0/all/0/1">Mohamed Seif</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jungchan Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1">H. Vincent Poor</a></p>
<p>Split Federated Learning (SFL) has recently emerged as a promising
distributed learning technology, leveraging the strengths of both federated
learning and split learning. It emphasizes the advantages of rapid convergence
while addressing privacy concerns. As a result, this innovation has received
significant attention from both industry and academia. However, since the model
is split at a specific layer, known as a cut layer, into both client-side and
server-side models for the SFL, the choice of the cut layer in SFL can have a
substantial impact on the energy consumption of clients and their privacy, as
it influences the training burden and the output of the client-side models.
Moreover, the design challenge of determining the cut layer is highly
intricate, primarily due to the inherent heterogeneity in the computing and
networking capabilities of clients. In this article, we provide a comprehensive
overview of the SFL process and conduct a thorough analysis of energy
consumption and privacy. This analysis takes into account the influence of
various system parameters on the cut layer selection strategy. Additionally, we
provide an illustrative example of the cut layer selection, aiming to minimize
the risk of clients from reconstructing the raw data at the server while
sustaining energy consumption within the required energy budget, which involve
trade-offs. Finally, we address open challenges in this field including their
applications to 6G technology. These directions represent promising avenues for
future research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09447">How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities. (arXiv:2311.09447v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1">Lingbo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boshi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huan Sun</a></p>
<p>The rapid progress in open-source Large Language Models (LLMs) is
significantly driving AI development forward. However, there is still a limited
understanding of their trustworthiness. Deploying these models at scale without
sufficient trustworthiness can pose significant risks, highlighting the need to
uncover these issues promptly. In this work, we conduct an assessment of
open-source LLMs on trustworthiness, scrutinizing them across eight different
aspects including toxicity, stereotypes, ethics, hallucination, fairness,
sycophancy, privacy, and robustness against adversarial demonstrations. We
propose an enhanced Chain of Utterances-based (CoU) prompting strategy by
incorporating meticulously crafted malicious demonstrations for trustworthiness
attack. Our extensive experiments encompass recent and representative series of
open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The
empirical outcomes underscore the efficacy of our attack strategy across
diverse aspects. More interestingly, our result analysis reveals that models
with superior performance in general NLP tasks do not always have greater
trustworthiness; in fact, larger models can be more vulnerable to attacks.
Additionally, models that have undergone instruction tuning, focusing on
instruction following, tend to be more susceptible, although fine-tuning LLMs
for safety alignment proves effective in mitigating adversarial trustworthiness
attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09449">HAL 9000: Skynet&#x27;s Risk Manager. (arXiv:2311.09449v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freitas_T/0/1/0/all/0/1">Tadeu Freitas</a>, <a href="http://arxiv.org/find/cs/1/au:+Neto_M/0/1/0/all/0/1">M&#xe1;rio Neto</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutra_I/0/1/0/all/0/1">In&#xea;s Dutra</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1">Jo&#xe3;o Soares</a>, <a href="http://arxiv.org/find/cs/1/au:+Correia_M/0/1/0/all/0/1">Manuel Correia</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1">Rolando Martins</a></p>
<p>Intrusion Tolerant Systems (ITSs) are a necessary component for
cyber-services/infrastructures. Additionally, as cyberattacks follow a
multi-domain attack surface, a similar defensive approach should be applied,
namely, the use of an evolving multi-disciplinary solution that combines ITS,
cybersecurity and Artificial Intelligence (AI). With the increased popularity
of AI solutions, due to Big Data use-case scenarios and decision support and
automation scenarios, new opportunities to apply Machine Learning (ML)
algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS
can augment its intrusion tolerance capability, by learning from previous
attacks and from known vulnerabilities. As such, this work's contribution is
twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and
incorporates new components to increase its intrusion tolerance capability and
its adaptability to new adversaries; (2) an improved Risk Manager design that
leverages AI to improve ITSs by automatically assessing OS risks to intrusions,
and advise with safer configurations. One of the reasons that intrusions are
successful is due to bad configurations or slow adaptability to new threats.
This can be caused by the dependency that systems have for human intervention.
One of the characteristics in Skynet and HAL 9000 design is the removal of
human intervention. Being fully automatized lowers the chance of successful
intrusions caused by human error. Our experiments using Skynet, shows that HAL
is able to choose 15% safer configurations than the state-of-the-art risk
manager.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09467">Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation. (arXiv:2311.09467v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yifu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1">Varun Embar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Benjamin Han</a></p>
<p>Neural knowledge-to-text generation models often struggle to faithfully
generate descriptions for the input facts: they may produce hallucinations that
contradict the given facts, or describe facts not present in the input. To
reduce hallucinations, we propose a novel decoding method, TWEAK (Think While
Effectively Articulating Knowledge). TWEAK treats the generated sequences at
each decoding step and its future sequences as hypotheses, and ranks each
generation candidate based on how well their corresponding hypotheses support
the input facts using a Hypothesis Verification Model (HVM). We first
demonstrate the effectiveness of TWEAK by using a Natural Language Inference
(NLI) model as the HVM and report improved faithfulness with minimal impact on
the quality. We then replace the NLI model with our task-specific HVM trained
with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
pairs input facts with their faithful and hallucinated descriptions with the
hallucinated spans marked. The new HVM improves the faithfulness and the
quality further and runs faster. Overall the best TWEAK variants improve on
average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and
TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality
measured by BERTScore over the same datasets. Since TWEAK is a decoding-only
approach, it can be integrated with any neural generative model without
retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09473">JAB: Joint Adversarial Prompting and Belief Augmentation. (arXiv:2311.09473v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1">Ninareh Mehrabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Palash Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishna_A/0/1/0/all/0/1">Anil Ramakrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1">Jwala Dhamala</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shalini Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1">Aram Galstyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rahul Gupta</a></p>
<p>With the recent surge of language models in different applications, attention
to safety and robustness of these models has gained significant importance.
Here we introduce a joint framework in which we simultaneously probe and
improve the robustness of a black-box target model via adversarial prompting
and belief augmentation using iterative feedback loops. This framework utilizes
an automated red teaming approach to probe the target model, along with a
belief augmenter to generate instructions for the target model to improve its
robustness to those adversarial probes. Importantly, the adversarial model and
the belief generator leverage the feedback from past interactions to improve
the effectiveness of the adversarial prompts and beliefs, respectively. In our
experiments, we demonstrate that such a framework can reduce toxic content
generation both in dynamic cases where an adversary directly interacts with a
target model and static cases where we use a static benchmark dataset to
evaluate our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09476">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems. (arXiv:2311.09476v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1">Jon Saad-Falcon</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1">Omar Khattab</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei Zaharia</a></p>
<p>Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. Using synthetic training data, ARES finetunes lightweight LM
judges to assess the quality of individual RAG components. To mitigate
potential prediction errors, ARES utilizes a small set of human-annotated
datapoints for prediction-powered inference (PPI). Across six different
knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG
systems while using a few hundred human annotations during evaluation.
Furthermore, ARES judges remain effective across domain shifts, proving
accurate even after changing the type of queries and/or documents used in the
evaluated RAG systems. We make our datasets and code for replication and
deployment available at https://github.com/stanford-futuredata/ARES.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09483">Adaptive Interventions with User-Defined Goals for Health Behavior Change. (arXiv:2311.09483v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mandyam_A/0/1/0/all/0/1">Aishwarya Mandyam</a>, <a href="http://arxiv.org/find/cs/1/au:+Joerke_M/0/1/0/all/0/1">Matthew Joerke</a>, <a href="http://arxiv.org/find/cs/1/au:+Engelhardt_B/0/1/0/all/0/1">Barbara E. Engelhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1">Emma Brunskill</a></p>
<p>Physical inactivity remains a major public health concern, having
associations with adverse health outcomes such as cardiovascular disease and
type-2 diabetes. Mobile health applications present a promising avenue for
low-cost, scalable physical activity promotion, yet often suffer from small
effect sizes and low adherence rates, particularly in comparison to human
coaching. Goal-setting is a critical component of health coaching that has been
underutilized in adaptive algorithms for mobile health interventions. This
paper introduces a modification to the Thompson sampling algorithm that places
emphasis on individualized goal-setting by optimizing personalized reward
functions. As a step towards supporting goal-setting, this paper offers a
balanced approach that can leverage shared structure while optimizing
individual preferences and goals. We prove that our modification incurs only a
constant penalty on the cumulative regret while preserving the sample
complexity benefits of data sharing. In a physical activity simulator, we
demonstrate that our algorithm achieves substantial improvements in cumulative
regret compared to baselines that do not share data or do not optimize for
individualized rewards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09505">SegMix: A Simple Structure-Aware Data Augmentation Method. (arXiv:2311.09505v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1">Yuxin Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhuse_P/0/1/0/all/0/1">Pushkar Bhuse</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengzhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a></p>
<p>Interpolation-based Data Augmentation (DA) methods (Mixup) linearly
interpolate the inputs and labels of two or more training examples. Mixup has
more recently been adapted to the field of Natural Language Processing (NLP),
mainly for sequence labeling tasks. However, such a simple adoption yields
mixed or unstable improvements over the baseline models. We argue that the
direct-adoption methods do not account for structures in NLP tasks. To this
end, we propose SegMix, a collection of interpolation-based DA algorithms that
can adapt to task-specific structures. SegMix poses fewer constraints on data
structures, is robust to various hyperparameter settings, applies to more task
settings, and adds little computational overhead. In the algorithm's core, we
apply interpolation methods on task-specific meaningful segments, in contrast
to applying them on sequences as in prior work. We find SegMix to be a flexible
framework that combines rule-based DA methods with interpolation-based methods,
creating interesting mixtures of DA techniques. We show that SegMix
consistently improves performance over strong baseline models in Named Entity
Recognition (NER) and Relation Extraction (RE) tasks, especially under
data-scarce settings. Furthermore, this method is easy to implement and adds
negligible training overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09520">MDFL: Multi-domain Diffusion-driven Feature Learning. (arXiv:2311.09520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Daixun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a></p>
<p>High-dimensional images, known for their rich semantic information, are
widely applied in remote sensing and other fields. The spatial information in
these images reflects the object's texture features, while the spectral
information reveals the potential spectral representations across different
bands. Currently, the understanding of high-dimensional images remains limited
to a single-domain perspective with performance degradation. Motivated by the
masking texture effect observed in the human visual system, we present a
multi-domain diffusion-driven feature learning network (MDFL) , a scheme to
redefine the effective information domain that the model really focuses on.
This method employs diffusion-based posterior sampling to explicitly consider
joint information interactions between the high-dimensional manifold structures
in the spectral, spatial, and frequency domains, thereby eliminating the
influence of masking texture effects in visual models. Additionally, we
introduce a feature reuse mechanism to gather deep and raw features of
high-dimensional data. We demonstrate that MDFL significantly improves the
feature extraction performance of high-dimensional data, thereby providing a
powerful aid for revealing the intrinsic patterns and structures of such data.
The experimental results on three multi-modal remote sensing datasets show that
MDFL reaches an average overall accuracy of 98.25%, outperforming various
state-of-the-art baseline schemes. The code will be released, contributing to
the computer vision community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09528">HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM. (arXiv:2311.09528v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jiaqi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_V/0/1/0/all/0/1">Virginia Adams</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1">Makesh Narsimhan Sreedhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Egert_D/0/1/0/all/0/1">Daniel Egert</a>, <a href="http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1">Olivier Delalleau</a>, <a href="http://arxiv.org/find/cs/1/au:+Scowcroft_J/0/1/0/all/0/1">Jane Polak Scowcroft</a>, <a href="http://arxiv.org/find/cs/1/au:+Kant_N/0/1/0/all/0/1">Neel Kant</a>, <a href="http://arxiv.org/find/cs/1/au:+Swope_A/0/1/0/all/0/1">Aidan Swope</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1">Oleksii Kuchaiev</a></p>
<p>Existing open-source helpfulness preference datasets do not specify what
makes some responses more helpful and others less so. Models trained on these
datasets can incidentally learn to model dataset artifacts (e.g. preferring
longer but unhelpful responses only due to their length). To alleviate this
problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated
for the various aspects that make responses helpful. Specifically, our
37k-sample dataset has annotations for correctness, coherence, complexity, and
verbosity in addition to overall helpfulness of responses. Training Llama 2 70B
using the HelpSteer dataset with SteerLM technique produces a model that scores
7.54 on MT Bench, which is currently the highest score for open models that do
not require training data from more powerful models (e.g. GPT4). We release
this dataset with CC-BY-4.0 license at
https://huggingface.co/datasets/nvidia/HelpSteer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09544">Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta. (arXiv:2311.09544v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1">Chen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Fang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuewei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ru Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yaning Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Dong Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhengxing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Min Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fenggang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minghai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huayu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yunnan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1">Zhan Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mindi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Sri Reddy</a></p>
<p>Effective user representations are pivotal in personalized advertising.
However, stringent constraints on training throughput, serving latency, and
memory, often limit the complexity and input feature set of online ads ranking
models. This challenge is magnified in extensive systems like Meta's, which
encompass hundreds of models with diverse specifications, rendering the
tailoring of user representation learning for each model impractical. To
address these challenges, we present Scaling User Modeling (SUM), a framework
widely deployed in Meta's ads ranking system, designed to facilitate efficient
and scalable sharing of online user representation across hundreds of ads
models. SUM leverages a few designated upstream user models to synthesize user
embeddings from massive amounts of user features with advanced modeling
techniques. These embeddings then serve as inputs to downstream online ads
ranking models, promoting efficient representation sharing. To adapt to the
dynamic nature of user features and ensure embedding freshness, we designed SUM
Online Asynchronous Platform (SOAP), a latency free online serving system
complemented with model freshness and embedding stabilization, which enables
frequent user model updates and online inference of user embeddings upon each
user request. We share our hands-on deployment experiences for the SUM
framework and validate its superiority through comprehensive experiments. To
date, SUM has been launched to hundreds of ads ranking models in Meta,
processing hundreds of billions of user requests daily, yielding significant
online metric gains and infrastructure cost savings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09553">Program-Aided Reasoners (better) Know What They Know. (arXiv:2311.09553v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabra_A/0/1/0/all/0/1">Anubha Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangreji_S/0/1/0/all/0/1">Sanketh Rangreji</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathur_Y/0/1/0/all/0/1">Yash Mathur</a>, <a href="http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1">Aman Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1">Emmy Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a></p>
<p>Prior work shows that program-aided reasoning, in which large language models
(LLMs) are combined with programs written in programming languages such as
Python, can significantly improve accuracy on various reasoning tasks. However,
while accuracy is essential, it is also important for such reasoners to "know
what they know", which can be quantified through the calibration of the model.
In this paper, we compare the calibration of Program Aided Language Models
(PAL) and text-based Chain-of-thought (COT) prompting techniques over 5
datasets and 2 model types: LLaMA models and OpenAI models. Our results
indicate that PAL leads to improved calibration in 75% of the instances. Our
analysis uncovers that prompting styles that produce lesser diversity in
generations also have more calibrated results, and thus we also experiment with
inducing lower generation diversity using temperature scaling and find that for
certain temperatures, PAL is not only more accurate but is also more calibrated
than COT. Overall, we demonstrate that, in the majority of cases, program-aided
reasoners better know what they know than text-based counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09559">Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler. (arXiv:2311.09559v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1">Gaurav Sahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1">Olga Vechtomova</a>, <a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1">Issam H. Laradji</a></p>
<p>This work tackles the task of extractive text summarization in a limited
labeled data scenario using a semi-supervised approach. Specifically, we
propose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate
our method on three text summarization datasets: TweetSumm, WikiHow, and
ArXiv/PubMed. Our experiments show that by using an LLM to evaluate and
generate pseudolabels, we can improve the ROUGE-1 by 10-20\% on the different
datasets, which is akin to enhancing pretrained models. We also show that such
a method needs a smaller pool of unlabeled examples to perform better.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09564">LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks. (arXiv:2311.09564v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1">Mihir Parmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1">Aakanksha Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Himanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_D/0/1/0/all/0/1">Disha Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a></p>
<p>Many large language models (LLMs) for medicine have largely been evaluated on
short texts, and their ability to handle longer sequences such as a complete
electronic health record (EHR) has not been systematically explored. Assessing
these models on long sequences is crucial since prior work in the general
domain has demonstrated performance degradation of LLMs on longer texts.
Motivated by this, we introduce LongBoX, a collection of seven medical datasets
in text-to-text format, designed to investigate model performance on long
sequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)
and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We
further evaluate two techniques designed for long-sequence handling: (i)
local-global attention, and (ii) Fusion-in-Decoder (FiD). Our results
demonstrate mixed results with long-sequence handling - while scores on some
datasets increase, there is substantial room for improvement. We hope that
LongBoX facilitates the development of more effective long-sequence techniques
for the medical domain. Data and source code are available at
https://github.com/Mihir3009/LongBoX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09569">Prompt Optimisation with Random Sampling. (arXiv:2311.09569v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1">Sebastian Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a></p>
<p>Using the generative nature of a language model to generate task-relevant
separators has shown competitive results compared to human-curated prompts like
"TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as
separators can achieve near-state-of-the-art performance. We analyse this
phenomenon in detail using three different random generation strategies,
establishing that the language space is rich with potential good separators,
regardless of the underlying language model size. These observations challenge
the common assumption that an effective prompt should be human-readable or
task-relevant. Experimental results show that using random separators leads to
an average 16% relative improvement across nine text classification tasks on
seven language models, compared to human-curated separators, and is on par with
automatic prompt searching methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09574">LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1">Vivek Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoli Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1">Vrishab Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1">Brent Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1">Oscar Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1">Rebecca Rojansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1">Andrew Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1">Fabiola Valvert</a>, <a href="http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1">Edward Briercheck</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1">David Weinstock</a>, <a href="http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1">Yasodha Natkunam</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1">Sebastian Fernandez-Pol</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1">Pranav Rajpurkar</a></p>
<p>The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09576">Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads. (arXiv:2311.09576v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a></p>
<p>AI agents excel in executing predefined tasks, but the dynamic management of
work state information during task execution remains an underexplored area. We
propose a work state-centric AI agent model employing "work notes" to record
and reflect the state throughout task execution. This paper details the model's
architecture, featuring worker threads for task oversight, planner modules for
task decomposition and planning, and executor modules for performing subtasks
using a ReAct-inspired thought-action loop. We provide an exhaustive work state
record incorporating plans and outcomes, constituting a comprehensive work
journal. Our results show that this model not only improves task execution
efficiency but also lays a solid foundation for subsequent task analysis and
auditing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09578">Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying. (arXiv:2311.09578v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1">Adithya Renduchintala</a>, <a href="http://arxiv.org/find/cs/1/au:+Konuk_T/0/1/0/all/0/1">Tugrul Konuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1">Oleksii Kuchaiev</a></p>
<p>We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective
training to further increase parameter efficiency of the Low-rank adaptation
(LoRA) method. Our investigations include all feasible combinations parameter
training/freezing in conjunction with weight tying to identify the optimal
balance between performance and the number of trainable parameters. Through
experiments covering a variety of tasks and two base language models, we
provide analysis revealing trade-offs between efficiency and performance. Our
experiments uncovered a particular Tied-LoRA configuration that stands out by
demonstrating comparable performance across several tasks while employing only
13~\% percent of parameters utilized by the standard LoRA method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09593">Multi-Step Dialogue Workflow Action Prediction. (arXiv:2311.09593v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_R/0/1/0/all/0/1">Ramya Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Elenberg_E/0/1/0/all/0/1">Ethan Elenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Narangodage_H/0/1/0/all/0/1">Hashan Narangodage</a>, <a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1">Ryan McDonald</a></p>
<p>In task-oriented dialogue, a system often needs to follow a sequence of
actions, called a workflow, that complies with a set of guidelines in order to
complete a task. In this paper, we propose the novel problem of multi-step
workflow action prediction, in which the system predicts multiple future
workflow actions. Accurate prediction of multiple steps allows for multi-turn
automation, which can free up time to focus on more complex tasks. We propose
three modeling approaches that are simple to implement yet lead to more action
automation: 1) fine-tuning on a training dataset, 2) few-shot in-context
learning leveraging retrieval and large language model prompting, and 3)
zero-shot graph traversal, which aggregates historical action sequences into a
graph for prediction. We show that multi-step action prediction produces
features that improve accuracy on downstream dialogue tasks like predicting
task success, and can increase automation of steps by 20% without requiring as
much feedback from a human overseeing the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09601">Code Models are Zero-shot Precondition Reasoners. (arXiv:2311.09601v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1">Lajanugen Logeswaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1">Sungryull Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1">Yiwei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anthony Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dong-Ki Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1">Dongsub Shim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Moontae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Honglak Lee</a></p>
<p>One of the fundamental skills required for an agent acting in an environment
to complete tasks is the ability to understand what actions are plausible at
any given point. This work explores a novel use of code representations to
reason about action preconditions for sequential decision making tasks. Code
representations offer the flexibility to model procedural activities and
associated constraints as well as the ability to execute and verify constraint
satisfaction. Leveraging code representations, we extract action preconditions
from demonstration trajectories in a zero-shot manner using pre-trained code
models. Given these extracted preconditions, we propose a precondition-aware
action sampling strategy that ensures actions predicted by a policy are
consistent with preconditions. We demonstrate that the proposed approach
enhances the performance of few-shot policy learning approaches across
task-oriented dialog and embodied textworld benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09613">Digital Socrates: Evaluating LLMs through explanation critiques. (arXiv:2311.09613v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuling Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1">Oyvind Tafjord</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a></p>
<p>While LLMs can provide reasoned explanations along with their answers, the
nature and quality of those explanations are still poorly understood. In
response, our goal is to define a detailed way of characterizing the
explanation capabilities of modern models and to create a nuanced,
interpretable explanation evaluation tool that can generate such
characterizations automatically, without relying on expensive API calls or
human annotations. Our approach is to (a) define the new task of explanation
critiquing - identifying and categorizing any main flaw in an explanation and
providing suggestions to address the flaw, (b) create a sizeable,
human-verified dataset for this task, and (c) train an open-source, automatic
critiquing model (called Digital Socrates) using this data. Through
quantitative and qualitative analysis, we demonstrate how Digital Socrates is
useful for revealing insights about student models by examining their reasoning
chains, and how it can provide high-quality, nuanced, automatic evaluation of
those model explanations for the first time. Digital Socrates thus fills an
important gap in evaluation tools for understanding and improving the
explanation behavior of models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09614">Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images. (arXiv:2311.09614v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahamed_S/0/1/0/all/0/1">Shadab Ahamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yixi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gowdy_C/0/1/0/all/0/1">Claire Gowdy</a>, <a href="http://arxiv.org/find/cs/1/au:+O_J/0/1/0/all/0/1">Joo H. O</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloise_I/0/1/0/all/0/1">Ingrid Bloise</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1">Don Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Martineau_P/0/1/0/all/0/1">Patrick Martineau</a>, <a href="http://arxiv.org/find/cs/1/au:+Benard_F/0/1/0/all/0/1">Fran&#xe7;ois B&#xe9;nard</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousefirizi_F/0/1/0/all/0/1">Fereshteh Yousefirizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1">Rahul Dodhia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavista_J/0/1/0/all/0/1">Juan M. Lavista</a>, <a href="http://arxiv.org/find/cs/1/au:+Weeks_W/0/1/0/all/0/1">William B. Weeks</a>, <a href="http://arxiv.org/find/cs/1/au:+Uribe_C/0/1/0/all/0/1">Carlos F. Uribe</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1">Arman Rahmim</a></p>
<p>This study performs comprehensive evaluation of four neural network
architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion
segmentation from PET/CT images. These networks were trained, validated, and
tested on a diverse, multi-institutional dataset of 611 cases. Internal testing
(88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed
SegResNet as the top performer with a median Dice similarity coefficient (DSC)
of 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a
median false negative volume (FNV) of 0 ml. On the unseen external test set
(145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best
median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.
We assessed reproducibility of six lesion measures, calculated their prediction
errors, and examined DSC performance in relation to these lesion measures,
offering insights into segmentation accuracy and clinical relevance.
Additionally, we introduced three lesion detection criteria, addressing the
clinical need for identifying lesions, counting them, and segmenting based on
metabolic characteristics. We also performed expert intra-observer variability
analysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to
assist in the development of more resilient segmentation algorithms. Finally,
we performed inter-observer agreement assessment underscoring the importance of
a standardized ground truth segmentation protocol involving multiple expert
annotators. Code is available at:
https://github.com/microsoft/lymphoma-segmentation-dnn
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09624">AI Recommendation System for Enhanced Customer Experience: A Novel Image-to-Text Method. (arXiv:2311.09624v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ayedi_M/0/1/0/all/0/1">Mohamaed Foued Ayedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Salem_H/0/1/0/all/0/1">Hiba Ben Salem</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammami_S/0/1/0/all/0/1">Soulaimen Hammami</a>, <a href="http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1">Ahmed Ben Said</a>, <a href="http://arxiv.org/find/cs/1/au:+Jabbar_R/0/1/0/all/0/1">Rateb Jabbar</a>, <a href="http://arxiv.org/find/cs/1/au:+CHabbouh_A/0/1/0/all/0/1">Achraf CHabbouh</a></p>
<p>Existing fashion recommendation systems encounter difficulties in using
visual data for accurate and personalized recommendations. This research
describes an innovative end-to-end pipeline that uses artificial intelligence
to provide fine-grained visual interpretation for fashion recommendations. When
customers upload images of desired products or outfits, the system
automatically generates meaningful descriptions emphasizing stylistic elements.
These captions guide retrieval from a global fashion product catalogue to offer
similar alternatives that fit the visual characteristics of the original image.
On a dataset of over 100,000 categorized fashion photos, the pipeline was
trained and evaluated. The F1-score for the object detection model was 0.97,
exhibiting exact fashion object recognition capabilities optimized for
recommendation. This visually aware system represents a key advancement in
customer engagement through personalized fashion recommendations
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09627">CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model. (arXiv:2311.09627v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1">Nakyeong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1">Taegwan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kyomin Jung</a></p>
<p>Large language models (LLMs) executing tasks through instruction-based
prompts often face challenges stemming from distribution differences between
user instructions and training instructions. This leads to distractions and
biases, especially when dealing with inconsistent dynamic labels. In this
paper, we introduces a novel bias mitigation method, CRISPR, designed to
alleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods
to identify bias neurons influencing biased outputs and employs pruning to
eliminate the bias neurons. Experimental results demonstrate the method's
effectiveness in mitigating biases in instruction-based prompting, enhancing
language model performance on social bias benchmarks without compromising
pre-existing knowledge. CRISPR proves highly practical, model-agnostic,
offering flexibility in adapting to evolving social biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09632">Online Continual Knowledge Learning for Language Models. (arXiv:2311.09632v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1">Tongjun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1">Karthick Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Seah_C/0/1/0/all/0/1">Chun Wei Seah</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuhao Zhang</a></p>
<p>Large Language Models (LLMs) serve as repositories of extensive world
knowledge, enabling them to perform tasks such as question-answering and
fact-checking. However, this knowledge can become obsolete as global contexts
change. In this paper, we introduce a novel problem in the realm of continual
learning: Online Continual Knowledge Learning (OCKL). This problem formulation
aims to manage the dynamic nature of world knowledge in LMs under real-time
constraints. We propose a new benchmark and evaluation metric designed to
measure both the rate of new knowledge acquisition and the retention of
previously learned knowledge. Our empirical evaluation, conducted using a
variety of state-of-the-art methods, establishes robust base-lines for OCKL.
Our results reveal that existing continual learning approaches are
unfortunately insufficient for tackling the unique challenges posed by OCKL. We
identify key factors that influence the trade-off between knowledge acquisition
and retention, thereby advancing our understanding of how to train LMs in a
continually evolving environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09641">On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models. (arXiv:2311.09641v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiongxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Junlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1">Yevgeniy Vorobeychik</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a></p>
<p>Reinforcement Learning with Human Feedback (RLHF) is a methodology designed
to align Large Language Models (LLMs) with human preferences, playing an
important role in LLMs alignment. Despite its advantages, RLHF relies on human
annotators to rank the text, which can introduce potential security
vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the
ranking score by up-ranking any malicious text to steer the LLM adversarially.
To assess the red-teaming of RLHF against human preference data poisoning, we
propose RankPoison, a poisoning attack method on candidates' selection of
preference rank flipping to reach certain malicious behaviors (e.g., generating
longer sequences, which can increase the computational cost). With poisoned
dataset generated by RankPoison, we can perform poisoning attacks on LLMs to
generate longer tokens without hurting the original safety alignment
performance. Moreover, applying RankPoison, we also successfully implement a
backdoor attack where LLMs can generate longer answers under questions with the
trigger word. Our findings highlight critical security challenges in RLHF,
underscoring the necessity for more robust alignment methods for LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09651">&quot;It&#x27;s not like Jarvis, but it&#x27;s pretty close!&quot; -- Examining ChatGPT&#x27;s Usage among Undergraduate Students in Computer Science. (arXiv:2311.09651v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1">Ishika Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Budhiraja_R/0/1/0/all/0/1">Ritvik Budhiraja</a>, <a href="http://arxiv.org/find/cs/1/au:+Akolekar_H/0/1/0/all/0/1">Harshal D Akolekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Challa_J/0/1/0/all/0/1">Jagat Sesh Challa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1">Dhruv Kumar</a></p>
<p>Large language models (LLMs) such as ChatGPT and Google Bard have garnered
significant attention in the academic community. Previous research has
evaluated these LLMs for various applications such as generating programming
exercises and solutions. However, these evaluations have predominantly been
conducted by instructors and researchers, not considering the actual usage of
LLMs by students. This study adopts a student-first approach to comprehensively
understand how undergraduate computer science students utilize ChatGPT, a
popular LLM, released by OpenAI. We employ a combination of student surveys and
interviews to obtain valuable insights into the benefits, challenges, and
suggested improvements related to ChatGPT. Our findings suggest that a majority
of students (over 57%) have a convincingly positive outlook towards adopting
ChatGPT as an aid in coursework-related tasks. However, our research also
highlights various challenges that must be resolved for long-term acceptance of
ChatGPT amongst students. The findings from this investigation have broader
implications and may be applicable to other LLMs and their role in computing
education.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09656">Structured Chemistry Reasoning with Large Language Models. (arXiv:2311.09656v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Siru Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuosheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bing Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianhui Qin</a></p>
<p>This paper studies the problem of solving complex chemistry problems with
large language models (LLMs). Despite the extensive general knowledge in LLMs
(such as GPT-4), they struggle with chemistry reasoning that requires faithful
grounded reasoning with diverse chemical knowledge and an integrative
understanding of chemical interactions. We propose InstructChem, a new
structured reasoning approach that substantially boosts the LLMs' chemical
reasoning capabilities. InstructChem explicitly decomposes the reasoning into
three critical phrases, including chemical formulae generation by LLMs that
offers the basis for subsequent grounded reasoning, step-by-step reasoning that
makes multi-step derivations with the identified formulae for a preliminary
answer, and iterative review-and-refinement that steers LLMs to progressively
revise the previous phases for increasing confidence, leading to the final
high-confidence answer. We conduct extensive experiments on four different
chemistry challenges, including quantum chemistry, quantum mechanics, physical
chemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on
chemistry reasoning, yielding an 8% average absolute improvement and a 30% peak
improvement. We further use the generated reasoning by GPT-4 to fine-tune
smaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs.
This validates our approach and enables LLMs to generate high-quality
reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09680">Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
in reliable methods. Despite the abundance of literature on trustworthy LMs in
language, a systematic survey specifically delving into the trustworthiness of
LMs in vision remains absent. In order to mitigate this gap, we summarize four
relevant concerns that obstruct the trustworthy usage in vision of LMs in this
survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers'
understanding of the field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09682">MacGyver: Are Large Language Models Creative Problem Solvers?. (arXiv:2311.09682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yufei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1">Abhilasha Ravichander</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianhui Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1">Ronan Le Bras</a>, <a href="http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1">Raja Marjieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a></p>
<p>We explore the creative problem-solving capabilities of modern large language
models (LLMs) in a constrained setting. The setting requires circumventing a
cognitive bias known in psychology as ''functional fixedness'' to use familiar
objects in innovative or unconventional ways. To this end, we create MacGyver,
an automatically generated dataset consisting of 1,600 real-world problems that
deliberately trigger functional fixedness and require thinking
'out-of-the-box'. We then present our collection of problems to both LLMs and
humans to compare and contrast their problem-solving abilities. We show that
MacGyver is challenging for both groups, but in unique and complementary ways.
For example, humans typically excel in solving problems that they are familiar
with but may struggle with tasks requiring domain-specific knowledge, leading
to a higher variance. On the other hand, LLMs, being exposed to a variety of
highly specialized knowledge, attempt broader problems but are prone to
overconfidence and propose actions that are physically infeasible or
inefficient. We also provide a detailed error analysis of LLMs, and demonstrate
the potential of enhancing their problem-solving ability with novel prompting
techniques such as iterative step-wise reflection and divergent-convergent
thinking. This work provides insight into the creative problem-solving
capabilities of humans and AI and illustrates how psychological paradigms can
be extended into large-scale tasks for comparing humans and machines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09684">Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation. (arXiv:2311.09684v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zonghai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaafar_A/0/1/0/all/0/1">Ahmed Jaafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Beining Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yue Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhichao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09692">Augmenting Unsupervised Reinforcement Learning with Self-Reference. (arXiv:2311.09692v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1">Andrew Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1">Erle Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Rui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Matthieu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong-Jin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a></p>
<p>Humans possess the ability to draw on past experiences explicitly when
learning new tasks and applying them accordingly. We believe this capacity for
self-referencing is especially advantageous for reinforcement learning agents
in the unsupervised pretrain-then-finetune setting. During pretraining, an
agent's past experiences can be explicitly utilized to mitigate the
nonstationarity of intrinsic rewards. In the finetuning phase, referencing
historical trajectories prevents the unlearning of valuable exploratory
behaviors. Motivated by these benefits, we propose the Self-Reference (SR)
approach, an add-on module explicitly designed to leverage historical
information and enhance agent performance within the pretrain-finetune
paradigm. Our approach achieves state-of-the-art results in terms of
Interquartile Mean (IQM) performance and Optimality Gap reduction on the
Unsupervised Reinforcement Learning Benchmark for model-free methods, recording
an 86% IQM and a 16% Optimality Gap. Additionally, it improves current
algorithms by up to 17% IQM and reduces the Optimality Gap by 31%. Beyond
performance enhancement, the Self-Reference add-on also increases sample
efficiency, a crucial attribute for real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09693">BLT: Can Large Language Models Handle Basic Legal Text?. (arXiv:2311.09693v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blair_Stanek_A/0/1/0/all/0/1">Andrew Blair-Stanek</a>, <a href="http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1">Nils Holzenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a></p>
<p>We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently
perform poorly at basic text handling required of lawyers or paralegals, such
as looking up the text at a line of a witness deposition or at a subsection of
a contract. We introduce a benchmark to quantify this poor performance, which
casts into doubt LLMs' current reliability as-is for legal practice. Finetuning
for these tasks brings an older LLM to near-perfect performance on our test set
and also raises performance on a related legal task. This stark result
highlights the need for more domain expertise in LLM training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09702">Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?. (arXiv:2311.09702v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bangzheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Ben Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xingyu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>Despite the recent advancement in large language models (LLMs) and their high
performances across numerous benchmarks, recent research has unveiled that LLMs
suffer from hallucinations and unfaithful reasoning. This work studies a
specific type of hallucination induced by semantic associations. Specifically,
we investigate to what extent LLMs take shortcuts from certain keyword/entity
biases in the prompt instead of following the correct reasoning path. To
quantify this phenomenon, we propose a novel probing method and benchmark
called EureQA. We start from questions that LLMs will answer correctly with
utmost certainty, and mask the important entity with evidence sentence
recursively, asking models to find masked entities according to a chain of
evidence before answering the question.
</p>
<p>During the construction of the evidence, we purposefully replace semantic
clues (entities) that may lead to the correct answer with distractor clues
(evidence) that will not directly lead to the correct answer but require a
chain-like reasoning process. We evaluate if models can follow the correct
reasoning chain instead of short-cutting through distractor clues. We find that
existing LLMs lack the necessary capabilities to follow correct reasoning paths
and resist the attempt of greedy shortcuts. We show that the distractor
semantic associations often lead to model hallucination, which is strong
evidence that questions the validity of current LLM reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09706">Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance. (arXiv:2311.09706v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Takagi_S/0/1/0/all/0/1">Shiro Takagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamauchi_R/0/1/0/all/0/1">Ryutaro Yamauchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumagai_W/0/1/0/all/0/1">Wataru Kumagai</a></p>
<p>Research automation efforts usually employ AI as a tool to automate specific
tasks within the research process. To create an AI that truly conduct research
themselves, it must independently generate hypotheses, design verification
plans, and execute verification. Therefore, we investigated if an AI itself
could autonomously generate and verify hypothesis for a toy machine learning
research problem. We prompted GPT-4 to generate hypotheses and Python code for
hypothesis verification with limited methodological guidance. Our findings
suggest that, in some instances, GPT-4 can autonomously generate and validate
hypotheses without detailed guidance. While this is a promising result, we also
found that none of the verifications were flawless, and there remain
significant challenges in achieving autonomous, human-level research using only
generic instructions. These findings underscore the need for continued
exploration to develop a general and autonomous AI researcher.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09718">You don&#x27;t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments. (arXiv:2311.09718v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shu_B/0/1/0/all/0/1">Bangzhao Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lechen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minje Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dunagan_L/0/1/0/all/0/1">Lavinia Dunagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1">Dallas Card</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a></p>
<p>The versatility of Large Language Models (LLMs) on natural language
understanding tasks has made them popular for research in social sciences. In
particular, to properly understand the properties and innate personas of LLMs,
researchers have performed studies that involve using prompts in the form of
questions that ask LLMs of particular opinions. In this study, we take a
cautionary step back and examine whether the current format of prompting
enables LLMs to provide responses in a consistent and robust manner. We first
construct a dataset that contains 693 questions encompassing 39 different
instruments of persona measurement on 115 persona axes. Additionally, we design
a set of prompts containing minor variations and examine LLM's capabilities to
generate accurate answers, as well as consistency variations to examine their
consistency towards simple perturbations such as switching the option order.
Our experiments on 15 different open-source LLMs reveal that even simple
perturbations are sufficient to significantly downgrade a model's
question-answering ability, and that most LLMs have low negation consistency.
Our results suggest that the currently widespread practice of prompting is
insufficient to accurately capture model perceptions, and we discuss potential
alternatives to improve such issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09724">Outcome-supervised Verifiers for Planning in Mathematical Reasoning. (arXiv:2311.09724v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1">Anningzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a></p>
<p>Large language models (LLMs) often struggle with maintaining accuracy across
a sequence of intermediate reasoning steps in mathematical reasoning, leading
to error propagation that undermines the final result. The current methodology
to mitigate this issue primarily involves using a verifier model to assess the
correctness of generated solution candidates, focusing either on the overall
reasoning path or on an incomplete reasoning path. By rethinking this approach,
we argue that assessing potentials of incomplete reasoning paths could be more
advantageous as it guides towards correct final answers, transforming the task
into a \textit{planning} problem. Our proposed verifier, the
Outcome-supervision Value Model (OVM), employs outcome supervision for
training, offering an efficient and intuitive method for \textit{planning} by
prioritizing steps that lead to accurate conclusions over mere per-step
correctness. Furthermore, the OVM eschews the need for labor-intensive
annotations on step-level correctness, enhancing its scalability. Our
experiments on two multi-step mathematical reasoning datasets, GSM8K and Game
of 24, demonstrate the superior performance of the OVM model. Notably, in
GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up
to 13B parameters}; especially it does not utilize GPT-4 or code execution.
These findings offer a novel perspective on the role of outcome supervision in
training verifiers for multi-step reasoning tasks and provide theoretical
justification for its advantage in value estimation for planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09730">Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks. (arXiv:2311.09730v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huaman Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jiaxin Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minje Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a></p>
<p>Human perception of language depends on personal backgrounds like gender and
ethnicity. While existing studies have shown that large language models (LLMs)
hold values that are closer to certain societal groups, it is unclear whether
their prediction behaviors on subjective NLP tasks also exhibit a similar bias.
In this study, leveraging the POPQUORN dataset which contains annotations of
diverse demographic backgrounds, we conduct a series of experiments on four
popular LLMs to investigate their capability to understand group differences
and potential biases in their predictions for politeness and offensiveness. We
find that for both tasks, model predictions are closer to the labels from White
and female participants. We further explore prompting with the target
demographic labels and show that including the target demographic in the prompt
actually worsens the model's performance. More specifically, when being
prompted to respond from the perspective of "Black" and "Asian" individuals,
models show lower performance in predicting both overall scores as well as the
scores from corresponding groups. Our results suggest that LLMs hold gender and
racial biases for subjective NLP tasks and that demographic-infused prompts
alone may be insufficient to mitigate such effects. Code and data are available
at https://github.com/Jiaxin-Pei/LLM-Group-Bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09731">Prudent Silence or Foolish Babble? Examining Large Language Models&#x27; Responses to the Unknown. (arXiv:2311.09731v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Genglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lifan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hao Peng</a></p>
<p>Large Language Models (LLMs) often struggle when faced with situations where
they lack the prerequisite knowledge to generate a sensical response. In these
cases, models tend to fabricate and hallucinate, rather than appropriately
signaling uncertainty as humans would. This behavior misaligns with human
conversational norms and presents challenges surrounding responsible and
ethical AI development. This work aims to systematically investigate LLMs'
behaviors in such situations. We curate an adversarial question-answering
benchmark containing unanswerable questions targeting information absent from
the LLM's training data. Concretely, these unanswerable questions contain
non-existent concepts or false premises. When presented with such unanswerable
questions, an LLM should appropriately convey uncertainty, and be able to
challenge the premise and refuse to generate a response. While facing
answerable valid questions, a model should demonstrate a positive correlation
between accuracy and confidence. Using a model-agnostic unified confidence
elicitation approach, we observe that LLMs that have gone through instruction
finetuning and reinforcement learning from human feedback (RLHF) perform
significantly better than their counterparts that do not. Moreover, uncertainty
expression 1 through our elicitation method does not always stay consistent
with the perceived confidence of the direct response of an LLM. Our findings
call for further research into teaching LLMs to proactively and reliably
express uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09732">Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources. (arXiv:2311.09732v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yipei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1">Dakuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiaqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yipeng Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1">Yingsi Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hengkui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Ken Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+zhang_r/0/1/0/all/0/1">ruiji zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a></p>
<p>Pre-trained language models (PLMs) have established the new paradigm in the
field of NLP. For more powerful PLMs, one of the most popular and successful
way is to continuously scale up sizes of the models and the pre-training
corpora. These large corpora are generally obtained by converging smaller ones
from multiple sources, they are thus growing increasingly diverse. However, the
side-effects of these colossal converged corpora remain understudied. In this
paper, we identify the disadvantage of heterogeneous corpora from multiple
sources for pre-training PLMs. Towards coordinated pre-training on diverse
corpora, we further propose source prompts (SP), which explicitly prompt the
model of the data source at the pre-training and fine-tuning stages. Results of
extensive experiments demonstrate that PLMs pre-trained with SP on diverse
corpora gain significant improvement in various downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09740">Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations. (arXiv:2311.09740v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Sarkar_R/0/1/0/all/0/1">Rajat Kumar Sarkar</a>, <a href="http://arxiv.org/find/physics/1/au:+Majumdar_R/0/1/0/all/0/1">Ritam Majumdar</a>, <a href="http://arxiv.org/find/physics/1/au:+Jadhav_V/0/1/0/all/0/1">Vishal Jadhav</a>, <a href="http://arxiv.org/find/physics/1/au:+Sakhinana_S/0/1/0/all/0/1">Sagar Srinivas Sakhinana</a>, <a href="http://arxiv.org/find/physics/1/au:+Runkana_V/0/1/0/all/0/1">Venkataramana Runkana</a></p>
<p>In Computational Fluid Dynamics (CFD), coarse mesh simulations offer
computational efficiency but often lack precision. Applying conventional
super-resolution to these simulations poses a significant challenge due to the
fundamental contrast between downsampling high-resolution images and
authentically emulating low-resolution physics. The former method conserves
more of the underlying physics, surpassing the usual constraints of real-world
scenarios. We propose a novel definition of super-resolution tailored for
PDE-based problems. Instead of simply downsampling from a high-resolution
dataset, we use coarse-grid simulated data as our input and predict fine-grid
simulated outcomes. Employing a physics-infused UNet upscaling method, we
demonstrate its efficacy across various 2D-CFD problems such as discontinuity
detection in Burger's equation, Methane combustion, and fouling in Industrial
heat exchangers. Our method enables the generation of fine-mesh solutions
bypassing traditional simulation, ensuring considerable computational saving
and fidelity to the original ground truth outcomes. Through diverse boundary
conditions during training, we further establish the robustness of our method,
paving the way for its broad applications in engineering and scientific CFD
solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09744">Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages. (arXiv:2311.09744v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1">Leopold M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemmer_P/0/1/0/all/0/1">Patrick Hemmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Queisner_M/0/1/0/all/0/1">Moritz Queisner</a>, <a href="http://arxiv.org/find/cs/1/au:+Sauer_I/0/1/0/all/0/1">Igor Sauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Allmendinger_S/0/1/0/all/0/1">Simeon Allmendinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakubik_J/0/1/0/all/0/1">Johannes Jakubik</a>, <a href="http://arxiv.org/find/cs/1/au:+Vossing_M/0/1/0/all/0/1">Michael V&#xf6;ssing</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a></p>
<p>A significant challenge in image-guided surgery is the accurate measurement
task of relevant structures such as vessel segments, resection margins, or
bowel lengths. While this task is an essential component of many surgeries, it
involves substantial human effort and is prone to inaccuracies. In this paper,
we develop a novel human-AI-based method for laparoscopic measurements
utilizing stereo vision that has been guided by practicing surgeons. Based on a
holistic qualitative requirements analysis, this work proposes a comprehensive
measurement method, which comprises state-of-the-art machine learning
architectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed
in various realistic experimental evaluation environments. Our results outline
the potential of our method achieving high accuracies in distance measurements
with errors below 1 mm. Furthermore, on-surface measurements demonstrate
robustness when applied in challenging environments with textureless regions.
Overall, by addressing the inherent challenges of image-guided surgery, we lay
the foundation for a more robust and accurate solution for intra- and
postoperative measurements, enabling more precise, safe, and efficient surgical
procedures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09757">UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution. (arXiv:2311.09757v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Le Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Li Yan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1">Tie Yong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1">Shi Hui Ying</a></p>
<p>Partially supervised segmentation is a label-saving method based on datasets
with fractional classes labeled and intersectant. However, it is still far from
landing on real-world medical applications due to privacy concerns and data
heterogeneity. As a remedy without privacy leakage, federated partially
supervised segmentation (FPSS) is formulated in this work. The main challenges
for FPSS are class heterogeneity and client drift. We propose a Unified
Federated Partially-labeled Segmentation (UFPS) framework to segment pixels
within all classes for partially-annotated datasets by training a totipotential
global model without class collision. Our framework includes Unified Label
Learning and sparsed Unified Sharpness Aware Minimization for unification of
class and feature space, respectively. We find that vanilla combinations for
traditional methods in partially supervised segmentation and federated learning
are mainly hampered by class collision through empirical study. Our
comprehensive experiments on real medical datasets demonstrate better
deconflicting and generalization ability of UFPS compared with modified
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09761">MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification. (arXiv:2311.09761v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Helwe_C/0/1/0/all/0/1">Chadi Helwe</a>, <a href="http://arxiv.org/find/cs/1/au:+Calamai_T/0/1/0/all/0/1">Tom Calamai</a>, <a href="http://arxiv.org/find/cs/1/au:+Paris_P/0/1/0/all/0/1">Pierre-Henri Paris</a>, <a href="http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1">Chlo&#xe9; Clavel</a>, <a href="http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1">Fabian Suchanek</a></p>
<p>Fallacies can be used to spread disinformation, fake news, and propaganda,
underlining the importance of their detection. Automated detection and
classification of fallacies, however, remain challenging, mainly because of the
innate subjectivity of the task and the need for a comprehensive, unified
approach in existing research. Addressing these limitations, our study
introduces a novel taxonomy of fallacies that aligns and refines previous
classifications, a new annotation scheme tailored for subjective NLP tasks, and
a new evaluation method designed to handle subjectivity, adapted to precision,
recall, and F1-Score metrics. Using our annotation scheme, the paper introduces
MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset.
MAFALDA is based on examples from various previously existing fallacy datasets
under our unified taxonomy across three levels of granularity. We then evaluate
several language models under a zero-shot learning setting using MAFALDA to
assess their fallacy detection and classification capability. Our comprehensive
evaluation not only benchmarks the performance of these models but also
provides valuable insights into their strengths and limitations in addressing
fallacious reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09762">Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models. (arXiv:2311.09762v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Ameen Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_O/0/1/0/all/0/1">Omar Zia Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunwoo J. Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joo-Kyung Kim</a></p>
<p>Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning
capabilities of Large Language Models (LLMs) by generating a series of
rationales before the final answer. We analyze the reasoning paths generated by
CoT and find two issues in multi-step reasoning: (i) Generating rationales
irrelevant to the question, (ii) Unable to compose subquestions or queries for
generating/retrieving all the relevant information. To address them, we propose
a graph-guided CoT prompting method, which guides the LLMs to reach the correct
answer with graph representation/verification steps. Specifically, we first
leverage LLMs to construct a "question/rationale graph" by using knowledge
extraction prompting given the initial question and the rationales generated in
the previous steps. Then, the graph verification step diagnoses the current
rationale triplet by comparing it with the existing question/rationale graph to
filter out irrelevant rationales and generate follow-up questions to obtain
relevant information. Additionally, we generate CoT paths that exclude the
extracted graph information to represent the context information missed from
the graph extraction. Our graph-guided reasoning method shows superior
performance compared to previous CoT prompting and the variants on multi-hop
question answering benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09765">Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders. (arXiv:2311.09765v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyunji Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1">Luca Soldaini</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minjoon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1">Kyle Lo</a></p>
<p>Prevailing research practice today often relies on training dense retrievers
on existing large datasets such as MSMARCO and then experimenting with ways to
improve zero-shot generalization capabilities to unseen domains. While prior
work has tackled this challenge through resource-intensive steps such as data
augmentation, architectural modifications, increasing model size, or even
further base model pretraining, comparatively little investigation has examined
whether the training procedures themselves can be improved to yield better
generalization capabilities in the resulting models. In this work, we recommend
a simple recipe for training dense encoders: Train on MSMARCO with
parameter-efficient methods, such as LoRA, and opt for using in-batch negatives
unless given well-constructed hard negatives. We validate these recommendations
using the BEIR benchmark and find results are persistent across choice of dense
encoder and base model size and are complementary to other resource-intensive
strategies for out-of-domain generalization such as architectural modifications
or additional pretraining. We hope that this thorough and impartial study
around various training techniques, which augments other resource-intensive
methods, offers practical insights for developing a dense retrieval model that
effectively generalizes, even when trained on a single dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09774">HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs. (arXiv:2311.09774v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1">Anningzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Feng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shunian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dingjie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wenya Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1">Chuyi Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianquan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiang Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a></p>
<p>Adapting a language model into a specific domain, a.k.a `domain adaption', is
a common practice when specialized knowledge, e.g. medicine, is not
encapsulated in a general language model like Llama2. The challenge lies in the
heterogeneity of data across the two training stages, as it varies in
languages, genres, or formats. To tackle this and simplify the learning
protocol, we propose to transform heterogeneous data, from the both
pre-training and supervised stages, into a unified, simple input-output pair
format. We validate the new protocol in the domains where proprietary LLMs like
ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The
developed model, HuatuoGPT-II, has shown state-of-the-art performance in
Chinese medicine domain on a number of benchmarks, e.g. medical licensing
exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some
aspects, especially in Traditional Chinese Medicine. Expert manual evaluations
further validate HuatuoGPT-II's advantages over existing LLMs. Notably,
HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing
Examination where it achieved the best performance, showcasing not only its
effectiveness but also its generalization capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09780">Model Checking for Closed-Loop Robot Reactive Planning. (arXiv:2311.09780v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chandler_C/0/1/0/all/0/1">Christopher Chandler</a> (School of Computing Science, University of Glasgow), <a href="http://arxiv.org/find/cs/1/au:+Porr_B/0/1/0/all/0/1">Bernd Porr</a> (School of Biomedical Engineering, University of Glasgow), <a href="http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1">Alice Miller</a> (School of Computing Science, University of Glasgow), <a href="http://arxiv.org/find/cs/1/au:+Lafratta_G/0/1/0/all/0/1">Giulia Lafratta</a> (School of Engineering, University of Glasgow)</p>
<p>In this paper, we show how model checking can be used to create multi-step
plans for a differential drive wheeled robot so that it can avoid immediate
danger. Using a small, purpose built model checking algorithm in situ we
generate plans in real-time in a way that reflects the egocentric reactive
response of simple biological agents. Our approach is based on chaining
temporary control systems which are spawned to eliminate disturbances in the
local environment that disrupt an autonomous agent from its preferred action
(or resting state). The method involves a novel discretization of 2D LiDAR data
which is sensitive to bounded stochastic variations in the immediate
environment. We operationalise multi-step planning using invariant checking by
forward depth-first search, using a cul-de-sac scenario as a first test case.
Our results demonstrate that model checking can be used to plan efficient
trajectories for local obstacle avoidance, improving on the performance of a
reactive agent which can only plan one step. We achieve this in near real-time
using no pre-computed data. While our method has limitations, we believe our
approach shows promise as an avenue for the development of safe, reliable and
transparent trajectory planning in the context of autonomous vehicles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09783">Investigating Data Contamination in Modern Benchmarks for Large Language Models. (arXiv:2311.09783v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chunyuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>Recent observations have underscored a disparity between the inflated
benchmark scores and the actual performance of LLMs, raising concerns about
potential contamination of evaluation benchmarks. This issue is especially
critical for closed-source models and certain open-source models where training
data transparency is lacking. In this paper we study data contamination by
proposing two methods tailored for both open-source and proprietary LLMs. We
first introduce a retrieval-based system to explore potential overlaps between
evaluation benchmarks and pretraining corpora. We further present a novel
investigation protocol named \textbf{T}estset \textbf{S}lot Guessing
(\textit{TS-Guessing}), applicable to both open and proprietary models. This
approach entails masking a wrong answer in a multiple-choice question and
prompting the model to fill in the gap. Additionally, it involves obscuring an
unlikely word in an evaluation example and asking the model to produce it. We
find that certain commercial LLMs could surprisingly guess the missing option
in various test sets. Specifically, in the TruthfulQA benchmark, we find that
LLMs exhibit notable performance improvement when provided with additional
metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4
demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing
the missing options in benchmark test data. We hope these results underscore
the need for more robust evaluation methodologies and benchmarks in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09784">Automatic Generation of Scenarios for System-level Simulation-based Verification of Autonomous Driving Systems. (arXiv:2311.09784v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1">Srajan Goyal</a> (Fondazione Bruno Kessler and University of Trento), <a href="http://arxiv.org/find/cs/1/au:+Griggio_A/0/1/0/all/0/1">Alberto Griggio</a> (Fondazione Bruno Kessler), <a href="http://arxiv.org/find/cs/1/au:+Kimblad_J/0/1/0/all/0/1">Jacob Kimblad</a> (Fondazione Bruno Kessler), <a href="http://arxiv.org/find/cs/1/au:+Tonetta_S/0/1/0/all/0/1">Stefano Tonetta</a> (Fondazione Bruno Kessler)</p>
<p>With increasing complexity of Automated Driving Systems (ADS), ensuring their
safety and reliability has become a critical challenge. The Verification and
Validation (V&amp;V) of these systems are particularly demanding when AI components
are employed to implement perception and/or control functions. In ESA-funded
project VIVAS, we developed a generic framework for system-level
simulation-based V&amp;V of autonomous systems. The approach is based on a
simulation model of the system, an abstract model that describes symbolically
the system behavior, and formal methods to generate scenarios and verify the
simulation executions. Various coverage criteria can be defined to guide the
automated generation of the scenarios.
</p>
<p>In this paper, we describe the instantiation of the VIVAS framework for an
ADS case study. This is based on the integration of CARLA, a widely-used
driving simulator, and its ScenarioRunner tool, which enables the creation of
diverse and complex driving scenarios. This is also used in the CARLA
Autonomous Driving Challenge to validate different ADS agents for perception
and control based on AI, shared by the CARLA community. We describe the
development of an abstract ADS model and the formulation of a coverage
criterion that focuses on the behaviors of vehicles relative to the vehicle
with ADS under verification. Leveraging the VIVAS framework, we generate and
execute various driving scenarios, thus testing the capabilities of the AI
components. The results show the effectiveness of VIVAS in automatically
generating scenarios for system-level simulation-based V&amp;V of an automated
driving system using CARLA and ScenarioRunner. Therefore, they highlight the
potential of the approach as a powerful tool in the future of ADS V&amp;V
methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09786">Correct-by-Construction Control for Stochastic and Uncertain Dynamical Models via Formal Abstractions. (arXiv:2311.09786v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Badings_T/0/1/0/all/0/1">Thom Badings</a> (Radboud University), <a href="http://arxiv.org/find/eess/1/au:+Jansen_N/0/1/0/all/0/1">Nils Jansen</a> (Radboud University), <a href="http://arxiv.org/find/eess/1/au:+Romao_L/0/1/0/all/0/1">Licio Romao</a> (University of Oxford), <a href="http://arxiv.org/find/eess/1/au:+Abate_A/0/1/0/all/0/1">Alessandro Abate</a> (University of Oxford)</p>
<p>Automated synthesis of correct-by-construction controllers for autonomous
systems is crucial for their deployment in safety-critical scenarios. Such
autonomous systems are naturally modeled as stochastic dynamical models. The
general problem is to compute a controller that provably satisfies a given
task, represented as a probabilistic temporal logic specification. However,
factors such as stochastic uncertainty, imprecisely known parameters, and
hybrid features make this problem challenging. We have developed an abstraction
framework that can be used to solve this problem under various modeling
assumptions. Our approach is based on a robust finite-state abstraction of the
stochastic dynamical model in the form of a Markov decision process with
intervals of probabilities (iMDP). We use state-of-the-art verification
techniques to compute an optimal policy on the iMDP with guarantees for
satisfying the given specification. We then show that, by construction, we can
refine this policy into a feedback controller for which these guarantees carry
over to the dynamical model. In this short paper, we survey our recent research
in this area and highlight two challenges (related to scalability and dealing
with nonlinear dynamics) that we aim to address with our ongoing research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09787">3vLTL: A Tool to Generate Automata for Three-valued LTL. (arXiv:2311.09787v1 [cs.FL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belardinelli_F/0/1/0/all/0/1">Francesco Belardinelli</a> (Imperial College London), <a href="http://arxiv.org/find/cs/1/au:+Ferrando_A/0/1/0/all/0/1">Angelo Ferrando</a> (University of Genoa), <a href="http://arxiv.org/find/cs/1/au:+Malvone_V/0/1/0/all/0/1">Vadim Malvone</a> (Telecom Paris)</p>
<p>Multi-valued logics have a long tradition in the literature on system
verification, including run-time verification. However, comparatively fewer
model-checking tools have been developed for multi-valued specification
languages. We present 3vLTL, a tool to generate Buchi automata from formulas in
Linear-time Temporal Logic (LTL) interpreted on a three-valued semantics. Given
an LTL formula, a set of atomic propositions as the alphabet for the automaton,
and a truth value, our procedure generates a Buchi automaton that accepts all
the words that assign the chosen truth value to the LTL formula. Given the
particular type of the output of the tool, it can also be seamlessly processed
by third-party libraries in a natural way. That is, the Buchi automaton can
then be used in the context of formal verification to check whether an LTL
formula is true, false, or undefined on a given model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09790">Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting. (arXiv:2311.09790v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Romain_I/0/1/0/all/0/1">Ilbert Romain</a>, <a href="http://arxiv.org/find/cs/1/au:+Thai_V/0/1/0/all/0/1">V. Hoang Thai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zonghua_Z/0/1/0/all/0/1">Zhang Zonghua</a>, <a href="http://arxiv.org/find/cs/1/au:+Themis_P/0/1/0/all/0/1">Palpanas Themis</a></p>
<p>Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
% In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. % Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. %
Our optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. % Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09796">Interpreting User Requests in the Context of Natural Language Standing Instructions. (arXiv:2311.09796v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1">Nikita Moghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1">Patrick Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1">Jason Eisner</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a>, <a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1">Harsh Jhamtani</a></p>
<p>Users of natural language interfaces, generally powered by Large Language
Models (LLMs),often must repeat their preferences each time they make a similar
request. To alleviate this, we propose including some of a user's preferences
and instructions in natural language -- collectively termed standing
instructions -- as additional context for such interfaces. For example, when a
user states I'm hungry, their previously expressed preference for Persian food
will be automatically added to the LLM prompt, so as to influence the search
for relevant restaurants. We develop NLSI, a language-to-program dataset
consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is
paired with a user profile (a set of users specific standing instructions) and
corresponding structured representations (API calls). A key challenge in NLSI
is to identify which subset of the standing instructions is applicable to a
given dialogue. NLSI contains diverse phenomena, from simple preferences to
interdependent instructions such as triggering a hotel search whenever the user
is booking tickets to an event. We conduct experiments on NLSI using prompting
with large language models and various retrieval approaches, achieving a
maximum of 44.7% exact match on API prediction. Our results demonstrate the
challenges in identifying the relevant standing instructions and their
interpretation into API calls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09802">Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs. (arXiv:2311.09802v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Leyang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1">Lidong Bing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1">Wai Lam</a></p>
<p>Though prompting LLMs with various reasoning structures produces reasoning
proofs along with answers, these proofs are not ensured to be causal and
reliable due to the inherent defects of LLMs. Tracking such deficiencies, we
present a neuro-symbolic integration method, in which a neural LLM is used to
represent the knowledge of the problem while an LLM-free symbolic solver is
adopted to do deliberative reasoning using the knowledge. Specifically, our
customized meta-interpreters allow the production of reasoning proofs and
support flexible search strategies. These reasoning proofs are ensured to be
causal and reliable because of the deterministic executing nature of the
symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT
baseline by nearly double in accuracy and more than triple in proof similarity.
On GSM8K, our method also shows accuracy improvements and nearly doubled proof
similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09809">Comparing Differentiable Logics for Learning Systems: A Research Preview. (arXiv:2311.09809v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Flinkow_T/0/1/0/all/0/1">Thomas Flinkow</a> (Maynooth University), <a href="http://arxiv.org/find/cs/1/au:+Pearlmutter_B/0/1/0/all/0/1">Barak A. Pearlmutter</a> (Maynooth University), <a href="http://arxiv.org/find/cs/1/au:+Monahan_R/0/1/0/all/0/1">Rosemary Monahan</a> (Maynooth University)</p>
<p>Extensive research on formal verification of machine learning (ML) systems
indicates that learning from data alone often fails to capture underlying
background knowledge. A variety of verifiers have been developed to ensure that
a machine-learnt model satisfies correctness and safety properties, however,
these verifiers typically assume a trained network with fixed weights.
ML-enabled autonomous systems are required to not only detect incorrect
predictions, but should also possess the ability to self-correct, continuously
improving and adapting. A promising approach for creating ML models that
inherently satisfy constraints is to encode background knowledge as logical
constraints that guide the learning process via so-called differentiable
logics. In this research preview, we compare and evaluate various logics from
the literature in weakly-supervised contexts, presenting our findings and
highlighting open problems for future work. Our experimental results are
broadly consistent with results reported previously in literature; however,
learning with differentiable logics introduces a new hyperparameter that is
difficult to tune and has significant influence on the effectiveness of the
logics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09810">Towards Formal Fault Injection for Safety Assessment of Automated Systems. (arXiv:2311.09810v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farooqui_A/0/1/0/all/0/1">Ashfaq Farooqui</a> (Dependable Transport Systems, RISE Research Institutes of Sweden, Bor&#xe5;s, Sweden), <a href="http://arxiv.org/find/cs/1/au:+Sangchoolie_B/0/1/0/all/0/1">Behrooz Sangchoolie</a> (Dependable Transport Systems, RISE Research Institutes of Sweden, Bor&#xe5;s, Sweden)</p>
<p>Reasoning about safety, security, and other dependability attributes of
autonomous systems is a challenge that needs to be addressed before the
adoption of such systems in day-to-day life. Formal methods is a class of
methods that mathematically reason about a system's behavior. Thus, a
correctness proof is sufficient to conclude the system's dependability.
However, these methods are usually applied to abstract models of the system,
which might not fully represent the actual system. Fault injection, on the
other hand, is a testing method to evaluate the dependability of systems.
However, the amount of testing required to evaluate the system is rather large
and often a problem. This vision paper introduces formal fault injection, a
fusion of these two techniques throughout the development lifecycle to enhance
the dependability of autonomous systems. We advocate for a more cohesive
approach by identifying five areas of mutual support between formal methods and
fault injection. By forging stronger ties between the two fields, we pave the
way for developing safe and dependable autonomous systems. This paper delves
into the integration's potential and outlines future research avenues,
addressing open challenges along the way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09816">Performance Trade-offs of Watermarking Large Language Models. (arXiv:2311.09816v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1">Anirudh Ajith</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1">Danish Pruthi</a></p>
<p>Amidst growing concerns of large language models (LLMs) being misused for
generating misinformation or completing homework assignments, watermarking has
emerged as an effective solution for distinguishing human-written and
LLM-generated text. A prominent watermarking strategy is to embed a signal into
generated text by upsampling a (pseudorandomly-chosen) subset of tokens at
every generation step. Although this signal is imperceptible to a human reader,
it is detectable through statistical testing. However, implanting such signals
alters the model's output distribution and can have unintended effects when
watermarked LLMs are used for downstream applications. In this work, we
evaluate the performance of watermarked LLMs on a diverse suite of tasks,
including text classification, textual entailment, reasoning, question
answering, translation, summarization, and language modeling. We find that
watermarking has negligible impact on the performance of tasks posed as k-class
classification problems in the average case. However, the accuracy can plummet
to that of a random classifier for some scenarios (that occur with
non-negligible probability). Tasks that are cast as multiple-choice questions
and short-form generation are surprisingly unaffected by watermarking. For
long-form generation tasks, including summarization and translation, we see a
drop of 15-20% in the performance due to watermarking. Our findings highlight
the trade-offs that users should be cognizant of when using watermarked models,
and point to cases where future research could improve existing trade-offs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09819">PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments. (arXiv:2311.09819v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongbin Liu</a></p>
<p>In surgical procedures, correct instrument counting is essential. Instance
segmentation is a location method that locates not only an object's bounding
box but also each pixel's specific details. However, obtaining mask-level
annotations is labor-intensive in instance segmentation. To address this issue,
we propose a novel yet effective weakly-supervised surgical instrument instance
segmentation approach, named Point-based Weakly-supervised Instance
Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with
point-to-box and point-to-mask branches to model the relationships between
feature points and bounding boxes, as well as feature points and segmentation
masks on FPN, accomplishing instrument detection and segmentation jointly in a
single model. Since mask level annotations are hard to available in the real
world, for point-to-mask training, we introduce an unsupervised projection
loss, utilizing the projected relation between predicted masks and bboxes as
supervision signal. On the other hand, we annotate a few pixels as the key
pixel for each instrument. Based on this, we further propose a key pixel
association loss and a key pixel distribution loss, driving the point-to-mask
branch to generate more accurate segmentation predictions. To comprehensively
evaluate this task, we unveil a novel surgical instrument dataset with manual
annotations, setting up a benchmark for further research. Our comprehensive
research trial validated the superior performance of our PWISeg. The results
show that the accuracy of surgical instrument segmentation is improved,
surpassing most methods of instance segmentation via weakly supervised bounding
boxes. This improvement is consistently observed in our proposed dataset and
when applied to the public HOSPI-Tools dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09830">AutoPlanBench: : Automatically generating benchmarks for LLM planners from PDDL. (arXiv:2311.09830v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stein_K/0/1/0/all/0/1">Katharina Stein</a>, <a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1">Alexander Koller</a></p>
<p>LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present a
novel method for automatically converting planning benchmarks written in PDDL
into textual descriptions and offer a benchmark dataset created with our
method. We show that while the best LLM planners do well on many planning
tasks, others remain out of reach of current methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09835">ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks. (arXiv:2311.09835v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zefan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junjie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1">Yanjun Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zexuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Helan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zengxian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1">Kaikai An</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Ruijun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Shuzheng Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haozhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1">Yiming Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhiwei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1">Baobao Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wangchunshu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a></p>
<p>Large language models have shown promising performance in code generation
benchmarks. However, a considerable divide exists between these benchmark
achievements and their practical applicability, primarily attributed to
real-world programming's reliance on pre-existing libraries. Instead of
evaluating LLMs to code from scratch, this work aims to propose a new
evaluation setup where LLMs use open-source libraries to finish machine
learning tasks. Therefore, we propose ML-Bench, an expansive benchmark
developed to assess the effectiveness of LLMs in leveraging existing functions
in open-source libraries. Consisting of 10044 samples spanning 130 tasks over
14 notable machine learning GitHub repositories. In this setting, given a
specific machine learning task instruction and the accompanying README in a
codebase, an LLM is tasked to generate code to accomplish the task. This
necessitates the comprehension of long and language-code interleaved documents,
as well as the understanding of complex cross-file code structures, introducing
new challenges. Notably, while GPT-4 exhibits remarkable improvement over other
LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space
for improvement. We address these challenges by proposing ML-Agent, designed to
effectively navigate the codebase, locate documentation, retrieve code, and
generate executable code. Empirical results demonstrate that ML-Agent, built
upon GPT-4, results in further improvements. Code, data, and models are
available at \url{https://ml-bench.github.io/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09836">PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization. (arXiv:2311.09836v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peper_J/0/1/0/all/0/1">Joseph J. Peper</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Wenzhao Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a></p>
<p>We investigate pre-training techniques for abstractive multi-document
summarization (MDS), which is much less studied than summarizing single
documents. Though recent work has demonstrated the effectiveness of
highlighting information salience for pre-training strategy design, it
struggles to generate abstractive and reflective summaries, which are critical
properties for MDS. To this end, we present PELMS, a pre-trained model that
uses objectives based on semantic coherence heuristics and faithfulness
constraints with un-labeled multi-document inputs, to promote the generation of
concise, fluent, and faithful summaries. To support the training of PELMS, we
compile MultiPT, a multi-document pre-training corpus containing over 93
million documents to form more than 3 million unlabeled topic-centric document
clusters, covering diverse genres such as product reviews, news, and general
knowledge. We perform extensive evaluation of PELMS in low-shot settings on a
wide range of MDS datasets. Our approach consistently outperforms competitive
comparisons with respect to overall informativeness, abstractiveness,
coherence, and faithfulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09841">Leveraging LLMs in Scholarly Knowledge Graph Question Answering. (arXiv:2311.09841v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taffa_T/0/1/0/all/0/1">Tilahun Abedissa Taffa</a>, <a href="http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1">Ricardo Usbeck</a></p>
<p>This paper presents a scholarly Knowledge Graph Question Answering (KGQA)
that answers bibliographic natural language questions by leveraging a large
language model (LLM) in a few-shot manner. The model initially identifies the
top-n similar training questions related to a given test question via a
BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the
top-n similar question-SPARQL pairs as an example and the test question creates
a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs
the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and
returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of
the Scholarly-QALD-23 challenge benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09854">SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits/Records. (arXiv:2311.09854v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hung Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Eng_Jon_O/0/1/0/all/0/1">Ong Eng-Jon</a>, <a href="http://arxiv.org/find/cs/1/au:+Miroslaw_B/0/1/0/all/0/1">Bober Miroslaw</a></p>
<p>The accurate prediction of survival times for patients with severe diseases
remains a critical challenge despite recent advances in artificial
intelligence. This study introduces "SurvTimeSurvival: Survival Analysis On
Patients With Multiple Visits/Records", utilizing the Transformer model to not
only handle the complexities of time-varying covariates but also covariates
data. We also tackle the data sparsity issue common to survival analysis
datasets by integrating synthetic data generation into the learning process of
our model. We show that our method outperforms state-of-the-art deep learning
approaches on both covariates and time-varying covariates datasets. Our
approach aims not only to enhance the understanding of individual patient
survival trajectories across various medical conditions, thereby improving
prediction accuracy, but also to play a pivotal role in designing clinical
trials and creating new treatments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09861">PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models. (arXiv:2311.09861v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junlei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hongliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_N/0/1/0/all/0/1">Nirui Song</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shuyuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_%5C/0/1/0/all/0/1">\\Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Huachuan Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Anqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model's strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models'
strengths and weaknesses and assist in practical application in the field of
psychology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09868">INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanbin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1">Ganqu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1">Ning Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Ge Yu</a></p>
<p>This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics
human code repairing behavior (iteratively judging, rethinking, and repairing)
and prompts the coding ability of regard Large Language Models (LLMs).
Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code
Teacher, to play different roles in code repairing and work interactively to
repair the generated codes. The Code Learner is asked to generate and repair
code according to the instructions from the Code Teacher. The Code Teacher
rethinks the code errors according to the corresponding feedback from compilers
and iteratively generates the chain-of-repairing (CoR) to guide the code
repairing process for Code Learner. Our experiments show that INTERVENOR
outperforms the state-of-the-art methods and achieves about 13% and 4.5%
improvements over the GPT-3.5 model in code generation and code translation
tasks, respectively. Our further analyses show that CoR can illuminate the bug
reasons and solution plans via natural language. Thanks to the feedback of code
compilers, INTERVENOR can accurately identify the syntax errors and assertion
errors in the code and provide precise instructions to repair codes, making
LLMs achieve the plateau performance with only three repairing turns. All data
and codes are available at https://github.com/NEUIR/INTERVENOR
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09919">DSR-Diff: Depth Map Super-Resolution with Diffusion Model. (arXiv:2311.09919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1">Bin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qingmin Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenming Yang</a></p>
<p>Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09930">A Framework for Monitoring and Retraining Language Models in Real-World Applications. (arXiv:2311.09930v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasundra_J/0/1/0/all/0/1">Jaykumar Kasundra</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1">Claudia Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirsafian_M/0/1/0/all/0/1">Melicaalsadat Mirsafian</a>, <a href="http://arxiv.org/find/cs/1/au:+Skylaki_S/0/1/0/all/0/1">Stavroula Skylaki</a></p>
<p>In the Machine Learning (ML) model development lifecycle, training candidate
models using an offline holdout dataset and identifying the best model for the
given task is only the first step. After the deployment of the selected model,
continuous model monitoring and model retraining is required in many real-world
applications. There are multiple reasons for retraining, including data or
concept drift, which may be reflected on the model performance as monitored by
an appropriate metric. Another motivation for retraining is the acquisition of
increasing amounts of data over time, which may be used to retrain and improve
the model performance even in the absence of drifts. We examine the impact of
various retraining decision points on crucial factors, such as model
performance and resource utilization, in the context of Multilabel
Classification models. We explain our key decision points and propose a
reference framework for designing an effective model retraining strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09993">Generative AI for Hate Speech Detection: Evaluation and Findings. (arXiv:2311.09993v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pendzel_S/0/1/0/all/0/1">Sagi Pendzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1">Tomer Wullach</a>, <a href="http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1">Amir Adler</a>, <a href="http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1">Einat Minkov</a></p>
<p>Automatic hate speech detection using deep neural models is hampered by the
scarcity of labeled datasets, leading to poor generalization. To mitigate this
problem, generative AI has been utilized to generate large amounts of synthetic
hate speech sequences from available labeled examples, leveraging the generated
data in finetuning large pre-trained language models (LLMs). In this chapter,
we provide a review of relevant methods, experimental setups and evaluation of
this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,
we apply and evaluate the impact of train set augmentation with generated data
using LLMs that have been already adapted for hate detection, including
RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical
study corroborates our previous findings, showing that this approach improves
hate speech generalization, boosting recall performance across data
distributions. In addition, we explore and compare the performance of the
finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results
demonstrate that while better generalization is achieved using the GPT-3.5
model, it achieves mediocre recall and low precision on most datasets. It is an
open question whether the sensitivity of models such as GPT-3.5, and onward,
can be improved using similar techniques of text generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09994">Towards more Practical Threat Models in Artificial Intelligence Security. (arXiv:2311.09994v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grosse_K/0/1/0/all/0/1">Kathrin Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Bieringer_L/0/1/0/all/0/1">Lukas Bieringer</a>, <a href="http://arxiv.org/find/cs/1/au:+Besold_T/0/1/0/all/0/1">Tarek Richard Besold</a>, <a href="http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1">Alexandre Alahi</a></p>
<p>Recent works have identified a gap between research and practice in
artificial intelligence security: threats studied in academia do not always
reflect the practical use and security risks of AI. For example, while models
are often studied in isolation, they form part of larger ML pipelines in
practice. Recent works also brought forward that adversarial manipulations
introduced by academic attacks are impractical. We take a first step towards
describing the full extent of this disparity. To this end, we revisit the
threat models of the six most studied attacks in AI security research and match
them to AI usage in practice via a survey with \textbf{271} industrial
practitioners. On the one hand, we find that all existing threat models are
indeed applicable. On the other hand, there are significant mismatches:
research is often too generous with the attacker, assuming access to
information not frequently available in real-world settings. Our paper is thus
a call for action to study more practical threat models in artificial
intelligence security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10002">Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network. (arXiv:2311.10002v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hongda Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Ping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayana_C/0/1/0/all/0/1">C V Aswartha Narayana</a></p>
<p>Federated Learning (FL) enables many resource-limited devices to train a
model collaboratively without data sharing. However, many existing works focus
on model-homogeneous FL, where the global and local models are the same size,
ignoring the inherently heterogeneous computational capabilities of different
devices and restricting resource-constrained devices from contributing to FL.
In this paper, we consider model-heterogeneous FL and propose Federated Partial
Model Training (FedPMT), where devices with smaller computational capabilities
work on partial models (subsets of the global model) and contribute to the
global model. Different from Dropout-based partial model generation, which
removes neurons in hidden layers at random, model training in FedPMT is
achieved from the back-propagation perspective. As such, all devices in FedPMT
prioritize the most crucial parts of the global model. Theoretical analysis
shows that the proposed partial model training design has a similar convergence
rate to the widely adopted Federated Averaging (FedAvg) algorithm,
$\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor
related to the model splitting design in FedPMT. Empirical results show that
FedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile,
compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the
learning target in a shorter completion time, thus achieving a better trade-off
between learning accuracy and completion time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.11482">Representations of epistemic uncertainty and its perception in data-driven strategies. (arXiv:2110.11482v6 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Angelelli_M/0/1/0/all/0/1">Mario Angelelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gervasi_M/0/1/0/all/0/1">Massimiliano Gervasi</a></p>
<p>The diffusion of AI and big data is reshaping decision-making processes by
increasing the amount of information that supports decisions while reducing
direct interaction with data and empirical evidence. This paradigm shift
introduces new sources of uncertainty, as limited data observability results in
ambiguity and a lack of interpretability. The need for the proper analysis of
data-driven strategies motivates the search for new models that can describe
this type of bounded access to knowledge. This contribution presents a novel
theoretical model for uncertainty in knowledge representation and its transfer
mediated by agents. We provide a dynamical description of knowledge states by
endowing our model with a structure to compare and combine them. Specifically,
an update is represented through combinations, and its explainability is based
on its consistency in different dimensional representations. We look at
inequivalent knowledge representations in terms of multiplicity of inferences,
preference relations, and information measures. Furthermore, we define a formal
analogy with two scenarios that illustrate non-classical uncertainty in terms
of ambiguity (Ellsberg's model) and reasoning about knowledge mediated by other
agents observing data (Wigner's friend). Finally, we discuss some implications
of the proposed model for data-driven strategies, with special attention to
reasoning under uncertainty about business value dimensions and the design of
measurement tools for their assessment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.03178">Student of Games: A unified learning algorithm for both perfect and imperfect information games. (arXiv:2112.03178v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmid_M/0/1/0/all/0/1">Martin Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Moravcik_M/0/1/0/all/0/1">Matej Moravcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Burch_N/0/1/0/all/0/1">Neil Burch</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadlec_R/0/1/0/all/0/1">Rudolf Kadlec</a>, <a href="http://arxiv.org/find/cs/1/au:+Davidson_J/0/1/0/all/0/1">Josh Davidson</a>, <a href="http://arxiv.org/find/cs/1/au:+Waugh_K/0/1/0/all/0/1">Kevin Waugh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bard_N/0/1/0/all/0/1">Nolan Bard</a>, <a href="http://arxiv.org/find/cs/1/au:+Timbers_F/0/1/0/all/0/1">Finbarr Timbers</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1">Marc Lanctot</a>, <a href="http://arxiv.org/find/cs/1/au:+Holland_G/0/1/0/all/0/1">G. Zacharias Holland</a>, <a href="http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1">Elnaz Davoodi</a>, <a href="http://arxiv.org/find/cs/1/au:+Christianson_A/0/1/0/all/0/1">Alden Christianson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowling_M/0/1/0/all/0/1">Michael Bowling</a></p>
<p>Games have a long history as benchmarks for progress in artificial
intelligence. Approaches using search and learning produced strong performance
across many perfect information games, and approaches using game-theoretic
reasoning and learning demonstrated strong performance for specific imperfect
information poker variants. We introduce Student of Games, a general-purpose
algorithm that unifies previous approaches, combining guided search, self-play
learning, and game-theoretic reasoning. Student of Games achieves strong
empirical performance in large perfect and imperfect information games -- an
important step towards truly general algorithms for arbitrary environments. We
prove that Student of Games is sound, converging to perfect play as available
computation and approximation capacity increases. Student of Games reaches
strong performance in chess and Go, beats the strongest openly available agent
in heads-up no-limit Texas hold'em poker, and defeats the state-of-the-art
agent in Scotland Yard, an imperfect information game that illustrates the
value of guided search, learning, and game-theoretic reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10367">Probabilities of the third type: Statistical Relational Learning and Reasoning with Relative Frequencies. (arXiv:2202.10367v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weitkamper_F/0/1/0/all/0/1">Felix Weitk&#xe4;mper</a></p>
<p>Dependencies on the relative frequency of a state in the domain are common
when modelling probabilistic dependencies on relational data. For instance, the
likelihood of a school closure during an epidemic might depend on the
proportion of infected pupils exceeding a threshold. Often, rather than
depending on discrete thresholds, dependencies are continuous: for instance,
the likelihood of any one mosquito bite transmitting an illness depends on the
proportion of carrier mosquitoes. Current approaches usually only consider
probabilities over possible worlds rather than over domain elements themselves.
An exception are the recently introduced Lifted Bayesian Networks for
Conditional Probability Logic, which express discrete dependencies on
probabilistic data. We introduce functional lifted Bayesian networks, a
formalism that explicitly incorporates continuous dependencies on relative
frequencies into statistical relational artificial intelligence. and compare
and contrast them with ifted Bayesian Networks for Conditional Probability
Logic. Incorporating relative frequencies is not only beneficial to modelling;
it also provides a more rigorous approach to learning problems where training
and test or application domains have different sizes. To this end, we provide a
representation of the asymptotic probability distributions induced by
functional lifted Bayesian networks on domains of increasing sizes. Since that
representation has well-understood scaling behaviour across domain sizes, it
can be used to estimate parameters for a large domain consistently from
randomly sampled subpopulations. Furthermore, we show that in parametric
families of FLBN, convergence is uniform in the parameters, which ensures a
meaningful dependence of the asymptotic probabilities on the parameters of the
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.07394">Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems Using Online Reinforcement Learning. (arXiv:2205.07394v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1">Gagandeep Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadig_R/0/1/0/all/0/1">Rakesh Nadig</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jisung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Bera_R/0/1/0/all/0/1">Rahul Bera</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajinazar_N/0/1/0/all/0/1">Nastaran Hajinazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Novo_D/0/1/0/all/0/1">David Novo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_Luna_J/0/1/0/all/0/1">Juan G&#xf3;mez-Luna</a>, <a href="http://arxiv.org/find/cs/1/au:+Stuijk_S/0/1/0/all/0/1">Sander Stuijk</a>, <a href="http://arxiv.org/find/cs/1/au:+Corporaal_H/0/1/0/all/0/1">Henk Corporaal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Mutlu</a></p>
<p>Hybrid storage systems (HSS) use multiple different storage devices to
provide high and scalable storage capacity at high performance. Recent research
proposes various techniques that aim to accurately identify
performance-critical data to place it in a "best-fit" storage device.
Unfortunately, most of these techniques are rigid, which (1) limits their
adaptivity to perform well for a wide range of workloads and storage device
configurations, and (2) makes it difficult for designers to extend these
techniques to different storage system configurations (e.g., with a different
number or different types of storage devices) than the configuration they are
designed for. We introduce Sibyl, the first technique that uses reinforcement
learning for data placement in hybrid storage systems. Sibyl observes different
features of the running workload as well as the storage devices to make
system-aware data placement decisions. For every decision it makes, Sibyl
receives a reward from the system that it uses to evaluate the long-term
performance impact of its decision and continuously optimizes its data
placement policy online. We implement Sibyl on real systems with various HSS
configurations. Our results show that Sibyl provides 21.6%/19.9% performance
improvement in a performance-oriented/cost-oriented HSS configuration compared
to the best previous data placement technique. Our evaluation using an HSS
configuration with three different storage devices shows that Sibyl outperforms
the state-of-the-art data placement policy by 23.9%-48.2%, while significantly
reducing the system architect's burden in designing a data placement mechanism
that can simultaneously incorporate three storage devices. We show that Sibyl
achieves 80% of the performance of an oracle policy that has complete knowledge
of future access patterns while incurring a very modest storage overhead of
only 124.4 KiB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.12683">Rethinking Fano&#x27;s Inequality in Ensemble Learning. (arXiv:2205.12683v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Horiguchi_S/0/1/0/all/0/1">Shota Horiguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozaki_H/0/1/0/all/0/1">Hiroaki Ozaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Nukaga_N/0/1/0/all/0/1">Nobuo Nukaga</a></p>
<p>We propose a fundamental theory on ensemble learning that answers the central
question: what factors make an ensemble system good or bad? Previous studies
used a variant of Fano's inequality of information theory and derived a lower
bound of the classification error rate on the basis of the $\textit{accuracy}$
and $\textit{diversity}$ of models. We revisit the original Fano's inequality
and argue that the studies did not take into account the information lost when
multiple model predictions are combined into a final prediction. To address
this issue, we generalize the previous theory to incorporate the information
loss, which we name $\textit{combination loss}$. Further, we empirically
validate and demonstrate the proposed theory through extensive experiments on
actual systems. The theory reveals the strengths and weaknesses of systems on
each metric, which will push the theoretical understanding of ensemble learning
and give us insights into designing systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.04876">On the Intrinsic Structures of Spiking Neural Networks. (arXiv:2207.04876v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shao-Qun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jia-Yi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jin-Hui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Huan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Bin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhi-Hua Zhou</a></p>
<p>Recent years have emerged a surge of interest in SNNs owing to their
remarkable potential to handle time-dependent and event-driven data. The
performance of SNNs hinges not only on selecting an apposite architecture and
fine-tuning connection weights, similar to conventional ANNs, but also on the
meticulous configuration of intrinsic structures within spiking computations.
However, there has been a dearth of comprehensive studies examining the impact
of intrinsic structures. Consequently, developers often find it challenging to
apply a standardized configuration of SNNs across diverse datasets or tasks.
This work delves deep into the intrinsic structures of SNNs. Initially, we
unveil two pivotal components of intrinsic structures: the integration
operation and firing-reset mechanism, by elucidating their influence on the
expressivity of SNNs. Furthermore, we draw two key conclusions: the membrane
time hyper-parameter is intimately linked to the eigenvalues of the integration
operation, dictating the functional topology of spiking dynamics, and various
hyper-parameters of the firing-reset mechanism govern the overall firing
capacity of an SNN, mitigating the injection ratio or sampling density of input
data. These findings elucidate why the efficacy of SNNs hinges heavily on the
configuration of intrinsic structures and lead to a recommendation that
enhancing the adaptability of these structures contributes to improving the
overall performance and applicability of SNNs. Inspired by this recognition, we
propose two feasible approaches to enhance SNN learning. These involve
leveraging self-connection architectures and employing stochastic spiking
neurons to augment the adaptability of the integration operation and
firing-reset mechanism, respectively. We verify the effectiveness of the
proposed methods from perspectives of theory and practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10161">MUDGUARD: Taming Malicious Majorities in Federated Learning using Privacy-Preserving Byzantine-Robust Clustering. (arXiv:2208.10161v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingkai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanhuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Decouchant_J/0/1/0/all/0/1">J&#xe9;r&#xe9;mie Decouchant</a>, <a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1">Stjepan Picek</a>, <a href="http://arxiv.org/find/cs/1/au:+Laoutaris_N/0/1/0/all/0/1">Nikolaos Laoutaris</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaitai Liang</a></p>
<p>Byzantine-robust Federated Learning (FL) aims to counter malicious clients
and train an accurate global model while maintaining an extremely low attack
success rate. Most existing systems, however, are only robust when most of the
clients are honest. FLTrust (NDSS '21) and Zeno++ (ICML '20) do not make such
an honest majority assumption but can only be applied to scenarios where the
server is provided with an auxiliary dataset used to filter malicious updates.
FLAME (USENIX '22) and EIFFeL (CCS '22) maintain the semi-honest majority
assumption to guarantee robustness and the confidentiality of updates. It is
therefore currently impossible to ensure Byzantine robustness and
confidentiality of updates without assuming a semi-honest majority. To tackle
this problem, we propose a novel Byzantine-robust and privacy-preserving FL
system, called MUDGUARD, that can operate under malicious minority \emph{or
majority} in both the server and client sides. Based on DBSCAN, we design a new
method for extracting features from model updates via pairwise adjusted cosine
similarity to boost the accuracy of the resulting clustering. To thwart attacks
from a malicious majority, we develop a method called \textit{Model
Segmentation}, that aggregates together only the updates from within a cluster,
sending the corresponding model only to the clients of the corresponding
cluster. The fundamental idea is that even if malicious clients are in their
majority, their poisoned updates cannot harm benign clients if they are
confined only within the malicious cluster. We also leverage multiple
cryptographic tools to conduct clustering without sacrificing training
correctness and updates confidentiality. We present a detailed security proof
and empirical evaluation along with a convergence analysis for MUDGUARD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14838">EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Angelica Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1">David M. Dohan</a>, <a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1">David R. So</a></p>
<p>Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03290">AmQA: Amharic Question Answering Dataset. (arXiv:2303.03290v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abedissa_T/0/1/0/all/0/1">Tilahun Abedissa</a>, <a href="http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1">Ricardo Usbeck</a>, <a href="http://arxiv.org/find/cs/1/au:+Assabie_Y/0/1/0/all/0/1">Yaregal Assabie</a></p>
<p>Question Answering (QA) returns concise answers or answer lists from natural
language text given a context document. Many resources go into curating QA
datasets to advance robust models' development. There is a surge of QA datasets
for languages like English, however, this is not true for Amharic. Amharic, the
official language of Ethiopia, is the second most spoken Semitic language in
the world. There is no published or publicly available Amharic QA dataset.
Hence, to foster the research in Amharic QA, we present the first Amharic QA
(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia
articles. Additionally, we run an XLMR Large-based baseline model to spark
open-domain QA research interest. The best-performing baseline achieves an
F-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension
settings respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07438">Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1">Meihua Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>Despite the success of autoregressive large language models in text
generation, it remains a major challenge to generate text that satisfies
complex constraints: sampling from the conditional distribution
${\Pr}(\text{text} | \alpha)$ is intractable for even the simplest lexical
constraints $\alpha$. To overcome this challenge, we propose to use tractable
probabilistic models (TPMs) to impose lexical constraints in autoregressive
text generation models, which we refer to as GeLaTo (Generating Language with
Tractable Constraints). To demonstrate the effectiveness of this framework, we
use distilled hidden Markov models, where we can efficiently compute
${\Pr}(\text{text} | \alpha)$, to guide autoregressive generation from GPT2.
GeLaTo achieves state-of-the-art performance on challenging benchmarks for
constrained text generation (e.g., CommonGen), beating various strong baselines
by a large margin. Our work not only opens up new avenues for controlling large
language models but also motivates the development of more expressive TPMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02547">PersonaLLM: Investigating the Ability of Large Language Models to Express Big Five Personality Traits. (arXiv:2305.02547v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiajie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xubo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabbara_J/0/1/0/all/0/1">Jad Kabbara</a></p>
<p>Despite the many use cases for large language models (LLMs) in creating
personalized chatbots, there has been limited research on evaluating the extent
to which the behaviors of personalized LLMs accurately and consistently reflect
specific personality traits. We consider studying the behavior of LLM-based
agents, referred to as LLM personas, and present a case study with ChatGPT and
GPT-4. The study investigates whether LLMs can generate content that aligns
with their assigned personality profiles. To this end, we create distinct LLM
personas based on the Big Five personality model, have them complete the
44-item Big Five Inventory (BFI) personality test and a story writing task, and
then assess their essays with automatic and human evaluations. Results show
that LLM personas' self-reported BFI scores are consistent with their
designated personality types, with large effect sizes observed across five
traits. Additionally, there are significant correlations between the assigned
personality types and certain psycholinguistic features of their writings, as
measured by the Linguistic Inquiry and Word Count (LIWC) tool. Interestingly,
human evaluators perceive the stories as less personal when told that the
stories are authored by AI. However, their judgments on other aspects of the
writing such as readability, cohesiveness, redundancy, likeability, and
believability remain largely unaffected. Notably, when evaluators were informed
about the AI authorship, their accuracy in identifying the intended personality
traits from the stories decreased by more than 10% for some traits. This
research marks a significant step forward in understanding the capabilities of
LLMs to express personality traits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06626">When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Abebe_R/0/1/0/all/0/1">Rediet Abebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences in opinion across groups, not
noise. Thus, a crucial problem in hate speech detection is determining whether
a statement is offensive to the demographic group that it targets, when that
group may constitute a small fraction of the annotator pool. We construct a
model that predicts individual annotator ratings on potentially offensive text
and combines this information with the predicted target group of the text to
model the opinions of target group members. We show gains across a range of
metrics, including raising performance over the baseline by 22% at predicting
individual annotators' ratings and by 33% at predicting variance among
annotators, which provides a metric for model uncertainty downstream. We find
that annotator ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12025">Biomembrane-based Memcapacitive Reservoir Computing System for Energy Efficient Temporal Data Processing. (arXiv:2305.12025v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1">Md Razuan Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1">Ahmed Salah Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Armendarez_N/0/1/0/all/0/1">Nicholas Xavier Armendarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Najem_J/0/1/0/all/0/1">Joseph S. Najem</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Md Sakib Hasan</a></p>
<p>Reservoir computing is a highly efficient machine learning framework for
processing temporal data by extracting features from the input signal and
mapping them into higher dimensional spaces. Physical reservoir layers have
been realized using spintronic oscillators, atomic switch networks, silicon
photonic modules, ferroelectric transistors, and volatile memristors. However,
these devices are intrinsically energy-dissipative due to their resistive
nature, which leads to increased power consumption. Therefore, capacitive
memory devices can provide a more energy-efficient approach. Here, we leverage
volatile biomembrane-based memcapacitors that closely mimic certain short-term
synaptic plasticity functions as reservoirs to solve classification tasks and
analyze time-series data in simulation and experimentally. Our system achieves
a 99.6% accuracy rate for spoken digit classification and a normalized mean
square error of 7.81*10^{-4} in a second-order non-linear regression task.
Furthermore, to showcase the device's real-time temporal data processing
capability, we achieve 100% accuracy for a real-time epilepsy detection problem
from an inputted electroencephalography (EEG) signal. Most importantly, we
demonstrate that each memcapacitor consumes an average of 41.5 fJ of energy per
spike, regardless of the selected input voltage pulse width, while maintaining
an average power of 415 fW for a pulse width of 100 ms. These values are orders
of magnitude lower than those achieved by state-of-the-art memristors used as
reservoirs. Lastly, we believe the biocompatible, soft nature of our
memcapacitor makes it highly suitable for computing and signal-processing
applications in biological environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14456">Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1">Tarek Naous</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryan_M/0/1/0/all/0/1">Michael J. Ryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1">Alan Ritter</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>It is important that language models appropriately adapt to specific cultural
contexts. However, as we show in this paper, multilingual and Arabic
monolingual language models default to Western culture even when prompted in
Arabic and contextualized by an Arab cultural setting. To measure this Western
bias, we introduce CAMeL, a dataset of naturally occurring Arabic prompts
spanning eight diverse cultural aspects and an extensive list of 20,504
cultural targets corresponding to Arab or Western culture. Using CAMeL, we show
that models favor Western targets and demonstrate cultural unfairness on
downstream tasks such as named entity recognition and sentiment analysis. Our
analyses of pretraining corpora also reveal that commonly used sources such as
Wikipedia may not be suited to build culturally aware models, underscoring the
importance of carefully curating pretraining data in constructing language
models to serve a global population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17718">FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions. (arXiv:2305.17718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rotstein_N/0/1/0/all/0/1">Noam Rotstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Bensaid_D/0/1/0/all/0/1">David Bensaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Brody_S/0/1/0/all/0/1">Shaked Brody</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1">Roy Ganz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1">Ron Kimmel</a></p>
<p>The advent of vision-language pre-training techniques enhanced substantial
progress in the development of models for image captioning. However, these
models frequently produce generic captions and may omit semantically important
image details. This limitation can be traced back to the image-text datasets;
while their captions typically offer a general description of image content,
they frequently omit salient details. Considering the magnitude of these
datasets, manual reannotation is impractical, emphasizing the need for an
automated approach. To address this challenge, we leverage existing captions
and explore augmenting them with visual details using "frozen" vision experts
including an object detector, an attribute recognizer, and an Optical Character
Recognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such
vision experts with the original captions using a large language model (LLM),
yielding comprehensive image descriptions. We automatically curate a training
set of 12M image-enriched caption pairs. These pairs undergo extensive
evaluation through both quantitative and qualitative analyses. Subsequently,
this data is utilized to train a captioning generation BLIP-based model. This
model outperforms current state-of-the-art approaches, producing more precise
and detailed descriptions, demonstrating the effectiveness of the proposed
data-centric approach. We release this large-scale dataset of enriched
image-caption pairs for the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18952">Exploring the Practicality of Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Soyoung Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chaeeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyunji Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Joel Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sohee Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minjoon Seo</a></p>
<p>Benchmarking the performance of information retrieval (IR) methods are mostly
conducted with a fixed set of documents (static corpora); in realistic
scenarios, this is rarely the case and the document to be retrieved are
constantly updated and added. In this paper, we focus on conducting a
comprehensive comparison between two categories of contemporary retrieval
systems, Dual Encoders (DE) and Generative Retrievals (GR), in a dynamic
scenario where the corpora to be retrieved is updated. We also conduct an
extensive evaluation of computational and memory efficiency, crucial factors
for IR systems for real-world deployment. Our results demonstrate that GR is
more adaptable to evolving knowledge (+13-18% on the StreamingQA Benchmark),
robust in handling data with temporal information (x 10 times), and efficient
in terms of memory (x 4 times), indexing time (x 6 times), and inference flops
(x 10 times). Our paper highlights GR's potential for future use in practical
IR systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00310">Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thudi_A/0/1/0/all/0/1">Anvith Thudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1">Hengrui Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Meehan_C/0/1/0/all/0/1">Casey Meehan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1">Ilia Shumailov</a>, <a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1">Nicolas Papernot</a></p>
<p>Differentially private stochastic gradient descent (DP-SGD) is the canonical
approach to private deep learning. While the current privacy analysis of DP-SGD
is known to be tight in some settings, several empirical results suggest that
models trained on common benchmark datasets leak significantly less privacy for
many datapoints. Yet, despite past attempts, a rigorous explanation for why
this is the case has not been reached. Is it because there exist tighter
privacy upper bounds when restricted to these dataset settings, or are our
attacks not strong enough for certain datapoints? In this paper, we provide the
first per-instance (i.e., ``data-dependent") DP analysis of DP-SGD. Our
analysis captures the intuition that points with similar neighbors in the
dataset enjoy better data-dependent privacy than outliers. Formally, this is
done by modifying the per-step privacy analysis of DP-SGD to introduce a
dependence on the distribution of model updates computed from a training
dataset. We further develop a new composition theorem to effectively use this
new per-step analysis to reason about an entire training run. Put all together,
our evaluation shows that this novel DP-SGD analysis allows us to now formally
show that DP-SGD leaks significantly less privacy for many datapoints (when
trained on common benchmarks) than the current data-independent guarantee. This
implies privacy attacks will necessarily fail against many datapoints if the
adversary does not have sufficient control over the possible training datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00002">An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wenzel_G/0/1/0/all/0/1">Georg Wenzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1">Adam Jatowt</a></p>
<p>Temporal commonsense reasoning refers to the ability to understand the
typical temporal context of phrases, actions, and events, and use it to reason
over problems requiring such knowledge. This trait is essential in temporal
natural language processing tasks, with possible applications such as timeline
summarization, temporal question answering, and temporal natural language
inference. Recent research on the performance of large language models suggests
that, although they are adept at generating syntactically correct sentences and
solving classification tasks, they often take shortcuts in their reasoning and
fall prey to simple linguistic traps. This article provides an overview of
research in the domain of temporal commonsense reasoning, particularly focusing
on enhancing language model performance through a variety of augmentations and
their evaluation across a growing number of datasets. However, these augmented
models still struggle to approach human performance on reasoning tasks over
temporal common sense properties, such as the typical occurrence times,
orderings, or durations of events. We further emphasize the need for careful
interpretation of research to guard against overpromising evaluation results in
light of the shallow reasoning present in transformers. This can be achieved by
appropriately preparing datasets and suitable evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00245">The Hitchhiker&#x27;s Guide to Program Analysis: A Journey with Large Language Models. (arXiv:2308.00245v3 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haonan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yu Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yizhuo Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1">Zhiyun Qian</a></p>
<p>Static analysis is a widely used technique in software engineering for
identifying and mitigating bugs. However, a significant hurdle lies in
achieving a delicate balance between precision and scalability. Large Language
Models (LLMs) offer a promising alternative, as recent advances demonstrate
remarkable capabilities in comprehending, generating, and even debugging code.
Yet, the logic of bugs can be complex and require sophisticated reasoning and a
large analysis scope spanning multiple functions. Therefore, at this point,
LLMs are better used in an assistive role to complement static analysis. In
this paper, we take a deep dive into the open space of LLM-assisted static
analysis, using use-before-initialization (UBI) bugs as a case study. To this
end, we develop LLift, a fully automated framework that interfaces with both a
static analysis tool and an LLM. By carefully designing the framework and the
prompts, we are able to overcome a number of challenges, including bug-specific
modeling, the large problem scope, the non-deterministic nature of LLMs, etc.
Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs
produced by static analysis, LLift demonstrates a potent capability, showcasing
a reasonable precision (50%) and appearing to have no missing bugs. It even
identified 13 previously unknown UBI bugs in the Linux kernel. This research
paves the way for new opportunities and methodologies in using LLMs for bug
discovery in extensive, real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16137">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wenhan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sinong Wang</a></p>
<p>In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex domains, they often face the
need to follow longer user prompts or generate longer texts. In these
situations, the $\textit{length generalization failure}$ of LLMs on long
sequences becomes more prominent. Most pre-training schemes truncate training
sequences to a fixed length. LLMs often struggle to generate fluent and
coherent texts after longer contexts, even with relative positional encoding
specifically designed to cope with this problem. Common solutions such as
finetuning on longer corpora often involve daunting hardware and time costs and
require careful training process design. To more efficiently extrapolate
existing LLMs' generation quality to longer texts, we theoretically and
empirically investigate the main out-of-distribution (OOD) factors contributing
to this problem. Inspired by this diagnosis, we propose a simple yet effective
solution for on-the-fly length generalization, LM-Infinite. It involves only a
$\mathbf{\Lambda}$-shaped attention mask (to avoid excessive attended tokens)
and a distance limit (to avoid unseen distances) while requiring no parameter
updates or learning. We find it applicable to a variety of LLMs using
relative-position encoding methods. LM-Infinite is computationally efficient
with $O(n)$ time and space, and demonstrates consistent text generation fluency
and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with
2.72x decoding speedup. We will make the codes publicly available following
publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16328">Debunking Disinformation: Revolutionizing Truth with NLP in Fake News Detection. (arXiv:2308.16328v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Li He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Siyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_A/0/1/0/all/0/1">Ailun Pei</a></p>
<p>The Internet and social media have altered how individuals access news in the
age of instantaneous information distribution. While this development has
increased access to information, it has also created a significant problem: the
spread of fake news and information. Fake news is rapidly spreading on digital
platforms, which has a negative impact on the media ecosystem, public opinion,
decision-making, and social cohesion. Natural Language Processing(NLP), which
offers a variety of approaches to identify content as authentic, has emerged as
a potent weapon in the growing war against disinformation. This paper takes an
in-depth look at how NLP technology can be used to detect fake news and reveals
the challenges and opportunities it presents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11722">Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical Foundation Model: A Computational Analysis. (arXiv:2310.11722v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yaxin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Feng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>Foundation Models (FMs) have the potential to revolutionize the way users
self-diagnose through search engines by offering direct and efficient
suggestions. Recent studies primarily focused on the quality of FMs evaluated
by GPT-4 or their ability to pass medical exams, no studies have quantified the
extent of self-diagnostic atomic knowledge stored in FMs' memory, which is the
basis of foundation models to provide factual and reliable suggestions. In this
paper, we first constructed a benchmark of Self-diagnostic Atomic Knowledge
(SdAK), including the most common types of atomic knowledge involved in
self-diagnostic queries, with 17 atomic types and a total of 14, 048 pieces of
atomic knowledge. Then, we evaluated both generic and open-source Chinese
medical FMs on the benchmark. The experimental results showcase that generic
FMs perform better than medical FMs in terms of self-diagnostic atomic
knowledge. Error analysis revealed that both generic and medical FMs are
sycophantic, e.g., always catering to users' claims when it comes to unknown
knowledge. We further explored different types of data commonly adopted for
fine-tuning medical FMs, i.e., real-world, semi-distilled, and distilled data,
and found that distilled data can benefit FMs most. The code and data are
available at \url{https://github.com/FreedomIntelligence/SDAK}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13007">A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deck_L/0/1/0/all/0/1">Luca Deck</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1">Jakob Schoeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1">Maria De-Arteaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a></p>
<p>In this critical survey, we analyze typical claims on the relationship
between explainable AI (XAI) and fairness to disentangle the multidimensional
relationship between these two concepts. Based on a systematic literature
review and a subsequent qualitative content analysis, we identify seven
archetypal claims from 175 papers on the alleged fairness benefits of XAI. We
present crucial caveats with respect to these claims and provide an entry point
for future discussions around the potentials and limitations of XAI for
specific fairness desiderata. While the literature often suggests XAI to be an
enabler for several fairness desiderata, we notice a divide between these
desiderata and the capabilities of XAI. We encourage to conceive XAI as one of
many tools to approach the multidimensional, sociotechnical challenge of
algorithmic fairness and to be more specific about how exactly what kind of XAI
method enables whom to address which fairness desideratum.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16776">DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Devleena Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1">Vivek Khetan</a></p>
<p>Recent advances have led to the availability of many pre-trained language
models (PLMs); however, a question that remains is how much data is truly
needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,
a data-efficient fine-tuning framework that leverages unsupervised core-set
selection to minimize the amount of data needed to fine-tune PLMs for
downstream tasks. We demonstrate the efficacy of our DEFT framework in the
context of text-editing LMs, and compare to the state-of-the art text-editing
model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT
models are just as accurate as CoEDIT while being finetuned on ~70% less data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00619">Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jinadu_U/0/1/0/all/0/1">Uthman Jinadu</a>, <a href="http://arxiv.org/find/cs/1/au:+Annan_J/0/1/0/all/0/1">Jesse Annan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1">Shanshan Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yi Ding</a></p>
<p>Accounting for the opinions of all annotators of a dataset is critical for
fairness. However, when annotating large datasets, individual annotators will
frequently provide thousands of ratings which can lead to fatigue.
Additionally, these annotation processes can occur over multiple days which can
lead to an inaccurate representation of an annotator's opinion over time. To
combat this, we propose to learn a more accurate representation of diverse
opinions by utilizing multitask learning in conjunction with loss-based label
correction. We show that using our novel formulation, we can cleanly separate
agreeing and disagreeing annotations. Furthermore, we demonstrate that this
modification can improve prediction performance in a single or multi-annotator
setting. Lastly, we show that this method remains robust to additional label
noise that is applied to subjective data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02782">Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead. (arXiv:2311.02782v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yunkang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaonan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weiming Shen</a></p>
<p>Anomaly detection is a crucial task across different domains and data types.
However, existing anomaly detection models are often designed for specific
domains and modalities. This study explores the use of GPT-4V(ision), a
powerful visual-linguistic model, to address anomaly detection tasks in a
generic manner. We investigate the application of GPT-4V in multi-modality,
multi-domain anomaly detection tasks, including image, video, point cloud, and
time series data, across multiple application areas, such as industrial,
medical, logical, video, 3D anomaly detection, and localization tasks. To
enhance GPT-4V's performance, we incorporate different kinds of additional cues
such as class information, human expertise, and reference images as
prompts.Based on our experiments, GPT-4V proves to be highly effective in
detecting and explaining global and fine-grained semantic patterns in
zero/one-shot anomaly detection. This enables accurate differentiation between
normal and abnormal instances. Although we conducted extensive evaluations in
this study, there is still room for future evaluation to further exploit
GPT-4V's generic anomaly detection capacity from different aspects. These
include exploring quantitative metrics, expanding evaluation benchmarks,
incorporating multi-round interactions, and incorporating human feedback loops.
Nevertheless, GPT-4V exhibits promising performance in generic anomaly
detection and understanding, thus opening up a new avenue for anomaly
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02802">Incorporating Worker Perspectives into MTurk Annotation Practices for NLP. (arXiv:2311.02802v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_O/0/1/0/all/0/1">Olivia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>Current practices regarding data collection for natural language processing
on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on
data quality and heuristics shared among NLP researchers. However, without
considering the perspectives of MTurk workers, these approaches are susceptible
to issues regarding workers' rights and poor response quality. We conducted a
critical literature review and a survey of MTurk workers aimed at addressing
open questions regarding best practices for fair payment, worker privacy, data
quality, and considering worker incentives. We found that worker preferences
are often at odds with received wisdom among NLP researchers. Surveyed workers
preferred reliable, reasonable payments over uncertain, very high payments;
reported frequently lying on demographic questions; and expressed frustration
at having work rejected with no explanation. We also found that workers view
some quality control methods, such as requiring minimum response times or
Master's qualifications, as biased and largely ineffective. Based on the survey
results, we provide recommendations on how future NLP studies may better
account for MTurk workers' experiences in order to respect workers' rights and
improve data quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03220">ALYMPICS: Language Agents Meet Game Theory. (arXiv:2311.03220v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuzhe Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenshan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>This paper introduces Alympics, a platform that leverages Large Language
Model (LLM) agents to facilitate investigations in game theory. By employing
LLMs and autonomous agents to simulate human behavior and enable multi-agent
collaborations, we can construct realistic and dynamic models of human
interactions for game theory hypothesis formulating and testing. To demonstrate
this, we present and implement a survival game involving unequal competition
for limited resources. Through manipulation of resource availability and agent
personalities, we observe how different agents engage in the competition and
adapt their strategies. The use of LLM agents in game theory research offers
significant advantages, including simulating realistic behavior, providing a
controlled, scalable, and reproducible environment. Our work highlights the
potential of LLM agents in enhancing the understanding of strategic
decision-making within complex socioeconomic contexts. All codes are available
at https://github.com/microsoft/Alympics
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05371">Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data. (arXiv:2311.05371v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1">Yuxuan Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuntang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shwetak Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+McDuf_D/0/1/0/all/0/1">Daniel McDuf</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuzhe Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a></p>
<p>Recent advances in supervised deep learning techniques have demonstrated the
possibility to remotely measure human physiological vital signs (e.g.,
photoplethysmograph, heart rate) just from facial videos. However, the
performance of these methods heavily relies on the availability and diversity
of real labeled data. Yet, collecting large-scale real-world data with
high-quality labels is typically challenging and resource intensive, which also
raises privacy concerns when storing personal bio-metric data. Synthetic
video-based datasets (e.g., SCAMPS \cite{mcduff2022scamps}) with
photo-realistic synthesized avatars are introduced to alleviate the issues
while providing high-quality synthetic data. However, there exists a
significant gap between synthetic and real-world data, which hinders the
generalization of neural models trained on these synthetic datasets. In this
paper, we proposed several measures to add real-world noise to synthetic
physiological signals and corresponding facial videos. We experimented with
individual and combined augmentation methods and evaluated our framework on
three public real-world datasets. Our results show that we were able to reduce
the average MAE from 6.9 to 2.0.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06330">Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations. (arXiv:2311.06330v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zengqing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1">Run Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a></p>
<p>Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM's strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM's
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06694">Comparative Multi-View Language Grounding. (arXiv:2311.06694v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1">Chancharik Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Abrar Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Corona_R/0/1/0/all/0/1">Rodolfo Corona</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>In this work, we consider the task of resolving object referents when given a
comparative language description. We present a Multi-view Approach to Grounding
in Context (MAGiC) that leverages transformers to pragmatically reason over
both objects given multiple image views and a language description. In contrast
to past efforts that attempt to connect vision and language for this task
without fully considering the resulting referential context, MAGiC makes use of
the comparative information by jointly reasoning over multiple views of both
object referent candidates and the referring language expression. We present an
analysis demonstrating that comparative reasoning contributes to SOTA
performance on the SNARE object reference task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07468">Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse. (arXiv:2311.07468v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_A/0/1/0/all/0/1">Ang Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shufang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Q/0/1/0/all/0/1">Quan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Recent studies have highlighted a phenomenon in large language models (LLMs)
known as "the reversal curse," in which the order of knowledge entities in the
training data biases the models' comprehension. For example, if a model is
trained on sentences where entity A consistently appears before entity B, it
can respond to queries about A by providing B as the answer. However, it may
encounter confusion when presented with questions concerning B. We contend that
the reversal curse is partially a result of specific model training objectives,
particularly evident in the prevalent use of the next-token prediction within
most causal language models. For the next-token prediction, models solely focus
on a token's preceding context, resulting in a restricted comprehension of the
input. In contrast, we illustrate that the GLM, trained using the
autoregressive blank infilling objective where tokens to be predicted have
access to the entire context, exhibits better resilience against the reversal
curse. We propose a novel training method, BIdirectional Casual language
modeling Optimization (BICO), designed to mitigate the reversal curse when
fine-tuning pretrained causal language models on new data. BICO modifies the
causal attention mechanism to function bidirectionally and employs a mask
denoising optimization. In the task designed to assess the reversal curse, our
approach improves Llama's accuracy from the original 0% to around 70%. We hope
that more attention can be focused on exploring and addressing these inherent
weaknesses of the current LLMs, in order to achieve a higher level of
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07469">InCA: Rethinking In-Car Conversational System Assessment Leveraging Large Language Models. (arXiv:2311.07469v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Friedl_K/0/1/0/all/0/1">Ken E. Friedl</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Abbas Goher Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1">Soumya Ranjan Sahoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rony_M/0/1/0/all/0/1">Md Rashad Al Hasan Rony</a>, <a href="http://arxiv.org/find/cs/1/au:+Germies_J/0/1/0/all/0/1">Jana Germies</a>, <a href="http://arxiv.org/find/cs/1/au:+Suss_C/0/1/0/all/0/1">Christian S&#xfc;&#xdf;</a></p>
<p>The assessment of advanced generative large language models (LLMs) poses a
significant challenge, given their heightened complexity in recent
developments. Furthermore, evaluating the performance of LLM-based applications
in various industries, as indicated by Key Performance Indicators (KPIs), is a
complex undertaking. This task necessitates a profound understanding of
industry use cases and the anticipated system behavior. Within the context of
the automotive industry, existing evaluation metrics prove inadequate for
assessing in-car conversational question answering (ConvQA) systems. The unique
demands of these systems, where answers may relate to driver or car safety and
are confined within the car domain, highlight the limitations of current
metrics. To address these challenges, this paper introduces a set of KPIs
tailored for evaluating the performance of in-car ConvQA systems, along with
datasets specifically designed for these KPIs. A preliminary and comprehensive
empirical evaluation substantiates the efficacy of our proposed approach.
Furthermore, we investigate the impact of employing varied personas in prompts
and found that it enhances the model's capacity to simulate diverse viewpoints
in assessments, mirroring how individuals with different backgrounds perceive a
topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07587">Frontier Language Models are not Robust to Adversarial Arithmetic, or &quot;What do I need to say so you agree 2+2=5?. (arXiv:2311.07587v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_C/0/1/0/all/0/1">C. Daniel Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1">Laura Culp</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1">Maxwell L Bileschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1">Gamaleldin F Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1">Alex Rizkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1">Isabelle Simpson</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alex Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1">Ben Adlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gaurav Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1">Izzeddin Gur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1">JD Co-Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1">Jeffrey Pennington</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1">Kevin Swersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kshiteej Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lechao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1">Noah Constant</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1">Roman Novak</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yundi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1">Noah Fiedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a></p>
<p>We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name "prompt inversion rejection sampling" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07870">AutoML for Large Capacity Modeling of Meta&#x27;s Ranking Systems. (arXiv:2311.07870v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kuang-Hung Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Mengying Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Buyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sehgal_V/0/1/0/all/0/1">Vivek Sehgal</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchal_R/0/1/0/all/0/1">Rudresh Rajnikant Panchal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hotaj_E/0/1/0/all/0/1">Eugen Hotaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daifeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jamey Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shali Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huayu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhengxing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiyan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wei Wen</a></p>
<p>Web-scale ranking systems at Meta serving billions of users is complex.
Improving ranking models is essential but engineering heavy. Automated Machine
Learning (AutoML) can release engineers from labor intensive work of tuning
ranking models; however, it is unknown if AutoML is efficient enough to meet
tight production timeline in real-world and, at the same time, bring additional
improvements to the strong baselines. Moreover, to achieve higher ranking
performance, there is an ever-increasing demand to scale up ranking models to
even larger capacity, which imposes more challenges on the efficiency. The
large scale of models and tight production schedule requires AutoML to
outperform human baselines by only using a small number of model evaluation
trials (around 100). We presents a sampling-based AutoML method, focusing on
neural architecture search and hyperparameter optimization, addressing these
challenges in Meta-scale production when building large capacity models. Our
approach efficiently handles large-scale data demands. It leverages a
lightweight predictor-based searcher and reinforcement learning to explore vast
search spaces, significantly reducing the number of model evaluations. Through
experiments in large capacity modeling for CTR and CVR applications, we show
that our method achieves outstanding Return on Investment (ROI) versus human
tuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or
$25\%$ Query per Second (QPS) increase by only sampling one hundred models on
average from a curated search space. The proposed AutoML method has already
made real-world impact where a discovered Instagram CTR model with up to -0.36%
NE gain (over existing production baseline) was selected for large-scale online
A/B test and show statistically significant gain. These production results
proved AutoML efficacy and accelerated its adoption in ranking systems at Meta.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08398">Are Large Language Models Temporally Grounded?. (arXiv:2311.08398v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yifu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1">Yftah Ziser</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1">Edoardo M. Ponti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a></p>
<p>Are Large language models (LLMs) temporally grounded? Since LLMs cannot
perceive and interact with the environment, it is impossible to answer this
question directly. Instead, we provide LLMs with textual narratives and probe
them with respect to their common-sense knowledge of the structure and duration
of events, their ability to order events along a timeline, and self-consistency
within their temporal model (e.g., temporal relations such as after and before
are mutually exclusive for any pair of events). We evaluate state-of-the-art
LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.
Generally, we find that LLMs lag significantly behind both human performance as
well as small-scale, specialised LMs. In-context learning, instruction tuning,
and chain-of-thought prompting reduce this gap only to a limited degree.
Crucially, LLMs struggle the most with self-consistency, displaying incoherent
behaviour in at least 27.23% of their predictions. Contrary to expectations, we
also find that scaling the model size does not guarantee positive gains in
performance. To explain these results, we study the sources from which LLMs may
gather temporal information: we find that sentence ordering in unlabelled
texts, available during pre-training, is only weakly correlated with event
ordering. Moreover, public instruction tuning mixtures contain few temporal
tasks. Hence, we conclude that current LLMs lack a consistent temporal model of
textual narratives. Code, datasets, and LLM outputs are available at
https://github.com/yfqiu-nlp/temporal-llms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09174">AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph. (arXiv:2311.09174v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Haochen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Tianqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sehyun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Cognitive research indicates that abstraction ability is essential in human
intelligence, which remains under-explored in language models. In this paper,
we present AbsPyramid, a unified entailment graph of 221K textual descriptions
of abstraction knowledge. While existing resources only touch nouns or verbs
within simplified events or specific domains, AbsPyramid collects abstract
knowledge for three components of diverse events to comprehensively evaluate
the abstraction ability of language models in the open domain. Experimental
results demonstrate that current LLMs face challenges comprehending abstraction
knowledge in zero-shot and few-shot settings. By training on our rich
abstraction knowledge, we find LLMs can acquire basic abstraction abilities and
generalize to unseen events. In the meantime, we empirically show that our
benchmark is comprehensive to enhance LLMs across two previous abstraction
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.10242">Ulixes: Facial Recognition Privacy with Adversarial Machine Learning. (arXiv:2010.10242v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cilloni_T/0/1/0/all/0/1">Thomas Cilloni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_C/0/1/0/all/0/1">Charles Walter</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_C/0/1/0/all/0/1">Charles Fleming</a></p>
<p>Facial recognition tools are becoming exceptionally accurate in identifying
people from images. However, this comes at the cost of privacy for users of
online services with photo management (e.g. social media platforms).
Particularly troubling is the ability to leverage unsupervised learning to
recognize faces even when the user has not labeled their images. In this paper
we propose Ulixes, a strategy to generate visually non-invasive facial noise
masks that yield adversarial examples, preventing the formation of identifiable
user clusters in the embedding space of facial encoders. This is applicable
even when a user is unmasked and labeled images are available online. We
demonstrate the effectiveness of Ulixes by showing that various classification
and clustering methods cannot reliably label the adversarial examples we
generate. We also study the effects of Ulixes in various black-box settings and
compare it to the current state of the art in adversarial machine learning.
Finally, we challenge the effectiveness of Ulixes against adversarially trained
models and show that it is robust to countermeasures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00282">FedScore: A privacy-preserving framework for federated scoring system development. (arXiv:2303.00282v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1">Yilin Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1">Marcus Eng Hock Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1">Bibhas Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1">Chuan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1">Feng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Han Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingxuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckland_D/0/1/0/all/0/1">Daniel M. Buckland</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nan Liu</a></p>
<p>We propose FedScore, a privacy-preserving federated learning framework for
scoring system generation across multiple sites to facilitate
cross-institutional collaborations. The FedScore framework includes five
modules: federated variable ranking, federated variable transformation,
federated score derivation, federated model selection and federated model
evaluation. To illustrate usage and assess FedScore's performance, we built a
hypothetical global scoring system for mortality prediction within 30 days
after a visit to an emergency department using 10 simulated sites divided from
a tertiary hospital in Singapore. We employed a pre-existing score generator to
construct 10 local scoring systems independently at each site and we also
developed a scoring system using centralized data for comparison. We compared
the acquired FedScore model's performance with that of other scoring models
using the receiver operating characteristic (ROC) analysis. The FedScore model
achieved an average area under the curve (AUC) value of 0.763 across all sites,
with a standard deviation (SD) of 0.020. We also calculated the average AUC
values and SDs for each local model, and the FedScore model showed promising
accuracy and stability with a high average AUC value which was closest to the
one of the pooled model and SD which was lower than that of most local models.
This study demonstrates that FedScore is a privacy-preserving scoring system
generator with potentially good generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05853">Reframing Audience Expansion through the Lens of Probability Density Estimation. (arXiv:2311.05853v1 [cs.AI] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carvalhaes_C/0/1/0/all/0/1">Claudio Carvalhaes</a></p>
<p>Audience expansion has become an important element of prospective marketing,
helping marketers create target audiences based on a mere representative sample
of their current customer base. Within the realm of machine learning, a favored
algorithm for scaling this sample into a broader audience hinges on a binary
classification task, with class probability estimates playing a crucial role.
In this paper, we review this technique and introduce a key change in how we
choose training examples to ensure the quality of the generated audience. We
present a simulation study based on the widely used MNIST dataset, where
consistent high precision and recall values demonstrate our approach's ability
to identify the most relevant users for an expanded audience. Our results are
easily reproducible and a Python implementation is openly available on GitHub:
\url{https://github.com/carvalhaes-ai/audience-expansion}
</p>
</p>
</div>

    </div>
    </body>
    