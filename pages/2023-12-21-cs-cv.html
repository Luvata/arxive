<!DOCTYPE html>
<html>
<head>
<title>2023-12-21-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.11467">Glioblastoma Tumor Segmentation using an Ensemble of Vision Transformers. (arXiv:2312.11467v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1">Huafeng Liu</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Dowdell_B/0/1/0/all/0/1">Benjamin Dowdell</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Engelder_T/0/1/0/all/0/1">Todd Engelder</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Pulmano_Z/0/1/0/all/0/1">Zarah Pulmano</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Osa_N/0/1/0/all/0/1">Nicolas Osa</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Barman_A/0/1/0/all/0/1">Arko Barman</a> (1) ((1) Rice University)</p>
<p>Glioblastoma is one of the most aggressive and deadliest types of brain
cancer, with low survival rates compared to other types of cancer. Analysis of
Magnetic Resonance Imaging (MRI) scans is one of the most effective methods for
the diagnosis and treatment of brain cancers such as glioblastoma. Accurate
tumor segmentation in MRI images is often required for treatment planning and
risk assessment of treatment methods. Here, we propose a novel pipeline, Brain
Radiology Aided by Intelligent Neural NETworks (BRAINNET), which leverages
MaskFormer, a vision transformer model, and generates robust tumor segmentation
maks. We use an ensemble of nine predictions from three models separately
trained on each of the three orthogonal 2D slice directions (axial, sagittal,
and coronal) of a 3D brain MRI volume. We train and test our models on the
publicly available UPenn-GBM dataset, consisting of 3D multi-parametric MRI
(mpMRI) scans from 611 subjects. Using Dice coefficient (DC) and 95% Hausdorff
distance (HD) for evaluation, our models achieved state-of-the-art results in
segmenting all three different tumor regions -- tumor core (DC = 0.894, HD =
2.308), whole tumor (DC = 0.891, HD = 3.552), and enhancing tumor (DC = 0.812,
HD = 1.608).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11468">Unbiased Neural Networks for Parameter Estimation in Quantitative MRI. (arXiv:2312.11468v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Mao_A/0/1/0/all/0/1">Andrew Mao</a>, <a href="http://arxiv.org/find/physics/1/au:+Flassbeck_S/0/1/0/all/0/1">Sebastian Flassbeck</a>, <a href="http://arxiv.org/find/physics/1/au:+Asslander_J/0/1/0/all/0/1">Jakob Assl&#xe4;nder</a></p>
<p>Purpose: To develop neural-network (NN)-based quantitative MRI parameter
estimators with minimal bias and a variance close to the theoretical minimum,
the Cram\'er-Rao bound.
</p>
<p>Theory and Methods: We explicitly penalize the bias of the NN's estimates
during training, which involves averaging over multiple noise realizations of
the same measurements. Bias and variance properties of the resulting NNs are
studied for two quantitative neuroimaging applications.
</p>
<p>Results: In simulation, the proposed strategy reduces the estimates' bias
throughout parameter space and achieves a variance close to the Cram\'er-Rao
bound. In vivo, we observe good concordance between parameter maps estimated
with the proposed NNs and traditional estimators, such as non-linear
least-squares fitting, while state-of-the-art NNs show larger deviations.
</p>
<p>Conclusion: NNs trained with the proposed strategy are approximately minimum
variance unbiased estimators and offer significantly improved computational
efficiency over traditional estimators with comparable or better accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11470">Anomaly detection for automated inspection of power line insulators. (arXiv:2312.11470v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_L/0/1/0/all/0/1">Laya Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Gjorgiev_B/0/1/0/all/0/1">Blazhe Gjorgiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Sansavini_G/0/1/0/all/0/1">Giovanni Sansavini</a></p>
<p>Inspection of insulators is important to ensure reliable operation of the
power system. Deep learning has recently been explored to automate the
inspection process by leveraging aerial images captured by drones along with
powerful object detection models. However, a purely object detection-based
approach exhibits class imbalance-induced poor detection accuracy for faulty
insulators, especially for incipient faults. In order to address this issue in
a data-efficient manner, this article proposes a two-stage approach that
leverages object detection in conjunction with anomaly detection to reliably
detect faults in insulators. The article adopts an explainable deep neural
network-based one-class classifier for anomaly detection, that reduces the
reliance on plentifully available images of faulty insulators, that might be
difficult to obtain in real-life applications. The anomaly detection model is
trained with two datasets -- representing data abundant and data scarce
scenarios -- in unsupervised and semi-supervised manner. The results suggest
that including as few as six real anomalies in the training dataset
significantly improves the performance of the model, and enables reliable
detection of rarely occurring faults in insulators. An analysis of the
explanations provided by the anomaly detection model reveals that the model is
able to accurately identify faulty regions on the insulator disks, while also
exhibiting some false predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11473">Synthetic Shifts to Initial Seed Vector Exposes the Brittle Nature of Latent-Based Diffusion Models. (arXiv:2312.11473v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Po_Yuan_M/0/1/0/all/0/1">Mao Po-Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Foong_T/0/1/0/all/0/1">Tham Yik Foong</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a></p>
<p>Recent advances in Conditional Diffusion Models have led to substantial
capabilities in various domains. However, understanding the impact of
variations in the initial seed vector remains an underexplored area of concern.
Particularly, latent-based diffusion models display inconsistencies in image
generation under standard conditions when initialized with suboptimal initial
seed vectors. To understand the impact of the initial seed vector on generated
samples, we propose a reliability evaluation framework that evaluates the
generated samples of a diffusion model when the initial seed vector is
subjected to various synthetic shifts. Our results indicate that slight
manipulations to the initial seed vector of the state-of-the-art Stable
Diffusion (Rombach et al., 2022) can lead to significant disturbances in the
generated samples, consequently creating images without the effect of
conditioning variables. In contrast, GLIDE (Nichol et al., 2022) stands out in
generating reliable samples even when the initial seed vector is transformed.
Thus, our study sheds light on the importance of the selection and the impact
of the initial seed vector in the latent-based diffusion model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11480">Adaptive Smooth Activation for Improved Disease Diagnosis and Organ Segmentation from Radiology Scans. (arXiv:2312.11480v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1">Koushik Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1">Debesh Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomar_N/0/1/0/all/0/1">Nikhil Kumar Tomar</a>, <a href="http://arxiv.org/find/cs/1/au:+Durak_G/0/1/0/all/0/1">Gorkem Durak</a>, <a href="http://arxiv.org/find/cs/1/au:+Medetalibeyoglu_A/0/1/0/all/0/1">Alpay Medetalibeyoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Antalek_M/0/1/0/all/0/1">Matthew Antalek</a>, <a href="http://arxiv.org/find/cs/1/au:+Velichko_Y/0/1/0/all/0/1">Yury Velichko</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladner_D/0/1/0/all/0/1">Daniela Ladner</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohrani_A/0/1/0/all/0/1">Amir Bohrani</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1">Ulas Bagci</a></p>
<p>In this study, we propose a new activation function, called Adaptive Smooth
Activation Unit (ASAU), tailored for optimized gradient propagation, thereby
enhancing the proficiency of convolutional networks in medical image analysis.
We apply this new activation function to two important and commonly used
general tasks in medical image analysis: automatic disease diagnosis and organ
segmentation in CT and MRI. Our rigorous evaluation on the RadImageNet
abdominal/pelvis (CT and MRI) dataset and Liver Tumor Segmentation Benchmark
(LiTS) 2017 demonstrates that our ASAU-integrated frameworks not only achieve a
substantial (4.80\%) improvement over ReLU in classification accuracy (disease
detection) on abdominal CT and MRI but also achieves 1\%-3\% improvement in
dice coefficient compared to widely used activations for `healthy liver tissue'
segmentation. These improvements offer new baselines for developing a
diagnostic tool, particularly for complex, challenging pathologies. The
superior performance and adaptability of ASAU highlight its potential for
integration into a wide range of image classification and segmentation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11524">Assessing GPT4-V on Structured Reasoning Tasks. (arXiv:2312.11524v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mukul Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1">Jos&#xe9; Cambronero</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1">Sumit Gulwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1">Vu Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1">Gust Verbruggen</a></p>
<p>Multi-modality promises to unlock further uses for large language models.
Recently, the state-of-the-art language model GPT-4 was enhanced with vision
capabilities. We carry out a prompting evaluation of GPT-4V and five other
baselines on structured reasoning tasks, such as mathematical reasoning, visual
data analysis, and code generation. We show that visual Chain-of-Thought, an
extension of Chain-of-Thought to multi-modal LLMs, yields significant
improvements over the vanilla model. We also present a categorized analysis of
scenarios where these models perform well and where they struggle, highlighting
challenges associated with coherent multimodal reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11535">Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior. (arXiv:2312.11535v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1">Nan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Ting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuhui Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a></p>
<p>In this paper, we present a novel two-stage approach that fully utilizes the
information provided by the reference image to establish a customized knowledge
prior for image-to-3D generation. While previous approaches primarily rely on a
general diffusion prior, which struggles to yield consistent results with the
reference image, we propose a subject-specific and multi-modal diffusion model.
This model not only aids NeRF optimization by considering the shading mode for
improved geometry but also enhances texture from the coarse results to achieve
superior refinement. Both aspects contribute to faithfully aligning the 3D
content with the subject. Extensive experiments showcase the superiority of our
method, Customize-It-3D, outperforming previous works by a substantial margin.
It produces faithful 360-degree reconstructions with impressive visual quality,
making it well-suited for various applications, including text-to-3D creation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11537">FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline. (arXiv:2312.11537v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chien-Yu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qichen Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Merth_T/0/1/0/all/0/1">Thomas Merth</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Karren Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1">Anurag Ranjan</a></p>
<p>Super-resolution (SR) techniques have recently been proposed to upscale the
outputs of neural radiance fields (NeRF) and generate high-quality images with
enhanced inference speeds. However, existing NeRF+SR methods increase training
overhead by using extra input features, loss functions, and/or expensive
training procedures such as knowledge distillation. In this paper, we aim to
leverage SR for efficiency gains without costly training or architectural
changes. Specifically, we build a simple NeRF+SR pipeline that directly
combines existing modules, and we propose a lightweight augmentation technique,
random patch sampling, for training. Compared to existing NeRF+SR methods, our
pipeline mitigates the SR computing overhead and can be trained up to 23x
faster, making it feasible to run on consumer devices such as the Apple
MacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4x while
maintaining high quality, increasing inference speeds by up to 18x on an NVIDIA
V100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but
effective technique for improving the efficiency of NeRF models for consumer
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11538">Iterative Motion Editing with Natural Language. (arXiv:2312.11538v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1">Purvi Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kuan-Chieh Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">C. Karen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1">Kayvon Fatahalian</a></p>
<p>Text-to-motion diffusion models can generate realistic animations from text
prompts, but do not support fine-grained motion editing controls. In this paper
we present a method for using natural language to iteratively specify local
edits to existing character animations, a task that is common in most computer
animation workflows. Our key idea is to represent a space of motion edits using
a set of kinematic motion operators that have well-defined semantics for how to
modify specific frames of a target motion. We provide an algorithm that
leverages pre-existing language models to translate textual descriptions of
motion edits to sequences of motion editing operators (MEOs). Given new
keyframes produced by the MEOs, we use diffusion-based keyframe interpolation
to generate final motions. Through a user study and quantitative evaluation, we
demonstrate that our system can perform motion edits that respect the
animator's editing intent, remain faithful to the original animation (they edit
the original animation, not dramatically change it), and yield realistic
character animation results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11542">FER-C: Benchmarking Out-of-Distribution Soft Calibration for Facial Expression Recognition. (arXiv:2312.11542v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neo_D/0/1/0/all/0/1">Dexter Neo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tsuhan Chen</a></p>
<p>We present a soft benchmark for calibrating facial expression recognition
(FER). While prior works have focused on identifying affective states, we find
that FER models are uncalibrated. This is particularly true when
out-of-distribution (OOD) shifts further exacerbate the ambiguity of facial
expressions. While most OOD benchmarks provide hard labels, we argue that the
ground-truth labels for evaluating FER models should be soft in order to better
reflect the ambiguity behind facial behaviours. Our framework proposes soft
labels that closely approximates the average information loss based on
different types of OOD shifts. Finally, we show the benefits of calibration on
five state-of-the-art FER algorithms tested on our benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11548">Learning Interpretable Queries for Explainable Image Classification with Information Pursuit. (arXiv:2312.11548v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kolek_S/0/1/0/all/0/1">Stefan Kolek</a>, <a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1">Aditya Chattopadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrade_Loarca_H/0/1/0/all/0/1">Hector Andrade-Loarca</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1">Gitta Kutyniok</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1">R&#xe9;ne Vidal</a></p>
<p>Information Pursuit (IP) is an explainable prediction algorithm that greedily
selects a sequence of interpretable queries about the data in order of
information gain, updating its posterior at each step based on observed
query-answer pairs. The standard paradigm uses hand-crafted dictionaries of
potential data queries curated by a domain expert or a large language model
after a human prompt. However, in practice, hand-crafted dictionaries are
limited by the expertise of the curator and the heuristics of prompt
engineering. This paper introduces a novel approach: learning a dictionary of
interpretable queries directly from the dataset. Our query dictionary learning
problem is formulated as an optimization problem by augmenting IP's variational
formulation with learnable dictionary parameters. To formulate learnable and
interpretable queries, we leverage the latent space of large vision and
language models like CLIP. To solve the optimization problem, we propose a new
query dictionary learning algorithm inspired by classical sparse dictionary
learning. Our experiments demonstrate that learned dictionaries significantly
outperform hand-crafted dictionaries generated with large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11555">CR-SFP: Learning Consistent Representation for Soft Filter Pruning. (arXiv:2312.11555v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1">Jingyang Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuangzhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jianbiao Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>Soft filter pruning~(SFP) has emerged as an effective pruning technique for
allowing pruned filters to update and the opportunity for them to regrow to the
network. However, this pruning strategy applies training and pruning in an
alternative manner, which inevitably causes inconsistent representations
between the reconstructed network~(R-NN) at the training and the pruned
network~(P-NN) at the inference, resulting in performance degradation. In this
paper, we propose to mitigate this gap by learning consistent representation
for soft filter pruning, dubbed as CR-SFP. Specifically, for each training
step, CR-SFP optimizes the R-NN and P-NN simultaneously with different
distorted versions of the same training data, while forcing them to be
consistent by minimizing their posterior distribution via the bidirectional
KL-divergence loss. Meanwhile, the R-NN and P-NN share backbone parameters thus
only additional classifier parameters are introduced. After training, we can
export the P-NN for inference. CR-SFP is a simple yet effective training
framework to improve the accuracy of P-NN without introducing any additional
inference cost. It can also be combined with a variety of pruning criteria and
loss functions. Extensive experiments demonstrate our CR-SFP achieves
consistent improvements across various CNN architectures. Notably, on ImageNet,
our CR-SFP reduces more than 41.8\% FLOPs on ResNet18 with 69.2\% top-1
accuracy, improving SFP by 2.1\% under the same training settings. The code
will be publicly available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11556">StarVector: Generating Scalable Vector Graphics Code from Images. (arXiv:2312.11556v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1">Juan A. Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Shubham Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1">Issam H. Laradji</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pau Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1">David Vazquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1">Christopher Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1">Marco Pedersoli</a></p>
<p>Scalable Vector Graphics (SVGs) have become integral in modern image
rendering applications due to their infinite scalability in resolution,
versatile usability, and editing capabilities. SVGs are particularly popular in
the fields of web development and graphic design. Existing approaches for SVG
modeling using deep learning often struggle with generating complex SVGs and
are restricted to simpler ones that require extensive processing and
simplification. This paper introduces StarVector, a multimodal SVG generation
model that effectively integrates Code Generation Large Language Models
(CodeLLMs) and vision models. Our approach utilizes a CLIP image encoder to
extract visual representations from pixel-based images, which are then
transformed into visual tokens via an adapter module. These visual tokens are
pre-pended to the SVG token embeddings, and the sequence is modeled by the
StarCoder model using next-token prediction, effectively learning to align the
visual and code tokens. This enables StarVector to generate unrestricted SVGs
that accurately represent pixel images. To evaluate StarVector's performance,
we present SVG-Bench, a comprehensive benchmark for evaluating SVG methods
across multiple datasets and relevant metrics. Within this benchmark, we
introduce novel datasets including SVG-Stack, a large-scale dataset of
real-world SVG examples, and use it to pre-train StarVector as a large
foundation model for SVGs. Our results demonstrate significant enhancements in
visual quality and complexity handling over current methods, marking a notable
advancement in SVG generation technology. Code and models:
https://github.com/joanrod/star-vector
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11557">SAI3D: Segment Any Instance in 3D Scenes. (arXiv:2312.11557v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yingda Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuzheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yang Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jingwei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Baoquan Chen</a></p>
<p>Advancements in 3D instance segmentation have traditionally been tethered to
the availability of annotated datasets, limiting their application to a narrow
spectrum of object categories. Recent efforts have sought to harness
vision-language models like CLIP for open-set semantic reasoning, yet these
methods struggle to distinguish between objects of the same categories and rely
on specific prompts that are not universally applicable. In this paper, we
introduce SAI3D, a novel zero-shot 3D instance segmentation approach that
synergistically leverages geometric priors and semantic cues derived from
Segment Anything Model (SAM). Our method partitions a 3D scene into geometric
primitives, which are then progressively merged into 3D instance segmentations
that are consistent with the multi-view SAM masks. Moreover, we design a
hierarchical region-growing algorithm with a dynamic thresholding mechanism,
which largely improves the robustness of finegrained 3D scene parsing.
Empirical evaluations on Scan-Net and the more challenging ScanNet++ datasets
demonstrate the superiority of our approach. Notably, SAI3D outperforms
existing open-vocabulary baselines and even surpasses fully-supervised methods
in class-agnostic segmentation on ScanNet++.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11562">A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1">Ruihang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1">Mengzhe Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a></p>
<p>Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11568">VectorTalker: SVG Talking Face Generation with Progressive Vectorisation. (arXiv:2312.11568v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yanbo Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Caigui Jiang</a></p>
<p>High-fidelity and efficient audio-driven talking head generation has been a
key research topic in computer graphics and computer vision. In this work, we
study vector image based audio-driven talking head generation. Compared with
directly animating the raster image that most widely used in existing works,
vector image enjoys its excellent scalability being used for many applications.
There are two main challenges for vector image based talking head generation:
the high-quality vector image reconstruction w.r.t. the source portrait image
and the vivid animation w.r.t. the audio signal. To address these, we propose a
novel scalable vector graphic reconstruction and animation method, dubbed
VectorTalker. Specifically, for the highfidelity reconstruction, VectorTalker
hierarchically reconstructs the vector image in a coarse-to-fine manner. For
the vivid audio-driven facial animation, we propose to use facial landmarks as
intermediate motion representation and propose an efficient landmark-driven
vector image deformation module. Our approach can handle various styles of
portrait images within a unified framework, including Japanese manga, cartoon,
and photorealistic images. We conduct extensive quantitative and qualitative
evaluations and the experimental results demonstrate the superiority of
VectorTalker in both vector graphic reconstruction and audio-driven animation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11570">Understanding the Multi-modal Prompts of the Pre-trained Vision-Language Model. (arXiv:2312.11570v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuailei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chen-Wei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Ying Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Siyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiaqi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1">Xiaoyi Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuxin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yun Zheng</a></p>
<p>Prompt learning has emerged as an efficient alternative for fine-tuning
foundational models, such as CLIP, for various downstream tasks. However, there
is no work that provides a comprehensive explanation for the working mechanism
of the multi-modal prompts. In this paper, we conduct a direct analysis of the
multi-modal prompts by asking the following questions: $(i)$ How do the learned
multi-modal prompts improve the recognition performance? $(ii)$ What do the
multi-modal prompts learn? To answer these questions, we begin by isolating the
component of the formula where the prompt influences the calculation of
self-attention at each layer in two distinct ways, \ie, $(1)$ introducing
prompt embeddings makes the $[cls]$ token focus on foreground objects. $(2)$
the prompts learn a bias term during the update of token embeddings, allowing
the model to adapt to the target domain. Subsequently, we conduct extensive
visualization and statistical experiments on the eleven diverse downstream
recognition datasets. From the experiments, we reveal that the learned prompts
improve the performance mainly through the second way, which acts as the
dataset bias to improve the recognition performance of the pre-trained model on
the corresponding dataset. Based on this finding, we propose the bias tuning
way and demonstrate that directly incorporating the learnable bias outperforms
the learnable prompts in the same parameter settings. In datasets with limited
category information, \ie, EuroSAT, bias tuning surpasses prompt tuning by a
large margin. With a deeper understanding of the multi-modal prompt, we hope
our work can inspire new and solid research in this direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11576">Emotion Based Prediction in the Context of Optimized Trajectory Planning for Immersive Learning. (arXiv:2312.11576v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sungheetha_A/0/1/0/all/0/1">Akey Sungheetha</a>, <a href="http://arxiv.org/find/cs/1/au:+R_R/0/1/0/all/0/1">Rajesh Sharma R</a>, <a href="http://arxiv.org/find/cs/1/au:+R_C/0/1/0/all/0/1">Chinnaiyan R</a></p>
<p>In the virtual elements of immersive learning, the use of Google Expedition
and touch-screen-based emotion are examined. The objective is to investigate
possible ways to combine these technologies to enhance virtual learning
environments and learners emotional engagement. Pedagogical application,
affordances, and cognitive load are the corresponding measures that are
involved. Students will gain insight into the reason behind their significantly
higher post-assessment Prediction Systems scores compared to preassessment
scores through this work that leverages technology. This suggests that it is
effective to include emotional elements in immersive learning scenarios. The
results of this study may help develop new strategies by leveraging the
features of immersive learning technology in educational technologies to
improve virtual reality and augmented reality experiences. Furthermore, the
effectiveness of immersive learning environments can be raised by utilizing
magnetic, optical, or hybrid trackers that considerably improve object
tracking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11577">PR-NeuS: A Prior-based Residual Learning Paradigm for Fast Multi-view Neural Surface Reconstruction. (arXiv:2312.11577v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianyao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qingshan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1">Xinyao Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Wanjuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1">Yew-Soon Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1">Wenbing Tao</a></p>
<p>Neural surfaces learning has shown impressive performance in multi-view
surface reconstruction. However, most existing methods use large multilayer
perceptrons (MLPs) to train their models from scratch, resulting in hours of
training for a single scene. Recently, how to accelerate the neural surfaces
learning has received a lot of attention and remains an open problem. In this
work, we propose a prior-based residual learning paradigm for fast multi-view
neural surface reconstruction. This paradigm consists of two optimization
stages. In the first stage, we propose to leverage generalization models to
generate a basis signed distance function (SDF) field. This initial field can
be quickly obtained by fusing multiple local SDF fields produced by
generalization models. This provides a coarse global geometry prior. Based on
this prior, in the second stage, a fast residual learning strategy based on
hash-encoding networks is proposed to encode an offset SDF field for the basis
SDF field. Moreover, we introduce a prior-guided sampling scheme to help the
residual learning stage converge better, and thus recover finer structures.
With our designed paradigm, experimental results show that our method only
takes about 3 minutes to reconstruct the surface of a single scene, while
achieving competitive surface quality. Our code will be released upon
publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11578">Diffusion-Based Particle-DETR for BEV Perception. (arXiv:2312.11578v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nachkov_A/0/1/0/all/0/1">Asen Nachkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1">Martin Danelljan</a>, <a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1">Danda Pani Paudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>The Bird-Eye-View (BEV) is one of the most widely-used scene representations
for visual perception in Autonomous Vehicles (AVs) due to its well suited
compatibility to downstream tasks. For the enhanced safety of AVs, modeling
perception uncertainty in BEV is crucial. Recent diffusion-based methods offer
a promising approach to uncertainty modeling for visual perception but fail to
effectively detect small objects in the large coverage of the BEV. Such
degradation of performance can be attributed primarily to the specific network
architectures and the matching strategy used when training. Here, we address
this problem by combining the diffusion paradigm with current state-of-the-art
3D object detectors in BEV. We analyze the unique challenges of this approach,
which do not exist with deterministic detectors, and present a simple technique
based on object query interpolation that allows the model to learn positional
dependencies even in the presence of the diffusion noise. Based on this, we
present a diffusion-based DETR model for object detection that bears
similarities to particle methods. Abundant experimentation on the NuScenes
dataset shows equal or better performance for our generative approach, compared
to deterministic state-of-the-art methods. Our source code will be made
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11580">PlaNet-S: Automatic Semantic Segmentation of Placenta. (arXiv:2312.11580v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yamamoto_S/0/1/0/all/0/1">Shinnosuke Yamamoto</a>, <a href="http://arxiv.org/find/eess/1/au:+Saito_I/0/1/0/all/0/1">Isso Saito</a>, <a href="http://arxiv.org/find/eess/1/au:+Takaya_E/0/1/0/all/0/1">Eichi Takaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Harigai_A/0/1/0/all/0/1">Ayaka Harigai</a>, <a href="http://arxiv.org/find/eess/1/au:+Sato_T/0/1/0/all/0/1">Tomomi Sato</a>, <a href="http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1">Tomoya Kobayashi</a>, <a href="http://arxiv.org/find/eess/1/au:+Takase_K/0/1/0/all/0/1">Kei Takase</a>, <a href="http://arxiv.org/find/eess/1/au:+Ueda_T/0/1/0/all/0/1">Takuya Ueda</a></p>
<p>[Purpose] To develop a fully automated semantic placenta segmentation model
that integrates the U-Net and SegNeXt architectures through ensemble learning.
[Methods] A total of 218 pregnant women with suspected placental anomalies who
underwent magnetic resonance imaging (MRI) were enrolled, yielding 1090
annotated images for developing a deep learning model for placental
segmentation. The images were standardized and divided into training and test
sets. The performance of PlaNet-S, which integrates U-Net and SegNeXt within an
ensemble framework, was assessed using Intersection over Union (IoU) and
counting connected components (CCC) against the U-Net model. [Results] PlaNet-S
had significantly higher IoU (0.73 +/- 0.13) than that of U-Net (0.78 +/-
0.010) (p&lt;0.01). The CCC for PlaNet-S was significantly higher than that for
U-Net (p&lt;0.01), matching the ground truth in 86.0\% and 56.7\% of the cases,
respectively. [Conclusion]PlaNet-S performed better than the traditional U-Net
in placental segmentation tasks. This model addresses the challenges of
time-consuming physician-assisted manual segmentation and offers the potential
for diverse applications in placental imaging analyses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11587">Relightable Neural Actor with Intrinsic Decomposition and Pose Control. (arXiv:2312.11587v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1">Diogo Luvizon</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1">Adam Kortylewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1">Marc Habermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a></p>
<p>Creating a digital human avatar that is relightable, drivable, and
photorealistic is a challenging and important problem in Vision and Graphics.
Humans are highly articulated creating pose-dependent appearance effects like
self-shadows and wrinkles, and skin as well as clothing require complex and
space-varying BRDF models. While recent human relighting approaches can recover
plausible material-light decompositions from multi-view video, they do not
generalize to novel poses and still suffer from visual artifacts. To address
this, we propose Relightable Neural Actor, the first video-based method for
learning a photorealistic neural human model that can be relighted, allows
appearance editing, and can be controlled by arbitrary skeletal poses.
Importantly, for learning our human avatar, we solely require a multi-view
recording of the human under a known, but static lighting condition. To achieve
this, we represent the geometry of the actor with a drivable density field that
models pose-dependent clothing deformations and provides a mapping between 3D
and UV space, where normal, visibility, and materials are encoded. To evaluate
our approach in real-world scenarios, we collect a new dataset with four actors
recorded under different light conditions, indoors and outdoors, providing the
first benchmark of its kind for human relighting, and demonstrating
state-of-the-art relighting results for novel human poses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11593">Towards Establishing Dense Correspondence on Multiview Coronary Angiography: From Point-to-Point to Curve-to-Curve Query Matching. (arXiv:2312.11593v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yifan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jena_R/0/1/0/all/0/1">Rohit Jena</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulsun_M/0/1/0/all/0/1">Mehmet Gulsun</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1">Vivek Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Puneet Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1">James C. Gee</a></p>
<p>Coronary angiography is the gold standard imaging technique for studying and
diagnosing coronary artery disease. However, the resulting 2D X-ray projections
lose 3D information and exhibit visual ambiguities. In this work, we aim to
establish dense correspondence in multi-view angiography, serving as a
fundamental basis for various clinical applications and downstream tasks. To
overcome the challenge of unavailable annotated data, we designed a data
simulation pipeline using 3D Coronary Computed Tomography Angiography (CCTA).
We formulated the problem of dense correspondence estimation as a query
matching task over all points of interest in the given views. We established
point-to-point query matching and advanced it to curve-to-curve correspondence,
significantly reducing errors by minimizing ambiguity and improving topological
awareness. The method was evaluated on a set of 1260 image pairs from different
views across 8 clinically relevant angulation groups, demonstrating compelling
results and indicating the feasibility of establishing dense correspondence in
multi-view angiography.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11595">TIP: Text-Driven Image Processing with Semantic and Restoration Instructions. (arXiv:2312.11595v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1">Chenyang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhengzhong Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1">Keren Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Delbracio_M/0/1/0/all/0/1">Mauricio Delbracio</a>, <a href="http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1">Peyman Milanfar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1">Hossein Talebi</a></p>
<p>Text-driven diffusion models have become increasingly popular for various
image editing tasks, including inpainting, stylization, and object replacement.
However, it still remains an open research problem to adopt this
language-vision paradigm for more fine-level image processing tasks, such as
denoising, super-resolution, deblurring, and compression artifact removal. In
this paper, we develop TIP, a Text-driven Image Processing framework that
leverages natural language as a user-friendly interface to control the image
restoration process. We consider the capacity of text information in two
dimensions. First, we use content-related prompts to enhance the semantic
alignment, effectively alleviating identity ambiguity in the restoration
outcomes. Second, our approach is the first framework that supports fine-level
instruction through language-based quantitative specification of the
restoration strength, without the need for explicit task-specific design. In
addition, we introduce a novel fusion mechanism that augments the existing
ControlNet architecture by learning to rescale the generative prior, thereby
achieving better restoration fidelity. Our extensive experiments demonstrate
the superior restoration performance of TIP compared to the state of the arts,
alongside offering the flexibility of text-based control over the restoration
effects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11598">SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution. (arXiv:2312.11598v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhixuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1">Yao Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hengbo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1">Masayoshi Tomizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a></p>
<p>Diffusion models have demonstrated strong potential for robotic trajectory
planning. However, generating coherent and long-horizon trajectories from
high-level instructions remains challenging, especially for complex tasks
requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end
hierarchical planning framework integrating interpretable skill learning with
conditional diffusion planning to address this problem. At the higher level,
the skill abstraction module learns discrete, human-understandable skill
representations from visual observations and language instructions. These
learned skill embeddings are then used to condition the diffusion model to
generate customized latent trajectories aligned with the skills. It allows for
generating diverse state trajectories that adhere to the learnable skills. By
integrating skill learning with conditional trajectory generation,
SkillDiffuser produces coherent behavior following abstract instructions across
diverse tasks. Experiments on multi-task robotic manipulation benchmarks like
Meta-World and LOReL demonstrate state-of-the-art performance and
human-interpretable skill representations from SkillDiffuser.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11666">HAAR: Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles. (arXiv:2312.11666v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sklyarova_V/0/1/0/all/0/1">Vanessa Sklyarova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakharov_E/0/1/0/all/0/1">Egor Zakharov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1">Justus Thies</a></p>
<p>We present HAAR, a new strand-based generative model for 3D human hairstyles.
Specifically, based on textual inputs, HAAR produces 3D hairstyles that could
be used as production-level assets in modern computer graphics engines. Current
AI-based generative models take advantage of powerful 2D priors to reconstruct
3D content in the form of point clouds, meshes, or volumetric functions.
However, by using the 2D priors, they are intrinsically limited to only
recovering the visual parts. Highly occluded hair structures can not be
reconstructed with those methods, and they only model the ''outer shell'',
which is not ready to be used in physics-based rendering or simulation
pipelines. In contrast, we propose a first text-guided generative method that
uses 3D hair strands as an underlying representation. Leveraging 2D visual
question-answering (VQA) systems, we automatically annotate synthetic hair
models that are generated from a small set of artist-created hairstyles. This
allows us to train a latent diffusion model that operates in a common hairstyle
UV space. In qualitative and quantitative studies, we demonstrate the
capabilities of the proposed model and compare it to existing hairstyle
generation approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11707">Unified framework for diffusion generative models in SO(3): applications in computer vision and astrophysics. (arXiv:2312.11707v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jagvaral_Y/0/1/0/all/0/1">Yesukhei Jagvaral</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanusse_F/0/1/0/all/0/1">Francois Lanusse</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandelbaum_R/0/1/0/all/0/1">Rachel Mandelbaum</a></p>
<p>Diffusion-based generative models represent the current state-of-the-art for
image generation. However, standard diffusion models are based on Euclidean
geometry and do not translate directly to manifold-valued data. In this work,
we develop extensions of both score-based generative models (SGMs) and
Denoising Diffusion Probabilistic Models (DDPMs) to the Lie group of 3D
rotations, SO(3). SO(3) is of particular interest in many disciplines such as
robotics, biochemistry and astronomy/cosmology science. Contrary to more
general Riemannian manifolds, SO(3) admits a tractable solution to heat
diffusion, and allows us to implement efficient training of diffusion models.
We apply both SO(3) DDPMs and SGMs to synthetic densities on SO(3) and
demonstrate state-of-the-art results. Additionally, we demonstrate the
practicality of our model on pose estimation tasks and in predicting correlated
galaxy orientations for astrophysics/cosmology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11716">Squeezed Edge YOLO: Onboard Object Detection on Edge Devices. (arXiv:2312.11716v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Humes_E/0/1/0/all/0/1">Edward Humes</a>, <a href="http://arxiv.org/find/cs/1/au:+Navardi_M/0/1/0/all/0/1">Mozhgan Navardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohsenin_T/0/1/0/all/0/1">Tinoosh Mohsenin</a></p>
<p>Demand for efficient onboard object detection is increasing due to its key
role in autonomous navigation. However, deploying object detection models such
as YOLO on resource constrained edge devices is challenging due to the high
computational requirements of such models. In this paper, an compressed object
detection model named Squeezed Edge YOLO is examined. This model is compressed
and optimized to kilobytes of parameters in order to fit onboard such edge
devices. To evaluate Squeezed Edge YOLO, two use cases - human and shape
detection - are used to show the model accuracy and performance. Moreover, the
model is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA
Jetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO
model size is optimized by a factor of 8x which leads to 76% improvements in
energy efficiency and 3.3x faster throughout.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11748">Ultrasound Image Enhancement using CycleGAN and Perceptual Loss. (arXiv:2312.11748v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Athreya_S/0/1/0/all/0/1">Shreeram Athreya</a>, <a href="http://arxiv.org/find/eess/1/au:+Radhachandran_A/0/1/0/all/0/1">Ashwath Radhachandran</a>, <a href="http://arxiv.org/find/eess/1/au:+Ivezic_V/0/1/0/all/0/1">Vedrana Ivezi&#x107;</a>, <a href="http://arxiv.org/find/eess/1/au:+Sant_V/0/1/0/all/0/1">Vivek Sant</a>, <a href="http://arxiv.org/find/eess/1/au:+Arnold_C/0/1/0/all/0/1">Corey W. Arnold</a>, <a href="http://arxiv.org/find/eess/1/au:+Speier_W/0/1/0/all/0/1">William Speier</a></p>
<p>Purpose: The objective of this work is to introduce an advanced framework
designed to enhance ultrasound images, especially those captured by portable
hand-held devices, which often produce lower quality images due to hardware
constraints. Additionally, this framework is uniquely capable of effectively
handling non-registered input ultrasound image pairs, addressing a common
challenge in medical imaging. Materials and Methods: In this retrospective
study, we utilized an enhanced generative adversarial network (CycleGAN) model
for ultrasound image enhancement across five organ systems. Perceptual loss,
derived from deep features of pretrained neural networks, is applied to ensure
the human-perceptual quality of the enhanced images. These images are compared
with paired images acquired from high resolution devices to demonstrate the
model's ability to generate realistic high-quality images across organ systems.
Results: Preliminary validation of the framework reveals promising performance
metrics. The model generates images that result in a Structural Similarity
Index (SSI) score of 0.722, Locally Normalized Cross-Correlation (LNCC) score
of 0.902 and 28.802 for the Peak Signal-to-Noise Ratio (PSNR) metric.
Conclusion: This work presents a significant advancement in medical imaging
through the development of a CycleGAN model enhanced with Perceptual Loss (PL),
effectively bridging the quality gap between ultrasound images from varied
devices. By training on paired images, the model not only improves image
quality but also ensures the preservation of vital anatomic structural content.
This approach may improve equity in access to healthcare by enhancing portable
device capabilities, although further validation and optimizations are
necessary for broader clinical application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11763">ADMM-MM Algorithm for General Tensor Decomposition. (arXiv:2312.11763v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukai_M/0/1/0/all/0/1">Manabu Mukai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hontani_H/0/1/0/all/0/1">Hidekata Hontani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokota_T/0/1/0/all/0/1">Tatsuya Yokota</a></p>
<p>In this paper, we propose a new unified optimization algorithm for general
tensor decomposition which is formulated as an inverse problem for low-rank
tensors in the general linear observation models. The proposed algorithm
supports three basic loss functions ($\ell_2$-loss, $\ell_1$-loss and KL
divergence) and various low-rank tensor decomposition models (CP, Tucker, TT,
and TR decompositions). We derive the optimization algorithm based on
hierarchical combination of the alternating direction method of multiplier
(ADMM) and majorization-minimization (MM). We show that wide-range applications
can be solved by the proposed algorithm, and can be easily extended to any
established tensor decomposition models in a {plug-and-play} manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11770">Bridging the Gap: Generalising State-of-the-Art U-Net Models to Sub-Saharan African Populations. (arXiv:2312.11770v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amod_A/0/1/0/all/0/1">Alyssa R. Amod</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1">Alexandra Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Joubert_P/0/1/0/all/0/1">Pearly Joubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Raymond_C/0/1/0/all/0/1">Confidence Raymond</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Anazodo_U/0/1/0/all/0/1">Udunna C. Anazodo</a>, <a href="http://arxiv.org/find/cs/1/au:+Motchon_D/0/1/0/all/0/1">Dodzi Motchon</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutsvangwa_T/0/1/0/all/0/1">Tinashe E.M. Mutsvangwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Quetin_S/0/1/0/all/0/1">S&#xe9;bastien Quetin</a></p>
<p>A critical challenge for tumour segmentation models is the ability to adapt
to diverse clinical settings, particularly when applied to poor-quality
neuroimaging data. The uncertainty surrounding this adaptation stems from the
lack of representative datasets, leaving top-performing models without exposure
to common artifacts found in MRI data throughout Sub-Saharan Africa (SSA). We
replicated a framework that secured the 2nd position in the 2022 BraTS
competition to investigate the impact of dataset composition on model
performance and pursued four distinct approaches through training a model with:
1) BraTS-Africa data only (train_SSA, N=60), 2) BraTS-Adult Glioma data only
(train_GLI, N=1251), 3) both datasets together (train_ALL, N=1311), and 4)
through further training the train_GLI model with BraTS-Africa data
(train_ftSSA). Notably, training on a smaller low-quality dataset alone
(train_SSA) yielded subpar results, and training on a larger high-quality
dataset alone (train_GLI) struggled to delineate oedematous tissue in the
low-quality validation set. The most promising approach (train_ftSSA) involved
pre-training a model on high-quality neuroimages and then fine-tuning it on the
smaller, low-quality dataset. This approach outperformed the others, ranking
second in the MICCAI BraTS Africa global challenge external testing phase.
These findings underscore the significance of larger sample sizes and broad
exposure to data in improving segmentation performance. Furthermore, we
demonstrated that there is potential for improving such models by fine-tuning
them with a wider range of data locally.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11772">CAManim: Animating end-to-end network activation maps. (arXiv:2312.11772v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaczmarek_E/0/1/0/all/0/1">Emily Kaczmarek</a>, <a href="http://arxiv.org/find/cs/1/au:+Miguel_O/0/1/0/all/0/1">Olivier X. Miguel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowie_A/0/1/0/all/0/1">Alexa C. Bowie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ducharme_R/0/1/0/all/0/1">Robin Ducharme</a>, <a href="http://arxiv.org/find/cs/1/au:+Dingwall_Harvey_A/0/1/0/all/0/1">Alysha L.J. Dingwall-Harvey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hawken_S/0/1/0/all/0/1">Steven Hawken</a>, <a href="http://arxiv.org/find/cs/1/au:+Armour_C/0/1/0/all/0/1">Christine M. Armour</a>, <a href="http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1">Mark C. Walker</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_K/0/1/0/all/0/1">Kevin Dick</a></p>
<p>Deep neural networks have been widely adopted in numerous domains due to
their high performance and accessibility to developers and application-specific
end-users. Fundamental to image-based applications is the development of
Convolutional Neural Networks (CNNs), which possess the ability to
automatically extract features from data. However, comprehending these complex
models and their learned representations, which typically comprise millions of
parameters and numerous layers, remains a challenge for both developers and
end-users. This challenge arises due to the absence of interpretable and
transparent tools to make sense of black-box models. There exists a growing
body of Explainable Artificial Intelligence (XAI) literature, including a
collection of methods denoted Class Activation Maps (CAMs), that seek to
demystify what representations the model learns from the data, how it informs a
given prediction, and why it, at times, performs poorly in certain tasks. We
propose a novel XAI visualization method denoted CAManim that seeks to
simultaneously broaden and focus end-user understanding of CNN predictions by
animating the CAM-based network activation maps through all layers, effectively
depicting from end-to-end how a model progressively arrives at the final layer
activation. Herein, we demonstrate that CAManim works with any CAM-based method
and various CNN architectures. Beyond qualitative model assessments, we
additionally propose a novel quantitative assessment that expands upon the
Remove and Debias (ROAD) metric, pairing the qualitative end-to-end network
visual explanations assessment with our novel quantitative "yellow brick ROAD"
assessment (ybROAD). This builds upon prior research to address the increasing
demand for interpretable, robust, and transparent model assessment methodology,
ultimately improving an end-user's trust in a given model's predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11774">Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation. (arXiv:2312.11774v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuze He</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yushi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Matthieu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1">Jenny Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yubin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yu-Hui Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong-Jin Liu</a></p>
<p>By lifting the pre-trained 2D diffusion models into Neural Radiance Fields
(NeRFs), text-to-3D generation methods have made great progress. Many
state-of-the-art approaches usually apply score distillation sampling (SDS) to
optimize the NeRF representations, which supervises the NeRF optimization with
pre-trained text-conditioned 2D diffusion models such as Imagen. However, the
supervision signal provided by such pre-trained diffusion models only depends
on text prompts and does not constrain the multi-view consistency. To inject
the cross-view consistency into diffusion priors, some recent works finetune
the 2D diffusion model with multi-view data, but still lack fine-grained view
coherence. To tackle this challenge, we incorporate multi-view image conditions
into the supervision signal of NeRF optimization, which explicitly enforces
fine-grained view consistency. With such stronger supervision, our proposed
text-to-3D method effectively mitigates the generation of floaters (due to
excessive densities) and completely empty spaces (due to insufficient
densities). Our quantitative evaluations on the T$^3$Bench dataset demonstrate
that our method achieves state-of-the-art performance over existing text-to-3D
methods. We will make the code publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11775">Towards SAMBA: Segment Anything Model for Brain Tumor Segmentation in Sub-Sharan African Populations. (arXiv:2312.11775v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Barakat_M/0/1/0/all/0/1">Mohannad Barakat</a>, <a href="http://arxiv.org/find/eess/1/au:+Magdy_N/0/1/0/all/0/1">Noha Magdy</a>, <a href="http://arxiv.org/find/eess/1/au:+William_J/0/1/0/all/0/1">Jjuuko George William</a>, <a href="http://arxiv.org/find/eess/1/au:+Phiri_E/0/1/0/all/0/1">Ethel Phiri</a>, <a href="http://arxiv.org/find/eess/1/au:+Confidence_R/0/1/0/all/0/1">Raymond Confidence</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1">Udunna C Anazodo</a></p>
<p>Gliomas, the most prevalent primary brain tumors, require precise
segmentation for diagnosis and treatment planning. However, this task poses
significant challenges, particularly in the African population, were limited
access to high-quality imaging data hampers algorithm performance. In this
study, we propose an innovative approach combining the Segment Anything Model
(SAM) and a voting network for multi-modal glioma segmentation. By fine-tuning
SAM with bounding box-guided prompts (SAMBA), we adapt the model to the
complexities of African datasets. Our ensemble strategy, utilizing multiple
modalities and views, produces a robust consensus segmentation, addressing
intra-tumoral heterogeneity. Although the low quality of scans presents
difficulties, our methodology has the potential to profoundly impact clinical
practice in resource-limited settings such as Africa, improving treatment
decisions and advancing neuro-oncology research. Furthermore, successful
application to other brain tumor types and lesions in the future holds promise
for a broader transformation in neurological imaging, improving healthcare
outcomes across all settings. This study was conducted on the Brain Tumor
Segmentation (BraTS) Challenge Africa (BraTS-Africa) dataset, which provides a
valuable resource for addressing challenges specific to resource-limited
settings, particularly the African population, and facilitating the development
of effective and more generalizable segmentation algorithms. To illustrate our
approach's potential, our experiments on the BraTS-Africa dataset yielded
compelling results, with SAM attaining a Dice coefficient of 86.6 for binary
segmentation and 60.4 for multi-class segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11782">Learning Object State Changes in Videos: An Open-World Perspective. (arXiv:2312.11782v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zihui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1">Kumar Ashutosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1">Kristen Grauman</a></p>
<p>Object State Changes (OSCs) are pivotal for video understanding. While humans
can effortlessly generalize OSC understanding from familiar to unknown objects,
current approaches are confined to a closed vocabulary. Addressing this gap, we
introduce a novel open-world formulation for the video OSC problem. The goal is
to temporally localize the three stages of an OSC -- the object's initial
state, its transitioning state, and its end state -- whether or not the object
has been observed during training. Towards this end, we develop VidOSC, a
holistic learning approach that: (1) leverages text and vision-language models
for supervisory signals to obviate manually labeling OSC training data, and (2)
abstracts fine-grained shared state representations from objects to enhance
generalization. Furthermore, we present HowToChange, the first open-world
benchmark for video OSC localization, which offers an order of magnitude
increase in the label space and annotation volume compared to the best existing
benchmark. Experimental results demonstrate the efficacy of our approach, in
both traditional closed-world and open-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11793">An effective image copy-move forgery detection using entropy image. (arXiv:2312.11793v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhaowei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Li Jiang</a></p>
<p>Image forensics has become increasingly important in our daily lives. As a
fundamental type of forgeries, Copy-Move Forgery Detection (CMFD) has received
significant attention in the academic community. Keypoint-based algorithms,
particularly those based on SIFT, have achieved good results in CMFD. However,
the most of keypoint detection algorithms often fail to generate sufficient
matches when tampered patches are present in smooth areas. To tackle this
problem, we introduce entropy images to determine the coordinates and scales of
keypoints, resulting significantly increasing the number of keypoints.
Furthermore, we develop an entropy level clustering algorithm to avoid
increased matching complexity caused by non-ideal distribution of grayscale
values in keypoints. Experimental results demonstrate that our algorithm
achieves a good balance between performance and time efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11805">Gemini: A Family of Highly Capable Multimodal Models. (arXiv:2312.11805v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Team_Gemini/0/1/0/all/0/1">Gemini Team Google</a>: <a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1">Rohan Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1">Sebastian Borgeaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yonghui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1">Jean-Baptiste Alayrac</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiahui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1">Radu Soricut</a>, <a href="http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1">Johan Schalkwyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Andrew M. Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hauth_A/0/1/0/all/0/1">Anja Hauth</a>, <a href="http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1">Katie Millican</a>, <a href="http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1">David Silver</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1">Slav Petrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1">Melvin Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonoglou_I/0/1/0/all/0/1">Ioannis Antonoglou</a>, <a href="http://arxiv.org/find/cs/1/au:+Schrittwieser_J/0/1/0/all/0/1">Julian Schrittwieser</a>, <a href="http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1">Amelia Glaese</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitler_E/0/1/0/all/0/1">Emily Pitler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1">Timothy Lillicrap</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1">Angeliki Lazaridou</a>, <a href="http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1">Orhan Firat</a>, <a href="http://arxiv.org/find/cs/1/au:+Molloy_J/0/1/0/all/0/1">James Molloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Isard_M/0/1/0/all/0/1">Michael Isard</a>, <a href="http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1">Paul R. Barham</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1">Tom Hennigan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Benjamin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Viola_F/0/1/0/all/0/1">Fabio Viola</a>, <a href="http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1">Malcolm Reynolds</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuanzhong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Doherty_R/0/1/0/all/0/1">Ryan Doherty</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_E/0/1/0/all/0/1">Eli Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1">Clemens Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1">Eliza Rutherford</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1">Erica Moreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayoub_K/0/1/0/all/0/1">Kareem Ayoub</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_M/0/1/0/all/0/1">Megha Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1">George Tucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Piqueras_E/0/1/0/all/0/1">Enrique Piqueras</a>, <a href="http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1">Maxim Krikun</a>, <a href="http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1">Iain Barr</a>, <a href="http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1">Nikolay Savinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Danihelka_I/0/1/0/all/0/1">Ivo Danihelka</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_B/0/1/0/all/0/1">Becca Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1">Ana&#xef;s White</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1">Anders Andreassen</a>, <a href="http://arxiv.org/find/cs/1/au:+Glehn_T/0/1/0/all/0/1">Tamara von Glehn</a>, <a href="http://arxiv.org/find/cs/1/au:+Yagati_L/0/1/0/all/0/1">Lakshman Yagati</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1">Mehran Kazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_L/0/1/0/all/0/1">Lucas Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalman_M/0/1/0/all/0/1">Misha Khalman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1">Jakub Sygnowski</a>, et al. (890 additional authors not shown)</p>
<p>This report introduces a new family of multimodal models, Gemini, that
exhibit remarkable capabilities across image, audio, video, and text
understanding. The Gemini family consists of Ultra, Pro, and Nano sizes,
suitable for applications ranging from complex reasoning tasks to on-device
memory-constrained use-cases. Evaluation on a broad range of benchmarks shows
that our most-capable Gemini Ultra model advances the state of the art in 30 of
32 of these benchmarks - notably being the first model to achieve human-expert
performance on the well-studied exam benchmark MMLU, and improving the state of
the art in every one of the 20 multimodal benchmarks we examined. We believe
that the new capabilities of Gemini models in cross-modal reasoning and
language understanding will enable a wide variety of use cases and we discuss
our approach toward deploying them responsibly to users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11812">Advancements and Challenges in Arabic Optical Character Recognition: A Comprehensive Survey. (arXiv:2312.11812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasem_M/0/1/0/all/0/1">Mahmoud SalahEldin Kasem</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoud_M/0/1/0/all/0/1">Mohamed Mahmoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hyun-Soo Kang</a></p>
<p>Optical character recognition (OCR) is a vital process that involves the
extraction of handwritten or printed text from scanned or printed images,
converting it into a format that can be understood and processed by machines.
This enables further data processing activities such as searching and editing.
The automatic extraction of text through OCR plays a crucial role in digitizing
documents, enhancing productivity, improving accessibility, and preserving
historical records. This paper seeks to offer an exhaustive review of
contemporary applications, methodologies, and challenges associated with Arabic
Optical Character Recognition (OCR). A thorough analysis is conducted on
prevailing techniques utilized throughout the OCR process, with a dedicated
effort to discern the most efficacious approaches that demonstrate enhanced
outcomes. To ensure a thorough evaluation, a meticulous keyword-search
methodology is adopted, encompassing a comprehensive analysis of articles
relevant to Arabic OCR, including both backward and forward citation reviews.
In addition to presenting cutting-edge techniques and methods, this paper
critically identifies research gaps within the realm of Arabic OCR. By
highlighting these gaps, we shed light on potential areas for future
exploration and development, thereby guiding researchers toward promising
avenues in the field of Arabic OCR. The outcomes of this study provide valuable
insights for researchers, practitioners, and stakeholders involved in Arabic
OCR, ultimately fostering advancements in the field and facilitating the
creation of more accurate and efficient OCR systems for the Arabic language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11816">A Dual-way Enhanced Framework from Text Matching Point of View for Multimodal Entity Linking. (arXiv:2312.11816v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shezheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1">Tianwei Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shasha Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1">Xiaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a></p>
<p>Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with
multimodal information to entity in Knowledge Graph (KG) such as Wikipedia,
which plays a key role in many applications. However, existing methods suffer
from shortcomings, including modality impurity such as noise in raw image and
ambiguous textual entity representation, which puts obstacles to MEL. We
formulate multimodal entity linking as a neural text matching problem where
each multimodal information (text and image) is treated as a query, and the
model learns the mapping from each query to the relevant entity from candidate
entities. This paper introduces a dual-way enhanced (DWE) framework for MEL:
(1) our model refines queries with multimodal data and addresses semantic gaps
using cross-modal enhancers between text and image information. Besides, DWE
innovatively leverages fine-grained image attributes, including facial
characteristic and scene feature, to enhance and refine visual features. (2)By
using Wikipedia descriptions, DWE enriches entity semantics and obtains more
comprehensive textual representation, which reduces between textual
representation and the entities in KG. Extensive experiments on three public
benchmarks demonstrate that our method achieves state-of-the-art (SOTA)
performance, indicating the superiority of our model. The code is released on
https://github.com/season1blue/DWE
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11826">Decoupled Textual Embeddings for Customized Image Generation. (arXiv:2312.11826v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yufei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yuxiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Zhilong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jinfeng Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Hu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1">Wangmeng Zuo</a></p>
<p>Customized text-to-image generation, which aims to learn user-specified
concepts with a few images, has drawn significant attention recently. However,
existing methods usually suffer from overfitting issues and entangle the
subject-unrelated information (e.g., background and pose) with the learned
concept, limiting the potential to compose concept into new scenes. To address
these issues, we propose the DETEX, a novel approach that learns the
disentangled concept embedding for flexible customized text-to-image
generation. Unlike conventional methods that learn a single concept embedding
from the given images, our DETEX represents each image using multiple word
embeddings during training, i.e., a learnable image-shared subject embedding
and several image-specific subject-unrelated embeddings. To decouple irrelevant
attributes (i.e., background and pose) from the subject embedding, we further
present several attribute mappers that encode each image as several
image-specific subject-unrelated embeddings. To encourage these unrelated
embeddings to capture the irrelevant information, we incorporate them with
corresponding attribute words and propose a joint training strategy to
facilitate the disentanglement. During inference, we only use the subject
embedding for image generation, while selectively using image-specific
embeddings to retain image-specified attributes. Extensive experiments
demonstrate that the subject embedding obtained by our method can faithfully
represent the target concept, while showing superior editability compared to
the state-of-the-art methods. Our code will be made published available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11829">RadOcc: Learning Cross-Modality Occupancy Knowledge through Rendering Assisted Distillation. (arXiv:2312.11829v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_D/0/1/0/all/0/1">Dongfeng Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiantao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingbing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a></p>
<p>3D occupancy prediction is an emerging task that aims to estimate the
occupancy states and semantics of 3D scenes using multi-view images. However,
image-based scene perception encounters significant challenges in achieving
accurate prediction due to the absence of geometric priors. In this paper, we
address this issue by exploring cross-modal knowledge distillation in this
task, i.e., we leverage a stronger multi-modal model to guide the visual model
during training. In practice, we observe that directly applying features or
logits alignment, proposed and widely used in bird's-eyeview (BEV) perception,
does not yield satisfactory results. To overcome this problem, we introduce
RadOcc, a Rendering assisted distillation paradigm for 3D Occupancy prediction.
By employing differentiable volume rendering, we generate depth and semantic
maps in perspective views and propose two novel consistency criteria between
the rendered outputs of teacher and student models. Specifically, the depth
consistency loss aligns the termination distributions of the rendered rays,
while the semantic consistency loss mimics the intra-segment similarity guided
by vision foundation models (VLMs). Experimental results on the nuScenes
dataset demonstrate the effectiveness of our proposed method in improving
various 3D occupancy prediction approaches, e.g., our proposed methodology
enhances our baseline by 2.2% in the metric of mIoU and achieves 50% in Occ3D
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11837">Regulating Intermediate 3D Features for Vision-Centric Autonomous Driving. (arXiv:2312.11837v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Junkai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Liang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Haoran Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1">Linxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1">Dan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1">Wei Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Deng Cai</a></p>
<p>Multi-camera perception tasks have gained significant attention in the field
of autonomous driving. However, existing frameworks based on Lift-Splat-Shoot
(LSS) in the multi-camera setting cannot produce suitable dense 3D features due
to the projection nature and uncontrollable densification process. To resolve
this problem, we propose to regulate intermediate dense 3D features with the
help of volume rendering. Specifically, we employ volume rendering to process
the dense 3D features to obtain corresponding 2D features (e.g., depth maps,
semantic maps), which are supervised by associated labels in the training. This
manner regulates the generation of dense 3D features on the feature level,
providing appropriate dense and unified features for multiple perception tasks.
Therefore, our approach is termed Vampire, stands for "Volume rendering As
Multi-camera Perception Intermediate feature REgulator". Experimental results
on the Occ3D and nuScenes datasets demonstrate that Vampire facilitates
fine-grained and appropriate extraction of dense 3D features, and is
competitive with existing SOTA methods across diverse downstream perception
tasks like 3D occupancy prediction, LiDAR segmentation and 3D objection
detection, while utilizing moderate GPU resources. We provide a video
demonstration in the supplementary materials and Codes are available at
github.com/cskkxjk/Vampire.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11841">MixRT: Mixed Neural Representations For Real-Time NeRF Rendering. (arXiv:2312.11841v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaojian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Yingyan/0/1/0/all/0/1">Yingyan</a> (Celine)Lin</p>
<p>Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11849">Active contours driven by local and global intensity fitting energy with application to SAR image segmentation and its fast solvers. (arXiv:2312.11849v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guangming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Quanying Sun</a></p>
<p>In this paper, we propose a novel variational active contour model based on
Aubert-Aujol (AA) denoising model, which hybrides geodesic active contour (GAC)
model with active contours without edges (ACWE) model and can be used to
segment images corrupted by multiplicative gamma noise. We transform the
proposed model into classic ROF model by adding a proximity term. Inspired by a
fast denosing algorithm proposed by Jia-Zhao recently, we propose two fast
fixed point algorithms to solve SAR image segmentation question. Experimental
results for real SAR images show that the proposed image segmentation model can
efficiently stop the contours at weak or blurred edges, and can automatically
detect the exterior and interior boundaries of images with multiplicative gamma
noise. The proposed fast fixed point algorithms are robustness to
initialization contour, and can further reduce about 15% of the time needed for
algorithm proposed by Goldstein-Osher.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11850">GCNext: Towards the Unity of Graph Convolutions for Human Motion Prediction. (arXiv:2312.11850v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinshun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qiongjie Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>The past few years has witnessed the dominance of Graph Convolutional
Networks (GCNs) over human motion prediction.Various styles of graph
convolutions have been proposed, with each one meticulously designed and
incorporated into a carefully-crafted network architecture. This paper breaks
the limits of existing knowledge by proposing Universal Graph Convolution
(UniGC), a novel graph convolution concept that re-conceptualizes different
graph convolutions as its special cases. Leveraging UniGC on network-level, we
propose GCNext, a novel GCN-building paradigm that dynamically determines the
best-fitting graph convolutions both sample-wise and layer-wise. GCNext offers
multiple use cases, including training a new GCN from scratch or refining a
preexisting GCN. Experiments on Human3.6M, AMASS, and 3DPW datasets show that,
by incorporating unique module-to-network designs, GCNext yields up to 9x lower
computational cost than existing GCN methods, on top of achieving
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11856">Self-supervised Learning for Enhancing Geometrical Modeling in 3D-Aware Generative Adversarial Network. (arXiv:2312.11856v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiarong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a></p>
<p>3D-aware Generative Adversarial Networks (3D-GANs) currently exhibit
artifacts in their 3D geometrical modeling, such as mesh imperfections and
holes. These shortcomings are primarily attributed to the limited availability
of annotated 3D data, leading to a constrained "valid latent area" for
satisfactory modeling. To address this, we present a Self-Supervised Learning
(SSL) technique tailored as an auxiliary loss for any 3D-GAN, designed to
improve its 3D geometrical modeling capabilities. Our approach pioneers an
inversion technique for 3D-GANs, integrating an encoder that performs adaptive
spatially-varying range operations. Utilizing this inversion, we introduce the
Cyclic Generative Constraint (CGC), aiming to densify the valid latent space.
The CGC operates via augmented local latent vectors that maintain the same
geometric form, and it imposes constraints on the cycle path outputs,
specifically the generator-encoder-generator sequence. This SSL methodology
seamlessly integrates with the inherent GAN loss, ensuring the integrity of
pre-existing 3D-GAN architectures without necessitating alterations. We
validate our approach with comprehensive experiments across various datasets
and architectures, underscoring its efficacy. Our project website:
https://3dgan-ssl.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11862">Topo-MLP : A Simplicial Network Without Message Passing. (arXiv:2312.11862v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1">Karthikeyan Natesan Ramamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Guzman_Saenz_A/0/1/0/all/0/1">Aldo Guzm&#xe1;n-S&#xe1;enz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1">Mustafa Hajij</a></p>
<p>Due to their ability to model meaningful higher order relations among a set
of entities, higher order network models have emerged recently as a powerful
alternative for graph-based network models which are only capable of modeling
binary relationships. Message passing paradigm is still dominantly used to
learn representations even for higher order network models. While powerful,
message passing can have disadvantages during inference, particularly when the
higher order connectivity information is missing or corrupted. To overcome such
limitations, we propose Topo-MLP, a purely MLP-based simplicial neural network
algorithm to learn the representation of elements in a simplicial complex
without explicitly relying on message passing. Our framework utilizes a novel
Higher Order Neighborhood Contrastive (HONC) loss which implicitly incorporates
the simplicial structure into representation learning. Our proposed model's
simplicity makes it faster during inference. Moreover, we show that our model
is robust when faced with missing or corrupted connectivity structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11867">Point Cloud Part Editing: Segmentation, Generation, Assembly, and Selection. (arXiv:2312.11867v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Ximing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weizhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Cheng Jin</a></p>
<p>Ideal part editing should guarantee the diversity of edited parts, the
fidelity to the remaining parts, and the quality of the results. However,
previous methods do not disentangle each part completely, which means the
edited parts will affect the others, resulting in poor diversity and fidelity.
In addition, some methods lack constraints between parts, which need manual
selections of edited results to ensure quality. Therefore, we propose a
four-stage process for point cloud part editing: Segmentation, Generation,
Assembly, and Selection. Based on this process, we introduce SGAS, a model for
part editing that employs two strategies: feature disentanglement and
constraint. By independently fitting part-level feature distributions, we
realize the feature disentanglement. By explicitly modeling the transformation
from object-level distribution to part-level distributions, we realize the
feature constraint. Considerable experiments on different datasets demonstrate
the efficiency and effectiveness of SGAS on point cloud part editing. In
addition, SGAS can be pruned to realize unsupervised part-aware point cloud
generation and achieves state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11872">Beyond Prototypes: Semantic Anchor Regularization for Better Representation Learning. (arXiv:2312.11872v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yanqi Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1">Qiang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Ye Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1">Lixin Duan</a></p>
<p>One of the ultimate goals of representation learning is to achieve
compactness within a class and well-separability between classes. Many
outstanding metric-based and prototype-based methods following the
Expectation-Maximization paradigm, have been proposed for this objective.
However, they inevitably introduce biases into the learning process,
particularly with long-tail distributed training data. In this paper, we reveal
that the class prototype is not necessarily to be derived from training
features and propose a novel perspective to use pre-defined class anchors
serving as feature centroid to unidirectionally guide feature learning.
However, the pre-defined anchors may have a large semantic distance from the
pixel features, which prevents them from being directly applied. To address
this issue and generate feature centroid independent from feature learning, a
simple yet effective Semantic Anchor Regularization (SAR) is proposed. SAR
ensures the interclass separability of semantic anchors in the semantic space
by employing a classifier-aware auxiliary cross-entropy loss during training
via disentanglement learning. By pulling the learned features to these semantic
anchors, several advantages can be attained: 1) the intra-class compactness and
naturally inter-class separability, 2) induced bias or errors from feature
learning can be avoided, and 3) robustness to the long-tailed problem. The
proposed SAR can be used in a plug-and-play manner in the existing models.
Extensive experiments demonstrate that the SAR performs better than previous
sophisticated prototype-based methods. The implementation is available at
https://github.com/geyanqi/SAR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11880">Point Cloud Segmentation Using Transfer Learning with RandLA-Net: A Case Study on Urban Areas. (arXiv:2312.11880v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bayar_A/0/1/0/all/0/1">Alperen Enes Bayar</a>, <a href="http://arxiv.org/find/cs/1/au:+Uyan_U/0/1/0/all/0/1">Ufuk Uyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Toprak_E/0/1/0/all/0/1">Elif Toprak</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuheng_C/0/1/0/all/0/1">Cao Yuheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Juncheng_T/0/1/0/all/0/1">Tang Juncheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kindiroglu_A/0/1/0/all/0/1">Ahmet Alp Kindiroglu</a></p>
<p>Urban environments are characterized by complex structures and diverse
features, making accurate segmentation of point cloud data a challenging task.
This paper presents a comprehensive study on the application of RandLA-Net, a
state-of-the-art neural network architecture, for the 3D segmentation of
large-scale point cloud data in urban areas. The study focuses on three major
Chinese cities, namely Chengdu, Jiaoda, and Shenzhen, leveraging their unique
characteristics to enhance segmentation performance.
</p>
<p>To address the limited availability of labeled data for these specific urban
areas, we employed transfer learning techniques. We transferred the learned
weights from the Sensat Urban and Toronto 3D datasets to initialize our
RandLA-Net model. Additionally, we performed class remapping to adapt the model
to the target urban areas, ensuring accurate segmentation results.
</p>
<p>The experimental results demonstrate the effectiveness of the proposed
approach achieving over 80\% F1 score for each areas in 3D point cloud
segmentation. The transfer learning strategy proves to be crucial in overcoming
data scarcity issues, providing a robust solution for urban point cloud
analysis. The findings contribute to the advancement of point cloud
segmentation methods, especially in the context of rapidly evolving Chinese
urban areas.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11894">3D-LFM: Lifting Foundation Model. (arXiv:2312.11894v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dabhi_M/0/1/0/all/0/1">Mosam Dabhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1">Laszlo A. Jeni</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1">Simon Lucey</a></p>
<p>The lifting of 3D structure and camera from 2D landmarks is at the
cornerstone of the entire discipline of computer vision. Traditional methods
have been confined to specific rigid objects, such as those in
Perspective-n-Point (PnP) problems, but deep learning has expanded our
capability to reconstruct a wide range of object classes (e.g. C3PDO and PAUL)
with resilience to noise, occlusions, and perspective distortions. All these
techniques, however, have been limited by the fundamental need to establish
correspondences across the 3D training data -- significantly limiting their
utility to applications where one has an abundance of "in-correspondence" 3D
data. Our approach harnesses the inherent permutation equivariance of
transformers to manage varying number of points per 3D data instance,
withstands occlusions, and generalizes to unseen categories. We demonstrate
state of the art performance across 2D-3D lifting task benchmarks. Since our
approach can be trained across such a broad class of structures we refer to it
simply as a 3D Lifting Foundation Model (3D-LFM) -- the first of its kind.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11897">Text-Conditioned Resampler For Long Form Video Understanding. (arXiv:2312.11897v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Korbar_B/0/1/0/all/0/1">Bruno Korbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1">Yongqin Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1">Alessio Tonioni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a></p>
<p>Videos are highly redundant data source and it is often enough to identify a
few key moments to solve any given task. In this paper, we present a
text-conditioned video resampler (TCR) module that uses a pre-trained and
frozen visual encoder and large language model (LLM) to process long video
sequences for a task. TCR localises relevant visual features from the video
given a text condition and provides them to a LLM to generate a text response.
Due to its lightweight design and use of cross-attention, TCR can process more
than 100 frames at a time allowing the model to use much longer chunks of video
than earlier works. We make the following contributions: (i) we design a
transformer-based sampling architecture that can process long videos
conditioned on a task, together with a training method that enables it to
bridge pre-trained visual and language models; (ii) we empirically validate its
efficacy on a wide variety of evaluation tasks, and set a new state-of-the-art
on NextQA, EgoSchema, and the EGO4D-LTA challenge; and (iii) we determine tasks
which require longer video contexts and that can thus be used effectively for
further evaluation of long-range video models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11911">EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping. (arXiv:2312.11911v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1">Weipeng Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Peiyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huibin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Peng Lu</a></p>
<p>Event cameras are bio-inspired, motion-activated sensors that demonstrate
substantial potential in handling challenging situations, such as motion blur
and high-dynamic range. In this paper, we proposed EVI-SAM to tackle the
problem of 6 DoF pose tracking and 3D reconstruction using monocular event
camera. A novel event-based hybrid tracking framework is designed to estimate
the pose, leveraging the robustness of feature matching and the precision of
direct alignment. Specifically, we develop an event-based 2D-2D alignment to
construct the photometric constraint, and tightly integrate it with the
event-based reprojection constraint. The mapping module recovers the dense and
colorful depth of the scene through the image-guided event-based mapping
method. Subsequently, the appearance, texture, and surface mesh of the 3D scene
can be reconstructed by fusing the dense depth map from multiple viewpoints
using truncated signed distance function (TSDF) fusion. To the best of our
knowledge, this is the first non-learning work to realize event-based dense
mapping. Numerical evaluations are performed on both publicly available and
self-collected datasets, which qualitatively and quantitatively demonstrate the
superior performance of our method. Our EVI-SAM effectively balances accuracy
and robustness while maintaining computational efficiency, showcasing superior
pose tracking and dense mapping performance in challenging scenarios. Video
Demo: https://youtu.be/Nn40U4e5Si8.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11923">IPAD: Iterative, Parallel, and Diffusion-based Network for Scene Text Recognition. (arXiv:2312.11923v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaomeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1">Zhi Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a></p>
<p>Nowadays, scene text recognition has attracted more and more attention due to
its diverse applications. Most state-of-the-art methods adopt an
encoder-decoder framework with the attention mechanism, autoregressively
generating text from left to right. Despite the convincing performance, this
sequential decoding strategy constrains inference speed. Conversely,
non-autoregressive models provide faster, simultaneous predictions but often
sacrifice accuracy. Although utilizing an explicit language model can improve
performance, it burdens the computational load. Besides, separating linguistic
knowledge from vision information may harm the final prediction. In this paper,
we propose an alternative solution, using a parallel and iterative decoder that
adopts an easy-first decoding strategy. Furthermore, we regard text recognition
as an image-based conditional text generation task and utilize the discrete
diffusion strategy, ensuring exhaustive exploration of bidirectional contextual
information. Extensive experiments demonstrate that the proposed approach
achieves superior results on the benchmark datasets, including both Chinese and
English text images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11929">Transformer Network for Multi-Person Tracking and Re-Identification in Unconstrained Environment. (arXiv:2312.11929v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukhtar_H/0/1/0/all/0/1">Hamza Mukhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Muhammad Usman Ghani Khan</a></p>
<p>Multi-object tracking (MOT) has profound applications in a variety of fields,
including surveillance, sports analytics, self-driving, and cooperative
robotics. Despite considerable advancements, existing MOT methodologies tend to
falter when faced with non-uniform movements, occlusions, and
appearance-reappearance scenarios of the objects. Recognizing this inadequacy,
we put forward an integrated MOT method that not only marries object detection
and identity linkage within a singular, end-to-end trainable framework but also
equips the model with the ability to maintain object identity links over long
periods of time. Our proposed model, named STMMOT, is built around four key
modules: 1) candidate proposal generation, which generates object proposals via
a vision-transformer encoder-decoder architecture that detects the object from
each frame in the video; 2) scale variant pyramid, a progressive pyramid
structure to learn the self-scale and cross-scale similarities in multi-scale
feature maps; 3) spatio-temporal memory encoder, extracting the essential
information from the memory associated with each object under tracking; and 4)
spatio-temporal memory decoder, simultaneously resolving the tasks of object
detection and identity association for MOT. Our system leverages a robust
spatio-temporal memory module that retains extensive historical observations
and effectively encodes them using an attention-based aggregator. The
uniqueness of STMMOT lies in representing objects as dynamic query embeddings
that are updated continuously, which enables the prediction of object states
with attention mechanisms and eradicates the need for post-processing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11938">DMT: Comprehensive Distillation with Multiple Self-supervised Teachers. (arXiv:2312.11938v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a></p>
<p>Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, have been proposed to acquire powerful and general
representations from unlabeled data. However, these models are commonly
pretrained within their specific framework alone, failing to consider the
complementary nature of visual representations. To tackle this issue, we
introduce Comprehensive Distillation with Multiple Self-supervised Teachers
(DMT) for pretrained model compression, which leverages the strengths of
multiple off-the-shelf self-supervised models. Our experimental results on
prominent benchmark datasets exhibit that the proposed method significantly
surpasses state-of-the-art competitors while retaining favorable efficiency
metrics. On classification tasks, our DMT framework utilizing three different
self-supervised ViT-Base teachers enhances the performance of both small/tiny
models and the base model itself. For dense tasks, DMT elevates the AP/mIoU of
standard SSL models on MS-COCO and ADE20K datasets by 4.0%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11954">Adversarial AutoMixup. (arXiv:2312.11954v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Huafeng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Yacoubi_M/0/1/0/all/0/1">Mounim A. El-Yacoubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Data mixing augmentation has been widely applied to improve the
generalization ability of deep neural networks. Recently, offline data mixing
augmentation, e.g. handcrafted and saliency information-based mixup, has been
gradually replaced by automatic mixing approaches. Through minimizing two
sub-tasks, namely, mixed sample generation and mixup classification in an
end-to-end way, AutoMix significantly improves accuracy on image classification
tasks. However, as the optimization objective is consistent for the two
sub-tasks, this approach is prone to generating consistent instead of diverse
mixed samples, which results in overfitting for target task training. In this
paper, we propose AdAutomixup, an adversarial automatic mixup augmentation
approach that generates challenging samples to train a robust classifier for
image classification, by alternatively optimizing the classifier and the mixup
sample generator. AdAutomixup comprises two modules, a mixed example generator,
and a target classifier. The mixed sample generator aims to produce hard mixed
examples to challenge the target classifier while the target classifier`s aim
is to learn robust features from hard mixed examples to improve generalization.
To prevent the collapse of the inherent meanings of images, we further
introduce an exponential moving average (EMA) teacher and cosine similarity to
train AdAutomixup in an end-to-end way. Extensive experiments on seven image
benchmarks consistently prove that our approach outperforms the state of the
art in various classification scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11967">Context Disentangling and Prototype Inheriting for Robust Visual Grounding. (arXiv:2312.11967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1">Wei Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuejing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jinhui Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zechao Li</a></p>
<p>Visual grounding (VG) aims to locate a specific target in an image based on a
given language query. The discriminative information from context is important
for distinguishing the target from other objects, particularly for the targets
that have the same category as others. However, most previous methods
underestimate such information. Moreover, they are usually designed for the
standard scene (without any novel object), which limits their generalization to
the open-vocabulary scene. In this paper, we propose a novel framework with
context disentangling and prototype inheriting for robust visual grounding to
handle both scenes. Specifically, the context disentangling disentangles the
referent and context features, which achieves better discrimination between
them. The prototype inheriting inherits the prototypes discovered from the
disentangled visual features by a prototype bank to fully utilize the seen
data, especially for the open-vocabulary scene. The fused features, obtained by
leveraging Hadamard product on disentangled linguistic and visual features of
prototypes to avoid sharp adjusting the importance between the two types of
features, are then attached with a special token and feed to a vision
Transformer encoder for bounding box regression. Extensive experiments are
conducted on both standard and open-vocabulary scenes. The performance
comparisons indicate that our method outperforms the state-of-the-art methods
in both scenarios. {The code is available at
https://github.com/WayneTomas/TransCP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11972">Expressive Forecasting of 3D Whole-body Human Motions. (arXiv:2312.11972v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1">Pengxiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qiongjie Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haofan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>Human motion forecasting, with the goal of estimating future human behavior
over a period of time, is a fundamental task in many real-world applications.
However, existing works typically concentrate on predicting the major joints of
the human body without considering the delicate movements of the human hands.
In practical applications, hand gesture plays an important role in human
communication with the real world, and expresses the primary intention of human
beings. In this work, we are the first to formulate a whole-body human pose
forecasting task, which jointly predicts the future body and hand activities.
Correspondingly, we propose a novel Encoding-Alignment-Interaction (EAI)
framework that aims to predict both coarse (body joints) and fine-grained
(gestures) activities collaboratively, enabling expressive and
cross-facilitated forecasting of 3D whole-body human motions. Specifically, our
model involves two key constituents: cross-context alignment (XCA) and
cross-context interaction (XCI). Considering the heterogeneous information
within the whole-body, XCA aims to align the latent features of various human
components, while XCI focuses on effectively capturing the context interaction
among the human components. We conduct extensive experiments on a
newly-introduced large-scale benchmark and achieve state-of-the-art
performance. The code is public for research purposes at
https://github.com/Dingpx/EAI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11973">Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Haeyong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>Inspired by the Regularized Lottery Ticket Hypothesis (RLTH), which
highlights the presence of competitive subnetworks within dense networks for
continual learning tasks, we introduce Winning Subnetworks (WSN). This approach
utilizes reused weights in dense networks to enhance learning in Task
Incremental Learning (TIL) scenarios. To mitigate overfitting in Few-Shot Class
Incremental Learning (FSCIL), we have developed WSN variants referred to as the
Soft subnetwork (SoftNet). Furthermore, addressing WSN's limitation of sparse
reused weights in Video Incremental Learning (VIL), we propose the Fourier
Subneural Operator (FSO). The FSO, operating in Fourier space, adaptively and
compactly encodes videos, discovering reusable subnetworks with diverse
bandwidths. We have applied FSO's Fourier representations to various continual
learning contexts, including VIL, TIL, and FSCIL. Our extensive experiments
across these scenarios demonstrate FSO's remarkable efficacy in continual
learning, significantly enhancing task performance at various convolutional
representational levels: it boosts performance in the higher layers for TIL and
FSCIL and the lower layers for VIL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11994">Optimizing Diffusion Noise Can Serve As Universal Motion Priors. (arXiv:2312.11994v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karunratanakul_K/0/1/0/all/0/1">Korrawe Karunratanakul</a>, <a href="http://arxiv.org/find/cs/1/au:+Preechakul_K/0/1/0/all/0/1">Konpat Preechakul</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1">Emre Aksan</a>, <a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1">Thabo Beeler</a>, <a href="http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1">Supasorn Suwajanakorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siyu Tang</a></p>
<p>We propose Diffusion Noise Optimization (DNO), a new method that effectively
leverages existing motion diffusion models as motion priors for a wide range of
motion-related tasks. Instead of training a task-specific diffusion model for
each new task, DNO operates by optimizing the diffusion latent noise of an
existing pre-trained text-to-motion model. Given the corresponding latent noise
of a human motion, it propagates the gradient from the target criteria defined
on the motion space through the whole denoising process to update the diffusion
latent noise. As a result, DNO supports any use cases where criteria can be
defined as a function of motion. In particular, we show that, for motion
editing and control, DNO outperforms existing methods in both achieving the
objective and preserving the motion content. DNO accommodates a diverse range
of editing modes, including changing trajectory, pose, joint locations, or
avoiding newly added obstacles. In addition, DNO is effective in motion
denoising and completion, producing smooth and realistic motion from noisy and
partial inputs. DNO achieves these results at inference time without the need
for model retraining, offering great versatility for any defined reward or loss
function on the motion representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12000">Diffusing More Objects for Semi-Supervised Domain Adaptation with Less Labeling. (arXiv:2312.12000v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Heuvel_L/0/1/0/all/0/1">Leander van den Heuvel</a>, <a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1">Gertjan Burghouts</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">David W. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Englebienne_G/0/1/0/all/0/1">Gwenn Englebienne</a>, <a href="http://arxiv.org/find/cs/1/au:+Rooij_S/0/1/0/all/0/1">Sabina B. van Rooij</a></p>
<p>For object detection, it is possible to view the prediction of bounding boxes
as a reverse diffusion process. Using a diffusion model, the random bounding
boxes are iteratively refined in a denoising step, conditioned on the image. We
propose a stochastic accumulator function that starts each run with random
bounding boxes and combines the slightly different predictions. We empirically
verify that this improves detection performance. The improved detections are
leveraged on unlabelled images as weighted pseudo-labels for semi-supervised
learning. We evaluate the method on a challenging out-of-domain test set. Our
method brings significant improvements and is on par with human-selected
pseudo-labels, while not requiring any human involvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12023">Progressive Frequency-Aware Network for Laparoscopic Image Desmoking. (arXiv:2312.12023v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jiale Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1">Wenfeng Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_X/0/1/0/all/0/1">Xiangyun Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1">Qiong Wang</a></p>
<p>Laparoscopic surgery offers minimally invasive procedures with better patient
outcomes, but smoke presence challenges visibility and safety. Existing
learning-based methods demand large datasets and high computational resources.
We propose the Progressive Frequency-Aware Network (PFAN), a lightweight GAN
framework for laparoscopic image desmoking, combining the strengths of CNN and
Transformer for progressive information extraction in the frequency domain.
PFAN features CNN-based Multi-scale Bottleneck-Inverting (MBI) Blocks for
capturing local high-frequency information and Locally-Enhanced Axial Attention
Transformers (LAT) for efficiently handling global low-frequency information.
PFAN efficiently desmokes laparoscopic images even with limited training data.
Our method outperforms state-of-the-art approaches in PSNR, SSIM, CIEDE2000,
and visual quality on the Cholec80 dataset and retains only 629K parameters.
Our code and models are made publicly available at:
https://github.com/jlzcode/PFAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12028">EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Siamul Karim Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tinsley_P/0/1/0/all/0/1">Patrick Tinsley</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitcheff_M/0/1/0/all/0/1">Mahsa Mitcheff</a>, <a href="http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1">Patrick Flynn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1">Kevin W. Bowyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1">Adam Czajka</a></p>
<p>Synthesis of same-identity biometric iris images, both for existing and
non-existing identities while preserving the identity across a wide range of
pupil sizes, is complex due to intricate iris muscle constriction mechanism,
requiring a precise model of iris non-linear texture deformations to be
embedded into the synthesis pipeline. This paper presents the first method of
fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris
images. This approach is capable of synthesizing images of irises with
different pupil sizes representing non-existing identities as well as
non-linearly deforming the texture of iris images of existing subjects given
the segmentation mask of the target iris image. Iris recognition experiments
suggest that the proposed deformation model not only preserves the identity
when changing the pupil size but offers better similarity between same-identity
iris samples with significant differences in pupil size, compared to
state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation
models. Two immediate applications of the proposed approach are: (a) synthesis
of, or enhancement of the existing biometric datasets for iris recognition,
mimicking those acquired with iris sensors, and (b) helping forensic human
experts in examining iris image pairs with significant differences in pupil
dilation. Source codes and weights of the models are made available with the
paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12030">Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method. (arXiv:2312.12030v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiachun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hanshu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1">Jun Hao Liew</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1">Vincent Y. F. Tan</a></p>
<p>Training-free guided sampling in diffusion models leverages off-the-shelf
pre-trained networks, such as an aesthetic evaluation model, to guide the
generation process. Current training-free guided sampling algorithms obtain the
guidance energy function based on a one-step estimate of the clean image.
However, since the off-the-shelf pre-trained networks are trained on clean
images, the one-step estimation procedure of the clean image may be inaccurate,
especially in the early stages of the generation process in diffusion models.
This causes the guidance in the early time steps to be inaccurate. To overcome
this problem, we propose Symplectic Adjoint Guidance (SAG), which calculates
the gradient guidance in two inner stages. Firstly, SAG estimates the clean
image via $n$ function calls, where $n$ serves as a flexible hyperparameter
that can be tailored to meet specific image quality requirements. Secondly, SAG
uses the symplectic adjoint method to obtain the gradients accurately and
efficiently in terms of the memory requirements. Extensive experiments
demonstrate that SAG generates images with higher qualities compared to the
baselines in both guided image and video generation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12042">Pose2Gaze: Generating Realistic Human Gaze Behaviour from Full-body Poses using an Eye-body Coordination Model. (arXiv:2312.12042v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiahui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1">Syn Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulling_A/0/1/0/all/0/1">Andreas Bulling</a></p>
<p>While generating realistic body movements, e.g., for avatars in virtual
reality, is widely studied in computer vision and graphics, the generation of
eye movements that exhibit realistic coordination with the body remains
under-explored. We first report a comprehensive analysis of the coordination of
human eye and full-body movements during everyday activities based on data from
the MoGaze and GIMO datasets. We show that eye gaze has strong correlations
with head directions and also full-body motions and there exists a noticeable
time delay between body and eye movements. Inspired by the analyses, we then
present Pose2Gaze -- a novel eye-body coordination model that first uses a
convolutional neural network and a spatio-temporal graph convolutional neural
network to extract features from head directions and full-body poses
respectively and then applies a convolutional neural network to generate
realistic eye movements. We compare our method with state-of-the-art methods
that predict eye gaze only from head movements for three different generation
tasks and demonstrate that Pose2Gaze significantly outperforms these baselines
on both datasets with an average improvement of 26.4% and 21.6% in mean angular
error, respectively. Our findings underline the significant potential of
cross-modal human gaze behaviour analysis and modelling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.10985">Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding. (arXiv:2112.10985v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kailun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yiwen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changshui Zhang</a></p>
<p>Drawing on theoretical insights, we advocate an error-based thresholding
(EBT) mechanism for learned ISTA (LISTA), which utilizes a function of the
layer-wise reconstruction error to suggest a specific threshold for each
observation in the shrinkage function of each layer. We show that the proposed
EBT mechanism well disentangles the learnable parameters in the shrinkage
functions from the reconstruction errors, endowing the obtained models with
improved adaptivity to possible data variations. With rigorous analyses, we
further show that the proposed EBT also leads to a faster convergence on the
basis of LISTA or its variants, in addition to its higher adaptivity. Extensive
experimental results confirm our theoretical analyses and verify the
effectiveness of our methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.03005">Self-Supervised Face Image Restoration with a One-Shot Reference. (arXiv:2203.03005v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yanhui Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fangzhou Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shaoyuan Xu</a></p>
<p>For image restoration, methods leveraging priors from generative models have
been proposed and demonstrated a promising capacity to robustly restore
photorealistic and high-quality results. However, these methods are susceptible
to semantic ambiguity, particularly with images that have obviously correct
semantics such as facial images. In this paper, we propose a semantic-aware
latent space exploration method for image restoration (SAIR). By explicitly
modeling semantics information from a given reference image, SAIR is able to
reliably restore severely degraded images not only to high-resolution and
highly realistic looks but also to correct semantics. Quantitative and
qualitative experiments collectively demonstrate the superior performance of
the proposed SAIR. Our code is available at https://github.com/Liamkuo/SAIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.16284">FIRe: Fast Inverse Rendering using Directional and Signed Distance Functions. (arXiv:2203.16284v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yenamandra_T/0/1/0/all/0/1">Tarun Yenamandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1">Ayush Tewari</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1">Nan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1">Florian Bernard</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>Neural 3D implicit representations learn priors that are useful for diverse
applications, such as single- or multiple-view 3D reconstruction. A major
downside of existing approaches while rendering an image is that they require
evaluating the network multiple times per camera ray so that the high
computational time forms a bottleneck for downstream applications. We address
this problem by introducing a novel neural scene representation that we call
the directional distance function (DDF). To this end, we learn a signed
distance function (SDF) along with our DDF model to represent a class of
shapes. Specifically, our DDF is defined on the unit sphere and predicts the
distance to the surface along any given direction. Therefore, our DDF allows
rendering images with just a single network evaluation per camera ray. Based on
our DDF, we present a novel fast algorithm (FIRe) to reconstruct 3D shapes
given a posed depth map. We evaluate our proposed method on 3D reconstruction
from single-view depth images, where we empirically show that our algorithm
reconstructs 3D shapes more accurately and it is more than 15 times faster (per
iteration) than competing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.16557">COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical Image Segmentation based on Target-aware Domain Translation and Iterative Self-Training. (arXiv:2203.16557v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1">Hyungseob Shin</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1">Hyeongyu Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Sewon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Jun_Y/0/1/0/all/0/1">Yohan Jun</a>, <a href="http://arxiv.org/find/eess/1/au:+Eo_T/0/1/0/all/0/1">Taejoon Eo</a>, <a href="http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1">Dosik Hwang</a></p>
<p>Recent advances in deep learning-based medical image segmentation studies
achieve nearly human-level performance when in fully supervised condition.
However, acquiring pixel-level expert annotations is extremely expensive and
laborious in medical imaging fields. Unsupervised domain adaptation can
alleviate this problem, which makes it possible to use annotated data in one
imaging modality to train a network that can successfully perform segmentation
on target imaging modality with no labels. In this work, we propose a
self-training based unsupervised domain adaptation framework for 3D medical
image segmentation named COSMOS and validate it with automatic segmentation of
Vestibular Schwannoma (VS) and cochlea on high-resolution T2 Magnetic Resonance
Images (MRI). Our target-aware contrast conversion network translates source
domain annotated T1 MRI to pseudo T2 MRI to enable segmentation training on
target domain, while preserving important anatomical features of interest in
the converted images. Iterative self-training is followed to incorporate
unlabeled data to training and incrementally improve the quality of
pseudo-labels, thereby leading to improved performance of segmentation. COSMOS
won the 1\textsuperscript{st} place in the Cross-Modality Domain Adaptation
(crossMoDA) challenge held in conjunction with the 24th International
Conference on Medical Image Computing and Computer Assisted Intervention
(MICCAI 2021). It achieves mean Dice score and Average Symmetric Surface
Distance of 0.871(0.063) and 0.437(0.270) for VS, and 0.842(0.020) and
0.152(0.030) for cochlea.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.03553">From heavy rain removal to detail restoration: A faster and better network. (arXiv:2205.03553v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yuanbo Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a></p>
<p>The profound accumulation of precipitation during intense rainfall events can
markedly degrade the quality of images, leading to the erosion of textural
details. Despite the improvements observed in existing learning-based methods
specialized for heavy rain removal, it is discerned that a significant
proportion of these methods tend to overlook the precise reconstruction of the
intricate details. In this work, we introduce a simple dual-stage progressive
enhancement network, denoted as DPENet, aiming to achieve effective deraining
while preserving the structural accuracy of rain-free images. This approach
comprises two key modules, a rain streaks removal network (R$^2$Net) focusing
on accurate rain removal, and a details reconstruction network (DRNet) designed
to recover the textural details of rain-free images. Firstly, we introduce a
dilated dense residual block (DDRB) within R$^2$Net, enabling the aggregation
of high-level and low-level features. Secondly, an enhanced residual pixel-wise
attention block (ERPAB) is integrated into DRNet to facilitate the
incorporation of contextual information. To further enhance the fidelity of our
approach, we employ a comprehensive loss function that accentuates both the
marginal and regional accuracy of rain-free images. Extensive experiments
conducted on publicly available benchmarks demonstrates the noteworthy
efficiency and effectiveness of our proposed DPENet. The source code and
pre-trained models are currently available at
\url{https://github.com/chdwyb/DPENet}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15677">Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Liang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yige Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Songtao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Siyuan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a></p>
<p>Training generative adversarial networks (GANs) with limited data is
challenging because the discriminator is prone to overfitting. Previously
proposed differentiable augmentation demonstrates improved data efficiency of
training GANs. However, the augmentation implicitly introduces undesired
invariance to augmentation for the discriminator since it ignores the change of
semantics in the label space caused by data transformation, which may limit the
representation learning ability of the discriminator and ultimately affect the
generative modeling performance of the generator. To mitigate the negative
impact of invariance while inheriting the benefits of data augmentation, we
propose a novel augmentation-aware self-supervised discriminator that predicts
the augmentation parameter of the augmented data. Particularly, the prediction
targets of real data and generated data are required to be distinguished since
they are different during training. We further encourage the generator to
adversarially learn from the self-supervised discriminator by generating
augmentation-predictable real and not fake data. This formulation connects the
learning objective of the generator and the arithmetic $-$ harmonic mean
divergence under certain assumptions. We compare our method with
state-of-the-art (SOTA) methods using the class-conditional BigGAN and
unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,
FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate
significant improvements of our method over SOTA methods in training
data-efficient GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.14513">Uncertainty-Driven Action Quality Assessment. (arXiv:2207.14513v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Caixia Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yaping Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Haibin Ling</a></p>
<p>Automatic action quality assessment (AQA) has attracted increasing attention
due to its wide applications. However, most existing AQA methods employ
deterministic models to predict the final score for each action, while
overlooking the subjectivity and diversity among expert judges during the
scoring process. In this paper, we propose a novel probabilistic model, named
Uncertainty-Driven AQA (UD-AQA), to utilize and capture the diversity among
multiple judge scores. Specifically, we design a Conditional Variational
Auto-Encoder (CVAE)-based module to encode the uncertainty in expert
assessment, where multiple judge scores can be produced by sampling latent
features from the learned latent space multiple times. To further utilize the
uncertainty, we generate the estimation of uncertainty for each prediction,
which is employed to re-weight AQA regression loss, effectively reducing the
influence of uncertain samples during training. Moreover, we further design an
uncertainty-guided training strategy to dynamically adjust the learning order
of the samples from low uncertainty to high uncertainty. The experiments show
that our proposed method achieves competitive results on three benchmarks
including the Olympic events MTL-AQA and FineDiving, and the surgical skill
JIGSAWS datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.14197">PointVST: Self-Supervised Pre-training for 3D Point Clouds via View-Specific Point-to-Image Translation. (arXiv:2212.14197v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qijian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a></p>
<p>The past few years have witnessed the great success and prevalence of
self-supervised representation learning within the language and 2D vision
communities. However, such advancements have not been fully migrated to the
field of 3D point cloud learning. Different from existing pre-training
paradigms designed for deep point cloud feature extractors that fall into the
scope of generative modeling or contrastive learning, this paper proposes a
translative pre-training framework, namely PointVST, driven by a novel
self-supervised pretext task of cross-modal translation from 3D point clouds to
their corresponding diverse forms of 2D rendered images. More specifically, we
begin with deducing view-conditioned point-wise embeddings through the
insertion of the viewpoint indicator, and then adaptively aggregate a
view-specific global codeword, which can be further fed into subsequent 2D
convolutional translation heads for image generation. Extensive experimental
evaluations on various downstream task scenarios demonstrate that our PointVST
shows consistent and prominent performance superiority over current
state-of-the-art approaches as well as satisfactory domain transfer capability.
Our code will be publicly available at https://github.com/keeganhk/PointVST.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11145">Learning from Mistakes: Self-Regularizing Hierarchical Representations in Point Cloud Semantic Segmentation. (arXiv:2301.11145v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Camuffo_E/0/1/0/all/0/1">Elena Camuffo</a>, <a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1">Umberto Michieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1">Simone Milani</a></p>
<p>Recent advances in autonomous robotic technologies have highlighted the
growing need for precise environmental analysis. LiDAR semantic segmentation
has gained attention to accomplish fine-grained scene understanding by acting
directly on raw content provided by sensors. Recent solutions showed how
different learning techniques can be used to improve the performance of the
model, without any architectural or dataset change. Following this trend, we
present a coarse-to-fine setup that LEArns from classification mistaKes (LEAK)
derived from a standard model. First, classes are clustered into macro groups
according to mutual prediction errors; then, the learning process is
regularized by: (1) aligning class-conditional prototypical feature
representation for both fine and coarse classes, (2) weighting instances with a
per-class fairness index. Our LEAK approach is very general and can be
seamlessly applied on top of any segmentation architecture; indeed,
experimental results showed that it enables state-of-the-art performances on
different architectures, datasets and tasks, while ensuring more balanced
class-wise results and faster convergence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04977">Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines. (arXiv:2302.04977v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1">Eugene Bagdasaryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1">Vitaly Shmatikov</a></p>
<p>Machine learning (ML) models trained on data from potentially untrusted
sources are vulnerable to poisoning. A small, maliciously crafted subset of the
training inputs can cause the model to learn a "backdoor" task (e.g.,
misclassify inputs with a certain feature) in addition to its main task. Recent
research proposed many hypothetical backdoor attacks whose efficacy heavily
depends on the configuration and training hyperparameters of the target model.
</p>
<p>Given the variety of potential backdoor attacks, ML engineers who are not
security experts have no way to measure how vulnerable their current training
pipelines are, nor do they have a practical way to compare training
configurations so as to pick the more resistant ones. Deploying a defense
requires evaluating and choosing from among dozens of research papers and
re-engineering the training pipeline.
</p>
<p>In this paper, we aim to provide ML engineers with pragmatic tools to audit
the backdoor resistance of their training pipelines and to compare different
training configurations, to help choose one that best balances accuracy and
security.
</p>
<p>First, we propose a universal, attack-agnostic resistance metric based on the
minimum number of training inputs that must be compromised before the model
learns any backdoor.
</p>
<p>Second, we design, implement, and evaluate Mithridates a multi-stage approach
that integrates backdoor resistance into the training-configuration search. ML
developers already rely on hyperparameter search to find configurations that
maximize the model's accuracy. Mithridates extends this standard tool to
balance accuracy and resistance without disruptive changes to the training
pipeline. We show that hyperparameters found by Mithridates increase resistance
to multiple types of backdoor attacks by 3-5x with only a slight impact on
accuracy. We also discuss extensions to AutoML and federated learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14696">Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection. (arXiv:2302.14696v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ni Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghazzai_H/0/1/0/all/0/1">Hakim Ghazzai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1">Peter Wonka</a></p>
<p>In this paper, we introduce \textit{DIA}, dissolving is amplifying. DIA is a
fine-grained anomaly detection framework for medical images. We describe two
novel components in the paper. First, we introduce \textit{dissolving
transformations}. Our main observation is that generative diffusion models are
feature-aware and applying them to medical images in a certain manner can
remove or diminish fine-grained discriminative features such as tumors or
hemorrhaging. Second, we introduce an \textit{amplifying framework} based on
contrastive learning to learn a semantically meaningful representation of
medical images in a self-supervised manner. The amplifying framework contrasts
additional pairs of images with and without dissolving transformations applied
and thereby boosts the learning of fine-grained feature representations. DIA
significantly improves the medical anomaly detection performance with around
18.40\% AUC boost against the baseline method and achieves an overall SOTA
against other benchmark methods. Our code is available at
\url{https://github.com/shijianjian/DIA.git}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06999">Identifying Label Errors in Object Detection Datasets by Loss Inspection. (arXiv:2303.06999v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1">Marius Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1">Tobias Riedlinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahl_K/0/1/0/all/0/1">Karsten Kahl</a>, <a href="http://arxiv.org/find/cs/1/au:+Kroll_D/0/1/0/all/0/1">Daniel Kr&#xf6;ll</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoenen_S/0/1/0/all/0/1">Sebastian Schoenen</a>, <a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1">Sini&#x161;a &#x160;egvi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1">Matthias Rottmann</a></p>
<p>Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages' classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector's score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., we detect label errors with a
precision for a) of up to 71.5% and for b) with 97%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08906">VVS: Video-to-Video Retrieval with Irrelevant Frame Suppression. (arXiv:2303.08906v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jo_W/0/1/0/all/0/1">Won Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1">Geuntaek Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gwangjin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1">Byungsoo Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yukyung Choi</a></p>
<p>In content-based video retrieval (CBVR), dealing with large-scale
collections, efficiency is as important as accuracy; thus, several video-level
feature-based studies have actively been conducted. Nevertheless, owing to the
severe difficulty of embedding a lengthy and untrimmed video into a single
feature, these studies have been insufficient for accurate retrieval compared
to frame-level feature-based studies. In this paper, we show that appropriate
suppression of irrelevant frames can provide insight into the current obstacles
of the video-level approaches. Furthermore, we propose a Video-to-Video
Suppression network (VVS) as a solution. VVS is an end-to-end framework that
consists of an easy distractor elimination stage to identify which frames to
remove and a suppression weight generation stage to determine the extent to
suppress the remaining frames. This structure is intended to effectively
describe an untrimmed video with varying content and meaningless information.
Its efficacy is proved via extensive experiments, and we show that our approach
is not only state-of-the-art in video-level approaches but also has a fast
inference time despite possessing retrieval capabilities close to those of
frame-level approaches. Code is available at https://github.com/sejong-rcv/VVS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10343">Supervision Interpolation via LossMix: Generalizing Mixup for Object Detection and Beyond. (arXiv:2303.10343v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Thanh Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baochen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Bodi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngai_A/0/1/0/all/0/1">Alex Ngai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yueqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Frahm_J/0/1/0/all/0/1">Jan-Michael Frahm</a></p>
<p>The success of data mixing augmentations in image classification tasks has
been well-received. However, these techniques cannot be readily applied to
object detection due to challenges such as spatial misalignment,
foreground/background distinction, and plurality of instances. To tackle these
issues, we first introduce a novel conceptual framework called Supervision
Interpolation (SI), which offers a fresh perspective on interpolation-based
augmentations by relaxing and generalizing Mixup. Based on SI, we propose
LossMix, a simple yet versatile and effective regularization that enhances the
performance and robustness of object detectors and more. Our key insight is
that we can effectively regularize the training on mixed data by interpolating
their loss errors instead of ground truth labels. Empirical results on the
PASCAL VOC and MS COCO datasets demonstrate that LossMix can consistently
outperform state-of-the-art methods widely adopted for detection. Furthermore,
by jointly leveraging LossMix with unsupervised domain adaptation, we
successfully improve existing approaches and set a new state of the art for
cross-domain object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11681">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models. (arXiv:2303.11681v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a></p>
<p>Collecting and annotating images with pixel-wise labels is time-consuming and
laborious. In contrast, synthetic data can be freely available using a
generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that
it is possible to automatically obtain accurate semantic masks of synthetic
images generated by the Off-the-shelf Stable Diffusion model, which uses only
text-image pairs during training. Our approach, called DiffuMask, exploits the
potential of the cross-attention map between text and image, which is natural
and seamless to extend the text-driven image synthesis to semantic mask
generation. DiffuMask uses text-guided cross-attention information to localize
class/word-specific regions, which are combined with practical techniques to
create a novel high-resolution and class-discriminative pixel-wise mask. The
methods help to reduce data collection and annotation costs obviously.
Experiments demonstrate that the existing segmentation methods trained on
synthetic data of DiffuMask can achieve a competitive performance over the
counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),
DiffuMask presents promising performance, close to the stateof-the-art result
of real data (within 3% mIoU gap). Moreover, in the open-vocabulary
segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on
Unseen class of VOC 2012. The project website can be found at
https://weijiawu.github.io/DiffusionMask/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14027">Poincar\&#x27;e ResNet. (arXiv:2303.14027v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spengler_M/0/1/0/all/0/1">Max van Spengler</a>, <a href="http://arxiv.org/find/cs/1/au:+Berkhout_E/0/1/0/all/0/1">Erwin Berkhout</a>, <a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1">Pascal Mettes</a></p>
<p>This paper introduces an end-to-end residual network that operates entirely
on the Poincar\'e ball model of hyperbolic space. Hyperbolic learning has
recently shown great potential for visual understanding, but is currently only
performed in the penultimate layer(s) of deep networks. All visual
representations are still learned through standard Euclidean networks. In this
paper we investigate how to learn hyperbolic representations of visual data
directly from the pixel-level. We propose Poincar\'e ResNet, a hyperbolic
counterpart of the celebrated residual network, starting from Poincar\'e 2D
convolutions up to Poincar\'e residual connections. We identify three
roadblocks for training convolutional networks entirely in hyperbolic space and
propose a solution for each: (i) Current hyperbolic network initializations
collapse to the origin, limiting their applicability in deeper networks. We
provide an identity-based initialization that preserves norms over many layers.
(ii) Residual networks rely heavily on batch normalization, which comes with
expensive Fr\'echet mean calculations in hyperbolic space. We introduce
Poincar\'e midpoint batch normalization as a faster and equally effective
alternative. (iii) Due to the many intermediate operations in Poincar\'e
layers, we lastly find that the computation graphs of deep learning libraries
blow up, limiting our ability to train on deep hyperbolic networks. We provide
manual backward derivations of core hyperbolic operations to maintain
manageable computation graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14628">Multi-Frame Self-Supervised Depth Estimation with Multi-Scale Feature Fusion in Dynamic Scenes. (arXiv:2303.14628v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1">Jiquan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaolin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiao Yu</a></p>
<p>Multi-frame methods improve monocular depth estimation over single-frame
approaches by aggregating spatial-temporal information via feature matching.
However, the spatial-temporal feature leads to accuracy degradation in dynamic
scenes. To enhance the performance, recent methods tend to propose complex
architectures for feature matching and dynamic scenes. In this paper, we show
that a simple learning framework, together with designed feature augmentation,
leads to superior performance. (1) A novel dynamic objects detecting method
with geometry explainability is proposed. The detected dynamic objects are
excluded during training, which guarantees the static environment assumption
and relieves the accuracy degradation problem of the multi-frame depth
estimation. (2) Multi-scale feature fusion is proposed for feature matching in
the multi-frame depth network, which improves feature matching, especially
between frames with large camera motion. (3) The robust knowledge distillation
with a robust teacher network and reliability guarantee is proposed, which
improves the multi-frame depth estimation without computation complexity
increase during the test. The experiments show that our proposed methods
achieve great performance improvement on the multi-frame depth estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17338">Local region-learning modules for point cloud classification. (arXiv:2303.17338v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turgut_K/0/1/0/all/0/1">Kaya Turgut</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutagaci_H/0/1/0/all/0/1">Helin Dutagaci</a></p>
<p>Data organization via forming local regions is an integral part of deep
learning networks that process 3D point clouds in a hierarchical manner. At
each level, the point cloud is sampled to extract representative points and
these points are used to be centers of local regions. The organization of local
regions is of considerable importance since it determines the location and size
of the receptive field at a particular layer of feature aggregation. In this
paper, we present two local region-learning modules: Center Shift Module to
infer the appropriate shift for each center point, and Radius Update Module to
alter the radius of each local region. The parameters of the modules are
learned through optimizing the loss associated with the particular task within
an end-to-end network. We present alternatives for these modules through
various ways of modeling the interactions of the features and locations of 3D
points in the point cloud. We integrated both modules independently and
together to the PointNet++ and PointCNN object classification architectures,
and demonstrated that the modules contributed to a significant increase in
classification accuracy for the ScanObjectNN data set consisting of scans of
real-world objects. Our further experiments on ShapeNet data set showed that
the modules are also effective on 3D CAD models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06252">Embedded Feature Similarity Optimization with Specific Parameter Initialization for 2D/3D Medical Image Registration. (arXiv:2305.06252v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhirun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shuheng Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1">Youyong Kong</a></p>
<p>We present a novel deep learning-based framework: Embedded Feature Similarity
Optimization with Specific Parameter Initialization (SOPI) for 2D/3D medical
image registration which is a most challenging problem due to the difficulty
such as dimensional mismatch, heavy computation load and lack of golden
evaluation standard. The framework we design includes a parameter specification
module to efficiently choose initialization pose parameter and a
fine-registration module to align images. The proposed framework takes
extracting multi-scale features into consideration using a novel composite
connection encoder with special training techniques. We compare the method with
both learning-based methods and optimization-based methods on a in-house
CT/X-ray dataset as well as simulated data to further evaluate performance. Our
experiments demonstrate that the method in this paper has improved the
registration performance, and thereby outperforms the existing methods in terms
of accuracy and running time. We also show the potential of the proposed method
as an initial pose estimator. The code is available at
https://github.com/m1nhengChen/SOPI
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07490">ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter. (arXiv:2305.07490v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zhengqing Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yanfang Ye</a></p>
<p>In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists' descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10701">Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models. (arXiv:2305.10701v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1">Felix Juefei-Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yutong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Ming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">Geguang Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Although recent personalization methods have democratized high-resolution
image synthesis by enabling swift concept acquisition with minimal examples and
lightweight computation, they also present an exploitable avenue for high
accessible backdoor attacks. This paper investigates a critical and unexplored
aspect of text-to-image (T2I) diffusion models - their potential vulnerability
to backdoor attacks via personalization. Our study focuses on a zero-day
backdoor vulnerability prevalent in two families of personalization methods,
epitomized by Textual Inversion and DreamBooth.Compared to traditional backdoor
attacks, our proposed method can facilitate more precise, efficient, and easily
accessible attacks with a lower barrier to entry. We provide a comprehensive
review of personalization in T2I diffusion models, highlighting the operation
and exploitation potential of this backdoor vulnerability. To be specific, by
studying the prompt processing of Textual Inversion and DreamBooth, we have
devised dedicated backdoor attacks according to the different ways of dealing
with unseen tokens and analyzed the influence of triggers and concept images on
the attack effect. Through comprehensive empirical study, we endorse the
utilization of the nouveau-token backdoor attack due to its impressive
effectiveness, stealthiness, and integrity, markedly outperforming the
legacy-token backdoor attack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12015">Inventing art styles with no artistic training data. (arXiv:2305.12015v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abrahamsen_N/0/1/0/all/0/1">Nilin Abrahamsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiahao Yao</a></p>
<p>We propose two procedures to create painting styles using models trained only
on natural images, providing objective proof that the model is not plagiarizing
human art styles. In the first procedure we use the inductive bias from the
artistic medium to achieve creative expression. Abstraction is achieved by
using a reconstruction loss. The second procedure uses an additional natural
image as inspiration to create a new style. These two procedures make it
possible to invent new painting styles with no artistic training data. We
believe that our approach can help pave the way for the ethical employment of
generative AI in art, without infringing upon the originality of human
creators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18072">Image Captioning with Multi-Context Synthetic Data. (arXiv:2305.18072v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Feipeng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yizhou Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_F/0/1/0/all/0/1">Fengyun Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yueyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaoyan Sun</a></p>
<p>Image captioning requires numerous annotated image-text pairs, resulting in
substantial annotation costs. Recently, large models (e.g. diffusion models and
large language models) have excelled in producing high-quality images and text.
This potential can be harnessed to create synthetic image-text pairs for
training captioning models. Synthetic data can improve cost and time efficiency
in data collection, allow for customization to specific domains, bootstrap
generalization capability for zero-shot performance, and circumvent privacy
concerns associated with real-world data. However, existing methods struggle to
attain satisfactory performance solely through synthetic data. We identify the
issue as generated images from simple descriptions mostly capture a solitary
perspective with limited context, failing to align with the intricate scenes
prevalent in real-world imagery. To tackle this, we present an innovative
pipeline that introduces multi-context data generation. Beginning with an
initial text corpus, our approach employs a large language model to extract
multiple sentences portraying the same scene from diverse viewpoints. These
sentences are then condensed into a single sentence with multiple contexts.
Subsequently, we generate intricate images using the condensed captions through
diffusion models. Our model is exclusively trained on synthetic image-text
pairs crafted through this process. The effectiveness of our pipeline is
validated through experimental results in both the in-domain and cross-domain
settings, where it achieves state-of-the-art performance on well-known datasets
such as MSCOCO, Flickr30k, and NoCaps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04744">WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models. (arXiv:2306.04744v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changhoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1">Kyle Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1">Maitreya Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Sheng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yezhou Yang</a></p>
<p>The rapid advancement of generative models, facilitating the creation of
hyper-realistic images from textual descriptions, has concurrently escalated
critical societal concerns such as misinformation. Although providing some
mitigation, traditional fingerprinting mechanisms fall short in attributing
responsibility for the malicious use of synthetic images. This paper introduces
a novel approach to model fingerprinting that assigns responsibility for the
generated images, thereby serving as a potential countermeasure to model
misuse. Our method modifies generative models based on each user's unique
digital fingerprint, imprinting a unique identifier onto the resultant content
that can be traced back to the user. This approach, incorporating fine-tuning
into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates
near-perfect attribution accuracy with a minimal impact on output quality.
Through extensive evaluation, we show that our method outperforms baseline
methods with an average improvement of 11\% in handling image post-processes.
Our method presents a promising and novel avenue for accountable model
distribution and responsible use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07915">Image Captioners Are Scalable Vision Learners Too. (arXiv:2306.07915v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1">Michael Tschannen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1">Manoj Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1">Andreas Steiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaohua Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1">Neil Houlsby</a>, <a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1">Lucas Beyer</a></p>
<p>Contrastive pretraining on image-text pairs from the web is one of the most
popular large-scale pretraining strategies for vision backbones, especially in
the context of large multimodal models. At the same time, image captioning on
this type of data is commonly considered an inferior pretraining strategy. In
this paper, we perform a fair comparison of these two pretraining strategies,
carefully matching training data, compute, and model capacity. Using a standard
encoder-decoder transformer, we find that captioning alone is surprisingly
effective: on classification tasks, captioning produces vision encoders
competitive with contrastively pretrained encoders, while surpassing them on
vision &amp; language tasks. We further analyze the effect of the model
architecture and scale, as well as the pretraining data on the representation
quality, and find that captioning exhibits the same or better scaling behavior
along these axes. Overall our results show that plain image captioning is a
more powerful pretraining strategy than was previously believed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12681">One at a Time: Progressive Multi-step Volumetric Probability Learning for Reliable 3D Scene Perception. (arXiv:2306.12681v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yasheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jingxin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Wenjun Zeng</a></p>
<p>Numerous studies have investigated the pivotal role of reliable 3D volume
representation in scene perception tasks, such as multi-view stereo (MVS) and
semantic scene completion (SSC). They typically construct 3D probability
volumes directly with geometric correspondence, attempting to fully address the
scene perception tasks in a single forward pass. However, such a single-step
solution makes it hard to learn accurate and convincing volumetric probability,
especially in challenging regions like unexpected occlusions and complicated
light reflections. Therefore, this paper proposes to decompose the complicated
3D volume representation learning into a sequence of generative steps to
facilitate fine and reliable scene perception. Considering the recent advances
achieved by strong generative diffusion models, we introduce a multi-step
learning framework, dubbed as VPD, dedicated to progressively refining the
Volumetric Probability in a Diffusion process. Extensive experiments are
conducted on scene perception tasks including multi-view stereo (MVS) and
semantic scene completion (SSC), to validate the efficacy of our method in
learning reliable volumetric representations. Notably, for the SSC task, our
work stands out as the first to surpass LiDAR-based methods on the
SemanticKITTI dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01097">MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion. (arXiv:2307.01097v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shitao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiacheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1">Yasutaka Furukawa</a></p>
<p>This paper introduces MVDiffusion, a simple yet effective method for
generating consistent multi-view images from text prompts given pixel-to-pixel
correspondences (e.g., perspective crops from a panorama or multi-view images
given depth maps and poses). Unlike prior methods that rely on iterative image
warping and inpainting, MVDiffusion simultaneously generates all images with a
global awareness, effectively addressing the prevalent error accumulation
issue. At its core, MVDiffusion processes perspective images in parallel with a
pre-trained text-to-image diffusion model, while integrating novel
correspondence-aware attention layers to facilitate cross-view interactions.
For panorama generation, while only trained with 10k panoramas, MVDiffusion is
able to generate high-resolution photorealistic images for arbitrary texts or
extrapolate one perspective image to a 360-degree view. For multi-view
depth-to-image generation, MVDiffusion demonstrates state-of-the-art
performance for texturing a scene mesh. The project page is at
https://mvdiffusion.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06962">Color-NeuS: Reconstructing Neural Implicit Surfaces with Color. (arXiv:2308.06962v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1">Licheng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lixin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kailin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhen_H/0/1/0/all/0/1">Haoyu Zhen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1">Mei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cewu Lu</a></p>
<p>The reconstruction of object surfaces from multi-view images or monocular
video is a fundamental issue in computer vision. However, much of the recent
research concentrates on reconstructing geometry through implicit or explicit
methods. In this paper, we shift our focus towards reconstructing mesh in
conjunction with color. We remove the view-dependent color from neural volume
rendering while retaining volume rendering performance through a relighting
network. Mesh is extracted from the signed distance function (SDF) network for
the surface, and color for each surface vertex is drawn from the global color
network. To evaluate our approach, we conceived a in hand object scanning task
featuring numerous occlusions and dramatic shifts in lighting conditions. We've
gathered several videos for this task, and the results surpass those of any
existing methods capable of reconstructing mesh alongside color. Additionally,
our method's performance was assessed using public datasets, including DTU,
BlendedMVS, and OmniObject3D. The results indicated that our method performs
well across all these datasets. Project page:
https://colmar-zlicheng.github.io/color_neus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08288">Improving Audio-Visual Segmentation with Bidirectional Generation. (arXiv:2308.08288v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_D/0/1/0/all/0/1">Dawei Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuxin Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bowen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaodong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuchao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yiran Zhong</a></p>
<p>The aim of audio-visual segmentation (AVS) is to precisely differentiate
audible objects within videos down to the pixel level. Traditional approaches
often tackle this challenge by combining information from various modalities,
where the contribution of each modality is implicitly or explicitly modeled.
Nevertheless, the interconnections between different modalities tend to be
overlooked in audio-visual modeling. In this paper, inspired by the human
ability to mentally simulate the sound of an object and its visual appearance,
we introduce a bidirectional generation framework. This framework establishes
robust correlations between an object's visual characteristics and its
associated sound, thereby enhancing the performance of AVS. To achieve this, we
employ a visual-to-audio projection component that reconstructs audio features
from object segmentation masks and minimizes reconstruction errors. Moreover,
recognizing that many sounds are linked to object movements, we introduce an
implicit volumetric motion estimation module to handle temporal dynamics that
may be challenging to capture using conventional optical flow methods. To
showcase the effectiveness of our approach, we conduct comprehensive
experiments and analyses on the widely recognized AVSBench benchmark. As a
result, we establish a new state-of-the-art performance level in the AVS
benchmark, particularly excelling in the challenging MS3 subset which involves
segmenting multiple sound sources. To facilitate reproducibility, we plan to
release both the source code and the pre-trained model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13304">Rapid Artefact Removal and H&amp;E-Stained Tissue Segmentation. (arXiv:2308.13304v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Schreiber_B/0/1/0/all/0/1">B. A. Schreiber</a>, <a href="http://arxiv.org/find/eess/1/au:+Denholm_J/0/1/0/all/0/1">J. Denholm</a>, <a href="http://arxiv.org/find/eess/1/au:+Jaeckle_F/0/1/0/all/0/1">F. Jaeckle</a>, <a href="http://arxiv.org/find/eess/1/au:+Arends_M/0/1/0/all/0/1">M. J. Arends</a>, <a href="http://arxiv.org/find/eess/1/au:+Branson_K/0/1/0/all/0/1">K. M. Branson</a>, <a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1">C.-B. Sch&#xf6;nlieb</a>, <a href="http://arxiv.org/find/eess/1/au:+Soilleux_E/0/1/0/all/0/1">E. J. Soilleux</a></p>
<p>We present an innovative method for rapidly segmenting hematoxylin and eosin
(H&amp;E)-stained tissue in whole-slide images (WSIs) that eliminates a wide range
of undesirable artefacts such as pen marks and scanning artefacts. Our method
involves taking a single-channel representation of a lowmagnification RGB
overview of the WSI in which the pixel values are bimodally distributed such
that H&amp;E-stained tissue is easily distinguished from both background and a wide
variety of artefacts. We demonstrate our method on 30 WSIs prepared from a wide
range of institutions and WSI digital scanners, each containing substantial
artefacts, and compare it to segmentations provided by Otsu thresholding and
Histolab tissue segmentation and pen filtering tools. We found that our method
segmented the tissue and fully removed all artefacts in 29 out of 30 WSIs,
whereas Otsu thresholding failed to remove any artefacts, and the Histolab pen
filtering tools only partially removed the pen marks. The beauty of our
approach lies in its simplicity: manipulating RGB colour space and using Otsu
thresholding allows for the segmentation of H&amp;E-stained tissue and the rapid
removal of artefacts without the need for machine learning or parameter tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02318">TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction. (arXiv:2309.02318v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhenghong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huangxuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiemin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1">Dongqiao Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lingxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feihong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuansheng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a></p>
<p>Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical
role in the diagnosis of many medical diseases, such as Arteriovenous
Malformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significant
application value, the reconstruction of 4D DSA demands numerous views to
effectively model the intricate vessels and radiocontrast flow, thereby
implying a significant radiation dose. To address this high radiation issue, we
propose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA
reconstruction, which paves the way for high-quality 4D imaging. Additionally,
2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA
images. TiAVox introduces 4D attenuation voxel grids, which reflect attenuation
properties from both spatial and temporal dimensions. It is optimized by
minimizing discrepancies between the rendered images and sparse 2D DSA images.
Without any neural network involved, TiAVox enjoys specific physical
interpretability. The parameters of each learnable voxel represent the
attenuation coefficients. We validated the TiAVox approach on both clinical and
simulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for
novel view synthesis using only 30 views on the clinically sourced dataset,
whereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly,
with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32
for novel view synthesis and 41.40 for 3D reconstruction. We also executed
ablation studies to corroborate the essential components of TiAVox. The code
will be publically available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10625">NoisyNN: Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiaowei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Li Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dajiang Zhu</a></p>
<p>We explore the impact of entropy change in deep learning systems via noise
injection at different levels, i.e., the latent space and input image. The
series of models that employ our methodology are collectively known as Noisy
Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise
is conventionally viewed as a harmful perturbation in various deep learning
architectures, such as convolutional neural networks (CNNs) and vision
transformers (ViTs), as well as different learning tasks like image
classification and transfer learning. However, this work shows noise can be an
effective way to change the entropy of the learning system. We demonstrate that
specific noise can boost the performance of various deep architectures under
certain conditions. We theoretically prove the enhancement gained from positive
noise by reducing the task complexity defined by information entropy and
experimentally show the significant performance gain in large image datasets,
such as the ImageNet. Herein, we use the information entropy to define the
complexity of the task. We categorize the noise into two types, positive noise
(PN) and harmful noise (HN), based on whether the noise can help reduce the
complexity of the task. Extensive experiments of CNNs and ViTs have shown
performance improvements by proactively injecting positive noise, where we
achieved an unprecedented top 1 accuracy of over 95$\%$ on ImageNet. Both
theoretical analysis and empirical evidence have confirmed that the presence of
positive noise, can benefit the learning process, while the traditionally
perceived harmful noise indeed impairs deep learning models. The different
roles of noise offer new explanations for deep models on specific tasks and
provide a new paradigm for improving model performance. Moreover, it reminds us
that we can influence the performance of learning systems via information
entropy change.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15289">SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhiqian Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuxuan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1">Yao Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengbo Eben Li</a></p>
<p>Motion prediction is crucial for autonomous vehicles to operate safely in
complex traffic environments. Extracting effective spatiotemporal relationships
among traffic elements is key to accurate forecasting. Inspired by the
successful practice of pretrained large language models, this paper presents
SEPT, a modeling framework that leverages self-supervised learning to develop
powerful spatiotemporal understanding for complex traffic scenes. Specifically,
our approach involves three masking-reconstruction modeling tasks on scene
inputs including agents' trajectories and road network, pretraining the scene
encoder to capture kinematics within trajectory, spatial structure of road
network, and interactions among roads and agents. The pretrained encoder is
then finetuned on the downstream forecasting task. Extensive experiments
demonstrate that SEPT, without elaborate architectural design or manual feature
engineering, achieves state-of-the-art performance on the Argoverse 1 and
Argoverse 2 motion forecasting benchmarks, outperforming previous methods on
all main metrics by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00757">Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models. (arXiv:2310.00757v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1">Soroosh Tayebi Arasteh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_C/0/1/0/all/0/1">Christiane Kuhl</a>, <a href="http://arxiv.org/find/cs/1/au:+Saehn_M/0/1/0/all/0/1">Marwin-Jonathan Saehn</a>, <a href="http://arxiv.org/find/cs/1/au:+Isfort_P/0/1/0/all/0/1">Peter Isfort</a>, <a href="http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1">Daniel Truhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Nebelung_S/0/1/0/all/0/1">Sven Nebelung</a></p>
<p>Developing robust artificial intelligence (AI) models that generalize well to
unseen datasets is challenging and usually requires large and variable
datasets, preferably from multiple institutions. In federated learning (FL), a
model is trained collaboratively at numerous sites that hold local datasets
without exchanging them. So far, the impact of training strategy, i.e., local
versus collaborative, on the diagnostic on-domain and off-domain performance of
AI models interpreting chest radiographs has not been assessed. Consequently,
using 610,000 chest radiographs from five institutions across the globe, we
assessed diagnostic performance as a function of training strategy (i.e., local
vs. collaborative), network architecture (i.e., convolutional vs.
transformer-based), generalization performance (i.e., on-domain vs.
off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia,
atelectasis, consolidation, pneumothorax, and no abnormality), dataset size
(i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large
datasets not only showed minimal performance gains with FL but, in some
instances, even exhibited decreases. In contrast, smaller datasets revealed
marked improvements. Thus, on-domain performance was mainly driven by training
data size. However, off-domain performance leaned more on training diversity.
When trained collaboratively across diverse external institutions, AI models
consistently surpassed models trained locally for off-domain tasks, emphasizing
FL's potential in leveraging data diversity. In conclusion, FL can bolster
diagnostic privacy, reproducibility, and off-domain reliability of AI models
and, potentially, optimize healthcare outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04780">IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers. (arXiv:2310.04780v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhenglin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1">Xianan Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Na Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1">Xiaomei Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Biao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a></p>
<p>Data augmentation has been proven effective for training high-accuracy
convolutional neural network classifiers by preventing overfitting. However,
building deep neural networks in real-world scenarios requires not only high
accuracy on clean data but also robustness when data distributions shift. While
prior methods have proposed that there is a trade-off between accuracy and
robustness, we propose IPMix, a simple data augmentation approach to improve
robustness without hurting clean accuracy. IPMix integrates three levels of
data augmentation (image-level, patch-level, and pixel-level) into a coherent
and label-preserving technique to increase the diversity of training data with
limited computational overhead. To further improve the robustness, IPMix
introduces structural complexity at different levels to generate more diverse
images and adopts the random mixing method for multi-scale information fusion.
Experiments demonstrate that IPMix outperforms state-of-the-art corruption
robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also
significantly improves the other safety measures, including robustness to
adversarial perturbations, calibration, prediction consistency, and anomaly
detection, achieving state-of-the-art or comparable results on several
benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08206">Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Ji Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jianhang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaobo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a></p>
<p>Long-tailed (LT) classification is an unavoidable and challenging problem in
the real world. Most existing long-tailed classification methods focus only on
solving the class-wise imbalance while ignoring the attribute-wise imbalance.
The deviation of a classification model is caused by both class-wise and
attribute-wise imbalance. Due to the fact that attributes are implicit in most
datasets and the combination of attributes is complex, attribute-wise imbalance
is more difficult to handle. For this purpose, we proposed a novel long-tailed
classification framework, aiming to build a multi-granularity classification
model by means of invariant feature learning. This method first unsupervisedly
constructs Coarse-Grained forest (CLF) to better characterize the distribution
of attributes within a class. Depending on the distribution of attributes, one
can customize suitable sampling strategies to construct different imbalanced
datasets. We then introduce multi-center loss (MCL) that aims to gradually
eliminate confusing attributes during feature learning process. The proposed
framework does not necessarily couple to a specific LT classification model
structure and can be integrated with any existing LT method as an independent
component. Extensive experiments show that our approach achieves
state-of-the-art performance on both existing benchmarks ImageNet-GLT and
MSCOCO-GLT and can improve the performance of existing LT methods. Our codes
are available on GitHub: \url{https://github.com/jinyery/cognisance}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10198">MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations. (arXiv:2310.10198v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Heyuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhenhua Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ao_T/0/1/0/all/0/1">Tenglong Ao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Baoquan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Libin Liu</a></p>
<p>In this work, we present MoConVQ, a novel unified framework for physics-based
motion control leveraging scalable discrete representations. Building upon
vector quantized variational autoencoders (VQ-VAE) and model-based
reinforcement learning, our approach effectively learns motion embeddings from
a large, unstructured dataset spanning tens of hours of motion examples. The
resultant motion representation not only captures diverse motion skills but
also offers a robust and intuitive interface for various applications. We
demonstrate the versatility of MoConVQ through several applications: universal
tracking control from various motion sources, interactive character control
with latent motion representations using supervised learning, physics-based
motion generation from natural language descriptions using the GPT framework,
and, most interestingly, seamless integration with large language models (LLMs)
with in-context learning to tackle complex and abstract tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15646">Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework. (arXiv:2310.15646v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1">Weixi Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a></p>
<p>Unsupervised domain adaptation object detection (UDAOD) research on Detection
Transformer(DETR) mainly focuses on feature alignment and existing methods can
be divided into two kinds, each of which has its unresolved issues. One-stage
feature alignment methods can easily lead to performance fluctuation and
training stagnation. Two-stage feature alignment method based on mean teacher
comprises a pretraining stage followed by a self-training stage, each facing
problems in obtaining reliable pretrained model and achieving consistent
performance gains. Methods mentioned above have not yet explore how to utilize
the third related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e. Mean
Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we
utilize labeled target-like images produced by image style transfer to avoid
performance fluctuation. In the self-training stage, we leverage unlabeled
target images by pseudo labels based on mean teacher and propose a module
called Object Queries Knowledge Transfer (OQKT) to ensure consistent
performance gains of the student model. Most importantly, we propose masked
feature alignment methods including Masked Domain Query-based Feature Alignment
(MDQFA) and Masked Token-wise Feature Alignment (MTWFA) to alleviate domain
shift in a more robust way, which not only prevent training stagnation and lead
to a robust pretrained model in the pretraining stage, but also enhance the
model's target performance in the self-training stage. Experiments on three
challenging scenarios and a theoretical analysis verify the effectiveness of
MTM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07594">How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. (arXiv:2311.07594v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shezheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shasha Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jie Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1">Xiaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weimin Zhang</a></p>
<p>This review paper explores Multimodal Large Language Models (MLLMs), which
integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data
such as text and vision. MLLMs demonstrate capabilities like generating image
narratives and answering image-based questions, bridging the gap towards
real-world human-computer interactions and hinting at a potential pathway to
artificial general intelligence. However, MLLMs still face challenges in
processing the semantic gap in multimodality, which may lead to erroneous
generation, posing potential risks to society. Choosing the appropriate
modality alignment method is crucial, as improper methods might require more
parameters with limited performance improvement. This paper aims to explore
modality alignment methods for LLMs and their existing capabilities.
Implementing modality alignment allows LLMs to address environmental issues and
enhance accessibility. The study surveys existing modal alignment methods in
MLLMs into four groups: (1) Multimodal Converters that change data into
something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs
perceive different types of data; (3) Tools Assistance for changing data into
one common format, usually text; and (4) Data-Driven methods that teach LLMs to
understand specific types of data in a dataset. This field is still in a phase
of exploration and experimentation, and we will organize and update various
existing research methods for multimodal information alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15570">UFDA: Universal Federated Domain Adaptation with Practical Assumptions. (arXiv:2311.15570v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinhui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenghao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_W/0/1/0/all/0/1">Wei Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1">Gairui Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yihan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jizhong Zhao</a></p>
<p>Conventional Federated Domain Adaptation (FDA) approaches usually demand an
abundance of assumptions, which makes them significantly less feasible for
real-world situations and introduces security hazards. This paper relaxes the
assumptions from previous FDAs and studies a more practical scenario named
Universal Federated Domain Adaptation (UFDA). It only requires the black-box
model and the label set information of each source domain, while the label sets
of different source domains could be inconsistent, and the target-domain label
set is totally blind. Towards a more effective solution for our newly proposed
UFDA scenario, we propose a corresponding methodology called Hot-Learning with
Contrastive Label Disambiguation (HCLD). It particularly tackles UFDA's domain
shifts and category gaps problems by using one-hot outputs from the black-box
models of various source domains. Moreover, to better distinguish the shared
and unknown classes, we further present a cluster-level strategy named
Mutual-Voting Decision (MVD) to extract robust consensus knowledge across peer
classes from both source and target domains. Extensive experiments on three
benchmark datasets demonstrate that our method achieves comparable performance
for our UFDA scenario with much fewer assumptions, compared to previous
methodologies with comprehensive additional assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16502">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. (arXiv:2311.16502v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yuansheng Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruoqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1">Samuel Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Dongfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Weiming Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Cong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Botao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Renliang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1">Ming Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Boyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhenzhu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a></p>
<p>We introduce MMMU: a new benchmark designed to evaluate multimodal models on
massive multi-discipline tasks demanding college-level subject knowledge and
deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal
questions from college exams, quizzes, and textbooks, covering six core
disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp;
Social Science, and Tech &amp; Engineering. These questions span 30 subjects and
183 subfields, comprising 30 highly heterogeneous image types, such as charts,
diagrams, maps, tables, music sheets, and chemical structures. Unlike existing
benchmarks, MMMU focuses on advanced perception and reasoning with
domain-specific knowledge, challenging models to perform tasks akin to those
faced by experts. The evaluation of 14 open-source LMMs as well as the
proprietary GPT-4V(ision) and Gemini highlights the substantial challenges
posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve
accuracies of 56% and 59% respectively, indicating significant room for
improvement. We believe MMMU will stimulate the community to build
next-generation multimodal foundation models towards expert artificial general
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17280">Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1">Ishika Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>Data augmentation via back-translation is common when pretraining
Vision-and-Language Navigation (VLN) models, even though the generated
instructions are noisy. But: does that noise matter? We find that nonsensical
or irrelevant language instructions during pretraining can have little effect
on downstream performance for both HAMT and VLN-BERT on R2R, and is still
better than only using clean, human data. To underscore these results, we
concoct an efficient augmentation method, Unigram + Object, which generates
nonsensical instructions that nonetheless improve downstream performance. Our
findings suggest that what matters for VLN R2R pretraining is the quantity of
visual trajectories, not the quality of instructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00634">A Recent Survey of Vision Transformers for Medical Image Segmentation. (arXiv:2312.00634v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1">Asifullah Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1">Zunaira Rauf</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1">Abdul Rehman Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Rathore_S/0/1/0/all/0/1">Saima Rathore</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1">Saddam Hussain Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1">Najmus Saher Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Farooq_U/0/1/0/all/0/1">Umair Farooq</a>, <a href="http://arxiv.org/find/eess/1/au:+Asif_H/0/1/0/all/0/1">Hifsa Asif</a>, <a href="http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1">Aqsa Asif</a>, <a href="http://arxiv.org/find/eess/1/au:+Zahoora_U/0/1/0/all/0/1">Umme Zahoora</a>, <a href="http://arxiv.org/find/eess/1/au:+Khalil_R/0/1/0/all/0/1">Rafi Ullah Khalil</a>, <a href="http://arxiv.org/find/eess/1/au:+Qamar_S/0/1/0/all/0/1">Suleman Qamar</a>, <a href="http://arxiv.org/find/eess/1/au:+Asif_U/0/1/0/all/0/1">Umme Hani Asif</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1">Faiza Babar Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Majid_A/0/1/0/all/0/1">Abdul Majid</a>, <a href="http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1">Jeonghwan Gwak</a></p>
<p>Medical image segmentation plays a crucial role in various healthcare
applications, enabling accurate diagnosis, treatment planning, and disease
monitoring. Traditionally, convolutional neural networks (CNNs) dominated this
domain, excelling at local feature extraction. However, their limitations in
capturing long-range dependencies across image regions pose challenges for
segmenting complex, interconnected structures often encountered in medical
data. In recent years, Vision Transformers (ViTs) have emerged as a promising
technique for addressing the challenges in medical image segmentation. Their
multi-scale attention mechanism enables effective modeling of long-range
dependencies between distant structures, crucial for segmenting organs or
lesions spanning the image. Additionally, ViTs' ability to discern subtle
pattern heterogeneity allows for the precise delineation of intricate
boundaries and edges, a critical aspect of accurate medical image segmentation.
However, they do lack image-related inductive bias and translational
invariance, potentially impacting their performance. Recently, researchers have
come up with various ViT-based approaches that incorporate CNNs in their
architectures, known as Hybrid Vision Transformers (HVTs) to capture local
correlation in addition to the global information in the images. This survey
paper provides a detailed review of the recent advancements in ViTs and HVTs
for medical image segmentation. Along with the categorization of ViT and
HVT-based medical image segmentation approaches, we also present a detailed
overview of their real-time applications in several medical image modalities.
This survey may serve as a valuable resource for researchers, healthcare
practitioners, and students in understanding the state-of-the-art approaches
for ViT-based medical image segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04326">iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image Diffusion Model for Interior Design. (arXiv:2312.04326v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1">Ruyi Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaojun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuanhe Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dixiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Renliang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pingjian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yan Song</a></p>
<p>With the open-sourcing of text-to-image models (T2I) such as stable diffusion
(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned
in specific domains based on the open-source SD model, such as in anime,
character portraits, etc. However, there are few specialized models in certain
domains, such as interior design, which is attributed to the complex textual
descriptions and detailed visual elements inherent in design, alongside the
necessity for adaptable resolution. Therefore, text-to-image models for
interior design are required to have outstanding prompt-following capabilities,
as well as iterative collaboration with design professionals to achieve the
desired outcome. In this paper, we collect and optimize text-image data in the
design field and continue training in both English and Chinese on the basis of
the open-source CLIP model. We also proposed a fine-tuning strategy with
curriculum learning and reinforcement learning from CLIP feedback to enhance
the prompt-following capabilities of our approach so as to improve the quality
of image generation. The experimental results on the collected dataset
demonstrate the effectiveness of the proposed approach, which achieves
impressive results and outperforms strong baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05281">X2-Softmax: Margin Adaptive Loss Function for Face Recognition. (arXiv:2312.05281v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiamu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoxiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_Y/0/1/0/all/0/1">Yain-Whar Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaofan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zheng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xueyuan Gong</a></p>
<p>Learning the discriminative features of different faces is an important task
in face recognition. By extracting face features in neural networks, it becomes
easy to measure the similarity of different face images, which makes face
recognition possible. To enhance the neural network's face feature
separability, incorporating an angular margin during training is common
practice. State-of-the-art loss functions CosFace and ArcFace apply fixed
margins between weights of classes to enhance the inter-class separation of
face features. Since the distribution of samples in the training set is
imbalanced, similarities between different identities are unequal. Therefore,
using an inappropriately fixed angular margin may lead to the problem that the
model is difficult to converge or the face features are not discriminative
enough. It is more in line with our intuition that the margins are angular
adaptive, which could increase with the angles between classes growing. In this
paper, we propose a new angular margin loss named X2-Softmax. X2-Softmax loss
has adaptive angular margins, which provide the margin that increases with the
angle between different classes growing. The angular adaptive margin ensures
model flexibility and effectively improves the effect of face recognition. We
have trained the neural network with X2-Softmax loss on the MS1Mv3 dataset and
tested it on several evaluation benchmarks to demonstrate the effectiveness and
superiority of our loss function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06171">Jointly Explicit and Implicit Cross-Modal Interaction Network for Anterior Chamber Inflammation Diagnosis. (arXiv:2312.06171v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1">Qian Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Ye Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1">Haochao Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_W/0/1/0/all/0/1">Wei Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a></p>
<p>Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)
for optimal treatment. However, current diagnostic methods only rely on a
limited single-modal disease perspective, which leads to poor performance. In
this paper, we investigate a promising yet challenging way to fuse multimodal
data for ACI diagnosis. Notably, existing fusion paradigms focus on empowering
implicit modality interactions (i.e., self-attention and its variants), but
neglect to inject explicit modality interactions, especially from clinical
knowledge and imaging property. To this end, we propose a jointly Explicit and
implicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber
Inflammation Diagnosis that uses anterior segment optical coherence tomography
(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we
first develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract
efficient feature representations in different modalities. Then, we devise an
Explicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a
kind of explicit clinical knowledge based on the tabular feature maps, then
integrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder
to focus on more effective informativeness of the slit-lamp images. After that,
the Implicit Cross-Modal Interaction Module (ICIM), a transformer-based
network, further implicitly enhances modality interactions. Finally, we
construct a considerable real-world dataset from our collaborative hospital and
conduct sufficient experiments to demonstrate the superior performance of our
proposed EiCI-Net compared with the state-of-the-art classification methods in
various metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07021">Transferring Modality-Aware Pedestrian Attentive Learning for Visible-Infrared Person Re-identification. (arXiv:2312.07021v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuwei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1">Licheng Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fang Liu</a></p>
<p>Visible-infrared person re-identification (VI-ReID) aims to search the same
pedestrian of interest across visible and infrared modalities. Existing models
mainly focus on compensating for modality-specific information to reduce
modality variation. However, these methods often lead to a higher computational
overhead and may introduce interfering information when generating the
corresponding images or features. To address this issue, it is critical to
leverage pedestrian-attentive features and learn modality-complete and
-consistent representation. In this paper, a novel Transferring Modality-Aware
Pedestrian Attentive Learning (TMPA) model is proposed, focusing on the
pedestrian regions to efficiently compensate for missing modality-specific
features. Specifically, we propose a region-based data augmentation module
PedMix to enhance pedestrian region coherence by mixing the corresponding
regions from different modalities. A lightweight hybrid compensation module,
i.e., the Modality Feature Transfer (MFT), is devised to integrate cross
attention and convolution networks to fully explore the discriminative
modality-complete features with minimal computational overhead. Extensive
experiments conducted on the benchmark SYSU-MM01 and RegDB datasets
demonstrated the effectiveness of our proposed TMPA model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07266">ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open-Vocabulary Object Detection. (arXiv:2312.07266v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1">Joonhyun Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1">Geondo Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1">Jayeon Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1">Hyungsik Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Heesu Kim</a></p>
<p>Open-vocabulary object detection (OVOD) aims to recognize novel objects whose
categories are not included in the training set. In order to classify these
unseen classes during training, many OVOD frameworks leverage the zero-shot
capability of largely pretrained vision and language models, such as CLIP. To
further improve generalization on the unseen novel classes, several approaches
proposed to additionally train with pseudo region labeling on the external data
sources that contain a substantial number of novel category labels beyond the
existing training data. Albeit its simplicity, these pseudo-labeling methods
still exhibit limited improvement with regard to the truly unseen novel classes
that were not pseudo-labeled. In this paper, we present a novel, yet simple
technique that helps generalization on the overall distribution of novel
classes. Inspired by our observation that numerous novel classes reside within
the convex hull constructed by the base (seen) classes in the CLIP embedding
space, we propose to synthesize proxy-novel classes approximating novel classes
via linear mixup between a pair of base classes. By training our detector with
these synthetic proxy-novel classes, we effectively explore the embedding space
of novel classes. The experimental results on various OVOD benchmarks such as
LVIS and COCO demonstrate superior performance on novel classes compared to the
other state-of-the-art methods. Code is available at
https://github.com/clovaai/ProxyDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07374">Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects. (arXiv:2312.07374v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiayi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weitong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1">Shaogang Gong</a></p>
<p>Camouflaged object detection (COD) approaches heavily rely on pixel-level
annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse
annotations like scribbles or points to reduce annotation effort, but this can
lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable
segmentation ability with sparse prompts like points. However, manual prompt is
not always feasible, as it may not be accessible in real-world application.
Additionally, it only provides localization information instead of semantic
one, which can intrinsically cause ambiguity in interpreting the targets. In
this work, we aim to eliminate the need for manual prompt. The key idea is to
employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts
using the semantic information given by a generic text prompt. To that end, we
introduce a test-time adaptation per-instance mechanism called Generalizable
SAM (GenSAM) to automatically enerate and optimize visual prompts the generic
task prompt for WSCOD. In particular, CCTP maps a single generic text prompt
onto image-specific consensus foreground and background heatmaps using
vision-language models, acquiring reliable visual prompts. Moreover, to
test-time adapt the visual prompts, we further propose Progressive Mask
Generation (PMG) to iteratively reweight the input image, guiding the model to
focus on the targets in a coarse-to-fine manner. Crucially, all network
parameters are fixed, avoiding the need for additional training. Experiments
demonstrate the superiority of GenSAM. Experiments on three benchmarks
demonstrate that GenSAM outperforms point supervision approaches and achieves
comparable results to scribble supervision ones, solely relying on general task
descriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07937">BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. (arXiv:2312.07937v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Molin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuxuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Juze Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jingyi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingya Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lan Xu</a></p>
<p>The recently emerging text-to-motion advances have spired numerous attempts
for convenient and interactive human motion generation. Yet, existing methods
are largely limited to generating body motions only without considering the
rich two-hand motions, let alone handling various conditions like body dynamics
or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal
dataset for two-hand motion generation. Our dataset includes accurate motion
tracking for the human body and hands and provides pair-wised finger-level hand
annotations and body descriptions. We further provide a strong baseline method,
BOTH2Hands, for the novel task: generating vivid two-hand motions from both
implicit body dynamics and explicit text prompts. We first warm up two parallel
body-to-hand and text-to-hand diffusion models and then utilize the
cross-attention transformer for motion blending. Extensive experiments and
cross-validations demonstrate the effectiveness of our approach and dataset for
generating convincing two-hand motions from the hybrid body-and-textual
conditions. Our dataset and code will be disseminated to the community for
future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09147">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers. (arXiv:2312.09147v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zi-Xin Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhipeng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuan-Chen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangguang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Ding Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Song-Hai Zhang</a></p>
<p>Recent advancements in 3D reconstruction from single images have been driven
by the evolution of generative models. Prominent among these are methods based
on Score Distillation Sampling (SDS) and the adaptation of diffusion models in
the 3D domain. Despite their progress, these techniques often face limitations
due to slow optimization or rendering processes, leading to extensive training
and optimization times. In this paper, we introduce a novel approach for
single-view reconstruction that efficiently generates a 3D model from a single
image via feed-forward inference. Our method utilizes two transformer-based
networks, namely a point decoder and a triplane decoder, to reconstruct 3D
objects using a hybrid Triplane-Gaussian intermediate representation. This
hybrid representation strikes a balance, achieving a faster rendering speed
compared to implicit representations while simultaneously delivering superior
rendering quality than explicit representations. The point decoder is designed
for generating point clouds from single images, offering an explicit
representation which is then utilized by the triplane decoder to query Gaussian
features for each point. This design choice addresses the challenges associated
with directly regressing explicit 3D Gaussian attributes characterized by their
non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to
enable rapid rendering through splatting. Both decoders are built upon a
scalable, transformer-based architecture and have been efficiently trained on
large-scale 3D datasets. The evaluations conducted on both synthetic datasets
and real-world images demonstrate that our method not only achieves higher
quality but also ensures a faster runtime in comparison to previous
state-of-the-art techniques. Please see our project page at
https://zouzx.github.io/TriplaneGaussian/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09313">LatentEditor: Text Driven Local Editing of 3D Scenes. (arXiv:2312.09313v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1">Umar Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1">Hasan Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1">Nazmul Karim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1">Jing Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a></p>
<p>While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09754">PPFM: Image denoising in photon-counting CT using single-step posterior sampling Poisson flow generative models. (arXiv:2312.09754v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hein_D/0/1/0/all/0/1">Dennis Hein</a>, <a href="http://arxiv.org/find/eess/1/au:+Holmin_S/0/1/0/all/0/1">Staffan Holmin</a>, <a href="http://arxiv.org/find/eess/1/au:+Szczykutowicz_T/0/1/0/all/0/1">Timothy Szczykutowicz</a>, <a href="http://arxiv.org/find/eess/1/au:+Maltz_J/0/1/0/all/0/1">Jonathan S Maltz</a>, <a href="http://arxiv.org/find/eess/1/au:+Danielsson_M/0/1/0/all/0/1">Mats Danielsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Ge Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Persson_M/0/1/0/all/0/1">Mats Persson</a></p>
<p>Diffusion and Poisson flow models have shown impressive performance in a wide
range of generative tasks, including low-dose CT image denoising. However, one
limitation in general, and for clinical applications in particular, is slow
sampling. Due to their iterative nature, the number of function evaluations
(NFE) required is usually on the order of $10-10^3$, both for conditional and
unconditional generation. In this paper, we present posterior sampling Poisson
flow generative models (PPFM), a novel image denoising technique for low-dose
and photon-counting CT that produces excellent image quality whilst keeping
NFE=1. Updating the training and sampling processes of Poisson flow generative
models (PFGM)++, we learn a conditional generator which defines a trajectory
between the prior noise distribution and the posterior distribution of
interest. We additionally hijack and regularize the sampling process to achieve
NFE=1. Our results shed light on the benefits of the PFGM++ framework compared
to diffusion models. In addition, PPFM is shown to perform favorably compared
to current state-of-the-art diffusion-style models with NFE=1, consistency
models, as well as popular deep learning and non-deep learning-based image
denoising techniques, on clinical low-dose CT images and clinical images from a
prototype photon-counting CT system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09783">Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning. (arXiv:2312.09783v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Tom Nuno Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1">Fabian Bongratz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1">Anne-Marie Rickmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1">Sebastian P&#xf6;lsterl</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1">Christian Wachinger</a></p>
<p>Explaining predictions of black-box neural networks is crucial when applied
to decision-critical tasks. Thus, attribution maps are commonly used to
identify important image regions, despite prior work showing that humans prefer
explanations based on similar examples. To this end, ProtoPNet learns a set of
class-representative feature vectors (prototypes) for case-based reasoning.
During inference, similarities of latent features to prototypes are linearly
classified to form predictions and attribution maps are provided to explain the
similarity. In this work, we evaluate whether architectures for case-based
reasoning fulfill established axioms required for faithful explanations using
the example of ProtoPNet. We show that such architectures allow the extraction
of faithful explanations. However, we prove that the attribution maps used to
explain the similarities violate the axioms. We propose a new procedure to
extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,
these explanations are Shapley values, calculated on the similarity scores of
each prototype. They allow to faithfully answer which prototypes are present in
an unseen image and quantify each pixel's contribution to that presence,
thereby complying with all axioms. The theoretical violations of ProtoPNet
manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,
RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,
ResNeXt50). Our experiments show a qualitative difference between the
explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the
explanations with the Area Over the Perturbation Curve, on which ProtoPFaith
outperforms ProtoPNet on all experiments by a factor $&gt;10^3$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10088">On Robustness to Missing Video for Audiovisual Speech Recognition. (arXiv:2312.10088v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chang_O/0/1/0/all/0/1">Oscar Chang</a>, <a href="http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1">Otavio Braga</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1">Hank Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Serdyuk_D/0/1/0/all/0/1">Dmitriy Serdyuk</a>, <a href="http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1">Olivier Siohan</a></p>
<p>It has been shown that learning audiovisual features can lead to improved
speech recognition performance over audio-only features, especially for noisy
speech. However, in many common applications, the visual features are partially
or entirely missing, e.g.~the speaker might move off screen. Multi-modal models
need to be robust: missing video frames should not degrade the performance of
an audiovisual model to be worse than that of a single-modality audio-only
model. While there have been many attempts at building robust models, there is
little consensus on how robustness should be evaluated. To address this, we
introduce a framework that allows claims about robustness to be evaluated in a
precise and testable way. We also conduct a systematic empirical study of the
robustness of common audiovisual speech recognition architectures on a range of
acoustic noise conditions and test suites. Finally, we show that an
architecture-agnostic solution based on cascades can consistently achieve
robustness to missing video, even in settings where existing techniques for
robustness like dropout fall short.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10237">Vertical Federated Alzheimer&#x27;s Detection on Multimodal Data. (arXiv:2312.10237v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1">Paul K. Mandal</a></p>
<p>In the era of rapidly advancing medical technologies, the segmentation of
medical data has become inevitable, necessitating the development of privacy
preserving machine learning algorithms that can train on distributed data.
Consolidating sensitive medical data is not always an option particularly due
to the stringent privacy regulations imposed by the Health Insurance
Portability and Accountability Act (HIPAA). In this paper, we introduce a HIPAA
compliant framework that can train from distributed data. We then propose a
multimodal vertical federated model for Alzheimer's Disease (AD) detection, a
serious neurodegenerative condition that can cause dementia, severely impairing
brain function and hindering simple tasks, especially without preventative
care. This vertical federated model offers a distributed architecture that
enables collaborative learning across diverse sources of medical data while
respecting privacy constraints imposed by HIPAA. It is also able to leverage
multiple modalities of data, enhancing the robustness and accuracy of AD
detection. Our proposed model not only contributes to the advancement of
federated learning techniques but also holds promise for overcoming the hurdles
posed by data segmentation in medical research. By using vertical federated
learning, this research strives to provide a framework that enables healthcare
institutions to harness the collective intelligence embedded in their
distributed datasets without compromising patient privacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10300">Shot2Story20K: A New Benchmark for Comprehensive Understanding of Multi-shot Videos. (arXiv:2312.10300v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1">Mingfei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Linjie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiaojun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Heng Wang</a></p>
<p>A short clip of video may contain progression of multiple events and an
interesting story line. A human need to capture both the event in every shot
and associate them together to understand the story behind it. In this work, we
present a new multi-shot video understanding benchmark Shot2Story20K with
detailed shot-level captions and comprehensive video summaries. To facilitate
better semantic understanding of videos, we provide captions for both visual
signals and human narrations. We design several distinct tasks including
single-shot video and narration captioning, multi-shot video summarization, and
video retrieval with shot descriptions. Preliminary experiments show some
challenges to generate a long and comprehensive video summary. Nevertheless,
the generated imperfect summaries can already significantly boost the
performance of existing video understanding tasks such as video
question-answering, promoting an under-explored setting of video understanding
with detailed summaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10422">Learning Dense Correspondence for NeRF-Based Face Reenactment. (arXiv:2312.10422v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yushi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiangyu Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bo Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jing Dong</a></p>
<p>Face reenactment is challenging due to the need to establish dense
correspondence between various face representations for motion transfer. Recent
studies have utilized Neural Radiance Field (NeRF) as fundamental
representation, which further enhanced the performance of multi-view face
reenactment in photo-realism and 3D consistency. However, establishing dense
correspondence between different face NeRFs is non-trivial, because implicit
representations lack ground-truth correspondence annotations like mesh-based 3D
parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning
3DMM space with NeRF-based face representations can realize motion control, it
is sub-optimal for their limited face-only modeling and low identity fidelity.
Therefore, we are inspired to ask: Can we learn the dense correspondence
between different NeRF-based face representations without a 3D parametric model
prior? To address this challenge, we propose a novel framework, which adopts
tri-planes as fundamental NeRF representation and decomposes face tri-planes
into three components: canonical tri-planes, identity deformations, and motion.
In terms of motion control, our key contribution is proposing a Plane
Dictionary (PlaneDict) module, which efficiently maps the motion conditions to
a linear weighted addition of learnable orthogonal plane bases. To the best of
our knowledge, our framework is the first method that achieves one-shot
multi-view face reenactment without a 3D parametric model prior. Extensive
experiments demonstrate that we produce better results in fine-grained motion
control and identity preservation than previous methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10439">Simple Image-level Classification Improves Open-vocabulary Object Detection. (arXiv:2312.10439v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Ruohuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiao Bai</a></p>
<p>Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a
given set of base categories on which the detection model is trained. Recent
OVOD methods focus on adapting the image-level pre-trained vision-language
models (VLMs), such as CLIP, to a region-level object detection task via, eg.,
region-level knowledge distillation, regional prompt learning, or region-text
pre-training, to expand the detection vocabulary. These methods have
demonstrated remarkable performance in recognizing regional visual concepts,
but they are weak in exploiting the VLMs' powerful global scene understanding
ability learned from the billion-scale image-level text descriptions. This
limits their capability in detecting hard objects of small, blurred, or
occluded appearance from novel/base categories, whose detection heavily relies
on contextual information. To address this, we propose a novel approach, namely
Simple Image-level Classification for Context-Aware Detection Scoring
(SIC-CADS), to leverage the superior global knowledge yielded from CLIP for
complementing the current OVOD models from a global perspective. The core of
SIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the
object co-occurrence-based contextual information from CLIP to recognize all
possible object categories in the scene. These image-level MLR scores can then
be utilized to refine the instance-level detection scores of the current OVOD
models in detecting those hard objects. This is verified by extensive empirical
results on two popular benchmarks, OV-LVIS and OV-COCO, which show that
SIC-CADS achieves significant and consistent improvement when combined with
different types of OVOD models. Further, SIC-CADS also improves the
cross-dataset generalization ability on Objects365 and OpenImages. The code is
available at https://github.com/mala-lab/SIC-CADS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10447">Finger Biometric Recognition With Feature Selection. (arXiv:2312.10447v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1">Asish Bera</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1">Debotosh Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasipuri_M/0/1/0/all/0/1">Mita Nasipuri</a></p>
<p>Biometrics is indispensable in this modern digital era for secure automated
human authentication in various fields of machine learning and pattern
recognition. Hand geometry is a promising physiological biometric trait with
ample deployed application areas for identity verification. Due to the
intricate anatomic foundation of the thumb and substantial inter-finger posture
variation, satisfactory performances cannot be achieved while the thumb is
included in the contact-free environment. To overcome the hindrances associated
with the thumb, four finger-based (excluding the thumb) biometric approaches
have been devised. In this chapter, a four-finger based biometric method has
been presented. Again, selection of salient features is essential to reduce the
feature dimensionality by eliminating the insignificant features. Weights are
assigned according to the discriminative efficiency of the features to
emphasize on the essential features. Two different strategies namely, the
global and local feature selection methods are adopted based on the adaptive
forward-selection and backward-elimination (FoBa) algorithm. The identification
performances are evaluated using the weighted k-nearest neighbor (wk-NN) and
random forest (RF) classifiers. The experiments are conducted using the
selected feature subsets over the 300 subjects of the Bosphorus hand database.
The best identification accuracy of 98.67%, and equal error rate (EER) of 4.6%
have been achieved using the subset of 25 features which are selected by the
rank-based local FoBa algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10656">VidToMe: Video Token Merging for Zero-Shot Video Editing. (arXiv:2312.10656v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xirui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Diffusion models have made significant advances in generating high-quality
images, but their application to video generation has remained challenging due
to the complexity of temporal motion. Zero-shot video editing offers a solution
by utilizing pre-trained image diffusion models to translate source videos into
new ones. Nevertheless, existing methods struggle to maintain strict temporal
consistency and efficient memory consumption. In this work, we propose a novel
approach to enhance temporal consistency in generated videos by merging
self-attention tokens across frames. By aligning and compressing temporally
redundant tokens across frames, our method improves temporal coherence and
reduces memory consumption in self-attention computations. The merging strategy
matches and aligns tokens according to the temporal correspondence between
frames, facilitating natural temporal consistency in generated video frames. To
manage the complexity of video processing, we divide videos into chunks and
develop intra-chunk local token merging and inter-chunk global token merging,
ensuring both short-term video continuity and long-term content consistency.
Our video editing approach seamlessly extends the advancements in image editing
to video editing, rendering favorable results in temporal consistency over
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10686">Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated Outlier Class Learning. (arXiv:2312.10686v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1">Wenjun Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jin Zheng</a></p>
<p>Existing out-of-distribution (OOD) methods have shown great success on
balanced datasets but become ineffective in long-tailed recognition (LTR)
scenarios where 1) OOD samples are often wrongly classified into head classes
and/or 2) tail-class samples are treated as OOD samples. To address these
issues, current studies fit a prior distribution of auxiliary/pseudo OOD data
to the long-tailed in-distribution (ID) data. However, it is difficult to
obtain such an accurate prior distribution given the unknowingness of real OOD
samples and heavy class imbalance in LTR. A straightforward solution to avoid
the requirement of this prior is to learn an outlier class to encapsulate the
OOD samples. The main challenge is then to tackle the aforementioned confusion
between OOD samples and head/tail-class samples when learning the outlier
class. To this end, we introduce a novel calibrated outlier class learning
(COCL) approach, in which 1) a debiased large margin learning method is
introduced in the outlier class learning to distinguish OOD samples from both
head and tail classes in the representation space and 2) an outlier-class-aware
logit calibration method is defined to enhance the long-tailed classification
confidence. Extensive empirical results on three popular benchmarks CIFAR10-LT,
CIFAR100-LT, and ImageNet-LT demonstrate that COCL substantially outperforms
state-of-the-art OOD detection methods in LTR while being able to improve the
classification accuracy on ID data. Code is available at
https://github.com/mala-lab/COCL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10737">Traffic Incident Database with Multiple Labels Including Various Perspective Environmental Information. (arXiv:2312.10737v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nishiyama_S/0/1/0/all/0/1">Shota Nishiyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_T/0/1/0/all/0/1">Takuma Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakamura_R/0/1/0/all/0/1">Ryo Nakamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohtani_G/0/1/0/all/0/1">Go Ohtani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1">Hirokatsu Kataoka</a>, <a href="http://arxiv.org/find/cs/1/au:+Hara_K/0/1/0/all/0/1">Kensho Hara</a></p>
<p>A large dataset of annotated traffic accidents is necessary to improve the
accuracy of traffic accident recognition using deep learning models.
Conventional traffic accident datasets provide annotations on traffic accidents
and other teacher labels, improving traffic accident recognition performance.
However, the labels annotated in conventional datasets need to be more
comprehensive to describe traffic accidents in detail. Therefore, we propose
V-TIDB, a large-scale traffic accident recognition dataset annotated with
various environmental information as multi-labels. Our proposed dataset aims to
improve the performance of traffic accident recognition by annotating ten types
of environmental information as teacher labels in addition to the presence or
absence of traffic accidents. V-TIDB is constructed by collecting many videos
from the Internet and annotating them with appropriate environmental
information. In our experiments, we compare the performance of traffic accident
recognition when only labels related to the presence or absence of traffic
accidents are trained and when environmental information is added as a
multi-label. In the second experiment, we compare the performance of the
training with only contact level, which represents the severity of the traffic
accident, and the performance with environmental information added as a
multi-label. The results showed that 6 out of 10 environmental information
labels improved the performance of recognizing the presence or absence of
traffic accidents. In the experiment on the degree of recognition of traffic
accidents, the performance of recognition of car wrecks and contacts was
improved for all environmental information. These experiments show that V-TIDB
can be used to learn traffic accident recognition models that take
environmental information into account in detail and can be used for
appropriate traffic accident analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10798">Land use/land cover classification of fused Sentinel-1 and Sentinel-2 imageries using ensembles of Random Forests. (arXiv:2312.10798v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1">Shivam Pande</a></p>
<p>The study explores the synergistic combination of Synthetic Aperture Radar
(SAR) and Visible-Near Infrared-Short Wave Infrared (VNIR-SWIR) imageries for
land use/land cover (LULC) classification. Image fusion, employing Bayesian
fusion, merges SAR texture bands with VNIR-SWIR imageries. The research aims to
investigate the impact of this fusion on LULC classification. Despite the
popularity of random forests for supervised classification, their limitations,
such as suboptimal performance with fewer features and accuracy stagnation, are
addressed. To overcome these issues, ensembles of random forests (RFE) are
created, introducing random rotations using the Forest-RC algorithm. Three
rotation approaches: principal component analysis (PCA), sparse random rotation
(SRP) matrix, and complete random rotation (CRP) matrix are employed.
Sentinel-1 SAR data and Sentinel-2 VNIR-SWIR data from the IIT-Kanpur region
constitute the training datasets, including SAR, SAR with texture, VNIR-SWIR,
VNIR-SWIR with texture, and fused VNIR-SWIR with texture. The study evaluates
classifier efficacy, explores the impact of SAR and VNIR-SWIR fusion on
classification, and significantly enhances the execution speed of Bayesian
fusion code. The SRP-based RFE outperforms other ensembles for the first two
datasets, yielding average overall kappa values of 61.80% and 68.18%, while the
CRP-based RFE excels for the last three datasets with average overall kappa
values of 95.99%, 96.93%, and 96.30%. The fourth dataset achieves the highest
overall kappa of 96.93%. Furthermore, incorporating texture with SAR bands
results in a maximum overall kappa increment of 10.00%, while adding texture to
VNIR-SWIR bands yields a maximum increment of approximately 3.45%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11315">CaRe-CNN: Cascading Refinement CNN for Myocardial Infarct Segmentation with Microvascular Obstructions. (arXiv:2312.11315v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thaler_F/0/1/0/all/0/1">Franz Thaler</a>, <a href="http://arxiv.org/find/cs/1/au:+Gsell_M/0/1/0/all/0/1">Matthias A.F. Gsell</a>, <a href="http://arxiv.org/find/cs/1/au:+Plank_G/0/1/0/all/0/1">Gernot Plank</a>, <a href="http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1">Martin Urschler</a></p>
<p>Late gadolinium enhanced (LGE) magnetic resonance (MR) imaging is widely
established to assess the viability of myocardial tissue of patients after
acute myocardial infarction (MI). We propose the Cascading Refinement CNN
(CaRe-CNN), which is a fully 3D, end-to-end trained, 3-stage CNN cascade that
exploits the hierarchical structure of such labeled cardiac data. Throughout
the three stages of the cascade, the label definition changes and CaRe-CNN
learns to gradually refine its intermediate predictions accordingly.
Furthermore, to obtain more consistent qualitative predictions, we propose a
series of post-processing steps that take anatomical constraints into account.
Our CaRe-CNN was submitted to the FIMH 2023 MYOSAIQ challenge, where it ranked
second out of 18 participating teams. CaRe-CNN showed great improvements most
notably when segmenting the difficult but clinically most relevant myocardial
infarct tissue (MIT) as well as microvascular obstructions (MVO). When
computing the average scores over all labels, our method obtained the best
score in eight out of ten metrics. Thus, accurate cardiac segmentation after
acute MI via our CaRe-CNN allows generating patient-specific models of the
heart serving as an important step towards personalized medicine.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11376">CLIM: Contrastive Language-Image Mosaic for Region Representation. (arXiv:2312.11376v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Size Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lumin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Sheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wentao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>Detecting objects accurately from a large or open vocabulary necessitates the
vision-language alignment on region representations. However, learning such a
region-text alignment by obtaining high-quality box annotations with text
labels or descriptions is expensive and infeasible. In contrast, collecting
image-text pairs is simpler but lacks precise object location information to
associate regions with texts. In this paper, we propose a novel approach called
Contrastive Language-Image Mosaic (CLIM), which leverages large-scale
image-text pairs effectively for aligning region and text representations. CLIM
combines multiple images into a mosaicked image and treats each image as a
`pseudo region'. The feature of each pseudo region is extracted and trained to
be similar to the corresponding text embedding while dissimilar from others by
a contrastive loss, enabling the model to learn the region-text alignment
without costly box annotations. As a generally applicable approach, CLIM
consistently improves different open-vocabulary object detection methods that
use caption supervision. Furthermore, CLIM can effectively enhance the region
representation of vision-language models, thus providing stronger backbones for
open-vocabulary object detectors. Our experimental results demonstrate that
CLIM improves different baseline open-vocabulary object detectors by a large
margin on both OV-COCO and OV-LVIS benchmarks. The code is available at
https://github.com/wusize/CLIM.
</p>
</p>
</div>

    </div>
    </body>
    