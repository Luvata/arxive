<!DOCTYPE html>
<html>
<head>
<title>2023-12-07-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.02168">The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch. (arXiv:2312.02168v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tim Z. Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenn_J/0/1/0/all/0/1">Johannes Zenn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1">Robert Bamler</a></p>
<p>The Street View House Numbers (SVHN) dataset is a popular benchmark dataset
in deep learning. Originally designed for digit classification tasks, the SVHN
dataset has been widely used as a benchmark for various other tasks including
generative modeling. However, with this work, we aim to warn the community
about an issue of the SVHN dataset as a benchmark for generative modeling
tasks: we discover that the official split into training set and test set of
the SVHN dataset are not drawn from the same distribution. We empirically show
that this distribution mismatch has little impact on the classification task
(which may explain why this issue has not been detected before), but it
severely affects the evaluation of probabilistic generative models, such as
Variational Autoencoders and diffusion models. As a workaround, we propose to
mix and re-split the official training and test set when SVHN is used for tasks
other than classification. We publish a new split and the indices we used to
create it at https://jzenn.github.io/svhn-remix/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02178">Hierarchical ML Codebook Design for Extreme MIMO Beam Management. (arXiv:2312.02178v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dreifuerst_R/0/1/0/all/0/1">Ryan M. Dreifuerst</a>, <a href="http://arxiv.org/find/eess/1/au:+Heath_R/0/1/0/all/0/1">Robert W. Heath Jr</a></p>
<p>Beam management is a strategy to unify beamforming and channel state
information (CSI) acquisition with large antenna arrays in 5G. Codebooks serve
multiple uses in beam management including beamforming reference signals, CSI
reporting, and analog beam training. In this paper, we propose and evaluate a
machine learning-refined codebook design process for extremely large
multiple-input multiple-output (X-MIMO) systems. We propose a neural network
and beam selection strategy to design the initial access and refinement
codebooks using end-to-end learning from beamspace representations. The
algorithm, called Extreme-Beam Management (X-BM), can significantly improve the
performance of extremely large arrays as envisioned for 6G and capture
realistic wireless and physical layer aspects. Our results show an 8dB
improvement in initial access and overall effective spectral efficiency
improvements compared to traditional codebook methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02179">Training Chain-of-Thought via Latent-Variable Inference. (arXiv:2312.02179v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phan_D/0/1/0/all/0/1">Du Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1">Matthew D. Hoffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1">David Dohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Douglas_S/0/1/0/all/0/1">Sholto Douglas</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Tuan Anh Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sountsov_P/0/1/0/all/0/1">Pavel Sountsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1">Charles Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+Vikram_S/0/1/0/all/0/1">Sharad Vikram</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1">Rif A. Saurous</a></p>
<p>Large language models (LLMs) solve problems more accurately and interpretably
when instructed to work out the answer step by step using a
``chain-of-thought'' (CoT) prompt. One can also improve LLMs' performance on a
specific task by supervised fine-tuning, i.e., by using gradient ascent on some
tunable parameters to maximize the average log-likelihood of correct answers
from a labeled training set. Naively combining CoT with supervised tuning
requires supervision not just of the correct answers, but also of detailed
rationales that lead to those answers; these rationales are expensive to
produce by hand. Instead, we propose a fine-tuning strategy that tries to
maximize the \emph{marginal} log-likelihood of generating a correct answer
using CoT prompting, approximately averaging over all possible rationales. The
core challenge is sampling from the posterior over rationales conditioned on
the correct answer; we address it using a simple Markov-chain Monte Carlo
(MCMC) expectation-maximization (EM) algorithm inspired by the self-taught
reasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent
contrastive divergence. This algorithm also admits a novel control-variate
technique that drives the variance of our gradient estimates to zero as the
model improves. Applying our technique to GSM8K and the tasks in BIG-Bench
Hard, we find that this MCMC-EM fine-tuning technique typically improves the
model's accuracy on held-out examples more than STaR or prompt-tuning with or
without CoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02181">How Generative-AI can be Effectively used in Government Chatbots. (arXiv:2312.02181v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zeteng Lin</a></p>
<p>With the rapid development of artificial intelligence and breakthroughs in
machine learning and natural language processing, intelligent
question-answering robots have become widely used in government affairs. This
paper conducts a horizontal comparison between Guangdong Province's government
chatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the
strengths and weaknesses of existing government chatbots and AIGC technology.
The study finds significant differences between government chatbots and large
language models. China's government chatbots are still in an exploratory stage
and have a gap to close to achieve "intelligence." To explore the future
direction of government chatbots more deeply, this research proposes targeted
optimization paths to help generative AI be effectively applied in government
chatbot conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02182">Adam-like Algorithm with Smooth Clipping Attains Global Minima: Analysis Based on Ergodicity of Functional SDEs. (arXiv:2312.02182v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1">Keisuke Suzuki</a></p>
<p>In this paper, we prove that an Adam-type algorithm with smooth clipping
approaches the global minimizer of the regularized non-convex loss function.
Adding smooth clipping and taking the state space as the set of all
trajectories, we can apply the ergodic theory of Markov semigroups for this
algorithm and investigate its asymptotic behavior. The ergodic theory we
establish in this paper reduces the problem of evaluating the convergence,
generalization error and discretization error of this algorithm to the problem
of evaluating the difference between two functional stochastic differential
equations (SDEs) with different drift coefficients. As a result of our
analysis, we have shown that this algorithm minimizes the the regularized
non-convex loss function with errors of the form $n^{-1/2}$, $\eta^{1/4}$,
$\beta^{-1} \log (\beta + 1)$ and $e^{- c t}$. Here, $c$ is a constant and $n$,
$\eta$, $\beta$ and $t$ denote the size of the training dataset, learning rate,
inverse temperature and time, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02184">Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map based Complex-valued Precoding Network Approach. (arXiv:2312.02184v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiacheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zeyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuhang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Haibo Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuemin/0/1/0/all/0/1">Xuemin</a> (Sherman) <a href="http://arxiv.org/find/cs/1/au:+Shen/0/1/0/all/0/1">Shen</a></p>
<p>As the demand for high-quality services proliferates, an innovative network
architecture, the fully-decoupled RAN (FD-RAN), has emerged for more flexible
spectrum resource utilization and lower network costs. However, with the
decoupling of uplink base stations and downlink base stations in FD-RAN, the
traditional transmission mechanism, which relies on real-time channel feedback,
is not suitable as the receiver is not able to feedback accurate and timely
channel state information to the transmitter. This paper proposes a novel
transmission scheme without relying on physical layer channel feedback.
Specifically, we design a radio map based complex-valued precoding
network~(RMCPNet) model, which outputs the base station precoding based on user
location. RMCPNet comprises multiple subnets, with each subnet responsible for
extracting unique modal features from diverse input modalities. Furthermore,
the multi-modal embeddings derived from these distinct subnets are integrated
within the information fusion layer, culminating in a unified representation.
We also develop a specific RMCPNet training algorithm that employs the negative
spectral efficiency as the loss function. We evaluate the performance of the
proposed scheme on the public DeepMIMO dataset and show that RMCPNet can
achieve 16\% and 76\% performance improvements over the conventional
real-valued neural network and statistical codebook approach, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02185">Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition. (arXiv:2312.02185v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duc-Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1">Cuong Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Khac_N/0/1/0/all/0/1">Nhien-An Le-Khac</a></p>
<p>Various types of sensors can be used for Human Activity Recognition (HAR),
and each of them has different strengths and weaknesses. Sometimes a single
sensor cannot fully observe the user's motions from its perspective, which
causes wrong predictions. While sensor fusion provides more information for
HAR, it comes with many inherent drawbacks like user privacy and acceptance,
costly set-up, operation, and maintenance. To deal with this problem, we
propose Virtual Fusion - a new method that takes advantage of unlabeled data
from multiple time-synchronized sensors during training, but only needs one
sensor for inference. Contrastive learning is adopted to exploit the
correlation among sensors. Virtual Fusion gives significantly better accuracy
than training with the same single sensor, and in some cases, it even surpasses
actual fusion using multiple sensors at test time. We also extend this method
to a more general version called Actual Fusion within Virtual Fusion (AFVF),
which uses a subset of training sensors during inference. Our method achieves
state-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark
datasets. Implementation is available upon request.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02186">Identifying Spurious Correlations using Counterfactual Alignment. (arXiv:2312.02186v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1">Joseph Paul Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Blankemeier_L/0/1/0/all/0/1">Louis Blankemeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1">Akshay Chaudhari</a></p>
<p>Models driven by spurious correlations often yield poor generalization
performance. We propose the counterfactual alignment method to detect and
explore spurious correlations of black box classifiers. Counterfactual images
generated with respect to one classifier can be input into other classifiers to
see if they also induce changes in the outputs of these classifiers. The
relationship between these responses can be quantified and used to identify
specific instances where a spurious correlation exists as well as compute
aggregate statistics over a dataset. Our work demonstrates the ability to
detect spurious correlations in face attribute classifiers. This is validated
by observing intuitive trends in a face attribute classifier as well as
fabricating spurious correlations and detecting their presence, both visually
and quantitatively. Further, utilizing the CF alignment method, we demonstrate
that we can rectify spurious correlations identified in classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02195">Cancer Subtype Identification through Integrating Inter and Intra Dataset Relationships in Multi-Omics Data. (arXiv:2312.02195v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peelen_M/0/1/0/all/0/1">Mark Peelen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagheriye_L/0/1/0/all/0/1">Leila Bagheriye</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwisthout_J/0/1/0/all/0/1">Johan Kwisthout</a></p>
<p>The integration of multi-omics data has emerged as a promising approach for
gaining comprehensive insights into complex diseases such as cancer. This paper
proposes a novel approach to identify cancer subtypes through the integration
of multi-omics data for clustering. The proposed method, named LIDAF utilises
affinity matrices based on linear relationships between and within different
omics datasets (Linear Inter and Intra Dataset Affinity Fusion (LIDAF)).
Canonical Correlation Analysis is in this paper employed to create distance
matrices based on Euclidean distances between canonical variates. The distance
matrices are converted to affinity matrices and those are fused in a three-step
process. The proposed LIDAF addresses the limitations of the existing method
resulting in improvement of clustering performance as measured by the Adjusted
Rand Index and the Normalized Mutual Information score. Moreover, our proposed
LIDAF approach demonstrates a notable enhancement in 50% of the log10 rank
p-values obtained from Cox survival analysis, surpassing the performance of the
best reported method, highlighting its potential of identifying distinct cancer
subtypes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02199">USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery. (arXiv:2312.02199v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1">Jeremy Irvin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1">Lucas Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joanne Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuntao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Nashold_L/0/1/0/all/0/1">Langston Nashold</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Benjamin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1">Andrew Y. Ng</a></p>
<p>Large, self-supervised vision models have led to substantial advancements for
automatically interpreting natural images. Recent works have begun tailoring
these methods to remote sensing data which has rich structure with
multi-sensor, multi-spectral, and temporal information providing massive
amounts of self-labeled data that can be used for self-supervised pre-training.
In this work, we develop a new encoder architecture called USat that can input
multi-spectral data from multiple sensors for self-supervised pre-training.
USat is a vision transformer with modified patch projection layers and
positional encodings to model spectral bands with varying spatial scales from
multiple sensors. We integrate USat into a Masked Autoencoder (MAE)
self-supervised pre-training procedure and find that a pre-trained USat
outperforms state-of-the-art self-supervised MAE models trained on remote
sensing data on multiple remote sensing benchmark datasets (up to 8%) and leads
to improvements in low data regimes (up to 7%). Code and pre-trained weights
are available at https://github.com/stanfordmlgroup/USat .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02203">Learning High-Order Relationships of Brain Regions. (arXiv:2312.02203v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Qiu_W/0/1/0/all/0/1">Weikang Qiu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chu_H/0/1/0/all/0/1">Huangrui Chu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_S/0/1/0/all/0/1">Selena Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zuo_H/0/1/0/all/0/1">Haolan Zuo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1">Xiaoxiao Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhao_Y/0/1/0/all/0/1">Yize Zhao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ying_R/0/1/0/all/0/1">Rex Ying</a></p>
<p>Discovering reliable and informative interactions among brain regions from
functional magnetic resonance imaging (fMRI) signals is essential in
neuroscientific predictions of cognition. Most of the current methods fail to
accurately characterize those interactions because they only focus on pairwise
connections and overlook the high-order relationships of brain regions. We
delve into this problem and argue that these high-order relationships should be
maximally informative and minimally redundant (MIMR). However, identifying such
high-order relationships is challenging and highly under-explored. Methods that
can be tailored to our context are also non-existent. In response to this gap,
we propose a novel method named HyBRiD that aims to extract MIMR high-order
relationships from fMRI data. HyBRiD employs a Constructor to identify
hyperedge structures, and a Weighter to compute a weight for each hyperedge.
HyBRiD achieves the MIMR objective through an innovative information bottleneck
framework named multi-head drop-bottleneck with theoretical guarantees. Our
comprehensive experiments demonstrate the effectiveness of our model. Our model
outperforms the state-of-the-art predictive model by an average of 12.1%,
regarding the quality of hyperedges measured by CPM, a standard protocol for
studying brain connections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02204">Can We Learn Communication-Efficient Optimizers?. (arXiv:2312.02204v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joseph_C/0/1/0/all/0/1">Charles-&#xc9;tienne Joseph</a>, <a href="http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1">Benjamin Th&#xe9;rien</a>, <a href="http://arxiv.org/find/cs/1/au:+Moudgil_A/0/1/0/all/0/1">Abhinav Moudgil</a>, <a href="http://arxiv.org/find/cs/1/au:+Knyazev_B/0/1/0/all/0/1">Boris Knyazev</a>, <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a></p>
<p>Communication-efficient variants of SGD, specifically local SGD, have
received a great deal of interest in recent years. These approaches compute
multiple gradient steps locally, that is on each worker, before averaging model
parameters, helping relieve the critical communication bottleneck in
distributed deep learning training. Although many variants of these approaches
have been proposed, they can sometimes lag behind state-of-the-art adaptive
optimizers for deep learning. In this work, we investigate if the recent
progress in the emerging area of learned optimizers can potentially close this
gap while remaining communication-efficient. Specifically, we meta-learn how to
perform global updates given an update from local SGD iterations. Our results
demonstrate that learned optimizers can substantially outperform local SGD and
its sophisticated variants while maintaining their communication efficiency.
Learned optimizers can even generalize to unseen and much larger datasets and
architectures, including ImageNet and ViTs, and to unseen modalities such as
language modeling. We therefore demonstrate the potential of learned optimizers
for improving communication-efficient distributed learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02205">Disentangling the Effects of Data Augmentation and Format Transform in Self-Supervised Learning of Image Representations. (arXiv:2312.02205v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1">Neha Kalibhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Morningstar_W/0/1/0/all/0/1">Warren Morningstar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bijamov_A/0/1/0/all/0/1">Alex Bijamov</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Luyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1">Philip Mansfield</a></p>
<p>Self-Supervised Learning (SSL) enables training performant models using
limited labeled data. One of the pillars underlying vision SSL is the use of
data augmentations/perturbations of the input which do not significantly alter
its semantic content. For audio and other temporal signals, augmentations are
commonly used alongside format transforms such as Fourier transforms or wavelet
transforms. Unlike augmentations, format transforms do not change the
information contained in the data; rather, they express the same information in
different coordinates. In this paper, we study the effects of format transforms
and augmentations both separately and together on vision SSL. We define
augmentations in frequency space called Fourier Domain Augmentations (FDA) and
show that training SSL models on a combination of these and image augmentations
can improve the downstream classification accuracy by up to 1.3% on
ImageNet-1K. We also show improvements against SSL baselines in few-shot and
transfer learning setups using FDA. Surprisingly, we also observe that format
transforms can improve the quality of learned representations even without
augmentations; however, the combination of the two techniques yields better
quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02210">Low-Precision Mixed-Computation Models for Inference on Edge. (arXiv:2312.02210v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1">Seyedarmin Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazemi_M/0/1/0/all/0/1">Mahdi Nazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamal_M/0/1/0/all/0/1">Mehdi Kamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1">Massoud Pedram</a></p>
<p>This paper presents a mixed-computation neural network processing approach
for edge applications that incorporates low-precision (low-width) Posit and
low-precision fixed point (FixP) number systems. This mixed-computation
approach employs 4-bit Posit (Posit4), which has higher precision around zero,
for representing weights with high sensitivity, while it uses 4-bit FixP
(FixP4) for representing other weights. A heuristic for analyzing the
importance and the quantization error of the weights is presented to assign the
proper number system to different weights. Additionally, a gradient
approximation for Posit representation is introduced to improve the quality of
weight updates in the backpropagation process. Due to the high energy
consumption of the fully Posit-based computations, neural network operations
are carried out in FixP or Posit/FixP. An efficient hardware implementation of
a MAC operation with a first Posit operand and FixP for a second operand and
accumulator is presented. The efficacy of the proposed low-precision
mixed-computation approach is extensively assessed on vision and language
models. The results show that, on average, the accuracy of the
mixed-computation is about 1.5% higher than that of FixP with a cost of 0.19%
energy overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02213">JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization. (arXiv:2312.02213v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shang-Ching Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">ShengKun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wenqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsiung_C/0/1/0/all/0/1">Chung-Wei Hsiung</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_Y/0/1/0/all/0/1">Yi-Chen Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu-Ping Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Sian-Hong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1">Tsungyao Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianwei Zhang</a></p>
<p>In this study, we introduce JarviX, a sophisticated data analytics framework.
JarviX is designed to employ Large Language Models (LLMs) to facilitate an
automated guide and execute high-precision data analyzes on tabular datasets.
This framework emphasizes the significance of varying column types,
capitalizing on state-of-the-art LLMs to generate concise data insight
summaries, propose relevant analysis inquiries, visualize data effectively, and
provide comprehensive explanations for results drawn from an extensive data
analysis pipeline. Moreover, JarviX incorporates an automated machine learning
(AutoML) pipeline for predictive modeling. This integration forms a
comprehensive and automated optimization cycle, which proves particularly
advantageous for optimizing machine configuration. The efficacy and
adaptability of JarviX are substantiated through a series of practical use case
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02220">QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers. (arXiv:2312.02220v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baras_A/0/1/0/all/0/1">Amit Baras</a>, <a href="http://arxiv.org/find/cs/1/au:+Zolfi_A/0/1/0/all/0/1">Alon Zolfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1">Yuval Elovici</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1">Asaf Shabtai</a></p>
<p>In recent years, there has been a significant trend in deep neural networks
(DNNs), particularly transformer-based models, of developing ever-larger and
more capable models. While they demonstrate state-of-the-art performance, their
growing scale requires increased computational resources (e.g., GPUs with
greater memory capacity). To address this problem, quantization techniques
(i.e., low-bit-precision representation and matrix multiplication) have been
proposed. Most quantization techniques employ a static strategy in which the
model parameters are quantized, either during training or inference, without
considering the test-time sample. In contrast, dynamic quantization techniques,
which have become increasingly popular, adapt during inference based on the
input provided, while maintaining full-precision performance. However, their
dynamic behavior and average-case performance assumption makes them vulnerable
to a novel threat vector -- adversarial attacks that target the model's
efficiency and availability. In this paper, we present QuantAttack, a novel
attack that targets the availability of quantized models, slowing down the
inference, and increasing memory usage and energy consumption. We show that
carefully crafted adversarial examples, which are designed to exhaust the
resources of the operating system, can trigger worst-case performance. In our
experiments, we demonstrate the effectiveness of our attack on vision
transformers on a wide range of tasks, both uni-modal and multi-modal. We also
examine the effect of different attack variants (e.g., a universal
perturbation) and the transferability between different models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02225">Digital Histopathology with Graph Neural Networks: Concepts and Explanations for Clinicians. (arXiv:2312.02225v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Villaforesta_A/0/1/0/all/0/1">Alessandro Farace di Villaforesta</a>, <a href="http://arxiv.org/find/physics/1/au:+Magister_L/0/1/0/all/0/1">Lucie Charlotte Magister</a>, <a href="http://arxiv.org/find/physics/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/physics/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>To address the challenge of the ``black-box" nature of deep learning in
medical settings, we combine GCExplainer - an automated concept discovery
solution - along with Logic Explained Networks to provide global explanations
for Graph Neural Networks. We demonstrate this using a generally applicable
graph construction and classification pipeline, involving panoptic segmentation
with HoVer-Net and cancer prediction with Graph Convolution Networks. By
training on H&amp;E slides of breast cancer, we show promising results in offering
explainable and trustworthy AI tools for clinicians.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02227">Improving Multimodal Sentiment Analysis: Supervised Angular Margin-based Contrastive Learning for Enhanced Fusion Representation. (arXiv:2312.02227v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cong-Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1">Duc Anh Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a></p>
<p>The effectiveness of a model is heavily reliant on the quality of the fusion
representation of multiple modalities in multimodal sentiment analysis.
Moreover, each modality is extracted from raw input and integrated with the
rest to construct a multimodal representation. Although previous methods have
proposed multimodal representations and achieved promising results, most of
them focus on forming positive and negative pairs, neglecting the variation in
sentiment scores within the same class. Additionally, they fail to capture the
significance of unimodal representations in the fusion vector. To address these
limitations, we introduce a framework called Supervised Angular-based
Contrastive Learning for Multimodal Sentiment Analysis. This framework aims to
enhance discrimination and generalizability of the multimodal representation
and overcome biases in the fusion vector's modality. Our experimental results,
along with visualizations on two widely used datasets, demonstrate the
effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02229">Synthetic Data Generation Techniques for Developing AI-based Speech Assessments for Parkinson&#x27;s Disease (A Comparative Study). (arXiv:2312.02229v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parsapoor_M/0/1/0/all/0/1">Mahboobeh Parsapoor</a></p>
<p>Changes in speech and language are among the first signs of Parkinson's
disease (PD). Thus, clinicians have tried to identify individuals with PD from
their voices for years. Doctors can leverage AI-based speech assessments to
spot PD thanks to advancements in artificial intelligence (AI). Such AI systems
can be developed using machine learning classifiers that have been trained
using individuals' voices. Although several studies have shown reasonable
results in developing such AI systems, these systems would need more data
samples to achieve promising performance. This paper explores using deep
learning-based data generation techniques on the accuracy of machine learning
classifiers that are the core of such systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02230">A Simple and Scalable Representation for Graph Generation. (arXiv:2312.02230v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yunhui Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seul Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sungsoo Ahn</a></p>
<p>Recently, there has been a surge of interest in employing neural networks for
graph generation, a fundamental statistical learning problem with critical
applications like molecule design and community analysis. However, most
approaches encounter significant limitations when generating large-scale
graphs. This is due to their requirement to output the full adjacency matrices
whose size grows quadratically with the number of nodes. In response to this
challenge, we introduce a new, simple, and scalable graph representation named
gap encoded edge list (GEEL) that has a small representation size that aligns
with the number of edges. In addition, GEEL significantly reduces the
vocabulary size by incorporating the gap encoding and bandwidth restriction
schemes. GEEL can be autoregressively generated with the incorporation of node
positional encoding, and we further extend GEEL to deal with attributed graphs
by designing a new grammar. Our findings reveal that the adoption of this
compact representation not only enhances scalability but also bolsters
performance by simplifying the graph generation process. We conduct a
comprehensive evaluation across ten non-attributed and two molecular graph
generation tasks, demonstrating the effectiveness of GEEL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02236">Rethinking Adversarial Training with Neural Tangent Kernel. (arXiv:2312.02236v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Han Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shangwei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a></p>
<p>Adversarial training (AT) is an important and attractive topic in deep
learning security, exhibiting mysteries and odd properties. Recent studies of
neural network training dynamics based on Neural Tangent Kernel (NTK) make it
possible to reacquaint AT and deeply analyze its properties. In this paper, we
perform an in-depth investigation of AT process and properties with NTK, such
as NTK evolution. We uncover three new findings that are missed in previous
works. First, we disclose the impact of data normalization on AT and the
importance of unbiased estimators in batch normalization layers. Second, we
experimentally explore the kernel dynamics and propose more time-saving AT
methods. Third, we study the spectrum feature inside the kernel to address the
catastrophic overfitting problem. To the best of our knowledge, it is the first
work leveraging the observations of kernel dynamics to improve existing AT
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02239">Model-based Deep Learning for Beam Prediction based on a Channel Chart. (arXiv:2312.02239v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yassine_T/0/1/0/all/0/1">Taha Yassine</a> (IETR, INSA Rennes), <a href="http://arxiv.org/find/cs/1/au:+Chatelier_B/0/1/0/all/0/1">Baptiste Chatelier</a> (IETR, MERCE-France, INSA Rennes), <a href="http://arxiv.org/find/cs/1/au:+Corlay_V/0/1/0/all/0/1">Vincent Corlay</a> (MERCE-France), <a href="http://arxiv.org/find/cs/1/au:+Crussiere_M/0/1/0/all/0/1">Matthieu Crussi&#xe8;re</a> (IETR, INSA Rennes), <a href="http://arxiv.org/find/cs/1/au:+Paquelet_S/0/1/0/all/0/1">Stephane Paquelet</a>, <a href="http://arxiv.org/find/cs/1/au:+Tirkkonen_O/0/1/0/all/0/1">Olav Tirkkonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Magoarou_L/0/1/0/all/0/1">Luc Le Magoarou</a> (INSA Rennes, IETR)</p>
<p>Channel charting builds a map of the radio environment in an unsupervised
way. The obtained chart locations can be seen as low-dimensional compressed
versions of channel state information that can be used for a wide variety of
applications, including beam prediction. In non-standalone or cell-free
systems, chart locations computed at a given base station can be transmitted to
several other base stations (possibly operating at different frequency bands)
for them to predict which beams to use. This potentially yields a dramatic
reduction of the overhead due to channel estimation or beam management, since
only the base station performing charting requires channel state information,
the others directly predicting the beam from the chart location. In this paper,
advanced model-based neural network architectures are proposed for both channel
charting and beam prediction. The proposed methods are assessed on realistic
synthetic channels, yielding promising results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02243">FlowHON: Representing Flow Fields Using Higher-Order Networks. (arXiv:2312.02243v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhihong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jun Tao</a></p>
<p>Flow fields are often partitioned into data blocks for massively parallel
computation and analysis based on blockwise relationships. However, most of the
previous techniques only consider the first-order dependencies among blocks,
which is insufficient in describing complex flow patterns. In this work, we
present FlowHON, an approach to construct higher-order networks (HONs) from
flow fields. FlowHON captures the inherent higher-order dependencies in flow
fields as nodes and estimates the transitions among them as edges. We formulate
the HON construction as an optimization problem with three linear
transformations. The first two layers correspond to the node generation and the
third one corresponds to edge estimation. Our formulation allows the node
generation and edge estimation to be solved in a unified framework. With
FlowHON, the rich set of traditional graph algorithms can be applied without
any modification to analyze flow fields, while leveraging the higher-order
information to understand the inherent structure and manage flow data for
efficiency. We demonstrate the effectiveness of FlowHON using a series of
downstream tasks, including estimating the density of particles during tracing,
partitioning flow fields for data management, and understanding flow fields
using the node-link diagram representation of networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02246">Conditional Variational Diffusion Models. (arXiv:2312.02246v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1">Gabriel della Maggiora</a>, <a href="http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1">Luis Alberto Croquevielle</a>, <a href="http://arxiv.org/find/cs/1/au:+Desphande_N/0/1/0/all/0/1">Nikita Desphande</a>, <a href="http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1">Harry Horsley</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1">Thomas Heinis</a>, <a href="http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1">Artur Yakimovich</a></p>
<p>Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02247">Federated Active Learning for Target Domain Generalisation. (arXiv:2312.02247v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caramalau_R/0/1/0/all/0/1">Razvan Caramalau</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1">Binod Bhattarai</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1">Danail Stoyanov</a></p>
<p>In this paper, we introduce Active Learning framework in Federated Learning
for Target Domain Generalisation, harnessing the strength from both learning
paradigms. Our framework, FEDALV, composed of Active Learning (AL) and
Federated Domain Generalisation (FDG), enables generalisation of an image
classification model trained from limited source domain client's data without
sharing images to an unseen target domain. To this end, our FDG, FEDA, consists
of two optimisation updates during training, one at the client and another at
the server level. For the client, the introduced losses aim to reduce feature
complexity and condition alignment, while in the server, the regularisation
limits free energy biases between source and target obtained by the global
model. The remaining component of FEDAL is AL with variable budgets, which
queries the server to retrieve and sample the most informative local data for
the targeted client. We performed multiple experiments on FDG w/ and w/o AL and
compared with both conventional FDG baselines and Federated Active Learning
baselines. Our extensive quantitative experiments demonstrate the superiority
of our method in accuracy and efficiency compared to the multiple contemporary
methods. FEDALV manages to obtain the performance of the full training target
accuracy while sampling as little as 5% of the source client's data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02248">Towards early diagnosis of Alzheimer&#x27;s disease: Advances in immune-related blood biomarkers and computational modeling approaches. (arXiv:2312.02248v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Krix_S/0/1/0/all/0/1">Sophia Krix</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wilczynski_E/0/1/0/all/0/1">Ella Wilczynski</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Falgas_N/0/1/0/all/0/1">Neus Falg&#xe0;s</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sanchez_Valle_R/0/1/0/all/0/1">Raquel S&#xe1;nchez-Valle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yoles_E/0/1/0/all/0/1">Eti Yoles</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nevo_U/0/1/0/all/0/1">Uri Nevo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Baruch_K/0/1/0/all/0/1">Kuti Baruch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Frohlich_H/0/1/0/all/0/1">Holger Fr&#xf6;hlich</a></p>
<p>Alzheimer's disease has an increasing prevalence in the population
world-wide, yet current diagnostic methods based on recommended biomarkers are
only available in specialized clinics. Due to these circumstances, Alzheimer's
disease is usually diagnosed late, which contrasts with the currently available
treatment options that are only effective for patients at an early stage.
Blood-based biomarkers could fill in the gap of easily accessible and low-cost
methods for early diagnosis of the disease. In particular, immune-based
blood-biomarkers might be a promising option, given the recently discovered
cross-talk of immune cells of the central nervous system with those in the
peripheral immune system. With the help of machine learning algorithms and
mechanistic modeling approaches, such as agent-based modeling, an in-depth
analysis of the simulation of cell dynamics is possible as well as of
high-dimensional omics resources indicative of pathway signaling changes. Here,
we give a background on advances in research on brain-immune system cross-talk
in Alzheimer's disease and review recent machine learning and mechanistic
modeling approaches which leverage modern omics technologies for blood-based
immune system-related biomarker discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02251">Fine-Tuning Language Models for Context-Specific SQL Query Generation. (arXiv:2312.02251v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rebei_A/0/1/0/all/0/1">Amine Rebei</a></p>
<p>The ability to generate SQL queries from natural language has significant
implications for making data accessible to non-specialists. This paper presents
a novel approach to fine-tuning open-source large language models (LLMs) for
the task of transforming natural language into SQL queries within the retail
domain. We introduce models specialized in generating SQL queries, trained on
synthetic datasets tailored to the Snowflake SQL and GoogleSQL dialects. Our
methodology involves generating a context-specific dataset using GPT-4, then
fine-tuning three open-source LLMs(Starcoder Plus, Code-Llama, and Mistral)
employing the LoRa technique to optimize for resource constraints. The
fine-tuned models demonstrate superior performance in zero-shot settings
compared to the baseline GPT-4, with Code-Llama achieving the highest accuracy
rates, at 81.58% for Snowflake SQL and 82.66% for GoogleSQL. These results
underscore the effectiveness of fine-tuning LLMs on domain-specific tasks and
suggest a promising direction for enhancing the accessibility of relational
databases through natural language interfaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02253">Diversify, Don&#x27;t Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images. (arXiv:2312.02253v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhuoran Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chenchen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1">Sean Culatana</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1">Raghuraman Krishnamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1">Fanyi Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yong Jae Lee</a></p>
<p>Recent advances in generative deep learning have enabled the creation of
high-quality synthetic images in text-to-image generation. Prior work shows
that fine-tuning a pretrained diffusion model on ImageNet and generating
synthetic training images from the finetuned model can enhance an ImageNet
classifier's performance. However, performance degrades as synthetic images
outnumber real ones. In this paper, we explore whether generative fine-tuning
is essential for this improvement and whether it is possible to further scale
up training using more synthetic data. We present a new framework leveraging
off-the-shelf generative models to generate synthetic training images,
addressing multiple challenges: class name ambiguity, lack of diversity in
naive prompts, and domain shifts. Specifically, we leverage large language
models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we
propose contextualized diversification (CD) and stylized diversification (SD)
methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage
domain adaptation techniques with auxiliary batch normalization for synthetic
images. Our framework consistently enhances recognition model performance with
more synthetic data, up to 6x of original ImageNet size showcasing the
potential of synthetic data for improved recognition models and strong
out-of-domain generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02254">Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction. (arXiv:2312.02254v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_I/0/1/0/all/0/1">Ishaan Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayalasomayajula_S/0/1/0/all/0/1">Samyutha Ayalasomayajula</a>, <a href="http://arxiv.org/find/cs/1/au:+Shashidhara_Y/0/1/0/all/0/1">Yashas Shashidhara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kataria_A/0/1/0/all/0/1">Anish Kataria</a>, <a href="http://arxiv.org/find/cs/1/au:+Shashidhara_S/0/1/0/all/0/1">Shreyas Shashidhara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kataria_K/0/1/0/all/0/1">Krishita Kataria</a>, <a href="http://arxiv.org/find/cs/1/au:+Undurti_A/0/1/0/all/0/1">Aditya Undurti</a></p>
<p>The prediction of crop yields internationally is a crucial objective in
agricultural research. Thus, this study implements 6 regression models (Linear,
Tree, Gradient Descent, Gradient Boosting, K- Nearest Neighbors, and Random
Forest) to predict crop yields in 196 countries. Given 4 key training
parameters, pesticides (tonnes), rainfall (mm), temperature (Celsius), and
yield (hg/ha), it was found that our Random Forest Regression model achieved a
determination coefficient (r^2) of 0.94, with a margin of error (ME) of .03.
The models were trained and tested using the Food and Agricultural Organization
of the United Nations data, along with the World Bank Climate Change Data
Catalog. Furthermore, each parameter was analyzed to understand how varying
factors could impact overall yield. We used unconventional models, contrary to
generally used Deep Learning (DL) and Machine Learning (ML) models, combined
with recently collected data to implement a unique approach in our research.
Existing scholarship would benefit from understanding the most optimal model
for agricultural research, specifically using the United Nations data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02255">Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields through Novel Views Synthesis. (arXiv:2312.02255v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tristram_F/0/1/0/all/0/1">Felix Tristram</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1">Stefano Gasperini</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1">Benjamin Busam</a></p>
<p>Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis
capabilities even in large-scale, unbounded scenes, albeit requiring hundreds
of views or introducing artifacts in sparser settings. Their optimization
suffers from shape-radiance ambiguities wherever only a small visual overlap is
available. This leads to erroneous scene geometry and artifacts. In this paper,
we propose Re-Nerfing, a simple and general multi-stage approach that leverages
NeRF's own view synthesis to address these limitations. With Re-Nerfing, we
increase the scene's coverage and enhance the geometric consistency of novel
views as follows: First, we train a NeRF with the available views. Then, we use
the optimized NeRF to synthesize pseudo-views next to the original ones to
simulate a stereo or trifocal setup. Finally, we train a second NeRF with both
original and pseudo views while enforcing structural, epipolar constraints via
the newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset
show the effectiveness of Re-Nerfing across denser and sparser input scenarios,
bringing improvements to the state-of-the-art Zip-NeRF, even when trained with
all views.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02264">Scaling Laws in Jet Classification. (arXiv:2312.02264v1 [hep-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Batson_J/0/1/0/all/0/1">Joshua Batson</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Kahn_Y/0/1/0/all/0/1">Yonatan Kahn</a></p>
<p>We demonstrate the emergence of scaling laws in the benchmark top versus QCD
jet classification problem in collider physics. Six distinct
physically-motivated classifiers exhibit power-law scaling of the binary
cross-entropy test loss as a function of training set size, with distinct power
law indices. This result highlights the importance of comparing classifiers as
a function of dataset size rather than for a fixed training set, as the optimal
classifier may change considerably as the dataset is scaled up. We speculate on
the interpretation of our results in terms of previous models of scaling laws
observed in natural language and image datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02277">ALEXR: Optimal Single-Loop Algorithms for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wang_B/0/1/0/all/0/1">Bokun Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a></p>
<p>This paper revisits a class of convex Finite-Sum Coupled Compositional
Stochastic Optimization (cFCCO) problems with many applications, including
group distributionally robust optimization (GDRO), reinforcement learning, and
learning to rank. To better solve these problems, we introduce a unified family
of efficient single-loop primal-dual block-coordinate proximal algorithms,
dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror
ascent updates for the dual variable and stochastic proximal gradient descent
updates for the primal variable. We establish the convergence rates of ALEXR in
both convex and strongly convex cases under smoothness and non-smoothness
conditions of involved functions, which not only improve the best rates in
previous works on smooth cFCCO problems but also expand the realm of cFCCO for
solving more challenging non-smooth problems such as the dual form of GDRO.
Finally, we present lower complexity bounds to demonstrate that the convergence
rates of ALEXR are optimal among first-order block-coordinate stochastic
algorithms for the considered class of cFCCO problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02296">LLMs Accelerate Annotation for Medical Information Extraction. (arXiv:2312.02296v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1">Akshay Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gueta_A/0/1/0/all/0/1">Almog Gueta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilon_O/0/1/0/all/0/1">Omry Gilon</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Erell_S/0/1/0/all/0/1">Sofia Erell</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Lan Huong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1">Xiaohong Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaber_B/0/1/0/all/0/1">Bolous Jaber</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Shashir Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kartha_R/0/1/0/all/0/1">Rupesh Kartha</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_J/0/1/0/all/0/1">Jean Steiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Laish_I/0/1/0/all/0/1">Itay Laish</a>, <a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1">Amir Feder</a></p>
<p>The unstructured nature of clinical notes within electronic health records
often conceals vital patient-related information, making it challenging to
access or interpret. To uncover this hidden information, specialized Natural
Language Processing (NLP) models are required. However, training these models
necessitates large amounts of labeled data, a process that is both
time-consuming and costly when relying solely on human experts for annotation.
In this paper, we propose an approach that combines Large Language Models
(LLMs) with human expertise to create an efficient method for generating ground
truth labels for medical text annotation. By utilizing LLMs in conjunction with
human annotators, we significantly reduce the human annotation burden, enabling
the rapid creation of labeled datasets. We rigorously evaluate our method on a
medical information extraction task, demonstrating that our approach not only
substantially cuts down on human intervention but also maintains high accuracy.
The results highlight the potential of using LLMs to improve the utilization of
unstructured clinical data, allowing for the swift deployment of tailored NLP
solutions in healthcare.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02298">MoE-AMC: Enhancing Automatic Modulation Classification Performance Using Mixture-of-Experts. (arXiv:2312.02298v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1">Jiaxin Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Cao_Q/0/1/0/all/0/1">Qinglong Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yuntian Chen</a></p>
<p>Automatic Modulation Classification (AMC) plays a vital role in time series
analysis, such as signal classification and identification within wireless
communications. Deep learning-based AMC models have demonstrated significant
potential in this domain. However, current AMC models inadequately consider the
disparities in handling signals under conditions of low and high
Signal-to-Noise Ratio (SNR), resulting in an unevenness in their performance.
In this study, we propose MoE-AMC, a novel Mixture-of-Experts (MoE) based model
specifically crafted to address AMC in a well-balanced manner across varying
SNR conditions. Utilizing the MoE framework, MoE-AMC seamlessly combines the
strengths of LSRM (a Transformer-based model) for handling low SNR signals and
HSRM (a ResNet-based model) for high SNR signals. This integration empowers
MoE-AMC to achieve leading performance in modulation classification, showcasing
its efficacy in capturing distinctive signal features under diverse SNR
scenarios. We conducted experiments using the RML2018.01a dataset, where
MoE-AMC achieved an average classification accuracy of 71.76% across different
SNR levels, surpassing the performance of previous SOTA models by nearly 10%.
This study represents a pioneering application of MoE techniques in the realm
of AMC, offering a promising avenue for elevating signal classification
accuracy within wireless communication systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02299">Cotton Yield Prediction Using Random Forest. (arXiv:2312.02299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Alakananda Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Beegum_S/0/1/0/all/0/1">Sahila Beegum</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisher_D/0/1/0/all/0/1">David Fleisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1">Vangimalla R. Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wenguang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_C/0/1/0/all/0/1">Chittaranjan Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Timlin_D/0/1/0/all/0/1">Dennis Timlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Malakar_A/0/1/0/all/0/1">Arindam Malakar</a></p>
<p>The cotton industry in the United States is committed to sustainable
production practices that minimize water, land, and energy use while improving
soil health and cotton output. Climate-smart agricultural technologies are
being developed to boost yields while decreasing operating expenses. Crop yield
prediction, on the other hand, is difficult because of the complex and
nonlinear impacts of cultivar, soil type, management, pest and disease,
climate, and weather patterns on crops. To solve this issue, we employ machine
learning (ML) to forecast production while considering climate change, soil
diversity, cultivar, and inorganic nitrogen levels. From the 1980s to the
1990s, field data were gathered across the southern cotton belt of the United
States. To capture the most current effects of climate change over the previous
six years, a second data source was produced using the process-based crop
model, GOSSYM. We concentrated our efforts on three distinct areas inside each
of the three southern states: Texas, Mississippi, and Georgia. To simplify the
amount of computations, accumulated heat units (AHU) for each set of
experimental data were employed as an analogy to use time-series weather data.
The Random Forest Regressor yielded a 97.75% accuracy rate, with a root mean
square error of 55.05 kg/ha and an R2 of around 0.98. These findings
demonstrate how an ML technique may be developed and applied as a reliable and
easy-to-use model to support the cotton climate-smart initiative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02300">Reconsideration on evaluation of machine learning models in continuous monitoring using wearables. (arXiv:2312.02300v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Cheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhicheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1">Ran Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nahab_F/0/1/0/all/0/1">Fadi B Nahab</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a></p>
<p>This paper explores the challenges in evaluating machine learning (ML) models
for continuous health monitoring using wearable devices beyond conventional
metrics. We state the complexities posed by real-world variability, disease
dynamics, user-specific characteristics, and the prevalence of false
notifications, necessitating novel evaluation strategies. Drawing insights from
large-scale heart studies, the paper offers a comprehensive guideline for
robust ML model evaluation on continuous health monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02308">AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design. (arXiv:2312.02308v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lacombe_R/0/1/0/all/0/1">Romain Lacombe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendren_L/0/1/0/all/0/1">Lucas Hendren</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Awady_K/0/1/0/all/0/1">Khalid El-Awady</a></p>
<p>A central challenge of the clean energy transition is the development of
catalysts for low-emissions technologies. Recent advances in Machine Learning
for quantum chemistry drastically accelerate the computation of catalytic
activity descriptors such as adsorption energies. Here we introduce AdsorbRL, a
Deep Reinforcement Learning agent aiming to identify potential catalysts given
a multi-objective binding energy target, trained using offline learning on the
Open Catalyst 2020 and Materials Project data sets. We experiment with Deep
Q-Network agents to traverse the space of all ~160,000 possible unary, binary
and ternary compounds of 55 chemical elements, with very sparse rewards based
on adsorption energy known for only between 2,000 and 3,000 catalysts per
adsorbate. To constrain the actions space, we introduce Random Edge Traversal
and train a single-objective DQN agent on the known states subgraph, which we
find strengthens target binding energy by an average of 4.1 eV. We extend this
approach to multi-objective, goal-conditioned learning, and train a DQN agent
to identify materials with the highest (respectively lowest) adsorption
energies for multiple simultaneous target adsorbates. We experiment with
Objective Sub-Sampling, a novel training scheme aimed at encouraging
exploration in the multi-objective setup, and demonstrate simultaneous
adsorption energy improvement across all target adsorbates, by an average of
0.8 eV. Overall, our results suggest strong potential for Deep Reinforcement
Learning applied to the inverse catalysts design problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02309">Training Reinforcement Learning Agents and Humans With Difficulty-Conditioned Generators. (arXiv:2312.02309v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tio_S/0/1/0/all/0/1">Sidney Tio</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1">Jimmy Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1">Pradeep Varakantham</a></p>
<p>We adapt Parameterized Environment Response Model (PERM), a method for
training both Reinforcement Learning (RL) Agents and human learners in
parameterized environments by directly modeling difficulty and ability.
Inspired by Item Response Theory (IRT), PERM aligns environment difficulty with
individual ability, creating a Zone of Proximal Development-based curriculum.
Remarkably, PERM operates without real-time RL updates and allows for offline
training, ensuring its adaptability across diverse students. We present a
two-stage training process that capitalizes on PERM's adaptability, and
demonstrate its effectiveness in training RL agents and humans in an empirical
study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02310">VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding. (arXiv:2312.02310v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1">Uttaran Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yun Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gang Wu</a></p>
<p>Recent advancements in language-model-based video understanding have been
progressing at a remarkable pace, spurred by the introduction of Large Language
Models (LLMs). However, the focus of prior research has been predominantly on
devising a projection layer that maps video features to tokens, an approach
that is both rudimentary and inefficient. In our study, we introduce a
cutting-edge framework, VaQuitA, designed to refine the synergy between video
and textual information. At the data level, instead of sampling frames
uniformly, we implement a sampling method guided by CLIP-score rankings, which
enables a more aligned selection of frames with the given question. At the
feature level, we integrate a trainable Video Perceiver alongside a
Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the
interplay between the input question and the video features. We also discover
that incorporating a simple prompt, "Please be critical", into the LLM input
can substantially enhance its video comprehension capabilities. Our
experimental results indicate that VaQuitA consistently sets a new benchmark
for zero-shot video question-answering tasks and is adept at producing
high-quality, multi-turn video dialogues with users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02312">Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games. (arXiv:2312.02312v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1">Lukas Sch&#xe4;fer</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1">Logan Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1">Anssi Kanervisto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuhan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1">Tabish Rashid</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1">Raluca Georgescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bignell_D/0/1/0/all/0/1">Dave Bignell</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1">Siddhartha Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavito_A/0/1/0/all/0/1">Andrea Trevi&#xf1;o Gavito</a>, <a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1">Sam Devlin</a></p>
<p>Video games have served as useful benchmarks for the decision making
community, but going beyond Atari games towards training agents in modern games
has been prohibitively expensive for the vast majority of the research
community. Recent progress in the research, development and open release of
large vision models has the potential to amortize some of these costs across
the community. However, it is currently unclear which of these models have
learnt representations that retain information critical for sequential decision
making. Towards enabling wider participation in the research of gameplaying
agents in modern games, we present a systematic study of imitation learning
with publicly available visual encoders compared to the typical, task-specific,
end-to-end training approach in Minecraft, Minecraft Dungeons and
Counter-Strike: Global Offensive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02327">FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation. (arXiv:2312.02327v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Abhirup Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1">Cecilia Mascolo</a></p>
<p>Learning a global model by abstracting the knowledge, distributed across
multiple clients, without aggregating the raw data is the primary goal of
Federated Learning (FL). Typically, this works in rounds alternating between
parallel local training at several clients, followed by model aggregation at a
server. We found that existing FL methods under-perform when local datasets are
small and present severe label skew as these lead to over-fitting and local
model bias. This is a realistic setting in many real-world applications. To
address the problem, we propose \textit{FLea}, a unified framework that tackles
over-fitting and local bias by encouraging clients to exchange
privacy-protected features to aid local training. The features refer to
activations from an intermediate layer of the model, which are obfuscated
before being shared with other clients to protect sensitive information in the
data. \textit{FLea} leverages a novel way of combining local and shared
features as augmentations to enhance local model learning. Our extensive
experiments demonstrate that \textit{FLea} outperforms the start-of-the-art FL
methods, sharing only model parameters, by up to $17.6\%$, and FL methods that
share data augmentations by up to $6.3\%$, while reducing the privacy
vulnerability associated with shared data augmentations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02331">Revisiting Topic-Guided Language Models. (arXiv:2312.02331v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Carolina Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Vafa_K/0/1/0/all/0/1">Keyon Vafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1">David M. Blei</a></p>
<p>A recent line of work in natural language processing has aimed to combine
language models and topic models. These topic-guided language models augment
neural language models with topic models, unsupervised learning methods that
can discover document-level patterns of word use. This paper compares the
effectiveness of these methods in a standardized setting. We study four
topic-guided language models and two baselines, evaluating the held-out
predictive performance of each model on four corpora. Surprisingly, we find
that none of these methods outperform a standard LSTM language model baseline,
and most fail to learn good topics. Further, we train a probe of the neural
language model that shows that the baseline's hidden states already encode
topic information. We make public all code used for this study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02339">Expressive Sign Equivariant Networks for Spectral Geometric Learning. (arXiv:2312.02339v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1">Derek Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1">Joshua Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1">Stefanie Jegelka</a>, <a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1">Haggai Maron</a></p>
<p>Recent work has shown the utility of developing machine learning models that
respect the structure and symmetries of eigenvectors. These works promote sign
invariance, since for any eigenvector v the negation -v is also an eigenvector.
However, we show that sign invariance is theoretically limited for tasks such
as building orthogonally equivariant models and learning node positional
encodings for link prediction in graphs. In this work, we demonstrate the
benefits of sign equivariance for these tasks. To obtain these benefits, we
develop novel sign equivariant neural network architectures. Our models are
based on a new analytic characterization of sign equivariant polynomials and
thus inherit provable expressiveness properties. Controlled synthetic
experiments show that our networks can achieve the theoretically predicted
benefits of sign equivariant models. Code is available at
https://github.com/cptq/Sign-Equivariant-Nets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02344">STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a real-world Dataset. (arXiv:2312.02344v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pollak_A/0/1/0/all/0/1">Anton Pollak</a>, <a href="http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1">Rajesh Menon</a></p>
<p>Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that
has tremendous potential in applications where two domains of images and the
need for translation between the two exist, such as the removal of fog. For
example, this could be useful for autonomous vehicles, which currently struggle
with adverse weather conditions like fog. However, datasets for I2I tasks are
not abundant and typically hard to acquire. Here, we introduce STEREOFOG, a
dataset comprised of $10,067$ paired fogged and clear images, captured using a
custom-built device, with the purpose of exploring I2I's potential in this
domain. It is the only real-world dataset of this kind to the best of our
knowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to
this dataset. With the final model achieving an average Complex
Wavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the
technique's suitability for the problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02352">Working Backwards: Learning to Place by Picking. (arXiv:2312.02352v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Limoyo_O/0/1/0/all/0/1">Oliver Limoyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Konar_A/0/1/0/all/0/1">Abhisek Konar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1">Trevor Ablett</a>, <a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1">Jonathan Kelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1">Francois R. Hogan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Gregory Dudek</a></p>
<p>We present Learning to Place by Picking (LPP), a method capable of
autonomously collecting demonstrations for a family of placing tasks in which
objects must be manipulated to specific locations. With LPP, we approach the
learning of robotic object placement policies by reversing the grasping process
and exploiting the inherent symmetry of the pick and place problems.
Specifically, we obtain placing demonstrations from a set of grasp sequences of
objects that are initially located at their target placement locations. Our
system is capable of collecting hundreds of demonstrations without human
intervention by using a combination of tactile sensing and compliant control
for grasps. We train a policy directly from visual observations through
behaviour cloning, using the autonomously-collected demonstrations. By doing
so, the policy can generalize to object placement scenarios outside of the
training environment without privileged information (e.g., placing a plate
picked up from a table and not at the original placement location). We validate
our approach on home robotic scenarios that include dishwasher loading and
table setting. Our approach yields robotic placing policies that outperform
policies trained with kinesthetic teaching, both in terms of performance and
data efficiency, while requiring no human supervision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02355">When is Offline Policy Selection Sample Efficient for Reinforcement Learning?. (arXiv:2312.02355v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1">Vincent Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagarajan_P/0/1/0/all/0/1">Prabhat Nagarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Patterson_A/0/1/0/all/0/1">Andrew Patterson</a>, <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Martha White</a></p>
<p>Offline reinforcement learning algorithms often require careful
hyperparameter tuning. Consequently, before deployment, we need to select
amongst a set of candidate policies. As yet, however, there is little
understanding about the fundamental limits of this offline policy selection
(OPS) problem. In this work we aim to provide clarity on when sample efficient
OPS is possible, primarily by connecting OPS to off-policy policy evaluation
(OPE) and Bellman error (BE) estimation. We first show a hardness result, that
in the worst case, OPS is just as hard as OPE, by proving a reduction of OPE to
OPS. As a result, no OPS method can be more sample efficient than OPE in the
worst case. We then propose a BE method for OPS, called Identifiable BE
Selection (IBES), that has a straightforward method for selecting its own
hyperparameters. We highlight that using IBES for OPS generally has more
requirements than OPE methods, but if satisfied, can be more sample efficient.
We conclude with an empirical study comparing OPE and IBES, and by showing the
difficulty of OPS on an offline Atari benchmark dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02364">Class-Discriminative Attention Maps for Vision Transformers. (arXiv:2312.02364v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1">Lennart Brocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1">Neo Christopher Chung</a></p>
<p>Interpretability methods are critical components for examining and exploring
deep neural networks (DNN), as well as increasing our understanding of and
trust in them. Vision transformers (ViT), which can be trained to
state-of-the-art performance with a self-supervised learning (SSL) training
method, provide built-in attention maps (AM). While AMs can provide
high-quality semantic segmentation of input images, they do not account for any
signal coming from a downstream classifier. We introduce class-discriminative
attention maps (CDAM), a novel post-hoc explanation method that is highly
sensitive to the target class. Our method essentially scales attention scores
by how relevant the corresponding tokens are for the predictions of a
classifier head. Alternative to classifier outputs, CDAM can also explain a
user-defined concept by targeting similarity measures in the latent space of
the ViT. This allows for explanations of arbitrary concepts, defined by the
user through a few sample images. We investigate the operating characteristics
of CDAM in comparison with relevance propagation (RP) and token ablation maps
(TAM), an alternative to pixel occlusion methods. CDAM is highly
class-discriminative and semantically relevant, while providing implicit
regularization of relevance scores.
</p>
<p>PyTorch implementation: \url{https://github.com/lenbrocki/CDAM}
</p>
<p>Web live demo: \url{https://cdam.informatism.com/}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02368">RINAS: Training with Dataset Shuffling Can Be General and Fast. (arXiv:2312.02368v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Tianle Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiechen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xindi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1">Qiang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1">Geoffrey Fox</a></p>
<p>Deep learning datasets are expanding at an unprecedented pace, creating new
challenges for data processing in model training pipelines. A crucial aspect of
these pipelines is dataset shuffling, which significantly improves unbiased
learning and convergence accuracy by adhering to the principles of random
sampling. However, loading shuffled data for large datasets incurs significant
overhead in the deep learning pipeline and severely impacts the end-to-end
training throughput. To mitigate this, current deep learning systems often
resort to partial dataset shuffling, sacrificing global randomness to maintain
acceptable training throughput on large datasets, still leaving global
shuffling efficiency issues not fully explored.
</p>
<p>In this work, we present RINAS, a data loading framework that systematically
addresses the performance bottleneck of loading global shuffled datasets. Our
key contribution is to offer an intra-batch unordered data fetching approach,
which unleashes unexplored parallelism of data loading. We implement RINAS
under the PyTorch framework for common dataset libraries HuggingFace and
TorchVision. Our experimental results show that RINAS improves the throughput
of general language model training and vision model training by up to 59% and
89%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02372">On the Trade-Off between Stability and Representational Capacity in Graph Neural Networks. (arXiv:2312.02372v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1">Zhan Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Prorok_A/0/1/0/all/0/1">Amanda Prorok</a>, <a href="http://arxiv.org/find/eess/1/au:+Isufi_E/0/1/0/all/0/1">Elvin Isufi</a></p>
<p>Analyzing the stability of graph neural networks (GNNs) under topological
perturbations is key to understanding their transferability and the role of
each architecture component. However, stability has been investigated only for
particular architectures, questioning whether it holds for a broader spectrum
of GNNs or only for a few instances. To answer this question, we study the
stability of EdgeNet: a general GNN framework that unifies more than twenty
solutions including the convolutional and attention-based classes, as well as
graph isomorphism networks and hybrid architectures. We prove that all GNNs
within the EdgeNet framework are stable to topological perturbations. By
studying the effect of different EdgeNet categories on the stability, we show
that GNNs with fewer degrees of freedom in their parameter space, linked to a
lower representational capacity, are more stable. The key factor yielding this
trade-off is the eigenvector misalignment between the EdgeNet parameter
matrices and the graph shift operator. For example, graph convolutional neural
networks that assign a single scalar per signal shift (hence, with a perfect
alignment) are more stable than the more involved node or edge-varying
counterparts. Extensive numerical results corroborate our theoretical findings
and highlight the role of different architecture components in the trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02375">CityTFT: Temporal Fusion Transformer for Urban Building Energy Modeling. (arXiv:2312.02375v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dai_T/0/1/0/all/0/1">Ting-Yu Dai</a>, <a href="http://arxiv.org/find/stat/1/au:+Niyogi_D/0/1/0/all/0/1">Dev Niyogi</a>, <a href="http://arxiv.org/find/stat/1/au:+Nagy_Z/0/1/0/all/0/1">Zoltan Nagy</a></p>
<p>Urban Building Energy Modeling (UBEM) is an emerging method to investigate
urban design and energy systems against the increasing energy demand at urban
and neighborhood levels. However, current UBEM methods are mostly physic-based
and time-consuming in multiple climate change scenarios. This work proposes
CityTFT, a data-driven UBEM framework, to accurately model the energy demands
in urban environments. With the empowerment of the underlying TFT framework and
an augmented loss function, CityTFT could predict heating and cooling triggers
in unseen climate dynamics with an F1 score of 99.98 \% while RMSE of loads of
13.57 kWh.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02380">FaultFormer: Transformer-based Prediction of Bearing Faults. (arXiv:2312.02380v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Anthony Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1">Amir Barati Farimani</a></p>
<p>The growth of deep learning in the past decade has motivated important
applications to smart manufacturing and machine health monitoring. In
particular, vibration data offers a rich and reliable source to provide
meaningful insights into machine health and predictive maintenance. In this
work, we present a Transformer based framework for analyzing vibration signals
to predict different types of bearing faults (FaultFormer). In particular, we
process signal data using data augmentations and extract their Fourier modes to
train a transformer encoder to achieve state of the art accuracies. The
attention mechanism as well as model outputs were analyzed to confirm the
transformer's ability to automatically extract features within signals and
learn both global and local relationships to make classifications. Lastly, two
pretraining strategies were proposed to pave the way for large, generalizable
transformers that could adapt to new data, situations, or machinery on the
production floor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02387">Dissecting Medical Referral Mechanisms in Health Services: Role of Physician Professional Networks. (arXiv:2312.02387v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duarte_R/0/1/0/all/0/1">Regina de Brito Duarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1">Qiwei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1">Claudia Soares</a></p>
<p>Medical referrals between primary care physicians (PC) and specialist care
(SC) physicians profoundly impact patient care regarding quality, satisfaction,
and cost. This paper investigates the influence of professional networks among
medical doctors on referring patients from PC to SC. Using five-year
consultation data from a Portuguese private health provider, we conducted
exploratory data analysis and constructed both professional and referral
networks among physicians. We then apply Graph Neural Network (GNN) models to
learn latent representations of the referral network. Our analysis supports the
hypothesis that doctors' professional social connections can predict medical
referrals, potentially enhancing collaboration within organizations and
improving healthcare services. This research contributes to dissecting the
underlying mechanisms in primary-specialty referrals, thereby providing
valuable insights for enhancing patient care and effective healthcare
management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02396">Unsupervised Change Detection for Space Habitats Using 3D Point Clouds. (arXiv:2312.02396v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1">Jamie Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinkel_H/0/1/0/all/0/1">Holly Dinkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_J/0/1/0/all/0/1">Julia Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1">Paulo V.K. Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreira_M/0/1/0/all/0/1">Marina Moreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexandrov_O/0/1/0/all/0/1">Oleg Alexandrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Coltin_B/0/1/0/all/0/1">Brian Coltin</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1">Trey Smith</a></p>
<p>This work presents an algorithm for scene change detection from point clouds
to enable autonomous robotic caretaking in future space habitats. Autonomous
robotic systems will help maintain future deep-space habitats, such as the
Gateway space station, which will be uncrewed for extended periods. Existing
scene analysis software used on the International Space Station (ISS) relies on
manually-labeled images for detecting changes. In contrast, the algorithm
presented in this work uses raw, unlabeled point clouds as inputs. The
algorithm first applies modified Expectation-Maximization Gaussian Mixture
Model (GMM) clustering to two input point clouds. It then performs change
detection by comparing the GMMs using the Earth Mover's Distance. The algorithm
is validated quantitatively and qualitatively using a test dataset collected by
an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth
images taken directly by Astrobee and full-scene reconstructed maps built with
RGB-D and pose data from Astrobee. The runtimes of the approach are also
analyzed in depth. The source code is publicly released to promote further
development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02400">Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation. (arXiv:2312.02400v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chilukoti_S/0/1/0/all/0/1">Sai Venkatesh Chilukoti</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossen_M/0/1/0/all/0/1">Md Imran Hossen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1">Liqun Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tida_V/0/1/0/all/0/1">Vijay Srinivas Tida</a>, <a href="http://arxiv.org/find/cs/1/au:+Hei_X/0/1/0/all/0/1">Xiai Hei</a></p>
<p>DP-SGD has emerged as a popular method to protect personally identifiable
information in deep learning applications. Unfortunately, DP-SGD's per-sample
gradient clipping and uniform noise addition during training can significantly
degrade model utility. To enhance the model's utility, researchers proposed
various adaptive DP-SGD methods. However, we examine and discover that these
techniques result in greater privacy leakage or lower accuracy than the
traditional DP-SGD method, or a lack of evaluation on a complex data set such
as CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our
method automates clipping threshold estimation based on the DL model's gradient
norm and scales the gradients of each training sample without losing gradient
information. This helps to improve the algorithm's utility while using a less
privacy budget. To further improve accuracy, we introduce automatic noise
multiplier decay mechanisms to decrease the noise multiplier after every epoch.
Finally, we develop closed-form mathematical expressions using tCDP accountant
for automatic noise multiplier and automatic clipping threshold estimation.
Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms
existing SOTA DP-SGD methods in privacy and accuracy on various benchmark
datasets. We also show that privacy can be improved by lowering the scale
factor and using learning rate schedulers without significantly reducing
accuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier,
improves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10,
CIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a
substantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37
for the corresponding data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02401">Harmonizing Global Voices: Culturally-Aware Models for Enhanced Content Moderation. (arXiv:2312.02401v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chan_A/0/1/0/all/0/1">Alex J. Chan</a>, <a href="http://arxiv.org/find/stat/1/au:+Garcia_J/0/1/0/all/0/1">Jos&#xe9; Luis Redondo Garc&#xed;a</a>, <a href="http://arxiv.org/find/stat/1/au:+Silvestri_F/0/1/0/all/0/1">Fabrizio Silvestri</a>, <a href="http://arxiv.org/find/stat/1/au:+ODonnel_C/0/1/0/all/0/1">Colm O&#x27;Donnel</a>, <a href="http://arxiv.org/find/stat/1/au:+Palla_K/0/1/0/all/0/1">Konstantina Palla</a></p>
<p>Content moderation at scale faces the challenge of considering local cultural
distinctions when assessing content. While global policies aim to maintain
decision-making consistency and prevent arbitrary rule enforcement, they often
overlook regional variations in interpreting natural language as expressed in
content. In this study, we are looking into how moderation systems can tackle
this issue by adapting to local comprehension nuances. We train large language
models on extensive datasets of media news and articles to create culturally
attuned models. The latter aim to capture the nuances of communication across
geographies with the goal of recognizing cultural and societal variations in
what is considered offensive content. We further explore the capability of
these models to generate explanations for instances of content violation,
aiming to shed light on how policy guidelines are perceived when cultural and
societal contexts change. We find that training on extensive media datasets
successfully induced cultural awareness and resulted in improvements in
handling content violations on a regional basis. Additionally, these
advancements include the ability to provide explanations that align with the
specific local norms and nuances as evidenced by the annotators' preference in
our conducted study. This multifaceted success reinforces the critical role of
an adaptable content moderation approach in keeping pace with the ever-evolving
nature of the content it oversees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02406">Efficient Online Data Mixing For Language Model Pre-Training. (arXiv:2312.02406v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1">Alon Albalak</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Liangming Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>The data used to pretrain large language models has a decisive impact on a
model's downstream performance, which has led to a large body of work on data
selection methods that aim to automatically determine the most suitable data to
use for pretraining. Existing data selection methods suffer from slow and
computationally expensive processes, a problem amplified by the increasing size
of models and of pretraining datasets. Data mixing, on the other hand, reduces
the complexity of data selection by grouping data points together and
determining sampling probabilities across entire groups. However, data mixing
proportions are typically fixed before training and therefore cannot adapt to
changing training dynamics. To address these limitations, we develop an
efficient algorithm for Online Data Mixing (ODM) that combines elements from
both data selection and data mixing. Based on multi-armed bandit algorithms,
our online approach optimizes the data mixing proportions during training.
Remarkably, our method trains a model that reaches the final perplexity of the
next best method with 19\% fewer training iterations, and improves performance
on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible
wall-clock time during pretraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02407">Robust Clustering using Hyperdimensional Computing. (arXiv:2312.02407v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1">Lulu Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Parhi_K/0/1/0/all/0/1">Keshab K. Parhi</a></p>
<p>This paper addresses the clustering of data in the hyperdimensional computing
(HDC) domain. In prior work, an HDC-based clustering framework, referred to as
HDCluster, has been proposed. However, the performance of the existing
HDCluster is not robust. The performance of HDCluster is degraded as the
hypervectors for the clusters are chosen at random during the initialization
step. To overcome this bottleneck, we assign the initial cluster hypervectors
by exploring the similarity of the encoded data, referred to as \textit{query}
hypervectors. Intra-cluster hypervectors have a higher similarity than
inter-cluster hypervectors. Harnessing the similarity results among query
hypervectors, this paper proposes four HDC-based clustering algorithms:
similarity-based k-means, equal bin-width histogram, equal bin-height
histogram, and similarity-based affinity propagation. Experimental results
illustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based
clustering algorithms can achieve better accuracy, more robust performance,
fewer iterations, and less execution time. Similarity-based affinity
propagation outperforms the other three HDC-based clustering algorithms on
eight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass
clustering, i.e., without any iterative update of the cluster hypervectors, our
proposed algorithms can provide more robust clustering accuracy than HDCluster.
(iii) Over eight datasets, five out of eight can achieve higher or comparable
accuracy when projected onto the hyperdimensional space. Traditional clustering
is more desirable than HDC when the number of clusters, $k$, is large.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02416">Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor. (arXiv:2312.02416v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinqian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jihua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qinghai Zheng</a></p>
<p>Federated learning encounters a critical challenge of data heterogeneity,
adversely affecting the performance and convergence of the federated model.
Various approaches have been proposed to address this issue, yet their
effectiveness is still limited. Recent studies have revealed that the federated
model suffers severe forgetting in local training, leading to global forgetting
and performance degradation. Although the analysis provides valuable insights,
a comprehensive understanding of the vulnerable classes and their impact
factors is yet to be established. In this paper, we aim to bridge this gap by
systematically analyzing the forgetting degree of each class during local
training across different communication rounds. Our observations are: (1) Both
missing and non-dominant classes suffer similar severe forgetting during local
training, while dominant classes show improvement in performance. (2) When
dynamically reducing the sample size of a dominant class, catastrophic
forgetting occurs abruptly when the proportion of its samples is below a
certain threshold, indicating that the local model struggles to leverage a few
samples of a specific class effectively to prevent forgetting. Motivated by
these findings, we propose a novel and straightforward algorithm called
Federated Knowledge Anchor (FedKA). Assuming that all clients have a single
shared sample for each class, the knowledge anchor is constructed before each
local training stage by extracting shared samples for missing classes and
randomly selecting one sample per class for non-dominant classes. The knowledge
anchor is then utilized to correct the gradient of each mini-batch towards the
direction of preserving the knowledge of the missing and non-dominant classes.
Extensive experimental results demonstrate that our proposed FedKA achieves
fast and stable convergence, significantly improving accuracy on popular
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02418">Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data. (arXiv:2312.02418v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aaditya K. Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1">Mostafa Elhoushi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoud_A/0/1/0/all/0/1">Anas Mahmoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Tirumala_K/0/1/0/all/0/1">Kushal Tirumala</a>, <a href="http://arxiv.org/find/cs/1/au:+Gloeckle_F/0/1/0/all/0/1">Fabian Gloeckle</a>, <a href="http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1">Baptiste Rozi&#xe8;re</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Carole-Jean Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1">Ari S. Morcos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1">Newsha Ardalani</a></p>
<p>Code datasets, often collected from diverse and uncontrolled sources such as
GitHub, potentially suffer from quality issues, thereby affecting the
performance and training efficiency of Large Language Models (LLMs) optimized
for code generation. Previous studies demonstrated the benefit of using
embedding spaces for data pruning, but they mainly focused on duplicate removal
or increasing variety, and in other modalities, such as images. Our work
focuses on using embeddings to identify and remove "low-quality" code data.
First, we explore features of "low-quality" code in embedding space, through
the use of synthetic corruptions. Armed with this knowledge, we devise novel
pruning metrics that operate in embedding space to identify and remove
low-quality entries in the Stack dataset. We demonstrate the benefits of this
synthetic corruption informed pruning (SCIP) approach on the well-established
HumanEval and MBPP benchmarks, outperforming existing embedding-based methods.
Importantly, we achieve up to a 3% performance improvement over no pruning,
thereby showing the promise of insights from synthetic corruptions for data
pruning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02429">PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models. (arXiv:2312.02429v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1">Wei-Cheng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jyun-Yu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Darabsah_M/0/1/0/all/0/1">Mutasem Al-Darabsah</a>, <a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1">Choon Hui Teo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hsiang-Fu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishwanathan_S/0/1/0/all/0/1">S.V.N. Vishwanathan</a></p>
<p>Embedding-based Retrieval Models (ERMs) have emerged as a promising framework
for large-scale text retrieval problems due to powerful large language models.
Nevertheless, fine-tuning ERMs to reach state-of-the-art results can be
expensive due to the extreme scale of data as well as the complexity of
multi-stages pipelines (e.g., pre-training, fine-tuning, distillation). In this
work, we propose the PEFA framework, namely ParamEter-Free Adapters, for fast
tuning of ERMs without any backward pass in the optimization. At index building
stage, PEFA equips the ERM with a non-parametric k-nearest neighbor (kNN)
component. At inference stage, PEFA performs a convex combination of two
scoring functions, one from the ERM and the other from the kNN. Based on the
neighborhood definition, PEFA framework induces two realizations, namely
PEFA-XL (i.e., extra large) using double ANN indices and PEFA-XS (i.e., extra
small) using a single ANN index. Empirically, PEFA achieves significant
improvement on two retrieval applications. For document retrieval, regarding
Recall@100 metric, PEFA improves not only pre-trained ERMs on Trivia-QA by an
average of 13.2%, but also fine-tuned ERMs on NQ-320K by an average of 5.5%,
respectively. For product search, PEFA improves the Recall@100 of the
fine-tuned ERMs by an average of 5.3% and 14.5%, for PEFA-XS and PEFA-XL,
respectively. Our code is available at https://github.com/
amzn/pecos/tree/mainline/examples/pefa-wsdm24
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02438">Adaptive Instrument Design for Indirect Experiments. (arXiv:2312.02438v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1">Yash Chandak</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1">Shiv Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1">Vasilis Syrgkanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1">Emma Brunskill</a></p>
<p>Indirect experiments provide a valuable framework for estimating treatment
effects in situations where conducting randomized control trials (RCTs) is
impractical or unethical. Unlike RCTs, indirect experiments estimate treatment
effects by leveraging (conditional) instrumental variables, enabling estimation
through encouragement and recommendation rather than strict treatment
assignment. However, the sample efficiency of such estimators depends not only
on the inherent variability in outcomes but also on the varying compliance
levels of users with the instrumental variables and the choice of estimator
being used, especially when dealing with numerous instrumental variables. While
adaptive experiment design has a rich literature for direct experiments, in
this paper we take the initial steps towards enhancing sample efficiency for
indirect experiments by adaptively designing a data collection policy over
instrumental variables. Our main contribution is a practical computational
procedure that utilizes influence functions to search for an optimal data
collection policy, minimizing the mean-squared error of the desired
(non-linear) estimator. Through experiments conducted in various domains
inspired by real-world applications, we showcase how our method can
significantly improve the sample efficiency of indirect experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02450">GIT-Net: Generalized Integral Transform for Operator Learning. (arXiv:2312.02450v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Thiery_A/0/1/0/all/0/1">Alexandre Hoang Thiery</a></p>
<p>This article introduces GIT-Net, a deep neural network architecture for
approximating Partial Differential Equation (PDE) operators, inspired by
integral transform operators. GIT-NET harnesses the fact that differential
operators commonly used for defining PDEs can often be represented
parsimoniously when expressed in specialized functional bases (e.g., Fourier
basis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive
generalized integral transforms with deep neural networks. When compared to
several recently proposed alternatives, GIT-Net's computational and memory
requirements scale gracefully with mesh discretizations, facilitating its
application to PDE problems on complex geometries. Numerical experiments
demonstrate that GIT-Net is a competitive neural network operator, exhibiting
small test errors and low evaluations across a range of PDE problems. This
stands in contrast to existing neural network operators, which typically excel
in just one of these areas.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02462">Dimensionality Reduction and Dynamical Mode Recognition of Circular Arrays of Flame Oscillators Using Deep Neural Network. (arXiv:2312.02462v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a></p>
<p>Oscillatory combustion in aero engines and modern gas turbines often has
significant adverse effects on their operation, and accurately recognizing
various oscillation modes is the prerequisite for understanding and controlling
combustion instability. However, the high-dimensional spatial-temporal data of
a complex combustion system typically poses considerable challenges to the
dynamical mode recognition. Based on a two-layer bidirectional long short-term
memory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and
a two-dimensional Wasserstein distance-based classifier (WDC), this study
proposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes
in oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension
reduction model was introduced to reduce the high-dimensional spatial-temporal
data of the combustion system to a low-dimensional phase space; Gaussian kernel
density estimates (GKDE) were computed based on the distribution of phase
points in a grid; two-dimensional WD values were calculated from the GKDE maps
to recognize the oscillation modes. The time-series data used in this study
were obtained from numerical simulations of circular arrays of laminar flame
oscillators. The results show that the novel Bi-LSTM-VAE method can produce a
non-overlapping distribution of phase points, indicating an effective
unsupervised mode recognition and classification. Furthermore, the present
method exhibits a more prominent performance than VAE and PCA (principal
component analysis) for distinguishing dynamical modes in complex flame
systems, implying its potential in studying turbulent combustion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02469">Learning Energy-based Model via Dual-MCMC Teaching. (arXiv:2312.02469v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiali Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tian Han</a></p>
<p>This paper studies the fundamental learning problem of the energy-based model
(EBM). Learning the EBM can be achieved using the maximum likelihood estimation
(MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling,
such as the Langevin dynamics. However, the noise-initialized Langevin dynamics
can be challenging in practice and hard to mix. This motivates the exploration
of joint training with the generator model where the generator model serves as
a complementary model to bypass MCMC sampling. However, such a method can be
less accurate than the MCMC and result in biased EBM learning. While the
generator can also serve as an initializer model for better MCMC sampling, its
learning can be biased since it only matches the EBM and has no access to
empirical training examples. Such biased generator learning may limit the
potential of learning the EBM. To address this issue, we present a joint
learning framework that interweaves the maximum likelihood learning algorithm
for both the EBM and the complementary generator model. In particular, the
generator model is learned by MLE to match both the EBM and the empirical data
distribution, making it a more informative initializer for MCMC sampling of
EBM. Learning generator with observed examples typically requires inference of
the generator posterior. To ensure accurate and efficient inference, we adopt
the MCMC posterior sampling and introduce a complementary inference model to
initialize such latent MCMC sampling. We show that three separate models can be
seamlessly integrated into our joint framework through two (dual-) MCMC
teaching, enabling effective and efficient EBM learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02470">Generator Born from Classifier. (arXiv:2312.02470v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Runpeng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>In this paper, we make a bold attempt toward an ambitious task: given a
pre-trained classifier, we aim to reconstruct an image generator, without
relying on any data samples. From a black-box perspective, this challenge seems
intractable, since it inevitably involves identifying the inverse function for
a classifier, which is, by nature, an information extraction process. As such,
we resort to leveraging the knowledge encapsulated within the parameters of the
neural network. Grounded on the theory of Maximum-Margin Bias of gradient
descent, we propose a novel learning paradigm, in which the generator is
trained to ensure that the convergence conditions of the network parameters are
satisfied over the generated distribution of the samples. Empirical validation
from various image generation tasks substantiates the efficacy of our strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02471">Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks. (arXiv:2312.02471v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhongyuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Perazzone_J/0/1/0/all/0/1">Jake Perazzone</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1">Gunjan Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1">Santiago Segarra</a></p>
<p>Computational offloading has become an enabling component for edge
intelligence in mobile and smart devices. Existing offloading schemes mainly
focus on mobile devices and servers, while ignoring the potential network
congestion caused by tasks from multiple mobile devices, especially in wireless
multi-hop networks. To fill this gap, we propose a low-overhead,
congestion-aware distributed task offloading scheme by augmenting a distributed
greedy framework with graph-based machine learning. In simulated wireless
multi-hop networks with 20-110 nodes and a resource allocation scheme based on
shortest path routing and contention-based link scheduling, our approach is
demonstrated to be effective in reducing congestion or unstable queues under
the context-agnostic baseline, while improving the execution latency over local
computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02473">NeutronStream: A Dynamic GNN Training Framework with Sliding Window for Graph Streams. (arXiv:2312.02473v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1">Dechao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiange Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zhenbo Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuecang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Junhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yu Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Ge Yu</a></p>
<p>Existing Graph Neural Network (GNN) training frameworks have been designed to
help developers easily create performant GNN implementations. However, most
existing GNN frameworks assume that the input graphs are static, but ignore
that most real-world graphs are constantly evolving. Though many dynamic GNN
models have emerged to learn from evolving graphs, the training process of
these dynamic GNNs is dramatically different from traditional GNNs in that it
captures both the spatial and temporal dependencies of graph updates. This
poses new challenges for designing dynamic GNN training frameworks. First, the
traditional batched training method fails to capture real-time structural
evolution information. Second, the time-dependent nature makes parallel
training hard to design. Third, it lacks system supports for users to
efficiently implement dynamic GNNs. In this paper, we present NeutronStream, a
framework for training dynamic GNN models. NeutronStream abstracts the input
dynamic graph into a chronologically updated stream of events and processes the
stream with an optimized sliding window to incrementally capture the
spatial-temporal dependencies of events. Furthermore, NeutronStream provides a
parallel execution engine to tackle the sequential event processing challenge
to achieve high performance. NeutronStream also integrates a built-in graph
storage structure that supports dynamic updates and provides a set of
easy-to-use APIs that allow users to express their dynamic GNNs. Our
experimental results demonstrate that, compared to state-of-the-art dynamic GNN
implementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X
and an average accuracy improvement of 3.97%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02478">RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum Handoffs, Disconnectivity, and Energy Consumption. (arXiv:2312.02478v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cherif_N/0/1/0/all/0/1">Nesrine Cherif</a>, <a href="http://arxiv.org/find/eess/1/au:+Jaafar_W/0/1/0/all/0/1">Wael Jaafar</a>, <a href="http://arxiv.org/find/eess/1/au:+Yanikomeroglu_H/0/1/0/all/0/1">Halim Yanikomeroglu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yongacoglu_A/0/1/0/all/0/1">Abbas Yongacoglu</a></p>
<p>Unmanned aerial vehicle (UAV) is a promising technology for last-mile cargo
delivery. However, the limited on-board battery capacity, cellular
unreliability, and frequent handoffs in the airspace are the main obstacles to
unleash its full potential. Given that existing cellular networks were
primarily designed to service ground users, re-utilizing the same architecture
for highly mobile aerial users, e.g., cargo-UAVs, is deemed challenging.
Indeed, to ensure a safe delivery using cargo-UAVs, it is crucial to utilize
the available energy efficiently, while guaranteeing reliable connectivity for
command-and-control and avoiding frequent handoff. To achieve this goal, we
propose a novel approach for joint cargo-UAV trajectory planning and cell
association. Specifically, we formulate the cargo-UAV mission as a
multi-objective problem aiming to 1) minimize energy consumption, 2) reduce
handoff events, and 3) guarantee cellular reliability along the trajectory. We
leverage reinforcement learning (RL) to jointly optimize the cargo-UAV's
trajectory and cell association. Simulation results demonstrate a performance
improvement of our proposed method, in terms of handoffs, disconnectivity, and
energy consumption, compared to benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02490">Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems. (arXiv:2312.02490v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dinh_P/0/1/0/all/0/1">Phai Vu Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quang Uy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1">Dinh Thai Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Diep N. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1">Son Pham Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1">Eryk Dutkiewicz</a></p>
<p>Intrusion detection systems (IDSs) play a critical role in protecting
billions of IoT devices from malicious attacks. However, the IDSs for IoT
devices face inherent challenges of IoT systems, including the heterogeneity of
IoT data/devices, the high dimensionality of training data, and the imbalanced
data. Moreover, the deployment of IDSs on IoT systems is challenging, and
sometimes impossible, due to the limited resources such as memory/storage and
computing capability of typical IoT devices. To tackle these challenges, this
article proposes a novel deep neural network/architecture called Constrained
Twin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with
more separable/distinguishable and lower-dimensional representation data.
Additionally, in comparison to the state-of-the-art neural networks used in
IDSs, CTVAE requires less memory/storage and computing power, hence making it
more suitable for IoT IDS systems. Extensive experiments with the 11 most
popular IoT botnet datasets show that CTVAE can boost around 1% in terms of
accuracy and Fscore in detection attack compared to the state-of-the-art
machine learning and representation learning methods, whilst the running time
for attack detection is lower than 2E-6 seconds and the model size is lower
than 1 MB. We also further investigate various characteristics of CTVAE in the
latent space and in the reconstruction representation to demonstrate its
efficacy compared with current well-known methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02491">Pseudo Replay-based Class Continual Learning for Online New Category Anomaly Detection in Additive Manufacturing. (arXiv:2312.02491v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhangyue Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianxin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuxuan Li</a></p>
<p>The incorporation of advanced sensors and machine learning techniques has
enabled modern manufacturing enterprises to perform data-driven in-situ quality
monitoring based on the sensor data collected in manufacturing processes.
However, one critical challenge is that newly presented defect category may
manifest as the manufacturing process continues, resulting in monitoring
performance deterioration of previously trained machine learning models. Hence,
there is an increasing need for empowering machine learning model to learn
continually. Among all continual learning methods, memory-based continual
learning has the best performance but faces the constraints of data storage
capacity. To address this issue, this paper develops a novel pseudo
replay-based continual learning by integrating class incremental learning and
oversampling-based data generation. Without storing all the data, the developed
framework could generate high-quality data representing previous classes to
train machine learning model incrementally when new category anomaly occurs. In
addition, it could even enhance the monitoring performance since it also
effectively improves the data quality. The effectiveness of the proposed
framework is validated in an additive manufacturing process, which leverages
supervised classification problem for anomaly detection. The experimental
results show that the developed method is very promising in detecting novel
anomaly while maintaining a good performance on the previous task and brings up
more flexibility in model architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02494">ReconU-Net: a direct PET image reconstruction using U-Net architecture with back projection-induced skip connection. (arXiv:2312.02494v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1">Fumio Hashimoto</a>, <a href="http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1">Kibo Ote</a></p>
<p>[Objective] This study aims to introduce a novel back projection-induced
U-Net-shaped architecture, called ReconU-Net, for deep learning-based direct
positron emission tomography (PET) image reconstruction. Additionally, our
objective is to analyze the behavior of direct PET image reconstruction and
gain deeper insights by comparing the proposed ReconU-Net architecture with
other encoder-decoder architectures without skip connections. [Approach] The
proposed ReconU-Net architecture uniquely integrates the physical model of the
back projection operation into the skip connection. This distinctive feature
facilitates the effective transfer of intrinsic spatial information from the
input sinogram to the reconstructed image via an embedded physical model. The
proposed ReconU-Net was trained using Monte Carlo simulation data from the
Brainweb phantom and tested on both simulated and real Hoffman brain phantom
data. [Main results] The proposed ReconU-Net method generated a reconstructed
image with a more accurate structure compared to other deep learning-based
direct reconstruction methods. Further analysis showed that the proposed
ReconU-Net architecture has the ability to transfer features of multiple
resolutions, especially non-abstract high-resolution information, through skip
connections. Despite limited training on simulated data, the proposed
ReconU-Net successfully reconstructed the real Hoffman brain phantom, unlike
other deep learning-based direct reconstruction methods, which failed to
produce a reconstructed image. [Significance] The proposed ReconU-Net can
improve the fidelity of direct PET image reconstruction, even when dealing with
small training datasets, by leveraging the synergistic relationship between
data-driven modeling and the physics model of the imaging process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02515">ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU. (arXiv:2312.02515v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhengmao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dengchun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Jingqi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1">Tingfeng Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jie Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1">Lei Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yexi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_J/0/1/0/all/0/1">Jian Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Ke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Mingjie Tang</a></p>
<p>Transformer-based large language models (LLMs) have demonstrated outstanding
performance across diverse domains, particularly when fine-turned for specific
domains. Recent studies suggest that the resources required for fine-tuning
LLMs can be economized through parameter-efficient methods such as Low-Rank
Adaptation (LoRA). While LoRA effectively reduces computational burdens and
resource demands, it currently supports only a single-job fine-tuning setup.
</p>
<p>In this paper, we present ASPEN, a high-throughput framework for fine-tuning
LLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA
method, leveraging shared pre-trained model and adaptive scheduling. ASPEN is
compatible with transformer-based language models like LLaMA and ChatGLM, etc.
Experiments show that ASPEN saves 53% of GPU memory when training multiple
LLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about
17% compared to existing methods when training with various pre-trained models
on different GPUs. The adaptive scheduling algorithm reduces turnaround time by
24%, end-to-end training latency by 12%, prioritizing jobs and preventing
out-of-memory issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02517">Simplifying Neural Network Training Under Class Imbalance. (arXiv:2312.02517v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shwartz_Ziv_R/0/1/0/all/0/1">Ravid Shwartz-Ziv</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1">Micah Goldblum</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yucen Lily Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1">C. Bayan Bruss</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a></p>
<p>Real-world datasets are often highly class-imbalanced, which can adversely
impact the performance of deep learning models. The majority of research on
training neural networks under class imbalance has focused on specialized loss
functions, sampling techniques, or two-stage training procedures. Notably, we
demonstrate that simply tuning existing components of standard deep learning
pipelines, such as the batch size, data augmentation, optimizer, and label
smoothing, can achieve state-of-the-art performance without any such
specialized class imbalance methods. We also provide key prescriptions and
considerations for training under class imbalance, and an understanding of why
imbalance methods succeed or fail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02519">Creative Agents: Empowering Agents with Imagination for Creative Tasks. (arXiv:2312.02519v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Penglin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yuhui Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haoqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a></p>
<p>We study building embodied agents for open-ended creative tasks. While
existing methods build instruction-following agents that can perform diverse
open-ended tasks, none of them demonstrates creativity -- the ability to give
novel and diverse task solutions implicit in the language instructions. This
limitation comes from their inability to convert abstract language instructions
into concrete task goals in the environment and perform long-horizon planning
for such complicated goals. Given the observation that humans perform creative
tasks with the help of imagination, we propose a class of solutions for
creative agents, where the controller is enhanced with an imaginator that
generates detailed imaginations of task outcomes conditioned on language
instructions. We introduce several approaches to implementing the components of
creative agents. We implement the imaginator with either a large language model
for textual imagination or a diffusion model for visual imagination. The
controller can either be a behavior-cloning policy learned from data or a
pre-trained foundation model generating executable codes in the environment. We
benchmark creative tasks with the challenging open-world game Minecraft, where
the agents are asked to create diverse buildings given free-form language
instructions. In addition, we propose novel evaluation metrics for open-ended
creative tasks utilizing GPT-4V, which holds many advantages over existing
metrics. We perform a detailed experimental analysis of creative agents,
showing that creative agents are the first AI agents accomplishing diverse
building creation in the survival mode of Minecraft. Our benchmark and models
are open-source for future research on creative agents
(https://github.com/PKU-RL/Creative-Agents).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02522">MASP: Scalable GNN-based Planning for Multi-Agent Navigation. (arXiv:2312.02522v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinting Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiayu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huazhong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a></p>
<p>We investigate the problem of decentralized multi-agent navigation tasks,
where multiple agents need to reach initially unassigned targets in a limited
time. Classical planning-based methods suffer from expensive computation
overhead at each step and offer limited expressiveness for complex cooperation
strategies. In contrast, reinforcement learning (RL) has recently become a
popular paradigm for addressing this issue. However, RL struggles with low data
efficiency and cooperation when directly exploring (nearly) optimal policies in
the large search space, especially with an increased agent number (e.g., 10+
agents) or in complex environments (e.g., 3D simulators). In this paper, we
propose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned
hierarchical planner for navigation tasks with a substantial number of agents.
MASP adopts a hierarchical framework to divide a large search space into
multiple smaller spaces, thereby reducing the space complexity and accelerating
training convergence. We also leverage graph neural networks (GNN) to model the
interaction between agents and goals, improving goal achievement. Besides, to
enhance generalization capabilities in scenarios with unseen team sizes, we
divide agents into multiple groups, each with a previously trained number of
agents. The results demonstrate that MASP outperforms classical planning-based
competitors and RL baselines, achieving a nearly 100% success rate with minimal
training data in both multi-agent particle environments (MPE) with 50 agents
and a quadrotor 3-dimensional environment (OmniDrones) with 20 agents.
Furthermore, the learned policy showcases zero-shot generalization across
unseen team sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02530">MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection. (arXiv:2312.02530v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Junho Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Keonwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jeonglyul Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sungzoon Cho</a></p>
<p>Detecting anomalies in real-world multivariate time series data is
challenging due to complex temporal dependencies and inter-variable
correlations. Recently, reconstruction-based deep models have been widely used
to solve the problem. However, these methods still suffer from an
over-generalization issue and fail to deliver consistently high performance. To
address this issue, we propose the MEMTO, a memory-guided Transformer using a
reconstruction-based approach. It is designed to incorporate a novel memory
module that can learn the degree to which each memory item should be updated in
response to the input data. To stabilize the training procedure, we use a
two-phase training paradigm which involves using K-means clustering for
initializing memory items. Additionally, we introduce a bi-dimensional
deviation-based detection criterion that calculates anomaly scores considering
both input space and latent space. We evaluate our proposed method on five
real-world datasets from diverse domains, and it achieves an average anomaly
detection F1-score of 95.74%, significantly outperforming the previous
state-of-the-art methods. We also conduct extensive experiments to empirically
validate the effectiveness of our proposed model's key components.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02537">Asymmetric leader-laggard cluster synchronization for collective decision-making with laser network. (arXiv:2312.02537v1 [physics.optics])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kotoku_S/0/1/0/all/0/1">Shun Kotoku</a>, <a href="http://arxiv.org/find/physics/1/au:+Mihana_T/0/1/0/all/0/1">Takatomo Mihana</a>, <a href="http://arxiv.org/find/physics/1/au:+Rohm_A/0/1/0/all/0/1">Andr&#xe9; R&#xf6;hm</a>, <a href="http://arxiv.org/find/physics/1/au:+Horisaki_R/0/1/0/all/0/1">Ryoichi Horisaki</a>, <a href="http://arxiv.org/find/physics/1/au:+Naruse_M/0/1/0/all/0/1">Makoto Naruse</a></p>
<p>Photonic accelerators have recently attracted soaring interest, harnessing
the ultimate nature of light for information processing. Collective
decision-making with a laser network, employing the chaotic and synchronous
dynamics of optically interconnected lasers to address the competitive
multi-armed bandit (CMAB) problem, is a highly compelling approach due to its
scalability and experimental feasibility. We investigated essential network
structures for collective decision-making through quantitative stability
analysis. Moreover, we demonstrated the asymmetric preferences of players in
the CMAB problem, extending its functionality to more practical applications.
Our study highlights the capability and significance of machine learning built
upon chaotic lasers and photonic devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02544">Characterization of Locality in Spin States and Forced Moves for Optimizations. (arXiv:2312.02544v1 [physics.app-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Sato_Y/0/1/0/all/0/1">Yoshiki Sato</a>, <a href="http://arxiv.org/find/physics/1/au:+Konoshima_M/0/1/0/all/0/1">Makiko Konoshima</a>, <a href="http://arxiv.org/find/physics/1/au:+Tamura_H/0/1/0/all/0/1">Hirotaka Tamura</a>, <a href="http://arxiv.org/find/physics/1/au:+Ohkubo_J/0/1/0/all/0/1">Jun Ohkubo</a></p>
<p>Ising formulations are widely utilized to solve combinatorial optimization
problems, and a variety of quantum or semiconductor-based hardware has recently
been made available. In combinatorial optimization problems, the existence of
local minima in energy landscapes is problematic to use to seek the global
minimum. We note that the aim of the optimization is not to obtain exact
samplings from the Boltzmann distribution, and there is thus no need to satisfy
detailed balance conditions. In light of this fact, we develop an algorithm to
get out of the local minima efficiently while it does not yield the exact
samplings. For this purpose, we utilize a feature that characterizes locality
in the current state, which is easy to obtain with a type of specialized
hardware. Furthermore, as the proposed algorithm is based on a rejection-free
algorithm, the computational cost is low. In this work, after presenting the
details of the proposed algorithm, we report the results of numerical
experiments that demonstrate the effectiveness of the proposed feature and
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02547">On Optimal Consistency-Robustness Trade-Off for Learning-Augmented Multi-Option Ski Rental. (arXiv:2312.02547v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1">Yongho Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Changyeol Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1">Hyung-Chan An</a></p>
<p>The learning-augmented multi-option ski rental problem generalizes the
classical ski rental problem in two ways: the algorithm is provided with a
prediction on the number of days we can ski, and the ski rental options now
come with a variety of rental periods and prices to choose from, unlike the
classical two-option setting. Subsequent to the initial study of the
multi-option ski rental problem (without learning augmentation) due to Zhang,
Poon, and Xu, significant progress has been made for this problem recently in
particular. The problem is very well understood when we relinquish one of the
two generalizations -- for the learning-augmented classical ski rental problem,
algorithms giving best-possible trade-off between consistency and robustness
exist; for the multi-option ski rental problem without learning augmentation,
deterministic/randomized algorithms giving the best-possible competitiveness
have been found. However, in presence of both generalizations, there remained a
huge gap between the algorithmic and impossibility results. In fact, for
randomized algorithms, we did not have any nontrivial lower bounds on the
consistency-robustness trade-off before.
</p>
<p>This paper bridges this gap for both deterministic and randomized algorithms.
For deterministic algorithms, we present a best-possible algorithm that
completely matches the known lower bound. For randomized algorithms, we show
the first nontrivial lower bound on the consistency-robustness trade-off, and
also present an improved randomized algorithm. Our algorithm matches our lower
bound on robustness within a factor of e/2 when the consistency is at most
1.086.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02554">ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference. (arXiv:2312.02554v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianchi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xierui Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiyan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1">Fei Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guannan Zhang</a></p>
<p>Language model alignment is a cutting-edge technique in large language model
training to align the model output to user's intent, e.g., being helpful and
harmless. Recent alignment framework consists of two steps: supervised
fine-tuning with demonstration data and preference learning with human
preference data. Previous preference learning methods, such as RLHF and DPO,
mainly focus on pair-wise preference data. However, in many real-world
scenarios where human feedbacks are intrinsically point-wise, these methods
will suffer from information loss or even fail. To fill this gap, in this
paper, we first develop a preference learning method called point-wise DPO to
tackle point-wise preference data. Further revelation on the connection between
supervised fine-tuning and point-wise preference learning enables us to develop
a unified framework for both human demonstration and point-wise preference
data, which sheds new light on the construction of preference dataset.
Extensive experiments on point-wise datasets with binary or continuous labels
demonstrate the superior performance and efficiency of our proposed methods. A
new dataset with high-quality demonstration samples on harmlessness is
constructed and made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02566">Structured World Representations in Maze-Solving Transformers. (arXiv:2312.02566v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ivanitskiy_M/0/1/0/all/0/1">Michael Igorevich Ivanitskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Spies_A/0/1/0/all/0/1">Alex F. Spies</a>, <a href="http://arxiv.org/find/cs/1/au:+Rauker_T/0/1/0/all/0/1">Tilman R&#xe4;uker</a>, <a href="http://arxiv.org/find/cs/1/au:+Corlouer_G/0/1/0/all/0/1">Guillaume Corlouer</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathwin_C/0/1/0/all/0/1">Chris Mathwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Quirke_L/0/1/0/all/0/1">Lucia Quirke</a>, <a href="http://arxiv.org/find/cs/1/au:+Rager_C/0/1/0/all/0/1">Can Rager</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rusheb Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentine_D/0/1/0/all/0/1">Dan Valentine</a>, <a href="http://arxiv.org/find/cs/1/au:+Behn_C/0/1/0/all/0/1">Cecilia Diniz Behn</a>, <a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1">Katsumi Inoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1">Samy Wu Fung</a></p>
<p>Transformer models underpin many recent advances in practical machine
learning applications, yet understanding their internal behavior continues to
elude researchers. Given the size and complexity of these models, forming a
comprehensive picture of their inner workings remains a significant challenge.
To this end, we set out to understand small transformer models in a more
tractable setting: that of solving mazes. In this work, we focus on the
abstractions formed by these models and find evidence for the consistent
emergence of structured internal representations of maze topology and valid
paths. We demonstrate this by showing that the residual stream of only a single
token can be linearly decoded to faithfully reconstruct the entire maze. We
also find that the learned embeddings of individual tokens have spatial
structure. Furthermore, we take steps towards deciphering the circuity of
path-following by identifying attention heads (dubbed $\textit{adjacency
heads}$), which are implicated in finding valid subsequent tokens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02573">UTBoost: A Tree-boosting based System for Uplift Modeling. (arXiv:2312.02573v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junjie Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiangyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">DongDong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhixiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Bangqi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a></p>
<p>Uplift modeling refers to the set of machine learning techniques that a
manager may use to estimate customer uplift, that is, the net effect of an
action on some customer outcome. By identifying the subset of customers for
whom a treatment will have the greatest effect, uplift models assist
decision-makers in optimizing resource allocations and maximizing overall
returns. Accurately estimating customer uplift poses practical challenges, as
it requires assessing the difference between two mutually exclusive outcomes
for each individual. In this paper, we propose two innovative adaptations of
the well-established Gradient Boosting Decision Trees (GBDT) algorithm, which
learn the causal effect in a sequential way and overcome the counter-factual
nature. Both approaches innovate existing techniques in terms of ensemble
learning method and learning objectives, respectively. Experiments on
large-scale datasets demonstrate the usefulness of the proposed methods, which
often yielding remarkable improvements over base models. To facilitate the
application, we develop the UTBoost, an end-to-end tree boosting system
specifically designed for uplift modeling. The package is open source and has
been optimized for training speed to meet the needs of real industrial
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02592">FRAPP\&#x27;E: A Post-Processing Framework for Group Fairness Regularization. (arXiv:2312.02592v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tifrea_A/0/1/0/all/0/1">Alexandru &#x162;ifrea</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahoti_P/0/1/0/all/0/1">Preethi Lahoti</a>, <a href="http://arxiv.org/find/cs/1/au:+Packer_B/0/1/0/all/0/1">Ben Packer</a>, <a href="http://arxiv.org/find/cs/1/au:+Halpern_Y/0/1/0/all/0/1">Yoni Halpern</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a>, <a href="http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1">Flavien Prost</a></p>
<p>Post-processing mitigation techniques for group fairness generally adjust the
decision threshold of a base model in order to improve fairness. Methods in
this family exhibit several advantages that make them appealing in practice:
post-processing requires no access to the model training pipeline, is agnostic
to the base model architecture, and offers a reduced computation cost compared
to in-processing. Despite these benefits, existing methods face other
challenges that limit their applicability: they require knowledge of the
sensitive attributes at inference time and are oftentimes outperformed by
in-processing. In this paper, we propose a general framework to transform any
in-processing method with a penalized objective into a post-processing
procedure. The resulting method is specifically designed to overcome the
aforementioned shortcomings of prior post-processing approaches. Furthermore,
we show theoretically and through extensive experiments on real-world data that
the resulting post-processing method matches or even surpasses the
fairness-error trade-off offered by the in-processing counterpart.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02596">TSVR+: Twin support vector regression with privileged information. (arXiv:2312.02596v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumari_A/0/1/0/all/0/1">Anuradha Kumari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1">M. Tanveer</a></p>
<p>In the realm of machine learning, the data may contain additional attributes,
known as privileged information (PI). The main purpose of PI is to assist in
the training of the model and then utilize the acquired knowledge to make
predictions for unseen samples. Support vector regression (SVR) is an effective
regression model, however, it has a low learning speed due to solving a convex
quadratic problem (QP) subject to a pair of constraints. In contrast, twin
support vector regression (TSVR) is more efficient than SVR as it solves two
QPs each subject to one set of constraints. However, TSVR and its variants are
trained only on regular features and do not use privileged features for
training. To fill this gap, we introduce a fusion of TSVR with learning using
privileged information (LUPI) and propose a novel approach called twin support
vector regression with privileged information (TSVR+). The regularization terms
in the proposed TSVR+ capture the essence of statistical learning theory and
implement the structural risk minimization principle. We use the successive
overrelaxation (SOR) technique to solve the optimization problem of the
proposed TSVR+, which enhances the training efficiency. As far as our knowledge
extends, the integration of the LUPI concept into twin variants of regression
models is a novel advancement. The numerical experiments conducted on UCI,
stock and time series data collectively demonstrate the superiority of the
proposed model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02608">Panoptica -- instance-wise evaluation of 3D semantic and instance segmentation maps. (arXiv:2312.02608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1">Florian Kofler</a>, <a href="http://arxiv.org/find/cs/1/au:+Moller_H/0/1/0/all/0/1">Hendrik M&#xf6;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Buchner_J/0/1/0/all/0/1">Josef A. Buchner</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1">Ezequiel de la Rosa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1">Ivan Ezhov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosier_M/0/1/0/all/0/1">Marcel Rosier</a>, <a href="http://arxiv.org/find/cs/1/au:+Mekki_I/0/1/0/all/0/1">Isra Mekki</a>, <a href="http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1">Suprosanna Shit</a>, <a href="http://arxiv.org/find/cs/1/au:+Negwer_M/0/1/0/all/0/1">Moritz Negwer</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Maskari_R/0/1/0/all/0/1">Rami Al-Maskari</a>, <a href="http://arxiv.org/find/cs/1/au:+Erturk_A/0/1/0/all/0/1">Ali Ert&#xfc;rk</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinayahalingam_S/0/1/0/all/0/1">Shankeeth Vinayahalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1">Fabian Isensee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pati_S/0/1/0/all/0/1">Sarthak Pati</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1">Jan S. Kirschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehrlich_S/0/1/0/all/0/1">Stefan K. Ehrlich</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1">Annika Reinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1">Benedikt Wiestler</a>, <a href="http://arxiv.org/find/cs/1/au:+Piraud_M/0/1/0/all/0/1">Marie Piraud</a></p>
<p>This paper introduces panoptica, a versatile and performance-optimized
package designed for computing instance-wise segmentation quality metrics from
2D and 3D segmentation maps. panoptica addresses the limitations of existing
metrics and provides a modular framework that complements the original
intersection over union-based panoptic quality with other metrics, such as the
distance metric Average Symmetric Surface Distance. The package is open-source,
implemented in Python, and accompanied by comprehensive documentation and
tutorials. panoptica employs a three-step metrics computation process to cover
diverse use cases. The efficacy of panoptica is demonstrated on various
real-world biomedical datasets, where an instance-wise evaluation is
instrumental for an accurate representation of the underlying clinical task.
Overall, we envision panoptica as a valuable tool facilitating in-depth
evaluation of segmentation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02611">Privacy-Aware Data Acquisition under Data Similarity in Regression Markets. (arXiv:2312.02611v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1">Shashi Raj Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinson_P/0/1/0/all/0/1">Pierre Pinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1">Petar Popovski</a></p>
<p>Data markets facilitate decentralized data exchange for applications such as
prediction, learning, or inference. The design of these markets is challenged
by varying privacy preferences as well as data similarity among data owners.
Related works have often overlooked how data similarity impacts pricing and
data value through statistical information leakage. We demonstrate that data
similarity and privacy preferences are integral to market design and propose a
query-response protocol using local differential privacy for a two-party data
acquisition mechanism. In our regression data market model, we analyze
strategic interactions between privacy-aware owners and the learner as a
Stackelberg game over the asked price and privacy factor. Finally, we
numerically evaluate how data similarity affects market participation and
traded data value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02614">Prompt Optimization via Adversarial In-Context Learning. (arXiv:2312.02614v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1">Xuan Long Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiran Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1">Hannah Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">James Xu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nancy F. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Michael Qizhe Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a></p>
<p>We propose a new method, Adversarial In-Context Learning (adv-ICL), to
optimize prompt for in-context learning (ICL) by employing one LLM as a
generator, another as a discriminator, and a third as a prompt modifier. As in
traditional adversarial learning, adv-ICL is implemented as a two-player game
between the generator and discriminator, where the generator tries to generate
realistic enough output to fool the discriminator. In each round, given an
input prefixed by task instructions and several exemplars, the generator
produces an output. The discriminator is then tasked with classifying the
generator input-output pair as model-generated or real data. Based on the
discriminator loss, the prompt modifier proposes possible edits to the
generator and discriminator prompts, and the edits that most improve the
adversarial loss are selected. We show that adv-ICL results in significant
improvements over state-of-the-art prompt optimization techniques for both open
and closed-source models on 11 generation and classification tasks including
summarization, arithmetic reasoning, machine translation, data-to-text
generation, and the MMLU and big-bench hard benchmarks. In addition, because
our method uses pre-trained models and updates only prompts rather than model
parameters, it is computationally efficient, easy to extend to any LLM and
task, and effective in low-resource settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02615">Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models. (arXiv:2312.02615v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sungik Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hankook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Honglak Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Moontae Lee</a></p>
<p>Novelty detection is a fundamental task of machine learning which aims to
detect abnormal ($\textit{i.e.}$ out-of-distribution (OOD)) samples. Since
diffusion models have recently emerged as the de facto standard generative
framework with surprising generation results, novelty detection via diffusion
models has also gained much attention. Recent methods have mainly utilized the
reconstruction property of in-distribution samples. However, they often suffer
from detecting OOD samples that share similar background information to the
in-distribution data. Based on our observation that diffusion models can
\emph{project} any sample to an in-distribution sample with similar background
information, we propose \emph{Projection Regret (PR)}, an efficient novelty
detection method that mitigates the bias of non-semantic information. To be
specific, PR computes the perceptual distance between the test image and its
diffusion-based projection to detect abnormality. Since the perceptual distance
often fails to capture semantic changes when the background information is
dominant, we cancel out the background bias by comparing it against recursive
projections. Extensive experiments demonstrate that PR outperforms the prior
art of generative-model-based novelty detection methods by a significant
margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02619">Rethinking and Simplifying Bootstrapped Graph Latents. (arXiv:2312.02619v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wangbin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jintang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bingzhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1">Yatao Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a></p>
<p>Graph contrastive learning (GCL) has emerged as a representative paradigm in
graph self-supervised learning, where negative samples are commonly regarded as
the key to preventing model collapse and producing distinguishable
representations. Recent studies have shown that GCL without negative samples
can achieve state-of-the-art performance as well as scalability improvement,
with bootstrapped graph latent (BGRL) as a prominent step forward. However,
BGRL relies on a complex architecture to maintain the ability to scatter
representations, and the underlying mechanisms enabling the success remain
largely unexplored. In this paper, we introduce an instance-level decorrelation
perspective to tackle the aforementioned issue and leverage it as a springboard
to reveal the potential unnecessary model complexity within BGRL. Based on our
findings, we present SGCL, a simple yet effective GCL framework that utilizes
the outputs from two consecutive iterations as positive pairs, eliminating the
negative samples. SGCL only requires a single graph augmentation and a single
graph encoder without additional parameters. Extensive experiments conducted on
various graph benchmarks demonstrate that SGCL can achieve competitive
performance with fewer parameters, lower time and space costs, and significant
convergence speedup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02622">On the Initialization of Graph Neural Networks. (arXiv:2312.02622v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yakun Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xiang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1">David Paul Wipf</a></p>
<p>Graph Neural Networks (GNNs) have displayed considerable promise in graph
representation learning across various applications. The core learning process
requires the initialization of model weight matrices within each GNN layer,
which is typically accomplished via classic initialization methods such as
Xavier initialization. However, these methods were originally motivated to
stabilize the variance of hidden embeddings and gradients across layers of
Feedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to
avoid vanishing gradients and maintain steady information flow. In contrast,
within the GNN context classical initializations disregard the impact of the
input graph structure and message passing on variance. In this paper, we
analyze the variance of forward and backward propagation across GNN layers and
show that the variance instability of GNN initializations comes from the
combined effect of the activation function, hidden dimension, graph structure
and message passing. To better account for these influence factors, we propose
a new initialization method for Variance Instability Reduction within GNN
Optimization (Virgo), which naturally tends to equate forward and backward
variances across successive layers. We conduct comprehensive experiments on 15
datasets to show that Virgo can lead to superior model performance and more
stable variance at initialization on node classification, link prediction and
graph classification tasks. Codes are in
https://github.com/LspongebobJH/virgo_icml2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02646">SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting. (arXiv:2312.02646v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xiaobei Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Luolin Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurths_J/0/1/0/all/0/1">Jurgen Kurths</a></p>
<p>Spatio-temporal forecasting in various domains, like traffic prediction and
weather forecasting, is a challenging endeavor, primarily due to the
difficulties in modeling propagation dynamics and capturing high-dimensional
interactions among nodes. Despite the significant strides made by graph-based
networks in spatio-temporal forecasting, there remain two pivotal factors
closely related to forecasting performance that need further consideration:
time delays in propagation dynamics and multi-scale high-dimensional
interactions. In this work, we present a Series-Aligned Multi-Scale Graph
Learning (SAMSGL) framework, aiming to enhance forecasting performance. In
order to handle time delays in spatial interactions, we propose a
series-aligned graph convolution layer to facilitate the aggregation of
non-delayed graph signals, thereby mitigating the influence of time delays for
the improvement in accuracy. To understand global and local spatio-temporal
interactions, we develop a spatio-temporal architecture via multi-scale graph
learning, which encompasses two essential components: multi-scale graph
structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale
graph structure learning includes a global graph structure to learn both
delayed and non-delayed node embeddings, as well as a local one to learn node
variations influenced by neighboring factors. The Graph-FC blocks
synergistically fuse spatial and temporal information to boost prediction
accuracy. To evaluate the performance of SAMSGL, we conduct experiments on
meteorological and traffic forecasting datasets, which demonstrate its
effectiveness and superiority.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02649">A Q-learning approach to the continuous control problem of robot inverted pendulum balancing. (arXiv:2312.02649v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Safeea_M/0/1/0/all/0/1">Mohammad Safeea</a>, <a href="http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1">Pedro Neto</a></p>
<p>This study evaluates the application of a discrete action space reinforcement
learning method (Q-learning) to the continuous control problem of robot
inverted pendulum balancing. To speed up the learning process and to overcome
technical difficulties related to the direct learning on the real robotic
system, the learning phase is performed in simulation environment. A
mathematical model of the system dynamics is implemented, deduced by curve
fitting on data acquired from the real system. The proposed approach
demonstrated feasible, featuring its application on a real world robot that
learned to balance an inverted pendulum. This study also reinforces and
demonstrates the importance of an accurate representation of the physical world
in simulation to achieve a more efficient implementation of reinforcement
learning algorithms in real world, even when using a discrete action space
algorithm to control a continuous action.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02652">What Machine Learning Can Do for Focusing Aerogel Detectors. (arXiv:2312.02652v1 [hep-ex])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ex/1/au:+Shipilov_F/0/1/0/all/0/1">Foma Shipilov</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Barnyakov_A/0/1/0/all/0/1">Alexander Barnyakov</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Bobrovnikov_V/0/1/0/all/0/1">Vladimir Bobrovnikov</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Kononov_S/0/1/0/all/0/1">Sergey Kononov</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Ratnikov_F/0/1/0/all/0/1">Fedor Ratnikov</a></p>
<p>Particle identification at the Super Charm-Tau factory experiment will be
provided by a Focusing Aerogel Ring Imaging CHerenkov detector (FARICH). The
specifics of detector location make proper cooling difficult, therefore a
significant number of ambient background hits are captured. They must be
mitigated to reduce the data flow and improve particle velocity resolution. In
this work we present several approaches to filtering signal hits, inspired by
machine learning techniques from computer vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02658">Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\&#x27;an. (arXiv:2312.02658v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Charlton_Perez_A/0/1/0/all/0/1">Andrew J. Charlton-Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Dacre_H/0/1/0/all/0/1">Helen F. Dacre</a>, <a href="http://arxiv.org/find/cs/1/au:+Driscoll_S/0/1/0/all/0/1">Simon Driscoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1">Suzanne L. Gray</a>, <a href="http://arxiv.org/find/cs/1/au:+Harvey_B/0/1/0/all/0/1">Ben Harvey</a>, <a href="http://arxiv.org/find/cs/1/au:+Harvey_N/0/1/0/all/0/1">Natalie J. Harvey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hunt_K/0/1/0/all/0/1">Kieran M. R. Hunt</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Robert W. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1">Ranjini Swaminathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vandaele_R/0/1/0/all/0/1">Remy Vandaele</a>, <a href="http://arxiv.org/find/cs/1/au:+Volonte_A/0/1/0/all/0/1">Ambrogio Volont&#xe9;</a></p>
<p>There has been huge recent interest in the potential of making operational
weather forecasts using machine learning techniques. As they become a part of
the weather forecasting toolbox, there is a pressing need to understand how
well current machine learning models can simulate high-impactweather events. We
compare forecasts of Storm Ciar\'an, a European windstorm that caused sixteen
deaths and extensive damage in Northern Europe, made by machine learning and
numericalweather prediction models. The four machine learning models considered
(FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts
that accurately capture the synoptic-scale structure of the cyclone including
the position of the cloud head, shape of the warm sector and location of warm
conveyor belt jet, and the large-scale dynamical drivers important for the
rapid storm development such as the position of the storm relative to the
upper-level jet exit. However, their ability to resolve the more detailed
structures important for issuing weather warnings is more mixed. All of the
machine learning models underestimate the peak amplitude of winds associated
with the storm, only some machine learning models resolve the warm core
seclusion and none of the machine learning models capture the sharp bent-back
warm frontal gradient. Our study shows there is a great deal about the
performance and properties of machine learning weather forecasts that can be
derived from case studies of high-impact weather events such as Storm Ciar\'an.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02661">A Self-Commissioning Edge Computing Method for Data-Driven Anomaly Detection in Power Electronic Systems. (arXiv:2312.02661v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1">Pere Izquierdo Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gajardo_M/0/1/0/all/0/1">Miguel E. Lopez Gajardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mijatovic_N/0/1/0/all/0/1">Nenad Mijatovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragicevic_T/0/1/0/all/0/1">Tomislav Dragicevic</a></p>
<p>Ensuring the reliability of power electronic converters is a matter of great
importance, and data-driven condition monitoring techniques are cementing
themselves as an important tool for this purpose. However, translating methods
that work well in controlled lab environments to field applications presents
significant challenges, notably because of the limited diversity and accuracy
of the lab training data. By enabling the use of field data, online machine
learning can be a powerful tool to overcome this problem, but it introduces
additional challenges in ensuring the stability and predictability of the
training processes. This work presents an edge computing method that mitigates
these shortcomings with minimal additional memory usage, by employing an
autonomous algorithm that prioritizes the storage of training samples with
larger prediction errors. The method is demonstrated on the use case of a
self-commissioning condition monitoring system, in the form of a thermal
anomaly detection scheme for a variable frequency motor drive, where the
algorithm self-learned to distinguish normal and anomalous operation with
minimal prior knowledge. The obtained results, based on experimental data, show
a significant improvement in prediction accuracy and training speed, when
compared to equivalent models trained online without the proposed data
selection process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02665">Lights out: training RL agents robust to temporary blindness. (arXiv:2312.02665v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ordonez_N/0/1/0/all/0/1">N. Ordonez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tromp_M/0/1/0/all/0/1">M. Tromp</a>, <a href="http://arxiv.org/find/cs/1/au:+Julbe_P/0/1/0/all/0/1">P. M. Julbe</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohmer_W/0/1/0/all/0/1">W. B&#xf6;hmer</a></p>
<p>Agents trained with DQN rely on an observation at each timestep to decide
what action to take next. However, in real world applications observations can
change or be missing entirely. Examples of this could be a light bulb breaking
down, or the wallpaper in a certain room changing. While these situations
change the actual observation, the underlying optimal policy does not change.
Because of this we want our agent to continue taking actions until it receives
a (recognized) observation again. To achieve this we introduce a combination of
a neural network architecture that uses hidden representations of the
observations and a novel n-step loss function. Our implementation is able to
withstand location based blindness stretches longer than the ones it was
trained on, and therefore shows robustness to temporary blindness. For access
to our implementation, please email Nathan, Marije, or Pau.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02671">Learning a Sparse Representation of Barron Functions with the Inverse Scale Space Flow. (arXiv:2312.02671v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Heeringa_T/0/1/0/all/0/1">Tjeerd Jan Heeringa</a>, <a href="http://arxiv.org/find/stat/1/au:+Roith_T/0/1/0/all/0/1">Tim Roith</a>, <a href="http://arxiv.org/find/stat/1/au:+Brune_C/0/1/0/all/0/1">Christoph Brune</a>, <a href="http://arxiv.org/find/stat/1/au:+Burger_M/0/1/0/all/0/1">Martin Burger</a></p>
<p>This paper presents a method for finding a sparse representation of Barron
functions. Specifically, given an $L^2$ function $f$, the inverse scale space
flow is used to find a sparse measure $\mu$ minimising the $L^2$ loss between
the Barron function associated to the measure $\mu$ and the function $f$. The
convergence properties of this method are analysed in an ideal setting and in
the cases of measurement noise and sampling bias. In an ideal setting the
objective decreases strictly monotone in time to a minimizer with
$\mathcal{O}(1/t)$, and in the case of measurement noise or sampling bias the
optimum is achieved up to a multiplicative or additive constant. This
convergence is preserved on discretization of the parameter space, and the
minimizers on increasingly fine discretizations converge to the optimum on the
full parameter space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02674">Amortized Bayesian Decision Making for simulation-based models. (arXiv:2312.02674v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorecki_M/0/1/0/all/0/1">Mila Gorecki</a>, <a href="http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1">Jakob H. Macke</a>, <a href="http://arxiv.org/find/cs/1/au:+Deistler_M/0/1/0/all/0/1">Michael Deistler</a></p>
<p>Simulation-based inference (SBI) provides a powerful framework for inferring
posterior distributions of stochastic simulators in a wide range of domains. In
many settings, however, the posterior distribution is not the end goal itself
-- rather, the derived parameter values and their uncertainties are used as a
basis for deciding what actions to take. Unfortunately, because posterior
distributions provided by SBI are (potentially crude) approximations of the
true posterior, the resulting decisions can be suboptimal. Here, we address the
question of how to perform Bayesian decision making on stochastic simulators,
and how one can circumvent the need to compute an explicit approximation to the
posterior. Our method trains a neural network on simulated data and can predict
the expected cost given any data and action, and can, thus, be directly used to
infer the action with lowest cost. We apply our method to several benchmark
problems and demonstrate that it induces similar cost as the true posterior
distribution. We then apply the method to infer optimal actions in a real-world
simulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient,
and demonstrate that it allows to infer actions associated with low cost after
few simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02682">H-GAP: Humanoid Control with a Generalist Planner. (arXiv:2312.02682v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhengyao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yingchen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1">Nolan Wagener</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yicheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1">Michael Janner</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a></p>
<p>Humanoid control is an important research challenge offering avenues for
integration into human-centric infrastructures and enabling physics-driven
humanoid animations. The daunting challenges in this field stem from the
difficulty of optimizing in high-dimensional action spaces and the instability
introduced by the bipedal morphology of humanoids. However, the extensive
collection of human motion-captured data and the derived datasets of humanoid
trajectories, such as MoCapAct, paves the way to tackle these challenges. In
this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a
state-action trajectory generative model trained on humanoid trajectories
derived from human motion-captured data, capable of adeptly handling downstream
control tasks with Model Predictive Control (MPC). For 56 degrees of freedom
humanoid, we empirically demonstrate that H-GAP learns to represent and
generate a wide range of motor behaviours. Further, without any learning from
online interactions, it can also flexibly transfer these behaviors to solve
novel downstream control tasks via planning. Notably, H-GAP excels established
MPC baselines that have access to the ground truth dynamics model, and is
superior or comparable to offline RL methods trained for individual tasks.
Finally, we do a series of empirical studies on the scaling properties of
H-GAP, showing the potential for performance gains via additional data but not
computing. Code and videos are available at
https://ycxuyingchen.github.io/hgap/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02683">Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler. (arXiv:2312.02683v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gonzalez_P/0/1/0/all/0/1">Philippe Gonzalez</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1">Zheng-Hua Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ostergaard_J/0/1/0/all/0/1">Jan &#xd8;stergaard</a>, <a href="http://arxiv.org/find/eess/1/au:+Jensen_J/0/1/0/all/0/1">Jesper Jensen</a>, <a href="http://arxiv.org/find/eess/1/au:+Alstrom_T/0/1/0/all/0/1">Tommy Sonne Alstr&#xf8;m</a>, <a href="http://arxiv.org/find/eess/1/au:+May_T/0/1/0/all/0/1">Tobias May</a></p>
<p>Diffusion models are a new class of generative models that have recently been
applied to speech enhancement successfully. Previous works have demonstrated
their superior performance in mismatched conditions compared to state-of-the
art discriminative models. However, this was investigated with a single
database for training and another one for testing, which makes the results
highly dependent on the particular databases. Moreover, recent developments
from the image generation literature remain largely unexplored for speech
enhancement. These include several design aspects of diffusion models, such as
the noise schedule or the reverse sampler. In this work, we systematically
assess the generalization performance of a diffusion-based speech enhancement
model by using multiple speech, noise and binaural room impulse response (BRIR)
databases to simulate mismatched acoustic conditions. We also experiment with a
noise schedule and a sampler that have not been applied to speech enhancement
before. We show that the proposed system substantially benefits from using
multiple databases for training, and achieves superior performance compared to
state-of-the-art discriminative models in both matched and mismatched
conditions. We also show that a Heun-based sampler achieves superior
performance at a smaller computational cost compared to a sampler commonly used
for speech enhancement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02684">DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors. (arXiv:2312.02684v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaze Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Ziheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1">Qi Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuejie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenchao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1">Rui Feng</a></p>
<p>Point clouds have shown significant potential in various domains, including
Simultaneous Localization and Mapping (SLAM). However, existing approaches
either rely on dense point clouds to achieve high localization accuracy or use
generalized descriptors to reduce map size. Unfortunately, these two aspects
seem to conflict with each other. To address this limitation, we propose a
unified architecture, DeepPointMap, achieving excellent preference on both
aspects. We utilize neural network to extract highly representative and sparse
neural descriptors from point clouds, enabling memory-efficient map
representation and accurate multi-scale localization tasks (e.g., odometry and
loop-closure). Moreover, we showcase the versatility of our framework by
extending it to more challenging multi-agent collaborative SLAM. The promising
results obtained in these scenarios further emphasize the effectiveness and
potential of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02696">Analyzing and Improving the Training Dynamics of Diffusion Models. (arXiv:2312.02696v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1">Tero Karras</a>, <a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1">Miika Aittala</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1">Jaakko Lehtinen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1">Janne Hellsten</a>, <a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1">Timo Aila</a>, <a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1">Samuli Laine</a></p>
<p>Diffusion models currently dominate the field of data-driven image synthesis
with their unparalleled scaling to large datasets. In this paper, we identify
and rectify several causes for uneven and ineffective training in the popular
ADM diffusion model architecture, without altering its high-level structure.
Observing uncontrolled magnitude changes and imbalances in both the network
activations and weights over the course of training, we redesign the network
layers to preserve activation, weight, and update magnitudes on expectation. We
find that systematic application of this philosophy eliminates the observed
drifts and imbalances, resulting in considerably better networks at equal
computational complexity. Our modifications improve the previous record FID of
2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic
sampling.
</p>
<p>As an independent contribution, we present a method for setting the
exponential moving average (EMA) parameters post-hoc, i.e., after completing
the training run. This allows precise tuning of EMA length without the cost of
performing several training runs, and reveals its surprising interactions with
network architecture, training time, and guidance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02708">(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More. (arXiv:2312.02708v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1">Jan Schuchardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1">Yan Scholten</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>A machine learning model is traditionally considered robust if its prediction
remains (almost) constant under input perturbations with small norm. However,
real-world tasks like molecular property prediction or point cloud segmentation
have inherent equivariances, such as rotation or permutation equivariance. In
such tasks, even perturbations with large norm do not necessarily change an
input's semantic content. Furthermore, there are perturbations for which a
model's prediction explicitly needs to change. For the first time, we propose a
sound notion of adversarial robustness that accounts for task equivariance. We
then demonstrate that provable robustness can be achieved by (1) choosing a
model that matches the task's equivariances (2) certifying traditional
adversarial robustness. Certification methods are, however, unavailable for
many models, such as those with continuous equivariances. We close this gap by
developing the framework of equivariance-preserving randomized smoothing, which
enables architecture-agnostic certification. We additionally derive the first
architecture-specific graph edit distance certificates, i.e. sound robustness
guarantees for isomorphism equivariant tasks like node classification. Overall,
a sound notion of robustness is an important prerequisite for future work at
the intersection of robust and geometric machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02720">Towards the Inferrence of Structural Similarity of Combinatorial Landscapes. (arXiv:2312.02720v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Mingyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a></p>
<p>One of the most common problem-solving heuristics is by analogy. For a given
problem, a solver can be viewed as a strategic walk on its fitness landscape.
Thus if a solver works for one problem instance, we expect it will also be
effective for other instances whose fitness landscapes essentially share
structural similarities with each other. However, due to the black-box nature
of combinatorial optimization, it is far from trivial to infer such similarity
in real-world scenarios. To bridge this gap, by using local optima network as a
proxy of fitness landscapes, this paper proposed to leverage graph data mining
techniques to conduct qualitative and quantitative analyses to explore the
latent topological structural information embedded in those landscapes. By
conducting large-scale empirical experiments on three classic combinatorial
optimization problems, we gain concrete evidence to support the existence of
structural similarity between landscapes of the same classes within neighboring
dimensions. We also interrogated the relationship between landscapes of
different problem classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02730">Towards Measuring Representational Similarity of Large Language Models. (arXiv:2312.02730v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klabunde_M/0/1/0/all/0/1">Max Klabunde</a>, <a href="http://arxiv.org/find/cs/1/au:+Amor_M/0/1/0/all/0/1">Mehdi Ben Amor</a>, <a href="http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1">Michael Granitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemmerich_F/0/1/0/all/0/1">Florian Lemmerich</a></p>
<p>Understanding the similarity of the numerous released large language models
(LLMs) has many uses, e.g., simplifying model selection, detecting illegal
model reuse, and advancing our understanding of what makes LLMs perform well.
In this work, we measure the similarity of representations of a set of LLMs
with 7B parameters. Our results suggest that some LLMs are substantially
different from others. We identify challenges of using representational
similarity measures that suggest the need of careful study of similarity scores
to avoid false conclusions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02739">LExCI: A Framework for Reinforcement Learning with Embedded Systems. (arXiv:2312.02739v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Badalian_K/0/1/0/all/0/1">Kevin Badalian</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1">Lucas Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Brinkmann_T/0/1/0/all/0/1">Tobias Brinkmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Picerno_M/0/1/0/all/0/1">Mario Picerno</a>, <a href="http://arxiv.org/find/cs/1/au:+Wegener_M/0/1/0/all/0/1">Marius Wegener</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sung-Yong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Andert_J/0/1/0/all/0/1">Jakob Andert</a></p>
<p>Advances in artificial intelligence (AI) have led to its application in many
areas of everyday life. In the context of control engineering, reinforcement
learning (RL) represents a particularly promising approach as it is centred
around the idea of allowing an agent to freely interact with its environment to
find an optimal strategy. One of the challenges professionals face when
training and deploying RL agents is that the latter often have to run on
dedicated embedded devices. This could be to integrate them into an existing
toolchain or to satisfy certain performance criteria like real-time
constraints. Conventional RL libraries, however, cannot be easily utilised in
conjunction with that kind of hardware. In this paper, we present a framework
named LExCI, the Learning and Experiencing Cycle Interface, which bridges this
gap and provides end-users with a free and open-source tool for training agents
on embedded systems using the open-source library RLlib. Its operability is
demonstrated with two state-of-the-art RL-algorithms and a rapid control
prototyping system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02748">Compositional Generalization for Data-to-Text Generation. (arXiv:2312.02748v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xinnuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1">Ivan Titov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1">Mirella Lapata</a></p>
<p>Data-to-text generation involves transforming structured data, often
represented as predicate-argument tuples, into coherent textual descriptions.
Despite recent advances, systems still struggle when confronted with unseen
combinations of predicates, producing unfaithful descriptions (e.g.
hallucinations or omissions). We refer to this issue as compositional
generalisation, and it encouraged us to create a benchmark for assessing the
performance of different approaches on this specific problem. Furthermore, we
propose a novel model that addresses compositional generalization by clustering
predicates into groups. Our model generates text in a sentence-by-sentence
manner, relying on one cluster of predicates at a time. This approach
significantly outperforms T5~baselines across all evaluation metrics.Notably,
it achieved a 31% improvement over T5 in terms of a metric focused on
maintaining faithfulness to the input.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02753">C3: High-performance and low-complexity neural compression from a single image or video. (arXiv:2312.02753v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1">Hyunjik Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Bauer_M/0/1/0/all/0/1">Matthias Bauer</a>, <a href="http://arxiv.org/find/eess/1/au:+Theis_L/0/1/0/all/0/1">Lucas Theis</a>, <a href="http://arxiv.org/find/eess/1/au:+Schwarz_J/0/1/0/all/0/1">Jonathan Richard Schwarz</a>, <a href="http://arxiv.org/find/eess/1/au:+Dupont_E/0/1/0/all/0/1">Emilien Dupont</a></p>
<p>Most neural compression models are trained on large datasets of images or
videos in order to generalize to unseen data. Such generalization typically
requires large and expressive architectures with a high decoding complexity.
Here we introduce C3, a neural compression method with strong rate-distortion
(RD) performance that instead overfits a small model to each image or video
separately. The resulting decoding complexity of C3 can be an order of
magnitude lower than neural baselines with similar RD performance. C3 builds on
COOL-CHIC (Ladune et al.) and makes several simple and effective improvements
for images. We further develop new methodology to apply C3 to videos. On the
CLIC2020 image benchmark, we match the RD performance of VTM, the reference
implementation of the H.266 codec, with less than 3k MACs/pixel for decoding.
On the UVG video benchmark, we match the RD performance of the Video
Compression Transformer (Mentzer et al.), a well-established neural video
codec, with less than 5k MACs/pixel for decoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02770">Learning &quot;Look-Ahead&quot; Nonlocal Traffic Dynamics in a Ring Road. (arXiv:2312.02770v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenguang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Huan Yu</a></p>
<p>The macroscopic traffic flow model is widely used for traffic control and
management. To incorporate drivers' anticipative behaviors and to remove
impractical speed discontinuity inherent in the classic
Lighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differential
equation (PDE) models with ``look-ahead" dynamics have been proposed, which
assume that the speed is a function of weighted downstream traffic density.
However, it lacks data validation on two important questions: whether there
exist nonlocal dynamics, and how the length and weight of the ``look-ahead"
window affect the spatial temporal propagation of traffic densities. In this
paper, we adopt traffic trajectory data from a ring-road experiment and design
a physics-informed neural network to learn the fundamental diagram and
look-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal
LWR model via minimizing the loss function combining the data discrepancy and
the nonlocal model discrepancy. Results show that the learned nonlocal LWR
yields a more accurate prediction of traffic wave propagation in three
different scenarios: stop-and-go oscillations, congested, and free traffic. We
first demonstrate the existence of ``look-ahead" effect with real traffic data.
The optimal nonlocal kernel is found out to take a length of around 35 to 50
meters, and the kernel weight within 5 meters accounts for the majority of the
nonlocal effect. Our results also underscore the importance of choosing a
priori physics in machine learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02780">Scaling Laws for Adversarial Attacks on Language Model Activations. (arXiv:2312.02780v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1">Stanislav Fort</a></p>
<p>We explore a class of adversarial attacks targeting the activations of
language models. By manipulating a relatively small subset of model
activations, $a$, we demonstrate the ability to control the exact prediction of
a significant number (in some cases up to 1000) of subsequent tokens $t$. We
empirically verify a scaling law where the maximum number of target tokens
$t_\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose
activations the attacker controls as $t_\mathrm{max} = \kappa a$. We find that
the number of bits of control in the input space needed to control a single bit
in the output space (what we call attack resistance $\chi$) is remarkably
constant between $\approx 16$ and $\approx 25$ over 2 orders of magnitude of
model sizes for different language models. Compared to attacks on tokens,
attacks on activations are predictably much stronger, however, we identify a
surprising regularity where one bit of input steered either via activations or
via tokens is able to exert control over a similar amount of output bits. This
gives support for the hypothesis that adversarial attacks are a consequence of
dimensionality mismatch between the input and output spaces. A practical
implication of the ease of attacking language model activations instead of
tokens is for multi-modal and selected retrieval models, where additional data
sources are added as activations directly, sidestepping the tokenized input.
This opens up a new, broad attack surface. By using language models as a
controllable test-bed to study adversarial attacks, we were able to experiment
with input-output dimensions that are inaccessible in computer vision,
especially where the output dimension dominates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02783">Large Language Models on Graphs: A Comprehensive Survey. (arXiv:2312.02783v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1">Bowen Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Meng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a></p>
<p>Large language models (LLMs), such as ChatGPT and LLaMA, are creating
significant advancements in natural language processing, due to their strong
text encoding/decoding ability and newly found emergent capability (e.g.,
reasoning). While LLMs are mainly designed to process pure texts, there are
many real-world scenarios where text data are associated with rich structure
information in the form of graphs (e.g., academic networks, and e-commerce
networks) or scenarios where graph data are paired with rich textual
information (e.g., molecules with descriptions). Besides, although LLMs have
shown their pure text-based reasoning ability, it is underexplored whether such
ability can be generalized to graph scenarios (i.e., graph-based reasoning). In
this paper, we provide a systematic review of scenarios and techniques related
to large language models on graphs. We first summarize potential scenarios of
adopting LLMs on graphs into three categories, namely pure graphs, text-rich
graphs, and text-paired graphs. We then discuss detailed techniques for
utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM
as Aligner, and compare the advantages and disadvantages of different schools
of models. Furthermore, we mention the real-world applications of such methods
and summarize open-source codes and benchmark datasets. Finally, we conclude
with potential future research directions in this fast-growing field. The
related source can be found at
https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02786">Machine Learning Driven Sensitivity Analysis of E3SM Land Model Parameters for Wetland Methane Emissions. (arXiv:2312.02786v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Chinta_S/0/1/0/all/0/1">Sandeep Chinta</a>, <a href="http://arxiv.org/find/physics/1/au:+Gao_X/0/1/0/all/0/1">Xiang Gao</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhu_Q/0/1/0/all/0/1">Qing Zhu</a></p>
<p>Methane (CH4) is the second most critical greenhouse gas after carbon
dioxide, contributing to 16-25% of the observed atmospheric warming. Wetlands
are the primary natural source of methane emissions globally. However, wetland
methane emission estimates from biogeochemistry models contain considerable
uncertainty. One of the main sources of this uncertainty arises from the
numerous uncertain model parameters within various physical, biological, and
chemical processes that influence methane production, oxidation, and transport.
Sensitivity Analysis (SA) can help identify critical parameters for methane
emission and achieve reduced biases and uncertainties in future projections.
This study performs SA for 19 selected parameters responsible for critical
biogeochemical processes in the methane module of the Energy Exascale Earth
System Model (E3SM) land model (ELM). The impact of these parameters on various
CH4 fluxes is examined at 14 FLUXNET- CH4 sites with diverse vegetation types.
Given the extensive number of model simulations needed for global
variance-based SA, we employ a machine learning (ML) algorithm to emulate the
complex behavior of ELM methane biogeochemistry. ML enables the computational
time to be shortened significantly from 6 CPU hours to 0.72 milliseconds,
achieving reduced computational costs. We found that parameters linked to CH4
production and diffusion generally present the highest sensitivities despite
apparent seasonal variation. Comparing simulated emissions from perturbed
parameter sets against FLUXNET-CH4 observations revealed that better
performances can be achieved at each site compared to the default parameter
values. This presents a scope for further improving simulated emissions using
parameter calibration with advanced optimization techniques like Bayesian
optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02796">Materials Expert-Artificial Intelligence for Materials Discovery. (arXiv:2312.02796v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Liu_Y/0/1/0/all/0/1">Yanjun Liu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Jovanovic_M/0/1/0/all/0/1">Milena Jovanovic</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Mallayya_K/0/1/0/all/0/1">Krishnanand Mallayya</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Maddox_W/0/1/0/all/0/1">Wesley J. Maddox</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Klemenz_S/0/1/0/all/0/1">Sebastian Klemenz</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Schoop_L/0/1/0/all/0/1">Leslie M. Schoop</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kim_E/0/1/0/all/0/1">Eun-Ah Kim</a></p>
<p>The advent of material databases provides an unprecedented opportunity to
uncover predictive descriptors for emergent material properties from vast data
space. However, common reliance on high-throughput ab initio data necessarily
inherits limitations of such data: mismatch with experiments. On the other
hand, experimental decisions are often guided by an expert's intuition honed
from experiences that are rarely articulated. We propose using machine learning
to "bottle" such operational intuition into quantifiable descriptors using
expertly curated measurement-based data. We introduce "Materials
Expert-Artificial Intelligence" (ME-AI) to encapsulate and articulate this
human intuition. As a first step towards such a program, we focus on the
topological semimetal (TSM) among square-net materials as the property inspired
by the expert-identified descriptor based on structural information: the
tolerance factor. We start by curating a dataset encompassing 12 primary
features of 879 square-net materials, using experimental data whenever
possible. We then use Dirichlet-based Gaussian process regression using a
specialized kernel to reveal composite descriptors for square-net topological
semimetals. The ME-AI learned descriptors independently reproduce expert
intuition and expand upon it. Specifically, new descriptors point to
hypervalency as a critical chemical feature predicting TSM within square-net
compounds. Our success with a carefully defined problem points to the "machine
bottling human insight" approach as promising for machine learning-aided
material discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02798">Weakly Supervised Detection of Hallucinations in LLM Activations. (arXiv:2312.02798v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rateike_M/0/1/0/all/0/1">Miriam Rateike</a>, <a href="http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1">Celia Cintas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wamburu_J/0/1/0/all/0/1">John Wamburu</a>, <a href="http://arxiv.org/find/cs/1/au:+Akumu_T/0/1/0/all/0/1">Tanya Akumu</a>, <a href="http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1">Skyler Speakman</a></p>
<p>We propose an auditing method to identify whether a large language model
(LLM) encodes patterns such as hallucinations in its internal states, which may
propagate to downstream tasks. We introduce a weakly supervised auditing
technique using a subset scanning approach to detect anomalous patterns in LLM
activations from pre-trained models. Importantly, our method does not need
knowledge of the type of patterns a-priori. Instead, it relies on a reference
dataset devoid of anomalies during testing. Further, our approach enables the
identification of pivotal nodes responsible for encoding these patterns, which
may offer crucial insights for fine-tuning specific sub-networks for bias
mitigation. We introduce two new scanning methods to handle LLM activations for
anomalous sentences that may deviate from the expected distribution in either
direction. Our results confirm prior findings of BERT's limited internal
capacity for encoding hallucinations, while OPT appears capable of encoding
hallucination information internally. Importantly, our scanning approach,
without prior exposure to false statements, performs comparably to a fully
supervised out-of-distribution classifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02804">Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems. (arXiv:2312.02804v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Comte_C/0/1/0/all/0/1">C&#xe9;line Comte</a>, <a href="http://arxiv.org/find/cs/1/au:+Jonckheere_M/0/1/0/all/0/1">Matthieu Jonckheere</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanders_J/0/1/0/all/0/1">Jaron Sanders</a>, <a href="http://arxiv.org/find/cs/1/au:+Senen_Cerda_A/0/1/0/all/0/1">Albert Senen-Cerda</a></p>
<p>Stochastic networks and queueing systems often lead to Markov decision
processes (MDPs) with large state and action spaces as well as nonconvex
objective functions, which hinders the convergence of many reinforcement
learning (RL) algorithms. Policy-gradient methods perform well on MDPs with
large state and action spaces, but they sometimes experience slow convergence
due to the high variance of the gradient estimator. In this paper, we show that
some of these difficulties can be circumvented by exploiting the structure of
the underlying MDP. We first introduce a new family of gradient estimators
called score-aware gradient estimators (SAGEs). When the stationary
distribution of the MDP belongs to an exponential family parametrized by the
policy parameters, SAGEs allow us to estimate the policy gradient without
relying on value-function estimation, contrary to classical policy-gradient
methods like actor-critic. To demonstrate their applicability, we examine two
common control problems arising in stochastic networks and queueing systems
whose stationary distributions have a product-form, a special case of
exponential families. As a second contribution, we show that, under appropriate
assumptions, the policy under a SAGE-based policy-gradient method has a large
probability of converging to an optimal policy, provided that it starts
sufficiently close to it, even with a nonconvex objective function and multiple
maximizers. Our key assumptions are that, locally around a maximizer, a
nondegeneracy property of the Hessian of the objective function holds and a
Lyapunov function exists. Finally, we conduct a numerical comparison between a
SAGE-based policy-gradient method and an actor-critic algorithm. The results
demonstrate that the SAGE-based method finds close-to-optimal policies more
rapidly, highlighting its superior performance over the traditional
actor-critic method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02826">Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis. (arXiv:2312.02826v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Forest_F/0/1/0/all/0/1">Florent Forest</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an
effective and flexible solution, attracting extensive research. Deep neural
networks can learn rich representations from vast amounts of representative
labeled data for various applications. In IFD, they achieve high classification
performance from signals in an end-to-end manner, without requiring extensive
domain knowledge. However, deep learning models usually only perform well on
the data distribution they have been trained on. When applied to a different
distribution, they may experience performance drops. This is also observed in
IFD, where assets are often operated in working conditions different from those
in which labeled data have been collected. Unsupervised domain adaptation (UDA)
deals with the scenario where labeled data are available in a source domain,
and only unlabeled data are available in a target domain, where domains may
correspond to operating conditions. Recent methods rely on training with
confident pseudo-labels for target samples. However, the confidence-based
selection of pseudo-labels is hindered by poorly calibrated confidence
estimates in the target domain, primarily due to over-confident predictions,
which limits the quality of pseudo-labels and leads to error accumulation. In
this paper, we propose a novel UDA method called Calibrated Adaptive Teacher
(CAT), where we propose to calibrate the predictions of the teacher network
throughout the self-training process, leveraging post-hoc calibration
techniques. We evaluate CAT on domain-adaptive IFD and perform extensive
experiments on the Paderborn benchmark for bearing fault diagnosis under
varying operating conditions. Our proposed method achieves state-of-the-art
performance on most transfer tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02828">Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Karandikar_R/0/1/0/all/0/1">Rajeeva L. Karandikar</a>, <a href="http://arxiv.org/find/stat/1/au:+Vidyasagar_M/0/1/0/all/0/1">M. Vidyasagar</a></p>
<p>The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro
in 1951 has been a standard method for solving equations of the form
$\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements
of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) =
\nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can
also be used to find a stationary point of $J(\cdot)$. In much of the
literature, it is assumed that the error term ${\boldsymbol {xi}}_{t+1}$ has
zero conditional mean, and that its conditional variance is bounded as a
function of $t$ (though not necessarily with respect to ${\boldsymbol
{\theta}}_t$). Also, for the most part, the emphasis has been on
``synchronous'' SA, whereby, at each time $t$, \textit{every} component of
${\boldsymbol {\theta}}_t$ is updated. Over the years, SA has been applied to a
variety of areas, out of which two are the focus in this paper: Convex and
nonconvex optimization, and Reinforcement Learning (RL). As it turns out, in
these applications, the above-mentioned assumptions do not always hold. In
zero-order methods, the error neither has zero mean nor bounded conditional
variance. In the present paper, we extend SA theory to encompass errors with
nonzero conditional mean and/or unbounded conditional variance, and also
asynchronous SA. In addition, we derive estimates for the rate of convergence
of the algorithm. Then we apply the new results to problems in nonconvex
optimization, and to Markovian SA, a recently emerging area in RL. We prove
that SA converges in these situations, and compute the ``optimal step size
sequences'' to maximize the estimated rate of convergence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02829">MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition. (arXiv:2312.02829v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Menet_N/0/1/0/all/0/1">Nicolas Menet</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Hersche_M/0/1/0/all/0/1">Michael Hersche</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Karunaratne_G/0/1/0/all/0/1">Geethan Karunaratne</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1">Luca Benini</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1">Abu Sebastian</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1">Abbas Rahimi</a> (1) ((1) IBM Research - Zurich, (2) ETH Zurich)</p>
<p>With the advent of deep learning, progressively larger neural networks have
been designed to solve complex tasks. We take advantage of these capacity-rich
models to lower the cost of inference by exploiting computation in
superposition. To reduce the computational burden per input, we propose
Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling
many inputs at once. MIMONets augment various deep neural network architectures
with variable binding mechanisms to represent an arbitrary number of inputs in
a compositional data structure via fixed-width distributed representations.
Accordingly, MIMONets adapt nonlinear neural transformations to process the
data structure holistically, leading to a speedup nearly proportional to the
number of superposed input items in the data structure. After processing in
superposition, an unbinding mechanism recovers each transformed input of
interest. MIMONets also provide a dynamic trade-off between accuracy and
throughput by an instantaneous on-demand switching between a set of
accuracy-throughput operating points, yet within a single set of fixed
parameters. We apply the concept of MIMONets to both CNN and Transformer
architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical
evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy
delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and
CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining
a high average accuracy within a [-1.07, -3.43]% delta on the long range arena
benchmark. Finally, we provide mathematical bounds on the interference between
superposition channels in MIMOFormer. Our code is available at
https://github.com/IBM/multiple-input-multiple-output-nets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02843">Are Vision Transformers More Data Hungry Than Newborn Visual Systems?. (arXiv:2312.02843v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandey_L/0/1/0/all/0/1">Lalit Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_S/0/1/0/all/0/1">Samantha M. W. Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_J/0/1/0/all/0/1">Justin N. Wood</a></p>
<p>Vision transformers (ViTs) are top performing models on many computer vision
benchmarks and can accurately predict human behavior on object recognition
tasks. However, researchers question the value of using ViTs as models of
biological learning because ViTs are thought to be more data hungry than
brains, with ViTs requiring more training data to reach similar levels of
performance. To test this assumption, we directly compared the learning
abilities of ViTs and animals, by performing parallel controlled rearing
experiments on ViTs and newborn chicks. We first raised chicks in impoverished
visual environments containing a single object, then simulated the training
data available in those environments by building virtual animal chambers in a
video game engine. We recorded the first-person images acquired by agents
moving through the virtual chambers and used those images to train self
supervised ViTs that leverage time as a teaching signal, akin to biological
visual systems. When ViTs were trained through the eyes of newborn chicks, the
ViTs solved the same view invariant object recognition tasks as the chicks.
Thus, ViTs were not more data hungry than newborn visual systems: both learned
view invariant object representations in impoverished visual environments. The
flexible and generic attention based learning mechanism in ViTs combined with
the embodied data streams available to newborn animals appears sufficient to
drive the development of animal-like object recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02849">Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space. (arXiv:2312.02849v1 [math.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Jiang_Y/0/1/0/all/0/1">Yiheng Jiang</a>, <a href="http://arxiv.org/find/math/1/au:+Chewi_S/0/1/0/all/0/1">Sinho Chewi</a>, <a href="http://arxiv.org/find/math/1/au:+Pooladian_A/0/1/0/all/0/1">Aram-Alexandre Pooladian</a></p>
<p>We develop a theory of finite-dimensional polyhedral subsets over the
Wasserstein space and optimization of functionals over them via first-order
methods. Our main application is to the problem of mean-field variational
inference, which seeks to approximate a distribution $\pi$ over $\mathbb{R}^d$
by a product measure $\pi^\star$. When $\pi$ is strongly log-concave and
log-smooth, we provide (1) approximation rates certifying that $\pi^\star$ is
close to the minimizer $\pi^\star_\diamond$ of the KL divergence over a
\emph{polyhedral} set $\mathcal{P}_\diamond$, and (2) an algorithm for
minimizing $\text{KL}(\cdot\|\pi)$ over $\mathcal{P}_\diamond$ with accelerated
complexity $O(\sqrt \kappa \log(\kappa d/\varepsilon^2))$, where $\kappa$ is
the condition number of $\pi$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02850">A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis. (arXiv:2312.02850v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hou_T/0/1/0/all/0/1">Tingting Hou</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_C/0/1/0/all/0/1">Chang Jiang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lu_Q/0/1/0/all/0/1">Qing Lu</a></p>
<p>The recent development of artificial intelligence (AI) technology, especially
the advance of deep neural network (DNN) technology, has revolutionized many
fields. While DNN plays a central role in modern AI technology, it has been
rarely used in sequencing data analysis due to challenges brought by
high-dimensional sequencing data (e.g., overfitting). Moreover, due to the
complexity of neural networks and their unknown limiting distributions,
building association tests on neural networks for genetic association analysis
remains a great challenge. To address these challenges and fill the important
gap of using AI in high-dimensional sequencing data analysis, we introduce a
new kernel-based neural network (KNN) test for complex association analysis of
sequencing data. The test is built on our previously developed KNN framework,
which uses random effects to model the overall effects of high-dimensional
genetic data and adopts kernel-based neural network structures to model complex
genotype-phenotype relationships. Based on KNN, a Wald-type test is then
introduced to evaluate the joint association of high-dimensional genetic data
with a disease phenotype of interest, considering non-linear and non-additive
effects (e.g., interaction effects). Through simulations, we demonstrated that
our proposed method attained higher power compared to the sequence kernel
association test (SKAT), especially in the presence of non-linear and
interaction effects. Finally, we apply the methods to the whole genome
sequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative
(ADNI) study, investigating new genes associated with the hippocampal volume
change over time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02852">Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems. (arXiv:2312.02852v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1">Tom Savage</a>, <a href="http://arxiv.org/find/cs/1/au:+Chanona_E/0/1/0/all/0/1">Ehecatl Antonio del Rio Chanona</a></p>
<p>Domain experts often possess valuable physical insights that are overlooked
in fully automated decision-making processes such as Bayesian optimisation. In
this article we apply high-throughput (batch) Bayesian optimisation alongside
anthropological decision theory to enable domain experts to influence the
selection of optimal experiments. Our methodology exploits the hypothesis that
humans are better at making discrete choices than continuous ones and enables
experts to influence critical early decisions. At each iteration we solve an
augmented multi-objective optimisation problem across a number of alternate
solutions, maximising both the sum of their utility function values and the
determinant of their covariance matrix, equivalent to their total variability.
By taking the solution at the knee point of the Pareto front, we return a set
of alternate solutions at each iteration that have both high utility values and
are reasonably distinct, from which the expert selects one for evaluation. We
demonstrate that even in the case of an uninformed practitioner, our algorithm
recovers the regret of standard Bayesian optimisation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02855">Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study. (arXiv:2312.02855v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qiao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wengui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1">Jorge Cardoso</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1">Odej Kao</a></p>
<p>In large-scale datacenters, memory failure is a common cause of server
crashes, with uncorrectable errors (UEs) being a major indicator of Dual Inline
Memory Module (DIMM) defects. Existing approaches primarily focus on predicting
UEs using correctable errors (CEs), without fully considering the information
provided by error bits. However, error bit patterns have a strong correlation
with the occurrence of uncorrectable errors (UEs). In this paper, we present a
comprehensive study on the correlation between CEs and UEs, specifically
emphasizing the importance of spatio-temporal error bit information. Our
analysis reveals a strong correlation between spatio-temporal error bits and UE
occurrence. Through evaluations using real-world datasets, we demonstrate that
our approach significantly improves prediction performance by 15% in F1-score
compared to the state-of-the-art algorithms. Overall, our approach effectively
reduces the number of virtual machine interruptions caused by UEs by
approximately 59%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02858">Towards Causal Representations of Climate Model Data. (arXiv:2312.02858v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boussard_J/0/1/0/all/0/1">Julien Boussard</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagda_C/0/1/0/all/0/1">Chandni Nagda</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaltenborn_J/0/1/0/all/0/1">Julia Kaltenborn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_C/0/1/0/all/0/1">Charlotte Emilie Elektra Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1">Philippe Brouillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurwicz_Y/0/1/0/all/0/1">Yaniv Gurwicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowack_P/0/1/0/all/0/1">Peer Nowack</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1">David Rolnick</a></p>
<p>Climate models, such as Earth system models (ESMs), are crucial for
simulating future climate change based on projected Shared Socioeconomic
Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated
and invaluable, machine learning-based emulators trained on existing simulation
data can project additional climate scenarios much faster and are
computationally efficient. However, they often lack generalizability and
interpretability. This work delves into the potential of causal representation
learning, specifically the \emph{Causal Discovery with Single-parent Decoding}
(CDSD) method, which could render climate model emulation efficient
\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,
focusing on emissions, temperature, and precipitation. Our findings shed light
on the challenges, limitations, and promise of using CDSD as a stepping stone
towards more interpretable and robust climate model emulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02859">Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring. (arXiv:2312.02859v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zytek_A/0/1/0/all/0/1">Alexandra Zytek</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei-En Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koukoura_S/0/1/0/all/0/1">Sofia Koukoura</a>, <a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1">Kalyan Veeramachaneni</a></p>
<p>Through past experiences deploying what we call usable ML (one step beyond
explainable ML, including both explanations and other augmenting information)
to real-world domains, we have learned three key lessons. First, many
organizations are beginning to hire people who we call ``bridges'' because they
bridge the gap between ML developers and domain experts, and these people fill
a valuable role in developing usable ML applications. Second, a configurable
system that enables easily iterating on usable ML interfaces during
collaborations with bridges is key. Finally, there is a need for continuous,
in-deployment evaluations to quantify the real-world impact of usable ML.
Throughout this paper, we apply these lessons to the task of wind turbine
monitoring, an essential task in the renewable energy domain. Turbine engineers
and data analysts must decide whether to perform costly in-person
investigations on turbines to prevent potential cases of brakepad failure, and
well-tuned usable ML interfaces can aid with this decision-making process.
Through the applications of our lessons to this task, we hope to demonstrate
the potential real-world impact of usable ML in the renewable energy domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02867">Semi-Supervised Health Index Monitoring with Feature Generation and Fusion. (arXiv:2312.02867v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frusque_G/0/1/0/all/0/1">Ga&#xeb;tan Frusque</a>, <a href="http://arxiv.org/find/cs/1/au:+Nejjar_I/0/1/0/all/0/1">Ismail Nejjar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabavi_M/0/1/0/all/0/1">Majid Nabavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>The Health Index (HI) is crucial for evaluating system health, aiding tasks
like anomaly detection and predicting remaining useful life for systems
demanding high safety and reliability. Tight monitoring is crucial for
achieving high precision at a lower cost, with applications such as spray
coating. Obtaining HI labels in real-world applications is often
cost-prohibitive, requiring continuous, precise health measurements. Therefore,
it is more convenient to leverage run-to failure datasets that may provide
potential indications of machine wear condition, making it necessary to apply
semi-supervised tools for HI construction. In this study, we adapt the Deep
Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use
the DeepSAD embedding as a condition indicators to address interpretability
challenges and sensitivity to system-specific factors. Then, we introduce a
diversity loss to enrich condition indicators. We employ an alternating
projection algorithm with isotonic constraints to transform the DeepSAD
embedding into a normalized HI with an increasing trend. Validation on the PHME
2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates
meaningful HIs estimations. Our methodology is then applied to monitor wear
states of thermal spray coatings using high-frequency voltage. Our
contributions create opportunities for more accessible and reliable HI
estimation, particularly in cases where obtaining ground truth HI labels is
unfeasible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02871">Attention-enhanced neural differential equations for physics-informed deep learning of ion transport. (arXiv:2312.02871v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rehman_D/0/1/0/all/0/1">Danyal Rehman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lienhard_J/0/1/0/all/0/1">John H. Lienhard</a></p>
<p>Species transport models typically combine partial differential equations
(PDEs) with relations from hindered transport theory to quantify
electromigrative, convective, and diffusive transport through complex
nanoporous systems; however, these formulations are frequently substantial
simplifications of the governing dynamics, leading to the poor generalization
performance of PDE-based models. Given the growing interest in deep learning
methods for the physical sciences, we develop a machine learning-based approach
to characterize ion transport across nanoporous membranes. Our proposed
framework centers around attention-enhanced neural differential equations that
incorporate electroneutrality-based inductive biases to improve generalization
performance relative to conventional PDE-based methods. In addition, we study
the role of the attention mechanism in illuminating physically-meaningful
ion-pairing relationships across diverse mixture compositions. Further, we
investigate the importance of pre-training on simulated data from PDE-based
models, as well as the performance benefits from hard vs. soft inductive
biases. Our results indicate that physics-informed deep learning solutions can
outperform their classical PDE-based counterparts and provide promising avenues
for modelling complex transport phenomena across diverse applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02872">Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction. (arXiv:2312.02872v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Melo_A/0/1/0/all/0/1">Angie Nataly Melo</a>, <a href="http://arxiv.org/find/cs/1/au:+Salinas_C/0/1/0/all/0/1">Carlota Salinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sotelo_M/0/1/0/all/0/1">Miguel Angel Sotelo</a></p>
<p>In the context of autonomous driving, pedestrian crossing prediction is a key
component for improving road safety. Presently, the focus of these predictions
extends beyond achieving trustworthy results; it is shifting towards the
explainability and interpretability of these predictions. This research
introduces a novel neuro-symbolic approach that combines deep learning and
fuzzy logic for an explainable and interpretable pedestrian crossing
prediction. We have developed an explainable predictor (ExPedCross), which
utilizes a set of explainable features and employs a fuzzy inference system to
predict whether the pedestrian will cross or not. Our approach was evaluated on
both the PIE and JAAD datasets. The results offer experimental insights into
achieving explainability and interpretability in the pedestrian crossing
prediction task. Furthermore, the testing results yield a set of guidelines and
recommendations regarding the process of dataset selection, feature selection,
and explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02873">Toward autocorrection of chemical process flowsheets using large language models. (arXiv:2312.02873v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balhorn_L/0/1/0/all/0/1">Lukas Schulze Balhorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Caballero_M/0/1/0/all/0/1">Marc Caballero</a>, <a href="http://arxiv.org/find/cs/1/au:+Schweidtmann_A/0/1/0/all/0/1">Artur M. Schweidtmann</a></p>
<p>The process engineering domain widely uses Process Flow Diagrams (PFDs) and
Process and Instrumentation Diagrams (P&amp;IDs) to represent process flows and
equipment configurations. However, the P&amp;IDs and PFDs, hereafter called
flowsheets, can contain errors causing safety hazards, inefficient operation,
and unnecessary expenses. Correcting and verifying flowsheets is a tedious,
manual process. We propose a novel generative AI methodology for automatically
identifying errors in flowsheets and suggesting corrections to the user, i.e.,
autocorrecting flowsheets. Inspired by the breakthrough of Large Language
Models (LLMs) for grammatical autocorrection of human language, we investigate
LLMs for the autocorrection of flowsheets. The input to the model is a
potentially erroneous flowsheet and the output of the model are suggestions for
a corrected flowsheet. We train our autocorrection model on a synthetic dataset
in a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5
accuracy of 84% on an independent test dataset of synthetically generated
flowsheets. The results suggest that the model can learn to autocorrect the
synthetic flowsheets. We envision that flowsheet autocorrection will become a
useful tool for chemical engineers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.01347">Traffic Signal Control with Communicative Deep Reinforcement Learning Agents: a Case Study. (arXiv:2107.01347v4 [cs.MA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fazzini_P/0/1/0/all/0/1">Paolo Fazzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Wheeler_I/0/1/0/all/0/1">Isaac Wheeler</a>, <a href="http://arxiv.org/find/cs/1/au:+Petracchini_F/0/1/0/all/0/1">Francesco Petracchini</a></p>
<p>In this work we analyze Multi-Agent Advantage Actor-Critic (MA2C) a recently
proposed multi-agent reinforcement learning algorithm that can be applied to
adaptive traffic signal control (ATSC) problems. To evaluate its potential we
compare MA2C with Independent Advantage Actor-Critic (IA2C) and other
Reinforcement Learning or heuristic based algorithms. Specifically, we analyze
MA2C theoretically with the framework provided by non-Markov decision
processes, which allows a deeper insight of the algorithm, and we critically
examine the effectiveness and the robustness of the method by testing it in two
traffic areas located in Bologna (Italy) simulated in SUMO, a software modeling
tool for ATSC problems. Our results indicate that MA2C, trained with
pseudo-random vehicle flows, is a promising technique able to outperform the
alternative methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.00955">Source Free Unsupervised Graph Domain Adaptation. (arXiv:2112.00955v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haitao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Lun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yujia Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qiang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zelin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Graph Neural Networks (GNNs) have achieved great success on a variety of
tasks with graph-structural data, among which node classification is an
essential one. Unsupervised Graph Domain Adaptation (UGDA) shows its practical
value of reducing the labeling cost for node classification. It leverages
knowledge from a labeled graph (i.e., source domain) to tackle the same task on
another unlabeled graph (i.e., target domain). Most existing UGDA methods
heavily rely on the labeled graph in the source domain. They utilize labels
from the source domain as the supervision signal and are jointly trained on
both the source graph and the target graph. However, in some real-world
scenarios, the source graph is inaccessible because of privacy issues.
Therefore, we propose a novel scenario named Source Free Unsupervised Graph
Domain Adaptation (SFUGDA). In this scenario, the only information we can
leverage from the source domain is the well-trained source model, without any
exposure to the source graph and its labels. As a result, existing UGDA methods
are not feasible anymore. To address the non-trivial adaptation challenges in
this practical scenario, we propose a model-agnostic algorithm called SOGA for
domain adaptation to fully exploit the discriminative ability of the source
model while preserving the consistency of structural proximity on the target
graph. We prove the effectiveness of the proposed algorithm both theoretically
and empirically. The experimental results on four cross-domain tasks show
consistent improvements in the Macro-F1 score and Macro-AUC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.06467">NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release. (arXiv:2202.06467v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Donghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a></p>
<p>Privacy-preserving data release algorithms have gained increasing attention
for their ability to protect user privacy while enabling downstream machine
learning tasks. However, the utility of current popular algorithms is not
always satisfactory. Mixup of raw data provides a new way of data augmentation,
which can help improve utility. However, its performance drastically
deteriorates when differential privacy (DP) noise is added. To address this
issue, this paper draws inspiration from the recently observed Neural Collapse
(NC) phenomenon, which states that the last layer features of a neural network
concentrate on the vertices of a simplex as Equiangular Tight Frame (ETF). We
propose a scheme to mixup the Neural Collapse features to exploit the ETF
simplex structure and release noisy mixed features to enhance the utility of
the released data. By using Gaussian Differential Privacy (GDP), we obtain an
asymptotic rate for the optimal mixup degree. To further enhance the utility
and address the label collapse issue when the mixup degree is large, we propose
a Hierarchical sampling method to stratify the mixup samples on a small number
of classes. This method remarkably improves utility when the number of classes
is large. Extensive experiments demonstrate the effectiveness of our proposed
method in protecting against attacks and improving utility. In particular, our
approach shows significantly improved utility compared to directly training
classification networks with DPSGD on CIFAR100 and MiniImagenet datasets,
highlighting the benefits of using privacy-preserving data release. We release
reproducible code in https://github.com/Lidonghao1996/NeuroMixGDP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.02016">Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1">Joschka Boedecker</a></p>
<p>Reinforcement learning (RL) is recognized as lacking generalization and
robustness under environmental perturbations, which excessively restricts its
application for real-world robotics. Prior work claimed that adding
regularization to the value function is equivalent to learning a robust policy
with uncertain transitions. Although the regularization-robustness
transformation is appealing for its simplicity and efficiency, it is still
lacking in continuous control tasks. In this paper, we propose a new
regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer
(USR), by formulating the uncertainty set on the parameter space of the
transition function. In particular, USR is flexible enough to be plugged into
any existing RL framework. To deal with unknown uncertainty sets, we further
propose a novel adversarial approach to generate them based on the value
function. We evaluate USR on the Real-world Reinforcement Learning (RWRL)
benchmark, demonstrating improvements in the robust performance for perturbed
testing environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06033">Bayesian Soft Actor-Critic: A Directed Acyclic Strategy Graph Based Deep Reinforcement Learning. (arXiv:2208.06033v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Parasuraman_R/0/1/0/all/0/1">Ramviyas Parasuraman</a></p>
<p>Adopting reasonable strategies is challenging but crucial for an intelligent
agent with limited resources working in hazardous, unstructured, and dynamic
environments to improve the system's utility, decrease the overall cost, and
increase mission success probability. This paper proposes a novel directed
acyclic strategy graph decomposition approach based on Bayesian chaining to
separate an intricate policy into several simple sub-policies and organize
their relationships as Bayesian strategy networks (BSN). We integrate this
approach into the state-of-the-art DRL method -- soft actor-critic (SAC), and
build the corresponding Bayesian soft actor-critic (BSAC) model by organizing
several sub-policies as a joint policy. We compare our method against the
state-of-the-art deep reinforcement learning algorithms on the standard
continuous control benchmarks in the OpenAI Gym environment. The results
demonstrate that the promising potential of the BSAC method significantly
improves training efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.02900">Grassmann Manifold Flows for Stable Shape Generation. (arXiv:2211.02900v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yataka_R/0/1/0/all/0/1">Ryoma Yataka</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirashima_K/0/1/0/all/0/1">Kazuki Hirashima</a>, <a href="http://arxiv.org/find/cs/1/au:+Shiraishi_M/0/1/0/all/0/1">Masashi Shiraishi</a></p>
<p>Recently, studies on machine learning have focused on methods that use
symmetry implicit in a specific manifold as an inductive bias. Grassmann
manifolds provide the ability to handle fundamental shapes represented as shape
spaces, enabling stable shape analysis. In this paper, we present a novel
approach in which we establish the theoretical foundations for learning
distributions on the Grassmann manifold via continuous normalization flows,
with the explicit goal of generating stable shapes. Our approach facilitates
more robust generation by effectively eliminating the influence of extraneous
transformations, such as rotations and inversions, through learning and
generating within a Grassmann manifold designed to accommodate the essential
shape information of the object. The experimental results indicated that the
proposed method could generate high-quality samples by capturing the data
structure. Furthermore, the proposed method significantly outperformed
state-of-the-art methods in terms of the log-likelihood or evidence lower
bound. The results obtained are expected to stimulate further research in this
field, leading to advances for stable shape generation and analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.15498">Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Pilar_P/0/1/0/all/0/1">Philipp Pilar</a>, <a href="http://arxiv.org/find/stat/1/au:+Wahlstrom_N/0/1/0/all/0/1">Niklas Wahlstr&#xf6;m</a></p>
<p>Physics-informed neural networks (PINNs) constitute a flexible approach to
both finding solutions and identifying parameters of partial differential
equations. Most works on the topic assume noiseless data, or data contaminated
with weak Gaussian noise. We show that the standard PINN framework breaks down
in case of non-Gaussian noise. We give a way of resolving this fundamental
issue and we propose to jointly train an energy-based model (EBM) to learn the
correct noise distribution. We illustrate the improved performance of our
approach using multiple examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16934">VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing. (arXiv:2211.16934v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yihan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Junliang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ruihua Song</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1">Arul Menezes</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Video dubbing aims to translate the original speech in a film or television
program into the speech in a target language, which can be achieved with a
cascaded system consisting of speech recognition, machine translation and
speech synthesis. To ensure the translated speech to be well aligned with the
corresponding video, the length/duration of the translated speech should be as
close as possible to that of the original speech, which requires strict length
control. Previous works usually control the number of words or characters
generated by the machine translation model to be similar to the source
sentence, without considering the isochronicity of speech as the speech
duration of words/characters in different languages varies. In this paper, we
propose a machine translation system tailored for the task of video dubbing,
which directly considers the speech duration of each token in translation, to
match the length of source and target speech. Specifically, we control the
speech length of generated sentence by guiding the prediction of each word with
the duration information, including the speech duration of itself as well as
how much duration is left for the remaining words. We design experiments on
four language directions (German -&gt; English, Spanish -&gt; English, Chinese &lt;-&gt;
English), and the results show that the proposed method achieves better length
control ability on the generated speech than baseline methods. To make up the
lack of real-world datasets, we also construct a real-world test set collected
from films to provide comprehensive evaluations on the video dubbing task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16994">Continual Learning with Distributed Optimization: Does CoCoA Forget?. (arXiv:2211.16994v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hellkvist_M/0/1/0/all/0/1">Martin Hellkvist</a>, <a href="http://arxiv.org/find/stat/1/au:+Ozcelikkale_A/0/1/0/all/0/1">Ay&#xe7;a &#xd6;z&#xe7;elikkale</a>, <a href="http://arxiv.org/find/stat/1/au:+Ahlen_A/0/1/0/all/0/1">Anders Ahl&#xe9;n</a></p>
<p>We focus on the continual learning problem where the tasks arrive
sequentially and the aim is to perform well on the newly arrived task without
performance degradation on the previously seen tasks. In contrast to the
continual learning literature focusing on the centralized setting, we
investigate the distributed estimation framework. We consider the
well-established distributed learning algorithm COCOA. We derive closed form
expressions for the iterations for the overparametrized case. We illustrate the
convergence and the error performance of the algorithm based on the
over/under-parameterization of the problem. Our results show that depending on
the problem dimensions and data generation assumptions, COCOA can perform
continual learning over a sequence of tasks, i.e., it can learn a new task
without forgetting previously learned tasks, with access only to one task at a
time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00433">Regularization Trade-offs with Fake Features. (arXiv:2212.00433v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hellkvist_M/0/1/0/all/0/1">Martin Hellkvist</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozcelikkale_A/0/1/0/all/0/1">Ay&#xe7;a &#xd6;z&#xe7;elikkale</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahlen_A/0/1/0/all/0/1">Anders Ahl&#xe9;n</a></p>
<p>Recent successes of massively overparameterized models have inspired a new
line of work investigating the underlying conditions that enable
overparameterized models to generalize well. This paper considers a framework
where the possibly overparametrized model includes fake features, i.e.,
features that are present in the model but not in the data. We present a
non-asymptotic high-probability bound on the generalization error of the ridge
regression problem under the model misspecification of having fake features.
Our highprobability results provide insights into the interplay between the
implicit regularization provided by the fake features and the explicit
regularization provided by the ridge parameter. Numerical results illustrate
the trade-off between the number of fake features and how the optimal ridge
parameter may heavily depend on the number of fake features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03150">MOTOR: A Time-To-Event Foundation Model For Structured Medical Records. (arXiv:2301.03150v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1">Ethan Steinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1">Jason Fries</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yizhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Nigam Shah</a></p>
<p>We present a self-supervised, time-to-event (TTE) foundation model called
MOTOR (Many Outcome Time Oriented Representations) which is pretrained on
timestamped sequences of events in electronic health records (EHR) and health
insurance claims. TTE models are used for estimating the probability
distribution of the time until a specific event occurs, which is an important
task in medical settings. TTE models provide many advantages over
classification using fixed time horizons, including naturally handling censored
observations, but are challenging to train with limited labeled data. MOTOR
addresses this challenge by pretraining on up to 55M patient records (9B
clinical events). We evaluate MOTOR's transfer learning performance on 19
tasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merative
claims data). Task-specific models adapted from MOTOR improve time-dependent C
statistics by 4.6% over state-of-the-art, improve label efficiency by up to 95%
,and are more robust to temporal distributional shifts. We further evaluate
cross-site portability by adapting our MOTOR foundation model for six
prediction tasks on the MIMIC-IV dataset, where it outperforms all baselines.
MOTOR is the first foundation model for medical TTE predictions and we release
a 143M parameter pretrained model for research use at [redacted URL].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03573">Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction. (arXiv:2301.03573v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Bowen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shuren He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallick_B/0/1/0/all/0/1">Bani K. Mallick</a></p>
<p>Despite impressive performance, deep neural networks require significant
memory and computation costs, prohibiting their application in
resource-constrained scenarios. Sparse training is one of the most common
techniques to reduce these costs, however, the sparsity constraints add
difficulty to the optimization, resulting in an increase in training time and
instability. In this work, we aim to overcome this problem and achieve
space-time co-efficiency. To accelerate and stabilize the convergence of sparse
training, we analyze the gradient changes and develop an adaptive gradient
correction method. Specifically, we approximate the correlation between the
current and previous gradients, which is used to balance the two gradients to
obtain a corrected gradient. Our method can be used with the most popular
sparse training pipelines under both standard and adversarial setups.
Theoretically, we prove that our method can accelerate the convergence rate of
sparse training. Extensive experiments on multiple datasets, model
architectures, and sparsities demonstrate that our method outperforms leading
sparse training methods by up to \textbf{5.0\%} in accuracy given the same
number of training epochs, and reduces the number of training epochs by up to
\textbf{52.1\%} to achieve the same accuracy. Our code is available on:
\url{https://github.com/StevenBoys/AGENT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03962">A Unified Theory of Diversity in Ensemble Learning. (arXiv:2301.03962v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wood_D/0/1/0/all/0/1">Danny Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1">Tingting Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_A/0/1/0/all/0/1">Andrew Webb</a>, <a href="http://arxiv.org/find/cs/1/au:+Reeve_H/0/1/0/all/0/1">Henry Reeve</a>, <a href="http://arxiv.org/find/cs/1/au:+Lujan_M/0/1/0/all/0/1">Mikel Lujan</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1">Gavin Brown</a></p>
<p>We present a theory of ensemble diversity, explaining the nature of diversity
for a wide range of supervised learning scenarios. This challenge, of
understanding ensemble diversity, has been referred to as the "holy grail" of
ensemble learning, an open research issue for over 30 years. Our framework
reveals that diversity is in fact a hidden dimension in the bias-variance
decomposition of the ensemble loss. We prove a family of exact
bias-variance-diversity decompositions, for both regression and classification,
e.g., squared, cross-entropy, and Poisson losses. For losses where an additive
bias-variance decomposition is not available (e.g., 0/1 loss) we present an
alternative approach, which precisely quantifies the effects of diversity,
turning out to be dependent on the label distribution. Experiments show how we
can use our framework to understand the diversity-encouraging mechanisms of
popular methods: Bagging, Boosting, and Random Forests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10250">Solving Inverse Physics Problems with Score Matching. (arXiv:2301.10250v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holzschuh_B/0/1/0/all/0/1">Benjamin J. Holzschuh</a>, <a href="http://arxiv.org/find/cs/1/au:+Vegetti_S/0/1/0/all/0/1">Simona Vegetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Thuerey_N/0/1/0/all/0/1">Nils Thuerey</a></p>
<p>We propose to solve inverse problems involving the temporal evolution of
physics systems by leveraging recent advances from diffusion models. Our method
moves the system's current state backward in time step by step by combining an
approximate inverse physics simulator and a learned correction function. A
central insight of our work is that training the learned correction with a
single-step loss is equivalent to a score matching objective, while recursively
predicting longer parts of the trajectory during training relates to maximum
likelihood training of a corresponding probability flow. We highlight the
advantages of our algorithm compared to standard denoising score matching and
implicit score matching, as well as fully learned baselines for a wide range of
inverse physics problems. The resulting inverse solver has excellent accuracy
and temporal stability and, in contrast to other learned inverse solvers,
allows for sampling the posterior of the solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08272">Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Juodelyte_D/0/1/0/all/0/1">Dovile Juodelyte</a>, <a href="http://arxiv.org/find/cs/1/au:+Jimenez_Sanchez_A/0/1/0/all/0/1">Amelia Jim&#xe9;nez-S&#xe1;nchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1">Veronika Cheplygina</a></p>
<p>While a key component to the success of deep learning is the availability of
massive amounts of training data, medical image datasets are often limited in
diversity and size. Transfer learning has the potential to bridge the gap
between related yet different domains. For medical applications, however, it
remains unclear whether it is more beneficial to pre-train on natural or
medical images. We aim to shed light on this problem by comparing
initialization on ImageNet and RadImageNet on seven medical classification
tasks. Our work includes a replication study, which yields results contrary to
previously published findings. In our experiments, ResNet50 models pre-trained
on ImageNet tend to outperform those trained on RadImageNet. To gain further
insights, we investigate the learned representations using Canonical
Correlation Analysis (CCA) and compare the predictions of the different models.
Our results indicate that, contrary to intuition, ImageNet and RadImageNet may
converge to distinct intermediate representations, which appear to diverge
further during fine-tuning. Despite these distinct representations, the
predictions of the models remain similar. Our findings show that the similarity
between networks before and after fine-tuning does not correlate with
performance gains, suggesting that the advantages of transfer learning might
not solely originate from the reuse of features in the early layers of a
convolutional neural network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00438">A Framework for Neurosymbolic Robot Action Planning using Large Language Models. (arXiv:2303.00438v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Capitanelli_A/0/1/0/all/0/1">Alessio Capitanelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastrogiovanni_F/0/1/0/all/0/1">Fulvio Mastrogiovanni</a></p>
<p>Symbolic task planning is a widely used approach to enforce robot autonomy
due to its ease of understanding and deployment. However, symbolic task
planning is difficult to scale in real-world when frequent re-planning is
needed, for example, due to human-robot interactions or unforeseen events. Plan
length and planning time can hinder the robot's efficiency and negatively
affect the overall human-robot interaction's fluency. We present a framework,
Teriyaki, designed to bridge the gap between symbolic task planning and machine
learning approaches, by training Large Language Models (LLMs), namely GPT-3,
into neurosymbolic task planners compatible with the Planning Domain Definition
Language (PDDL). Potential benefits include: (i) better scalability in so far
as the planning domain complexity increases, since LLMs' response time linearly
scales with the combined length of the input and the output, instead of
super-linearly as in the case of symbolic task planners, and (ii) the ability
to synthesize a plan action-by-action instead of end-to-end, and to make each
action available for execution as soon as it is generated, which in turn
enables concurrent planning and execution. In the past year, significant
efforts have been devoted by the research community to evaluate the overall
cognitive abilities of LLMs, with alternate successes. Instead, with Teriyaki
we aim to providing an overall planning performance comparable to traditional
planners in specific planning domains, while leveraging LLMs capabilities in
other metrics which are used to build a look-ahead predictive planning model.
Preliminary results in selected domains show that our method can: (i) solve
95.5% of problems in a test data set of 1000 samples; (ii) produce plans up to
13.5% shorter than a traditional symbolic planner; (iii) reduce average overall
waiting times for a plan availability by up to 61.4%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07527">Domain Generalization via Nuclear Norm Regularization. (arXiv:2303.07527v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenmei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1">Yifei Ming</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Ying Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yingyu Liang</a></p>
<p>The ability to generalize to unseen domains is crucial for machine learning
systems deployed in the real world, especially when we only have data from
limited training domains. In this paper, we propose a simple and effective
regularization method based on the nuclear norm of the learned features for
domain generalization. Intuitively, the proposed regularizer mitigates the
impacts of environmental features and encourages learning domain-invariant
features. Theoretically, we provide insights into why nuclear norm
regularization is more effective compared to ERM and alternative regularization
methods. Empirically, we conduct extensive experiments on both synthetic and
real datasets. We show nuclear norm regularization achieves strong performance
compared to baselines in a wide range of domain generalization tasks. Moreover,
our regularizer is broadly applicable with various methods such as ERM and SWAD
with consistently improved performance, e.g., 1.7% and 0.9% test accuracy
improvements respectively on the DomainBed benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06059">Efficient Deep Learning Models for Privacy-preserving People Counting on Low-resolution Infrared Arrays. (arXiv:2304.06059v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chen Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Daghero_F/0/1/0/all/0/1">Francesco Daghero</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yukai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Castellano_M/0/1/0/all/0/1">Marco Castellano</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandolfi_L/0/1/0/all/0/1">Luca Gandolfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Calimera_A/0/1/0/all/0/1">Andrea Calimera</a>, <a href="http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1">Enrico Macii</a>, <a href="http://arxiv.org/find/cs/1/au:+Poncino_M/0/1/0/all/0/1">Massimo Poncino</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1">Daniele Jahier Pagliari</a></p>
<p>Ultra-low-resolution Infrared (IR) array sensors offer a low-cost,
energy-efficient, and privacy-preserving solution for people counting, with
applications such as occupancy monitoring. Previous work has shown that Deep
Learning (DL) can yield superior performance on this task. However, the
literature was missing an extensive comparative analysis of various efficient
DL architectures for IR array-based people counting, that considers not only
their accuracy, but also the cost of deploying them on memory- and
energy-constrained Internet of Things (IoT) edge nodes. In this work, we
address this need by comparing 6 different DL architectures on a novel dataset
composed of IR images collected from a commercial 8x8 array, which we made
openly available. With a wide architectural exploration of each model type, we
obtain a rich set of Pareto-optimal solutions, spanning cross-validated
balanced accuracy scores in the 55.70-82.70% range. When deployed on a
commercial Microcontroller (MCU) by STMicroelectronics, the STM32L4A6ZG, these
models occupy 0.41-9.28kB of memory, and require 1.10-7.74ms per inference,
while consuming 17.18-120.43 $\mu$J of energy. Our models are significantly
more accurate than a previous deterministic method (up to +39.9%), while being
up to 3.53x faster and more energy efficient. Further, our models' accuracy is
comparable to state-of-the-art DL solutions on similar resolution sensors,
despite a much lower complexity. All our models enable continuous, real-time
inference on a MCU-based IoT node, with years of autonomous operation without
battery recharging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13616">CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing. (arXiv:2304.13616v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Altmann_P/0/1/0/all/0/1">Philipp Altmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritz_F/0/1/0/all/0/1">Fabian Ritz</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuchtinger_L/0/1/0/all/0/1">Leonard Feuchtinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Nusslein_J/0/1/0/all/0/1">Jonas N&#xfc;&#xdf;lein</a>, <a href="http://arxiv.org/find/cs/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1">Claudia Linnhoff-Popien</a>, <a href="http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1">Thomy Phan</a></p>
<p>The safe application of reinforcement learning (RL) requires generalization
from limited training data to unseen scenarios. Yet, fulfilling tasks under
changing circumstances is a key challenge in RL. Current state-of-the-art
approaches for generalization apply data augmentation techniques to increase
the diversity of training data. Even though this prevents overfitting to the
training environment(s), it hinders policy optimization. Crafting a suitable
observation, only containing crucial information, has been shown to be a
challenging task itself. To improve data efficiency and generalization
capabilities, we propose Compact Reshaped Observation Processing (CROP) to
reduce the state information used for policy optimization. By providing only
relevant information, overfitting to a specific training layout is precluded
and generalization to unseen environments is improved. We formulate three CROPs
that can be applied to fully observable observation- and action-spaces and
provide methodical foundation. We empirically show the improvements of CROP in
a distributionally shifted safety gridworld. We furthermore provide benchmark
comparisons to full observability and data-augmentation in two different-sized
procedurally generated mazes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01090">Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems. (arXiv:2305.01090v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kevin Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jesus_C/0/1/0/all/0/1">Carlos E. P&#xe9;rez De Jes&#xfa;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_A/0/1/0/all/0/1">Andrew J. Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1">Michael D. Graham</a></p>
<p>While many phenomena in physics and engineering are formally
high-dimensional, their long-time dynamics often live on a lower-dimensional
manifold. The present work introduces an autoencoder framework that combines
implicit regularization with internal linear layers and $L_2$ regularization
(weight decay) to automatically estimate the underlying dimensionality of a
data set, produce an orthogonal manifold coordinate system, and provide the
mapping functions between the ambient space and manifold space, allowing for
out-of-sample projections. We validate our framework's ability to estimate the
manifold dimension for a series of datasets from dynamical systems of varying
complexities and compare to other state-of-the-art estimators. We analyze the
training dynamics of the network to glean insight into the mechanism of
low-rank learning and find that collectively each of the implicit regularizing
layers compound the low-rank representation and even self-correct during
training. Analysis of gradient descent dynamics for this architecture in the
linear case reveals the role of the internal linear layers in leading to faster
decay of a "collective weight variable" incorporating all layers, and the role
of weight decay in breaking degeneracies and thus driving convergence along
directions in which no decay would occur in its absence. We show that this
framework can be naturally extended for applications of state-space modeling
and forecasting by generating a data-driven dynamic model of a spatiotemporally
chaotic partial differential equation using only the manifold coordinates.
Finally, we demonstrate that our framework is robust to hyperparameter choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15930">End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maraval_A/0/1/0/all/0/1">Alexandre Maraval</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1">Matthieu Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1">Antoine Grosnit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ammar_H/0/1/0/all/0/1">Haitham Bou Ammar</a></p>
<p>Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of
Bayesian optimisation by leveraging data from related tasks. While previous
methods successfully meta-learn either a surrogate model or an acquisition
function independently, joint training of both components remains an open
challenge. This paper proposes the first end-to-end differentiable meta-BO
framework that generalises neural processes to learn acquisition functions via
transformer architectures. We enable this end-to-end framework with
reinforcement learning (RL) to tackle the lack of labelled acquisition data.
Early on, we notice that training transformer-based neural processes from
scratch with RL is challenging due to insufficient supervision, especially when
rewards are sparse. We formalise this claim with a combinatorial analysis
showing that the widely used notion of regret as a reward signal exhibits a
logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we
augment the RL objective with an auxiliary task that guides part of the
architecture to learn a valid probabilistic model as an inductive bias. We
demonstrate that our method achieves state-of-the-art regret results against
various baselines in experiments on standard hyperparameter optimisation tasks
and also outperforms others in the real-world problems of mixed-integer
programming tuning, antibody design, and logic synthesis for electronic design
automation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16573">Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasegawa_N/0/1/0/all/0/1">Naoya Hasegawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a></p>
<p>Recognition problems in long-tailed data, in which the sample size per class
is heavily skewed, have gained importance because the distribution of the
sample size per class in a dataset is generally exponential unless the sample
size is intentionally adjusted. Various methods have been devised to address
these problems. Recently, weight balancing, which combines well-known classical
regularization techniques with two-stage training, has been proposed. Despite
its simplicity, it is known for its high performance compared with existing
methods devised in various ways. However, there is a lack of understanding as
to why this method is effective for long-tailed data. In this study, we analyze
weight balancing by focusing on neural collapse and the cone effect at each
training stage and found that it can be decomposed into an increase in Fisher's
discriminant ratio of the feature extractor caused by weight decay and cross
entropy loss and implicit logit adjustment caused by weight decay and
class-balanced loss. Our analysis enables the training method to be further
simplified by reducing the number of training stages to one while increasing
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19798">Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1">Qinghua Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonin_F/0/1/0/all/0/1">Francesco Tonin</a>, <a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1">Johan A.K. Suykens</a></p>
<p>Recently, a new line of works has emerged to understand and improve
self-attention in Transformers by treating it as a kernel machine. However,
existing works apply the methods for symmetric kernels to the asymmetric
self-attention, resulting in a nontrivial gap between the analytical
understanding and numerical implementation. In this paper, we provide a new
perspective to represent and optimize self-attention through asymmetric Kernel
Singular Value Decomposition (KSVD), which is also motivated by the low-rank
property of self-attention normally observed in deep layers. Through asymmetric
KSVD, $i$) a primal-dual representation of self-attention is formulated, where
the optimization objective is cast to maximize the projection variances in the
attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,
is proposed via the primal representation of KSVD, avoiding explicit
computation of the kernel matrix in the dual; $iii$) with KKT conditions, we
prove that the stationary solution to the KSVD optimization in Primal-Attention
yields a zero-value objective. In this manner, KSVD optimization can be
implemented by simply minimizing a regularization loss, so that low-rank
property is promoted without extra decomposition. Numerical experiments show
state-of-the-art performance of our Primal-Attention with improved efficiency.
Moreover, we demonstrate that the deployed KSVD optimization regularizes
Primal-Attention with a sharper singular value decay than that of the canonical
self-attention, further verifying the great potential of our method. To the
best of our knowledge, this is the first work that provides a primal-dual
representation for the asymmetric kernel in self-attention and successfully
applies it to modeling and optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04026">Value Functions are Control Barrier Functions: Verification of Safe Policies using Control Theory. (arXiv:2306.04026v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1">Daniel C.H. Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Acero_F/0/1/0/all/0/1">Fernando Acero</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarthy_R/0/1/0/all/0/1">Robert McCarthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanoulas_D/0/1/0/all/0/1">Dimitrios Kanoulas</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhibin Li</a></p>
<p>Guaranteeing safe behaviour of reinforcement learning (RL) policies poses
significant challenges for safety-critical applications, despite RL's
generality and scalability. To address this, we propose a new approach to apply
verification methods from control theory to learned value functions. By
analyzing task structures for safety preservation, we formalize original
theorems that establish links between value functions and control barrier
functions. Further, we propose novel metrics for verifying value functions in
safe control tasks and practical implementation details to improve learning.
Our work presents a novel method for certificate learning, which unlocks a
diversity of verification techniques from control theory for RL policies, and
marks a significant step towards a formal framework for the general, scalable,
and verifiable design of RL-based control systems. Code and videos are
available at this https url: https://rl-cbf.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04037">Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1">Akshatha Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1">Joshua Peeples</a></p>
<p>We present a comprehensive analysis of quantitatively evaluating explainable
artificial intelligence (XAI) techniques for remote sensing image
classification. Our approach leverages state-of-the-art machine learning
approaches to perform remote sensing image classification across multiple
modalities. We investigate the results of the models qualitatively through XAI
methods. Additionally, we compare the XAI methods quantitatively through
various categories of desired properties. Through our analysis, we offer
insights and recommendations for selecting the most appropriate XAI method(s)
to gain a deeper understanding of the models' decision-making processes. The
code for this work is publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09750">Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beltran_E/0/1/0/all/0/1">Enrique Tom&#xe1;s Mart&#xed;nez Beltr&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1">&#xc1;ngel Luis Perales G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1">Pedro Miguel S&#xe1;nchez S&#xe1;nchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernal_S/0/1/0/all/0/1">Sergio L&#xf3;pez Bernal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1">G&#xe9;r&#xf4;me Bovet</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1">Manuel Gil P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Gregorio Mart&#xed;nez P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1">Alberto Huertas Celdr&#xe1;n</a></p>
<p>In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train
Machine Learning (ML) models across the participants of a federation while
preserving data privacy. Since its birth, Centralized FL (CFL) has been the
most used approach, where a central entity aggregates participants' models to
create a global one. However, CFL presents limitations such as communication
bottlenecks, single point of failure, and reliance on a central server.
Decentralized Federated Learning (DFL) addresses these issues by enabling
decentralized model aggregation and minimizing dependency on a central entity.
Despite these advances, current platforms training DFL models struggle with key
issues such as managing heterogeneous federation network topologies. To
overcome these challenges, this paper presents Fedstellar, a novel platform
designed to train FL models in a decentralized, semi-decentralized, and
centralized fashion across diverse federations of physical or virtualized
devices. The Fedstellar implementation encompasses a web application with an
interactive graphical interface, a controller for deploying federations of
nodes using physical or virtual devices, and a core deployed on each device
which provides the logic needed to train, aggregate, and communicate in the
network. The effectiveness of the platform has been demonstrated in two
scenarios: a physical deployment involving single-board devices such as
Raspberry Pis for detecting cyberattacks, and a virtualized deployment
comparing various FL approaches in a controlled environment using MNIST and
CIFAR-10 datasets. In both scenarios, Fedstellar demonstrated consistent
performance and adaptability, achieving F1 scores of 91%, 98%, and 91.2% using
DFL for detecting cyberattacks and classifying MNIST and CIFAR-10,
respectively, reducing training time by 32% compared to centralized approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10191">Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1">Matthew Wallingford</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1">Vivek Ramanujan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1">Alex Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a>, <a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1">Roozbeh Mottaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1">Aniruddha Kembhavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1">Ludwig Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a></p>
<p>We propose Neural Priming, a technique for adapting large pretrained models
to distribution shifts and downstream tasks given few or no labeled examples.
Presented with class names or unlabeled test samples, Neural Priming enables
the model to recall and conditions its parameters on relevant data seen
throughout pretraining, thereby priming it for the test distribution. Neural
Priming can be performed at test time, even for pretraining datasets as large
as LAION-2B. Performing lightweight updates on the recalled data significantly
improves accuracy across a variety of distribution shift and transfer learning
benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in
accuracy on ImageNet and 3.81% accuracy improvement on average across standard
transfer learning benchmarks. Further, using Neural Priming at inference to
adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2.
These results demonstrate the effectiveness of Neural Priming in addressing the
challenge of limited labeled data and changing distributions. Code is available
at github.com/RAIVNLab/neural-priming.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10797">Variability of echo state network prediction horizon for partially observed dynamical systems. (arXiv:2306.10797v3 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mahata_A/0/1/0/all/0/1">Ajit Mahata</a>, <a href="http://arxiv.org/find/eess/1/au:+Padhi_R/0/1/0/all/0/1">Reetish Padhi</a>, <a href="http://arxiv.org/find/eess/1/au:+Apte_A/0/1/0/all/0/1">Amit Apte</a></p>
<p>Study of dynamical systems using partial state observation is an important
problem due to its applicability to many real-world systems. We address the
problem by studying an echo state network (ESN) framework with partial state
input with partial or full state output. Application to the Lorenz system and
Chua's oscillator (both numerically simulated and experimental systems)
demonstrate the effectiveness of our method. We show that the ESN, as an
autonomous dynamical system, is capable of making short-term predictions up to
a few Lyapunov times. However, the prediction horizon has high variability
depending on the initial condition-an aspect that we explore in detail using
the distribution of the prediction horizon. Further, using a variety of
statistical metrics to compare the long-term dynamics of the ESN predictions
with numerically simulated or experimental dynamics and observed similar
results, we show that the ESN can effectively learn the system's dynamics even
when trained with noisy numerical or experimental datasets. Thus, we
demonstrate the potential of ESNs to serve as cheap surrogate models for
simulating the dynamics of systems where complete observations are unavailable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16334">On the Identifiability of Quantized Factors. (arXiv:2306.16334v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barin_Pacela_V/0/1/0/all/0/1">Vit&#xf3;ria Barin-Pacela</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1">Kartik Ahuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1">Simon Lacoste-Julien</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1">Pascal Vincent</a></p>
<p>Disentanglement aims to recover meaningful latent ground-truth factors from
the observed distribution solely, and is formalized through the theory of
identifiability. The identifiability of independent latent factors is proven to
be impossible in the unsupervised i.i.d. setting under a general nonlinear map
from factors to observations. In this work, however, we demonstrate that it is
possible to recover quantized latent factors under a generic nonlinear
diffeomorphism. We only assume that the latent factors have independent
discontinuities in their density, without requiring the factors to be
statistically independent. We introduce this novel form of identifiability,
termed quantized factor identifiability, and provide a comprehensive proof of
the recovery of the quantized factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00908">Quantum Machine Learning on Near-Term Quantum Devices: Current State of Supervised and Unsupervised Techniques for Real-World Applications. (arXiv:2307.00908v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Gujju_Y/0/1/0/all/0/1">Yaswitha Gujju</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Matsuo_A/0/1/0/all/0/1">Atsushi Matsuo</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Raymond_R/0/1/0/all/0/1">Rudy Raymond</a></p>
<p>The past decade has witnessed significant advancements in quantum hardware,
encompassing improvements in speed, qubit quantity, and quantum volume-a metric
defining the maximum size of a quantum circuit effectively implementable on
near-term quantum devices. This progress has led to a surge in Quantum Machine
Learning (QML) applications on real hardware, aiming to achieve quantum
advantage over classical approaches. This survey focuses on selected supervised
and unsupervised learning applications executed on quantum hardware,
specifically tailored for real-world scenarios. The exploration includes a
thorough analysis of current QML implementation limitations on quantum
hardware, covering techniques like encoding, ansatz structure, error
mitigation, and gradient methods to address these challenges. Furthermore, the
survey evaluates the performance of QML implementations in comparison to
classical counterparts. In conclusion, we discuss existing bottlenecks related
to applying QML on real quantum devices and propose potential solutions to
overcome these challenges in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06046">A Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jincheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1">Beatrice Bevilacqua</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1">Bruno Ribeiro</a></p>
<p>The task of inductive link prediction in (discrete) attributed multigraphs
infers missing attributed links (relations) between nodes in new test
multigraphs. Traditional relational learning methods face the challenge of
limited generalization to test multigraphs containing both novel nodes and
novel relation types not seen in training. Recently, under the only assumption
that all relation types share the same structural predictive patterns (single
task), Gao et al. (2023) proposed a link prediction method using the
theoretical concept of double equivariance (equivariance for nodes &amp; relation
types), in contrast to the (single) equivariance (only for nodes) used to
design Graph Neural Networks (GNNs). In this work we further extend the double
equivariance concept to multi-task double equivariance, where we define link
prediction in attributed multigraphs that can have distinct and potentially
conflicting predictive patterns for different sets of relation types (multiple
tasks). Our empirical results on real-world datasets demonstrate that our
approach can effectively generalize to test graphs with multi-task structures
without access to additional information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07980">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xingrong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhaoxian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1">Qing Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhi Tian</a></p>
<p>This paper studies distributed online learning under Byzantine attacks. The
performance of an online learning algorithm is often characterized by
(adversarial) regret, which evaluates the quality of one-step-ahead
decision-making when an environment provides adversarial losses, and a
sublinear bound is preferred. But we prove that, even with a class of
state-of-the-art robust aggregation rules, in an adversarial environment and in
the presence of Byzantine participants, distributed online gradient descent can
only achieve a linear adversarial regret bound, which is tight. This is the
inevitable consequence of Byzantine attacks, even though we can control the
constant of the linear adversarial regret to a reasonable level. Interestingly,
when the environment is not fully adversarial so that the losses of the honest
participants are i.i.d. (independent and identically distributed), we show that
sublinear stochastic regret, in contrast to the aforementioned adversarial
regret, is possible. We develop a Byzantine-robust distributed online momentum
algorithm to attain such a sublinear stochastic regret bound. Extensive
numerical experiments corroborate our theoretical analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10988">On minimizing the training set fill distance in machine learning regression. (arXiv:2307.10988v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Climaco_P/0/1/0/all/0/1">Paolo Climaco</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1">Jochen Garcke</a></p>
<p>For regression tasks one often leverages large datasets for training
predictive machine learning models. However, using large datasets may not be
feasible due to computational limitations or high data labelling costs.
Therefore, suitably selecting small training sets from large pools of
unlabelled data points is essential to maximize model performance while
maintaining efficiency. In this work, we study Farthest Point Sampling (FPS), a
data selection approach that aims to minimize the fill distance of the selected
set. We derive an upper bound for the maximum expected prediction error,
conditional to the location of the unlabelled data points, that linearly
depends on the training set fill distance. For empirical validation, we perform
experiments using two regression models on three datasets. We empirically show
that selecting a training set by aiming to minimize the fill distance, thereby
minimizing our derived bound, significantly reduces the maximum prediction
error of various regression models, outperforming alternative sampling
approaches by a large margin. Furthermore, we show that selecting training sets
with the FPS can also increase model stability for the specific case of
Gaussian kernel regression approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06053">Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinyue Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Suyeon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jonghyun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1">Myeongjae Jeon</a></p>
<p>Continual learning (CL) trains NN models incrementally from a continuous
stream of tasks. To remember previously learned knowledge, prior studies store
old samples over a memory hierarchy and replay them when new tasks arrive. Edge
devices that adopt CL to preserve data privacy are typically energy-sensitive
and thus require high model accuracy while not compromising energy efficiency,
i.e., cost-effectiveness. Our work is the first to explore the design space of
hierarchical memory replay-based CL to gain insights into achieving
cost-effectiveness on edge devices. We present Miro, a novel system runtime
that carefully integrates our insights into the CL framework by enabling it to
dynamically configure the CL system based on resource states for the best
cost-effectiveness. To reach this goal, Miro also performs online profiling on
parameters with clear accuracy-energy trade-offs and adapts to optimal values
with low overhead. Extensive evaluations show that Miro significantly
outperforms baseline systems we build for comparison, consistently achieving
higher cost-effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08410">Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach. (arXiv:2308.08410v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Grandits_T/0/1/0/all/0/1">Thomas Grandits</a>, <a href="http://arxiv.org/find/math/1/au:+Verhulsdonk_J/0/1/0/all/0/1">Jan Verh&#xfc;lsdonk</a>, <a href="http://arxiv.org/find/math/1/au:+Haase_G/0/1/0/all/0/1">Gundolf Haase</a>, <a href="http://arxiv.org/find/math/1/au:+Effland_A/0/1/0/all/0/1">Alexander Effland</a>, <a href="http://arxiv.org/find/math/1/au:+Pezzuto_S/0/1/0/all/0/1">Simone Pezzuto</a></p>
<p>The eikonal equation has become an indispensable tool for modeling cardiac
electrical activation accurately and efficiently. In principle, by matching
clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible
to build patient-specific models of cardiac electrophysiology in a purely
non-invasive manner. Nonetheless, the fitting procedure remains a challenging
task. The present study introduces a novel method, Geodesic-BP, to solve the
inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine
learning frameworks, allowing us to optimize the parameters of the eikonal
equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a
simulated cardiac activation with high accuracy in a synthetic test case, even
in the presence of modeling inaccuracies. Furthermore, we apply our algorithm
to a publicly available dataset of a biventricular rabbit model, with promising
results. Given the future shift towards personalized medicine, Geodesic-BP has
the potential to help in future functionalizations of cardiac models meeting
clinical time constraints while maintaining the physiological accuracy of
state-of-the-art cardiac models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11068">Topological Graph Signal Compression. (arXiv:2308.11068v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bernardez_G/0/1/0/all/0/1">Guillermo Bern&#xe1;rdez</a>, <a href="http://arxiv.org/find/cs/1/au:+Telyatnikov_L/0/1/0/all/0/1">Lev Telyatnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Alarcon_E/0/1/0/all/0/1">Eduard Alarc&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1">Albert Cabellos-Aparicio</a>, <a href="http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1">Pere Barlet-Ros</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>Recently emerged Topological Deep Learning (TDL) methods aim to extend
current Graph Neural Networks (GNN) by naturally processing higher-order
interactions, going beyond the pairwise relations and local neighborhoods
defined by graph representations. In this paper we propose a novel TDL-based
method for compressing signals over graphs, consisting in two main steps:
first, disjoint sets of higher-order structures are inferred based on the
original signal --by clustering $N$ datapoints into $K\ll N$ collections; then,
a topological-inspired message passing gets a compressed representation of the
signal within those multi-element sets. Our results show that our framework
improves both standard GNN and feed-forward architectures in compressing
temporal link-based signals from two real-word Internet Service Provider
Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors
across all evaluation scenarios--, suggesting that it better captures and
exploits spatial and temporal correlations over the whole graph-based network
structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02521">Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gyawali_D/0/1/0/all/0/1">Dipesh Gyawali</a></p>
<p>Deep Learning(DL) and Machine Learning(ML) applications are rapidly
increasing in recent days. Massive amounts of data are being generated over the
internet which can derive meaningful results by the use of ML and DL
algorithms. Hardware resources and open-source libraries have made it easy to
implement these algorithms. Tensorflow and Pytorch are one of the leading
frameworks for implementing ML projects. By using those frameworks, we can
trace the operations executed on both GPU and CPU to analyze the resource
allocations and consumption. This paper presents the time and memory allocation
of CPU and GPU while training deep neural networks using Pytorch. This paper
analysis shows that GPU has a lower running time as compared to CPU for deep
neural networks. For a simpler network, there are not many significant
improvements in GPU over the CPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10313">Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuexiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Shengbang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1">Mu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1">Qing Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yong Jae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a></p>
<p>Following the success of GPT4, there has been a surge in interest in
multimodal large language model (MLLM) research. This line of research focuses
on developing general-purpose LLMs through fine-tuning pre-trained LLMs and
vision models. However, catastrophic forgetting, a notorious phenomenon where
the fine-tuned model fails to retain similar performance compared to the
pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).
In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the
catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.
We first apply EMT to evaluate several open-source fine-tuned MLLMs and we
discover that almost all evaluated MLLMs fail to retain the same performance
levels as their vision encoders on standard image classification tasks.
Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess
performance throughout the fine-tuning. Interestingly, our results suggest that
early-stage fine-tuning on an image dataset improves performance across other
image datasets, by enhancing the alignment of text and visual features.
However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in
a significant loss of generalizability, even when the image encoder remains
frozen. Our results suggest that MLLMs have yet to demonstrate performance on
par with their vision models on standard image classification tasks and the
current MLLM fine-tuning procedure still has room for improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12307">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yukang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengju Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haotian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1">Xin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhijian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Song Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jiaya Jia</a></p>
<p>We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shifted sparse attention (S$^2$-Attn) effectively enables context
extension, leading to non-trivial computation saving with similar performance
to fine-tuning with vanilla attention. Particularly, it can be implemented with
only two lines of code in training, while being optional in inference. On the
other hand, we revisit the parameter-efficient fine-tuning regime for context
expansion. Notably, we find that LoRA for context extension works well under
the premise of trainable embedding and normalization. LongLoRA combines this
improved LoRA with S$^2$-Attn. LongLoRA demonstrates strong empirical results
on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA adopts Llama2 7B
from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine.
LongLoRA extends models' context while retaining their original architectures,
and is compatible with most existing techniques, like Flash-Attention2. In
addition, we further conduct supervised fine-tuning with LongLoRA and our long
instruction-following LongAlpaca dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12334">Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vie_J/0/1/0/all/0/1">Jill-J&#xea;nn Vie</a> (SODA), <a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1">Hisashi Kashima</a></p>
<p>Knowledge tracing consists in predicting the performance of some students on
new questions given their performance on previous questions, and can be a prior
step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a
competitive model for knowledge tracing relying on recurrent neural networks,
even if some simpler models may match its performance. However, little is known
about why DKT works so well. In this paper, we frame deep knowledge tracing as
a encoderdecoder architecture. This viewpoint not only allows us to propose
better models in terms of performance, simplicity or expressivity but also
opens up promising avenues for future research directions. In particular, we
show on several small and large datasets that a simpler decoder, with possibly
fewer parameters than the one used by DKT, can predict student performance
better.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14235">Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving. (arXiv:2309.14235v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1">Haoyi Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qimao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jianming Hu</a></p>
<p>The deployment of autonomous vehicles (AVs) has faced hurdles due to the
dominance of rare but critical corner cases within the long-tail distribution
of driving scenarios, which negatively affects their overall performance. To
address this challenge, adversarial generation methods have emerged as a class
of efficient approaches to synthesize safety-critical scenarios for AV testing.
However, these generated scenarios are often underutilized for AV training,
resulting in the potential for continual AV policy improvement remaining
untapped, along with a deficiency in the closed-loop design needed to achieve
it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately
characterize the hierarchical nature of vehicle interaction dynamics,
facilitating iterative improvement by engaging background vehicles (BVs) and AV
in a sequential game-like interaction paradigm. With AV acting as the leader
and BVs as followers, this leader-follower modeling ensures that AV would
consistently refine its policy, always taking into account the additional
information that BVs play the best response to challenge AV. Extensive
experiments have shown that our algorithm exhibits superior performance
compared to several baselines especially in higher dimensional scenarios,
leading to substantial advancements in AV capabilities while continually
generating progressively challenging scenarios. Code is available at
https://github.com/BlueCat-de/SDM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02903">FroSSL: Frobenius Norm Minimization for Self-Supervised Learning. (arXiv:2310.02903v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Skean_O/0/1/0/all/0/1">Oscar Skean</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhakal_A/0/1/0/all/0/1">Aayush Dhakal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1">Nathan Jacobs</a>, <a href="http://arxiv.org/find/cs/1/au:+Giraldo_L/0/1/0/all/0/1">Luis Gonzalo Sanchez Giraldo</a></p>
<p>Self-supervised learning (SSL) is an increasingly popular paradigm for
representation learning. Recent methods can be classified as
sample-contrastive, dimension-contrastive, or asymmetric network-based, with
each family having its own approach to avoiding informational collapse. While
dimension-contrastive methods converge to similar solutions as
sample-contrastive methods, it can be empirically shown that some methods
require more epochs of training to converge. Motivated by closing this divide,
we present the objective function FroSSL which is both sample- and
dimension-contrastive up to embedding normalization. FroSSL works by minimizing
covariance Frobenius norms for avoiding collapse and minimizing mean-squared
error for augmentation invariance. We show that FroSSL converges more quickly
than a variety of other SSL methods and provide theoretical and empirical
support that this faster convergence is due to how FroSSL affects the
eigenvalues of the embedding covariance matrices. We also show that FroSSL
learns competitive representations on linear probe evaluation when used to
train a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03152">Towards out-of-distribution generalizable predictions of chemical kinetics properties. (arXiv:2310.03152v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yongqiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">James Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1">Hanghang Tong</a></p>
<p>Machine Learning (ML) techniques have found applications in estimating
chemical kinetic properties. With the accumulated drug molecules identified
through "AI4drug discovery", the next imperative lies in AI-driven design for
high-throughput chemical synthesis processes, with the estimation of properties
of unseen reactions with unexplored molecules. To this end, the existing ML
approaches for kinetics property prediction are required to be
Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD
kinetic property prediction into three levels (structure, condition, and
mechanism), revealing unique aspects of such problems. Under this framework, we
create comprehensive datasets to benchmark (1) the state-of-the-art ML
approaches for reaction prediction in the OOD setting and (2) the
state-of-the-art graph OOD methods in kinetics property prediction problems.
Our results demonstrated the challenges and opportunities in OOD kinetics
property prediction. Our datasets and benchmarks can further support research
in this direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04406">Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Andy Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlapentokh_Rothman_M/0/1/0/all/0/1">Michal Shlapentokh-Rothman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Xiong Wang</a></p>
<p>While large language models (LLMs) have demonstrated impressive performance
on a range of decision-making tasks, they rely on simple acting processes and
fall short of broad deployment as autonomous agents. We introduce LATS
(Language Agent Tree Search), a general framework that synergizes the
capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration
from Monte Carlo tree search in model-based reinforcement learning, LATS
employs LLMs as agents, value functions, and optimizers, repurposing their
latent strengths for enhanced decision-making. What is crucial in this method
is the use of an environment for external feedback, which offers a more
deliberate and adaptive problem-solving mechanism that moves beyond the
limitations of existing techniques. Our experimental evaluation across diverse
domains, such as programming, HotPotQA, and WebShop, illustrates the
applicability of LATS for both reasoning and acting. In particular, LATS
achieves 94.4% for programming on HumanEval with GPT-4 and an average score of
75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness
and generality of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05063">Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain. (arXiv:2310.05063v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woo_G/0/1/0/all/0/1">Gerald Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Akshat Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1">Doyen Sahoo</a></p>
<p>Time series has been left behind in the era of pre-training and transfer
learning. While research in the fields of natural language processing and
computer vision are enjoying progressively larger datasets to train massive
models, the most popular time series datasets consist of only tens of thousands
of time steps, limiting our ability to study the effectiveness of pre-training
and scaling. Recent studies have also cast doubt on the need for expressive
models and scale. To alleviate these issues, we introduce three large-scale
time series forecasting datasets from the cloud operations (CloudOps) domain,
the largest having billions of observations, enabling further study into
pre-training and scaling of time series models. We build the empirical
groundwork for studying pre-training and scaling of time series models and pave
the way for future research by identifying a promising candidate architecture.
We show that it is a strong zero-shot baseline and benefits from further
scaling, both in model and dataset size. Accompanying these datasets and
results is a suite of comprehensive benchmark results comparing classical and
deep learning baselines to our pre-trained method - achieving a 27% reduction
in error on the largest dataset. Code and datasets can be found
https://github.com/SalesforceAIResearch/pretrain-time-series-cloudops.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06609">Discovering Interpretable Physical Models using Symbolic Regression and Discrete Exterior Calculus. (arXiv:2310.06609v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manti_S/0/1/0/all/0/1">Simone Manti</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucantonio_A/0/1/0/all/0/1">Alessandro Lucantonio</a></p>
<p>Computational modeling is a key resource to gather insight into physical
systems in modern scientific research and engineering. While access to large
amount of data has fueled the use of Machine Learning (ML) to recover physical
models from experiments and increase the accuracy of physical simulations,
purely data-driven models have limited generalization and interpretability. To
overcome these limitations, we propose a framework that combines Symbolic
Regression (SR) and Discrete Exterior Calculus (DEC) for the automated
discovery of physical models starting from experimental data. Since these
models consist of mathematical expressions, they are interpretable and amenable
to analysis, and the use of a natural, general-purpose discrete mathematical
language for physics favors generalization with limited input data.
Importantly, DEC provides building blocks for the discrete analogue of field
theories, which are beyond the state-of-the-art applications of SR to physical
problems. Further, we show that DEC allows to implement a strongly-typed SR
procedure that guarantees the mathematical consistency of the recovered models
and reduces the search space of symbolic expressions. Finally, we prove the
effectiveness of our methodology by re-discovering three models of Continuum
Physics from synthetic experimental data: Poisson equation, the Euler's
Elastica and the equations of Linear Elasticity. Thanks to their
general-purpose nature, the methods developed in this paper may be applied to
diverse contexts of physical modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09485">Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction. (arXiv:2310.09485v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pal_J/0/1/0/all/0/1">Jai Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1">Bryan Hong</a></p>
<p>Artificial intelligence (AI) is a powerful tool for reshaping healthcare
systems. In healthcare, AI is invaluable for its capacity to manage vast
amounts of data, which can lead to more accurate and speedy diagnoses,
ultimately easing the workload on healthcare professionals. As a result, AI has
proven itself to be a power tool across various industries, simplifying complex
tasks and pattern recognition that would otherwise be overwhelming for humans
or traditional computer algorithms. In this paper, we review the strengths and
weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring
cutting edge virus analysis to healthcare professionals around the world. The
model's accuracy assessment revealed promising results, with room for
improvement primarily related to data organization. In addition, the severity
index serves as a valuable tool to gain a broad overview of patient care needs,
aligning with healthcare professionals' preference for broader categorizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11479">On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1">Seohyeon Cha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Honggu Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Joonhyuk Kang</a></p>
<p>Accurate uncertainty quantification in graph neural networks (GNNs) is
essential, especially in high-stakes domains where GNNs are frequently
employed. Conformal prediction (CP) offers a promising framework for
quantifying uncertainty by providing $\textit{valid}$ prediction sets for any
black-box model. CP ensures formal probabilistic guarantees that a prediction
set contains a true label with a desired probability. However, the size of
prediction sets, known as $\textit{inefficiency}$, is influenced by the
underlying model and data generating process. On the other hand, Bayesian
learning also provides a credible region based on the estimated posterior
distribution, but this region is $\textit{well-calibrated}$ only when the model
is correctly specified. Building on a recent work that introduced a scaling
parameter for constructing valid credible regions from posterior estimate, our
study explores the advantages of incorporating a temperature parameter into
Bayesian GNNs within CP framework. We empirically demonstrate the existence of
temperatures that result in more efficient prediction sets. Furthermore, we
conduct an analysis to identify the factors contributing to inefficiency and
offer valuable insights into the relationship between CP performance and model
calibration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13572">Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space. (arXiv:2310.13572v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yufei Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Aste_T/0/1/0/all/0/1">Tomaso Aste</a></p>
<p>Double descent presents a counter-intuitive aspect within the machine
learning domain, and researchers have observed its manifestation in various
models and tasks. While some theoretical explanations have been proposed for
this phenomenon in specific contexts, an accepted theory to account for its
occurrence in deep learning remains yet to be established. In this study, we
revisit the phenomenon of double descent and demonstrate that its occurrence is
strongly influenced by the presence of noisy data. Through conducting a
comprehensive analysis of the feature space of learned representations, we
unveil that double descent arises in imperfect models trained with noisy data.
We argue that double descent is a consequence of the model first learning the
noisy data until interpolation and then adding implicit regularization via
over-parameterization acquiring therefore capability to separate the
information from the noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14768">Policy Gradient with Kernel Quadrature. (arXiv:2310.14768v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayakawa_S/0/1/0/all/0/1">Satoshi Hayakawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Morimura_T/0/1/0/all/0/1">Tetsuro Morimura</a></p>
<p>Reward evaluation of episodes becomes a bottleneck in a broad range of
reinforcement learning tasks. Our aim in this paper is to select a small but
representative subset of a large batch of episodes, only on which we actually
compute rewards for more efficient policy gradient iterations. We build a
Gaussian process modeling of discounted returns or rewards to derive a positive
definite kernel on the space of episodes, run an ``episodic" kernel quadrature
method to compress the information of sample episodes, and pass the reduced
episodes to the policy network for gradient updates. We present the theoretical
background of this procedure as well as its numerical illustrations in MuJoCo
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16546">Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_T/0/1/0/all/0/1">Taehyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Seungyub Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Heesoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungjae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungwoo Lee</a></p>
<p>Distributional reinforcement learning algorithms have attempted to utilize
estimated uncertainty for exploration, such as optimism in the face of
uncertainty. However, using the estimated variance for optimistic exploration
may cause biased data collection and hinder convergence or performance. In this
paper, we present a novel distributional reinforcement learning algorithm that
selects actions by randomizing risk criterion to avoid one-sided tendency on
risk. We provide a perturbed distributional Bellman optimality operator by
distorting the risk measure and prove the convergence and optimality of the
proposed method with the weaker contraction property. Our theoretical results
support that the proposed method does not fall into biased exploration and is
guaranteed to converge to an optimal return. Finally, we empirically show that
our method outperforms other existing distribution-based algorithms in various
environments including Atari 55 games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18428">The Bayesian Stability Zoo. (arXiv:2310.18428v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Schefler_H/0/1/0/all/0/1">Hilla Schefler</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1">Jonathan Shafer</a></p>
<p>We show that many definitions of stability found in the learning theory
literature are equivalent to one another. We distinguish between two families
of definitions of stability: distribution-dependent and
distribution-independent Bayesian stability. Within each family, we establish
equivalences between various definitions, encompassing approximate differential
privacy, pure differential privacy, replicability, global stability, perfect
generalization, TV stability, mutual information stability, KL-divergence
stability, and R\'enyi-divergence stability. Along the way, we prove boosting
results that enable the amplification of the stability of a learning rule. This
work is a step towards a more systematic taxonomy of stability notions in
learning theory, which can promote clarity and an improved understanding of an
array of stability concepts that have emerged in recent years.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18784">High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Armacki_A/0/1/0/all/0/1">Aleksandar Armacki</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pranay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1">Gauri Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajovic_D/0/1/0/all/0/1">Dragana Bajovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakovetic_D/0/1/0/all/0/1">Dusan Jakovetic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1">Soummya Kar</a></p>
<p>Several recent works have studied the convergence \textit{in high
probability} of stochastic gradient descent (SGD) and its clipped variant.
Compared to vanilla SGD, clipped SGD is practically more stable and has the
additional theoretical benefit of logarithmic dependence on the failure
probability. However, the convergence of other practical nonlinear variants of
SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved
communication efficiency or accelerated convergence is much less understood. In
this work, we study the convergence bounds \textit{in high probability} of a
broad class of nonlinear SGD methods. For strongly convex loss functions with
Lipschitz continuous gradients, we prove a logarithmic dependence on the
failure probability, even when the noise is heavy-tailed. Strictly more general
than the results for clipped SGD, our results hold for any nonlinearity with
bounded (component-wise or joint) outputs, such as clipping, normalization, and
quantization. Further, existing results with heavy-tailed noise assume bounded
$\eta$-th central moments, with $\eta \in (1,2]$. In contrast, our refined
analysis works even for $\eta=1$, strictly relaxing the noise moment
assumptions in the literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20204">General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shim_C/0/1/0/all/0/1">Chaeeun Shim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bosco Seong Kyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Im_C/0/1/0/all/0/1">Chami Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Sung Yoon Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1">Han-Gil Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Edward Choi</a></p>
<p>Developing clinical prediction models (e.g., mortality prediction) based on
electronic health records (EHRs) typically relies on expert opinion for feature
selection and adjusting observation window size. This burdens experts and
creates a bottleneck in the development process. We propose Retrieval-Enhanced
Medical prediction model (REMed) to address such challenges. REMed can
essentially evaluate an unlimited number of clinical events, select the
relevant ones, and make predictions. This approach effectively eliminates the
need for manual feature selection and enables an unrestricted observation
window. We verified these properties through experiments on 27 clinical tasks
and two independent cohorts from publicly available EHR datasets, where REMed
outperformed other contemporary architectures that aim to handle as many events
as possible. Notably, we found that the preferences of REMed align closely with
those of medical experts. We expect our approach to significantly expedite the
development of EHR prediction models by minimizing clinicians' need for manual
involvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00613">Controllable Music Production with Diffusion Models and Guidance Gradients. (arXiv:2311.00613v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1">Mark Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Giorgi_B/0/1/0/all/0/1">Bruno Di Giorgi</a>, <a href="http://arxiv.org/find/cs/1/au:+Weers_F/0/1/0/all/0/1">Floris Weers</a>, <a href="http://arxiv.org/find/cs/1/au:+Katharopoulos_A/0/1/0/all/0/1">Angelos Katharopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Nickson_T/0/1/0/all/0/1">Tom Nickson</a></p>
<p>We demonstrate how conditional generation from diffusion models can be used
to tackle a variety of realistic tasks in the production of music in 44.1kHz
stereo audio with sampling-time guidance. The scenarios we consider include
continuation, inpainting and regeneration of musical audio, the creation of
smooth transitions between two different music tracks, and the transfer of
desired stylistic characteristics to existing audio clips. We achieve this by
applying guidance at sampling time in a simple framework that supports both
reconstruction and classification losses, or any combination of the two. This
approach ensures that generated audio can match its surrounding context, or
conform to a class distribution or latent representation specified relative to
any suitable pre-trained classifier or embedding model. Audio samples are
available at https://machinelearning.apple.com/research/controllable-music
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02495">Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks. (arXiv:2311.02495v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Longze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jiang Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1">Aleksandar Vakanski</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yachun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1">Tiankai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_M/0/1/0/all/0/1">Min Xian</a></p>
<p>With the increased use of data-driven approaches and machine learning-based
methods in material science, the importance of reliable uncertainty
quantification (UQ) of the predicted variables for informed decision-making
cannot be overstated. UQ in material property prediction poses unique
challenges, including the multi-scale and multi-physics nature of advanced
materials, intricate interactions between numerous factors, limited
availability of large curated datasets for model training, etc. Recently,
Bayesian Neural Networks (BNNs) have emerged as a promising approach for UQ,
offering a probabilistic framework for capturing uncertainties within neural
networks. In this work, we introduce an approach for UQ within physics-informed
BNNs, which integrates knowledge from governing laws in material modeling to
guide the models toward physically consistent predictions. To evaluate the
effectiveness of this approach, we present case studies for predicting the
creep rupture life of steel alloys. Experimental validation with three datasets
of collected measurements from creep tests demonstrates the ability of BNNs to
produce accurate point and uncertainty estimates that are competitive or exceed
the performance of the conventional method of Gaussian Process Regression.
Similarly, we evaluated the suitability of BNNs for UQ in an active learning
application and reported competitive performance. The most promising framework
for creep life prediction is BNNs based on Markov Chain Monte Carlo
approximation of the posterior distribution of network parameters, as it
provided more reliable results in comparison to BNNs based on variational
inference approximation or related NNs with probabilistic outputs. The codes
are available at:
https://github.com/avakanski/Creep-uncertainty-quantification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03191">DeepInception: Hypnotize Large Language Model to Be Jailbreaker. (arXiv:2311.03191v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanke Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jianing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiangchao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a></p>
<p>Despite remarkable success in various applications, large language models
(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails
void. However, previous studies for jailbreaks usually resort to brute-force
optimization or extrapolations of a high computation cost, which might not be
practical or effective. In this paper, inspired by the Milgram experiment that
individuals can harm another person if they are told to do so by an
authoritative figure, we disclose a lightweight method, termed as
DeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock
its misusing risks. Specifically, DeepInception leverages the personification
ability of LLM to construct a novel nested scene to behave, which realizes an
adaptive way to escape the usage control in a normal scenario and provides the
possibility for further direct jailbreaks. Empirically, we conduct
comprehensive experiments to show its efficacy. Our DeepInception can achieve
competitive jailbreak success rates with previous counterparts and realize a
continuous jailbreak in subsequent interactions, which reveals the critical
weakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna,
Llama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay
more attention to the safety aspects of LLMs and a stronger defense against
their misuse risks. The code is publicly available at:
https://github.com/tmlr-group/DeepInception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03236">Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources. (arXiv:2311.03236v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Haotian Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1">Xiaobo Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a></p>
<p>Out-of-distribution (OOD) detection discerns OOD data where the predictor
cannot make valid predictions as in-distribution (ID) data, thereby increasing
the reliability of open-world classification. However, it is typically hard to
collect real out-of-distribution (OOD) data for training a predictor capable of
discerning ID and OOD patterns. This obstacle gives rise to data
generation-based learning methods, synthesizing OOD data via data generators
for predictor training without requiring any real OOD data. Related methods
typically pre-train a generator on ID data and adopt various selection
procedures to find those data likely to be the OOD cases. However, generated
data may still coincide with ID semantics, i.e., mistaken OOD generation
remains, confusing the predictor between ID and OOD data. To this end, we
suggest that generated data (with mistaken OOD generation) can be used to
devise an auxiliary OOD detection task to facilitate real OOD detection.
Specifically, we can ensure that learning from such an auxiliary task is
beneficial if the ID and the OOD parts have disjoint supports, with the help of
a well-designed training procedure for the predictor. Accordingly, we propose a
powerful data generation-based learning method named Auxiliary Task-based OOD
Learning (ATOL) that can relieve the mistaken OOD generation. We conduct
extensive experiments under various OOD detection setups, demonstrating the
effectiveness of our method against its advanced counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05440">A Practical Approach to Novel Class Discovery in Tabular Data. (arXiv:2311.05440v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Troisemaine_C/0/1/0/all/0/1">Colin Troisemaine</a>, <a href="http://arxiv.org/find/cs/1/au:+Reiffers_Masson_A/0/1/0/all/0/1">Alexandre Reiffers-Masson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gosselin_S/0/1/0/all/0/1">St&#xe9;phane Gosselin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1">Vincent Lemaire</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaton_S/0/1/0/all/0/1">Sandrine Vaton</a></p>
<p>The problem of Novel Class Discovery (NCD) consists in extracting knowledge
from a labeled set of known classes to accurately partition an unlabeled set of
novel classes. While NCD has recently received a lot of attention from the
community, it is often solved on computer vision problems and under unrealistic
conditions. In particular, the number of novel classes is usually assumed to be
known in advance, and their labels are sometimes used to tune hyperparameters.
Methods that rely on these assumptions are not applicable in real-world
scenarios. In this work, we focus on solving NCD in tabular data when no prior
knowledge of the novel classes is available. To this end, we propose to tune
the hyperparameters of NCD methods by adapting the $k$-fold cross-validation
process and hiding some of the known classes in each fold. Since we have found
that methods with too many hyperparameters are likely to overfit these hidden
classes, we define a simple deep NCD model. This method is composed of only the
essential elements necessary for the NCD problem and performs impressively well
under realistic conditions. Furthermore, we find that the latent space of this
method can be used to reliably estimate the number of novel classes.
Additionally, we adapt two unsupervised clustering algorithms ($k$-means and
Spectral Clustering) to leverage the knowledge of the known classes. Extensive
experiments are conducted on 7 tabular datasets and demonstrate the
effectiveness of the proposed method and hyperparameter tuning process, and
show that the NCD problem can be solved without relying on knowledge from the
novel classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11905">Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning. (arXiv:2311.11905v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dantas_J/0/1/0/all/0/1">Joao P. A. Dantas</a>, <a href="http://arxiv.org/find/cs/1/au:+Geraldo_D/0/1/0/all/0/1">Diego Geraldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Medeiros_F/0/1/0/all/0/1">Felipe L. L. Medeiros</a>, <a href="http://arxiv.org/find/cs/1/au:+Maximo_M/0/1/0/all/0/1">Marcos R. O. A. Maximo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoneyama_T/0/1/0/all/0/1">Takashi Yoneyama</a></p>
<p>Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. A
critical aspect of their effectiveness is the Engagement Zone (EZ), the spatial
region within which a SAM can effectively engage and neutralize a target.
Notably, the EZ is intrinsically related to the missile's maximum range; it
defines the furthest distance at which a missile can intercept a target. The
accurate computation of this EZ is essential but challenging due to the dynamic
and complex factors involved, which often lead to high computational costs and
extended processing times when using conventional simulation methods. In light
of these challenges, our study investigates the potential of machine learning
techniques, proposing an approach that integrates machine learning with a
custom-designed simulation tool to train supervised algorithms. We leverage a
comprehensive dataset of pre-computed SAM EZ simulations, enabling our model to
accurately predict the SAM EZ for new input parameters. It accelerates SAM EZ
simulations, enhances air defense strategic planning, and provides real-time
insights, improving SAM system performance. The study also includes a
comparative analysis of machine learning algorithms, illuminating their
capabilities and performance metrics and suggesting areas for future research,
highlighting the transformative potential of machine learning in SAM EZ
simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15218">Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis. (arXiv:2311.15218v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bathini_S/0/1/0/all/0/1">Sai Akash Bathini</a>, <a href="http://arxiv.org/find/cs/1/au:+Cihan_D/0/1/0/all/0/1">Dagli Cihan</a></p>
<p>The application of Machine learning to finance has become a familiar
approach, even more so in stock market forecasting. The stock market is highly
volatile and huge amounts of data are generated every minute globally. The
extraction of effective intelligence from this data is of critical importance.
However, a collaboration of numerical stock data with qualitative text data can
be a challenging task. In this work, we accomplish this and provide an
unprecedented, publicly available dataset with technical and fundamental data,
sentiment that we gathered from News Archives, TV news captions, Radio
Transcripts, Tweets, Daily financial newspapers, etc. The text data entries
used for sentiment extraction total more than 1.4 Million. The dataset consists
of daily entries from January 2018 to December 2022 for 8 companies
representing diverse industrial sectors and the Dow Jones Industrial Average
(DJIA) as a whole. Holistic Fundamental and Technical data is provided training
ready for Model learning and deployment. The data generated could be used for
Incremental online learning with real-time data points retrieved daily, since
there was no stagnant data utilized, all the data was retired from APIs or
self-designed scripts. Moreover, the utilization of Spearman's rank correlation
over real-time data, linking stock returns with sentiment analysis has produced
noteworthy results for the DJIA achieving accuracy levels surpassing 60\%. The
dataset is made available at https://github.com/batking24/Huge-Stock-Dataset
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17932">Generating Molecular Conformer Fields. (arXiv:2311.17932v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Elhag_A/0/1/0/all/0/1">Ahmed A. Elhag</a>, <a href="http://arxiv.org/find/physics/1/au:+Jaitly_N/0/1/0/all/0/1">Navdeep Jaitly</a>, <a href="http://arxiv.org/find/physics/1/au:+Susskind_J/0/1/0/all/0/1">Joshua M. Susskind</a>, <a href="http://arxiv.org/find/physics/1/au:+Bautista_M/0/1/0/all/0/1">Miguel Angel Bautista</a></p>
<p>In this paper we tackle the problem of generating conformers of a molecule in
3D space given its molecular graph. We parameterize these conformers as
continuous functions that map elements from the molecular graph to points in 3D
space. We then formulate the problem of learning to generate conformers as
learning a distribution over these functions using a diffusion generative
model, called Molecular Conformer Fields (MCF). Our approach is simple and
scalable, and achieves state-of-the-art performance on challenging molecular
conformer generation benchmarks while making no assumptions about the explicit
structure of molecules (e.g. modeling torsional angles). MCF represents an
advance in extending diffusion models to handle complex scientific problems in
a conceptually simple, scalable and effective manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18743">AlignBench: Benchmarking Chinese Alignment of Large Language Models. (arXiv:2311.18743v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xuanyu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhuoer Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bosi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jiale Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1">Pei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1">Weng Lam Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a></p>
<p>Alignment has become a critical step for instruction-tuned Large Language
Models (LLMs) to become helpful assistants. However, effective evaluation of
alignment for emerging Chinese LLMs is still significantly lacking, calling for
real-scenario grounded, open-ended, challenging and automatic evaluations
tailored for alignment. To fill in this gap, we introduce AlignBench, a
comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in
Chinese. Equipped with a human-in-the-loop data curation pipeline, our
benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with
Chain-of-Thought to generate explanations and final ratings as evaluations,
ensuring high reliability and interpretability. Furthermore, we report
AlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that
recovers 95% of GPT-4's evaluation ability. We will provide public APIs for
evaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'
Chinese alignment. All evaluation codes, data, and LLM generations are
available at \url{https://github.com/THUDM/AlignBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18826">Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference. (arXiv:2311.18826v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_K/0/1/0/all/0/1">Kaiwen Hou</a></p>
<p>This manuscript enriches the framework of continuous normalizing flows (CNFs)
within causal inference, primarily to augment the geometric properties of
parametric submodels used in targeted maximum likelihood estimation (TMLE). By
introducing an innovative application of CNFs, we construct a refined series of
parametric submodels that enable a directed interpolation between the prior
distribution $p_0$ and the empirical distribution $p_1$. This proposed
methodology serves to optimize the semiparametric efficiency bound in causal
inference by orchestrating CNFs to align with Wasserstein gradient flows. Our
approach not only endeavors to minimize the mean squared error in the
estimation but also imbues the estimators with geometric sophistication,
thereby enhancing robustness against misspecification. This robustness is
crucial, as it alleviates the dependence on the standard $n^{\frac{1}{4}}$ rate
for a doubly-robust perturbation direction in TMLE. By incorporating robust
optimization principles and differential geometry into the estimators, the
developed geometry-aware CNFs represent a significant advancement in the
pursuit of doubly robust causal inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00839">PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent Weight Prediction. (arXiv:2312.00839v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1">Lei Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiye Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xicheng Lu</a></p>
<p>Asynchronous pipeline model parallelism with a "1F1B" (one forward, one
backward) schedule generates little bubble overhead and always provides quite a
high throughput. However, the "1F1B" schedule inevitably leads to weight
inconsistency and weight staleness issues due to the cross-training of
different mini-batches across GPUs. To simultaneously address these two
problems, in this paper, we propose an optimizer-dependent weight prediction
strategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight
of our proposal is that we employ a weight prediction strategy in the forward
pass to ensure that each mini-batch uses consistent and staleness-free weights
to compute the forward pass. To be concrete, we first construct the weight
prediction scheme based on the update rule of the used optimizer when training
the deep neural network models. Then throughout the "1F1B" pipelined training,
each mini-batch is mandated to execute weight prediction ahead of the forward
pass, subsequently employing the predicted weights to perform the forward pass.
As a result, PipeOptim 1) inherits the advantage of the "1F1B" schedule and
generates pretty high throughput, and 2) can ensure effective parameter
learning regardless of the type of the used optimizer. To verify the
effectiveness of our proposal, we conducted extensive experimental evaluations
using eight different deep-learning models spanning three machine-learning
tasks including image classification, sentiment analysis, and machine
translation. The experiment results demonstrate that PipeOptim outperforms the
popular pipelined approaches including GPipe, PipeDream, PipeDream-2BW, and
SpecTrain. The code of PipeOptim can be accessible at
https://github.com/guanleics/PipeOptim.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00886">Nash Learning from Human Feedback. (arXiv:2312.00886v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/stat/1/au:+Azar_M/0/1/0/all/0/1">Mohammad Gheshlaghi Azar</a>, <a href="http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/stat/1/au:+Guo_Z/0/1/0/all/0/1">Zhaohan Daniel Guo</a>, <a href="http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1">Yunhao Tang</a>, <a href="http://arxiv.org/find/stat/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1">Thomas Mesnard</a>, <a href="http://arxiv.org/find/stat/1/au:+Michi_A/0/1/0/all/0/1">Andrea Michi</a>, <a href="http://arxiv.org/find/stat/1/au:+Selvi_M/0/1/0/all/0/1">Marco Selvi</a>, <a href="http://arxiv.org/find/stat/1/au:+Girgin_S/0/1/0/all/0/1">Sertan Girgin</a>, <a href="http://arxiv.org/find/stat/1/au:+Momchev_N/0/1/0/all/0/1">Nikola Momchev</a>, <a href="http://arxiv.org/find/stat/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a>, <a href="http://arxiv.org/find/stat/1/au:+Mankowitz_D/0/1/0/all/0/1">Daniel J. Mankowitz</a>, <a href="http://arxiv.org/find/stat/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>, <a href="http://arxiv.org/find/stat/1/au:+Piot_B/0/1/0/all/0/1">Bilal Piot</a></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
</p>
<p>In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
</p>
<p>In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01203">Harnessing Discrete Representations For Continual Reinforcement Learning. (arXiv:2312.01203v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meyer_E/0/1/0/all/0/1">Edan Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1">Adam White</a>, <a href="http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1">Marlos C. Machado</a></p>
<p>Reinforcement learning (RL) agents make decisions using nothing but
observations from the environment, and consequently, heavily rely on the
representations of those observations. Though some recent breakthroughs have
used vector-based categorical representations of observations, often referred
to as discrete representations, there is little work explicitly assessing the
significance of such a choice. In this work, we provide a thorough empirical
investigation of the advantages of representing observations as vectors of
categorical values within the context of reinforcement learning. We perform
evaluations on world-model learning, model-free RL, and ultimately continual RL
problems, where the benefits best align with the needs of the problem setting.
We find that, when compared to traditional continuous representations, world
models learned over discrete representations accurately model more of the world
with less capacity, and that agents trained with discrete representations learn
better policies with less data. In the context of continual RL, these benefits
translate into faster adapting agents. Additionally, our analysis suggests that
the observed performance improvements can be attributed to the information
contained within the latent vectors and potentially the encoding of the
discrete representation itself.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01239">Motion Informed Needle Segmentation in Ultrasound Images. (arXiv:2312.01239v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Goel_R/0/1/0/all/0/1">Raghavv Goel</a>, <a href="http://arxiv.org/find/eess/1/au:+Morales_C/0/1/0/all/0/1">Cecilia Morales</a>, <a href="http://arxiv.org/find/eess/1/au:+Singh_M/0/1/0/all/0/1">Manpreet Singh</a>, <a href="http://arxiv.org/find/eess/1/au:+Dubrawski_A/0/1/0/all/0/1">Artur Dubrawski</a>, <a href="http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1">John Galeotti</a>, <a href="http://arxiv.org/find/eess/1/au:+Choset_H/0/1/0/all/0/1">Howie Choset</a></p>
<p>Segmenting a moving needle in ultrasound images is challenging due to the
presence of artifacts, noise, and needle occlusion. This task becomes even more
demanding in scenarios where data availability is limited. Convolutional Neural
Networks (CNNs) have been successful in many computer vision applications, but
struggle to accurately segment needles without considering their motion. In
this paper, we present a novel approach for needle segmentation that combines
classical Kalman Filter (KF) techniques with data-driven learning,
incorporating both needle features and needle motion. Our method offers two key
contributions. First, we propose a compatible framework that seamlessly
integrates into commonly used encoder-decoder style architectures. Second, we
demonstrate superior performance compared to recent state-of-the-art needle
segmentation models using our novel convolutional neural network (CNN) based
KF-inspired block, achieving a 15\% reduction in pixel-wise needle tip error
and an 8\% reduction in length error. Third, to our knowledge we are the first
to implement a learnable filter to incorporate non-linear needle motion for
improving needle segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01529">T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training. (arXiv:2312.01529v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1">Cheng Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yinda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Quilodran_Casas_C/0/1/0/all/0/1">Cesar C&#xe9;sar Quilodr&#xe1;n-Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Anand Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1">Wenjia Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1">Rossella Arcucci</a></p>
<p>Expert annotation of 3D medical image for downstream analysis is
resource-intensive, posing challenges in clinical applications. Visual
self-supervised learning (vSSL), though effective for learning visual
invariance, neglects the incorporation of domain knowledge from medicine. To
incorporate medical knowledge into visual representation learning,
vision-language pre-training (VLP) has shown promising results in 2D image.
However, existing VLP approaches become generally impractical when applied to
high-resolution 3D medical images due to GPU hardware constraints and the
potential loss of critical details caused by downsampling, which is the
intuitive solution to hardware constraints. To address the above limitations,
we introduce T3D, the first VLP framework designed for high-resolution 3D
medical images. T3D incorporates two text-informed pretext tasks:
(\lowerromannumeral{1}) text-informed contrastive learning;
(\lowerromannumeral{2}) text-informed image restoration. These tasks focus on
learning 3D visual representations from high-resolution 3D medical images and
integrating clinical knowledge from radiology reports, without distorting
information through forced alignment of downsampled volumes with detailed
anatomical text. Trained on a newly curated large-scale dataset of 3D medical
images and radiology reports, T3D significantly outperforms current vSSL
methods in tasks like organ and tumor segmentation, as well as disease
classification. This underlines T3D's potential in representation learning for
3D medical image analysis. All data and code will be available upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01606">Deep Learning-Driven Enhancement of Welding Quality Control: Predicting Welding Depth and Pore Volume in Hairpin Welding. (arXiv:2312.01606v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Darwish_A/0/1/0/all/0/1">Amena Darwish</a>, <a href="http://arxiv.org/find/cs/1/au:+Ericson_S/0/1/0/all/0/1">Stefan Ericson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghasemi_R/0/1/0/all/0/1">Rohollah Ghasemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Andersson_T/0/1/0/all/0/1">Tobias Andersson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lonn_D/0/1/0/all/0/1">Dan L&#xf6;nn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lassila_A/0/1/0/all/0/1">Andreas Andersson Lassila</a>, <a href="http://arxiv.org/find/cs/1/au:+Salomonsson_K/0/1/0/all/0/1">Kent Salomonsson</a></p>
<p>To advance quality assurance in the welding process, this study presents a
robust deep learning model that enables the prediction of two critical welds
Key Performance Characteristics (KPCs): welding depth and average pore volume.
In the proposed approach, a comprehensive range of laser welding Key Input
Characteristics (KICs) is utilized, including welding beam geometries, welding
feed rates, path repetitions for weld beam geometries, and bright light weld
ratios for all paths, all of which were obtained from hairpin welding
experiments. Two deep learning networks are employed with multiple hidden dense
layers and linear activation functions to showcase the capabilities of deep
neural networks in capturing the intricate nonlinear connections inherent
within welding KPCs and KICs. Applying deep learning networks to the small
numerical experimental hairpin welding dataset has shown promising results,
achieving Mean Absolute Error (MAE) values as low as 0.1079 for predicting
welding depth and 0.0641 for average pore volume. Additionally, the validity
verification demonstrates the reliability of the proposed method. This, in
turn, promises significant advantages in controlling welding outcomes, moving
beyond the current trend of relying merely on monitoring for defect
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01678">Jellyfish: A Large Language Model for Data Preprocessing. (arXiv:2312.01678v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haochen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuyang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyamada_M/0/1/0/all/0/1">Masafumi Oyamada</a></p>
<p>In this paper, we present Jellyfish, an open-source LLM as a universal task
solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned
with the datasets of several typical DP tasks including error detection, data
imputation, schema matching, and entity matching, and delivers generalizability
to other tasks. Remarkably, Jellyfish can operate on a local, single, and
low-priced GPU with its 13 billion parameters, ensuring data security and
enabling further tuning. Its proficiency in understanding natural language
allows users to manually craft instructions for DP tasks. Unlike many existing
methods that heavily rely on prior knowledge, Jellyfish acquires domain
knowledge during its tuning process and integrates optional knowledge injection
during inference. A distinctive feature of Jellyfish is its interpreter, which
elucidates its output decisions. To construct Jellyfish, we develop a series of
pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance
serializer, which automatically translates raw data into model prompts, and a
knowledge injector, which optionally introduces task- and dataset-specific
knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range
of real datasets, shows its competitiveness compared to state-of-the-art
methods and its strong generalizability to unseen tasks. Jellyfish's
performance rivals that of GPT series models, and its interpreter offers
enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our
evaluation highlights the effectiveness of the techniques employed in
constructing Jellyfish. Our model is available at Hugging Face:
https://huggingface.co/NECOUDBFM/Jellyfish .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02111">TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology. (arXiv:2312.02111v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farndale_L/0/1/0/all/0/1">Lucas Farndale</a>, <a href="http://arxiv.org/find/cs/1/au:+Insall_R/0/1/0/all/0/1">Robert Insall</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Ke Yuan</a></p>
<p>Computational pathology models rarely utilise data that will not be available
for inference. This means most models cannot learn from highly informative data
such as additional immunohistochemical (IHC) stains and spatial
transcriptomics. We present TriDeNT, a novel self-supervised method for
utilising privileged data that is not available during inference to improve
performance. We demonstrate the efficacy of this method for a range of
different paired data including immunohistochemistry, spatial transcriptomics
and expert nuclei annotations. In all settings, TriDeNT outperforms other
state-of-the-art methods in downstream tasks, with observed improvements of up
to 101%. Furthermore, we provide qualitative and quantitative measurements of
the features learned by these models and how they differ from baselines.
TriDeNT offers a novel method to distil knowledge from scarce or costly data
during training, to create significantly better models for routine inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.14115">Quantum Neural Architecture Search with Quantum Circuits Metric and Bayesian Optimization. (arXiv:2206.14115v1 [quant-ph] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Duong_T/0/1/0/all/0/1">Trong Duong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Truong_S/0/1/0/all/0/1">Sang T. Truong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tam_M/0/1/0/all/0/1">Minh Tam</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Bach_B/0/1/0/all/0/1">Bao Bach</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ryu_J/0/1/0/all/0/1">Ju-Young Ryu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Rhee_J/0/1/0/all/0/1">June-Koo Kevin Rhee</a></p>
<p>Quantum neural networks are promising for a wide range of applications in the
Noisy Intermediate-Scale Quantum era. As such, there is an increasing demand
for automatic quantum neural architecture search. We tackle this challenge by
designing a quantum circuits metric for Bayesian optimization with Gaussian
process. To this goal, we propose a new quantum gates distance that
characterizes the gates' action over every quantum state and provide a
theoretical perspective on its geometrical properties. Our approach
significantly outperforms the benchmark on three empirical quantum machine
learning problems including training a quantum generative adversarial network,
solving combinatorial optimization in the MaxCut problem, and simulating
quantum Fourier transform. Our method can be extended to characterize behaviors
of various quantum machine learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11665">Statistical exploration of the Manifold Hypothesis. (arXiv:2208.11665v3 [stat.ME] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Whiteley_N/0/1/0/all/0/1">Nick Whiteley</a>, <a href="http://arxiv.org/find/stat/1/au:+Gray_A/0/1/0/all/0/1">Annie Gray</a>, <a href="http://arxiv.org/find/stat/1/au:+Rubin_Delanchy_P/0/1/0/all/0/1">Patrick Rubin-Delanchy</a></p>
<p>The Manifold Hypothesis is a widely accepted tenet of Machine Learning which
asserts that nominally high-dimensional data are in fact concentrated near a
low-dimensional manifold, embedded in high-dimensional space. This phenomenon
is observed empirically in many real world situations, has led to development
of a wide range of statistical methods in the last few decades, and has been
suggested as a key factor in the success of modern AI technologies. We show
that rich and sometimes intricate manifold structure in data can emerge from a
generic and remarkably simple statistical model -- the Latent Metric Model --
via elementary concepts such as latent variables, correlation and stationarity.
This establishes a general statistical explanation for why the Manifold
Hypothesis seems to hold in so many situations. Informed by the Latent Metric
Model we derive procedures to discover and interpret the geometry of
high-dimensional data, and explore hypotheses about the data generating
mechanism. These procedures operate under minimal assumptions and make use of
well known, scaleable graph-analytic algorithms.
</p>
</p>
</div>

    </div>
    </body>
    