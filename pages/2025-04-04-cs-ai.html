<!DOCTYPE html>
<html>
<head>
<title>2025-04-04-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    .copy-button {
        background-color: #007BFF;
        color: #fff;
        border: none;
        padding: 5px 10px;
        cursor: pointer;
        margin-top: 10px;
    }
    .copy-button:active {
        background-color: #0056b3;
    }
    .copy-message {
        display: none;
        background-color: #4CAF50;
        color: #fff;
        padding: 5px;
        margin-top: 10px;
        border-radius: 3px;
    }
    </style>
    <script>
    function copyToClipboard(text, buttonId) {
        const el = document.createElement('textarea');
        el.value = text;
        document.body.appendChild(el);
        el.select();
        document.execCommand('copy');
        document.body.removeChild(el);
        
        const messageElement = document.getElementById('copy-message-' + buttonId);
        messageElement.innerHTML = 'Copied "' + text + '" to clipboard';
        messageElement.style.display = 'block';
        setTimeout(function() {
            messageElement.style.display = 'none';
        }, 3000);
    }
    </script>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.01990">Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems</a></h1>
<p><b>Authors:</b> Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, Chenglin Wu</p>
<p>Abstract: The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01990', 0)">Copy Link</button>
<div id="copy-message-0" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.01995">Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics</a></h1>
<p><b>Authors:</b> Hamed Mahdavi, Alireza Hashemi, Majid Daliri, Pegah Mohammadipour, Alireza Farhadi, Samira Malek, Yekta Yazdanifard, Amir Khasahmadi, Vasant Honavar</p>
<p>Abstract: Recent advancements in large language models (LLMs) have shown impressive progress in mathematical reasoning tasks. However, current evaluation benchmarks predominantly focus on the accuracy of final answers, often overlooking the logical rigor crucial for mathematical problem-solving. The claim that state-of-the-art LLMs can solve Math Olympiad-level problems requires closer examination. To explore this, we conducted both qualitative and quantitative human evaluations of proofs generated by LLMs, and developed a schema for automatically assessing their reasoning capabilities. Our study reveals that current LLMs fall significantly short of solving challenging Olympiad-level problems and frequently fail to distinguish correct mathematical reasoning from clearly flawed solutions. We also found that occasional correct final answers provided by LLMs often result from pattern recognition or heuristic shortcuts rather than genuine mathematical reasoning. These findings underscore the substantial gap between LLM performance and human expertise in advanced mathematical reasoning and highlight the importance of developing benchmarks that prioritize the rigor and coherence of mathematical arguments rather than merely the correctness of final answers.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01995', 1)">Copy Link</button>
<div id="copy-message-1" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02058">Epistemic Closure and the Irreversibility of Misalignment: Modeling Systemic Barriers to Alignment Innovation</a></h1>
<p><b>Authors:</b> Andy Williams</p>
<p>Abstract: Efforts to ensure the safe development of artificial general intelligence (AGI) often rely on consensus-based alignment approaches grounded in axiomatic formalism, interpretability, and empirical validation. However, these methods may be structurally unable to recognize or incorporate novel solutions that fall outside their accepted epistemic frameworks. This paper introduces a functional model of epistemic closure, in which cognitive, institutional, social, and infrastructural filters combine to make many alignment proposals illegible to existing evaluation systems. We present a weighted closure model supported by both theoretical and empirical sources, including a meta-analysis performed by an AI system on patterns of rejection and non-engagement with a framework for decentralized collective intelligence (DCI). We argue that the recursive failure to assess models like DCI is not just a sociological oversight but a structural attractor, mirroring the very risks of misalignment we aim to avoid in AGI. Without the adoption of DCI or a similarly recursive model of epistemic correction, we may be on a predictable path toward irreversible misalignment. The development and acceptance of this paper, first through simulated review and then through formal channels, provide a case study supporting its central claim: that epistemic closure can only be overcome by recursive modeling of the constraints that sustain it.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02058', 2)">Copy Link</button>
<div id="copy-message-2" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02111">Exploring LLM Reasoning Through Controlled Prompt Variations</a></h1>
<p><b>Authors:</b> Giannis Chatziveroglou, Richard Yun, Maura Kelleher</p>
<p>Abstract: This study investigates the reasoning robustness of large language models (LLMs) on mathematical problem-solving tasks under systematically introduced input perturbations. Using the GSM8K dataset as a controlled testbed, we evaluate how well state-of-the-art models maintain logical consistency and correctness when confronted with four categories of prompt perturbations: irrelevant context, pathological instructions, factually relevant but non-essential context, and a combination of the latter two. Our experiments, conducted on thirteen open-source and closed-source LLMs, reveal that introducing irrelevant context within the model's context window significantly degrades performance, suggesting that distinguishing essential from extraneous details remains a pressing challenge. Surprisingly, performance regressions are relatively insensitive to the complexity of the reasoning task, as measured by the number of steps required, and are not strictly correlated with model size. Moreover, we observe that certain perturbations inadvertently trigger chain-of-thought-like reasoning behaviors, even without explicit prompting. Our findings highlight critical vulnerabilities in current LLMs and underscore the need for improved robustness against noisy, misleading, and contextually dense inputs, paving the way for more resilient and reliable reasoning in real-world applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02111', 3)">Copy Link</button>
<div id="copy-message-3" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02148">OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling</a></h1>
<p><b>Authors:</b> Heming Zhang, Tim Xu, Dekang Cao, Shunning Liang, Lars Schimmelpfennig, Levi Kaster, Di Huang, Carlos Cruchaga, Guangfu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li</p>
<p>Abstract: Complex cell signaling systems -- governed by varying protein abundances and interactions -- generate diverse cell types across organs. These systems evolve under influences such as age, sex, diet, environmental exposures, and diseases, making them challenging to decode given the involvement of tens of thousands of genes and proteins. Recently, hundreds of millions of single-cell omics data have provided a robust foundation for understanding these signaling networks within various cell subpopulations and conditions. Inspired by the success of large foundation models (for example, large language models and large vision models) pre-trained on massive datasets, we introduce OmniCellTOSG, the first dataset of cell text-omic signaling graphs (TOSGs). Each TOSG represents the signaling network of an individual or meta-cell and is labeled with information such as organ, disease, sex, age, and cell subtype. OmniCellTOSG offers two key contributions. First, it introduces a novel graph model that integrates human-readable annotations -- such as biological functions, cellular locations, signaling pathways, related diseases, and drugs -- with quantitative gene and protein abundance data, enabling graph reasoning to decode cell signaling. This approach calls for new joint models combining large language models and graph neural networks. Second, the dataset is built from single-cell RNA sequencing data of approximately 120 million cells from diverse tissues and conditions (healthy and diseased) and is fully compatible with PyTorch. This facilitates the development of innovative cell signaling models that could transform research in life sciences, healthcare, and precision medicine. The OmniCellTOSG dataset is continuously expanding and will be updated regularly. The dataset and code are available at https://github.com/FuhaiLiAiLab/OmniCellTOSG.</p>
<p>URLs: <a href="https://github.com/FuhaiLiAiLab/OmniCellTOSG.">https://github.com/FuhaiLiAiLab/OmniCellTOSG.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02148, https://github.com/FuhaiLiAiLab/OmniCellTOSG.', 4)">Copy Link</button>
<div id="copy-message-4" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02181">A Survey of Scaling in Large Language Model Reasoning</a></h1>
<p><b>Authors:</b> Zihan Chen, Song Wang, Zhen Tan, Xingbo Fu, Zhenyu Lei, Peng Wang, Huan Liu, Cong Shen, Jundong Li</p>
<p>Abstract: The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02181', 5)">Copy Link</button>
<div id="copy-message-5" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02193">More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment</a></h1>
<p><b>Authors:</b> Yifan Wang, Runjin Chen, Bolian Li, David Cho, Yihe Deng, Ruqi Zhang, Tianlong Chen, Zhangyang Wang, Ananth Grama, Junyuan Hong</p>
<p>Abstract: Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02193', 6)">Copy Link</button>
<div id="copy-message-6" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02227">VEGAS: Towards Visually Explainable and Grounded Artificial Social Intelligence</a></h1>
<p><b>Authors:</b> Hao Li, Hao Fei, Zechao Hu, Zhengwei Yang, Zheng Wang</p>
<p>Abstract: Social Intelligence Queries (Social-IQ) serve as the primary multimodal benchmark for evaluating a model's social intelligence level. While impressive multiple-choice question(MCQ) accuracy is achieved by current solutions, increasing evidence shows that they are largely, and in some cases entirely, dependent on language modality, overlooking visual context. Additionally, the closed-set nature further prevents the exploration of whether and to what extent the reasoning path behind selection is correct. To address these limitations, we propose the Visually Explainable and Grounded Artificial Social Intelligence (VEGAS) model. As a generative multimodal model, VEGAS leverages open-ended answering to provide explainable responses, which enhances the clarity and evaluation of reasoning paths. To enable visually grounded answering, we propose a novel sampling strategy to provide the model with more relevant visual frames. We then enhance the model's interpretation of these frames through Generalist Instruction Fine-Tuning (GIFT), which aims to: i) learn multimodal-language transformations for fundamental emotional social traits, and ii) establish multimodal joint reasoning capabilities. Extensive experiments, comprising modality ablation, open-ended assessments, and supervised MCQ evaluations, consistently show that VEGAS effectively utilizes visual information in reasoning to produce correct and also credible answers. We expect this work to of fer a new perspective on Social-IQ and advance the development of human-like social AI.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02227', 7)">Copy Link</button>
<div id="copy-message-7" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02269">Engineering Artificial Intelligence: Framework, Challenges, and Future Direction</a></h1>
<p><b>Authors:</b> Jay Lee, Hanqi Su, Dai-Yan Ji, Takanobu Minami</p>
<p>Abstract: Over the past ten years, the application of artificial intelligence (AI) and machine learning (ML) in engineering domains has gained significant popularity, showcasing their potential in data-driven contexts. However, the complexity and diversity of engineering problems often require the development of domain-specific AI approaches, which are frequently hindered by a lack of systematic methodologies, scalability, and robustness during the development process. To address this gap, this paper introduces the "ABCDE" as the key elements of Engineering AI and proposes a unified, systematic engineering AI ecosystem framework, including eight essential layers, along with attributes, goals, and applications, to guide the development and deployment of AI solutions for specific engineering needs. Additionally, key challenges are examined, and nine future research directions are highlighted. By providing a comprehensive perspective, this paper aims to advance the strategic implementation of AI, fostering the development of next-generation engineering AI solutions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02269', 8)">Copy Link</button>
<div id="copy-message-8" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02426">Narrative Studio: Visual narrative exploration using LLMs and Monte Carlo Tree Search</a></h1>
<p><b>Authors:</b> Parsa Ghaffari, Chris Hokamp</p>
<p>Abstract: Interactive storytelling benefits from planning and exploring multiple 'what if' scenarios. Modern LLMs are useful tools for ideation and exploration, but current chat-based user interfaces restrict users to a single linear flow. To address this limitation, we propose Narrative Studio -- a novel in-browser narrative exploration environment featuring a tree-like interface that allows branching exploration from user-defined points in a story. Each branch is extended via iterative LLM inference guided by system and user-defined prompts. Additionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand promising narrative paths based on user-specified criteria, enabling more diverse and robust story development. We also allow users to enhance narrative coherence by grounding the generated text in an entity graph that represents the actors and environment of the story.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02426', 9)">Copy Link</button>
<div id="copy-message-9" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02430">How Artificial Intelligence Leads to Knowledge Why: An Inquiry Inspired by Aristotle&#x27;s Posterior Analytics</a></h1>
<p><b>Authors:</b> Guus Eelink, Kilian R\"uckschlo{\ss}, Felix Weitk\"amper</p>
<p>Abstract: Bayesian networks and causal models provide frameworks for handling queries about external interventions and counterfactuals, enabling tasks that go beyond what probability distributions alone can address. While these formalisms are often informally described as capturing causal knowledge, there is a lack of a formal theory characterizing the type of knowledge required to predict the effects of external interventions. This work introduces the theoretical framework of causal systems to clarify Aristotle's distinction between knowledge that and knowledge why within artificial intelligence. By interpreting existing artificial intelligence technologies as causal systems, it investigates the corresponding types of knowledge. Furthermore, it argues that predicting the effects of external interventions is feasible only with knowledge why, providing a more precise understanding of the knowledge necessary for such tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02430', 10)">Copy Link</button>
<div id="copy-message-10" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02467">BOOST: Bootstrapping Strategy-Driven Reasoning Programs for Program-Guided Fact-Checking</a></h1>
<p><b>Authors:</b> Qisheng Hu, Quanyu Long, Wenya Wang</p>
<p>Abstract: Program-guided reasoning has shown promise in complex claim fact-checking by decomposing claims into function calls and executing reasoning programs. However, prior work primarily relies on few-shot in-context learning (ICL) with ad-hoc demonstrations, which limit program diversity and require manual design with substantial domain knowledge. Fundamentally, the underlying principles of effective reasoning program generation still remain underexplored, making it challenging to construct effective demonstrations. To address this, we propose BOOST, a bootstrapping-based framework for few-shot reasoning program generation. BOOST explicitly integrates claim decomposition and information-gathering strategies as structural guidance for program generation, iteratively refining bootstrapped demonstrations in a strategy-driven and data-centric manner without human intervention. This enables a seamless transition from zero-shot to few-shot strategic program-guided learning, enhancing interpretability and effectiveness. Experimental results show that BOOST outperforms prior few-shot baselines in both zero-shot and few-shot settings for complex claim verification.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02467', 11)">Copy Link</button>
<div id="copy-message-11" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02486">We Need Improved Data Curation and Attribution in AI for Scientific Discovery</a></h1>
<p><b>Authors:</b> Mara Graziani, Antonio Foncubierta, Dimitrios Christofidellis, Irina Espejo-Morales, Malina Molnar, Marvin Alberts, Matteo Manica, Jannis Born</p>
<p>Abstract: As the interplay between human-generated and synthetic data evolves, new challenges arise in scientific discovery concerning the integrity of the data and the stability of the models. In this work, we examine the role of synthetic data as opposed to that of real experimental data for scientific research. Our analyses indicate that nearly three-quarters of experimental datasets available on open-access platforms have relatively low adoption rates, opening new opportunities to enhance their discoverability and usability by automated methods. Additionally, we observe an increasing difficulty in distinguishing synthetic from real experimental data. We propose supplementing ongoing efforts in automating synthetic data detection by increasing the focus on watermarking real experimental data, thereby strengthening data traceability and integrity. Our estimates suggest that watermarking even less than half of the real world data generated annually could help sustain model robustness, while promoting a balanced integration of synthetic and human-generated content.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02486', 12)">Copy Link</button>
<div id="copy-message-12" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02489">The Self-Learning Agent with a Progressive Neural Network Integrated Transformer</a></h1>
<p><b>Authors:</b> Ajay Sivakumar,  Shalini, Vasantha Raj, Sebastian Sylvester</p>
<p>Abstract: This paper introduces a self-learning agent that integrates LLaMA 3.2 with a Progressive Neural Network (PNN) for continual learning in conversational AI and code generation. The framework dynamically collects data, fine-tunes tasks with minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA optimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances knowledge retention. Experimental results demonstrate improved adaptability and memory stability, positioning this approach as a scalable step toward Artificial General Intelligence (AGI).</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02489', 13)">Copy Link</button>
<div id="copy-message-13" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02509">A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D Printing Work Orders</a></h1>
<p><b>Authors:</b> Yuhao Liu, Maolin Yang, Pingyu Jiang</p>
<p>Abstract: With the rapid development of 3D printing, the demand for personalized and customized production on the manufacturing line is steadily increasing. Efficient merging of printing workpieces can significantly enhance the processing efficiency of the production line. Addressing the challenge, a Large Language Model (LLM)-driven method is established in this paper for the autonomous merging of 3D printing work orders, integrated with a memory-augmented learning strategy. In industrial scenarios, both device and order features are modeled into LLM-readable natural language prompt templates, and develop an order-device matching tool along with a merging interference checking module. By incorporating a self-memory learning strategy, an intelligent agent for autonomous order merging is constructed, resulting in improved accuracy and precision in order allocation. The proposed method effectively leverages the strengths of LLMs in industrial applications while reducing hallucination.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02509', 14)">Copy Link</button>
<div id="copy-message-14" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02577">Reasoning Inconsistencies and How to Mitigate Them in Deep Learning</a></h1>
<p><b>Authors:</b> Erik Arakelyan</p>
<p>Abstract: The recent advancements in Deep Learning models and techniques have led to significant strides in performance across diverse tasks and modalities. However, while the overall capabilities of models show promising growth, our understanding of their internal reasoning processes remains limited, particularly concerning systematic inconsistencies or errors patterns of logical or inferential flaws. These inconsistencies may manifest as contradictory outputs, failure to generalize across similar tasks, or erroneous conclusions in specific contexts. Even detecting and measuring such reasoning discrepancies is challenging, as they may arise from opaque internal procedures, biases and imbalances in training data, or the inherent complexity of the task. Without effective methods to detect, measure, and mitigate these errors, there is a risk of deploying models that are biased, exploitable, or logically unreliable. This thesis aims to address these issues by producing novel methods for deep learning models that reason over knowledge graphs, natural language, and images. The thesis contributes two techniques for detecting and quantifying predictive inconsistencies originating from opaque internal procedures in natural language and image processing models. To mitigate inconsistencies from biases in training data, this thesis presents a data efficient sampling method to improve fairness and performance and a synthetic dataset generation approach in low resource scenarios. Finally, the thesis offers two techniques to optimize the models for complex reasoning tasks. These methods enhance model performance while allowing for more faithful and interpretable exploration and exploitation during inference. Critically, this thesis provides a comprehensive framework to improve the robustness, fairness, and interpretability of deep learning models across diverse tasks and modalities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02577', 15)">Copy Link</button>
<div id="copy-message-15" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02623">Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions</a></h1>
<p><b>Authors:</b> PeiJie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang</p>
<p>Abstract: Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly access agents in single-mission scenarios, failing to capture real-world complexity. To bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark, each test case comprises multiple interrelated missions. This design requires agents to dynamically adapt to evolving demands. Moreover, the proposed benchmark explores all possible mission-switching patterns within a fixed mission number. Specifically, we propose a multi-agent data generation framework to construct the benchmark. We also propose a novel method to evaluate the accuracy and efficiency of agent decisions with dynamic decision trees. Experiments on diverse open-source and closed-source LLMs reveal critical factors influencing agent robustness and provide actionable insights to the tool invocation society.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02623', 16)">Copy Link</button>
<div id="copy-message-16" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02654">SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning</a></h1>
<p><b>Authors:</b> Ivo Amador, Nina Gierasimczuk</p>
<p>Abstract: We propose a learning architecture that allows symbolic control and guidance in reinforcement learning with deep neural networks. We introduce SymDQN, a novel modular approach that augments the existing Dueling Deep Q-Networks (DuelDQN) architecture with modules based on the neuro-symbolic framework of Logic Tensor Networks (LTNs). The modules guide action policy learning and allow reinforcement learning agents to display behaviour consistent with reasoning about the environment. Our experiment is an ablation study performed on the modules. It is conducted in a reinforcement learning environment of a 5x5 grid navigated by an agent that encounters various shapes, each associated with a given reward. The underlying DuelDQN attempts to learn the optimal behaviour of the agent in this environment, while the modules facilitate shape recognition and reward prediction. We show that our architecture significantly improves learning, both in terms of performance and the precision of the agent. The modularity of SymDQN allows reflecting on the intricacies and complexities of combining neural and symbolic approaches in reinforcement learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02654', 17)">Copy Link</button>
<div id="copy-message-17" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02670">Affordable AI Assistants with Knowledge Graph of Thoughts</a></h1>
<p><b>Authors:</b> Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, J\'on Gunnar Hannesson, Grzegorz Kwa\'sniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler</p>
<p>Abstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02670', 18)">Copy Link</button>
<div id="copy-message-18" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02701">Responsible Development of Offensive AI</a></h1>
<p><b>Authors:</b> Ryan Marinelli</p>
<p>Abstract: As AI advances, broader consensus is needed to determine research priorities. This endeavor discusses offensive AI and provides guidance by leveraging Sustainable Development Goals (SDGs) and interpretability techniques. The objective is to more effectively establish priorities that balance societal benefits against risks. The two forms of offensive AI evaluated in this study are vulnerability detection agents, which solve Capture- The-Flag challenges, and AI-powered malware.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02701', 19)">Copy Link</button>
<div id="copy-message-19" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02793">A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models</a></h1>
<p><b>Authors:</b> Gaurav Verma, Jiawei Zhou, Mohit Chandra, Srijan Kumar, Munmun De Choudhury</p>
<p>Abstract: Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often "superhuman", performance on standardized benchmarks. However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations. For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies. These challenges in applying large models necessitate cross-disciplinary innovations to align the models' capabilities with the needs of real-world applications. We introduce a framework that addresses this gap through a layer-wise abstraction of innovations aimed at meeting users' requirements with large models. Through multiple case studies, we illustrate how researchers and practitioners across various fields can operationalize this framework. Beyond modularizing the pipeline of transforming large models into useful "vertical systems", we also highlight the dynamism that exists within different layers of the framework. Finally, we discuss how our framework can guide researchers and practitioners to (i) optimally situate their innovations (e.g., when vertical-specific insights can empower broadly impactful vertical-agnostic innovations), (ii) uncover overlooked opportunities (e.g., spotting recurring problems across verticals to develop practically useful foundation models instead of chasing benchmarks), and (iii) facilitate cross-disciplinary communication of critical challenges (e.g., enabling a shared vocabulary for AI developers, domain experts, and human-computer interaction scholars).</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02793', 20)">Copy Link</button>
<div id="copy-message-20" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2504.02822">Do Two AI Scientists Agree?</a></h1>
<p><b>Authors:</b> Xinghong Fu, Ziming Liu, Max Tegmark</p>
<p>Abstract: When two AI models are trained on the same scientific task, do they learn the same theory or two different theories? Throughout history of science, we have witnessed the rise and fall of theories driven by experimental validation or falsification: many theories may co-exist when experimental data is lacking, but the space of survived theories become more constrained with more experimental data becoming available. We show the same story is true for AI scientists. With increasingly more systems provided in training data, AI scientists tend to converge in the theories they learned, although sometimes they form distinct groups corresponding to different theories. To mechanistically interpret what theories AI scientists learn and quantify their agreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI Scientists, trained on standard problems in physics, aggregating training results across many seeds simulating the different configurations of AI scientists. Our findings suggests for AI scientists switch from learning a Hamiltonian theory in simple setups to a Lagrangian formulation when more complex systems are introduced. We also observe strong seed dependence of the training dynamics and final learned weights, controlling the rise and fall of relevant theories. We finally demonstrate that not only can our neural networks aid interpretability, it can also be applied to higher dimensional problems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02822', 21)">Copy Link</button>
<div id="copy-message-21" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01963">LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems</a></h1>
<p><b>Authors:</b> R. M. Aratchige, W. M. K. S. Ilmini</p>
<p>Abstract: This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01963', 22)">Copy Link</button>
<div id="copy-message-22" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01970">Differentiable Optimization for Deep Learning-Enhanced DC Approximation of AC Optimal Power Flow</a></h1>
<p><b>Authors:</b> Andrew Rosemberg, Michael Klamkin</p>
<p>Abstract: The growing scale of power systems and the increasing uncertainty introduced by renewable energy sources necessitates novel optimization techniques that are significantly faster and more accurate than existing methods. The AC Optimal Power Flow (AC-OPF) problem, a core component of power grid optimization, is often approximated using linearized DC Optimal Power Flow (DC-OPF) models for computational tractability, albeit at the cost of suboptimal and inefficient decisions. To address these limitations, we propose a novel deep learning-based framework for network equivalency that enhances DC-OPF to more closely mimic the behavior of AC-OPF. The approach utilizes recent advances in differentiable optimization, incorporating a neural network trained to predict adjusted nodal shunt conductances and branch susceptances in order to account for nonlinear power flow behavior. The model can be trained end-to-end using modern deep learning frameworks by leveraging the implicit function theorem. Results demonstrate the framework's ability to significantly improve prediction accuracy, paving the way for more reliable and efficient power systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01970', 23)">Copy Link</button>
<div id="copy-message-23" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01973">Universally applicable and tunable graph-based coarse-graining for Machine learning force fields</a></h1>
<p><b>Authors:</b> Christoph Brunken, Sebastien Boyer, Mustafa Omar, Martin Maarand, Olivier Peltre, Solal Attias, Bakary N'tji Diallo, Anastasia Markina, Olaf Othersen, Oliver Bent</p>
<p>Abstract: Coarse-grained (CG) force field methods for molecular systems are a crucial tool to simulate large biological macromolecules and are therefore essential for characterisations of biomolecular systems. While state-of-the-art deep learning (DL)-based models for all-atom force fields have improved immensely over recent years, we observe and analyse significant limitations of the currently available approaches for DL-based CG simulations. In this work, we present the first transferable DL-based CG force field approach (i.e., not specific to only one narrowly defined system type) applicable to a wide range of biosystems. To achieve this, our CG algorithm does not rely on hard-coded rules and is tuned to output coarse-grained systems optimised for minimal statistical noise in the ground truth CG forces, which results in significant improvement of model training. Our force field model is also the first CG variant that is based on the MACE architecture and is trained on a custom dataset created by a new approach based on the fragmentation of large biosystems covering protein, RNA and lipid chemistry. We demonstrate that our model can be applied in molecular dynamics simulations to obtain stable and qualitatively accurate trajectories for a variety of systems, while also discussing cases for which we observe limited reliability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01973', 24)">Copy Link</button>
<div id="copy-message-24" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01979">Correlation-Attention Masked Temporal Transformer for User Identity Linkage Using Heterogeneous Mobility Data</a></h1>
<p><b>Authors:</b> Ziang Yan, Xingyu Zhao, Hanqing Ma, Wei Chen, Jianpeng Qi, Yanwei Yu, Junyu Dong</p>
<p>Abstract: With the rise of social media and Location-Based Social Networks (LBSN), check-in data across platforms has become crucial for User Identity Linkage (UIL). These data not only reveal users' spatio-temporal information but also provide insights into their behavior patterns and interests. However, cross-platform identity linkage faces challenges like poor data quality, high sparsity, and noise interference, which hinder existing methods from extracting cross-platform user information. To address these issues, we propose a Correlation-Attention Masked Transformer for User Identity Linkage Network (MT-Link), a transformer-based framework to enhance model performance by learning spatio-temporal co-occurrence patterns of cross-platform users. Our model effectively captures spatio-temporal co-occurrence in cross-platform user check-in sequences. It employs a correlation attention mechanism to detect the spatio-temporal co-occurrence between user check-in sequences. Guided by attention weight maps, the model focuses on co-occurrence points while filtering out noise, ultimately improving classification performance. Experimental results show that our model significantly outperforms state-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in terms of Macro-F1 and Area Under Curve (AUC).</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01979', 25)">Copy Link</button>
<div id="copy-message-25" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01980">Information Gain Is Not All You Need</a></h1>
<p><b>Authors:</b> Ludvig Ericson, Jos\'e Pedro, Patric Jensfelt</p>
<p>Abstract: Autonomous exploration in mobile robotics is driven by two competing objectives: coverage, to exhaustively observe the environment; and path length, to do so with the shortest path possible. Though it is difficult to evaluate the best course of action without knowing the unknown, the unknown can often be understood through models, maps, or common sense. However, previous work has shown that improving estimates of information gain through such prior knowledge leads to greedy behavior and ultimately causes backtracking, which degrades coverage performance. In fact, any information gain maximization will exhibit this behavior, even without prior knowledge. Information gained at task completion is constant, and cannot be maximized for. It is therefore an unsuitable choice as an optimization objective. Instead, information gain is a decision criterion for determining which candidate states should still be considered for exploration. The task therefore becomes to reach completion with the shortest total path. Since determining the shortest path is typically intractable, it is necessary to rely on a heuristic or estimate to identify candidate states that minimize the total path length. To address this, we propose a heuristic that reduces backtracking by preferring candidate states that are close to the robot, but far away from other candidate states. We evaluate the performance of the proposed heuristic in simulation against an information gain-based approach and frontier exploration, and show that our method significantly decreases total path length, both with and without prior knowledge of the environment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01980', 26)">Copy Link</button>
<div id="copy-message-26" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01981">NLS: Natural-Level Synthesis for Hardware Implementation Through GenAI</a></h1>
<p><b>Authors:</b> Kaiyuan Yang, Huang Ouyang, Xinyi Wang, Bingjie Lu, Yanbo Wang, Charith Abhayaratne, Sizhao Li, Long Jin, Tiantai Deng</p>
<p>Abstract: This paper introduces Natural-Level Synthesis, an innovative approach for generating hardware using generative artificial intelligence on both the system level and component-level. NLS bridges a gap in current hardware development processes, where algorithm and application engineers' involvement typically ends at the requirements stage. With NLS, engineers can participate more deeply in the development, synthesis, and test stages by using Gen-AI models to convert natural language descriptions directly into Hardware Description Language code. This approach not only streamlines hardware development but also improves accessibility, fostering a collaborative workflow between hardware and algorithm engineers. We developed the NLS tool to facilitate natural language-driven HDL synthesis, enabling rapid generation of system-level HDL designs while significantly reducing development complexity. Evaluated through case studies and benchmarks using Performance, Power, and Area metrics, NLS shows its potential to enhance resource efficiency in hardware development. This work provides a extensible, efficient solution for hardware synthesis and establishes a Visual Studio Code Extension to assess Gen-AI-driven HDL generation and system integration, laying a foundation for future AI-enhanced and AI-in-the-loop Electronic Design Automation tools.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01981', 27)">Copy Link</button>
<div id="copy-message-27" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01985">Multi-Dimensional AGV Path Planning in 3D Warehouses Using Ant Colony Optimization and Advanced Neural Networks</a></h1>
<p><b>Authors:</b> Bo Zhang, Xiubo Liang, Wei Song, Yulu Chen</p>
<p>Abstract: Within modern warehouse scenarios, the rapid expansion of e-commerce and increasingly complex, multi-level storage environments have exposed the limitations of traditional AGV (Automated Guided Vehicle) path planning methods--often reliant on static 2D models and expert-tuned heuristics that struggle to handle dynamic traffic and congestion. Addressing these limitations, this paper introduces a novel AGV path planning approach for 3D warehouse environments that leverages a hybrid framework combining ACO (Ant Colony Optimization) with deep learning models, called NAHACO (Neural Adaptive Heuristic Ant Colony Optimization). NAHACO integrates three key innovations: first, an innovative heuristic algorithm for 3D warehouse cargo modeling using multidimensional tensors, which addresses the challenge of achieving superior heuristic accuracy; second, integration of a congestion-aware loss function within the ACO framework to adjust path costs based on traffic and capacity constraints, called CARL (Congestion-Aware Reinforce Loss), enabling dynamic heuristic calibration for optimizing ACO-based path planning; and third, an adaptive attention mechanism that captures multi-scale spatial features, thereby addressing dynamic heuristic calibration for further optimization of ACO-based path planning and AGV navigation. NAHACO significantly boosts path planning efficiency, yielding faster computation times and superior performance over both vanilla and state-of-the-art methods, while automatically adapting to warehouse constraints for real-time optimization. NAHACO outperforms state-of-the-art methods, lowering the total cost by up to 24.7% on TSP benchmarks. In warehouse tests, NAHACO cuts cost by up to 41.5% and congestion by up to 56.1% compared to previous methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01985', 28)">Copy Link</button>
<div id="copy-message-28" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01986">TuRTLe: A Unified Evaluation of LLMs for RTL Generation</a></h1>
<p><b>Authors:</b> Dario Garcia-Gasulla, Gokcen Kestor, Emanuele Parisi, Miquel Albert'i-Binimelis, Cristian Gutierrez, Razine Moundir Ghorab, Orlando Montenegro, Bernat Homs, Miquel Moreto</p>
<p>Abstract: The rapid advancements in LLMs have driven the adoption of generative AI in various domains, including Electronic Design Automation (EDA). Unlike traditional software development, EDA presents unique challenges, as generated RTL code must not only be syntactically correct and functionally accurate but also synthesizable by hardware generators while meeting performance, power, and area constraints. These additional requirements introduce complexities that existing code-generation benchmarks often fail to capture, limiting their effectiveness in evaluating LLMs for RTL generation. To address this gap, we propose TuRTLe, a unified evaluation framework designed to systematically assess LLMs across key RTL generation tasks. TuRTLe integrates multiple existing benchmarks and automates the evaluation process, enabling a comprehensive assessment of LLM performance in syntax correctness, functional correctness, synthesis, PPA optimization, and exact line completion. Using this framework, we benchmark a diverse set of open LLMs and analyze their strengths and weaknesses in EDA-specific tasks. Our results show that reasoning-based models, such as DeepSeek R1, consistently outperform others across multiple evaluation criteria, but at the cost of increased computational overhead and inference latency. Additionally, base models are better suited in module completion tasks, while instruct-tuned models perform better in specification-to-RTL tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01986', 29)">Copy Link</button>
<div id="copy-message-29" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01992">Exploring the Societal and Economic Impacts of Artificial Intelligence: A Scenario Generation Methodology</a></h1>
<p><b>Authors:</b> Carlos J. Costa, Joao Tiago Aparicio</p>
<p>Abstract: This paper explores artificial intelligence's potential societal and economic impacts (AI) through generating scenarios that assess how AI may influence various sectors. We categorize and analyze key factors affecting AI's integration and adoption by applying an Impact-Uncertainty Matrix. A proposed methodology involves querying academic databases, identifying emerging trends and topics, and categorizing these into an impact uncertainty framework. The paper identifies critical areas where AI may bring significant change and outlines potential future scenarios based on these insights. This research aims to inform policymakers, industry leaders, and researchers on the strategic planning required to address the challenges and opportunities AI presents</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01992', 30)">Copy Link</button>
<div id="copy-message-30" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.01994">PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs</a></h1>
<p><b>Authors:</b> Jinendra Malekar, Peyton Chandarana, Md Hasibul Amin, Mohammed E. Elbtity, Ramtin Zand</p>
<p>Abstract: In this paper, we propose PIM-LLM, a hybrid architecture developed to accelerate 1-bit large language models (LLMs). PIM-LLM leverages analog processing-in-memory (PIM) architectures and digital systolic arrays to accelerate low-precision matrix multiplication (MatMul) operations in projection layers and high-precision MatMul operations in attention heads of 1-bit LLMs, respectively. Our design achieves up to roughly 80x improvement in tokens per second and a 70% increase in tokens per joule compared to conventional hardware accelerators. Additionally, PIM-LLM outperforms previous PIM-based LLM accelerators, setting a new benchmark with at least 2x and 5x improvement in GOPS and GOPS/W, respectively.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01994', 31)">Copy Link</button>
<div id="copy-message-31" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02000">AI Regulation and Capitalist Growth: Balancing Innovation, Ethics, and Global Governance</a></h1>
<p><b>Authors:</b> Vikram Kulothungan, Priya Ranjani Mohan, Deepti Gupta</p>
<p>Abstract: Artificial Intelligence (AI) is increasingly central to economic growth, promising new efficiencies and markets. This economic significance has sparked debate over AI regulation: do rules and oversight bolster long term growth by building trust and safeguarding the public, or do they constrain innovation and free enterprise? This paper examines the balance between AI regulation and capitalist ideals, focusing on how different approaches to AI data privacy can impact innovation in AI-driven applications. The central question is whether AI regulation enhances or inhibits growth in a capitalist economy. Our analysis synthesizes historical precedents, the current U.S. regulatory landscape, economic projections, legal challenges, and case studies of recent AI policies. We discuss that carefully calibrated AI data privacy regulations-balancing innovation incentives with the public interest can foster sustainable growth by building trust and ensuring responsible data use, while excessive regulation may risk stifling innovation and entrenching incumbents.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02000', 32)">Copy Link</button>
<div id="copy-message-32" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02008">Test-time Adaptation for Foundation Medical Segmentation Model without Parametric Updates</a></h1>
<p><b>Authors:</b> Kecheng Chen, Xinyu Luo, Tiexin Qin, Jie Liu, Hui Liu, Victor Ho Fun Lee, Hong Yan, Haoliang Li</p>
<p>Abstract: Foundation medical segmentation models, with MedSAM being the most popular, have achieved promising performance across organs and lesions. However, MedSAM still suffers from compromised performance on specific lesions with intricate structures and appearance, as well as bounding box prompt-induced perturbations. Although current test-time adaptation (TTA) methods for medical image segmentation may tackle this issue, partial (e.g., batch normalization) or whole parametric updates restrict their effectiveness due to limited update signals or catastrophic forgetting in large models. Meanwhile, these approaches ignore the computational complexity during adaptation, which is particularly significant for modern foundation models. To this end, our theoretical analyses reveal that directly refining image embeddings is feasible to approach the same goal as parametric updates under the MedSAM architecture, which enables us to realize high computational efficiency and segmentation performance without the risk of catastrophic forgetting. Under this framework, we propose to encourage maximizing factorized conditional probabilities of the posterior prediction probability using a proposed distribution-approximated latent conditional random field loss combined with an entropy minimization loss. Experiments show that we achieve about 3\% Dice score improvements across three datasets while reducing computational complexity by over 7 times.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02008', 33)">Copy Link</button>
<div id="copy-message-33" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02011">Random Conditioning with Distillation for Data-Efficient Diffusion Model Compression</a></h1>
<p><b>Authors:</b> Dohyun Kim, Sehwan Park, Geonhee Han, Seung Wook Kim, Paul Hongsuck Seo</p>
<p>Abstract: Diffusion models generate high-quality images through progressive denoising but are computationally intensive due to large model sizes and repeated sampling. Knowledge distillation, which transfers knowledge from a complex teacher to a simpler student model, has been widely studied in recognition tasks, particularly for transferring concepts unseen during student training. However, its application to diffusion models remains underexplored, especially in enabling student models to generate concepts not covered by the training images. In this work, we propose Random Conditioning, a novel approach that pairs noised images with randomly selected text conditions to enable efficient, image-free knowledge distillation. By leveraging this technique, we show that the student can generate concepts unseen in the training images. When applied to conditional diffusion model distillation, our method allows the student to explore the condition space without generating condition-specific images, resulting in notable improvements in both generation quality and efficiency. This promotes resource-efficient deployment of generative diffusion models, broadening their accessibility for both research and real-world applications. Code, models, and datasets are available at https://dohyun-as.github.io/Random-Conditioning .</p>
<p>URLs: <a href="https://dohyun-as.github.io/Random-Conditioning">https://dohyun-as.github.io/Random-Conditioning</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02011, https://dohyun-as.github.io/Random-Conditioning', 34)">Copy Link</button>
<div id="copy-message-34" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02014">HCAF-DTA: drug-target binding affinity prediction with cross-attention fused hypergraph neural networks</a></h1>
<p><b>Authors:</b> Jiannuo Li, Lan Yao</p>
<p>Abstract: Accurate prediction of the binding affinity between drugs and target proteins is a core task in computer-aided drug design. Existing deep learning methods tend to ignore the information of internal sub-structural features of drug molecules and drug-target interactions, resulting in limited prediction performance. In this paper, we propose a drug-target association prediction model HCAF-DTA based on cross-attention fusion hypergraph neural network. The model innovatively introduces hypergraph representation in the feature extraction stage: drug molecule hypergraphs are constructed based on the tree decomposition algorithm, and the sub-structural and global features extracted by fusing the hypergraph neural network with the graphical neural network through hopping connections, in which the hyper edges can efficiently characterise the functional functional groups and other key chemical features; for the protein feature extraction, a weighted graph is constructed based on the residues predicted by the ESM model contact maps to construct weighted graphs, and multilayer graph neural networks were used to capture spatial dependencies. In the prediction stage, a bidirectional multi-head cross-attention mechanism is designed to model intermolecular interactions from the dual viewpoints of atoms and amino acids, and cross-modal features with correlated information are fused by attention. Experiments on benchmark datasets such as Davis and KIBA show that HCAF-DTA outperforms state of the arts in all three performance evaluation metrics, with the MSE metrics reaching 0.198 and 0.122, respectively, with an improvement of up to 4% from the optimal baseline.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02014', 35)">Copy Link</button>
<div id="copy-message-35" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02019">Antithetic Sampling for Top-k Shapley Identification</a></h1>
<p><b>Authors:</b> Patrick Kolpaczki, Tim Nielen, Eyke H\"ullermeier</p>
<p>Abstract: Additive feature explanations rely primarily on game-theoretic notions such as the Shapley value by viewing features as cooperating players. The Shapley value's popularity in and outside of explainable AI stems from its axiomatic uniqueness. However, its computational complexity severely limits practicability. Most works investigate the uniform approximation of all features' Shapley values, needlessly consuming samples for insignificant features. In contrast, identifying the $k$ most important features can already be sufficiently insightful and yields the potential to leverage algorithmic opportunities connected to the field of multi-armed bandits. We propose Comparable Marginal Contributions Sampling (CMCS), a method for the top-$k$ identification problem utilizing a new sampling scheme taking advantage of correlated observations. We conduct experiments to showcase the efficacy of our method in compared to competitive baselines. Our empirical findings reveal that estimation quality for the approximate-all problem does not necessarily transfer to top-$k$ identification and vice versa.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02019', 36)">Copy Link</button>
<div id="copy-message-36" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02051">Self-Resource Allocation in Multi-Agent LLM Systems</a></h1>
<p><b>Authors:</b> Alfonso Amayuelas, Jingbo Yang, Saaket Agashe, Ashwin Nagarajan, Antonis Antoniades, Xin Eric Wang, William Wang</p>
<p>Abstract: With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02051', 37)">Copy Link</button>
<div id="copy-message-37" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02064">From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP</a></h1>
<p><b>Authors:</b> Fabio Y\'a\~nez-Romero, Andr\'es Montoyo, Armando Su\'arez, Yoan Guti\'errez, Ruslan Mitkov</p>
<p>Abstract: Researchers have relegated natural language processing tasks to Transformer-type models, particularly generative models, because these models exhibit high versatility when performing generation and classification tasks. As the size of these models increases, they achieve outstanding results. Given their widespread use, many explainability techniques are developed based on these models. However, this process becomes computationally expensive due to the large size of the models. Additionally, transformers interpret input information through tokens that fragment input words into sequences lacking inherent semantic meaning, complicating the explanation of the model from the very beginning. This study proposes a novel methodology to achieve explainability in natural language processing tasks by automatically converting sentences into graphs and maintaining semantics through nodes and relations that express fundamental linguistic concepts. It also allows the subsequent exploitation of this knowledge in subsequent tasks, making it possible to obtain trends and understand how the model associates the different elements inside the text with the explained task. The experiments delivered promising results in determining the most critical components within the text structure for a given classification.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02064', 38)">Copy Link</button>
<div id="copy-message-38" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02069">RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics</a></h1>
<p><b>Authors:</b> Zhiyuan Zhang, Yuxin He, Yong Sun, Junyu Shi, Lijiang Liu, Qiang Nie</p>
<p>Abstract: Visual Language Models (VLMs) have emerged as pivotal tools for robotic systems, enabling cross-task generalization, dynamic environmental interaction, and long-horizon planning through multimodal perception and semantic reasoning. However, existing open-source VLMs predominantly trained for generic vision-language alignment tasks fail to model temporally correlated action semantics that are crucial for robotic manipulation effectively. While current image-based fine-tuning methods partially adapt VLMs to robotic applications, they fundamentally disregard temporal evolution patterns in video sequences and suffer from visual feature entanglement between robotic agents, manipulated objects, and environmental contexts, thereby limiting semantic decoupling capability for atomic actions and compromising model generalizability.To overcome these challenges, this work presents RoboAct-CLIP with dual technical contributions: 1) A dataset reconstruction framework that performs semantic-constrained action unit segmentation and re-annotation on open-source robotic videos, constructing purified training sets containing singular atomic actions (e.g., "grasp"); 2) A temporal-decoupling fine-tuning strategy based on Contrastive Language-Image Pretraining (CLIP) architecture, which disentangles temporal action features across video frames from object-centric characteristics to achieve hierarchical representation learning of robotic atomic actions.Experimental results in simulated environments demonstrate that the RoboAct-CLIP pretrained model achieves a 12% higher success rate than baseline VLMs, along with superior generalization in multi-object manipulation tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02069', 39)">Copy Link</button>
<div id="copy-message-39" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02074">Trapped by Expectations: Functional Fixedness in LLM-Enabled Chat Search</a></h1>
<p><b>Authors:</b> Jiqun Liu, Jamshed Karimnazarov, Ryen W. White</p>
<p>Abstract: Functional fixedness, a cognitive bias that restricts users' interactions with a new system or tool to expected or familiar ways, limits the full potential of Large Language Model (LLM)-enabled chat search, especially in complex and exploratory tasks. To investigate its impact, we conducted a crowdsourcing study with 450 participants, each completing one of six decision-making tasks spanning public safety, diet and health management, sustainability, and AI ethics. Participants engaged in a multi-prompt conversation with ChatGPT to address the task, allowing us to compare pre-chat intent-based expectations with observed interactions. We found that: 1) Several aspects of pre-chat expectations are closely associated with users' prior experiences with ChatGPT, search engines, and virtual assistants; 2) Prior system experience shapes language use and prompting behavior. Frequent ChatGPT users reduced deictic terms and hedge words and frequently adjusted prompts. Users with rich search experience maintained structured, less-conversational queries with minimal modifications. Users of virtual assistants favored directive, command-like prompts, reinforcing functional fixedness; 3) When the system failed to meet expectations, participants generated more detailed prompts with increased linguistic diversity, reflecting adaptive shifts. These findings suggest that while preconceived expectations constrain early interactions, unmet expectations can motivate behavioral adaptation. With appropriate system support, this may promote broader exploration of LLM capabilities. This work also introduces a typology for user intents in chat search and highlights the importance of mitigating functional fixedness to support more creative and analytical use of LLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02074', 40)">Copy Link</button>
<div id="copy-message-40" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02080">Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses</a></h1>
<p><b>Authors:</b> Zhengchun Shang, Wenlan Wei</p>
<p>Abstract: Large Language Models (LLMs) are increasingly popular, powering a wide range of applications. Their widespread use has sparked concerns, especially through jailbreak attacks that bypass safety measures to produce harmful content.
  In this paper, we present a comprehensive security analysis of large language models (LLMs), addressing critical research questions on the evolution and determinants of model safety.
  Specifically, we begin by identifying the most effective techniques for detecting jailbreak attacks. Next, we investigate whether newer versions of LLMs offer improved security compared to their predecessors. We also assess the impact of model size on overall security and explore the potential benefits of integrating multiple defense strategies to enhance model robustness.
  Our study evaluates both open-source models (e.g., LLaMA and Mistral) and closed-source systems (e.g., GPT-4) by employing four state-of-the-art attack techniques and assessing the efficacy of three new defensive approaches.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02080', 41)">Copy Link</button>
<div id="copy-message-41" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02087">An Introductory Survey to Autoencoder-based Deep Clustering -- Sandboxes for Combining Clustering with Deep Learning</a></h1>
<p><b>Authors:</b> Collin Leiber, Lukas Miklautz, Claudia Plant, Christian B\"ohm</p>
<p>Abstract: Autoencoders offer a general way of learning low-dimensional, non-linear representations from data without labels. This is achieved without making any particular assumptions about the data type or other domain knowledge. The generality and domain agnosticism in combination with their simplicity make autoencoders a perfect sandbox for researching and developing novel (deep) clustering algorithms. Clustering methods group data based on similarity, a task that benefits from the lower-dimensional representation learned by an autoencoder, mitigating the curse of dimensionality. Specifically, the combination of deep learning with clustering, called Deep Clustering, enables to learn a representation tailored to specific clustering tasks, leading to high-quality results. This survey provides an introduction to fundamental autoencoder-based deep clustering algorithms that serve as building blocks for many modern approaches.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02087', 42)">Copy Link</button>
<div id="copy-message-42" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02094">FlowDistill: Scalable Traffic Flow Prediction via Distillation from LLMs</a></h1>
<p><b>Authors:</b> Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu</p>
<p>Abstract: Accurate traffic flow prediction is vital for optimizing urban mobility, yet it remains difficult in many cities due to complex spatio-temporal dependencies and limited high-quality data. While deep graph-based models demonstrate strong predictive power, their performance often comes at the cost of high computational overhead and substantial training data requirements, making them impractical for deployment in resource-constrained or data-scarce environments. We propose the FlowDistill, a lightweight and scalable traffic prediction framework based on knowledge distillation from large language models (LLMs). In this teacher-student setup, a fine-tuned LLM guides a compact multi-layer perceptron (MLP) student model using a novel combination of the information bottleneck principle and teacher-bounded regression loss, ensuring the distilled model retains only essential and transferable knowledge. Spatial and temporal correlations are explicitly encoded to enhance the model's generalization across diverse urban settings. Despite its simplicity, FlowDistill consistently outperforms state-of-the-art models in prediction accuracy while requiring significantly less training data, and achieving lower memory usage and inference latency, highlighting its efficiency and suitability for real-world, scalable deployment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02094', 43)">Copy Link</button>
<div id="copy-message-43" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02110">ScreenAudit: Detecting Screen Reader Accessibility Errors in Mobile Apps Using Large Language Models</a></h1>
<p><b>Authors:</b> Mingyuan Zhong, Ruolin Chen, Xia Chen, James Fogarty, Jacob O. Wobbrock</p>
<p>Abstract: Many mobile apps are inaccessible, thereby excluding people from their potential benefits. Existing rule-based accessibility checkers aim to mitigate these failures by identifying errors early during development but are constrained in the types of errors they can detect. We present ScreenAudit, an LLM-powered system designed to traverse mobile app screens, extract metadata and transcripts, and identify screen reader accessibility errors overlooked by existing checkers. We recruited six accessibility experts including one screen reader user to evaluate ScreenAudit's reports across 14 unique app screens. Our findings indicate that ScreenAudit achieves an average coverage of 69.2%, compared to only 31.3% with a widely-used accessibility checker. Expert feedback indicated that ScreenAudit delivered higher-quality feedback and addressed more aspects of screen reader accessibility compared to existing checkers, and that ScreenAudit would benefit app developers in real-world settings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02110', 44)">Copy Link</button>
<div id="copy-message-44" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02114">On Model Protection in Federated Learning against Eavesdropping Attacks</a></h1>
<p><b>Authors:</b> Dipankar Maity, Kushal Chakrabarti</p>
<p>Abstract: In this study, we investigate the protection offered by federated learning algorithms against eavesdropping adversaries. In our model, the adversary is capable of intercepting model updates transmitted from clients to the server, enabling it to create its own estimate of the model. Unlike previous research, which predominantly focuses on safeguarding client data, our work shifts attention protecting the client model itself. Through a theoretical analysis, we examine how various factors, such as the probability of client selection, the structure of local objective functions, global aggregation at the server, and the eavesdropper's capabilities, impact the overall level of protection. We further validate our findings through numerical experiments, assessing the protection by evaluating the model accuracy achieved by the adversary. Finally, we compare our results with methods based on differential privacy, underscoring their limitations in this specific context.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02114', 45)">Copy Link</button>
<div id="copy-message-45" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02118">LLMPi: Optimizing LLMs for High-Throughput on Raspberry Pi</a></h1>
<p><b>Authors:</b> Mahsa Ardakani, Jinendra Malekar, Ramtin Zand</p>
<p>Abstract: Deploying Large Language Models (LLMs) on resource-constrained edge devices like the Raspberry Pi presents challenges in computational efficiency, power consumption, and response latency. This paper explores quantization-based optimization techniques to enable high-throughput, energy-efficient execution of LLMs on low-power embedded systems. Our approach leverages k-quantization, a Post-Training Quantization (PTQ) method designed for different bit-widths, enabling efficient 2-bit, 4-bit, 6-bit, and 8-bit weight quantization. Additionally, we employ ternary quantization using Quantization-Aware Training (QAT) for BitNet models, allowing for more effective adaptation to lower-bit representations while preserving accuracy.
  Our findings highlight the potential of quantized LLMs for real-time conversational AI on edge devices, paving the way for low-power, high-efficiency AI deployment in mobile and embedded applications. This study demonstrates that aggressive quantization strategies can significantly reduce energy consumption while maintaining inference quality, making LLMs practical for resource-limited environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02118', 46)">Copy Link</button>
<div id="copy-message-46" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02128">Achieving Unanimous Consensus in Decision Making Using Multi-Agents</a></h1>
<p><b>Authors:</b> Apurba Pokharel, Ram Dantu, Shakila Zaman, Sirisha Talapuru, Vinh Quach</p>
<p>Abstract: Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures both unanimous consensus for definitive problems and graded confidence for prioritized decisions and policies. We provide a formalization of our system and use it to show that the properties of blockchains: consistency, agreement, liveness, and determinism are maintained. Moreover, experimental results demonstrate our system's feasibility, showcasing how our deliberation method's convergence, block properties, and accuracy enable decision-making on blockchain networks. We also address key challenges with this novel approach such as degeneration of thoughts, hallucinations, malicious models and nodes, resource consumption, and scalability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02128', 47)">Copy Link</button>
<div id="copy-message-47" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02137">Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID</a></h1>
<p><b>Authors:</b> Carolina Zheng, Minhui Huang, Dmitrii Pedchenko, Kaushik Rangadurai, Siyu Wang, Gaby Nahum, Jie Lei, Yang Yang, Tao Liu, Zutian Luo, Xiaohan Wei, Dinesh Ramasamy, Jiyan Yang, Yiping Han, Lin Yang, Hangjun Xu, Rong Jin, Shuang Yang</p>
<p>Abstract: The exponential growth of online content has posed significant challenges to ID-based models in industrial recommendation systems, ranging from extremely high cardinality and dynamically growing ID space, to highly skewed engagement distributions, to prediction instability as a result of natural id life cycles (e.g, the birth of new IDs and retirement of old IDs). To address these issues, many systems rely on random hashing to handle the id space and control the corresponding model parameters (i.e embedding table). However, this approach introduces data pollution from multiple ids sharing the same embedding, leading to degraded model performance and embedding representation instability.
  This paper examines these challenges and introduces Semantic ID prefix ngram, a novel token parameterization technique that significantly improves the performance of the original Semantic ID. Semantic ID prefix ngram creates semantically meaningful collisions by hierarchically clustering items based on their content embeddings, as opposed to random assignments. Through extensive experimentation, we demonstrate that Semantic ID prefix ngram not only addresses embedding instability but also significantly improves tail id modeling, reduces overfitting, and mitigates representation shifts. We further highlight the advantages of Semantic ID prefix ngram in attention-based models that contextualize user histories, showing substantial performance improvements. We also report our experience of integrating Semantic ID into Meta production Ads Ranking system, leading to notable performance gains and enhanced prediction stability in live deployments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02137', 48)">Copy Link</button>
<div id="copy-message-48" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02141">On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software</a></h1>
<p><b>Authors:</b> Ali Nouri, Johan Andersson, Kailash De Jesus Hornig, Zhennan Fei, Emil Knabe, Hakan Sivencrona, Beatriz Cabrero-Daniel, Christian Berger</p>
<p>Abstract: Automated Driving System (ADS) is a safety-critical software system responsible for the interpretation of the vehicle's environment and making decisions accordingly. The unbounded complexity of the driving context, including unforeseeable events, necessitate continuous improvement, often achieved through iterative DevOps processes. However, DevOps processes are themselves complex, making these improvements both time- and resource-intensive. Automation in code generation for ADS using Large Language Models (LLM) is one potential approach to address this challenge. Nevertheless, the development of ADS requires rigorous processes to verify, validate, assess, and qualify the code before it can be deployed in the vehicle and used. In this study, we developed and evaluated a prototype for automatic code generation and assessment using a designed pipeline of a LLM-based agent, simulation model, and rule-based feedback generator in an industrial setup. The LLM-generated code is evaluated automatically in a simulation model against multiple critical traffic scenarios, and an assessment report is provided as feedback to the LLM for modification or bug fixing. We report about the experimental results of the prototype employing Codellama:34b, DeepSeek (r1:32b and Coder:33b), CodeGemma:7b, Mistral:7b, and GPT4 for Adaptive Cruise Control (ACC) and Unsupervised Collision Avoidance by Evasive Manoeuvre (CAEM). We finally assessed the tool with 11 experts at two Original Equipment Manufacturers (OEMs) by conducting an interview study.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02141', 49)">Copy Link</button>
<div id="copy-message-49" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02144">Towards Interpretable Soft Prompts</a></h1>
<p><b>Authors:</b> Oam Patel, Jason Wang, Nikhil Shivakumar Nayak, Suraj Srinivas, Himabindu Lakkaraju</p>
<p>Abstract: Soft prompts have been popularized as a cheap and easy way to improve task-specific LLM performance beyond few-shot prompts. Despite their origin as an automated prompting method, however, soft prompts and other trainable prompts remain a black-box method with no immediately interpretable connections to prompting. We create a novel theoretical framework for evaluating the interpretability of trainable prompts based on two desiderata: faithfulness and scrutability. We find that existing methods do not naturally satisfy our proposed interpretability criterion. Instead, our framework inspires a new direction of trainable prompting methods that explicitly optimizes for interpretability. To this end, we formulate and test new interpretability-oriented objective functions for two state-of-the-art prompt tuners: Hard Prompts Made Easy (PEZ) and RLPrompt. Our experiments with GPT-2 demonstrate a fundamental trade-off between interpretability and the task-performance of the trainable prompt, explicating the hardness of the soft prompt interpretability problem and revealing odd behavior that arises when one optimizes for an interpretability proxy.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02144', 50)">Copy Link</button>
<div id="copy-message-50" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02151">Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP</a></h1>
<p><b>Authors:</b> Jiztom Kavalakkatt Francis, Matthew J Darr</p>
<p>Abstract: The rapid use of artificial intelligence (AI) in processes such as coding, image processing, and data prediction means it is crucial to understand and validate the data we are working with fully. This paper dives into the hurdles of analyzing high-dimensional data, especially when it gets too complex. Traditional methods in data analysis often look at direct connections between input variables, which can miss out on the more complicated relationships within the data.
  To address these issues, we explore several tested techniques, such as removing specific variables to see their impact and using statistical analysis to find connections between multiple variables. We also consider the role of synthetic data and how information can sometimes be redundant across different sensors. These analyses are typically very computationally demanding and often require much human effort to make sense of the results.
  A common approach is to treat the entire dataset as one unit and apply advanced models to handle it. However, this can become problematic with larger, noisier datasets and more complex models. So, we suggest methods to identify overall patterns that can help with tasks like classification or regression based on the idea that more straightforward approaches might be more understandable.
  Our research looks at two datasets: a real-world dataset and a synthetic one. The goal is to create a methodology that highlights key features on a global scale that lead to predictions, making it easier to validate or quantify the data set. By reducing the dimensionality with this method, we can simplify the models used and thus clarify the insights we gain. Furthermore, our method can reveal unexplored relationships between specific inputs and outcomes, providing a way to validate these new connections further.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02151', 51)">Copy Link</button>
<div id="copy-message-51" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02168">MDP: Multidimensional Vision Model Pruning with Latency Constraint</a></h1>
<p><b>Authors:</b> Xinglong Sun, Barath Lakshmanan, Maying Shen, Shiyi Lan, Jingde Chen, Jose M. Alvarez</p>
<p>Abstract: Current structural pruning methods face two significant limitations: (i) they often limit pruning to finer-grained levels like channels, making aggressive parameter reduction challenging, and (ii) they focus heavily on parameter and FLOP reduction, with existing latency-aware methods frequently relying on simplistic, suboptimal linear models that fail to generalize well to transformers, where multiple interacting dimensions impact latency. In this paper, we address both limitations by introducing Multi-Dimensional Pruning (MDP), a novel paradigm that jointly optimizes across a variety of pruning granularities-including channels, query, key, heads, embeddings, and blocks. MDP employs an advanced latency modeling technique to accurately capture latency variations across all prunable dimensions, achieving an optimal balance between latency and accuracy. By reformulating pruning as a Mixed-Integer Nonlinear Program (MINLP), MDP efficiently identifies the optimal pruned structure across all prunable dimensions while respecting latency constraints. This versatile framework supports both CNNs and transformers. Extensive experiments demonstrate that MDP significantly outperforms previous methods, especially at high pruning ratios. On ImageNet, MDP achieves a 28% speed increase with a +1.4 Top-1 accuracy improvement over prior work like HALP for ResNet50 pruning. Against the latest transformer pruning method, Isomorphic, MDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy improvement.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02168', 52)">Copy Link</button>
<div id="copy-message-52" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02169">On the Geometry of Receiver Operating Characteristic and Precision-Recall Curves</a></h1>
<p><b>Authors:</b> Reza Sameni</p>
<p>Abstract: We study the geometry of Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves in binary classification problems. The key finding is that many of the most commonly used binary classification metrics are merely functions of the composition function $G := F_p \circ F_n^{-1}$, where $F_p(\cdot)$ and $F_n(\cdot)$ are the class-conditional cumulative distribution functions of the classifier scores in the positive and negative classes, respectively. This geometric perspective facilitates the selection of operating points, understanding the effect of decision thresholds, and comparison between classifiers. It also helps explain how the shapes and geometry of ROC/PR curves reflect classifier behavior, providing objective tools for building classifiers optimized for specific applications with context-specific constraints. We further explore the conditions for classifier dominance, present analytical and numerical examples demonstrating the effects of class separability and variance on ROC and PR geometries, and derive a link between the positive-to-negative class leakage function $G(\cdot)$ and the Kullback--Leibler divergence. The framework highlights practical considerations, such as model calibration, cost-sensitive optimization, and operating point selection under real-world capacity constraints, enabling more informed approaches to classifier deployment and decision-making.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02169', 53)">Copy Link</button>
<div id="copy-message-53" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02199">ESC: Erasing Space Concept for Knowledge Deletion</a></h1>
<p><b>Authors:</b> Tae-Young Lee, Sundong Park, Minwoo Jeon, Hyoseok Hwang, Gyeong-Moon Park</p>
<p>Abstract: As concerns regarding privacy in deep learning continue to grow, individuals are increasingly apprehensive about the potential exploitation of their personal knowledge in trained models. Despite several research efforts to address this, they often fail to consider the real-world demand from users for complete knowledge erasure. Furthermore, our investigation reveals that existing methods have a risk of leaking personal knowledge through embedding features. To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD. Our extensive experiments on various datasets and models demonstrate that our proposed methods achieve the fastest and state-of-the-art performance. Notably, our methods are applicable to diverse forgetting scenarios, such as facial domain setting, demonstrating the generalizability of our methods. The code is available at http://github.com/KU-VGI/ESC .</p>
<p>URLs: <a href="http://github.com/KU-VGI/ESC">http://github.com/KU-VGI/ESC</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02199, http://github.com/KU-VGI/ESC', 54)">Copy Link</button>
<div id="copy-message-54" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02211">FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention</a></h1>
<p><b>Authors:</b> Huangliang Dai, Shixun Wu, Hairui Zhao, Jiajun Huang, Zizhe Jian, Yue Zhu, Haiyang Hu, Zizhong Chen</p>
<p>Abstract: Transformer models leverage self-attention mechanisms to capture complex dependencies, demonstrating exceptional performance in various applications. However, the long-duration high-load computations required for model inference impose stringent reliability demands on the computing platform, as soft errors that occur during execution can significantly degrade model performance. Existing fault tolerance methods protect each operation separately using decoupled kernels, incurring substantial computational and memory overhead. In this paper, we propose a novel error-resilient framework for Transformer models, integrating end-to-end fault tolerant attention (EFTA) to improve inference reliability against soft errors. Our approach enables error detection and correction within a fully fused attention kernel, reducing redundant data access and thereby mitigating memory faults. To further enhance error coverage and reduce overhead, we design a hybrid fault tolerance scheme tailored for the EFTA, introducing for the first time: 1) architecture-aware algorithm-based fault tolerance (ABFT) using tensor checksum, which minimizes inter-thread communication overhead on tensor cores during error detection; 2) selective neuron value restriction, which selectively applies adaptive fault tolerance constraints to neuron values, balancing error coverage and overhead; 3) unified verification, reusing checksums to streamline multiple computation steps into a single verification process. Experimental results show that EFTA achieves up to 7.56x speedup over traditional methods with an average fault tolerance overhead of 13.9%.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02211', 55)">Copy Link</button>
<div id="copy-message-55" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02221">Learning and Improving Backgammon Strategy</a></h1>
<p><b>Authors:</b> Gregory R. Galperin</p>
<p>Abstract: A novel approach to learning is presented, combining features of on-line and off-line methods to achieve considerable performance in the task of learning a backgammon value function in a process that exploits the processing power of parallel supercomputers. The off-line methods comprise a set of techniques for parallelizing neural network training and $TD(\lambda)$ reinforcement learning; here Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line policy improvement technique which applies resources to the decision points encountered during the search of the game tree to further augment the learned value function estimate. A level of play roughly as good as, or possibly better than, the current champion human and computer backgammon players has been achieved in a short period of learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02221', 56)">Copy Link</button>
<div id="copy-message-56" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02231">AC-LoRA: Auto Component LoRA for Personalized Artistic Style Image Generation</a></h1>
<p><b>Authors:</b> Zhipu Cui, Andong Tian, Zhi Ying, Jialiang Lu</p>
<p>Abstract: Personalized image generation allows users to preserve styles or subjects of a provided small set of images for further image generation. With the advancement in large text-to-image models, many techniques have been developed to efficiently fine-tune those models for personalization, such as Low Rank Adaptation (LoRA). However, LoRA-based methods often face the challenge of adjusting the rank parameter to achieve satisfactory results. To address this challenge, AutoComponent-LoRA (AC-LoRA) is proposed, which is able to automatically separate the signal component and noise component of the LoRA matrices for fast and efficient personalized artistic style image generation. This method is based on Singular Value Decomposition (SVD) and dynamic heuristics to update the hyperparameters during training. Superior performance over existing methods in overcoming model underfitting or overfitting problems is demonstrated. The results were validated using FID, CLIP, DINO, and ImageReward, achieving an average of 9% improvement.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02231', 57)">Copy Link</button>
<div id="copy-message-57" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02234">LLM Social Simulations Are a Promising Research Method</a></h1>
<p><b>Authors:</b> Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein</p>
<p>Abstract: Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these methods. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-tuning, and complementary methods. We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing. More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02234', 58)">Copy Link</button>
<div id="copy-message-58" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02252">Adapting World Models with Latent-State Dynamics Residuals</a></h1>
<p><b>Authors:</b> JB Lanier, Kyungmin Kim, Armin Karamzade, Yifei Liu, Ankita Sinha, Kat He, Davide Corsi, Roy Fox</p>
<p>Abstract: Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance. A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02252', 59)">Copy Link</button>
<div id="copy-message-59" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02254">LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks</a></h1>
<p><b>Authors:</b> Seunghyun Yoo</p>
<p>Abstract: Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game "Connections", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02254', 60)">Copy Link</button>
<div id="copy-message-60" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02260">Implicit Neural Differential Model for Spatiotemporal Dynamics</a></h1>
<p><b>Authors:</b> Deepak Akhare, Pan Du, Tengfei Luo, Jian-Xun Wang</p>
<p>Abstract: Hybrid neural-physics modeling frameworks through differentiable programming have emerged as powerful tools in scientific machine learning, enabling the integration of known physics with data-driven learning to improve prediction accuracy and generalizability. However, most existing hybrid frameworks rely on explicit recurrent formulations, which suffer from numerical instability and error accumulation during long-horizon forecasting. In this work, we introduce Im-PiNDiff, a novel implicit physics-integrated neural differentiable solver for stable and accurate modeling of spatiotemporal dynamics. Inspired by deep equilibrium models, Im-PiNDiff advances the state using implicit fixed-point layers, enabling robust long-term simulation while remaining fully end-to-end differentiable. To enable scalable training, we introduce a hybrid gradient propagation strategy that integrates adjoint-state methods with reverse-mode automatic differentiation. This approach eliminates the need to store intermediate solver states and decouples memory complexity from the number of solver iterations, significantly reducing training overhead. We further incorporate checkpointing techniques to manage memory in long-horizon rollouts. Numerical experiments on various spatiotemporal PDE systems, including advection-diffusion processes, Burgers' dynamics, and multi-physics chemical vapor infiltration processes, demonstrate that Im-PiNDiff achieves superior predictive performance, enhanced numerical stability, and substantial reductions in memory and runtime cost relative to explicit and naive implicit baselines. This work provides a principled, efficient, and scalable framework for hybrid neural-physics modeling.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02260', 61)">Copy Link</button>
<div id="copy-message-61" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02277">Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation</a></h1>
<p><b>Authors:</b> Amit Rand, Hadi Ibrahim</p>
<p>Abstract: Medical imaging, particularly X-ray analysis, often involves detecting multiple conditions simultaneously within a single scan, making multi-label classification crucial for real-world clinical applications. We present the Medical X-ray Attention (MXA) block, a novel attention mechanism tailored specifically to address the unique challenges of X-ray abnormality detection. The MXA block enhances traditional Multi-Head Self Attention (MHSA) by integrating a specialized module that efficiently captures both detailed local information and broader global context. To the best of our knowledge, this is the first work to propose a task-specific attention mechanism for diagnosing chest X-rays, as well as to attempt multi-label classification using an Efficient Vision Transformer (EfficientViT). By embedding the MXA block within the EfficientViT architecture and employing knowledge distillation, our proposed model significantly improves performance on the CheXpert dataset, a widely used benchmark for multi-label chest X-ray abnormality detection. Our approach achieves an area under the curve (AUC) of 0.85, an absolute improvement of 0.19 compared to our baseline model's AUC of 0.66, corresponding to a substantial approximate 233% relative improvement over random guessing (AUC = 0.5).</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02277', 62)">Copy Link</button>
<div id="copy-message-62" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02285">Tree-based Models for Vertical Federated Learning: A Survey</a></h1>
<p><b>Authors:</b> Bingchen Qian, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou</p>
<p>Abstract: Tree-based models have achieved great success in a wide range of real-world applications due to their effectiveness, robustness, and interpretability, which inspired people to apply them in vertical federated learning (VFL) scenarios in recent years. In this paper, we conduct a comprehensive study to give an overall picture of applying tree-based models in VFL, from the perspective of their communication and computation protocols. We categorize tree-based models in VFL into two types, i.e., feature-gathering models and label-scattering models, and provide a detailed discussion regarding their characteristics, advantages, privacy protection mechanisms, and applications. This study also focuses on the implementation of tree-based models in VFL, summarizing several design principles for better satisfying various requirements from both academic research and industrial deployment. We conduct a series of experiments to provide empirical observations on the differences and advances of different types of tree-based models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02285', 63)">Copy Link</button>
<div id="copy-message-63" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02293">State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla</a></h1>
<p><b>Authors:</b> Sharif Md. Abdullah, Abhijit Paul, Shebuti Rayana, Ahmedul Kabir, Zarif Masud</p>
<p>Abstract: Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language (BdSL) remains a understudied domain. Specifically, there are no works on Bangla text-to-gloss translation task. To address this gap, we begin by addressing the dataset problem. We take inspiration from grammatical rule based gloss generation used in Germany and American sign langauage (ASL) and adapt it for BdSL. We also leverage LLM to generate synthetic data and use back-translation, text generation for data augmentation. With dataset prepared, we started experimentation. We fine-tuned pretrained mBART-50 and mBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a novel seq-to-seq model with multi-head attention. We observe significant high performance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual model from Facebook. We then explored why we observe such high performance with mBART. We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data. And as we know, gloss form has shuffling property. So we hypothesize that mBART is inherently good at text-to-gloss tasks. To find support against this hypothesis, we trained mBART-50 on PHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50 finetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark, far outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 = 55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on the results, this study proposes a new paradigm for text-to-gloss task using mBART models. Additionally, our results show that BdSL text-to-gloss task can greatly benefit from rule-based synthetic dataset.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02293', 64)">Copy Link</button>
<div id="copy-message-64" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02312">OmniCam: Unified Multimodal Video Generation via Camera Control</a></h1>
<p><b>Authors:</b> Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, Cewu Lu, Lixin Yang</p>
<p>Abstract: Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities: the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02312', 65)">Copy Link</button>
<div id="copy-message-65" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02316">ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation</a></h1>
<p><b>Authors:</b> Yuan Zhou, Shilong Jin, Litao Hua, Wanjun Lv, Haoran Duan, Jungong Han</p>
<p>Abstract: Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel framework that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise camera parameters; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer effectively mitigates the multi-face Janus problem in text-to-3D generation, outperforming existing methods in both visual quality and consistency.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02316', 66)">Copy Link</button>
<div id="copy-message-66" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02317">Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation</a></h1>
<p><b>Authors:</b> Ye Su, Hezhe Qiao, Di Wu, Yuwen Chen, Lin Chen</p>
<p>Abstract: The imputation of the Multivariate time series (MTS) is particularly challenging since the MTS typically contains irregular patterns of missing values due to various factors such as instrument failures, interference from irrelevant data, and privacy regulations. Existing statistical methods and deep learning methods have shown promising results in time series imputation. In this paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order MTS imputation. The key idea is to leverage the Gaussian Copula to explore the cross-variable and temporal relationships based on the latent Gaussian representation. Subsequently, we employ an Expectation-Maximization (EM) algorithm to improve robustness in managing data with varying missing rates. Comprehensive experiments were conducted on three real-world MTS datasets. The results demonstrate that our TGC substantially outperforms the state-of-the-art imputation methods. Additionally, the TGC model exhibits stronger robustness to the varying missing ratios in the test dataset. Our code is available at https://github.com/MVL-Lab/TGC-MTS.</p>
<p>URLs: <a href="https://github.com/MVL-Lab/TGC-MTS.">https://github.com/MVL-Lab/TGC-MTS.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02317, https://github.com/MVL-Lab/TGC-MTS.', 67)">Copy Link</button>
<div id="copy-message-67" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02351">Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation</a></h1>
<p><b>Authors:</b> Chengxi Zeng, Yuxuan Jiang, Fan Zhang, Alberto Gambaruto, Tilo Burghardt</p>
<p>Abstract: The deployment of foundation models for medical imaging has demonstrated considerable success. However, their training overheads associated with downstream tasks remain substantial due to the size of the image encoders employed, and the inference complexity is also significantly high. Although lightweight variants have been obtained for these foundation models, their performance is constrained by their limited model capacity and suboptimal training strategies. In order to achieve an improved tradeoff between complexity and performance, we propose a new framework to improve the performance of low complexity models via knowledge distillation from multiple large medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each specializing in different vision tasks, with the goal to effectively bridge the performance gap for medical image segmentation tasks. The agglomerated model demonstrates superior generalization across 12 segmentation tasks, whereas specialized models require explicit training for each task. Our approach achieved an average performance gain of 2\% in Dice coefficient compared to simple distillation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02351', 68)">Copy Link</button>
<div id="copy-message-68" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02382">Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge</a></h1>
<p><b>Authors:</b> Yudi Sang, Yanzhen Liu, Sutuke Yibulayimu, Yunning Wang, Benjamin D. Killeen, Mingxu Liu, Ping-Cheng Ku, Ole Johannsen, Karol Gotkowski, Maximilian Zenk, Klaus Maier-Hein, Fabian Isensee, Peiyan Yue, Yi Wang, Haidong Yu, Zhaohong Pan, Yutong He, Xiaokun Liang, Daiqi Liu, Fuxin Fan, Artur Jurgas, Andrzej Skalski, Yuxi Ma, Jing Yang, Szymon P{\l}otka, Rafa{\l} Litka, Gang Zhu, Yingchun Song, Mathias Unberath, Mehran Armand, Dan Ruan, S. Kevin Zhou, Qiyong Cao, Chunpeng Zhao, Xinbao Wu, Yu Wang</p>
<p>Abstract: The segmentation of pelvic fracture fragments in CT and X-ray images is crucial for trauma diagnosis, surgical planning, and intraoperative guidance. However, accurately and efficiently delineating the bone fragments remains a significant challenge due to complex anatomy and imaging limitations. The PENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance automated fracture segmentation by benchmarking state-of-the-art algorithms on these complex tasks. A diverse dataset of 150 CT scans was collected from multiple clinical centers, and a large set of simulated X-ray images was generated using the DeepDRR method. Final submissions from 16 teams worldwide were evaluated under a rigorous multi-metric testing scheme. The top-performing CT algorithm achieved an average fragment-wise intersection over union (IoU) of 0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the best algorithm attained an IoU of 0.774, highlighting the greater challenges posed by overlapping anatomical structures. Beyond the quantitative evaluation, the challenge revealed methodological diversity in algorithm design. Variations in instance representation, such as primary-secondary classification versus boundary-core separation, led to differing segmentation strategies. Despite promising results, the challenge also exposed inherent uncertainties in fragment definition, particularly in cases of incomplete fractures. These findings suggest that interactive segmentation approaches, integrating human decision-making with task-relevant information, may be essential for improving model reliability and clinical applicability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02382', 69)">Copy Link</button>
<div id="copy-message-69" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02388">Steiner Traveling Salesman Problem with Quantum Annealing</a></h1>
<p><b>Authors:</b> Alessia Ciacco, Francesca Guerriero, Eneko Osaba</p>
<p>Abstract: The Steiner Traveling Salesman Problem (STSP) is a variant of the classical Traveling Salesman Problem. The STSP involves incorporating steiner nodes, which are extra nodes not originally part of the required visit set but that can be added to the route to enhance the overall solution and minimize the total travel cost. Given the NP-hard nature of the STSP, we propose a quantum approach to address it. Specifically, we employ quantum annealing using D-Wave's hardware to explore its potential for solving this problem. To enhance computational feasibility, we develop a preprocessing method that effectively reduces the network size. Our experimental results demonstrate that this reduction technique significantly decreases the problem complexity, making the Quadratic Unconstrained Binary Optimization formulation, the standard input for quantum annealers, better suited for existing quantum hardware. Furthermore, the results highlight the potential of quantum annealing as a promising and innovative approach for solving the STSP.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02388', 70)">Copy Link</button>
<div id="copy-message-70" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02402">EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling</a></h1>
<p><b>Authors:</b> Hao Yin, Shi Guo, Xu Jia, Xudong XU, Lu Zhang, Si Liu, Dong Wang, Huchuan Lu, Tianfan Xue</p>
<p>Abstract: When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02402', 71)">Copy Link</button>
<div id="copy-message-71" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02408">Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence</a></h1>
<p><b>Authors:</b> Naomi Silverstein, Efrat Leibowitz, Ron Beloosesky, Haim Azhari</p>
<p>Abstract: Ultrasound is a widely accessible and cost-effective medical imaging tool commonly used for prenatal evaluation of the fetal brain. However, it has limitations, particularly in the third trimester, where the complexity of the fetal brain requires high image quality for extracting quantitative data. In contrast, magnetic resonance imaging (MRI) offers superior image quality and tissue differentiation but is less available, expensive, and requires time-consuming acquisition. Thus, transforming ultrasonic images into an MRI-mimicking display may be advantageous and allow better tissue anatomy presentation. To address this goal, we have examined the use of artificial intelligence, implementing a diffusion model renowned for generating high-quality images. The proposed method, termed "Dual Diffusion Imposed Correlation" (DDIC), leverages a diffusion-based translation methodology, assuming a shared latent space between ultrasound and MRI domains. Model training was obtained utilizing the "HC18" dataset for ultrasound and the "CRL fetal brain atlas" along with the "FeTA " datasets for MRI. The generated pseudo-MRI images provide notable improvements in visual discrimination of brain tissue, especially in the lateral ventricles and the Sylvian fissure, characterized by enhanced contrast clarity. Improvement was demonstrated in Mutual information, Peak signal-to-noise ratio, Fr\'echet Inception Distance, and Contrast-to-noise ratio. Findings from these evaluations indicate statistically significant superior performance of the DDIC compared to other translation methodologies. In addition, a Medical Opinion Test was obtained from 5 gynecologists. The results demonstrated display improvement in 81% of the tested images. In conclusion, the presented pseudo-MRI images hold the potential for streamlining diagnosis and enhancing clinical outcomes through improved representation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02408', 72)">Copy Link</button>
<div id="copy-message-72" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02417">Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering</a></h1>
<p><b>Authors:</b> Lili Liang, Guanglu Sun</p>
<p>Abstract: Video Question Answering (VideoQA) is an important research direction in the field of artificial intelligence, enabling machines to understand video content and perform reasoning and answering based on natural language questions. Although methods based on static relationship reasoning have made certain progress, there are still deficiencies in the accuracy of static relationship recognition and representation, and they have not fully utilized the static relationship information in videos for in-depth reasoning and analysis. Therefore, this paper proposes a reasoning method for intra-type and inter-type message passing based on static relationships. This method constructs a dual graph for intra-type message passing reasoning and builds a heterogeneous graph based on static relationships for inter-type message passing reasoning. The intra-type message passing reasoning model captures the neighborhood information of targets and relationships related to the question in the dual graph, updating the dual graph to obtain intra-type clues for answering the question. The inter-type message passing reasoning model captures the neighborhood information of targets and relationships from different categories related to the question in the heterogeneous graph, updating the heterogeneous graph to obtain inter-type clues for answering the question. Finally, the answers are inferred by combining the intra-type and inter-type clues based on static relationships. Experimental results on the ANetQA and Next-QA datasets demonstrate the effectiveness of this method.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02417', 73)">Copy Link</button>
<div id="copy-message-73" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02438">Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation</a></h1>
<p><b>Authors:</b> Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan</p>
<p>Abstract: Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLaMP, a hierarchical video-language model that processes hour-long videos at ``mixed precision'' through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLaMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLaMP's superior performance across four video understanding benchmarks, particularly on long-form content. Notably, ViLaMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02438', 74)">Copy Link</button>
<div id="copy-message-74" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02441">Cognitive Memory in Large Language Models</a></h1>
<p><b>Authors:</b> Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu</p>
<p>Abstract: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02441', 75)">Copy Link</button>
<div id="copy-message-75" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02450">CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles</a></h1>
<p><b>Authors:</b> Jingyi Wang, Duanfeng Chu, Zejian Deng, Liping Lu</p>
<p>Abstract: To address the current challenges of low intelligence and simplistic vehicle behavior modeling in autonomous driving simulation scenarios, this paper proposes the Cognitive Hierarchical Agent with Reasoning and Motion Styles (CHARMS). The model can reason about the behavior of other vehicles like a human driver and respond with different decision-making styles, thereby improving the intelligence and diversity of the surrounding vehicles in the driving scenario. By introducing the Level-k behavioral game theory, the paper models the decision-making process of human drivers and employs deep reinforcement learning to train the models with diverse decision styles, simulating different reasoning approaches and behavioral characteristics. Building on the Poisson cognitive hierarchy theory, this paper also presents a novel driving scenario generation method. The method controls the proportion of vehicles with different driving styles in the scenario using Poisson and binomial distributions, thus generating controllable and diverse driving environments. Experimental results demonstrate that CHARMS not only exhibits superior decision-making capabilities as ego vehicles, but also generates more complex and diverse driving scenarios as surrounding vehicles. We will release code for CHARMS at https://github.com/WUTAD-Wjy/CHARMS.</p>
<p>URLs: <a href="https://github.com/WUTAD-Wjy/CHARMS.">https://github.com/WUTAD-Wjy/CHARMS.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02450, https://github.com/WUTAD-Wjy/CHARMS.', 76)">Copy Link</button>
<div id="copy-message-76" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02458">Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation</a></h1>
<p><b>Authors:</b> Liangbo Ning, Wenqi Fan, Qing Li</p>
<p>Abstract: Recently, Large Language Model (LLM)-empowered recommender systems have revolutionized personalized recommendation frameworks and attracted extensive attention. Despite the remarkable success, existing LLM-empowered RecSys have been demonstrated to be highly vulnerable to minor perturbations. To mitigate the negative impact of such vulnerabilities, one potential solution is to employ collaborative signals based on item-item co-occurrence to purify the malicious collaborative knowledge from the user's historical interactions inserted by attackers. On the other hand, due to the capabilities to expand insufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG) techniques provide unprecedented opportunities to enhance the robustness of LLM-empowered recommender systems by introducing external collaborative knowledge. Therefore, in this paper, we propose a novel framework (RETURN) by retrieving external collaborative signals to purify the poisoned user profiles and enhance the robustness of LLM-empowered RecSys in a plug-and-play manner. Specifically, retrieval-augmented perturbation positioning is proposed to identify potential perturbations within the users' historical sequences by retrieving external knowledge from collaborative item graphs. After that, we further retrieve the collaborative knowledge to cleanse the perturbations by using either deletion or replacement strategies and introduce a robust ensemble recommendation strategy to generate final robust predictions. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed RETURN.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02458', 77)">Copy Link</button>
<div id="copy-message-77" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02461">Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness</a></h1>
<p><b>Authors:</b> Juliett Su\'arez Ferreira, Marija Slavkovik, Jorge Casillas</p>
<p>Abstract: Current fairness metrics and mitigation techniques provide tools for practitioners to asses how non-discriminatory Automatic Decision Making (ADM) systems are. What if I, as an individual facing a decision taken by an ADM system, would like to know: Am I being treated fairly? We explore how to create the affordance for users to be able to ask this question of ADM. In this paper, we argue for the reification of fairness not only as a property of ADM, but also as an epistemic right of an individual to acquire information about the decisions that affect them and use that information to contest and seek effective redress against those decisions, in case they are proven to be discriminatory. We examine key concepts from existing research not only in algorithmic fairness but also in explainable artificial intelligence, accountability, and contestability. Integrating notions from these domains, we propose a conceptual framework to ascertain fairness by combining different tools that empower the end-users of ADM systems. Our framework shifts the focus from technical solutions aimed at practitioners to mechanisms that enable individuals to understand, challenge, and verify the fairness of decisions, and also serves as a blueprint for organizations and policymakers, bridging the gap between technical requirements and practical, user-centered accountability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02461', 78)">Copy Link</button>
<div id="copy-message-78" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02463">Evaluating AI Recruitment Sourcing Tools by Human Preference</a></h1>
<p><b>Authors:</b> Vladimir Slaykovskiy, Maksim Zvegintsev, Yury Sakhonchyk, Hrachik Ajamian</p>
<p>Abstract: This study introduces a benchmarking methodology designed to evaluate the performance of AI-driven recruitment sourcing tools. We created and utilized a dataset to perform a comparative analysis of search results generated by leading AI-based solutions, LinkedIn Recruiter, and our proprietary system, Pearch.ai. Human experts assessed the relevance of the returned candidates, and an Elo rating system was applied to quantitatively measure each tool's comparative performance. Our findings indicate that AI-driven recruitment sourcing tools consistently outperform LinkedIn Recruiter in candidate relevance, with Pearch.ai achieving the highest performance scores. Furthermore, we found a strong alignment between AI-based evaluations and human judgments, highlighting the potential for advanced AI technologies to substantially enhance talent acquisition effectiveness. Code and supporting data are publicly available at https://github.com/vslaykovsky/ai-sourcing-benchmark</p>
<p>URLs: <a href="https://github.com/vslaykovsky/ai-sourcing-benchmark">https://github.com/vslaykovsky/ai-sourcing-benchmark</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02463, https://github.com/vslaykovsky/ai-sourcing-benchmark', 79)">Copy Link</button>
<div id="copy-message-79" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02464">CornerPoint3D: Look at the Nearest Corner Instead of the Center</a></h1>
<p><b>Authors:</b> Ruixiao Zhang, Runwei Guan, Xiangyu Chen, Adam Prugel-Bennett, Xiaohao Cai</p>
<p>Abstract: 3D object detection aims to predict object centers, dimensions, and rotations from LiDAR point clouds. Despite its simplicity, LiDAR captures only the near side of objects, making center-based detectors prone to poor localization accuracy in cross-domain tasks with varying point distributions. Meanwhile, existing evaluation metrics designed for single-domain assessment also suffer from overfitting due to dataset-specific size variations. A key question arises: Do we really need models to maintain excellent performance in the entire 3D bounding boxes after being applied across domains? Actually, one of our main focuses is on preventing collisions between vehicles and other obstacles, especially in cross-domain scenarios where correctly predicting the sizes is much more difficult. To address these issues, we rethink cross-domain 3D object detection from a practical perspective. We propose two new metrics that evaluate a model's ability to detect objects' closer-surfaces to the LiDAR sensor. Additionally, we introduce EdgeHead, a refinement head that guides models to focus more on learnable closer surfaces, significantly improving cross-domain performance under both our new and traditional BEV/3D metrics. Furthermore, we argue that predicting the nearest corner rather than the object center enhances robustness. We propose a novel 3D object detector, coined as CornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise the learning and detection of the nearest corner of each object. Our proposed methods realize a balanced trade-off between the detection quality of entire bounding boxes and the locating accuracy of closer surfaces to the LiDAR sensor, outperforming the traditional center-based detector CenterPoint in multiple cross-domain tasks and providing a more practically reasonable and robust cross-domain 3D object detection solution.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02464', 80)">Copy Link</button>
<div id="copy-message-80" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02479">Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets</a></h1>
<p><b>Authors:</b> Stefano Covone, Italo Napolitano, Francesco De Lellis, Mario di Bernardo</p>
<p>Abstract: We propose a decentralized reinforcement learning solution for multi-agent shepherding of non-cohesive targets using policy-gradient methods. Our architecture integrates target-selection with target-driving through Proximal Policy Optimization, overcoming discrete-action constraints of previous Deep Q-Network approaches and enabling smoother agent trajectories. This model-free framework effectively solves the shepherding problem without prior dynamics knowledge. Experiments demonstrate our method's effectiveness and scalability with increased target numbers and limited sensing capabilities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02479', 81)">Copy Link</button>
<div id="copy-message-81" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02480">Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging</a></h1>
<p><b>Authors:</b> Kyungmin Choi, JaKeoung Koo, Stephen McLaughlin, Abderrahim Halimi</p>
<p>Abstract: Single-photon Lidar imaging offers a significant advantage in 3D imaging due to its high resolution and long-range capabilities, however it is challenging to apply in noisy environments with multiple targets per pixel. To tackle these challenges, several methods have been proposed. Statistical methods demonstrate interpretability on the inferred parameters, but they are often limited in their ability to handle complex scenes. Deep learning-based methods have shown superior performance in terms of accuracy and robustness, but they lack interpretability or they are limited to a single-peak per pixel. In this paper, we propose a deep unrolling algorithm for dual-peak single-photon Lidar imaging. We introduce a hierarchical Bayesian model for multiple targets and propose a neural network that unrolls the underlying statistical method. To support multiple targets, we adopt a dual depth maps representation and exploit geometric deep learning to extract features from the point cloud. The proposed method takes advantages of statistical methods and learning-based methods in terms of accuracy and quantifying uncertainty. The experimental results on synthetic and real data demonstrate the competitive performance when compared to existing methods, while also providing uncertainty information.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02480', 82)">Copy Link</button>
<div id="copy-message-82" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02492">Industrial Internet Robot Collaboration System and Edge Computing Optimization</a></h1>
<p><b>Authors:</b> Qian Zuo, Dajun Tao, Tian Qi, Jieyi Xie, Zijie Zhou, Zhen Tian, Yu Mingyu</p>
<p>Abstract: In a complex environment, for a mobile robot to safely and collision - free avoid all obstacles, it poses high requirements for its intelligence level. Given that the information such as the position and geometric characteristics of obstacles is random, the control parameters of the robot, such as velocity and angular velocity, are also prone to random deviations. To address this issue in the framework of the Industrial Internet Robot Collaboration System, this paper proposes a global path control scheme for mobile robots based on deep learning. First of all, the dynamic equation of the mobile robot is established. According to the linear velocity and angular velocity of the mobile robot, its motion behaviors are divided into obstacle - avoidance behavior, target - turning behavior, and target approaching behavior. Subsequently, the neural network method in deep learning is used to build a global path planning model for the robot. On this basis, a fuzzy controller is designed with the help of a fuzzy control algorithm to correct the deviations that occur during path planning, thereby achieving optimized control of the robot's global path. In addition, considering edge computing optimization, the proposed model can process local data at the edge device, reducing the communication burden between the robot and the central server, and improving the real time performance of path planning. The experimental results show that for the mobile robot controlled by the research method in this paper, the deviation distance of the path angle is within 5 cm, the deviation convergence can be completed within 10 ms, and the planned path is shorter. This indicates that the proposed scheme can effectively improve the global path planning ability of mobile robots in the industrial Internet environment and promote the collaborative operation of robots through edge computing optimization.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02492', 83)">Copy Link</button>
<div id="copy-message-83" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02495">Inference-Time Scaling for Generalist Reward Modeling</a></h1>
<p><b>Authors:</b> Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu</p>
<p>Abstract: Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02495', 84)">Copy Link</button>
<div id="copy-message-84" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02512">Towards Generalizing Temporal Action Segmentation to Unseen Views</a></h1>
<p><b>Authors:</b> Emad Bahrami, Olga Zatsarynna, Gianpiero Francesca, Juergen Gall</p>
<p>Abstract: While there has been substantial progress in temporal action segmentation, the challenge to generalize to unseen views remains unaddressed. Hence, we define a protocol for unseen view action segmentation where camera views for evaluating the model are unavailable during training. This includes changing from top-frontal views to a side view or even more challenging from exocentric to egocentric views. Furthermore, we present an approach for temporal action segmentation that tackles this challenge. Our approach leverages a shared representation at both the sequence and segment levels to reduce the impact of view differences during training. We achieve this by introducing a sequence loss and an action loss, which together facilitate consistent video and action representations across different views. The evaluation on the Assembly101, IkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a 12.8% increase in F1@50 for unseen exocentric views and a substantial 54% improvement for unseen egocentric views.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02512', 85)">Copy Link</button>
<div id="copy-message-85" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02526">Improving User Experience with FAICO: Towards a Framework for AI Communication in Human-AI Co-Creativity</a></h1>
<p><b>Authors:</b> Jeba Rezwana, Corey Ford</p>
<p>Abstract: How AI communicates with humans is crucial for effective human-AI co-creation. However, many existing co-creative AI tools cannot communicate effectively, limiting their potential as collaborators. This paper introduces our initial design of a Framework for designing AI Communication (FAICO) for co-creative AI based on a systematic review of 107 full-length papers. FAICO presents key aspects of AI communication and their impacts on user experience to guide the design of effective AI communication. We then show actionable ways to translate our framework into two practical tools: design cards for designers and a configuration tool for users. The design cards enable designers to consider AI communication strategies that cater to a diverse range of users in co-creative contexts, while the configuration tool empowers users to customize AI communication based on their needs and creative workflows. This paper contributes new insights within the literature on human-AI co-creativity and Human-Computer Interaction, focusing on designing AI communication to enhance user experience.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02526', 86)">Copy Link</button>
<div id="copy-message-86" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02544">Fourier Sliced-Wasserstein Embedding for Multisets and Measures</a></h1>
<p><b>Authors:</b> Tal Amir, Nadav Dym</p>
<p>Abstract: We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to embed multisets and measures over $\mathbb{R}^d$ into Euclidean space.
  Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures and bi-Lipschitz on multisets - a significant advantage over prevalent methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and, in many cases, not even injective. The required output dimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is the maximal input multiset size.
  Furthermore, we prove that it is impossible to embed distributions over $\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best possible.
  Through numerical experiments, we demonstrate that our method yields superior multiset representations that improve performance in practical learning tasks. Specifically, we show that (a) a simple combination of the FSW embedding with an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance; and (b) replacing max-pooling with the FSW embedding makes PointNet significantly more robust to parameter reduction, with only minor performance degradation even after a 40-fold reduction.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02544', 87)">Copy Link</button>
<div id="copy-message-87" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02546">GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning</a></h1>
<p><b>Authors:</b> Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</p>
<p>Abstract: Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. As illustrated in our paper, by eliminating both the critic and reference models, and avoiding KL divergence constraints, our approach significantly simplifies the training process when compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. Extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG.</p>
<p>URLs: <a href="https://github.com/AMAP-ML/GPG.">https://github.com/AMAP-ML/GPG.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02546, https://github.com/AMAP-ML/GPG.', 88)">Copy Link</button>
<div id="copy-message-88" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02558">Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results</a></h1>
<p><b>Authors:</b> Andrei Dumitriu, Florin Tatui, Florin Miron, Radu Tudor Ionescu, Radu Timofte</p>
<p>Abstract: Rip currents are the leading cause of fatal accidents and injuries on many beaches worldwide, emphasizing the importance of automatically detecting these hazardous surface water currents. In this paper, we address a novel task: rip current instance segmentation. We introduce a comprehensive dataset containing $2,466$ images with newly created polygonal annotations for instance segmentation, used for training and validation. Additionally, we present a novel dataset comprising $17$ drone videos (comprising about $24K$ frames) captured at $30 FPS$, annotated with both polygons for instance segmentation and bounding boxes for object detection, employed for testing purposes. We train various versions of YOLOv8 for instance segmentation on static images and assess their performance on the test dataset (videos). The best results were achieved by the YOLOv8-nano model (runnable on a portable device), with an mAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the test dataset. The results provide a baseline for future research in rip current segmentation. Our work contributes to the existing literature by introducing a detailed, annotated dataset, and training a deep learning model for instance segmentation of rip currents. The code, training details and the annotated dataset are made publicly available at https://github.com/Irikos/rip_currents.</p>
<p>URLs: <a href="https://github.com/Irikos/rip_currents.">https://github.com/Irikos/rip_currents.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02558, https://github.com/Irikos/rip_currents.', 89)">Copy Link</button>
<div id="copy-message-89" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02586">Deep learning for music generation. Four approaches and their comparative evaluation</a></h1>
<p><b>Authors:</b> Razvan Paroiu, Stefan Trausan-Matu</p>
<p>Abstract: This paper introduces four different artificial intelligence algorithms for music generation and aims to compare these methods not only based on the aesthetic quality of the generated music but also on their suitability for specific applications. The first set of melodies is produced by a slightly modified visual transformer neural network that is used as a language model. The second set of melodies is generated by combining chat sonification with a classic transformer neural network (the same method of music generation is presented in a previous research), the third set of melodies is generated by combining the Schillinger rhythm theory together with a classic transformer neural network, and the fourth set of melodies is generated using GPT3 transformer provided by OpenAI. A comparative analysis is performed on the melodies generated by these approaches and the results indicate that significant differences can be observed between them and regarding the aesthetic value of them, GPT3 produced the most pleasing melodies, and the newly introduced Schillinger method proved to generate better sounding music than previous sonification methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02586', 90)">Copy Link</button>
<div id="copy-message-90" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02589">Knowledge Graph Completion with Mixed Geometry Tensor Factorization</a></h1>
<p><b>Authors:</b> Viacheslav Yusupov, Maxim Rakhuba, Evgeny Frolov</p>
<p>Abstract: In this paper, we propose a new geometric approach for knowledge graph completion via low rank tensor approximation. We augment a pretrained and well-established Euclidean model based on a Tucker tensor decomposition with a novel hyperbolic interaction term. This correction enables more nuanced capturing of distributional properties in data better aligned with real-world knowledge graphs. By combining two geometries together, our approach improves expressivity of the resulting model achieving new state-of-the-art link prediction accuracy with a significantly lower number of parameters compared to the previous Euclidean and hyperbolic models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02589', 91)">Copy Link</button>
<div id="copy-message-91" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02605">Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</a></h1>
<p><b>Authors:</b> Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, Liang Xiang</p>
<p>Abstract: The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02605', 92)">Copy Link</button>
<div id="copy-message-92" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02606">Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification</a></h1>
<p><b>Authors:</b> Jonas Teufel, Annika Leinweber, Pascal Friederich</p>
<p>Abstract: Explainable AI (xAI) interventions aim to improve interpretability for complex black-box models, not only to improve user trust but also as a means to extract scientific insights from high-performing predictive systems. In molecular property prediction, counterfactual explanations offer a way to understand predictive behavior by highlighting which minimal perturbations in the input molecular structure cause the greatest deviation in the predicted property. However, such explanations only allow for meaningful scientific insights if they reflect the distribution of the true underlying property -- a feature we define as counterfactual truthfulness. To increase this truthfulness, we propose the integration of uncertainty estimation techniques to filter counterfactual candidates with high predicted uncertainty. Through computational experiments with synthetic and real-world datasets, we demonstrate that traditional uncertainty estimation methods, such as ensembles and mean-variance estimation, can already substantially reduce the average prediction error and increase counterfactual truthfulness, especially for out-of-distribution settings. Our results highlight the importance and potential impact of incorporating uncertainty estimation into explainability methods, especially considering the relatively high effectiveness of low-effort interventions like model ensembles.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02606', 93)">Copy Link</button>
<div id="copy-message-93" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02607">Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks</a></h1>
<p><b>Authors:</b> Samuel Tesfazgi, Leonhard Sprandl, Sandra Hirche</p>
<p>Abstract: The practical deployment of learning-based autonomous systems would greatly benefit from tools that flexibly obtain safety guarantees in the form of certificate functions from data. While the geometrical properties of such certificate functions are well understood, synthesizing them using machine learning techniques still remains a challenge. To mitigate this issue, we propose a diffeomorphic function learning framework where prior structural knowledge of the desired output is encoded in the geometry of a simple surrogate function, which is subsequently augmented through an expressive, topology-preserving state-space transformation. Thereby, we achieve an indirect function approximation framework that is guaranteed to remain in the desired hypothesis space. To this end, we introduce a novel approach to construct diffeomorphic maps based on RBF networks, which facilitate precise, local transformations around data. Finally, we demonstrate our approach by learning diffeomorphic Lyapunov functions from real-world data and apply our method to different attractor systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02607', 94)">Copy Link</button>
<div id="copy-message-94" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02620">Efficient Model Editing with Task-Localized Sparse Fine-tuning</a></h1>
<p><b>Authors:</b> Leonardo Iurada, Marco Ciccone, Tatiana Tommasi</p>
<p>Abstract: Task arithmetic has emerged as a promising approach for editing models by representing task-specific knowledge as composable task vectors. However, existing methods rely on network linearization to derive task vectors, leading to computational bottlenecks during training and inference. Moreover, linearization alone does not ensure weight disentanglement, the key property that enables conflict-free composition of task vectors. To address this, we propose TaLoS which allows to build sparse task vectors with minimal interference without requiring explicit linearization and sharing information across tasks. We find that pre-trained models contain a subset of parameters with consistently low gradient sensitivity across tasks, and that sparsely updating only these parameters allows for promoting weight disentanglement during fine-tuning. Our experiments prove that TaLoS improves training and inference efficiency while outperforming current methods in task addition and negation. By enabling modular parameter editing, our approach fosters practical deployment of adaptable foundation models in real-world applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02620', 95)">Copy Link</button>
<div id="copy-message-95" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02646">Prompt Optimization with Logged Bandit Data</a></h1>
<p><b>Authors:</b> Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims</p>
<p>Abstract: We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we propose a novel kernel-based off-policy gradient method, which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02646', 96)">Copy Link</button>
<div id="copy-message-96" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02685">STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability</a></h1>
<p><b>Authors:</b> Iv\'an Sevillano-Garc\'ia, Juli\'an Luengo, Francisco Herrera</p>
<p>Abstract: Out-of-Distribution (OOD) detection is a critical task in machine learning, particularly in safety-sensitive applications where model failures can have serious consequences. However, current OOD detection methods often suffer from restrictive distributional assumptions, limited scalability, and a lack of interpretability. To address these challenges, we propose STOOD-X, a two-stage methodology that combines a Statistical nonparametric Test for OOD Detection with eXplainability enhancements. In the first stage, STOOD-X uses feature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD samples without assuming a specific feature distribution. In the second stage, it generates user-friendly, concept-based visual explanations that reveal the features driving each decision, aligning with the BLUE XAI paradigm. Through extensive experiments on benchmark datasets and multiple architectures, STOOD-X achieves competitive performance against state-of-the-art post hoc OOD detectors, particularly in high-dimensional and complex settings. In addition, its explainability framework enables human oversight, bias detection, and model debugging, fostering trust and collaboration between humans and AI systems. The STOOD-X methodology therefore offers a robust, explainable, and scalable solution for real-world OOD detection tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02685', 97)">Copy Link</button>
<div id="copy-message-97" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02698">SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions</a></h1>
<p><b>Authors:</b> Shengrui XU, Tianchi Lu, Zikun Wang, Jixiu Zhai, Jingwan Wang</p>
<p>Abstract: Protein-Protein Interaction (PPI) prediction is a key task in uncovering cellular functional networks and disease mechanisms. However, traditional experimental methods are time-consuming and costly, and existing computational models face challenges in cross-modal feature fusion, robustness, and false-negative suppression. In this paper, we propose a novel supervised contrastive multimodal framework, SCMPPI, for PPI prediction. By integrating protein sequence features (AAC, DPC, CKSAAP-ESMC) with PPI network topology information (Node2Vec graph embedding), and combining an improved supervised contrastive learning strategy, SCMPPI significantly enhances PPI prediction performance. For the PPI task, SCMPPI introduces a negative sample filtering mechanism and modifies the contrastive loss function, effectively optimizing multimodal features. Experiments on eight benchmark datasets, including yeast, human, and H.pylori, show that SCMPPI outperforms existing state-of-the-art methods (such as DF-PPI and TAGPPI) in key metrics such as accuracy ( 98.01%) and AUC (99.62%), and demonstrates strong generalization in cross-species prediction (AUC > 99% on multi-species datasets). Furthermore, SCMPPI has been successfully applied to CD9 networks, the Wnt pathway, and cancer-specific networks, providing a reliable tool for disease target discovery. This framework also offers a new paradigm for multimodal biological information fusion and contrastive learning in collaborative optimization for various combined predictions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02698', 98)">Copy Link</button>
<div id="copy-message-98" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02724">Autonomous Human-Robot Interaction via Operator Imitation</a></h1>
<p><b>Authors:</b> Sammy Christen, David M\"uller, Agon Serifi, Ruben Grandia, Georg Wiedebach, Michael A. Hopkins, Espen Knoop, Moritz B\"acher</p>
<p>Abstract: Teleoperated robotic characters can perform expressive interactions with humans, relying on the operators' experience and social intuition. In this work, we propose to create autonomous interactive robots, by training a model to imitate operator data. Our model is trained on a dataset of human-robot interactions, where an expert operator is asked to vary the interactions and mood of the robot, while the operator commands as well as the pose of the human and robot are recorded. Our approach learns to predict continuous operator commands through a diffusion process and discrete commands through a classifier, all unified within a single transformer architecture. We evaluate the resulting model in simulation and with a user study on the real system. We show that our method enables simple autonomous human-robot interactions that are comparable to the expert-operator baseline, and that users can recognize the different robot moods as generated by our model. Finally, we demonstrate a zero-shot transfer of our model onto a different robotic platform with the same operator interface.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02724', 99)">Copy Link</button>
<div id="copy-message-99" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02737">RBR4DNN: Requirements-based Testing of Neural Networks</a></h1>
<p><b>Authors:</b> Nusrat Jahan Mozumder, Felipe Toledo, Swaroopa Dola, Matthew B. Dwyer</p>
<p>Abstract: Deep neural network (DNN) testing is crucial for the reliability and safety of critical systems, where failures can have severe consequences. Although various techniques have been developed to create robustness test suites, requirements-based testing for DNNs remains largely unexplored -- yet such tests are recognized as an essential component of software validation of critical systems. In this work, we propose a requirements-based test suite generation method that uses structured natural language requirements formulated in a semantic feature space to create test suites by prompting text-conditional latent diffusion models with the requirement precondition and then using the associated postcondition to define a test oracle to judge outputs of the DNN under test. We investigate the approach using fine-tuned variants of pre-trained generative models. Our experiments on the MNIST, CelebA-HQ, ImageNet, and autonomous car driving datasets demonstrate that the generated test suites are realistic, diverse, consistent with preconditions, and capable of revealing faults.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02737', 100)">Copy Link</button>
<div id="copy-message-100" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02764">Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model</a></h1>
<p><b>Authors:</b> Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, Yueqi Duan</p>
<p>Abstract: In this paper, we propose Scene Splatter, a momentum-based paradigm for video diffusion to generate generic scenes from single image. Existing methods, which employ video generation models to synthesize novel views, suffer from limited video length and scene inconsistency, leading to artifacts and distortions during further reconstruction. To address this issue, we construct noisy samples from original features as momentum to enhance video details and maintain scene consistency. However, for latent features with the perception field that spans both known and unknown regions, such latent-level momentum restricts the generative ability of video diffusion in unknown regions. Therefore, we further introduce the aforementioned consistent video as a pixel-level momentum to a directly generated video without momentum for better recovery of unseen regions. Our cascaded momentum enables video diffusion models to generate both high-fidelity and consistent novel views. We further finetune the global Gaussian representations with enhanced frames and render new frames for momentum update in the next step. In this manner, we can iteratively recover a 3D scene, avoiding the limitation of video length. Extensive experiments demonstrate the generalization capability and superior performance of our method in high-fidelity and consistent scene generation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02764', 101)">Copy Link</button>
<div id="copy-message-101" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02767">How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?</a></h1>
<p><b>Authors:</b> Andres Algaba, Vincent Holst, Floriano Tori, Melika Mobini, Brecht Verbeken, Sylvia Wenmackers, Vincent Ginis</p>
<p>Abstract: The spread of scientific knowledge depends on how researchers discover and cite previous work. The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices. However, it remains unclear to what extent LLMs align with human citation practices, how they perform across domains, and may influence citation dynamics. Here, we show that LLMs systematically reinforce the Matthew effect in citations by consistently favoring highly cited papers when generating references. This pattern persists across scientific domains despite significant field-specific variations in existence rates, which refer to the proportion of generated references that match existing records in external bibliometric databases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers, we find that LLM recommendations diverge from traditional citation patterns by preferring more recent references with shorter titles and fewer authors. Emphasizing their content-level relevance, the generated references are semantically aligned with the content of each paper at levels comparable to the ground truth references and display similar network effects while reducing author self-citations. These findings illustrate how LLMs may reshape citation practices and influence the trajectory of scientific discovery by reflecting and amplifying established trends. As LLMs become more integrated into the scientific research process, it is important to understand their role in shaping how scientific communities discover and build upon prior work.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02767', 102)">Copy Link</button>
<div id="copy-message-102" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02778">Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition</a></h1>
<p><b>Authors:</b> Vincent Gbouna Zakka, Luis J. Manso, Zhuangzhuang Dai</p>
<p>Abstract: Human activity recognition is increasingly vital for supporting independent living, particularly for the elderly and those in need of assistance. Domestic service robots with monitoring capabilities can enhance safety and provide essential support. Although image-based methods have advanced considerably in the past decade, their adoption remains limited by concerns over privacy and sensitivity to low-light or dark conditions. As an alternative, millimetre-wave (mmWave) radar can produce point cloud data which is privacy-preserving. However, processing the sparse and noisy point clouds remains a long-standing challenge. While graph-based methods and attention mechanisms show promise, they predominantly rely on "fixed" kernels; kernels that are applied uniformly across all neighbourhoods, highlighting the need for adaptive approaches that can dynamically adjust their kernels to the specific geometry of each local neighbourhood in point cloud data. To overcome this limitation, we introduce an adaptive approach within the graph convolutional framework. Instead of a single shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates multiple dynamic kernels, each capturing different aspects of the local feature space. By progressively refining local features while maintaining global spatial context, our method enables convolution kernels to adapt to varying local features. Experimental results on benchmark datasets confirm the effectiveness of our approach, achieving state-of-the-art performance in human activity recognition. Our source code is made publicly available at: https://github.com/Gbouna/MAK-GCN</p>
<p>URLs: <a href="https://github.com/Gbouna/MAK-GCN">https://github.com/Gbouna/MAK-GCN</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02778, https://github.com/Gbouna/MAK-GCN', 103)">Copy Link</button>
<div id="copy-message-103" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02780">From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks</a></h1>
<p><b>Authors:</b> Joshua Holstein, Moritz Diener, Philipp Spitzer</p>
<p>Abstract: The rise of Generative AI, and Large Language Models (LLMs) in particular, is fundamentally changing cognitive processes in knowledge work, raising critical questions about their impact on human reasoning and problem-solving capabilities. As these AI systems become increasingly integrated into workflows, they offer unprecedented opportunities for augmenting human thinking while simultaneously risking cognitive erosion through passive consumption of generated answers. This tension is particularly pronounced in open-ended tasks, where effective solutions require deep contextualization and integration of domain knowledge. Unlike structured tasks with established metrics, measuring the quality of human-LLM interaction in such open-ended tasks poses significant challenges due to the absence of ground truth and the iterative nature of solution development. To address this, we present a framework that analyzes interaction patterns along two dimensions: cognitive activity mode (exploration vs. exploitation) and cognitive engagement mode (constructive vs. detrimental). This framework provides systematic measurements to evaluate when LLMs are effective tools for thought rather than substitutes for human cognition, advancing theoretical understanding and practical guidance for developing AI systems that protect and augment human cognitive capabilities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02780', 104)">Copy Link</button>
<div id="copy-message-104" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02781">Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations</a></h1>
<p><b>Authors:</b> Selim Ickin, Shruti Bothe, Aman Raparia, Nitin Khanna, Erik Sanders</p>
<p>Abstract: Optimization of radio hardware and AI-based network management software yield significant energy savings in radio access networks. The execution of underlying Machine Learning (ML) models, which enable energy savings through recommended actions, may require additional compute and energy, highlighting the opportunity to explore and adopt accurate and energy-efficient ML technologies. This work evaluates the novel use of sparsely structured Neural Circuit Policies (NCPs) in a use case to estimate the energy consumption of base stations. Sparsity in ML models yields reduced memory, computation and energy demand, hence facilitating a low-cost and scalable solution. We also evaluate the generalization capability of NCPs in comparison to traditional and widely used ML models such as Long Short Term Memory (LSTM), via quantifying their sensitivity to varying model hyper-parameters (HPs). NCPs demonstrated a clear reduction in computational overhead and energy consumption. Moreover, results indicated that the NCPs are robust to varying HPs such as number of epochs and neurons in each layer, making them a suitable option to ease model management and to reduce energy consumption in Machine Learning Operations (MLOps) in telecommunications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02781', 105)">Copy Link</button>
<div id="copy-message-105" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02792">Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets</a></h1>
<p><b>Authors:</b> Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, Abhishek Gupta</p>
<p>Abstract: Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation required for most contemporary methods. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. We show that by simply controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/.</p>
<p>URLs: <a href="https://weirdlabuw.github.io/uwm/.">https://weirdlabuw.github.io/uwm/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02792, https://weirdlabuw.github.io/uwm/.', 106)">Copy Link</button>
<div id="copy-message-106" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02799">Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence</a></h1>
<p><b>Authors:</b> Anita Rau, Mark Endo, Josiah Aklilu, Jaewoo Heo, Khaled Saab, Alberto Paderno, Jeffrey Jopling, F. Christopher Holsinger, Serena Yeung-Levy</p>
<p>Abstract: Large Vision-Language Models offer a new paradigm for AI-driven image understanding, enabling models to perform tasks without task-specific training. This flexibility holds particular promise across medicine, where expert-annotated data is scarce. Yet, VLMs' practical utility in intervention-focused domains--especially surgery, where decision-making is subjective and clinical scenarios are variable--remains uncertain. Here, we present a comprehensive analysis of 11 state-of-the-art VLMs across 17 key visual understanding tasks in surgical AI--from anatomy recognition to skill assessment--using 13 datasets spanning laparoscopic, robotic, and open procedures. In our experiments, VLMs demonstrate promising generalizability, at times outperforming supervised models when deployed outside their training setting. In-context learning, incorporating examples during testing, boosted performance up to three-fold, suggesting adaptability as a key strength. Still, tasks requiring spatial or temporal reasoning remained difficult. Beyond surgery, our findings offer insights into VLMs' potential for tackling complex and dynamic scenarios in clinical and broader real-world applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02799', 107)">Copy Link</button>
<div id="copy-message-107" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02807">MegaMath: Pushing the Limits of Open Math Corpora</a></h1>
<p><b>Authors:</b> Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, Eric P. Xing</p>
<p>Abstract: Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02807', 108)">Copy Link</button>
<div id="copy-message-108" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02810">Generative Evaluation of Complex Reasoning in Large Language Models</a></h1>
<p><b>Authors:</b> Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang</p>
<p>Abstract: With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02810', 109)">Copy Link</button>
<div id="copy-message-109" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02819">GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings</a></h1>
<p><b>Authors:</b> Yuexi Du, Jiazhen Zhang, Nicha C. Dvornek, John A. Onofrey</p>
<p>Abstract: Symmetry, where certain features remain invariant under geometric transformations, can often serve as a powerful prior in designing convolutional neural networks (CNNs). While conventional CNNs inherently support translational equivariance, extending this property to rotation and reflection has proven challenging, often forcing a compromise between equivariance, efficiency, and information loss. In this work, we introduce Gaussian Mixture Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths radial symmetry using a mixture of Gaussian-weighted rings. This design mitigates discretization errors of circular kernels, thereby preserving robust rotation and reflection equivariance without incurring computational overhead. We further optimize both the space and speed efficiency of GMR-Conv via a novel parameterization and computation strategy, allowing larger kernels at an acceptable cost. Extensive experiments on eight classification and one segmentation datasets demonstrate that GMR-Conv not only matches conventional CNNs' performance but can also surpass it in applications with orientation-less data. GMR-Conv is also proven to be more robust and efficient than the state-of-the-art equivariant learning methods. Our work provides inspiring empirical evidence that carefully applied radial symmetry can alleviate the challenges of information loss, marking a promising advance in equivariant network architectures. The code is available at https://github.com/XYPB/GMR-Conv.</p>
<p>URLs: <a href="https://github.com/XYPB/GMR-Conv.">https://github.com/XYPB/GMR-Conv.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02819, https://github.com/XYPB/GMR-Conv.', 110)">Copy Link</button>
<div id="copy-message-110" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02821">Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</a></h1>
<p><b>Authors:</b> Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata</p>
<p>Abstract: Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02821', 111)">Copy Link</button>
<div id="copy-message-111" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02827">On Vanishing Variance in Transformer Length Generalization</a></h1>
<p><b>Authors:</b> Ruining Li (Jinghao), Gabrijel Boduljak (Jinghao),  Jensen (Jinghao),  Zhou</p>
<p>Abstract: It is a widely known issue that Transformers, when trained on shorter sequences, fail to generalize robustly to longer ones at test time. This raises the question of whether Transformer models are real reasoning engines, despite their impressive abilities in mathematical problem solving and code synthesis. In this paper, we offer a vanishing variance perspective on this issue. To the best of our knowledge, we are the first to demonstrate that even for today's frontier models, a longer sequence length results in a decrease in variance in the output of the multi-head attention modules. On the argmax retrieval and dictionary lookup tasks, our experiments show that applying layer normalization after the attention outputs leads to significantly better length generalization. Our analyses attribute this improvement to a reduction-though not a complete elimination-of the distribution shift caused by vanishing variance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02827', 112)">Copy Link</button>
<div id="copy-message-112" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2504.02828">Concept Lancet: Image Editing with Compositional Representation Transplant</a></h1>
<p><b>Authors:</b> Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Hancheng Min, Chris Callison-Burch, Ren\'e Vidal</p>
<p>Abstract: Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02828', 113)">Copy Link</button>
<div id="copy-message-113" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2405.17537">CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</a></h1>
<p><b>Authors:</b> ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang</p>
<p>Abstract: Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.17537', 114)">Copy Link</button>
<div id="copy-message-114" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2409.15127">Pareto-Optimized Open-Source LLMs for Healthcare via Context Retrieval</a></h1>
<p><b>Authors:</b> Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Dario Garcia-Gasulla</p>
<p>Abstract: This study leverages optimized context retrieval to enhance open-source Large Language Models (LLMs) for cost-effective, high performance healthcare AI. We demonstrate that this approach achieves state-of-the-art accuracy on medical question answering at a fraction of the cost of proprietary models, significantly improving the cost-accuracy Pareto frontier on the MedQA benchmark. Key contributions include: (1) OpenMedQA, a novel benchmark revealing a performance gap in open-ended medical QA compared to multiple-choice formats; (2) a practical, reproducible pipeline for context retrieval optimization; and (3) open-source resources (Prompt Engine, CoT/ToT/Thinking databases) to empower healthcare AI development. By advancing retrieval techniques and QA evaluation, we enable more affordable and reliable LLM solutions for healthcare.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.15127', 115)">Copy Link</button>
<div id="copy-message-115" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.15748">Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation</a></h1>
<p><b>Authors:</b> Shaonan Wu, Shuai Lu, Yeyun Gong, Nan Duan, Ping Wei</p>
<p>Abstract: Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 4.70% absolute performance improvement on Leandojo benchmark. Additionally, our approach achieves a 2.47% absolute performance gain on the out-of-distribution miniF2F benchmark based on the synthetic data.To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.15748', 116)">Copy Link</button>
<div id="copy-message-116" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.17233">ICPL: Few-shot In-context Preference Learning via LLMs</a></h1>
<p><b>Authors:</b> Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky</p>
<p>Abstract: Preference-based reinforcement learning is an effective way to handle tasks where rewards are hard to specify but can be exceedingly inefficient as preference learning is often tabula rasa. We demonstrate that Large Language Models (LLMs) have native preference-learning capabilities that allow them to achieve sample-efficient preference learning, addressing this challenge. We propose In-Context Preference Learning (ICPL), which uses in-context learning capabilities of LLMs to reduce human query inefficiency. ICPL uses the task description and basic environment code to create sets of reward functions which are iteratively refined by placing human feedback over videos of the resultant policies into the context of an LLM and then requesting better rewards. We first demonstrate ICPL's effectiveness through a synthetic preference study, providing quantitative evidence that it significantly outperforms baseline preference-based methods with much higher performance and orders of magnitude greater efficiency. We observe that these improvements are not solely coming from LLM grounding in the task but that the quality of the rewards improves over time, indicating preference learning capabilities. Additionally, we perform a series of real human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.17233', 117)">Copy Link</button>
<div id="copy-message-117" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2411.18526">NeuroAI for AI Safety</a></h1>
<p><b>Authors:</b> Patrick Mineault, Niccol\`o Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador</p>
<p>Abstract: As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.18526', 118)">Copy Link</button>
<div id="copy-message-118" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.16355">Social Science Is Necessary for Operationalizing Socially Responsible Foundation Models</a></h1>
<p><b>Authors:</b> Adam Davies, Elisa Nguyen, Michael Simeone, Erik Johnston, Martin Gubri</p>
<p>Abstract: With the rise of foundation models, there is growing concern about their potential social impacts. Social science has a long history of studying the social impacts of transformative technologies in terms of pre-existing systems of power and how these systems are disrupted or reinforced by new technologies. In this position paper, we build on prior work studying the social impacts of earlier technologies to propose a conceptual framework studying foundation models as sociotechnical systems, incorporating social science expertise to better understand how these models affect systems of power, anticipate the impacts of deploying these models in various applications, and study the effectiveness of technical interventions intended to mitigate social harms. We advocate for an interdisciplinary and collaborative research paradigm between AI and social science across all stages of foundation model research and development to promote socially responsible research practices and use cases, and outline several strategies to facilitate such research.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.16355', 119)">Copy Link</button>
<div id="copy-message-119" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.06152">The Value of Information in Human-AI Decision-making</a></h1>
<p><b>Authors:</b> Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman</p>
<p>Abstract: Multiple agents -- including humans and AI models -- are often paired on decision tasks with the expectation of achieving complementary performance, where the combined performance of both agents outperforms either one alone. However, knowing how to improve the performance of a human-AI team is often difficult without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information -- in AI-assisted decision workflows. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based explanation technique that adapts SHAP, a saliency-based explanation, to explain information value in decision making.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.06152', 120)">Copy Link</button>
<div id="copy-message-120" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2503.17604">OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery</a></h1>
<p><b>Authors:</b> Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</p>
<p>Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.17604', 121)">Copy Link</button>
<div id="copy-message-121" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2503.23037">Agentic Large Language Models, a survey</a></h1>
<p><b>Authors:</b> Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, Kees Joost Batenburg</p>
<p>Abstract: There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.23037', 122)">Copy Link</button>
<div id="copy-message-122" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2012.12689">The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?</a></h1>
<p><b>Authors:</b> Guido Fioretti, Andrea Policarpi</p>
<p>Abstract: We approach the debate on how ``intelligent'' artificial agents should be, by endowing the preys and predators of the Lotka-Volterra model with behavioural algorithms characterized by different levels of sophistication. We find that by endowing both preys and predators with the capability of making predictions based on linear extrapolation a novel sort of dynamic equilibrium appears, where both species co-exist while both populations grow indefinitely. While we confirm that, in general, simple agents favour the emergence of complex collective behaviour, we also suggest that the capability of individuals to take first-order derivatives of one other's behaviour may allow the collective computation of derivatives of any order.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2012.12689', 123)">Copy Link</button>
<div id="copy-message-123" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2311.01727">Noise-Agnostic Quantum Error Mitigation with Data Augmented Neural Models</a></h1>
<p><b>Authors:</b> Manwen Liao, Yan Zhu, Giulio Chiribella, Yuxiang Yang</p>
<p>Abstract: Quantum error mitigation, a data processing technique for recovering the statistics of target processes from their noisy version, is a crucial task for near-term quantum technologies. Most existing methods require prior knowledge of the noise model or the noise parameters. Deep neural networks have a potential to lift this requirement, but current models require training data produced by ideal processes in the absence of noise. Here we build a neural model that achieves quantum error mitigation without any prior knowledge of the noise and without training on noise-free data. To achieve this feature, we introduce a quantum augmentation technique for error mitigation. Our approach applies to quantum circuits and to the dynamics of many-body and continuous-variable quantum systems, accommodating various types of noise models. We demonstrate its effectiveness by testing it both on simulated noisy circuits and on real quantum hardware.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2311.01727', 124)">Copy Link</button>
<div id="copy-message-124" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2311.16176">Mitigating Shortcut Learning with Diffusion Counterfactuals and Diverse Ensembles</a></h1>
<p><b>Authors:</b> Luca Scimeca, Alexander Rubinstein, Damien Teney, Seong Joon Oh, Yoshua Bengio</p>
<p>Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut learning, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose DiffDiv an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalization and diversification on par with prior work that relies on auxiliary data collection.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2311.16176', 125)">Copy Link</button>
<div id="copy-message-125" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2312.12050">Extension of the Dip-test Repertoire -- Efficient and Differentiable p-value Calculation for Clustering</a></h1>
<p><b>Authors:</b> Lena G. M. Bauer, Collin Leiber, Christian B\"ohm, Claudia Plant</p>
<p>Abstract: Over the last decade, the Dip-test of unimodality has gained increasing interest in the data mining community as it is a parameter-free statistical test that reliably rates the modality in one-dimensional samples. It returns a so called Dip-value and a corresponding probability for the sample's unimodality (Dip-p-value). These two values share a sigmoidal relationship. However, the specific transformation is dependent on the sample size. Many Dip-based clustering algorithms use bootstrapped look-up tables translating Dip- to Dip-p-values for a certain limited amount of sample sizes. We propose a specifically designed sigmoid function as a substitute for these state-of-the-art look-up tables. This accelerates computation and provides an approximation of the Dip- to Dip-p-value transformation for every single sample size. Further, it is differentiable and can therefore easily be integrated in learning schemes using gradient descent. We showcase this by exploiting our function in a novel subspace clustering algorithm called Dip'n'Sub. We highlight in extensive experiments the various benefits of our proposal.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2312.12050', 126)">Copy Link</button>
<div id="copy-message-126" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2402.02791">PanGu-$\pi$ Pro:Rethinking Optimization and Architecture for Tiny Language Models</a></h1>
<p><b>Authors:</b> Yehui Tang, Kai Han, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Yunhe Wang</p>
<p>Abstract: The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, \ie, neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas. Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\pi$-1B Pro. Besides, PanGu-$\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance. The code is available at https://github.com/YuchuanTian/RethinkTinyLM.</p>
<p>URLs: <a href="https://github.com/YuchuanTian/RethinkTinyLM.">https://github.com/YuchuanTian/RethinkTinyLM.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.02791, https://github.com/YuchuanTian/RethinkTinyLM.', 127)">Copy Link</button>
<div id="copy-message-127" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2402.16442">On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions</a></h1>
<p><b>Authors:</b> Maximilian B\"other, Abraham Sebastian, Pranjal Awasthi, Ana Klimovic, Srikumar Ramalingam</p>
<p>Abstract: Modern datasets span billions of samples, making training on all available data infeasible. Selecting a high quality subset helps in reducing training costs and enhancing model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving such subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset in DRAM. At billion datapoint scale, even the subset may not fit a single machine, and the sequential algorithms are prohibitively slow. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, partition-based distributed greedy algorithm to identify the remaining subset. We discuss how to implement these algorithms in a distributed data processing framework and empirically analyze different configurations. We find high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in quality compared to centralized methods, and scale to a dataset with 13 billion points.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.16442', 128)">Copy Link</button>
<div id="copy-message-128" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2403.16843">Do LLM Agents Have Regret? A Case Study in Online Learning and Games</a></h1>
<p><b>Authors:</b> Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang</p>
<p>Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.16843', 129)">Copy Link</button>
<div id="copy-message-129" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2404.01551">Safety-Aware Multi-Agent Learning for Dynamic Network Bridging</a></h1>
<p><b>Authors:</b> Raffaele Galliera, Konstantinos Mitsopoulos, Niranjan Suri, Raffaele Romagnoli</p>
<p>Abstract: Addressing complex cooperative tasks in safety-critical environments poses significant challenges for multi-agent systems, especially under conditions of partial observability. We focus on a dynamic network bridging task, where agents must learn to maintain a communication path between two moving targets. To ensure safety during training and deployment, we integrate a control-theoretic safety filter that enforces collision avoidance through local setpoint updates. We develop and evaluate multi-agent reinforcement learning safety-informed message passing, showing that encoding safety filter activations as edge-level features improves coordination. The results suggest that local safety enforcement and decentralized learning can be effectively combined in distributed multi-agent tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2404.01551', 130)">Copy Link</button>
<div id="copy-message-130" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2404.11014">Towards Multi-agent Reinforcement Learning based Traffic Signal Control through Spatio-temporal Hypergraphs</a></h1>
<p><b>Authors:</b> Kang Wang, Zhishu Shen, Zhen Lei, Tiehua Zhang</p>
<p>Abstract: Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatio-temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent urban traffic management solutions. We release the code to support the reproducibility of this work at https://github.com/Edun-Eyes/TSC</p>
<p>URLs: <a href="https://github.com/Edun-Eyes/TSC">https://github.com/Edun-Eyes/TSC</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2404.11014, https://github.com/Edun-Eyes/TSC', 131)">Copy Link</button>
<div id="copy-message-131" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.05256">THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models</a></h1>
<p><b>Authors:</b> Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto</p>
<p>Abstract: Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term "Type I hallucinations". Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term "Type II hallucinations". Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline. Code is now available at https://github.com/amazon-science/THRONE .</p>
<p>URLs: <a href="https://github.com/amazon-science/THRONE">https://github.com/amazon-science/THRONE</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.05256, https://github.com/amazon-science/THRONE', 132)">Copy Link</button>
<div id="copy-message-132" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.08044">On the Volatility of Shapley-Based Contribution Metrics in Federated Learning</a></h1>
<p><b>Authors:</b> Arno Geimer, Beltran Fiz, Radu State</p>
<p>Abstract: Federated learning (FL) is a collaborative and privacy-preserving Machine Learning paradigm, allowing the development of robust models without the need to centralize sensitive data. A critical challenge in FL lies in fairly and accurately allocating contributions from diverse participants. Inaccurate allocation can undermine trust, lead to unfair compensation, and thus participants may lack the incentive to join or actively contribute to the federation. Various remuneration strategies have been proposed to date, including auction-based approaches and Shapley-value-based methods, the latter offering a means to quantify the contribution of each participant. However, little to no work has studied the stability of these contribution evaluation methods. In this paper, we evaluate participant contributions in federated learning using gradient-based model reconstruction techniques with Shapley values and compare the round-based contributions to a classic data contribution measurement scheme. We provide an extensive analysis of the discrepancies of Shapley values across a set of aggregation strategies, and examine them on an overall and a per-client level. We show that, between different aggregation techniques, Shapley values lead to unstable reward allocations among participants. Our analysis spans various data heterogeneity distributions, including independent and identically distributed (IID) and non-IID scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.08044', 133)">Copy Link</button>
<div id="copy-message-133" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.10347">Networking Systems for Video Anomaly Detection: A Tutorial and Survey</a></h1>
<p><b>Authors:</b> Jing Liu, Yang Liu, Jieyu Lin, Jielin Li, Liang Cao, Peng Sun, Bo Hu, Liang Song, Azzedine Boukerche, Victor C. M. Leung</p>
<p>Abstract: The increasing utilization of surveillance cameras in smart cities, coupled with the surge of online video applications, has heightened concerns regarding public security and privacy protection, which propelled automated Video Anomaly Detection (VAD) into a fundamental research task within the Artificial Intelligence (AI) community. With the advancements in deep learning and edge computing, VAD has made significant progress and advances synergized with emerging applications in smart cities and video internet, which has moved beyond the conventional research scope of algorithm engineering to deployable Networking Systems for VAD (NSVAD), a practical hotspot for intersection exploration in the AI, IoVT, and computing fields. In this article, we delineate the foundational assumptions, learning frameworks, and applicable scenarios of various deep learning-driven VAD routes, offering an exhaustive tutorial for novices in NSVAD. In addition, this article elucidates core concepts by reviewing recent advances and typical solutions and aggregating available research resources accessible at https://github.com/fdjingliu/NSVAD. Lastly, this article projects future development trends and discusses how the integration of AI and computing technologies can address existing research challenges and promote open opportunities, serving as an insightful guide for prospective researchers and engineers.</p>
<p>URLs: <a href="https://github.com/fdjingliu/NSVAD.">https://github.com/fdjingliu/NSVAD.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.10347, https://github.com/fdjingliu/NSVAD.', 134)">Copy Link</button>
<div id="copy-message-134" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2406.06777">MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension</a></h1>
<p><b>Authors:</b> Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla</p>
<p>Abstract: Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding. However, their proficiency within the chemistry domain remains restricted, especially in solving professional molecule-related tasks. This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e., SMILES strings. In this study, we seek to enhance the ability of LLMs to comprehend molecules by equipping them with a multi-modal external module, namely MolX. In particular, instead of directly using a SMILES string to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM. Moreover, a handcrafted molecular fingerprint is incorporated to leverage its embedded domain knowledge. Then, to establish an alignment between MolX and the LLM's textual input space, the whole model in which the LLM is frozen, is pre-trained with a versatile strategy including a diverse set of tasks. Experimental evaluations show that our proposed method outperforms baselines across 4 downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters 0.53% and 0.82%, respectively.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.06777', 135)">Copy Link</button>
<div id="copy-message-135" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2406.17949">The Overcooked Generalisation Challenge</a></h1>
<p><b>Authors:</b> Constantin Ruhdorfer, Matteo Bortoletto, Anna Penzkofer, Andreas Bulling</p>
<p>Abstract: We introduce the Overcooked Generalisation Challenge (OGC) - the first benchmark to study agents' zero-shot cooperation abilities when faced with novel partners and levels in the Overcooked-AI environment. This perspective starkly contrasts a large body of previous work that has trained and evaluated cooperating agents only on the same level, failing to capture generalisation abilities required for real-world human-AI cooperation. Our challenge interfaces with state-of-the-art dual curriculum design (DCD) methods to generate auto-curricula for training general agents in Overcooked. It is the first cooperative multi-agent environment specially designed for DCD methods and, consequently, the first benchmarked with state-of-the-art methods. It is fully GPU-accelerated, built on the DCD benchmark suite minimax, and freely available under an open-source license: https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. We show that current DCD algorithms struggle to produce useful policies in this novel challenge, even if combined with recent network architectures that were designed for scalability and generalisability. The OGC pushes the boundaries of real-world human-AI cooperation by enabling the research community to study the impact of generalisation on cooperating agents.</p>
<p>URLs: <a href="https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC.">https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.17949, https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC.', 136)">Copy Link</button>
<div id="copy-message-136" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2406.17961">NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization</a></h1>
<p><b>Authors:</b> Md Mahadi Hasan Nahid, Davood Rafiei</p>
<p>Abstract: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.17961', 137)">Copy Link</button>
<div id="copy-message-137" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2407.11606">The Foundations of Tokenization: Statistical and Computational Concerns</a></h1>
<p><b>Authors:</b> Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell</p>
<p>Abstract: Tokenization - the practice of converting strings of characters from an alphabet into sequences of tokens over a vocabulary - is a critical step in the NLP pipeline. The use of token representations is widely credited with increased model performance but is also the source of many undesirable behaviors, such as spurious ambiguity or inconsistency. Despite its recognized importance as a standard representation method in NLP, the theoretical underpinnings of tokenization are not yet fully understood. In particular, the impact of tokenization on language model estimation has been investigated primarily through empirical means. The present paper contributes to addressing this theoretical gap by proposing a unified formal framework for representing and analyzing tokenizer models. Based on the category of stochastic maps, this framework enables us to establish general conditions for a principled use of tokenizers and, most importantly, the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. In addition, we discuss statistical and computational concerns crucial for designing and implementing tokenizer models, such as inconsistency, ambiguity, finiteness, and sequentiality. The framework and results advanced in this paper contribute to building robust theoretical foundations for representations in neural language modeling that can inform future theoretical and empirical research.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.11606', 138)">Copy Link</button>
<div id="copy-message-138" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2407.15616">Sustainable broadcasting in Blockchain Networks with Reinforcement Learning</a></h1>
<p><b>Authors:</b> Danila Valko, Daniel Kudenko</p>
<p>Abstract: Recent estimates put the carbon footprint of Bitcoin and Ethereum at an average of 64 and 26 million tonnes of CO2 per year, respectively. To address this growing problem, several possible approaches have been proposed in the literature: creating alternative blockchain consensus mechanisms, applying redundancy reduction techniques, utilizing renewable energy sources, and employing energy-efficient devices, etc. In this paper, we follow the second avenue and propose an efficient approach based on reinforcement learning that improves the block broadcasting scheme in blockchain networks. The analysis and experimental results confirmed that the proposed improvement of the block propagation scheme could cleverly handle network dynamics and achieve better results than the default approach. Additionally, our technical integration of the simulator and developed RL environment can be used as a complete solution for further study of new schemes and protocols that use RL or other ML techniques.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.15616', 139)">Copy Link</button>
<div id="copy-message-139" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2408.10669">Tensor tree learns hidden relational structures in data to construct generative models</a></h1>
<p><b>Authors:</b> Kenji Harada, Tsuyoshi Okubo, Naoki Kawashima</p>
<p>Abstract: Based on the tensor tree network with the Born machine framework, we propose a general method for constructing a generative model by expressing the target distribution function as the amplitude of the quantum wave function represented by a tensor tree. The key idea is dynamically optimizing the tree structure that minimizes the bond mutual information. The proposed method offers enhanced performance and uncovers hidden relational structures in the target data. We illustrate potential practical applications with four examples: (i) random patterns, (ii) QMNIST handwritten digits, (iii) Bayesian networks, and (iv) the pattern of stock price fluctuation pattern in S&P500. In (i) and (ii), the strongly correlated variables were concentrated near the center of the network; in (iii), the causality pattern was identified; and in (iv), a structure corresponding to the eleven sectors emerged.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2408.10669', 140)">Copy Link</button>
<div id="copy-message-140" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.03944">HUMOS: Human Motion Model Conditioned on Body Shape</a></h1>
<p><b>Authors:</b> Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden, Carsten Stoll</p>
<p>Abstract: Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page https://CarstenEpic.github.io/humos/.</p>
<p>URLs: <a href="https://CarstenEpic.github.io/humos/.">https://CarstenEpic.github.io/humos/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.03944, https://CarstenEpic.github.io/humos/.', 141)">Copy Link</button>
<div id="copy-message-141" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.03804">Mixture of Attentions For Speculative Decoding</a></h1>
<p><b>Authors:</b> Matthieu Zimmer, Milan Gritta, Gerasimos Lampouras, Haitham Bou Ammar, Jun Wang</p>
<p>Abstract: The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy. Speculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel. Small models that utilise activations from the LLM currently achieve the fastest decoding speeds. However, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. To address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD. Our novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server. In a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%. In a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.03804', 142)">Copy Link</button>
<div id="copy-message-142" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.04133">An Electrocardiogram Foundation Model Built on over 10 Million Recordings with External Evaluation across Multiple Domains</a></h1>
<p><b>Authors:</b> Jun Li, Aaron Aguirre, Junior Moura, Che Liu, Lanhai Zhong, Chenxi Sun, Gari Clifford, Brandon Westover, Shenda Hong</p>
<p>Abstract: Artificial intelligence (AI) has demonstrated significant potential in ECG analysis and cardiovascular disease assessment. Recently, foundation models have played a remarkable role in advancing medical AI. The development of an ECG foundation model holds the promise of elevating AI-ECG research to new heights. However, building such a model faces several challenges, including insufficient database sample sizes and inadequate generalization across multiple domains. Additionally, there is a notable performance gap between single-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model (ECGFounder), a general-purpose model that leverages real-world ECG annotations from cardiology experts to broaden the diagnostic capabilities of ECG analysis. ECGFounder was trained on over 10 million ECGs with 150 label categories from the Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease diagnosis through ECG analysis. The model is designed to be both an effective out-of-the-box solution, and a to be fine-tunable for downstream tasks, maximizing usability. Importantly, we extended its application to lower rank ECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to supporting various downstream tasks in mobile monitoring scenarios. Experimental results demonstrate that ECGFounder achieves expert-level performance on internal validation sets, with AUROC exceeding 0.95 for eighty diagnoses. It also shows strong classification performance and generalization across various diagnoses on external validation sets. When fine-tuned, ECGFounder outperforms baseline models in demographic analysis, clinical event detection, and cross-modality cardiac rhythm diagnosis. The trained model and data will be publicly released upon publication through the bdsp.io. Our code is available at https://github.com/PKUDigitalHealth/ECGFounder</p>
<p>URLs: <a href="https://github.com/PKUDigitalHealth/ECGFounder">https://github.com/PKUDigitalHealth/ECGFounder</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.04133, https://github.com/PKUDigitalHealth/ECGFounder', 143)">Copy Link</button>
<div id="copy-message-143" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.12159">NSSI-Net: A Multi-Concept GAN for Non-Suicidal Self-Injury Detection Using High-Dimensional EEG in a Semi-Supervised Framework</a></h1>
<p><b>Authors:</b> Zhen Liang, Weishan Ye, Qile Liu, Li Zhang, Gan Huang, Yongjie Zhou</p>
<p>Abstract: Non-suicidal self-injury (NSSI) is a serious threat to the physical and mental health of adolescents, significantly increasing the risk of suicide and attracting widespread public concern. Electroencephalography (EEG), as an objective tool for identifying brain disorders, holds great promise. However, extracting meaningful and reliable features from high-dimensional EEG data, especially by integrating spatiotemporal brain dynamics into informative representations, remains a major challenge. In this study, we introduce an advanced semi-supervised adversarial network, NSSI-Net, to effectively model EEG features related to NSSI. NSSI-Net consists of two key modules: a spatial-temporal feature extraction module and a multi-concept discriminator. In the spatial-temporal feature extraction module, an integrated 2D convolutional neural network (2D-CNN) and a bi-directional Gated Recurrent Unit (BiGRU) are used to capture both spatial and temporal dynamics in EEG data. In the multi-concept discriminator, signal, gender, domain, and disease levels are fully explored to extract meaningful EEG features, considering individual, demographic, disease variations across a diverse population. Based on self-collected NSSI data (n=114), the model's effectiveness and reliability are demonstrated, with a 5.44% improvement in performance compared to existing machine learning and deep learning methods. This study advances the understanding and early diagnosis of NSSI in adolescents with depression, enabling timely intervention. The source code is available at https://github.com/Vesan-yws/NSSINet.</p>
<p>URLs: <a href="https://github.com/Vesan-yws/NSSINet.">https://github.com/Vesan-yws/NSSINet.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.12159, https://github.com/Vesan-yws/NSSINet.', 144)">Copy Link</button>
<div id="copy-message-144" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.14121">FedMSE: Semi-supervised federated learning approach for IoT network intrusion detection</a></h1>
<p><b>Authors:</b> Van Tuan Nguyen, Razvan Beuran</p>
<p>Abstract: This paper proposes a novel federated learning approach for improving IoT network intrusion detection. The rise of IoT has expanded the cyber attack surface, making traditional centralized machine learning methods insufficient due to concerns about data availability, computational resources, transfer costs, and especially privacy preservation. A semi-supervised federated learning model was developed to overcome these issues, combining the Shrink Autoencoder and Centroid one-class classifier (SAE-CEN). This approach enhances the performance of intrusion detection by effectively representing normal network data and accurately identifying anomalies in the decentralized strategy. Additionally, a mean square error-based aggregation algorithm (MSEAvg) was introduced to improve global model performance by prioritizing more accurate local models. The results obtained in our experimental setup, which uses various settings relying on the N-BaIoT dataset and Dirichlet distribution, demonstrate significant improvements in real-world heterogeneous IoT networks in detection accuracy from 93.98$\pm$2.90 to 97.30$\pm$0.49, reduced learning costs when requiring only 50\% of gateways participating in the training process, and robustness in large-scale networks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.14121', 145)">Copy Link</button>
<div id="copy-message-145" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.00915">Empower Vision Applications with LoRA LMM</a></h1>
<p><b>Authors:</b> Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang, Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen, Yunxin Liu</p>
<p>Abstract: Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype VaLoRA on five popular vision tasks on three LMMs. Experiment results reveal that VaLoRA improves 24-62% of the accuracy compared to the original LMMs and reduces 20-89% of the latency compared to the state-of-the-art LoRA model serving systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.00915', 146)">Copy Link</button>
<div id="copy-message-146" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.03862">ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization</a></h1>
<p><b>Authors:</b> Huayang Huang, Yu Wu, Qian Wang</p>
<p>Abstract: Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods. Code is available at https://github.com/Hannah1102/ROBIN.</p>
<p>URLs: <a href="https://github.com/Hannah1102/ROBIN.">https://github.com/Hannah1102/ROBIN.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.03862, https://github.com/Hannah1102/ROBIN.', 147)">Copy Link</button>
<div id="copy-message-147" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.08297">TowerDebias: A Novel Unfairness Removal Method Based on the Tower Property</a></h1>
<p><b>Authors:</b> Norman Matloff, Aditya Mittal</p>
<p>Abstract: Decision-making processes have increasingly come to rely on sophisticated machine learning tools, raising critical concerns about the fairness of their predictions with respect to sensitive groups. The widespread adoption of commercial "black-box" models necessitates careful consideration of their legal and ethical implications for consumers. When users interact with such black-box models, a key challenge arises: how can the influence of sensitive attributes, such as race or gender, be mitigated or removed from its predictions? We propose towerDebias (tDB), a novel post-processing method designed to reduce the influence of sensitive attributes in predictions made by black-box models. Our tDB approach leverages the Tower Property from probability theory to improve prediction fairness without requiring retraining of the original model. This method is highly versatile, as it requires no prior knowledge of the original algorithm's internal structure and is adaptable to a diverse range of applications. We present a formal fairness improvement theorem for tDB and showcase its effectiveness in both regression and classification tasks using multiple real-world datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.08297', 148)">Copy Link</button>
<div id="copy-message-148" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.14762">Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction</a></h1>
<p><b>Authors:</b> Huiwon Jang, Sihyun Yu, Jinwoo Shin, Pieter Abbeel, Younggyo Seo</p>
<p>Abstract: Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128$\times$128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.14762', 149)">Copy Link</button>
<div id="copy-message-149" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.07237">ArtFormer: Controllable Generation of Diverse 3D Articulated Objects</a></h1>
<p><b>Authors:</b> Jiayi Su, Youhe Feng, Zheng Li, Jinhua Song, Yangfan He, Botao Ren, Botian Xu</p>
<p>Abstract: This paper presents a novel framework for modeling and conditional generation of 3D articulated objects. Troubled by flexibility-quality tradeoffs, existing methods are often limited to using predefined structures or retrieving shapes from static datasets. To address these challenges, we parameterize an articulated object as a tree of tokens and employ a transformer to generate both the object's high-level geometry code and its kinematic relations. Subsequently, each sub-part's geometry is further decoded using a signed-distance-function (SDF) shape prior, facilitating the synthesis of high-quality 3D shapes. Our approach enables the generation of diverse objects with high-quality geometry and varying number of parts. Comprehensive experiments on conditional generation from text descriptions demonstrate the effectiveness and flexibility of our method.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.07237', 150)">Copy Link</button>
<div id="copy-message-150" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.07755">SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</a></h1>
<p><b>Authors:</b> Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko</p>
<p>Abstract: Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships, and not dynamic awareness of motion and space, i.e., reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos -- even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.07755', 151)">Copy Link</button>
<div id="copy-message-151" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.12153">Revisiting Weight Averaging for Model Merging</a></h1>
<p><b>Authors:</b> Jiho Choi, Donggyun Kim, Chanhyuk Lee, Seunghoon Hong</p>
<p>Abstract: Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors. Our method demonstrates robust and scalable performance on vision benchmarks across varying numbers of tasks and model sizes. Furthermore, we observe that our approach is applicable to natural language processing tasks with competitive performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.12153', 152)">Copy Link</button>
<div id="copy-message-152" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.17867">Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types</a></h1>
<p><b>Authors:</b> Ziming Guo, Chao Ma, Yinggang Sun, Tiancheng Zhao, Guangyao Wang, Hai Huang</p>
<p>Abstract: Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q\&A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries. Our dataset and code are publicly available at https://mcxiaoxiao.github.io/MMSQL.</p>
<p>URLs: <a href="https://mcxiaoxiao.github.io/MMSQL.">https://mcxiaoxiao.github.io/MMSQL.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.17867, https://mcxiaoxiao.github.io/MMSQL.', 153)">Copy Link</button>
<div id="copy-message-153" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.00398">TSPE: Task-Specific Prompt Ensemble for Improved Zero-Shot Audio Classification</a></h1>
<p><b>Authors:</b> Nishit Anand, Ashish Seth, Ramani Duraiswami, Dinesh Manocha</p>
<p>Abstract: Audio-language models (ALMs) excel in zero-shot audio classification, a task where models classify previously unseen audio clips at test time by leveraging descriptive natural language prompts. We introduce TSPE (Task-Specific Prompt Ensemble), a simple, training-free hard prompting method that boosts ALEs' zero-shot performance by customizing prompts for diverse audio classification tasks. Rather than using generic template-based prompts like "Sound of a car" we generate context-rich prompts, such as "Sound of a car coming from a tunnel". Specifically, we leverage label information to identify suitable sound attributes, such as "loud" and "feeble", and appropriate sound sources, such as "tunnel" and "street" and incorporate this information into the prompts used by Audio-Language Models (ALMs) for audio classification. Further, to enhance audio-text alignment, we perform prompt ensemble across TSPE-generated task-specific prompts. When evaluated on 12 diverse audio classification datasets, TSPE improves performance across ALMs by showing an absolute improvement of 1.23-16.36% over vanilla zero-shot evaluation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.00398', 154)">Copy Link</button>
<div id="copy-message-154" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.14846">Wormhole Memory: A Rubik&#x27;s Cube for Cross-Dialogue Retrieval</a></h1>
<p><b>Authors:</b> Libo Wang</p>
<p>Abstract: In view of the gap in the current large language model in sharing memory across dialogues, this research proposes a wormhole memory module (WMM) to realize memory as a Rubik's cube that can be arbitrarily retrieved between different dialogues. Through simulation experiments, the researcher built an experimental framework based on the Python environment and used setting memory barriers to simulate the current situation where memories between LLMs dialogues are difficult to share. The CoQA development data set was imported into the experiment, and the feasibility of its cross-dialogue memory retrieval function was verified for WMM's nonlinear indexing and dynamic retrieval, and a comparative analysis was conducted with the capabilities of Titans and MemGPT memory modules. Experimental results show that WMM demonstrated the ability to retrieve memory across dialogues and the stability of quantitative indicators in eight experiments. It contributes new technical approaches to the optimization of memory management of LLMs and provides experience for the practical application in the future.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.14846', 155)">Copy Link</button>
<div id="copy-message-155" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.00070">Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs&#x27; Performance and Biases in Evaluating over 1000 Economics Papers</a></h1>
<p><b>Authors:</b> Pat Pataranutaporn, Nattavudh Powdthavee, Chayapatr Achiwaranguprok, Pattie Maes</p>
<p>Abstract: This study examines the potential of large language models (LLMs) to augment the academic peer review process by reliably evaluating the quality of economics research without introducing systematic bias. We conduct one of the first large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5, Gemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we use nonparametric binscatter and linear regression techniques to analyze over 29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals excluded from the training data of current LLMs, along with a set of AI-generated submissions. The results show that LLMs consistently distinguish between higher- and lower-quality research based solely on textual content, producing quality gradients that closely align with established journal prestige measures. Claude and Gemma perform exceptionally well in capturing these gradients, while GPT excels in detecting AI-generated content. The second experiment comprises 8,910 evaluations designed to assess whether LLMs replicate human like biases in single blind reviews. By systematically varying author gender, institutional affiliation, and academic prominence across 330 papers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings to submissions from top male authors and elite institutions relative to the same papers presented anonymously. These results emphasize the importance of excluding author-identifying information when deploying LLMs in editorial screening. Overall, our findings provide compelling evidence and practical guidance for integrating LLMs into peer review to enhance efficiency, improve accuracy, and promote equity in the publication process of economics research.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.00070', 156)">Copy Link</button>
<div id="copy-message-156" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.02885">Expertized Caption Auto-Enhancement for Video-Text Retrieval</a></h1>
<p><b>Authors:</b> Baoyao Yang, Junxiang Chen, Wanyun Li, Wenbin Yao, Yang Zhou</p>
<p>Abstract: Video-text retrieval has been stuck in the information mismatch caused by personalized and inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders an effective cross-modal representation alignment, resulting in ambiguous retrieval results. Although text rewriting methods have been proposed to broaden text expressions, the modality gap remains significant, as the text representation space is hardly expanded with insufficient semantic enrichment.Instead, this paper turns to enhancing visual presentation, bridging video expression closer to textual representation via caption generation and thereby facilitating video-text matching.While multimodal large language models (mLLM) have shown a powerful capability to convert video content into text, carefully crafted prompts are essential to ensure the reasonableness and completeness of the generated captions. Therefore, this paper proposes an automatic caption enhancement method that improves expression quality and mitigates empiricism in augmented captions through self-learning.Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, further exploring the utilization potential of caption augmentation.Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo. Our code is publicly available at https://github.com/CaryXiang/ECA4VTR.</p>
<p>URLs: <a href="https://github.com/CaryXiang/ECA4VTR.">https://github.com/CaryXiang/ECA4VTR.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02885, https://github.com/CaryXiang/ECA4VTR.', 157)">Copy Link</button>
<div id="copy-message-157" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.03123">Disentanglement in Difference: Directly Learning Semantically Disentangled Representations by Maximizing Inter-Factor Differences</a></h1>
<p><b>Authors:</b> Xingshen Zhang, Lin Wang, Shuangrong Liu, Xintao Lu, Chaoran Pang, Bo Yang</p>
<p>Abstract: In this study, Disentanglement in Difference(DiD) is proposed to address the inherent inconsistency between the statistical independence of latent variables and the goal of semantic disentanglement in disentanglement representation learning. Conventional disentanglement methods achieve disentanglement representation by improving statistical independence among latent variables. However, the statistical independence of latent variables does not necessarily imply that they are semantically unrelated, thus, improving statistical independence does not always enhance disentanglement performance. To address the above issue, DiD is proposed to directly learn semantic differences rather than the statistical independence of latent variables. In the DiD, a Difference Encoder is designed to measure the semantic differences; a contrastive loss function is established to facilitate inter-dimensional comparison. Both of them allow the model to directly differentiate and disentangle distinct semantic factors, thereby resolving the inconsistency between statistical independence and semantic disentanglement. Experimental results on the dSprites and 3DShapes datasets demonstrate that the proposed DiD outperforms existing mainstream methods across various disentanglement metrics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03123', 158)">Copy Link</button>
<div id="copy-message-158" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.06829">Convolution-Based Converter : A Weak-Prior Approach For Modeling Stochastic Processes Based On Conditional Density Estimation</a></h1>
<p><b>Authors:</b> Chaoran Pang, Lin Wang, Shuangrong Liu, Shikun Tian, WenHao Yue, Xingshen Zhang, Bo Yang</p>
<p>Abstract: In this paper, a Convolution-Based Converter (CBC) is proposed to develop a methodology for removing the strong or fixed priors in estimating the probability distribution of targets based on observations in the stochastic process. Traditional approaches, e.g., Markov-based and Gaussian process-based methods, typically leverage observations to estimate targets based on strong or fixed priors (such as Markov properties or Gaussian prior). However, the effectiveness of these methods depends on how well their prior assumptions align with the characteristics of the problem. When the assumed priors are not satisfied, these approaches may perform poorly or even become unusable. To overcome the above limitation, we introduce the Convolution-Based converter (CBC), which implicitly estimates the conditional probability distribution of targets without strong or fixed priors, and directly outputs the expected trajectory of the stochastic process that satisfies the constraints from observations. This approach reduces the dependence on priors, enhancing flexibility and adaptability in modeling stochastic processes when addressing different problems. Experimental results demonstrate that our method outperforms existing baselines across multiple metrics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.06829', 159)">Copy Link</button>
<div id="copy-message-159" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.07156">Explaining 3D Computed Tomography Classifiers with Counterfactuals</a></h1>
<p><b>Authors:</b> Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari</p>
<p>Abstract: Counterfactual explanations enhance the interpretability of deep learning models in medical imaging, yet adapting them to 3D CT scans poses challenges due to volumetric complexity and resource demands. We extend the Latent Shift counterfactual generation method from 2D applications to explain 3D computed tomography (CT) scans classifiers. We address the challenges associated with 3D classifiers, such as limited training samples and high memory demands, by implementing a slice-based autoencoder and gradient blocking except for specific chunks of slices. This method leverages a 2D encoder trained on CT slices, which are subsequently combined to maintain 3D context. We demonstrate this technique on two models for clinical phenotype prediction and lung segmentation. Our approach is both memory-efficient and effective for generating interpretable counterfactuals in high-resolution 3D medical imaging.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.07156', 160)">Copy Link</button>
<div id="copy-message-160" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.16756">Towards Reinforcement Learning for Exploration of Speculative Execution Vulnerabilities</a></h1>
<p><b>Authors:</b> Evan Lai, Wenjie Xiong, Edward Suh, Mohit Tiwari, Mulong Luo</p>
<p>Abstract: Speculative attacks such as Spectre can leak secret information without being discovered by the operating system. Speculative execution vulnerabilities are finicky and deep in the sense that to exploit them, it requires intensive manual labor and intimate knowledge of the hardware. In this paper, we introduce SpecRL, a framework that utilizes reinforcement learning to find speculative execution leaks in post-silicon (black box) microprocessors.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.16756', 161)">Copy Link</button>
<div id="copy-message-161" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.16923">A Systematic Survey of Automatic Prompt Optimization Techniques</a></h1>
<p><b>Authors:</b> Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, Panpan Xu, Lin Lee Cheong</p>
<p>Abstract: Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.16923', 162)">Copy Link</button>
<div id="copy-message-162" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.19790">Mixtera: A Data Plane for Foundation Model Training</a></h1>
<p><b>Authors:</b> Maximilian B\"other, Xiaozhe Yao, Tolga Kerimoglu, Dan Graur, Viktor Gsteiger, Ana Klimovic</p>
<p>Abstract: State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources. As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors. Yet recent research shows that the data mixture and the order in which samples are visited during training can significantly influence model accuracy. We build and present Mixtera, a data plane for foundation model training that enables users to declaratively express which data samples should be used in which proportion and in which order during training. Mixtera is a centralized, read-only layer that is deployed on top of existing training data collections and can be declaratively queried. It operates independently of the filesystem structure and supports mixtures across arbitrary properties (e.g., language, source dataset) as well as dynamic adjustment of the mixture based on model feedback. We experimentally evaluate Mixtera and show that our implementation does not bottleneck training and scales to 256 GH200 superchips. We demonstrate how Mixtera supports recent advancements in mixing strategies by implementing the proposed Adaptive Data Optimization (ADO) algorithm in the system and evaluating its performance impact. We also explore the role of mixtures for vision-language models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.19790', 163)">Copy Link</button>
<div id="copy-message-163" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.00383">Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</a></h1>
<p><b>Authors:</b> Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Ling-Yu Duan, Alex C. Kot, Xudong Jiang</p>
<p>Abstract: By locally encoding raw data into intermediate features, collaborative inference enables end users to leverage powerful deep learning models without exposure of sensitive raw data to cloud servers. However, recent studies have revealed that these intermediate features may not sufficiently preserve privacy, as information can be leaked and raw data can be reconstructed via model inversion attacks (MIAs). Obfuscation-based methods, such as noise corruption, adversarial representation learning, and information filters, enhance the inversion robustness by obfuscating the task-irrelevant redundancy empirically. However, methods for quantifying such redundancy remain elusive, and the explicit mathematical relation between this redundancy minimization and inversion robustness enhancement has not yet been established. To address that, this work first theoretically proves that the conditional entropy of inputs given intermediate features provides a guaranteed lower bound on the reconstruction mean square error (MSE) under any MIA. Then, we derive a differentiable and solvable measure for bounding this conditional entropy based on the Gaussian mixture estimation and propose a conditional entropy maximization (CEM) algorithm to enhance the inversion robustness. Experimental results on four datasets demonstrate the effectiveness and adaptability of our proposed CEM; without compromising feature utility and computing efficiency, plugging the proposed CEM into obfuscation-based defense mechanisms consistently boosts their inversion robustness, achieving average gains ranging from 12.9\% to 48.2\%. Code is available at \href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}.</p>
<p>URLs: <a href="https://github.com/xiasong0501/CEM">https://github.com/xiasong0501/CEM</a>, <a href="https://github.com/xiasong0501/CEM">https://github.com/xiasong0501/CEM</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.00383, https://github.com/xiasong0501/CEM, https://github.com/xiasong0501/CEM', 164)">Copy Link</button>
<div id="copy-message-164" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.06212">GraphGen+: Advancing Distributed Subgraph Generation and Graph Learning On Industrial Graphs</a></h1>
<p><b>Authors:</b> Yue Jin, Yongchao Liu, Chuntao Hong</p>
<p>Abstract: Graph-based computations are crucial in a wide range of applications, where graphs can scale to trillions of edges. To enable efficient training on such large graphs, mini-batch subgraph sampling is commonly used, which allows training without loading the entire graph into memory. However, existing solutions face significant trade-offs: online subgraph generation, as seen in frameworks like DGL and PyG, is limited to a single machine, resulting in severe performance bottlenecks, while offline precomputed subgraphs, as in GraphGen, improve sampling efficiency but introduce large storage overhead and high I/O costs during training. To address these challenges, we propose \textbf{GraphGen+}, an integrated framework that synchronizes distributed subgraph generation with in-memory graph learning, eliminating the need for external storage while significantly improving efficiency. GraphGen+ achieves a \textbf{27$\times$} speedup in subgraph generation compared to conventional SQL-like methods and a \textbf{1.3$\times$} speedup over GraphGen, supporting training on 1 million nodes per iteration and removing the overhead associated with precomputed subgraphs, making it a scalable and practical solution for industry-scale graph learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.06212', 165)">Copy Link</button>
<div id="copy-message-165" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.06790">GenDR: Lightning Generative Detail Restorator</a></h1>
<p><b>Authors:</b> Yan Wang, Shijie Zhao, Kai Chen, Kexin Zhang, Junlin Li, Li Zhang</p>
<p>Abstract: Recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable success. However, fundamental misalignments between T2I and SR targets result in a dilemma between inference speed and detail fidelity. Specifically, T2I tasks prioritize multi-step inversion to synthesize coherent outputs aligned with textual prompts and shrink the latent space to reduce generating complexity. Contrariwise, SR tasks preserve most information from low-resolution input while solely restoring high-frequency details, thus necessitating sufficient latent space and fewer inference steps. To bridge the gap, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand latent space without enlarging the model size. Regarding step-distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.06790', 166)">Copy Link</button>
<div id="copy-message-166" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.09436">PromptMap: An Alternative Interaction Style for AI-Based Image Generation</a></h1>
<p><b>Authors:</b> Krzysztof Adamkiewicz, Pawe{\l} W. Wo\'zniak, Julia Dominiak, Andrzej Romanowski, Jakob Karolus, Stanislav Frolov</p>
<p>Abstract: Recent technological advances popularized the use of image generation among the general public. Crafting effective prompts can, however, be difficult for novice users. To tackle this challenge, we developed PromptMap, a new interaction style for text-to-image AI that allows users to freely explore a vast collection of synthetic prompts through a map-like view with semantic zoom. PromptMap groups images visually by their semantic similarity, allowing users to discover relevant examples. We evaluated PromptMap in a between-subject online study ($n=60$) and a qualitative within-subject study ($n=12$). We found that PromptMap supported users in crafting prompts by providing them with examples. We also demonstrated the feasibility of using LLMs to create vast example collections. Our work contributes a new interaction style that supports users unfamiliar with prompting in achieving a satisfactory image output.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.09436', 167)">Copy Link</button>
<div id="copy-message-167" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.15275">Challenges and Trends in Egocentric Vision: A Survey</a></h1>
<p><b>Authors:</b> Xiang Li, Heqian Qiu, Lanxiao Wang, Hanwen Zhang, Chenghao Qi, Linfeng Han, Huiyu Xiong, Hongliang Li</p>
<p>Abstract: With the rapid development of artificial intelligence technologies and wearable devices, egocentric vision understanding has emerged as a new and challenging research direction, gradually attracting widespread attention from both academia and industry. Egocentric vision captures visual and multimodal data through cameras or sensors worn on the human body, offering a unique perspective that simulates human visual experiences. This paper provides a comprehensive survey of the research on egocentric vision understanding, systematically analyzing the components of egocentric scenes and categorizing the tasks into four main areas: subject understanding, object understanding, environment understanding, and hybrid understanding. We explore in detail the sub-tasks within each category. We also summarize the main challenges and trends currently existing in the field. Furthermore, this paper presents an overview of high-quality egocentric vision datasets, offering valuable resources for future research. By summarizing the latest advancements, we anticipate the broad applications of egocentric vision technologies in fields such as augmented reality, virtual reality, and embodied intelligence, and propose future research directions based on the latest developments in the field.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.15275', 168)">Copy Link</button>
<div id="copy-message-168" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.16692">Limits of trust in medical AI</a></h1>
<p><b>Authors:</b> Joshua Hatherley</p>
<p>Abstract: Artificial intelligence (AI) is expected to revolutionize the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in a variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI's progress in medicine, however, has led to concerns regarding the potential effects of this technology upon relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied upon, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely upon AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.16692', 169)">Copy Link</button>
<div id="copy-message-169" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.16980">Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models</a></h1>
<p><b>Authors:</b> Haichao Zhang, Yun Fu</p>
<p>Abstract: Token-based video representation has emerged as a promising approach for enabling LLMs to interpret video content. However, existing token reduction, such as token pruning and token merging, often disrupt essential spatial-temporal positional embeddings, failing to adequately balance computational efficiency with fewer tokens. Consequently, these methods result in lengthy token sequences, limiting their applicability in scenarios requiring extreme token compression, such as video large language models. In this paper, we introduce the novel task of extreme short token reduction, aiming to represent extensive video sequences with a minimal number of tokens. To address this challenge, we propose Token Dynamics, a new video representation framework that dynamically reduces token count while preserving spatial-temporal coherence. Specifically, we disentangle video representations by separating visual embeddings from grid-level motion information, structuring them into: 1. a concise token hash table, created by clustering tokens that describe object-level content; 2. a token indices key map, capturing detailed spatial-temporal motion patterns across grids; 3. a token hash function, which vector-quantizes the token hash table to reconstruct the token sequence from the key map. Furthermore, we introduce a cross-dynamics attention mechanism that integrates motion features into the token base without increasing token length, thereby maintaining compactness and spatial-temporal integrity. The experiments demonstrate a reduction of token count to merely 0.07% of the original tokens, with only a minor performance drop of 1.13%. Additionally, we propose two novel subtasks within extreme token reduction (fixed-length and adaptive-length compression). Our method offers significantly lower theoretical complexity, fewer tokens, and enhanced throughput, thus providing an efficient solution for video LLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.16980', 170)">Copy Link</button>
<div id="copy-message-170" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.20871">VinaBench: Benchmark for Faithful and Consistent Visual Narratives</a></h1>
<p><b>Authors:</b> Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Syrielle Montariol, Antoine Bosselut</p>
<p>Abstract: Visual narrative generation transforms textual narratives into sequences of images illustrating the content of the text. However, generating visual narratives that are faithful to the input text and self-consistent across generated images remains an open challenge, due to the lack of knowledge constraints used for planning the stories. In this work, we propose a new benchmark, VinaBench, to address this challenge. Our benchmark annotates the underlying commonsense and discourse constraints in visual narrative samples, offering systematic scaffolds for learning the implicit strategies of visual storytelling. Based on the incorporated narrative constraints, we further propose novel metrics to closely evaluate the consistency of generated narrative images and the alignment of generations with the input textual narrative. Our results across three generative vision models demonstrate that learning with VinaBench's knowledge constraints effectively improves the faithfulness and cohesion of generated visual narratives.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.20871', 171)">Copy Link</button>
<div id="copy-message-171" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.22036">Cognitive Prompts Using Guilford&#x27;s Structure of Intellect Model</a></h1>
<p><b>Authors:</b> Oliver Kramer</p>
<p>Abstract: Large language models (LLMs) demonstrate strong language generation capabilities but often struggle with structured reasoning, leading to inconsistent or suboptimal problem-solving. To mitigate this limitation, Guilford's Structure of Intellect (SOI) model - a foundational framework from intelligence theory - is leveraged as the basis for cognitive prompt engineering. The SOI model categorizes cognitive operations such as pattern recognition, memory retrieval, and evaluation, offering a systematic approach to enhancing LLM reasoning and decision-making. This position paper presents a novel cognitive prompting approach for enforcing SOI-inspired reasoning for improving clarity, coherence, and adaptability in model responses.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.22036', 172)">Copy Link</button>
<div id="copy-message-172" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.22851">RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation</a></h1>
<p><b>Authors:</b> Feng Lin (Peter), Dong Jae Kim (Peter), Zhenhao Li (Peter), Jinqiu Yang (Peter),  Tse-Hsun (Peter),  Chen</p>
<p>Abstract: When using LLMs to address Non-Functional Requirements (NFRs), developers may behave differently (e.g., expressing the same NFR in different words). Robust LLMs should output consistent results across these variations; however, this aspect remains underexplored. We propose RobuNFR for evaluating the robustness of LLMs in NFR-aware code generation across four NFR dimensions: design, readability, reliability, and performance, using three methodologies: prompt variation, regression testing, and diverse workflows. Our experiments show that RobuNFR reveals robustness issues in the tested LLMs when considering NFRs in code generation. Specifically, under prompt variation, including NFRs leads to a decrease in Pass@1 by up to 39 percent and an increase in the standard deviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e., Function-Only). While incorporating NFRs generally improves overall NFR metrics, it also results in higher prompt sensitivity. In regression settings, some LLMs exhibit differences across versions, with improvements in one aspect (e.g., reduced code smells) often accompanied by regressions in another (e.g., decreased correctness), revealing inconsistencies that challenge their robustness. When varying workflows, the tested LLMs show significantly different NFR-aware code generation capabilities between two workflows: (1) integrating NFRs and functional requirements into the initial prompt and (2) enhancing Function-Only-generated code with the same NFR.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.22851', 173)">Copy Link</button>
<div id="copy-message-173" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.22879">Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</a></h1>
<p><b>Authors:</b> Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu</p>
<p>Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.</p>
<p>URLs: <a href="https://github.com/enyac-group/Quamba.">https://github.com/enyac-group/Quamba.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.22879, https://github.com/enyac-group/Quamba.', 174)">Copy Link</button>
<div id="copy-message-174" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.24108">PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis</a></h1>
<p><b>Authors:</b> Anwesa Choudhuri, Zhongpai Gao, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu</p>
<p>Abstract: Early detection, accurate segmentation, classification and tracking of polyps during colonoscopy are critical for preventing colorectal cancer. Many existing deep-learning-based methods for analyzing colonoscopic videos either require task-specific fine-tuning, lack tracking capabilities, or rely on domain-specific pre-training. In this paper, we introduce PolypSegTrack, a novel foundation model that jointly addresses polyp detection, segmentation, classification and unsupervised tracking in colonoscopic videos. Our approach leverages a novel conditional mask loss, enabling flexible training across datasets with either pixel-level segmentation masks or bounding box annotations, allowing us to bypass task-specific fine-tuning. Our unsupervised tracking module reliably associates polyp instances across frames using object queries, without relying on any heuristics. We leverage a robust vision foundation model backbone that is pre-trained unsupervisedly on natural images, thereby removing the need for domain-specific pre-training. Extensive experiments on multiple polyp benchmarks demonstrate that our method significantly outperforms existing state-of-the-art approaches in detection, segmentation, classification, and tracking.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.24108', 175)">Copy Link</button>
<div id="copy-message-175" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.24278">AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World</a></h1>
<p><b>Authors:</b> Zhiyuan Zhou, Pranav Atreya, You Liang Tan, Karl Pertsch, Sergey Levine</p>
<p>Abstract: Scalable and reproducible policy evaluation has been a long-standing challenge in robot learning. Evaluations are critical to assess progress and build better policies, but evaluation in the real world, especially at a scale that would provide statistically reliable results, is costly in terms of human time and hard to obtain. Evaluation of increasingly generalist robot policies requires an increasingly diverse repertoire of evaluation environments, making the evaluation bottleneck even more pronounced. To make real-world evaluation of robotic policies more practical, we propose AutoEval, a system to autonomously evaluate generalist robot policies around the clock with minimal human intervention. Users interact with AutoEval by submitting evaluation jobs to the AutoEval queue, much like how software jobs are submitted with a cluster scheduling system, and AutoEval will schedule the policies for evaluation within a framework supplying automatic success detection and automatic scene resets. We show that AutoEval can nearly fully eliminate human involvement in the evaluation process, permitting around the clock evaluations, and the evaluation results correspond closely to ground truth evaluations conducted by hand. To facilitate the evaluation of generalist policies in the robotics community, we provide public access to multiple AutoEval scenes in the popular BridgeData robot setup with WidowX robot arms. In the future, we hope that AutoEval scenes can be set up across institutions to form a diverse and distributed evaluation network.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.24278', 176)">Copy Link</button>
<div id="copy-message-176" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.00030">Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding</a></h1>
<p><b>Authors:</b> Aayush Gautam, Susav Shrestha, Narasimha Reddy</p>
<p>Abstract: Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \textit{GammaTune} and \textit{GammaTune+}, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\% ($\pm$5\%) with \textit{GammaTune} and 16\% ($\pm$3\%) with \textit{GammaTune+}, while reducing performance variance. This makes \textit{GammaTune} a robust and efficient solution for real-world deployment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.00030', 177)">Copy Link</button>
<div id="copy-message-177" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.00441">No Free Lunch with Guardrails</a></h1>
<p><b>Authors:</b> Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi</p>
<p>Abstract: As large language models (LLMs) and generative AI become widely adopted, guardrails have emerged as a key tool to ensure their safe use. However, adding guardrails isn't without tradeoffs; stronger security measures can reduce usability, while more flexible systems may leave gaps for adversarial attacks. In this work, we explore whether current guardrails effectively prevent misuse while maintaining practical utility. We introduce a framework to evaluate these tradeoffs, measuring how different guardrails balance risk, security, and usability, and build an efficient guardrail.
  Our findings confirm that there is no free lunch with guardrails; strengthening security often comes at the cost of usability. To address this, we propose a blueprint for designing better guardrails that minimize risk while maintaining usability. We evaluate various industry guardrails, including Azure Content Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI, Nemo Guardrails, and Enkrypt AI guardrails. Additionally, we assess how LLMs like GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest respond under different system prompts, including simple prompts, detailed prompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study provides a clear comparison of how different guardrails perform, highlighting the challenges in balancing security and usability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.00441', 178)">Copy Link</button>
<div id="copy-message-178" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.00457">Distilling Multi-view Diffusion Models into 3D Generators</a></h1>
<p><b>Authors:</b> Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</p>
<p>Abstract: We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher's probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: https://qinbaigao.github.io/DD3G_project/</p>
<p>URLs: <a href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.00457, https://qinbaigao.github.io/DD3G_project/', 179)">Copy Link</button>
<div id="copy-message-179" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01128">RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety</a></h1>
<p><b>Authors:</b> Andrei Dumitriu, Florin Tatui, Florin Miron, Aakash Ralhan, Radu Tudor Ionescu, Radu Timofte</p>
<p>Abstract: Rip currents are strong, localized and narrow currents of water that flow outwards into the sea, causing numerous beach-related injuries and fatalities worldwide. Accurate identification of rip currents remains challenging due to their amorphous nature and the lack of annotated data, which often requires expert knowledge. To address these issues, we present RipVIS, a large-scale video instance segmentation benchmark explicitly designed for rip current segmentation. RipVIS is an order of magnitude larger than previous datasets, featuring $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$ frames) are with rip currents, collected from various sources, including drones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse visual contexts, such as wave-breaking patterns, sediment flows, and water color variations, across multiple global locations, including USA, Mexico, Costa Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New Zealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic scenarios, supplemented by an additional $34$ videos ($48,800$ frames) without rip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade Mask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip current segmentation. Results are reported in terms of multiple metrics, with a particular focus on the $F_2$ score to prioritize recall and reduce false negatives. To enhance segmentation performance, we introduce a novel post-processing step based on Temporal Confidence Aggregation (TCA). RipVIS aims to set a new standard for rip current segmentation, contributing towards safer beach environments. We offer a benchmark website to share data, models, and results with the research community, encouraging ongoing collaboration and future contributions, at https://ripvis.ai.</p>
<p>URLs: <a href="https://ripvis.ai.">https://ripvis.ai.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01128, https://ripvis.ai.', 180)">Copy Link</button>
<div id="copy-message-180" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01281">Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding</a></h1>
<p><b>Authors:</b> Sakhinana Sagar Srinivas, Venkataramana Runkana</p>
<p>Abstract: We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01281', 181)">Copy Link</button>
<div id="copy-message-181" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01515">Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis</a></h1>
<p><b>Authors:</b> Zixuan Wang, Duo Peng, Feng Chen, Yuwei Yang, Yinjie Lei</p>
<p>Abstract: Conditional image synthesis is a crucial task with broad applications, such as artistic creation and virtual reality. However, current generative methods are often task-oriented with a narrow scope, handling a restricted condition with constrained applicability. In this paper, we propose a novel approach that treats conditional image synthesis as the modular combination of diverse fundamental condition units. Specifically, we divide conditions into three primary units: text, layout, and drag. To enable effective control over these conditions, we design a dedicated alignment module for each. For the text condition, we introduce a Dense Concept Alignment (DCA) module, which achieves dense visual-text alignment by drawing on diverse textual concepts. For the layout condition, we propose a Dense Geometry Alignment (DGA) module to enforce comprehensive geometric constraints that preserve the spatial configuration. For the drag condition, we introduce a Dense Motion Alignment (DMA) module to apply multi-level motion regularization, ensuring that each pixel follows its desired trajectory without visual artifacts. By flexibly inserting and combining these alignment modules, our framework enhances the model's adaptability to diverse conditional generation tasks and greatly expands its application range. Extensive experiments demonstrate the superior performance of our framework across a variety of conditions, including textual description, segmentation mask (bounding box), drag manipulation, and their combinations. Code is available at https://github.com/ZixuanWang0525/DADG.</p>
<p>URLs: <a href="https://github.com/ZixuanWang0525/DADG.">https://github.com/ZixuanWang0525/DADG.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01515, https://github.com/ZixuanWang0525/DADG.', 182)">Copy Link</button>
<div id="copy-message-182" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01707">InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation</a></h1>
<p><b>Authors:</b> Bowen Cao, Deng Cai, Wai Lam</p>
<p>Abstract: In-context learning (ICL) is critical for large language models (LLMs), but its effectiveness is constrained by finite context windows, particularly in ultra-long contexts. To overcome this, we introduce InfiniteICL, a framework that parallels context and parameters in LLMs with short- and long-term memory in human cognitive systems, focusing on transforming temporary context knowledge into permanent parameter updates. This approach significantly reduces memory usage, maintains robust performance across varying input lengths, and theoretically enables infinite context integration through the principles of context knowledge elicitation, selection, and consolidation. Evaluations demonstrate that our method reduces context length by 90% while achieving 103% average performance of full-context prompting across fact recall, grounded reasoning, and skill acquisition tasks. When conducting sequential multi-turn transformations on complex, real-world contexts (with length up to 2M tokens), our approach surpasses full-context prompting while using only 0.4% of the original contexts. These findings highlight InfiniteICL's potential to enhance the scalability and efficiency of LLMs by breaking the limitations of conventional context window sizes.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01707', 183)">Copy Link</button>
<div id="copy-message-183" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01724">DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</a></h1>
<p><b>Authors:</b> Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu</p>
<p>Abstract: While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.</p>
<p>URLs: <a href="https://grisoon.github.io/DreamActor-M1/.">https://grisoon.github.io/DreamActor-M1/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01724, https://grisoon.github.io/DreamActor-M1/.', 184)">Copy Link</button>
<div id="copy-message-184" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01905">Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries</a></h1>
<p><b>Authors:</b> Furkan \c{C}olhak, Hasan Co\c{s}kun, Tsafac Nkombong Regine Cyrille, Tedi Hoxa, Mert \.Ilhan Ecevit, Mehmet Nafiz Ayd{\i}n</p>
<p>Abstract: The Internet of Vehicles (IoV) may face challenging cybersecurity attacks that may require sophisticated intrusion detection systems, necessitating a rapid development and response system. This research investigates the performance advantages of GPU-accelerated libraries (cuML) compared to traditional CPU-based implementations (scikit-learn), focusing on the speed and efficiency required for machine learning models used in IoV threat detection environments. The comprehensive evaluations conducted employ four machine learning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across three distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings demonstrate that GPU-accelerated implementations dramatically improved computational efficiency, with training times reduced by a factor of up to 159 and prediction speeds accelerated by up to 95 times compared to traditional CPU processing, all while preserving detection accuracy. This remarkable performance breakthrough empowers researchers and security specialists to harness GPU acceleration for creating faster, more effective threat detection systems that meet the urgent real-time security demands of today's connected vehicle networks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01905', 185)">Copy Link</button>
<div id="copy-message-185" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.01919">Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</a></h1>
<p><b>Authors:</b> Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal</p>
<p>Abstract: The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges such as hallucinations, evaluation inconsistencies, and inherited biases while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.01919', 186)">Copy Link</button>
<div id="copy-message-186" class="copy-message"></div>
</div>

    </div>
    </body>
    