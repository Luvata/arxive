<!DOCTYPE html>
<html>
<head>
<title>2023-11-07-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.01463">Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1">Muhammad Aurangzeb Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaramis_I/0/1/0/all/0/1">Ilker Yaramis</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1">Taposh Dutta Roy</a></p>
<p>Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01468">Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ciosici_M/0/1/0/all/0/1">Manuel R. Ciosici</a>, <a href="http://arxiv.org/find/cs/1/au:+Hedges_A/0/1/0/all/0/1">Alex Hedges</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanampati_Y/0/1/0/all/0/1">Yash Kankanampati</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1">Justin Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1">Marjorie Freedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1">Ralph Weischedel</a></p>
<p>We explore using a moderately sized large language model (GPT-J 6B
parameters) to create a plan for a simulated robot to achieve 30 classes of
goals in ScienceWorld, a text game simulator for elementary science
experiments. Previously published empirical work claimed that large language
models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement
learning. Using the Markov assumption (a single previous step), the LLM
outperforms the reinforcement learning-based approach by a factor of 1.4. When
we fill the LLM's input buffer with as many prior steps as possible,
improvement rises to 3.5x. Even when training on only 6.5% of the training
data, we observe a 2.2x improvement over the reinforcement-learning-based
approach. Our experiments show that performance varies widely across the 30
classes of actions, indicating that averaging over tasks can hide significant
performance issues. In work contemporaneous with ours, Lin et al. (2023)
demonstrated a two-part approach (SwiftSage) that uses a small LLM (T5-large)
complemented by OpenAI's massive LLMs to achieve outstanding results in
ScienceWorld. Our 6-B parameter, single-stage GPT-J matches the performance of
SwiftSage's two-stage architecture when it incorporates GPT-3.5 turbo which has
29-times more parameters than GPT-J.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01469">Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vinella_A/0/1/0/all/0/1">Avalon Vinella</a>, <a href="http://arxiv.org/find/cs/1/au:+Capetz_M/0/1/0/all/0/1">Margaret Capetz</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattichis_R/0/1/0/all/0/1">Rebecca Pattichis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chance_C/0/1/0/all/0/1">Christina Chance</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1">Reshmi Ghosh</a></p>
<p>In recent years, climate change repercussions have increasingly captured
public interest. Consequently, corporations are emphasizing their environmental
efforts in sustainability reports to bolster their public image. Yet, the
absence of stringent regulations in review of such reports allows potential
greenwashing. In this study, we introduce a novel methodology to train a
language model on generated labels for greenwashing risk. Our primary
contributions encompass: developing a mathematical formulation to quantify
greenwashing risk, a fine-tuned ClimateBERT model for this problem, and a
comparative analysis of results. On a test set comprising of sustainability
reports, our best model achieved an average accuracy score of 86.34% and F1
score of 0.67, demonstrating that our methods show a promising direction of
exploration for this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01472">Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Jaeff Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dung_D/0/1/0/all/0/1">Duong Dung</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutchinson_D/0/1/0/all/0/1">Danielle Hutchinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1">Zubair Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Rosalie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dawson_R/0/1/0/all/0/1">Rebecca Dawson</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Aditya Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Samsung Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+MacIntyre_C/0/1/0/all/0/1">C Raina MacIntyre</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurdasani_D/0/1/0/all/0/1">Deepti Gurdasani</a></p>
<p>Relation Extraction from News Articles (RENA) is a browser-based tool
designed to extract key entities and their semantic relationships in English
language news articles related to infectious diseases. Constructed using the
React framework, this system presents users with an elegant and user-friendly
interface. It enables users to input a news article and select from a choice of
two models to generate a comprehensive list of relations within the provided
text. As a result, RENA allows real-time parsing of news articles to extract
key information for epidemic surveillance, contributing to EPIWATCH, an
open-source intelligence-based epidemic warning system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01487">What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yifan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hangyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chuyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1">Mingchen Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ruihua Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Visual instruction tuning is an essential approach to improving the zero-shot
generalization capability of Multi-modal Large Language Models (MLLMs). A surge
of visual instruction datasets with various focuses and characteristics have
been proposed recently, enabling MLLMs to achieve surprising results on
evaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to
investigate a more fundamental question: ``what makes for good visual
instructions?''. By conducting a comprehensive empirical study, we find that
instructions focused on complex visual reasoning tasks are particularly
effective in improving the performance of MLLMs on evaluation benchmarks.
Building upon this finding, we design a systematic approach to automatically
creating high-quality complex visual reasoning instructions. Our approach
employs a synthesis-complication-reformulation paradigm, leveraging multiple
stages to gradually increase the complexity of the instructions while
guaranteeing quality. Based on this approach, we create the synthetic visual
reasoning instruction dataset consisting of 32K examples, namely ComVint, and
fine-tune four MLLMs on it. Experimental results demonstrate that our dataset
consistently enhances the performance of all the compared MLLMs, e.g.,
improving the performance of MiniGPT-4 and BLIP-2 on MME-Cognition by 32.6% and
28.8%, respectively. Our code and data are publicly available at the link:
https://github.com/RUCAIBox/ComVint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01544">Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1">Bj&#xf6;rn Deiseroth</a>, <a href="http://arxiv.org/find/cs/1/au:+Meuer_M/0/1/0/all/0/1">Max Meuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gritsch_N/0/1/0/all/0/1">Nikolas Gritsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1">Constantin Eichenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1">Matthias A&#xdf;enmacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>Large Language Models (LLMs) have reshaped natural language processing with
their impressive capabilities. Their ever-increasing size, however, raised
concerns about their effective deployment and the need for LLM compressions.
This study introduces the Divergent Token metrics (DTMs), a novel approach for
assessing compressed LLMs, addressing the limitations of traditional measures
like perplexity that fail to accurately reflect text generation quality. DTMs
focus on token divergence, providing deeper insights into the subtleties of
model compression. Our results indicate that significant levels of precision
and sparsity can be achieved without compromising text generation quality.
Moreover, DTMs offers a more precise evaluation of each component's impact
individually. Utilizing the First Divergent Token metric (FDTM) in model
sparsification reveals that nearly 20% of all components can be pruned over
90%. In terms of quantization, the FDTM suggests that over 80% of parameters
can be straightforwardly transformed to int8 without special outlier
management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01555">Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weiwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1">Lingyong Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1">Pengjie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhumin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhaochun Ren</a></p>
<p>Recent studies have demonstrated the great potential of Large Language Models
(LLMs) serving as zero-shot relevance rankers. The typical approach involves
making comparisons between pairs or lists of documents. Although effective,
these listwise and pairwise methods are not efficient and also heavily rely on
intricate prompt engineering. To tackle this problem, we introduce a novel
instruction distillation method. The key idea is to distill the pairwise
ranking ability of open-sourced LLMs to a simpler but more efficient pointwise
ranking. Specifically, given the same LLM, we first rank documents using the
effective pairwise approach with complex instructions, and then distill the
teacher predictions to the pointwise approach with simpler instructions.
Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that
instruction distillation can improve efficiency by 10 to 100x and also enhance
the ranking performance of LLMs. Furthermore, our approach surpasses the
performance of existing supervised methods like monoT5 and is on par with the
state-of-the-art zero-shot methods. The code to reproduce our results is
available at www.github.com/sunnweiwei/RankGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01571">Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Mohammad Junayed Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Noor_S/0/1/0/all/0/1">Suhra Noor</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Ashrafuzzaman Khan</a></p>
<p>Clinical texts, such as admission notes, discharge summaries, and progress
notes, contain rich and valuable information that can be used for various
clinical outcome prediction tasks. However, applying large language models,
such as BERT-based models, to clinical texts poses two major challenges: the
limitation of input length and the diversity of data sources. This paper
proposes a novel method to preserve the knowledge of long clinical texts using
aggregated ensembles of large language models. Unlike previous studies which
use model ensembling or text aggregation methods separately, we combine
ensemble learning with text aggregation and train multiple large language
models on two clinical outcome tasks: mortality prediction and length of stay
prediction. We show that our method can achieve better results than baselines,
ensembling, and aggregation individually, and can improve the performance of
large language models while handling long inputs and diverse datasets. We
conduct extensive experiments on the admission notes from the MIMIC-III
clinical database by combining multiple unstructured and high-dimensional
datasets, demonstrating our method's effectiveness and superiority over
existing approaches. We also provide a comprehensive analysis and discussion of
our results, highlighting our method's applications and limitations for future
research in the domain of clinical healthcare. The results and analysis of this
study is supportive of our method assisting in clinical healthcare systems by
enabling clinical decision-making with robust performance overcoming the
challenges of long text inputs and varied datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01580">MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Guangyue Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1">Parisa Kordjamshidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Joyce Chai</a></p>
<p>Humans have the ability to learn novel compositional concepts by recalling
and generalizing primitive concepts acquired from past experiences. Inspired by
this observation, in this paper, we propose MetaReVision, a retrieval-enhanced
meta-learning model to address the visually grounded compositional concept
learning problem. The proposed MetaReVision consists of a retrieval module and
a meta-learning module which are designed to incorporate retrieved primitive
concepts as a supporting set to meta-train vision-anguage models for grounded
compositional concept recognition. Through meta-learning from episodes
constructed by the retriever, MetaReVision learns a generic compositional
representation that can be fast updated to recognize novel compositional
concepts. We create CompCOCO and CompFlickr to benchmark the grounded
compositional concept learning. Our experimental results show that MetaReVision
outperforms other competitive baselines and the retrieval module plays an
important role in this compositional learning process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01605">Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lopardo_G/0/1/0/all/0/1">Gianluigi Lopardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1">Frederic Precioso</a>, <a href="http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1">Damien Garreau</a></p>
<p>Interpretability is essential for machine learning models to be trusted and
deployed in critical domains. However, existing methods for interpreting text
models are often complex, lack solid mathematical foundations, and their
performance is not guaranteed. In this paper, we propose FRED (Faithful and
Robust Explainer for textual Documents), a novel method for interpreting
predictions over text. FRED identifies key words in a document that
significantly impact the prediction when removed. We establish the reliability
of FRED through formal definitions and theoretical analyses on interpretable
classifiers. Additionally, our empirical evaluation against state-of-the-art
methods demonstrates the effectiveness of FRED in providing insights into text
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01606">KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations. (arXiv:2311.01606v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ozsoy_G/0/1/0/all/0/1">G&#xf6;kberk &#xd6;zsoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Salamanca_L/0/1/0/all/0/1">Luis Salamanca</a>, <a href="http://arxiv.org/find/cs/1/au:+Connelly_M/0/1/0/all/0/1">Matthew Connelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Hicks_R/0/1/0/all/0/1">Raymond Hicks</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1">Fernando P&#xe9;rez-Cruz</a></p>
<p>In the current paper, we present the KG-FRUS dataset, comprised of more than
300,000 US government diplomatic documents encoded in a Knowledge Graph (KG).
We leverage the data of the Foreign Relations of the United States (FRUS)
(available as XML files) to extract information about the documents and the
individuals and countries mentioned within them. We use the extracted entities,
and associated metadata, to create a graph-based dataset. Further, we
supplement the created KG with additional entities and relations from Wikidata.
The relations in the KG capture the synergies and dynamics required to study
and understand the complex fields of diplomacy, foreign relations, and
politics. This goes well beyond a simple collection of documents which neglects
the relations between entities in the text. We showcase a range of
possibilities of the current dataset by illustrating different approaches to
probe the KG. In the paper, we exemplify how to use a query language to answer
simple research questions and how to use graph algorithms such as Node2Vec and
PageRank, that benefit from the complete graph structure. More importantly, the
chosen structure provides total flexibility for continuously expanding and
enriching the graph. Our solution is general, so the proposed pipeline for
building the KG can encode other original corpora of time-dependent and complex
phenomena. Overall, we present a mechanism to create KG databases providing a
more versatile representation of time-dependent related text data and a
particular application to the all-important FRUS database.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01615">FLAP: Fast Language-Audio Pre-training. (arXiv:2311.01615v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1">Ching-Feng Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Po-Yao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1">Vasu Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shang-Wen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gosh_G/0/1/0/all/0/1">Gargi Gosh</a></p>
<p>We propose Fast Language-Audio Pre-training (FLAP), a self-supervised
approach that efficiently and effectively learns aligned audio and language
representations through masking, contrastive learning and reconstruction. For
efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on
the remaining ones for self-supervision. Through inter-modal contrastive
learning, FLAP learns to align paired audio and text representations in a
shared latent space. Notably, FLAP leverages multiple augmented views via
masking for inter-modal contrast and learns to reconstruct the masked portion
of audio tokens. Moreover, FLAP leverages large language models (LLMs) to
augment the text inputs, contributing to improved performance. These approaches
lead to more robust and informative audio-text representations, enabling FLAP
to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on
AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01620">ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos. (arXiv:2311.01620v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Te-Lin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zi-Yi Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qingyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yu Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_N/0/1/0/all/0/1">Nischal Reddy Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1">Marjorie Freedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1">Ralph M. Weischedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Multimodal counterfactual reasoning is a vital yet challenging ability for AI
systems. It involves predicting the outcomes of hypothetical circumstances
based on vision and language inputs, which enables AI models to learn from
failures and explore hypothetical scenarios. Despite its importance, there are
only a few datasets targeting the counterfactual reasoning abilities of
multimodal models. Among them, they only cover reasoning over synthetic
environments or specific types of events (e.g. traffic collisions), making them
hard to reliably benchmark the model generalization ability in diverse
real-world scenarios and reasoning dimensions. To overcome these limitations,
we develop a video question answering dataset, ACQUIRED: it consists of 3.9K
annotated videos, encompassing a wide range of event types and incorporating
both first and third-person viewpoints, which ensures a focus on real-world
diversity. In addition, each video is annotated with questions that span three
distinct dimensions of reasoning, including physical, social, and temporal,
which can comprehensively evaluate the model counterfactual abilities along
multiple aspects. We benchmark our dataset against several state-of-the-art
language-only and multimodal models and experimental results demonstrate a
significant performance gap (&gt;13%) between models and humans. The findings
suggest that multimodal counterfactual reasoning remains an open challenge and
ACQUIRED is a comprehensive and reliable benchmark for inspiring future
research in this direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01623">VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhenting Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanchen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Pengzhan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Padmanabhan_A/0/1/0/all/0/1">Arthi Padmanabhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1">Hugo Latapie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Harry Xu</a></p>
<p>Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01650">MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ates_H/0/1/0/all/0/1">Halim Cagri Ates</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1">Shruti Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Site Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Maddula_S/0/1/0/all/0/1">Siddhardha Maddula</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Nalamalapu_A/0/1/0/all/0/1">Anil Kumar Nalamalapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1">Roman Hoang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozyildirim_M/0/1/0/all/0/1">Melis Ozyildirim</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Alkesh Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1">Dhivya Piraviperumal</a>, <a href="http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1">Vincent Renkens</a>, <a href="http://arxiv.org/find/cs/1/au:+Samal_A/0/1/0/all/0/1">Ankit Samal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Thy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1">Bo-Hsiang Tseng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1">Rong Zou</a></p>
<p>Successfully handling context is essential for any dialog understanding task.
This context maybe be conversational (relying on previous user queries or
system responses), visual (relying on what the user sees, for example, on their
screen), or background (based on signals such as a ringing alarm or playing
music). In this work, we present an overview of MARRS, or Multimodal Reference
Resolution System, an on-device framework within a Natural Language
Understanding system, responsible for handling conversational, visual and
background context. In particular, we present different machine learning models
to enable handing contextual queries; specifically, one to enable reference
resolution, and one to handle context via query rewriting. We also describe how
these models complement each other to form a unified, coherent, lightweight
system that can understand context while preserving user privacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01666">Plot Retrieval as an Assessment of Abstract Semantic Association. (arXiv:2311.01666v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shicheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1">Mo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Retrieving relevant plots from the book for a query is a critical task, which
can improve the reading experience and efficiency of readers. Readers usually
only give an abstract and vague description as the query based on their own
understanding, summaries, or speculations of the plot, which requires the
retrieval model to have a strong ability to estimate the abstract semantic
associations between the query and candidate plots. However, existing
information retrieval (IR) datasets cannot reflect this ability well. In this
paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the
performance of IR models on the novel task Plot Retrieval. Text pairs in Plot
Retrieval have less word overlap and more abstract semantic association, which
can reflect the ability of the IR models to estimate the abstract semantic
association, rather than just traditional lexical or semantic matching.
Extensive experiments across various lexical retrieval, sparse retrieval, dense
retrieval, and cross-encoder methods compared with human studies on Plot
Retrieval show current IR models still struggle in capturing abstract semantic
association between texts. Plot Retrieval can be the benchmark for further
research on the semantic association modeling ability of IR models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01677">DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1">Jiao Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junda Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yihong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuzheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1">Kun Gai</a></p>
<p>Large language models (LLMs) have achieved remarkable breakthroughs in new
dialogue capabilities, refreshing human's impressions on dialogue systems. The
long-standing goal of dialogue systems is to be human-like enough to establish
long-term connections with users by satisfying the need for communication,
affection and social belonging. Therefore, there has been an urgent need to
evaluate LLMs as human-like dialogue systems. In this paper, we propose
DialogBench, a dialogue evaluation benchmark that currently contains $12$
dialogue tasks to assess the capabilities of LLMs as human-like dialogue
systems should have. Specifically, we prompt GPT-4 to generate evaluation
instances for each task. We first design the basic prompt based on widely-used
design principles and further mitigate the existing biases to generate
higher-quality evaluation instances. Our extensive test over $28$ LLMs
(including pre-trained and supervised instruction-tuning) shows that
instruction fine-tuning benefits improve the human likeness of LLMs to a
certain extent, but there is still much room to improve those capabilities for
most LLMs as human-like dialogue systems. In addition, experimental results
also indicate that LLMs perform differently in various abilities that
human-like dialogue systems should have. We will publicly release DialogBench,
along with the associated evaluation code for the broader research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01684">CASE: Commonsense-Augmented Score with an Expanded Answer Space. (arXiv:2311.01684v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenkai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1">Sahithya Ravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1">Vered Shwartz</a></p>
<p>LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks
to the knowledge they acquired in their training. In multiple-choice QA tasks,
the LM probabilities are used as an imperfect measure of the plausibility of
each answer choice. One of the major limitations of the basic score is that it
treats all words as equally important. We propose CASE, a Commonsense-Augmented
Score with an Expanded Answer Space. CASE addresses this limitation by
assigning importance weights for individual words based on their semantic
relations to other words in the input. The dynamic weighting approach
outperforms basic LM scores, not only because it reduces noise from unimportant
words, but also because it informs the model of implicit commonsense knowledge
that may be useful for answering the question. We then also follow prior work
in expanding the answer space by generating lexically-divergent answers that
are conceptually-similar to the choices. When combined with answer space
expansion, our method outperforms strong baselines on 5 commonsense benchmarks.
We further show these two approaches are complementary and may be especially
beneficial when using smaller LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01689">Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1">Zheyuan Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinduo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hailin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tianyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a></p>
<p>Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the
model when original training data is unavailable. Previous works for DFKD in
NLP mainly focus on distilling encoder-only structures like BERT on
classification tasks, which overlook the notable progress of generative
language modeling. In this work, we propose a novel DFKD framework, namely
DFKD-T$^{3}$, where the pretrained generative language model can also serve as
a controllable data generator for model compression. This novel framework
DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to
transform the general domain corpus to compression-friendly task data,
targeting to improve both the \textit{specificity} and \textit{diversity}.
Extensive experiments show that our method can boost the distillation
performance in various downstream tasks such as sentiment analysis, linguistic
acceptability, and information extraction. Furthermore, we show that the
generated texts can be directly used for distilling other language models and
outperform the SOTA methods, making our method more appealing in a general DFKD
setting. Our code is available at
https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01712">A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers. (arXiv:2311.01712v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Beomjune Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eunsun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1">Dongbin Na</a></p>
<p>Many users reading online articles in various magazines may suffer
considerable difficulty in distinguishing the implicit intents in texts. In
this work, we focus on automatically recognizing the political intents of a
given online newspaper by understanding the context of the text. To solve this
task, we present a novel Korean text classification dataset that contains
various articles. We also provide deep-learning-based text classification
baseline models trained on the proposed dataset. Our dataset contains 12,000
news articles that may contain political intentions, from the politics section
of six of the most representative newspaper organizations in South Korea. All
the text samples are labeled simultaneously in two aspects (1) the level of
political orientation and (2) the level of pro-government. To the best of our
knowledge, our paper is the most large-scale Korean news dataset that contains
long text and addresses multi-task classification problems. We also train
recent state-of-the-art (SOTA) language models that are based on transformer
architectures and demonstrate that the trained models show decent text
classification performance. All the codes, datasets, and trained models are
available at https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01713">An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Junxian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haiqin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Junpeng_Y/0/1/0/all/0/1">Ye Junpeng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuxuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_H/0/1/0/all/0/1">Hao Mou</a></p>
<p>Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level
sentiment analysis. Current ASQP datasets are characterized by their small size
and low quadruple density, which hinders technical development. To expand
capacity, we construct two large Chinese ASQP datasets crawled from multiple
online platforms. The datasets hold several significant characteristics: larger
size (each with 10,000+ samples) and rich aspect categories, more words per
sentence, and higher density than existing ASQP datasets. Moreover, we are the
first to evaluate the performance of Generative Pre-trained Transformer (GPT)
series models on ASQP and exhibit potential issues. The experiments with
state-of-the-art ASQP baselines underscore the need to explore additional
techniques to address ASQP, as well as the importance of further investigation
into methods to improve the performance of GPTs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01732">Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sean Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1">Soroush Vosoughi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1">Saeed Hassanpour</a></p>
<p>Large Language Models (LLMs) have significantly advanced the field of Natural
Language Processing (NLP), but their lack of interpretability has been a major
concern. Current methods for interpreting LLMs are post hoc, applied after
inference time, and have limitations such as their focus on low-level features
and lack of explainability at higher level text units. In this work, we
introduce proto-lm, a prototypical network-based white-box framework that
allows LLMs to learn immediately interpretable embeddings during the
fine-tuning stage while maintaining competitive performance. Our method's
applicability and interpretability are demonstrated through experiments on a
wide range of NLP tasks, and our results indicate a new possibility of creating
interpretable models without sacrificing performance. This novel approach to
interpretability in LLMs can pave the way for more interpretable models without
the need to sacrifice performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01740">SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. (arXiv:2311.01740v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuohang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1">Kamalika Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Malin_B/0/1/0/all/0/1">Bradley A. Malin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sricharan Kumar</a></p>
<p>Hallucination detection is a critical step toward understanding the
trustworthiness of modern language models (LMs). To achieve this goal, we
re-examine existing detection approaches based on the self-consistency of LMs
and uncover two types of hallucinations resulting from 1) question-level and 2)
model-level, which cannot be effectively identified through self-consistency
check alone. Building upon this discovery, we propose a novel sampling-based
method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on
the principle of self-consistency checking. Our SAC$^3$ approach incorporates
additional mechanisms to detect both question-level and model-level
hallucinations by leveraging advances including semantically equivalent
question perturbation and cross-model response consistency checking. Through
extensive and systematic empirical analysis, we demonstrate that SAC$^3$
outperforms the state of the art in detecting both non-factual and factual
statements across multiple question-answering and open-domain generation
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01751">EmojiLM: Modeling the New Emoji Language. (arXiv:2311.01751v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Letian Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a></p>
<p>With the rapid development of the internet, online social media welcomes
people with different backgrounds through its diverse content. The increasing
usage of emoji becomes a noticeable trend thanks to emoji's rich information
beyond cultural or linguistic borders. However, the current study on emojis is
limited to single emoji prediction and there are limited data resources
available for further study of the interesting linguistic phenomenon. To this
end, we synthesize a large text-emoji parallel corpus, Text2Emoji, from a large
language model. Based on the parallel corpus, we distill a sequence-to-sequence
model, EmojiLM, which is specialized in the text-emoji bidirectional
translation. Extensive experiments on public benchmarks and human evaluation
demonstrate that our proposed model outperforms strong baselines and the
parallel corpus benefits emoji-related downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01757">Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suchrady_R/0/1/0/all/0/1">Randy Zakya Suchrady</a>, <a href="http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1">Ayu Purwarianti</a></p>
<p>Aspect-based sentiment analysis is a method in natural language processing
aimed at identifying and understanding sentiments related to specific aspects
of an entity. Aspects are words or phrases that represent an aspect or
attribute of a particular entity. Previous research has utilized generative
pre-trained language models to perform aspect-based sentiment analysis.
LEGO-ABSA is one framework that has successfully employed generative
pre-trained language models in aspect-based sentiment analysis, particularly in
English. LEGO-ABSA uses a multitask learning and prompting approach to enhance
model performance. However, the application of this approach has not been done
in the context of Bahasa Indonesia. Therefore, this research aims to implement
the multitask learning and prompting approach in aspect-based sentiment
analysis for Bahasa Indonesia using generative pre-trained language models. In
this study, the Indo LEGO-ABSA model is developed, which is an aspect-based
sentiment analysis model utilizing generative pre-trained language models and
trained with multitask learning and prompting. Indo LEGO-ABSA is trained with a
hotel domain dataset in the Indonesian language. The obtained results include
an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09%
for Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair
Extraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term
Extraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5
model, specifically mT5, by applying multitask learning to train all tasks
within aspect-based sentiment analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01766">Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weidong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shujun Li</a></p>
<p>Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01767">PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yiduo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zekai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yaobo Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_D/0/1/0/all/0/1">Duan Nan</a></p>
<p>Recent evaluations of Large Language Models (LLMs) have centered around
testing their zero-shot/few-shot capabilities for basic natural language tasks
and their ability to translate instructions into tool APIs. However, the
evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal
instructions in a complex multi-modal environment has not been investigated. To
address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark
to assess LLMs' ability to create and edit PPT files based on user
instructions. It contains 279 multi-turn sessions covering diverse topics and
hundreds of instructions involving multi-modal operations. We also propose the
PPTX-Match Evaluation System that evaluates if LLMs finish the instruction
based on the prediction file rather than the label API sequence, thus it
supports various LLM-generated API sequences. We measure 3 closed LLMs and 6
open-source LLMs. The results show that GPT-4 outperforms other LLMs with
75.1\% accuracy in single-turn dialogue testing but faces challenges in
completing entire sessions, achieving just 6\% session accuracy. We find three
main error causes in our benchmark: error accumulation in the multi-turn
session, long PPT template processing, and multi-modality perception. These
pose great challenges for future LLM and agent systems. We release the data,
code, and evaluation system of PPTC at \url{https://github.com/gydpku/PPTC}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01775">UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis. (arXiv:2311.01775v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ruiqi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianyi Liu</a></p>
<p>Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated
by linguistic steganography. Existing LS methods overlook the distinctive user
characteristics, leading to weak performance in social networks. The limited
occurrence of stegos further complicates detection. In this paper, we propose
the UP4LS, a novel framework with the User Profile for enhancing LS
performance. Specifically, by delving into post content, we explore user
attributes like writing habits, psychological states, and focal areas, thereby
building the user profile for LS. For each attribute, we design the identified
feature extraction module. The extracted features are mapped to
high-dimensional user features via deep-learning networks from existing
methods. Then the language model is employed to extract content features. The
user and content features are integrated to optimize feature representation.
During the training phase, we prioritize the distribution of stegos.
Experiments demonstrate that UP4LS can significantly enhance the performance of
existing methods, and an overall accuracy improvement of nearly 25%. In
particular, the improvement is especially pronounced with fewer stego samples.
Additionally, UP4LS also sets the stage for studies on related tasks,
encouraging extensive applications on LS tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01786">TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guoxing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jianyu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaohong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangyu Wang</a></p>
<p>Pre-training and fine-tuning have emerged as a promising paradigm across
various natural language processing (NLP) tasks. The effectiveness of
pretrained large language models (LLM) has witnessed further enhancement,
holding potential for applications in the field of medicine, particularly in
the context of Traditional Chinese Medicine (TCM). However, the application of
these general models to specific domains often yields suboptimal results,
primarily due to challenges like lack of domain knowledge, unique objectives,
and computational efficiency. Furthermore, their effectiveness in specialized
domains, such as Traditional Chinese Medicine, requires comprehensive
evaluation. To address the above issues, we propose a novel domain specific
TCMDA (TCM Domain Adaptation) approach, efficient pre-training with
domain-specific corpus. Specifically, we first construct a large TCM-specific
corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from
general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained
model's weights and uses rank decomposition matrices to efficiently train
specific dense layers for pre-training and fine-tuning, efficiently aligning
the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted
extensive experiments on two TCM tasks, including TCM examination and TCM
diagnosis. TCM-GPT-7B archived the best performance across both datasets,
outperforming other models by relative increments of 17% and 12% in accuracy,
respectively. To the best of our knowledge, our study represents the pioneering
validation of domain adaptation of a large language model with 7 billion
parameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B
model once accepted to facilitate interdisciplinary development in TCM and NLP,
serving as the foundation for further study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01792">AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yijia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shijie Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1">Dayou Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jianyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1">Ting Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Ningyi Xu</a></p>
<p>Large language models (LLMs) show great performance in various tasks, but
face deployment challenges from limited memory capacity and bandwidth. Low-bit
weight quantization can save memory and accelerate inference. Although
floating-point (FP) formats show good performance in LLM quantization, they
tend to perform poorly with small group sizes or sub-4 bits. We find the reason
is that the absence of asymmetry in previous FP quantization makes it
unsuitable for handling asymmetric value distribution of LLM weight tensors. In
this work, we propose asymmetric FP quantization (AFPQ), which sets separate
scales for positive and negative values. Our method leads to large accuracy
improvements and can be easily plugged into other quantization methods,
including GPTQ and AWQ, for better performance. Besides, no additional storage
is needed compared with asymmetric integer (INT) quantization. The code is
available at https://github.com/zhangsichengsjtu/AFPQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01817">Mitigating Framing Bias with Polarity Minimization Loss. (arXiv:2311.01817v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1">Yejin Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1">Pascale Fung</a></p>
<p>Framing bias plays a significant role in exacerbating political polarization
by distorting the perception of actual events. Media outlets with divergent
political stances often use polarized language in their reporting of the same
event. We propose a new loss function that encourages the model to minimize the
polarity difference between the polarized input articles to reduce framing
bias. Specifically, our loss is designed to jointly optimize the model to map
polarity ends bidirectionally. Our experimental results demonstrate that
incorporating the proposed polarity minimization loss leads to a substantial
reduction in framing bias when compared to a BART-based multi-document
summarization model. Notably, we find that the effectiveness of this approach
is most pronounced when the model is trained to minimize the polarity loss
associated with informational framing bias (i.e., skewed selection of
information to report).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01820">Minimalist Grammar: Construction without Overgeneration. (arXiv:2311.01820v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maier_I/0/1/0/all/0/1">Isidor Konrad Maier</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1">Johannes Kuhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Beisegel_J/0/1/0/all/0/1">Jesse Beisegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_Liebl_M/0/1/0/all/0/1">Markus Huber-Liebl</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolff_M/0/1/0/all/0/1">Matthias Wolff</a></p>
<p>In this paper we give instructions on how to write a minimalist grammar (MG).
In order to present the instructions as an algorithm, we use a variant of
context free grammars (CFG) as an input format. We can exclude overgeneration,
if the CFG has no recursion, i.e. no non-terminal can (indirectly) derive to a
right-hand side containing itself. The constructed MGs utilize licensors/-ees
as a special way of exception handling. A CFG format for a derivation
$A\_eats\_B\mapsto^* peter\_eats\_apples$, where $A$ and $B$ generate noun
phrases, normally leads to overgeneration, e.\,g., $i\_eats\_apples$. In order
to avoid overgeneration, a CFG would need many non-terminal symbols and rules,
that mainly produce the same word, just to handle exceptions. In our MGs
however, we can summarize CFG rules that produce the same word in one item and
handle exceptions by a proper distribution of licensees/-ors. The difficulty
with this technique is that in most generations the majority of licensees/-ors
is not needed, but still has to be triggered somehow. We solve this problem
with $\epsilon$-items called \emph{adapters}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01825">Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. (arXiv:2311.01825v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanger_M/0/1/0/all/0/1">Mario S&#xe4;nger</a>, <a href="http://arxiv.org/find/cs/1/au:+Mecquenem_N/0/1/0/all/0/1">Ninon De Mecquenem</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewinska_K/0/1/0/all/0/1">Katarzyna Ewa Lewi&#x144;ska</a>, <a href="http://arxiv.org/find/cs/1/au:+Bountris_V/0/1/0/all/0/1">Vasilis Bountris</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmann_F/0/1/0/all/0/1">Fabian Lehmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Leser_U/0/1/0/all/0/1">Ulf Leser</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosch_T/0/1/0/all/0/1">Thomas Kosch</a></p>
<p>Scientific workflow systems are increasingly popular for expressing and
executing complex data analysis pipelines over large datasets, as they offer
reproducibility, dependability, and scalability of analyses by automatic
parallelization on large compute clusters. However, implementing workflows is
difficult due to the involvement of many black-box tools and the deep
infrastructure stack necessary for their execution. Simultaneously,
user-supporting tools are rare, and the number of available examples is much
lower than in classical programming languages. To address these challenges, we
investigate the efficiency of Large Language Models (LLMs), specifically
ChatGPT, to support users when dealing with scientific workflows. We performed
three user studies in two scientific domains to evaluate ChatGPT for
comprehending, adapting, and extending workflows. Our results indicate that
LLMs efficiently interpret workflows but achieve lower performance for
exchanging components or purposeful workflow extensions. We characterize their
limitations in these challenging scenarios and suggest future research
directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01860">FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacob_S/0/1/0/all/0/1">Shahar Jacob</a>, <a href="http://arxiv.org/find/cs/1/au:+Shani_C/0/1/0/all/0/1">Chen Shani</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1">Dafna Shahaf</a></p>
<p>Analogy is one of the core capacities of human cognition; when faced with new
situations, we often transfer prior experience from other domains. Most work on
computational analogy relies heavily on complex, manually crafted input. In
this work, we relax the input requirements, requiring only names of entities to
be mapped. We automatically extract commonsense representations and use them to
identify a mapping between the entities. Unlike previous works, our framework
can handle partial analogies and suggest new entities to be added. Moreover,
our method's output is easily interpretable, allowing for users to understand
why a specific mapping was chosen.
</p>
<p>Experiments show that our model correctly maps 81.2% of classical 2x2 analogy
problems (guess level=50%). On larger problems, it achieves 77.8% accuracy
(mean guess level=13.1%). In another experiment, we show our algorithm
outperforms human performance, and the automatic suggestions of new entities
resemble those suggested by humans. We hope this work will advance
computational analogy by paving the way to more flexible, realistic input
requirements, with broader applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01862">$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation. (arXiv:2311.01862v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">He Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Siyu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Liuzhi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xinlin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1">Chuanjun Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1">Guangnan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_H/0/1/0/all/0/1">Hongfeng Chai</a></p>
<p>While current NL2SQL tasks constructed using Foundation Models have achieved
commendable results, their direct application to Natural Language to Graph
Query Language (NL2GQL) tasks poses challenges due to the significant
differences between GQL and SQL expressions, as well as the numerous types of
GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation
Models demonstrate superior cross-schema generalization abilities, while
smaller Foundation Models struggle to improve their GQL generation capabilities
through fine-tuning. However, after fine-tuning, smaller models exhibit better
intent comprehension and higher grammatical accuracy. Diverging from rule-based
and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller
and larger Foundation Models as reranker, rewriter and refiner. The approach
harnesses the comprehension ability of smaller models for information reranker
and rewriter, and the exceptional generalization and generation capabilities of
larger models to transform input natural language queries and code structure
schema into any form of GQLs. Recognizing the lack of established datasets in
this nascent domain, we have created a bilingual dataset derived from graph
database documentation and some open-source Knowledge Graphs (KGs). We tested
our approach on this dataset and the experimental results showed that delivers
promising performance and robustness.Our code and dataset is available at
https://github.com/zhiqix/NL2GQL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01864">SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rigutini_L/0/1/0/all/0/1">Leonardo Rigutini</a>, <a href="http://arxiv.org/find/cs/1/au:+Papini_T/0/1/0/all/0/1">Tiziano Papini</a>, <a href="http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1">Marco Maggini</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1">Franco Scarselli</a></p>
<p>The problem of relevance ranking consists of sorting a set of objects with
respect to a given criterion. Since users may prefer different relevance
criteria, the ranking algorithms should be adaptable to the user needs. Two
main approaches exist in literature for the task of learning to rank: 1) a
score function, learned by examples, which evaluates the properties of each
object yielding an absolute relevance value that can be used to order the
objects or 2) a pairwise approach, where a "preference function" is learned
using pairs of objects to define which one has to be ranked first. In this
paper, we present SortNet, an adaptive ranking algorithm which orders objects
using a neural network as a comparator. The neural network training set
provides examples of the desired ordering between pairs of items and it is
constructed by an iterative procedure which, at each iteration, adds the most
informative training examples. Moreover, the comparator adopts a connectionist
architecture that is particularly suited for implementing a preference
function. We also prove that such an architecture has the universal
approximation property and can implement a wide class of functions. Finally,
the proposed algorithm is evaluated on the LETOR dataset showing promising
performances in comparison with other state of the art algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01866">Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shani_C/0/1/0/all/0/1">Chen Shani</a>, <a href="http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1">Jilles Vreeken</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1">Dafna Shahaf</a></p>
<p>Concepts play a pivotal role in various human cognitive functions, including
learning, reasoning and communication. However, there is very little work on
endowing machines with the ability to form and reason with concepts. In
particular, state-of-the-art large language models (LLMs) work at the level of
tokens, not concepts.
</p>
<p>In this work, we analyze how well contemporary LLMs capture human concepts
and their structure. We then discuss ways to develop concept-aware LLMs, taking
place at different stages of the pipeline. We sketch a method for pretraining
LLMs using concepts, and also explore the simpler approach that uses the output
of existing LLMs. Despite its simplicity, our proof-of-concept is shown to
better match human intuition, as well as improve the robustness of predictions.
These preliminary results underscore the promise of concept-aware LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01870">Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinrui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1">Timothy Baldwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1">Trevor Cohn</a></p>
<p>We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K
multi-lingual documents collected from the European Parliament, spanning 24
languages. This dataset is designed to investigate fairness in a multilingual
information retrieval (IR) context to analyze both language and demographic
bias in a ranking context. It boasts an authentic multilingual corpus,
featuring topics translated into all 24 languages, as well as cross-lingual
relevance judgments. Furthermore, it offers rich demographic information
associated with its documents, facilitating the study of demographic bias. We
report the effectiveness of Multi-EuP for benchmarking both monolingual and
multilingual IR. We also conduct a preliminary experiment on language bias
caused by the choice of tokenization strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01873">Efficient Black-Box Adversarial Attacks on Neural Text Detectors. (arXiv:2311.01873v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fishchuk_V/0/1/0/all/0/1">Vitalii Fishchuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1">Daniel Braun</a></p>
<p>Neural text detectors are models trained to detect whether a given text was
generated by a language model or written by a human. In this paper, we
investigate three simple and resource-efficient strategies (parameter tweaking,
prompt engineering, and character-level mutations) to alter texts generated by
GPT-3.5 that are unsuspicious or unnoticeable for humans but cause
misclassification by neural text detectors. The results show that especially
parameter tweaking and character-level mutations are effective strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01876">Sentiment Analysis through LLM Negotiations. (arXiv:2311.01876v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaofei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoya Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a></p>
<p>A standard paradigm for sentiment analysis is to rely on a singular LLM and
makes the decision in a single round under the framework of in-context
learning. This framework suffers the key disadvantage that the single-turn
output generated by a single LLM might not deliver the perfect decision, just
as humans sometimes need multiple attempts to get things right. This is
especially true for the task of sentiment analysis where deep reasoning is
required to address the complex linguistic phenomenon (e.g., clause
composition, irony, etc) in the input.
</p>
<p>To address this issue, this paper introduces a multi-LLM negotiation
framework for sentiment analysis. The framework consists of a reasoning-infused
generator to provide decision along with rationale, a explanation-deriving
discriminator to evaluate the credibility of the generator. The generator and
the discriminator iterate until a consensus is reached. The proposed framework
naturally addressed the aforementioned challenge, as we are able to take the
complementary abilities of two LLMs, have them use rationale to persuade each
other for correction.
</p>
<p>Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie
Review, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed
approach: it consistently yields better performances than the ICL baseline
across all benchmarks, and even superior performances to supervised baselines
on the Twitter and movie review datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01882">Indicative Summarization of Long Discussions. (arXiv:2311.01882v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1">Shahbaz Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwabe_D/0/1/0/all/0/1">Dominik Schwabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1">Khalid Al-Khatib</a>, <a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1">Martin Potthast</a></p>
<p>Online forums encourage the exchange and discussion of different stances on
many topics. Not only do they provide an opportunity to present one's own
arguments, but may also gather a broad cross-section of others' arguments.
However, the resulting long discussions are difficult to overview. This paper
presents a novel unsupervised approach using large language models (LLMs) to
generating indicative summaries for long discussions that basically serve as
tables of contents. Our approach first clusters argument sentences, generates
cluster labels as abstractive summaries, and classifies the generated cluster
labels into argumentation frames resulting in a two-level summary. Based on an
extensively optimized prompt engineering approach, we evaluate 19~LLMs for
generative cluster labeling and frame classification. To evaluate the
usefulness of our indicative summaries, we conduct a purpose-driven user study
via a new visual interface called Discussion Explorer: It shows that our
proposed indicative summaries serve as a convenient navigation tool to explore
long discussions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01907">BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Knappich_V/0/1/0/all/0/1">Valentin Knappich</a>, <a href="http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1">Simon Razniewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1">Annemarie Friedrich</a></p>
<p>Automatic simplification can help laypeople to comprehend complex scientific
text. Language models are frequently applied to this task by translating from
complex to simple language. In this paper, we describe our system based on
Llama 2, which ranked first in the PLABA shared task addressing the
simplification of biomedical text. We find that the large portion of shared
tokens between input and output leads to weak training signals and
conservatively editing models. To mitigate these issues, we propose
sentence-level and token-level loss weights. They give higher weight to
modified tokens, indicated by edit distance and edit operations, respectively.
We conduct an empirical evaluation on the PLABA dataset and find that both
approaches lead to simplifications closer to those created by human annotators
(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /
1.8x edit distance) compared to the same model fine-tuned with standard cross
entropy. We furthermore show that the hyperparameter $\lambda$ in token-level
loss weights can be used to control the edit distance and the simplicity level
(FKGL).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01918">Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mingze Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_P/0/1/0/all/0/1">Peng Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiajia Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yunhao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zifan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lin Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bin Dong</a></p>
<p>With the rapid development of artificial intelligence, large language models
(LLMs) have shown promising capabilities in mimicking human-level language
comprehension and reasoning. This has sparked significant interest in applying
LLMs to enhance various aspects of healthcare, ranging from medical education
to clinical decision support. However, medicine involves multifaceted data
modalities and nuanced reasoning skills, presenting challenges for integrating
LLMs. This paper provides a comprehensive review on the applications and
implications of LLMs in medicine. It begins by examining the fundamental
applications of general-purpose and specialized LLMs, demonstrating their
utilities in knowledge retrieval, research support, clinical workflow
automation, and diagnostic assistance. Recognizing the inherent multimodality
of medicine, the review then focuses on multimodal LLMs, investigating their
ability to process diverse data types like medical imaging and EHRs to augment
diagnostic accuracy. To address LLMs' limitations regarding personalization and
complex clinical reasoning, the paper explores the emerging development of
LLM-powered autonomous agents for healthcare. Furthermore, it summarizes the
evaluation methodologies for assessing LLMs' reliability and safety in medical
contexts. Overall, this review offers an extensive analysis on the
transformative potential of LLMs in modern medicine. It also highlights the
pivotal need for continuous optimizations and ethical oversight before these
models can be effectively integrated into clinical practice. Visit
https://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying
GitHub repository containing latest papers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01927">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1">Tobias Katsch</a></p>
<p>Linear Recurrence has proven to be a powerful tool for modeling long
sequences efficiently. In this work, we show that existing models fail to take
full advantage of its potential. Motivated by this finding, we develop
GateLoop, a foundational sequence model that generalizes linear recurrent
models such as S4, S5, LRU and RetNet, by employing data-controlled state
transitions. Utilizing this theoretical advance, GateLoop empirically
outperforms existing models for auto-regressive language modeling. Our method
comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$
parallel mode making use of highly optimized associative scan implementations.
Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing
remarkable implications for Transformer and recently proposed architectures.
Specifically, we prove that our approach can be interpreted as providing
data-controlled relative-positional information to Attention. While many
existing models solely rely on data-controlled cumulative sums for context
aggregation, our findings suggest that incorporating data-controlled complex
cumulative products may be a crucial step towards more powerful sequence
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01928">Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games. (arXiv:2311.01928v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Keunwoo Peter Yu</a></p>
<p>In natural language processing, interactive text-based games serve as a test
bed for interactive AI systems. Prior work has proposed to play text-based
games by acting based on discrete knowledge graphs constructed by the Discrete
Graph Updater (DGU) to represent the game state from the natural language
description. While DGU has shown promising results with high interpretability,
it suffers from lower knowledge graph accuracy due to its lack of temporality
and limited generalizability to complex environments with objects with the same
label. In order to address DGU's weaknesses while preserving its high
interpretability, we propose the Temporal Discrete Graph Updater (TDGU), a
novel neural network model that represents dynamic knowledge graphs as a
sequence of timestamped graph events and models them using a temporal point
based graph neural network. Through experiments on the dataset collected from a
text-based game TextWorld, we show that TDGU outperforms the baseline DGU. We
further show the importance of temporal information for TDGU's performance
through an ablation study and demonstrate that TDGU has the ability to
generalize to more complex environments with objects with the same label. All
the relevant code can be found at
\url{https://github.com/yukw777/temporal-discrete-graph-updater}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01949">Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qingyan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1">Xinzhe Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chufan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lemao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haiyun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>In-context learning (ICL) ability has emerged with the increasing scale of
large language models (LLMs), enabling them to learn input-label mappings from
demonstrations and perform well on downstream tasks. However, under the
standard ICL setting, LLMs may sometimes neglect query-related information in
demonstrations, leading to incorrect predictions. To address this limitation,
we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to
explore the power of ICL in open-domain question answering, an important form
in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract
query-related knowledge from demonstrations, then concatenates the knowledge to
prompt LLMs in a more explicit way. Furthermore, we track the source of this
knowledge to identify specific examples, and introduce a Hint-related Example
Retriever (HER) to select informative examples for enhanced demonstrations. We
evaluate HICL with HER on 3 open-domain QA benchmarks, and observe average
performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM
score and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01955">Too Much Information: Keeping Training Simple for BabyLMs. (arXiv:2311.01955v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1">Lukas Edman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1">Lisa Bylinina</a></p>
<p>This paper details the work of the University of Groningen for the BabyLM
Challenge. We follow the idea that, like babies, language models should be
introduced to simpler concepts first and build off of that knowledge to
understand more complex concepts. We examine this strategy of
simple-then-complex through a variety of lenses, namely context size,
vocabulary, and overall linguistic complexity of the data. We find that only
one, context size, is truly beneficial to training a language model. However
this simple change to context size gives us improvements of 2 points on average
on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\% on average on BLiMP
tasks. Our context-limited model outperforms the baseline that was trained on
10$\times$ the amount of data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01964">Don&#x27;t Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yutao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhipeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wentong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a></p>
<p>Large language models~(LLMs) have greatly advanced the frontiers of
artificial intelligence, attaining remarkable improvement in model capacity. To
assess the model performance, a typical approach is to construct evaluation
benchmarks for measuring the ability level of LLMs in different aspects.
Despite that a number of high-quality benchmarks have been released, the
concerns about the appropriate use of these benchmarks and the fair comparison
of different models are increasingly growing. Considering these concerns, in
this paper, we discuss the potential risk and impact of inappropriately using
evaluation benchmarks and misleadingly interpreting the evaluation results.
Specially, we focus on a special issue that would lead to inappropriate
evaluation, \ie \emph{benchmark leakage}, referring that the data related to
evaluation sets is occasionally used for model training. This phenomenon now
becomes more common since pre-training data is often prepared ahead of model
test. We conduct extensive experiments to study the effect of benchmark
leverage, and find that it can dramatically boost the evaluation results, which
would finally lead to an unreliable assessment of model performance. To improve
the use of existing evaluation benchmarks, we finally present several
guidelines for both LLM developers and benchmark maintainers. We hope this work
can draw attention to appropriate training and evaluation of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01967">The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1">Alina Leidinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Rooij_R/0/1/0/all/0/1">Robert van Rooij</a>, <a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1">Ekaterina Shutova</a></p>
<p>The latest generation of LLMs can be prompted to achieve impressive zero-shot
or few-shot performance in many NLP tasks. However, since performance is highly
sensitive to the choice of prompts, considerable effort has been devoted to
crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we
still lack a systematic understanding of how linguistic properties of prompts
correlate with task performance. In this work, we investigate how LLMs of
different sizes, pre-trained and instruction-tuned, perform on prompts that are
semantically equivalent, but vary in linguistic structure. We investigate both
grammatical properties such as mood, tense, aspect and modality, as well as
lexico-semantic variation through the use of synonyms. Our findings contradict
the common assumption that LLMs achieve optimal performance on lower perplexity
prompts that reflect language use in pretraining or instruction-tuning data.
Prompts transfer poorly between datasets or models, and performance cannot
generally be explained by perplexity, word frequency, ambiguity or prompt
length. Based on our results, we put forward a proposal for a more robust and
comprehensive evaluation standard for prompting research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01981">ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models. (arXiv:2311.01981v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haotian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kunming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1">Cheng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Sixian Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinhao Chen</a></p>
<p>RNN-like language models are getting renewed attention from NLP researchers
in recent years and several models have made significant progress, which
demonstrates performance comparable to traditional transformers. However, due
to the recurrent nature of RNNs, this kind of language model can only store
information in a set of fixed-length state vectors. As a consequence, they
still suffer from forgetfulness though after a lot of improvements and
optimizations, when given complex instructions or prompts. As the prompted
generation is the main and most concerned function of LMs, solving the problem
of forgetting in the process of generation is no wonder of vital importance. In
this paper, focusing on easing the prompt forgetting during generation, we
proposed an architecture to teach the model memorizing prompt during generation
by synthetic gradient. To force the model to memorize the prompt, we derive the
states that encode the prompt, then transform it into model parameter
modification using low-rank gradient approximation, which hard-codes the prompt
into model parameters temporarily. We construct a dataset for experiments, and
the results have demonstrated the effectiveness of our method in solving the
problem of forgetfulness in the process of prompted generation. We will release
all the code upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02025">Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection. (arXiv:2311.02025v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarracen_G/0/1/0/all/0/1">Gretel Liz De la Pe&#xf1;a Sarrac&#xe9;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1">Paolo Rosso</a>, <a href="http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1">Robert Litschko</a>, <a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1">Goran Glava&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1">Simone Paolo Ponzetto</a></p>
<p>Cross-lingual transfer learning from high-resource to medium and low-resource
languages has shown encouraging results. However, the scarcity of resources in
target languages remains a challenge. In this work, we resort to data
augmentation and continual pre-training for domain adaptation to improve
cross-lingual abusive language detection. For data augmentation, we analyze two
existing techniques based on vicinal risk minimization and propose MIXAG, a
novel data augmentation method which interpolates pairs of instances based on
the angle of their representations. Our experiments involve seven languages
typologically distinct from English and three different domains. The results
reveal that the data augmentation strategies can enhance few-shot cross-lingual
abusive language detection. Specifically, we observe that consistently in all
target languages, MIXAG improves significantly in multidomain and multilingual
environments. Finally, we show through an error analysis how the domain
adaptation can favour the class of abusive texts (reducing false negatives),
but at the same time, declines the precision of the abusive language detection
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02049">Post Turing: Mapping the landscape of LLM Evaluation. (arXiv:2311.02049v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1">Alexey Tikhonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1">Ivan P. Yamshchikov</a></p>
<p>In the rapidly evolving landscape of Large Language Models (LLMs),
introduction of well-defined and standardized evaluation methodologies remains
a crucial challenge. This paper traces the historical trajectory of LLM
evaluations, from the foundational questions posed by Alan Turing to the modern
era of AI research. We categorize the evolution of LLMs into distinct periods,
each characterized by its unique benchmarks and evaluation criteria. As LLMs
increasingly mimic human-like behaviors, traditional evaluation proxies, such
as the Turing test, have become less reliable. We emphasize the pressing need
for a unified evaluation system, given the broader societal implications of
these models. Through an analysis of common evaluation methodologies, we
advocate for a qualitative shift in assessment approaches, underscoring the
importance of standardization and objective criteria. This work serves as a
call for the AI community to collaboratively address the challenges of LLM
evaluation, ensuring their reliability, fairness, and societal benefit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02069">Grounded Intuition of GPT-Vision&#x27;s Abilities with Scientific Images. (arXiv:2311.02069v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1">Alyssa Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Head_A/0/1/0/all/0/1">Andrew Head</a>, <a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1">Chris Callison-Burch</a></p>
<p>GPT-Vision has impressed us on a range of vision-language tasks, but it comes
with the familiar new challenge: we have little idea of its capabilities and
limitations. In our study, we formalize a process that many have instinctively
been trying already to develop "grounded intuition" of this new model. Inspired
by the recent movement away from benchmarking in favor of example-driven
qualitative evaluation, we draw upon grounded theory and thematic analysis in
social science and human-computer interaction to establish a rigorous framework
for qualitative evaluation in natural language processing. We use our technique
to examine alt text generation for scientific figures, finding that GPT-Vision
is particularly sensitive to prompting, counterfactual text in images, and
relative spatial relationships. Our method and analysis aim to help researchers
ramp up their own grounded intuitions of new models while exposing how
GPT-Vision can be applied to make information more accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1612.04765">CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization. (arXiv:1612.04765v12 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reichel_U/0/1/0/all/0/1">Uwe D. Reichel</a></p>
<p>The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and
(2) prosodic feature extraction from syllable to utterance level. CoPaSul
stands for contour-based, parametric, superpositional intonation stylization.
In this framework intonation is represented as a superposition of global and
local contours that are described parametrically in terms of polynomial
coefficients. On the global level (usually associated but not necessarily
restricted to intonation phrases) the stylization serves to represent register
in terms of time-varying F0 level and range. On the local level (e.g. accent
groups), local contour shapes are described. From this parameterization several
features related to prosodic boundaries and prominence can be derived.
Furthermore, by coefficient clustering prosodic contour classes can be obtained
in a bottom-up way. Next to the stylization-based feature extraction also
standard F0 and energy measures (e.g. mean and variance) as well as rhythmic
aspects can be calculated. At the current state automatic annotation comprises:
segmentation into interpausal chunks, syllable nucleus extraction, and
unsupervised localization of prosodic phrase boundaries and prominent
syllables. F0 and partly also energy feature sets can be derived for: standard
measurements (as median and IQR), register in terms of F0 level and range,
prosodic boundaries, local contour shapes, bottom-up derived contour classes,
Gestalt of accent groups in terms of their deviation from higher level prosodic
units, as well as for rhythmic aspects quantifying the relation between F0 and
energy contours and prosodic event rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.00763">Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weiwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shuyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1">Pengjie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhumin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1">Maarten de Rijke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhaochun Ren</a></p>
<p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or is very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capabilities. Moreover, the evaluation of user simulators is an open challenge.
</p>
<p>In this work, we propose a metaphorical user simulator for end-to-end TDS
evaluation, where we define a simulator to be metaphorical if it simulates
user's analogical thinking in interactions with systems. We also propose a
tester-based evaluation framework to generate variants, i.e., dialogue systems
with different capabilities. Our user simulator constructs a metaphorical user
model that assists the simulator in reasoning by referring to prior knowledge
when encountering new items. We estimate the quality of simulators by checking
the simulated interactions between simulators and variants. Our experiments are
conducted using three TDS datasets. The proposed user simulator demonstrates
better consistency with manual evaluation than an agenda-based simulator and a
seq2seq model on three datasets; our tester framework demonstrates efficiency
and has been tested on multiple tasks, such as conversational recommendation
and e-commerce dialogues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07596">Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yuxuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_E/0/1/0/all/0/1">Edwin Simpson</a></p>
<p>This paper introduces a novel pipeline for summarising timelines of events
reported by multiple news sources. Transformer-based models for abstractive
summarisation generate coherent and concise summaries of long documents but can
fail to outperform established extractive methods on specialised tasks such as
timeline summarisation (TLS). While extractive summaries are more faithful to
their sources, they may be less readable and contain redundant or unnecessary
information. This paper proposes a preference-based reinforcement learning
(PBRL) method for adapting pretrained abstractive summarisers to TLS, which can
overcome the drawbacks of extractive timeline summaries. We define a compound
reward function that learns from keywords of interest and pairwise preference
labels, which we use to fine-tune a pretrained abstractive summariser via
offline reinforcement learning. We carry out both automated and human
evaluation on three datasets, finding that our method outperforms a comparable
extractive TLS method on two of the three benchmark datasets, and participants
prefer our method's summaries to those of both the extractive TLS method and
the pretrained abstractive model. The method does not require expensive
reference summaries and needs only a small number of preferences to align the
generated summaries with human preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09146">Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+BehnamGhader_P/0/1/0/all/0/1">Parishad BehnamGhader</a>, <a href="http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1">Santiago Miret</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a></p>
<p>Augmenting pretrained language models with retrievers has shown promise in
effectively solving common NLP problems, such as language modeling and question
answering. In this paper, we evaluate the strengths and weaknesses of popular
retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,
Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved
statements across different tasks. Our findings indicate that the simple
similarity metric employed by retrievers is insufficient for retrieving all the
necessary statements for reasoning. Additionally, the language models do not
exhibit strong reasoning even when provided with only the required statements.
Furthermore, when combined with imperfect retrievers, the performance of the
language models becomes even worse, e.g., Flan-T5's performance drops by 28.6%
when retrieving 5 statements using Contriever. While larger language models
improve performance, there is still a substantial room for enhancement. Our
further analysis indicates that multihop retrieve-and-read is promising for
large language models like GPT-3.5, but does not generalize to other language
models like Flan-T5-xxl.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03843">Why think step by step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prystawski_B/0/1/0/all/0/1">Ben Prystawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Michael Y. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a></p>
<p>Humans have a powerful and mysterious capacity to reason. Working through a
set of mental steps enables us to make inferences we would not be capable of
making directly even though we get no additional data from the world.
Similarly, when large language models generate intermediate steps (a chain of
thought) before answering a question, they often produce better answers than
they would directly. We investigate why and how chain-of-thought reasoning is
useful in language models, testing the hypothesis that reasoning is effective
when training data consists of overlapping local clusters of variables that
influence each other strongly. These training conditions enable the chaining of
accurate local inferences to estimate relationships between variables that were
not seen together in training. We prove that there will exist a "reasoning
gap", where reasoning through intermediate variables reduces bias, for the
simple case of an autoregressive density estimator trained on local samples
from a chain-structured probabilistic model. We then test our hypothesis
experimentally in more complex models, training an autoregressive language
model on samples from Bayes nets but only including a subset of variables in
each sample. We test language models' ability to match conditional
probabilities with and without intermediate reasoning steps, finding that
intermediate steps are only helpful when the training data is locally
structured with respect to dependencies between variables. The combination of
locally structured observations and reasoning is much more data-efficient than
training on all variables. Our results illustrate how the effectiveness of
reasoning step by step is rooted in the local statistical structure of the
training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04370">OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v6 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1">Kai Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jianchao Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zelong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>Human Intelligence (HI) excels at combining basic skills to solve complex
tasks. This capability is vital for Artificial Intelligence (AI) and should be
embedded in comprehensive AI Agents, enabling them to harness expert models for
complex task-solving towards Artificial General Intelligence (AGI). Large
Language Models (LLMs) show promising learning and reasoning abilities, and can
effectively use external models, tools, plugins, or APIs to tackle complex
problems. In this work, we introduce OpenAGI, an open-source AGI research and
development platform designed for solving multi-step, real-world tasks.
Specifically, OpenAGI uses a dual strategy, integrating standard benchmark
tasks for benchmarking and evaluation, and open-ended tasks including more
expandable models, tools, plugins, or APIs for creative problem-solving. Tasks
are presented as natural language queries to the LLM, which then selects and
executes appropriate models. We also propose a Reinforcement Learning from Task
Feedback (RLTF) mechanism that uses task results to improve the LLM's
task-solving ability, which creates a self-improving AI feedback loop. While we
acknowledge that AGI is a broad and multifaceted research challenge with no
singularly defined solution path, the integration of LLMs with domain-specific
expert models, inspired by mirroring the blend of general and specialized
intelligence in humans, offers a promising approach towards AGI. We are
open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation
methods, and the UI demo to foster community involvement in AGI advancement:
https://github.com/agiresearch/OpenAGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11015">DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pourreza_M/0/1/0/all/0/1">Mohammadreza Pourreza</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1">Davood Rafiei</a></p>
<p>There is currently a significant gap between the performance of fine-tuned
models and prompting approaches using Large Language Models (LLMs) on the
challenging task of text-to-SQL, as evaluated on datasets such as Spider. To
improve the performance of LLMs in the reasoning process, we study how
decomposing the task into smaller sub-tasks can be effective. In particular, we
show that breaking down the generation problem into sub-problems and feeding
the solutions of those sub-problems into LLMs can be an effective approach for
significantly improving their performance. Our experiments with three LLMs show
that this approach consistently improves their simple few-shot performance by
roughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the
holdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9
and the new SOTA at the time of this writing using our approach is 85.3. Our
approach with in-context learning beats many heavily fine-tuned models by at
least 5%. Additionally, when evaluated on the BIRD benchmark, our approach
achieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test
set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07797">ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1">Sarik Ghazarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1">Yijia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1">Rujun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1">Aram Galstyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Commonsense reasoning is omnipresent in human communications and thus is an
important feature for open-domain dialogue systems. However, evaluating
commonsense in dialogue systems is still an open challenge. We take the first
step by focusing on event commonsense that considers events and their
relations, and is crucial in both dialogues and general commonsense reasoning.
We propose ACCENT, an event commonsense evaluation metric empowered by
commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation
tuples from a dialogue, and then evaluates the response by scoring the tuples
in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct
the first public event commonsense evaluation dataset for open-domain
dialogues. Our experiments show that ACCENT is an efficient metric for event
commonsense evaluation, which achieves higher correlations with human judgments
than existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13112">Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xinyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>The recent success of large language models (LLMs) has shown great potential
to develop more powerful conversational recommender systems (CRSs), which rely
on natural language conversations to satisfy user needs. In this paper, we
embark on an investigation into the utilization of ChatGPT for conversational
recommendation, revealing the inadequacy of the existing evaluation protocol.
It might over-emphasize the matching with the ground-truth items or utterances
generated by human annotators, while neglecting the interactive nature of being
a capable CRS. To overcome the limitation, we further propose an interactive
Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user
simulators. Our evaluation approach can simulate various interaction scenarios
between users and systems. Through the experiments on two publicly available
CRS datasets, we demonstrate notable improvements compared to the prevailing
evaluation protocol. Furthermore, we emphasize the evaluation of
explainability, and ChatGPT showcases persuasive explanation generation for its
recommendations. Our study contributes to a deeper comprehension of the
untapped potential of LLMs for CRSs and provides a more flexible and
easy-to-use evaluation framework for future research endeavors. The codes and
data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13484">Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v3 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jinghan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Alnaasan_N/0/1/0/all/0/1">Nawras Alnaasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1">Aamir Shafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1">Hari Subramoni</a>, <a href="http://arxiv.org/find/cs/1/au:+K%2E_D/0/1/0/all/0/1">Dhabaleswar K.</a> (DK) <a href="http://arxiv.org/find/cs/1/au:+Panda/0/1/0/all/0/1">Panda</a></p>
<p>Autoregressive models, despite their commendable performance in a myriad of
generative tasks, face challenges stemming from their inherently sequential
structure. Inference on these models, by design, harnesses a temporal
dependency, where the current token's probability distribution is conditioned
on preceding tokens. This inherent characteristic severely impedes
computational efficiency during inference as a typical inference request can
require more than thousands of tokens, where generating each token requires a
load of entire model weights, making the inference more memory-bound. The large
overhead becomes profound in real deployment where requests arrive randomly,
necessitating various generation lengths. Existing solutions, such as dynamic
batching and concurrent instances, introduce significant response delays and
bandwidth contention, falling short of achieving optimal latency and
throughput. To address these shortcomings, we propose Flover -- a temporal
fusion framework for efficiently inferring multiple requests in parallel. We
deconstruct the general generation pipeline into pre-processing and token
generation, and equip the framework with a dedicated work scheduler for fusing
the generation process temporally across all requests. By orchestrating the
token-level parallelism, Flover exhibits optimal hardware efficiency and
significantly spares the system resources. By further employing a fast buffer
reordering algorithm that allows memory eviction of finished tasks, it brings
over 11x inference speedup on GPT and 16x on LLAMA compared to the cutting-edge
solutions provided by NVIDIA FasterTransformer. Crucially, by leveraging the
advanced tensor parallel technique, Flover proves efficacious across diverse
computational landscapes, from single-GPU setups to distributed scenarios,
thereby offering robust performance optimization that adapts to variable use
cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13669">The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models. (arXiv:2305.13669v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Liangming Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junzhou Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>Large language models often necessitate grounding on external knowledge to
generate faithful and reliable answers. Yet even with the correct groundings in
the reference, they can ignore them and rely on wrong groundings or their
inherent biases to hallucinate when users, being largely unaware of the
specifics of the stored information, pose questions that might not directly
correlate with the retrieved groundings. In this work, we formulate this
knowledge alignment problem and introduce MixAlign, a framework that interacts
with both the human user and the knowledge base to obtain and integrate
clarifications on how the user question relates to the stored information.
MixAlign employs a language model to achieve automatic knowledge alignment and,
if necessary, further enhances this alignment through human user
clarifications. Experimental results highlight the crucial role of knowledge
alignment in boosting model performance and mitigating hallucination, with
improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the
effectiveness of MixAlign in improving knowledge alignment by producing
high-quality, user-centered clarifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14711">Gender Biases in Automatic Evaluation Metrics for Image Captioning. (arXiv:2305.14711v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Haoyi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zi-Yi Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianlu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1">Asli Celikyilmaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have
demonstrated decent correlations with human judgments in various language
generation tasks. However, their impact on fairness remains largely unexplored.
It is widely recognized that pretrained models can inadvertently encode
societal biases, thus employing these models for evaluation purposes may
inadvertently perpetuate and amplify biases. For example, an evaluation metric
may favor the caption "a woman is calculating an account book" over "a man is
calculating an account book," even if the image only shows male accountants. In
this paper, we conduct a systematic study of gender biases in model-based
automatic evaluation metrics for image captioning tasks. We start by curating a
dataset comprising profession, activity, and object concepts associated with
stereotypical gender associations. Then, we demonstrate the negative
consequences of using these biased metrics, including the inability to
differentiate between biased and unbiased generations, as well as the
propagation of biases to generation models through reinforcement learning.
Finally, we present a simple and effective way to mitigate the metric bias
without hurting the correlations with human judgments. Our dataset and
framework lay the foundation for understanding the potential harm of
model-based evaluation metrics, and facilitate future works to develop more
inclusive evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15004">LLMDet: A Third Party Large Language Models Generated Text Detection Tool. (arXiv:2305.15004v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kangxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>Generated texts from large language models (LLMs) are remarkably close to
high-quality human-authored text, raising concerns about their potential misuse
in spreading false information and academic misconduct. Consequently, there is
an urgent need for a highly practical detection tool capable of accurately
identifying the source of a given text. However, existing detection tools
typically rely on access to LLMs and can only differentiate between
machine-generated and human-authored text, failing to meet the requirements of
fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we
propose LLMDet, a model-specific, secure, efficient, and extendable detection
tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and
others. In LLMDet, we record the next-token probabilities of salient n-grams as
features to calculate proxy perplexity for each LLM. By jointly analyzing the
proxy perplexities of LLMs, we can determine the source of the generated text.
Experimental results show that LLMDet yields impressive detection performance
while ensuring speed and security, achieving 98.54% precision and x5.0 faster
for recognizing human-authored text. Additionally, LLMDet can effortlessly
extend its detection capabilities to a new open-source model. We will provide
an open-source tool at https://github.com/TrustedLLM/LLMDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16397">Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1">Benno Krojer</a>, <a href="http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1">Elinor Poole-Dayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1">Vikram Voleti</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1">Christopher Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a></p>
<p>Text-conditioned image generation models have recently shown immense
qualitative success using denoising diffusion processes. However, unlike
discriminative vision-and-language models, it is a non-trivial task to subject
these diffusion-based generative models to automatic fine-grained quantitative
evaluation of high-level phenomena such as compositionality. Towards this goal,
we perform two innovations. First, we transform diffusion-based models (in our
case, Stable Diffusion) for any image-text matching (ITM) task using a novel
method called DiffusionITM. Second, we introduce the Generative-Discriminative
Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language
tasks, bias evaluation and detailed analysis. We find that Stable Diffusion +
DiffusionITM is competitive on many tasks and outperforms CLIP on compositional
tasks like like CLEVR and Winoground. We further boost its compositional
performance with a transfer setup by fine-tuning on MS-COCO while retaining
generative capabilities. We also measure the stereotypical bias in diffusion
models, and find that Stable Diffusion 2.1 is, for the most part, less biased
than Stable Diffusion 1.5. Overall, our results point in an exciting direction
bringing discriminative and generative model evaluation closer. We will release
code and benchmark setup soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19234">Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuezhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1">Rif A. Saurous</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a></p>
<p>Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02231">Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1">Hiteshi Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Frujeri_F/0/1/0/all/0/1">Felipe Vieira Frujeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Shi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chenguang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael I. Jordan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jiantao Jiao</a></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as a reliable
approach to aligning large language models (LLMs) to human preferences. Among
the plethora of RLHF techniques, proximal policy optimization (PPO) is of the
most widely used methods. Despite its popularity, however, PPO may suffer from
mode collapse, instability, and poor sample efficiency. We show that these
issues can be alleviated by a novel algorithm that we refer to as
Advantage-Induced Policy Alignment (APA), which leverages a squared error loss
function based on the estimated advantages. We demonstrate empirically that APA
consistently outperforms PPO in language tasks by a large margin, when a
separate reward model is employed as the evaluator. In addition, compared with
PPO, APA offers a more stable form of control over the deviation from the
model's initial policy, ensuring that the model improves its performance
without collapsing to deterministic output. In addition to empirical results,
we also provide a theoretical justification supporting the design of our loss
function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10763">Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_L/0/1/0/all/0/1">Lakshya A Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1">Aditya Kanade</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1">Navin Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1">Shuvendu K. Lahiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajamani_S/0/1/0/all/0/1">Sriram K. Rajamani</a></p>
<p>Language models of code (LMs) work well when the surrounding code provides
sufficient context. This is not true when it becomes necessary to use types,
functionality or APIs defined elsewhere in the repository or a linked library,
especially those not seen during training. LMs suffer from limited awareness of
such global context and end up hallucinating.
</p>
<p>Integrated development environments (IDEs) assist developers in understanding
repository context using static analysis. We extend this assistance, enjoyed by
developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor
uses static analysis to guide the decoding. We construct a repository-level
dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On
models of varying parameter scale, by monitoring for type-consistent object
dereferences, MGD consistently improves compilation rates and agreement with
ground truth. Further, LMs with fewer parameters, when augmented with MGD, can
outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation
rate and next-identifier match than the much larger text-davinci-003 model.
</p>
<p>We also conduct a generalizability study to evaluate the ability of MGD to
generalize to multiple programming languages (Java, C# and Rust), coding
scenarios (e.g., correct number of arguments to method calls), and to enforce
richer semantic constraints (e.g., stateful API protocols). Our data and
implementation are available at https://github.com/microsoft/monitors4codegen .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02618">ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval. (arXiv:2308.02618v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Devunuri_S/0/1/0/all/0/1">Saipraneeth Devunuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiam_S/0/1/0/all/0/1">Shirin Qiam</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehe_L/0/1/0/all/0/1">Lewis Lehe</a></p>
<p>The General Transit Feed Specification (GTFS) standard for publishing transit
data is ubiquitous. GTFS being tabular data, with information spread across
different files, necessitates specialized tools or packages to retrieve
information. Concurrently, the use of Large Language Models(LLMs) for text and
information retrieval is growing. The idea of this research is to see if the
current widely adopted LLMs (ChatGPT) are able to understand GTFS and retrieve
information from GTFS using natural language instructions without explicitly
providing information. In this research, we benchmark OpenAI's GPT-3.5-Turbo
and GPT-4 LLMs which are the backbone of ChatGPT. ChatGPT demonstrates a
reasonable understanding of GTFS by answering 59.7% (GPT-3.5-Turbo) and 73.3%
(GPT-4) of our multiple-choice questions (MCQ) correctly. Furthermore, we
evaluated the LLMs on information extraction tasks using a filtered GTFS feed
containing four routes. We found that program synthesis techniques outperformed
zero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and
61% (41%) for complex ones using GPT-4 (GPT-3.5-Turbo).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02553">Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1">Javier Ferrando</a>, <a href="http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1">Matthias Sperber</a>, <a href="http://arxiv.org/find/cs/1/au:+Setiawan_H/0/1/0/all/0/1">Hendra Setiawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Telaar_D/0/1/0/all/0/1">Dominic Telaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1">Sa&#x161;a Hasan</a></p>
<p>Behavioral testing in NLP allows fine-grained evaluation of systems by
examining their linguistic capabilities through the analysis of input-output
behavior. Unfortunately, existing work on behavioral testing in Machine
Translation (MT) is currently restricted to largely handcrafted tests covering
a limited range of capabilities and languages. To address this limitation, we
propose to use Large Language Models (LLMs) to generate a diverse set of source
sentences tailored to test the behavior of MT models in a range of situations.
We can then verify whether the MT model exhibits the expected behavior through
matching candidate sets that are also generated using LLMs. Our approach aims
to make behavioral testing of MT systems practical while requiring only minimal
human effort. In our experiments, we apply our proposed evaluation framework to
assess multiple available MT systems, revealing that while in general
pass-rates follow the trends observable from traditional accuracy-based
metrics, our method was able to uncover several important differences and
potential bugs that go unnoticed when relying only on accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05280">Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yixin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1">Aman Chadha</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a></p>
<p>Recent advancements in Large Language Models empower them to follow freeform
instructions, including imitating generic or specific demographic personas in
conversations. We define generic personas to represent demographic groups, such
as "an Asian person", whereas specific personas may take the form of specific
popular Asian names like "Yumi". While the adoption of personas enriches user
experiences by making dialogue systems more engaging and approachable, it also
casts a shadow of potential risk by exacerbating social biases within model
responses, thereby causing societal harm through interactions with users. In
this paper, we systematically study "persona biases", which we define to be the
sensitivity of dialogue models' harmful behaviors contingent upon the personas
they adopt. We categorize persona biases into biases in harmful expression and
harmful agreement, and establish a comprehensive evaluation framework to
measure persona biases in five aspects: Offensiveness, Toxic Continuation,
Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to
investigate persona biases by experimenting with UNIVERSALPERSONA, a
systematically constructed persona dataset encompassing various types of both
generic and specific model personas. Through benchmarking on four different
models -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers
significant persona biases in dialogue systems. Our findings also underscore
the pressing need to revisit the use of personas in dialogue agents to ensure
safe application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05619">Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamp_J/0/1/0/all/0/1">Jonathan Kamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Beinborn_L/0/1/0/all/0/1">Lisa Beinborn</a>, <a href="http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1">Antske Fokkens</a></p>
<p>Feature attribution scores are used for explaining the prediction of a text
classifier to users by highlighting a k number of tokens. In this work, we
propose a way to determine the number of optimal k tokens that should be
displayed from sequential properties of the attribution scores. Our approach is
dynamic across sentences, method-agnostic, and deals with sentence length bias.
We compare agreement between multiple methods and humans on an NLI task, using
fixed k and dynamic k. We find that perturbation-based methods and Vanilla
Gradient exhibit highest agreement on most method--method and method--human
agreement metrics with a static k. Their advantage over other methods
disappears with dynamic ks which mainly improve Integrated Gradient and
GradientXInput. To our knowledge, this is the first evidence that sequential
properties of attribution scores are informative for consolidating attribution
signals for human interpretation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05628">Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models. (arXiv:2310.05628v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bronzini_M/0/1/0/all/0/1">Marco Bronzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolini_C/0/1/0/all/0/1">Carlo Nicolini</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1">Bruno Lepri</a>, <a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1">Andrea Passerini</a>, <a href="http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1">Jacopo Staiano</a></p>
<p>Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Such information is publicly released in a variety of
non-structured and multi-modal documentation. Hence, it is not straightforward
to aggregate and consolidate such data in a cohesive framework to further
derive insights about sustainability practices across companies and markets.
Given these premises, it is natural to resort to Information Extraction (IE)
techniques to provide concise, informative, and actionable data to the
stakeholders. Moving beyond traditional text processing techniques, in this
work we leverage Large Language Models (LLMs), along with the prominent
in-context learning technique and the Retrieved Augmented Generation (RAG)
paradigm, to extract semantically structured ESG-related information from
companies' sustainability reports. We then adopt graph-based representations to
conduct meaningful statistical, similarity and correlation analyses concerning
the ESG-related actions disclosed by companies in their sustainability reports.
These analyses unveiled that companies address ESG-related issues through
several actions encompassing recognition, compliance, and partnerships;
highlighting the complexity and joint efforts needed to address them. Moreover,
disclosure similarities emerged among companies from the same region or sector.
Lastly, we investigate which factual aspects impact the most on companies' ESG
scores using our findings and other company information. This analysis unveiled
that companies' disclosures affect ESG scores more than other financial or
company characteristics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08372">Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. (arXiv:2310.08372v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1">Boyang Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weichao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yasheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1">Lifeng Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kam-Fai Wong</a></p>
<p>Pretrained language models (PLMs) based knowledge-grounded dialogue systems
are prone to generate responses that are factually inconsistent with the
provided knowledge source. In such inconsistent responses, the dialogue models
fail to accurately express the external knowledge they rely upon. Inspired by
previous work which identified that feed-forward networks (FFNs) within
Transformers are responsible for factual knowledge expressions, we investigate
two methods to efficiently improve the factual expression capability {of FFNs}
by knowledge enhancement and alignment respectively. We first propose
\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers
to enhance factual knowledge expressions} given the specific patterns of
knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement
learning for factual consistency (RLFC) method to implicitly adjust FFNs'
expressions in responses by aligning with gold knowledge for the factual
consistency preference. To comprehensively assess the factual consistency and
dialogue quality of responses, we employ extensive automatic measures and human
evaluations including sophisticated fine-grained NLI-based metrics.
Experimental results on WoW and CMU\_DoG datasets demonstrate that our methods
efficiently enhance the ability of the FFN module to convey factual knowledge,
validating the efficacy of improving factual consistency for knowledge-grounded
dialogue systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09219">&quot;Kelly is a Warm Person, Joseph is a Role Model&quot;: Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yixin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">George Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1">Aparna Garimella</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Large Language Models (LLMs) have recently emerged as an effective tool to
assist individuals in writing various types of content, including professional
documents such as recommendation letters. Though bringing convenience, this
application also introduces unprecedented fairness concerns. Model-generated
reference letters might be directly used by users in professional scenarios. If
underlying biases exist in these model-constructed letters, using them without
scrutinization could lead to direct societal harms, such as sabotaging
application success rates for female applicants. In light of this pressing
issue, it is imminent and necessary to comprehensively study fairness issues
and associated harms in this real-world use case. In this paper, we critically
examine gender biases in LLM-generated reference letters. Drawing inspiration
from social science findings, we design evaluation methods to manifest biases
through 2 dimensions: (1) biases in language style and (2) biases in lexical
content. We further investigate the extent of bias propagation by analyzing the
hallucination bias of models, a term that we define to be bias exacerbation in
model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-
ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated
recommendation letters. Our findings not only warn against using LLMs for this
application without scrutinization, but also illuminate the importance of
thoroughly studying hidden biases and harms in LLM-generated professional
documents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14880">Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?. (arXiv:2310.14880v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1">Xiaoxi Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Lizhen Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1">Lay-Ki Soon</a>, <a href="http://arxiv.org/find/cs/1/au:+Trakic_A/0/1/0/all/0/1">Adnan Trakic</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1">Terry Yue Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Emerton_P/0/1/0/all/0/1">Patrick Charles Emerton</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_G/0/1/0/all/0/1">Genevieve Grant</a></p>
<p>Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions
recently in the legal domain due to its emergent ability to tackle a variety of
legal tasks. However, it is still unknown if LLMs are able to analyze a legal
case and perform reasoning in the same manner as lawyers. Therefore, we
constructed a novel corpus consisting of scenarios pertain to Contract Acts
Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to
perform analysis on the corpus using the IRAC method, which is a framework
widely used by legal professionals for organizing legal analysis. Each scenario
in the corpus is annotated with a complete IRAC analysis in a semi-structured
format so that both machines and legal professionals are able to interpret and
understand the annotations. In addition, we conducted the first empirical
assessment of ChatGPT for IRAC analysis in order to understand how well it
aligns with the analysis of legal professionals. Our experimental results shed
lights on possible future research directions to improve alignments between
LLMs and legal experts in terms of legal reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16789">Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weijia Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1">Anirudh Ajith</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Mengzhou Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yangsibo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Daogao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1">Terra Blevins</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a></p>
<p>Although large language models (LLMs) are widely deployed, the data used to
train them is rarely disclosed. Given the incredible scale of this data, up to
trillions of tokens, it is all but certain that it includes potentially
problematic text such as copyrighted materials, personally identifiable
information, and test data for widely reported reference benchmarks. However,
we currently have no way to know which data of these types is included or in
what proportions. In this paper, we study the pretraining data detection
problem: given a piece of text and black-box access to an LLM without knowing
the pretraining data, can we determine if the model was trained on the provided
text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that
uses data created before and after model training to support gold truth
detection. We also introduce a new detection method Min-K% Prob based on a
simple hypothesis: an unseen example is likely to contain a few outlier words
with low probabilities under the LLM, while a seen example is less likely to
have words with such low probabilities. Min-K% Prob can be applied without any
knowledge about the pretraining corpus or any additional training, departing
from previous detection methods that require training a reference model on data
that is similar to the pretraining data. Moreover, our experiments demonstrate
that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous
methods. We apply Min-K% Prob to three real-world scenarios, copyrighted book
detection, contaminated downstream example detection and privacy auditing of
machine unlearning, and find it a consistently effective solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20033">Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1">Prakamya Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zonghai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Beining Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1">Rohan Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>Large Language Models (LLMs) like the GPT and LLaMA families have
demonstrated exceptional capabilities in capturing and condensing critical
contextual information and achieving state-of-the-art performance in the
summarization task. However, community concerns about these models'
hallucination issues continue to rise. LLMs sometimes generate factually
hallucinated summaries, which can be extremely harmful in the clinical domain
NLP tasks (e.g., clinical note summarization), where factually incorrect
statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using
human feedback has shown the promise of aligning LLMs to be factually
consistent during generation, but such training procedure requires high-quality
human-annotated data, which can be extremely expensive to get in the clinical
domain. In this work, we propose a new pipeline using ChatGPT instead of human
experts to generate high-quality feedback data for improving factual
consistency in the clinical note summarization task. We focus specifically on
edit feedback because recent work discusses the shortcomings of human alignment
via preference feedback in complex situations (such as clinical NLP tasks that
require extensive expert knowledge), as well as some advantages of collecting
edit feedback from domain experts. In addition, although GPT has reached the
expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much
previous work discussing whether GPT can generate expert-level edit feedback
for LMs in the clinical note summarization task. We hope to fill this gap.
Finally, our evaluations demonstrate the potential use of GPT edits in human
alignment, especially from a factuality perspective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20550">CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qiying Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Quan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaosong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yufeng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yue Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinlong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingjing Liu</a></p>
<p>Large multimodal models demonstrate remarkable generalist ability to perform
diverse multimodal tasks in a zero-shot manner. Large-scale web-based
image-text pairs contribute fundamentally to this success, but suffer from
excessive noise. Recent studies use alternative captions synthesized by
captioning models and have achieved notable benchmark performance. However, our
experiments reveal significant Scalability Deficiency and World Knowledge Loss
issues in models trained with synthetic captions, which have been largely
obscured by their initial benchmark success. Upon closer examination, we
identify the root cause as the overly-simplified language structure and lack of
knowledge details in existing synthetic captions. To provide higher-quality and
more scalable multimodal pretraining data, we propose CapsFusion, an advanced
framework that leverages large language models to consolidate and refine
information from both web-based image-text pairs and synthetic captions.
Extensive experiments show that CapsFusion captions exhibit remarkable
all-round superiority over existing captions in terms of model performance
(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample
efficiency (requiring 11-16 times less computation than baselines), world
knowledge depth, and scalability. These effectiveness, efficiency and
scalability advantages position CapsFusion as a promising candidate for future
scaling of LMM training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00128">On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Opper_M/0/1/0/all/0/1">Mattia Opper</a>, <a href="http://arxiv.org/find/cs/1/au:+Morrison_J/0/1/0/all/0/1">J. Morrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1">N. Siddharth</a></p>
<p>This work explores the degree to which grammar acquisition is driven by
language `simplicity' and the source modality (speech vs. text) of data. Using
BabyBERTa as a probe, we find that grammar acquisition is largely driven by
exposure to speech data, and in particular through exposure to two of the
BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this
finding by examining various ways of presenting input data to our model. First,
we assess the impact of various sequence-level complexity based curricula. We
then examine the impact of learning over `blocks' -- covering spans of text
that are balanced for the number of tokens in each of the source corpora
(rather than number of lines). Finally, we explore curricula that vary the
degree to which the model is exposed to different corpora. In all cases, we
find that over-exposure to AO-Childes and Open Subtitles significantly drives
performance. We verify these findings through a comparable control dataset in
which exposure to these corpora, and speech more generally, is limited by
design. Our findings indicate that it is not the proportion of tokens occupied
by high-utility data that aids acquisition, but rather the proportion of
training steps assigned to such data. We hope this encourages future research
into the use of more developmentally plausible linguistic data (which tends to
be more scarce) to augment general purpose pre-training regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00687">Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ryan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yen_H/0/1/0/all/0/1">Howard Yen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1">Raja Marjieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a></p>
<p>How do we communicate with others to achieve our goals? We use our prior
experience or advice from others, or construct a candidate utterance by
predicting how it will be received. However, our experiences are limited and
biased, and reasoning about potential outcomes can be difficult and cognitively
challenging. In this paper, we explore how we can leverage Large Language Model
(LLM) simulations to help us communicate better. We propose the
Explore-Generate-Simulate (EGS) framework, which takes as input any scenario
where an individual is communicating to an audience with a goal they want to
achieve. EGS (1) explores the solution space by producing a diverse set of
advice relevant to the scenario, (2) generates communication candidates
conditioned on subsets of the advice, and (3) simulates the reactions from
various audiences to determine both the best candidate and advice to use. We
evaluate the framework on eight scenarios spanning the ten fundamental
processes of interpersonal communication. For each scenario, we collect a
dataset of human evaluations across candidates and baselines, and showcase that
our framework's chosen candidate is preferred over popular generation
mechanisms including Chain-of-Thought. We also find that audience simulations
achieve reasonably high agreement with human raters across 5 of the 8
scenarios. Finally, we demonstrate the generality of our framework by applying
it to real-world scenarios described by users on web forums. Through
evaluations and demonstrations, we show that EGS enhances the effectiveness and
outcomes of goal-oriented communication across a variety of situations, thus
opening up new possibilities for the application of large language models in
revolutionizing communication and decision-making processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01282">FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1">Ke Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guohao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qiuli Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiuhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kangdi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hanyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a></p>
<p>As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and &gt;50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
</p>
<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17688">Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1">Geoffrey Hinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1">Yuval Noah Harari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1">Shai Shalev-Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1">Gillian Hadfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1">Jeff Clune</a>, <a href="http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1">Tegan Maharaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, <a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1">Sheila McIlraith</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Ashwin Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Krueger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1">Stuart Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1">Daniel Kahneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a></p>
<p>In this short consensus paper, we outline risks from upcoming, advanced AI
systems. We examine large-scale social harms and malicious uses, as well as an
irreversible loss of human control over autonomous AI systems. In light of
rapid and continuing AI progress, we propose priorities for AI R&amp;D and
governance.
</p>
</p>
</div>

    </div>
    </body>
    