<!DOCTYPE html>
<html>
<head>
<title>2023-11-06-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.00732">tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Glazkova_A/0/1/0/all/0/1">Anna Glazkova</a></p>
<p>The paper describes a system developed for Task 1 at SMM4H 2023. The goal of
the task is to automatically distinguish tweets that self-report a COVID-19
diagnosis (for example, a positive test, clinical diagnosis, or
hospitalization) from those that do not. We investigate the use of different
techniques for preprocessing tweets using four transformer-based models. The
ensemble of fine-tuned language models obtained an F1-score of 84.5%, which is
4.1% higher than the average value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00739">Can Large Language Models Design Accurate Label Functions?. (arXiv:2311.00739v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_N/0/1/0/all/0/1">Naiqing Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kaiwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Koudas_N/0/1/0/all/0/1">Nick Koudas</a></p>
<p>Programmatic weak supervision methodologies facilitate the expedited labeling
of extensive datasets through the use of label functions (LFs) that encapsulate
heuristic data sources. Nonetheless, the creation of precise LFs necessitates
domain expertise and substantial endeavors. Recent advances in pre-trained
language models (PLMs) have exhibited substantial potential across diverse
tasks. However, the capacity of PLMs to autonomously formulate accurate LFs
remains an underexplored domain. In this research, we address this gap by
introducing DataSculpt, an interactive framework that harnesses PLMs for the
automated generation of LFs. Within DataSculpt, we incorporate an array of
prompting techniques, instance selection strategies, and LF filtration methods
to explore the expansive design landscape. Ultimately, we conduct a thorough
assessment of DataSculpt's performance on 12 real-world datasets, encompassing
a range of tasks. This evaluation unveils both the strengths and limitations of
contemporary PLMs in LF design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00762">Challenges for Linguistically-Driven Computer-Based Sign Recognition from Continuous Signing for American Sign Language. (arXiv:2311.00762v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neidle_C/0/1/0/all/0/1">Carol Neidle</a></p>
<p>There have been recent advances in computer-based recognition of isolated,
citation-form signs from video. There are many challenges for such a task, not
least the naturally occurring inter- and intra- signer synchronic variation in
sign production, including sociolinguistic variation in the realization of
certain signs. However, there are several significant factors that make
recognition of signs from continuous signing an even more difficult problem.
This article presents an overview of such challenges, based in part on findings
from a large corpus of linguistically annotated video data for American Sign
Language (ASL). Some linguistic regularities in the structure of signs that can
boost handshape and sign recognition are also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00768">Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yurong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1">Manuel Burger</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1">Gunnar R&#xe4;tsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsova_R/0/1/0/all/0/1">Rita Kuznetsova</a></p>
<p>In research areas with scarce data, representation learning plays a
significant role. This work aims to enhance representation learning for
clinical time series by deriving universal embeddings for clinical features,
such as heart rate and blood pressure. We use self-supervised training
paradigms for language models to learn high-quality clinical feature
embeddings, achieving a finer granularity than existing time-step and
patient-level representation learning. We visualize the learnt embeddings via
unsupervised dimension reduction techniques and observe a high degree of
consistency with prior clinical knowledge. We also evaluate the model
performance on the MIMIC-III benchmark and demonstrate the effectiveness of
using clinical feature embeddings. We publish our code online for replication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00790">Construction Artifacts in Metaphor Identification Datasets. (arXiv:2311.00790v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boisson_J/0/1/0/all/0/1">Joanne Boisson</a>, <a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1">Luis Espinosa-Anke</a>, <a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1">Jose Camacho-Collados</a></p>
<p>Metaphor identification aims at understanding whether a given expression is
used figuratively in context. However, in this paper we show how existing
metaphor identification datasets can be gamed by fully ignoring the potential
metaphorical expression or the context in which it occurs. We test this
hypothesis in a variety of datasets and settings, and show that metaphor
identification systems based on language models without complete information
can be competitive with those using the full context. This is due to the
construction procedures to build such datasets, which introduce unwanted biases
for positive and negative classes. Finally, we test the same hypothesis on
datasets that are carefully sampled from natural corpora and where this bias is
not present, making these datasets more challenging and reliable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00835">Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing. (arXiv:2311.00835v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yanlin Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1">Adithya Pratapa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1">David R Mortensen</a></p>
<p>Ultra-fine entity typing plays a crucial role in information extraction by
predicting fine-grained semantic types for entity mentions in text. However,
this task poses significant challenges due to the massive number of entity
types in the output space. The current state-of-the-art approaches, based on
standard multi-label classifiers or cross-encoder models, suffer from poor
generalization performance or inefficient inference. In this paper, we present
CASENT, a seq2seq model designed for ultra-fine entity typing that predicts
ultra-fine types with calibrated confidence scores. Our model takes an entity
mention as input and employs constrained beam search to generate multiple types
autoregressively. The raw sequence probabilities associated with the predicted
types are then transformed into confidence scores using a novel calibration
method. We conduct extensive experiments on the UFET dataset which contains
over 10k types. Our method outperforms the previous state-of-the-art in terms
of F1 score and calibration error, while achieving an inference speedup of over
50 times. Additionally, we demonstrate the generalization capabilities of our
model by evaluating it in zero-shot and few-shot settings on five specialized
domain entity typing datasets that are unseen during training. Remarkably, our
model outperforms large language models with 10 times more parameters in the
zero-shot setting, and when fine-tuned on 50 examples, it significantly
outperforms ChatGPT on all datasets. Our code, models and demo are available at
https://github.com/yanlinf/CASENT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00863">Training Dynamics of Contextual N-Grams in Language Models. (arXiv:2311.00863v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quirke_L/0/1/0/all/0/1">Lucia Quirke</a>, <a href="http://arxiv.org/find/cs/1/au:+Heindrich_L/0/1/0/all/0/1">Lovis Heindrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurnee_W/0/1/0/all/0/1">Wes Gurnee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1">Neel Nanda</a></p>
<p>Prior work has shown the existence of contextual neurons in language models,
including a neuron that activates on German text. We show that this neuron
exists within a broader contextual n-gram circuit: we find late layer neurons
which recognize and continue n-grams common in German text, but which only
activate if the German neuron is active. We investigate the formation of this
circuit throughout training and find that it is an example of what we call a
second-order circuit. In particular, both the constituent n-gram circuits and
the German detection circuit which culminates in the German neuron form with
independent functions early in training - the German detection circuit
partially through modeling German unigram statistics, and the n-grams by
boosting appropriate completions. Only after both circuits have already formed
do they fit together into a second-order circuit. Contrary to the hypotheses
presented in prior work, we find that the contextual n-gram circuit forms
gradually rather than in a sudden phase transition. We further present a range
of anomalous observations such as a simultaneous phase transition in many tasks
coinciding with the learning rate warm-up, and evidence that many context
neurons form simultaneously early in training but are later unlearned.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00867">Automatic Disfluency Detection from Untranscribed Speech. (arXiv:2311.00867v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Romana_A/0/1/0/all/0/1">Amrit Romana</a>, <a href="http://arxiv.org/find/eess/1/au:+Koishida_K/0/1/0/all/0/1">Kazuhito Koishida</a>, <a href="http://arxiv.org/find/eess/1/au:+Provost_E/0/1/0/all/0/1">Emily Mower Provost</a></p>
<p>Speech disfluencies, such as filled pauses or repetitions, are disruptions in
the typical flow of speech. Stuttering is a speech disorder characterized by a
high rate of disfluencies, but all individuals speak with some disfluencies and
the rates of disfluencies may by increased by factors such as cognitive load.
Clinically, automatic disfluency detection may help in treatment planning for
individuals who stutter. Outside of the clinic, automatic disfluency detection
may serve as a pre-processing step to improve natural language understanding in
downstream applications. With this wide range of applications in mind, we
investigate language, acoustic, and multimodal methods for frame-level
automatic disfluency detection and categorization. Each of these methods relies
on audio as an input. First, we evaluate several automatic speech recognition
(ASR) systems in terms of their ability to transcribe disfluencies, measured
using disfluency error rates. We then use these ASR transcripts as input to a
language-based disfluency detection model. We find that disfluency detection
performance is largely limited by the quality of transcripts and alignments. We
find that an acoustic-based approach that does not require transcription as an
intermediate step outperforms the ASR language approach. Finally, we present
multimodal architectures which we find improve disfluency detection performance
over the unimodal approaches. Ultimately, this work introduces novel approaches
for automatic frame-level disfluency and categorization. In the long term, this
will help researchers incorporate automatic disfluency detection into a range
of applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00871">Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1">Steve Yadlowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_L/0/1/0/all/0/1">Lyric Doshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tripuraneni_N/0/1/0/all/0/1">Nilesh Tripuraneni</a></p>
<p>Transformer models, notably large language models (LLMs), have the remarkable
ability to perform in-context learning (ICL) -- to perform new tasks when
prompted with unseen input-output examples without any explicit model training.
In this work, we study how effectively transformers can bridge between their
pretraining data mixture, comprised of multiple distinct task families, to
identify and learn new tasks in-context which are both inside and outside the
pretraining distribution. Building on previous work, we investigate this
question in a controlled setting, where we study transformer models trained on
sequences of $(x, f(x))$ pairs rather than natural language. Our empirical
results show transformers demonstrate near-optimal unsupervised model selection
capabilities, in their ability to first in-context identify different task
families and in-context learn within them when the task families are
well-represented in their pretraining data. However when presented with tasks
or functions which are out-of-domain of their pretraining data, we demonstrate
various failure modes of transformers and degradation of their generalization
for even simple extrapolation tasks. Together our results highlight that the
impressive ICL abilities of high-capacity sequence models may be more closely
tied to the coverage of their pretraining data mixtures than inductive biases
that create fundamental generalization capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00895">In-Context Prompt Editing For Conditional Audio Generation. (arXiv:2311.00895v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1">Ernie Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Pin-Jie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Sidd Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1">Gael Le Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kant_D/0/1/0/all/0/1">David Kant</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yangyang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1">Forrest Iandola</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a></p>
<p>Distributional shift is a central challenge in the deployment of machine
learning models as they can be ill-equipped for real-world data. This is
particularly evident in text-to-audio generation where the encoded
representations are easily undermined by unseen prompts, which leads to the
degradation of generated audio -- the limited set of the text-audio pairs
remains inadequate for conditional audio generation in the wild as user prompts
are under-specified. In particular, we observe a consistent audio quality
degradation in generated audio samples with user prompts, as opposed to
training set prompts. To this end, we present a retrieval-based in-context
prompt editing framework that leverages the training captions as demonstrative
exemplars to revisit the user prompts. We show that the framework enhanced the
audio quality across the set of collected user prompts, which were edited with
reference to the training captions as exemplars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00897">On The Open Prompt Challenge In Conditional Audio Generation. (arXiv:2311.00897v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1">Ernie Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Sidd Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luthra_M/0/1/0/all/0/1">Mahi Luthra</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Pin-Jie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1">Varun Nagaraja</a>, <a href="http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1">Forrest Iandola</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zechun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1">Zhaoheng Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Changsheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yangyang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a></p>
<p>Text-to-audio generation (TTA) produces audio from a text description,
learning from pairs of audio samples and hand-annotated text. However,
commercializing audio generation is challenging as user-input prompts are often
under-specified when compared to text descriptions used to train TTA models. In
this work, we treat TTA models as a ``blackbox'' and address the user prompt
challenge with two key insights: (1) User prompts are generally
under-specified, leading to a large alignment gap between user prompts and
training prompts. (2) There is a distribution of audio descriptions for which
TTA models are better at generating higher quality audio, which we refer to as
``audionese''. To this end, we rewrite prompts with instruction-tuned models
and propose utilizing text-audio alignment as feedback signals via margin
ranking learning for audio improvements. On both objective and subjective human
evaluations, we observed marked improvements in both text-audio alignment and
music audio quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00906">Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition. (arXiv:2311.00906v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haocheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Wei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Ngoc Dang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Lan Du</a></p>
<p>Active learning, a widely adopted technique for enhancing machine learning
models in text and image classification tasks with limited annotation
resources, has received relatively little attention in the domain of Named
Entity Recognition (NER). The challenge of data imbalance in NER has hindered
the effectiveness of active learning, as sequence labellers lack sufficient
learning signals. To address these challenges, this paper presents a novel
reweighting-based active learning strategy that assigns dynamic smoothed
weights to individual tokens. This adaptable strategy is compatible with
various token-level acquisition functions and contributes to the development of
robust active learners. Experimental results on multiple corpora demonstrate
the substantial performance improvement achieved by incorporating our
re-weighting strategy into existing acquisition functions, validating its
practical efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00913">Self-Influence Guided Data Reweighting for Language Model Pre-training. (arXiv:2311.00913v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1">Megh Thakkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1">Tolga Bolukbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1">Shikhar Vashishth</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1">Partha Talukdar</a></p>
<p>Language Models (LMs) pre-trained with self-supervision on large text corpora
have become the default starting point for developing models for various NLP
tasks. Once the pre-training corpus has been assembled, all data samples in the
corpus are treated with equal importance during LM pre-training. However, due
to varying levels of relevance and quality of data, equal importance to all the
data samples may not be the optimal choice. While data reweighting has been
explored in the context of task-specific supervised learning and LM
fine-tuning, model-driven reweighting for pre-training data has not been
explored. We fill this important gap and propose PRESENCE, a method for jointly
reweighting samples by leveraging self-influence (SI) scores as an indicator of
sample importance and pre-training. PRESENCE promotes novelty and stability for
model pre-training. Through extensive analysis spanning multiple model sizes,
datasets, and tasks, we present PRESENCE as an important first step in the
research direction of sample reweighting for pre-training language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00915">Task-Agnostic Low-Rank Adapters for Unseen English Dialects. (arXiv:2311.00915v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zedian Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1">William Held</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yanchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a></p>
<p>Large Language Models (LLMs) are trained on corpora disproportionally
weighted in favor of Standard American English. As a result, speakers of other
dialects experience significantly more failures when interacting with these
technologies. In practice, these speakers often accommodate their speech to be
better understood. Our work shares the belief that language technologies should
be designed to accommodate the diversity in English dialects and not the other
way around. However, prior works on dialect struggle with generalizing to
evolving and emerging dialects in a scalable manner. To fill this gap, our
method, HyperLoRA, leverages expert linguistic knowledge to enable
resource-efficient adaptation via hypernetworks. By disentangling
dialect-specific and cross-dialectal information, HyperLoRA improves
generalization to unseen dialects in a task-agnostic fashion. Not only is
HyperLoRA more scalable in the number of parameters, but it also achieves the
best or most competitive performance across 5 dialects in a zero-shot setting.
In this way, our approach facilitates access to language technology for
billions of English dialect speakers who are traditionally underrepresented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00945">E3 TTS: Easy End-to-End Diffusion-based Text to Speech. (arXiv:2311.00945v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1">Nobuyuki Morioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nanxin Chen</a></p>
<p>We propose Easy End-to-End Diffusion-based Text to Speech, a simple and
efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly
takes plain text as input and generates an audio waveform through an iterative
refinement process. Unlike many prior work, E3 TTS does not rely on any
intermediate representations like spectrogram features or alignment
information. Instead, E3 TTS models the temporal structure of the waveform
through the diffusion process. Without relying on additional conditioning
information, E3 TTS could support flexible latent structure within the given
audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as
editing without any additional training. Experiments show that E3 TTS can
generate high-fidelity audio, approaching the performance of a state-of-the-art
neural TTS system. Audio samples are available at https://e3tts.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00953">Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation. (arXiv:2311.00953v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Wanyu Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yangfeng Ji</a></p>
<p>The development of trustworthy conversational information-seeking systems
relies on dialogue models that can generate faithful and accurate responses
based on relevant knowledge texts. However, two main challenges hinder this
task. Firstly, language models may generate hallucinations due to data biases
present in their pretraining corpus. Secondly, knowledge texts often contain
redundant and irrelevant information that distracts the model's attention from
the relevant text span. Previous works use additional data annotations on the
knowledge texts to learn a knowledge identification module in order to bypass
irrelevant information, but collecting such high-quality span annotations can
be costly. In this work, we leverage reinforcement learning algorithms to
overcome the above challenges by introducing a novel reward function. Our
reward function combines an accuracy metric and a faithfulness metric to
provide a balanced quality judgment of generated responses, which can be used
as a cost-effective approximation to a human preference reward model when only
a few preference annotations are available. Empirical experiments on two
conversational information-seeking datasets demonstrate that our method can
compete with other strong supervised learning baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00958">IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems. (arXiv:2311.00958v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kautsar_M/0/1/0/all/0/1">Muhammad Dehan Al Kautsar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nurdini_R/0/1/0/all/0/1">Rahmah Khoirussyifa&#x27; Nurdini</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1">Samuel Cahyawijaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1">Genta Indra Winata</a>, <a href="http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1">Ayu Purwarianti</a></p>
<p>Task-oriented dialogue (ToD) systems have been mostly created for
high-resource languages, such as English and Chinese. However, there is a need
to develop ToD systems for other regional or local languages to broaden their
ability to comprehend the dialogue contexts in various languages. This paper
introduces IndoToD, an end-to-end multi domain ToD benchmark in Indonesian. We
extend two English ToD datasets to Indonesian, comprising four different
domains by delexicalization to efficiently reduce the size of annotations. To
ensure a high-quality data collection, we hire native speakers to manually
translate the dialogues. Along with the original English datasets, these new
Indonesian datasets serve as an effective benchmark for evaluating Indonesian
and English ToD systems as well as exploring the potential benefits of
cross-lingual and bilingual transfer learning approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00967">Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shirai_K/0/1/0/all/0/1">Keisuke Shirai</a>, <a href="http://arxiv.org/find/cs/1/au:+Beltran_Hernandez_C/0/1/0/all/0/1">Cristian C. Beltran-Hernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamaya_M/0/1/0/all/0/1">Masashi Hamaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1">Atsushi Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1">Shohei Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaharazuka_K/0/1/0/all/0/1">Kento Kawaharazuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1">Kazutoshi Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1">Yoshitaka Ushiku</a>, <a href="http://arxiv.org/find/cs/1/au:+Mori_S/0/1/0/all/0/1">Shinsuke Mori</a></p>
<p>Large language models (LLMs) are accelerating the development of
language-guided robot planners. Meanwhile, symbolic planners offer the
advantage of interpretability. This paper proposes a new task that bridges
these two trends, namely, multimodal planning problem specification. The aim is
to generate a problem description (PD), a machine-readable file used by the
planners to find a plan. By generating PDs from language instruction and scene
observation, we can drive symbolic planners in a language-guided framework. We
propose a Vision-Language Interpreter (ViLaIn), a new framework that generates
PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine
generated PDs via error message feedback from the symbolic planner. Our aim is
to answer the question: How accurately can ViLaIn and the symbolic planner
generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset
called the problem description generation (ProDG) dataset. The framework is
evaluated with four new evaluation metrics. Experimental results show that
ViLaIn can generate syntactically correct problems with more than 99% accuracy
and valid plans with more than 58% accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00998">Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia. (arXiv:2311.00998v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Susanto_L/0/1/0/all/0/1">Lucky Susanto</a>, <a href="http://arxiv.org/find/cs/1/au:+Diandaru_R/0/1/0/all/0/1">Ryandito Diandaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1">Adila Krisnadhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1">Ayu Purwarianti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1">Derry Wijaya</a></p>
<p>Neural machine translation (NMT) for low-resource local languages in
Indonesia faces significant challenges, including the need for a representative
benchmark and limited data availability. This work addresses these challenges
by comprehensively analyzing training NMT systems for four low-resource local
languages in Indonesia: Javanese, Sundanese, Minangkabau, and Balinese. Our
study encompasses various training approaches, paradigms, data sizes, and a
preliminary study into using large language models for synthetic low-resource
languages parallel data generation. We reveal specific trends and insights into
practical strategies for low-resource language translation. Our research
demonstrates that despite limited computational resources and textual data,
several of our NMT systems achieve competitive performances, rivaling the
translation quality of zero-shot gpt-3.5-turbo. These findings significantly
advance NMT for low-resource languages, offering valuable guidance for
researchers in similar contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01012">COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wibowo_H/0/1/0/all/0/1">Haryo Akbarianto Wibowo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuadi_E/0/1/0/all/0/1">Erland Hilman Fuadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1">Made Nindyatama Nityasya</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1">Radityo Eko Prasojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1">Alham Fikri Aji</a></p>
<p>We present publicly available COPAL-ID, a novel Indonesian language common
sense reasoning dataset. Unlike the previous Indonesian COPA dataset
(XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and
therefore, provides a more natural portrayal of day-to-day causal reasoning
within the Indonesian cultural sphere. Professionally written by natives from
scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the
translated XCOPA-ID. In addition, we present COPAL-ID in both standard
Indonesian and in Jakartan Indonesian--a dialect commonly used in daily
conversation. COPAL-ID poses a greater challenge for existing open-sourced and
closed state-of-the-art multilingual language models, yet is trivially easy for
humans. Our findings suggest that even the current best open-source,
multilingual model struggles to perform well, achieving 65.47% accuracy on
COPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%).
Despite GPT-4's impressive score, it suffers the same performance degradation
compared to its XCOPA-ID score, and it still falls short of human performance.
This shows that these language models are still way behind in comprehending the
local nuances of Indonesian.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01030">Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1">Hao Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaosu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>Aspect-based sentiment classification (ASC) aims to judge the sentiment
polarity conveyed by the given aspect term in a sentence. The sentiment
polarity is not only determined by the local context but also related to the
words far away from the given aspect term. Most recent efforts related to the
attention-based models can not sufficiently distinguish which words they should
pay more attention to in some cases. Meanwhile, graph-based models are coming
into ASC to encode syntactic dependency tree information. But these models do
not fully leverage syntactic dependency trees as they neglect to incorporate
dependency relation tag information into representation learning effectively.
In this paper, we address these problems by effectively modeling the local and
global features. Firstly, we design a local encoder containing: a Gaussian mask
layer and a covariance self-attention layer. The Gaussian mask layer tends to
adjust the receptive field around aspect terms adaptively to deemphasize the
effects of unrelated words and pay more attention to local information. The
covariance self-attention layer can distinguish the attention weights of
different words more obviously. Furthermore, we propose a dual-level graph
attention network as a global encoder by fully employing dependency tag
information to capture long-distance information effectively. Our model
achieves state-of-the-art performance on both SemEval 2014 and Twitter
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01036">ATHENA: Mathematical Reasoning with Thought Expansion. (arXiv:2311.01036v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">JB. Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hazel Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_J/0/1/0/all/0/1">Joonghyuk Hahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yo-Sub Han</a></p>
<p>Solving math word problems depends on how to articulate the problems, the
lens through which models view human linguistic expressions. Real-world
settings count on such a method even more due to the diverse practices of the
same mathematical operations. Earlier works constrain available thinking
processes by limited prediction strategies without considering their
significance in acquiring mathematical knowledge. We introduce Attention-based
THought Expansion Network Architecture (ATHENA) to tackle the challenges of
real-world practices by mimicking human thought expansion mechanisms in the
form of neural network propagation. A thought expansion recurrently generates
the candidates carrying the thoughts of possible math expressions driven from
the previous step and yields reasonable thoughts by selecting the valid
pathways to the goal. Our experiments show that ATHENA achieves a new
state-of-the-art stage toward the ideal model that is compelling in variant
questions even when the informativeness in training examples is restricted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01041">Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Lang Cao</a></p>
<p>Large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, enabling them to answer a wide range
of questions across various domains. However, these models are not flawless and
often produce responses that contain errors or misinformation. These
inaccuracies, commonly referred to as hallucinations, render LLMs unreliable
and even unusable in many scenarios. In this paper, our focus is on mitigating
the issue of hallucination in LLMs, particularly in the context of
question-answering. Instead of attempting to answer all questions, we explore a
refusal mechanism that instructs LLMs to refuse to answer challenging questions
in order to avoid errors. We then propose a simple yet effective solution
called Learn to Refuse (L2R), which incorporates the refusal mechanism to
enable LLMs to recognize and refuse to answer questions that they find
difficult to address. To achieve this, we utilize a structured knowledge base
to represent all the LLM's understanding of the world, enabling it to provide
traceable gold knowledge. This knowledge base is separate from the LLM and
initially empty, and it is progressively expanded with validated knowledge.
When an LLM encounters questions outside its domain, the system recognizes its
knowledge scope and determines whether it can answer the question
independently. Additionally, we introduce a method for automatically and
efficiently expanding the knowledge base of LLMs. Through qualitative and
quantitative analysis, we demonstrate that our approach enhances the
controllability and reliability of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01049">Multi-dimensional data refining strategy for effective fine-tuning LLMs. (arXiv:2311.01049v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ngoc_T/0/1/0/all/0/1">Thanh Nguyen Ngoc</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quang Nhat Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1">Arthur Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1">Bao Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thuy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Thanh Pham</a></p>
<p>Data is a cornerstone for fine-tuning large language models, yet acquiring
suitable data remains challenging. Challenges encompassed data scarcity,
linguistic diversity, and domain-specific content. This paper presents lessons
learned while crawling and refining data tailored for fine-tuning Vietnamese
language models. Crafting such a dataset, while accounting for linguistic
intricacies and striking a balance between inclusivity and accuracy, demands
meticulous planning. Our paper presents a multidimensional strategy including
leveraging existing datasets in the English language and developing customized
data-crawling scripts with the assistance of generative AI tools. A fine-tuned
LLM model for the Vietnamese language, which was produced using resultant
datasets, demonstrated good performance while generating Vietnamese news
articles from prompts. The study offers practical solutions and guidance for
future fine-tuning models in languages like Vietnamese.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01070">DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1">Thomas Palmeira Ferraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1">Marcely Zanon Boito</a>, <a href="http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1">Caroline Brun</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1">Vassilina Nikoulina</a></p>
<p>Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still under-performs on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01108">Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance. (arXiv:2311.01108v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Song Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhen Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Ruocheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jundong Li</a></p>
<p>Adopting a two-stage paradigm of pretraining followed by fine-tuning,
Pretrained Language Models (PLMs) have achieved substantial advancements in the
field of natural language processing. However, in real-world scenarios, data
labels are often noisy due to the complex annotation process, making it
essential to develop strategies for fine-tuning PLMs with such noisy labels. To
this end, we introduce an innovative approach for fine-tuning PLMs using noisy
labels, which incorporates the guidance of Large Language Models (LLMs) like
ChatGPT. This guidance assists in accurately distinguishing between clean and
noisy samples and provides supplementary information beyond the noisy labels,
thereby boosting the learning process during fine-tuning PLMs. Extensive
experiments on synthetic and real-world noisy datasets further demonstrate the
superior advantages of our framework over the state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01149">Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianghao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1">Pu Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1">Tengxiao Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1">Yidong Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chenglin Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1">Qianlong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guibo Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1">Chengqing Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiajun Zhang</a></p>
<p>During the development of large language models (LLMs), the scale and quality
of the pre-training data play a crucial role in shaping LLMs' capabilities. To
accelerate the research of LLMs, several large-scale datasets, such as C4 [1],
Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public.
However, most of the released corpus focus mainly on English, and there is
still lack of complete tool-chain for extracting clean texts from web data.
Furthermore, fine-grained information of the corpus, e.g. the quality of each
text, is missing. To address these challenges, we propose in this paper a new
complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data.
First, similar to previous work, manually crafted rules are employed to discard
explicit noisy texts from the raw crawled web contents. Second, a well-designed
evaluation model is leveraged to assess the remaining relatively clean data,
and each text is assigned a specific quality score. Finally, we can easily
utilize an appropriate threshold to select the high-quality pre-training data
for Chinese. Using our proposed approach, we release the largest and latest
large-scale high-quality Chinese web text ChineseWebText, which consists of
1.42 TB and each text is associated with a quality score, facilitating the LLM
researchers to choose the data according to the desired quality thresholds. We
also release a much cleaner subset of 600 GB Chinese data with the quality
exceeding 90%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01150">Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1">Peng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haobo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weikang Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junbo Zhao</a></p>
<p>In recent years, large language models (LLMs), such as GPTs, have attained
great impact worldwide. However, how to adapt these LLMs to better suit the
vertical domain-specific tasks by utilizing external knowledge remains not
completely solved. Indeed, there have emerged a few works on this line where
most of them rely on an alignment heuristic that is built to inject the
corresponding knowledge tuple into the associated text sample.
</p>
<p>However, despite the promise, we identify a pivotal problem in this work
ubiquitously. Simply put, we find that injecting unaligned (i.e., random)
knowledge tuple into the LLMs achieves comparable (and sometimes better)
results than the aligned knowledge being injected. We therefore take a thorough
investigation of this frustrating finding on a variety of related prior work
and further provide a chain of potential interpretations for the phenomenon.
Based on all that, we offer a simple remediated technique. Briefly, the core of
this technique is rooted in an ideological emphasis on the pruning and
purification of the external knowledge base to be injected into LLMs. At last,
we show that by integrating this technique into most (if not all) knowledge
injection frameworks and recent LLMs, it manages to overcome the aforementioned
sanity problem and further pushes the boundary of the performance of the
domain-adaptive LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01152">Predicting Question-Answering Performance of Large Language Models through Semantic Consistency. (arXiv:2311.01152v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1">Ella Rabinovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Ackerman_S/0/1/0/all/0/1">Samuel Ackerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Raz_O/0/1/0/all/0/1">Orna Raz</a>, <a href="http://arxiv.org/find/cs/1/au:+Farchi_E/0/1/0/all/0/1">Eitan Farchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1">Ateret Anaby-Tavor</a></p>
<p>Semantic consistency of a language model is broadly defined as the model's
ability to produce semantically-equivalent outputs, given
semantically-equivalent inputs. We address the task of assessing
question-answering (QA) semantic consistency of contemporary large language
models (LLMs) by manually creating a benchmark dataset with high-quality
paraphrases for factual questions, and release the dataset to the community.
</p>
<p>We further combine the semantic consistency metric with additional
measurements suggested in prior work as correlating with LLM QA accuracy, for
building and evaluating a framework for factual QA reference-less performance
prediction -- predicting the likelihood of a language model to accurately
answer a question. Evaluating the framework on five contemporary LLMs, we
demonstrate encouraging, significantly outperforming baselines, results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01153">ACES: Translation Accuracy Challenge Sets at WMT 2023. (arXiv:2311.01153v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1">Chantal Amrhein</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1">Nikita Moghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1">Liane Guillou</a></p>
<p>We benchmark the performance of segmentlevel metrics submitted to WMT 2023
using the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists
of 36K examples representing challenges from 68 phenomena and covering 146
language pairs. The phenomena range from simple perturbations at the
word/character level to more complex errors based on discourse and real-world
knowledge. For each metric, we provide a detailed profile of performance over a
range of error categories as well as an overall ACES-Score for quick
comparison. We also measure the incremental performance of the metrics
submitted to both WMT 2023 and 2022. We find that 1) there is no clear winner
among the metrics submitted to WMT 2023, and 2) performance change between the
2023 and 2022 versions of the metrics is highly variable. Our recommendations
are similar to those from WMT 2022. Metric developers should focus on: building
ensembles of metrics from different design families, developing metrics that
pay more attention to the source and rely less on surface-level overlap, and
carefully determining the influence of multilingual embeddings on MT
evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01161">Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering. (arXiv:2311.01161v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kang-il Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Segwang Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kyomin Jung</a></p>
<p>The problem of spurious programs is a longstanding challenge when training a
semantic parser from weak supervision. To eliminate such programs that have
wrong semantics but correct denotation, existing methods focus on exploiting
similarities between examples based on domain-specific knowledge. In this
paper, we propose a domain-agnostic filtering mechanism based on program
execution results. Specifically, for each program obtained through the search
process, we first construct a representation that captures the program's
semantics as execution results under various inputs. Then, we run a majority
vote on these representations to identify and filter out programs with
significantly different semantics from the other programs. In particular, our
method is orthogonal to the program search process so that it can easily
augment any of the existing weakly supervised semantic parsing frameworks.
Empirical evaluations on the Natural Language Visual Reasoning and
WikiTableQuestions demonstrate that applying our method to the existing
semantic parsers induces significantly improved performances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01166">Generative Input: Towards Next-Generation Input Methods Paradigm. (arXiv:2311.01166v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1">Keyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongcan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zihang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhenzhen Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shijin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a></p>
<p>Since the release of ChatGPT, generative models have achieved tremendous
success and become the de facto approach for various NLP tasks. However, its
application in the field of input methods remains under-explored. Many neural
network approaches have been applied to the construction of Chinese input
method engines(IMEs).Previous research often assumed that the input pinyin was
correct and focused on Pinyin-to-character(P2C) task, which significantly falls
short of meeting users' demands. Moreover, previous research could not leverage
user feedback to optimize the model and provide personalized results. In this
study, we propose a novel Generative Input paradigm named GeneInput. It uses
prompts to handle all input scenarios and other intelligent auxiliary input
functions, optimizing the model with user feedback to deliver personalized
results. The results demonstrate that we have achieved state-of-the-art
performance for the first time in the Full-mode Key-sequence to
Characters(FK2C) task. We propose a novel reward model training method that
eliminates the need for additional manual annotations and the performance
surpasses GPT-4 in tasks involving intelligent association and conversational
assistance. Compared to traditional paradigms, GeneInput not only demonstrates
superior performance but also exhibits enhanced robustness, scalability, and
online learning capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01173">CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL. (arXiv:2311.01173v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1">Mayank Kothyari</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhingra_D/0/1/0/all/0/1">Dhruva Dhingra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1">Sunita Sarawagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1">Soumen Chakrabarti</a></p>
<p>Existing Text-to-SQL generators require the entire schema to be encoded with
the user text. This is expensive or impractical for large databases with tens
of thousands of columns. Standard dense retrieval techniques are inadequate for
schema subsetting of a large structured database, where the correct semantics
of retrieval demands that we rank sets of schema elements rather than
individual elements. In response, we propose a two-stage process for effective
coverage during retrieval. First, we instruct an LLM to hallucinate a minimal
DB schema deemed adequate to answer the query. We use the hallucinated schema
to retrieve a subset of the actual schema, by composing the results from
multiple dense retrievals. Remarkably, hallucination $\unicode{x2013}$
generally considered a nuisance $\unicode{x2013}$ turns out to be actually
useful as a bridging mechanism. Since no existing benchmarks exist for schema
subsetting on large databases, we introduce three benchmarks. Two
semi-synthetic datasets are derived from the union of schemas in two well-known
datasets, SPIDER and BIRD, resulting in 4502 and 798 schema elements
respectively. A real-life benchmark called SocialDB is sourced from an actual
large data warehouse comprising 17844 schema elements. We show that our method1
leads to significantly higher recall than SOTA retrieval-based augmentation
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01200">A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gogoulou_E/0/1/0/all/0/1">Evangelia Gogoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1">Timoth&#xe9;e Lesort</a>, <a href="http://arxiv.org/find/cs/1/au:+Boman_M/0/1/0/all/0/1">Magnus Boman</a>, <a href="http://arxiv.org/find/cs/1/au:+Nivre_J/0/1/0/all/0/1">Joakim Nivre</a></p>
<p>The recent increase in data and model scale for language model pre-training
has led to huge training costs. In scenarios where new data become available
over time, updating a model instead of fully retraining it would therefore
provide significant gains. In this paper, we study the benefits and downsides
of updating a language model when new data comes from new languages - the case
of continual learning under language shift. Starting from a monolingual English
language model, we incrementally add data from Norwegian and Icelandic to
investigate how forward and backward transfer effects depend on the
pre-training order and characteristics of languages, for different model sizes
and learning rate schedulers. Our results show that, while forward transfer is
largely positive and independent of language order, backward transfer can be
either positive or negative depending on the order and characteristics of new
languages. To explain these patterns we explore several language similarity
metrics and find that syntactic similarity appears to have the best correlation
with our results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01256">An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1">Sinan Gultekin</a>, <a href="http://arxiv.org/find/cs/1/au:+Globo_A/0/1/0/all/0/1">Achille Globo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zugarini_A/0/1/0/all/0/1">Andrea Zugarini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ernandes_M/0/1/0/all/0/1">Marco Ernandes</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigutini_L/0/1/0/all/0/1">Leonardo Rigutini</a></p>
<p>Most Machine Learning research evaluates the best solutions in terms of
performance. However, in the race for the best performing model, many important
aspects are often overlooked when, on the contrary, they should be carefully
considered. In fact, sometimes the gaps in performance between different
approaches are neglectable, whereas factors such as production costs, energy
consumption, and carbon footprint must take into consideration. Large Language
Models (LLMs) are extensively adopted to address NLP problems in academia and
industry. In this work, we present a detailed quantitative comparison of LLM
and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes
into account both performance (standard indices) and alternative metrics such
as timing, power consumption and cost, in a word: the carbon-footprint. In our
analysis, we considered the prototyping phase (model selection by
training-validation-test iterations) and in-production phases separately, since
they follow different implementation procedures and also require different
resources. The results indicate that very often, the simplest algorithms
achieve performance very close to that of large LLMs but with very low power
consumption and lower resource demands. The results obtained could suggest
companies to include additional evaluations in the choice of Machine Learning
(ML) solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01270">People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sen_I/0/1/0/all/0/1">Indira Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Assenmacher_D/0/1/0/all/0/1">Dennis Assenmacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Samory_M/0/1/0/all/0/1">Mattia Samory</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1">Wil van der Aalst</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagne_C/0/1/0/all/0/1">Claudia Wagne</a></p>
<p>NLP models are used in a variety of critical social computing tasks, such as
detecting sexist, racist, or otherwise hateful content. Therefore, it is
imperative that these models are robust to spurious features. Past work has
attempted to tackle such spurious features using training data augmentation,
including Counterfactually Augmented Data (CADs). CADs introduce minimal
changes to existing training data points and flip their labels; training on
them may reduce model dependency on spurious features. However, manually
generating CADs can be time-consuming and expensive. Hence in this work, we
assess if this task can be automated using generative NLP models. We
automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate
their usefulness in improving model robustness compared to manually-generated
CADs. By testing both model performance on multiple out-of-domain test sets and
individual data point efficacy, our results show that while manual CADs are
still the most effective, CADs generated by ChatGPT come a close second. One
key reason for the lower performance of automated methods is that the changes
they introduce are often insufficient to flip the original label.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01273">Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations. (arXiv:2311.01273v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Markowska_M/0/1/0/all/0/1">Magdalena Markowska</a>, <a href="http://arxiv.org/find/cs/1/au:+Taghizadeh_M/0/1/0/all/0/1">Mohammad Taghizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Soubki_A/0/1/0/all/0/1">Adil Soubki</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1">Seyed Abolghasem Mirroshandel</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambow_O/0/1/0/all/0/1">Owen Rambow</a></p>
<p>When we communicate with other humans, we do not simply generate a sequence
of words. Rather, we use our cognitive state (beliefs, desires, intentions) and
our model of the audience's cognitive state to create utterances that affect
the audience's cognitive state in the intended manner. An important part of
cognitive state is the common ground, which is the content the speaker
believes, and the speaker believes the audience believes, and so on. While much
attention has been paid to common ground in cognitive science, there has not
been much work in natural language processing. In this paper, we introduce a
new annotation and corpus to capture common ground. We then describe some
initial experiments extracting propositions from dialog and tracking their
status in the common ground from the perspective of each speaker.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01282">FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1">Ke Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guohao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qiuli Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiuhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kangdi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hanyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a></p>
<p>As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and &gt;50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
</p>
<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01305">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haixiao Xu</a></p>
<p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01307">The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models. (arXiv:2311.01307v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1">Lovisa Hagstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Saynova_D/0/1/0/all/0/1">Denitsa Saynova</a>, <a href="http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1">Tobias Norlund</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_M/0/1/0/all/0/1">Moa Johansson</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1">Richard Johansson</a></p>
<p>Large Language Models (LLMs) make natural interfaces to factual knowledge,
but their usefulness is limited by their tendency to deliver inconsistent
answers to semantically equivalent questions. For example, a model might
predict both "Anne Redpath passed away in Edinburgh." and "Anne Redpath's life
ended in London." In this work, we identify potential causes of inconsistency
and evaluate the effectiveness of two mitigation strategies: up-scaling and
augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas
models show that both strategies reduce inconsistency while retrieval
augmentation is considerably more efficient. We further consider and
disentangle the consistency contributions of different components of Atlas. For
all LMs evaluated we find that syntactical form and other evaluation task
artifacts impact consistency. Taken together, our results provide a better
understanding of the factors affecting the factual consistency of language
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01326">Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information. (arXiv:2311.01326v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chepurova_A/0/1/0/all/0/1">Alla Chepurova</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulatov_A/0/1/0/all/0/1">Aydar Bulatov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuratov_Y/0/1/0/all/0/1">Yuri Kuratov</a>, <a href="http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1">Mikhail Burtsev</a></p>
<p>Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which
limits their potential performance. Knowledge Graph Completion (KGC) techniques
aim to address this issue. However, traditional KGC methods are computationally
intensive and impractical for large-scale KGs, necessitating the learning of
dense node embeddings and computing pairwise distances. Generative
transformer-based language models (e.g., T5 and recent KGT5) offer a promising
solution as they can predict the tail nodes directly. In this study, we propose
to include node neighborhoods as additional information to improve KGC methods
based on language models. We examine the effects of this imputation and show
that, on both inductive and transductive Wikidata subsets, our method
outperforms KGT5 and conventional KGC approaches. We also provide an extensive
analysis of the impact of neighborhood on model prediction and show its
importance. Furthermore, we point the way to significantly improve KGC through
more effective neighborhood selection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01361">GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks. (arXiv:2311.01361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinlu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yujie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weizhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1">An Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianke Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Heng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xifeng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1">Linda Ruth Petzold</a></p>
<p>Automatically evaluating vision-language tasks is challenging, especially
when it comes to reflecting human judgments due to limitations in accounting
for fine-grained details. Although GPT-4V has shown promising results in
various multi-modal tasks, leveraging GPT-4V as a generalist evaluator for
these tasks has not yet been systematically explored. We comprehensively
validate GPT-4V's capabilities for evaluation purposes, addressing tasks
ranging from foundational image-to-text and text-to-image synthesis to
high-level image-to-image translations and multi-images to text alignment. We
employ two evaluation methods, single-answer grading and pairwise comparison,
using GPT-4V. Notably, GPT-4V shows promising agreement with humans across
various tasks and evaluation methods, demonstrating immense potential for
multi-modal LLMs as evaluators. Despite limitations like restricted visual
clarity grading and real-world complex reasoning, its ability to provide
human-aligned scores enriched with detailed explanations is promising for
universal automatic evaluator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01386">Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics. (arXiv:2311.01386v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1">Edward Gibson</a>, <a href="http://arxiv.org/find/cs/1/au:+Davis_F/0/1/0/all/0/1">Forrest Davis</a></p>
<p>Language models (LMs) have been argued to overlap substantially with human
beings in grammaticality judgment tasks. But when humans systematically make
errors in language processing, should we expect LMs to behave like cognitive
models of language and mimic human behavior? We answer this question by
investigating LMs' more subtle judgments associated with "language illusions"
-- sentences that are vague in meaning, implausible, or ungrammatical but
receive unexpectedly high acceptability judgments by humans. We looked at three
illusions: the comparative illusion (e.g. "More people have been to Russia than
I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be
ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who
no villager believed to be trustworthy will ever shoot a bear"). We found that
probabilities represented by LMs were more likely to align with human judgments
of being "tricked" by the NPI illusion which examines a structural dependency,
compared to the comparative and the depth-charge illusions which require
sophisticated semantic understanding. No single LM or metric yielded results
that are entirely consistent with human behavior. Ultimately, we show that LMs
are limited both in their construal as cognitive models of human language
processing and in their capacity to recognize nuanced but critical information
in complicated language materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01398">Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants. (arXiv:2311.01398v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Youyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gondala_S/0/1/0/all/0/1">Sashank Gondala</a>, <a href="http://arxiv.org/find/cs/1/au:+Fraga_Silva_T/0/1/0/all/0/1">Thiago Fraga-Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Gysel_C/0/1/0/all/0/1">Christophe Van Gysel</a></p>
<p>On-device Virtual Assistants (VAs) powered by Automatic Speech Recognition
(ASR) require effective knowledge integration for the challenging entity-rich
query recognition. In this paper, we conduct an empirical study of modeling
strategies for server-side rescoring of spoken information domain queries using
various categories of Language Models (LMs) (N-gram word LMs, sub-word neural
LMs). We investigate the combination of on-device and server-side signals, and
demonstrate significant WER improvements of 23%-35% on various entity-centric
query subpopulations by integrating various server-side LMs compared to
performing ASR on-device only. We also perform a comparison between LMs trained
on domain data and a GPT-3 variant offered by OpenAI as a baseline.
Furthermore, we also show that model fusion of multiple server-side LMs trained
from scratch most effectively combines complementary strengths of each model
and integrates knowledge learned from domain-specific data to a VA ASR system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.01964">Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v4 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Kreppel_F/0/1/0/all/0/1">Fabian Kreppel</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Melzer_C/0/1/0/all/0/1">Christian Melzer</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Millan_D/0/1/0/all/0/1">Diego Olvera Mill&#xe1;n</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wagner_J/0/1/0/all/0/1">Janis Wagner</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Hilder_J/0/1/0/all/0/1">Janine Hilder</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Poschinger_U/0/1/0/all/0/1">Ulrich Poschinger</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Schmidt_Kaler_F/0/1/0/all/0/1">Ferdinand Schmidt-Kaler</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Brinkmann_A/0/1/0/all/0/1">Andr&#xe9; Brinkmann</a></p>
<p>The increasing capabilities of quantum computing hardware and the challenge
of realizing deep quantum circuits require fully automated and efficient tools
for compiling quantum circuits. To express arbitrary circuits in a sequence of
native gates specific to the quantum computer architecture, it is necessary to
make algorithms portable across the landscape of quantum hardware providers. In
this work, we present a compiler capable of transforming and optimizing a
quantum circuit targeting a shuttling-based trapped-ion quantum processor. It
consists of custom algorithms set on top of the quantum circuit framework
Pytket. The performance was evaluated for a wide range of quantum circuits and
the results show that the gate counts can be reduced by factors up to 5.1
compared to standard Pytket and up to 2.2 compared to standard Qiskit
compilation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06229">Improving word mover&#x27;s distance by leveraging self-attention matrix. (arXiv:2211.06229v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1">Hiroaki Yamagiwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1">Sho Yokoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1">Hidetoshi Shimodaira</a></p>
<p>Measuring the semantic similarity between two sentences is still an important
task. The word mover's distance (WMD) computes the similarity via the optimal
alignment between the sets of word embeddings. However, WMD does not utilize
word order, making it challenging to distinguish sentences with significant
overlaps of similar words, even if they are semantically very different. Here,
we attempt to improve WMD by incorporating the sentence structure represented
by BERT's self-attention matrix (SAM). The proposed method is based on the
Fused Gromov-Wasserstein distance, which simultaneously considers the
similarity of the word embedding and the SAM for calculating the optimal
transport between two sentences. Experiments demonstrate the proposed method
enhances WMD and its variants in paraphrase identification with near-equivalent
performance in semantic textual similarity. Our code is available at
\url{https://github.com/ymgw55/WSMD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09171">Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data. (arXiv:2212.09171v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Darrin_M/0/1/0/all/0/1">Maxime Darrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1">Pablo Piantanida</a>, <a href="http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1">Pierre Colombo</a></p>
<p>Implementing effective control mechanisms to ensure the proper functioning
and security of deployed NLP models, from translation to chatbots, is
essential. A key ingredient to ensure safe system behaviour is
Out-Of-Distribution (OOD) detection, which aims to detect whether an input
sample is statistically far from the training distribution. Although OOD
detection is a widely covered topic in classification tasks, most methods rely
on hidden features output by the encoder. In this work, we focus on leveraging
soft-probabilities in a black-box framework, i.e. we can access the
soft-predictions but not the internal states of the model. Our contributions
include: (i) RAINPROOF a Relative informAItioN Projection OOD detection
framework; and (ii) a more operational evaluation setting for OOD detection.
Surprisingly, we find that OOD detection is not necessarily aligned with
task-specific measures. The OOD detector may filter out samples well processed
by the model and keep samples that are not, leading to weaker performance. Our
results show that RAINPROOF provides OOD detection methods more aligned with
task-specific performance metrics than traditional OOD detectors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09663">Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oyama_M/0/1/0/all/0/1">Momose Oyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1">Sho Yokoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1">Hidetoshi Shimodaira</a></p>
<p>Distributed representations of words encode lexical semantic information, but
what type of information is encoded and how? Focusing on the skip-gram with
negative-sampling method, we found that the squared norm of static word
embedding encodes the information gain conveyed by the word; the information
gain is defined by the Kullback-Leibler divergence of the co-occurrence
distribution of the word to the unigram distribution. Our findings are
explained by the theoretical framework of the exponential family of probability
distributions and confirmed through precise experiments that remove spurious
correlations arising from word frequency. This theory also extends to
contextualized word embeddings in language models or any neural networks with
the softmax output layer. We also demonstrate that both the KL divergence and
the squared norm of embedding provide a useful metric of the informativeness of
a word in tasks such as keyword extraction, proper-noun discrimination, and
hypernym discrimination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08745">Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine. (arXiv:2301.08745v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenxiang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jen-tse Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a></p>
<p>This report provides a preliminary evaluation of ChatGPT for machine
translation, including translation prompt, multilingual translation, and
translation robustness. We adopt the prompts advised by ChatGPT to trigger its
translation ability and find that the candidate prompts generally work well
with minor performance differences. By evaluating on a number of benchmark test
sets, we find that ChatGPT performs competitively with commercial translation
products (e.g., Google Translate) on high-resource European languages but lags
behind significantly on low-resource or distant languages. As for the
translation robustness, ChatGPT does not perform as well as the commercial
systems on biomedical abstracts or Reddit comments but exhibits good results on
spoken language. Further, we explore an interesting strategy named
$\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to
translate the source sentence into a high-resource pivot language before into
the target language, improving the translation performance noticeably. With the
launch of the GPT-4 engine, the translation performance of ChatGPT is
significantly boosted, becoming comparable to commercial translation products,
even for distant languages. Human analysis on Google Translate and ChatGPT
suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and
mis-translation errors while that with GPT-4 makes the least errors. In other
words, ChatGPT has already become a good translator. Please refer to our Github
project for more details:
https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04391">The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tong Guo</a></p>
<p>In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The dev dataset evaluation
results and human evaluation results verify our idea.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08991">DeltaScore: Fine-Grained Story Evaluation with Perturbations. (arXiv:2303.08991v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhuohan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Miao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1">Trevor Cohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1">Jey Han Lau</a></p>
<p>Numerous evaluation metrics have been developed for natural language
generation tasks, but their effectiveness in evaluating stories is limited as
they are not specifically tailored to assess intricate aspects of storytelling,
such as fluency and interestingness. In this paper, we introduce DELTASCORE, a
novel methodology that employs perturbation techniques for the evaluation of
nuanced story aspects. Our central proposition posits that the extent to which
a story excels in a specific aspect (e.g., fluency) correlates with the
magnitude of its susceptibility to particular perturbations (e.g., the
introduction of typos). Given this, we measure the quality of an aspect by
calculating the likelihood difference between pre- and post-perturbation states
using pre-trained language models. We compare DELTASCORE with existing metrics
on storytelling datasets from two domains in five fine-grained story aspects:
fluency, coherence, relatedness, logicality, and interestingness. DELTASCORE
demonstrates remarkable performance, revealing a surprising finding that a
specific perturbation proves highly effective in capturing multiple aspects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17760">CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Language Model Society. (arXiv:2303.17760v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guohao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1">Hasan Abed Al Kader Hammoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Itani_H/0/1/0/all/0/1">Hani Itani</a>, <a href="http://arxiv.org/find/cs/1/au:+Khizbullin_D/0/1/0/all/0/1">Dmitrii Khizbullin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a></p>
<p>The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their "cognitive" processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: https://github.com/camel-ai/camel.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02426">ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback. (arXiv:2304.02426v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenxiang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jen-tse Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1">Tian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a></p>
<p>Large language models (LLMs) like ChatGPT have exhibited remarkable abilities
on a wide range of natural language processing~(NLP) tasks, including various
machine translation abilities accomplished during chat. However, these models
are only accessible through restricted APIs, which creates barriers to new
research and advancements in the field. Therefore, we propose ParroT, a
framework to enhance and regulate the translation abilities during chat based
on open-source LLMs (e.g., LLaMA), human-written translation and feedback data.
Specifically, ParroT reformulates translation data into the
instruction-following style, and introduces a "$\mathbf{Hint}$" field for
incorporating extra requirements to regulate the translation process.
Accordingly, we propose three instruction types for finetuning ParroT models,
including translation instruction, contrastive instruction, and error-guided
instruction. Experiments on Flores subsets and WMT22 test sets suggest that
translation instruction improves the translation performance of vanilla LLMs
significantly while error-guided instruction can lead to further improvement,
which demonstrates the importance of learning from low-quality translations
annotated by humans. We also demonstrate the potential of automatic evaluation
tools in providing quality information of translations, when constructing
error-guided instructions for directions that lack human annotation data.
Please refer to our Github project for more implementation details:
https://github.com/wxjiao/ParroT
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00586">How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanna_M/0/1/0/all/0/1">Michael Hanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1">Ollie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Variengien_A/0/1/0/all/0/1">Alexandre Variengien</a></p>
<p>Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as "The war lasted from the year 1732 to the year 17", and predict valid
two-digit end years (years &gt; 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01951">Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization. (arXiv:2305.01951v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1">Chi Seng Cheang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1">Hou Pong Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1">Derek F. Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuebo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaocong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yanming Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shudong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1">Lidia S. Chao</a></p>
<p>Recent pre-trained language models (PLMs) achieve promising results in
existing abstractive summarization datasets. However, existing summarization
benchmarks overlap in time with the standard pre-training corpora and
finetuning datasets. Hence, the strong performance of PLMs may rely on the
parametric knowledge that is memorized during pre-training and fine-tuning.
Moreover, the knowledge memorized by PLMs may quickly become outdated, which
affects the generalization performance of PLMs on future data. In this work, we
propose TempoSum, a novel benchmark that contains data samples from 2010 to
2022, to understand the temporal generalization ability of abstractive
summarization models. Through extensive human evaluation, we show that
parametric knowledge stored in summarization models significantly affects the
faithfulness of the generated summaries on future data. Moreover, existing
faithfulness enhancement methods cannot reliably improve the faithfulness of
summarization models on future data. Finally, we discuss several
recommendations to the research community on how to evaluate and improve the
temporal generalization capability of text summarization models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13009">Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1">Michael Hassid</a>, <a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1">Tal Remez</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tu Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1">Itai Gat</a>, <a href="http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1">Alexis Conneau</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1">Felix Kreuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1">Jade Copet</a>, <a href="http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1">Alexandre Defossez</a>, <a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1">Gabriel Synnaeve</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1">Emmanuel Dupoux</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1">Roy Schwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1">Yossi Adi</a></p>
<p>Speech language models (SpeechLMs) process and generate acoustic data only,
without textual supervision. In this work, we propose TWIST, a method for
training SpeechLMs using a warm-start from a pretrained textual language
models. We show using both automatic and human evaluations that TWIST
outperforms a cold-start SpeechLM across the board. We empirically analyze the
effect of different model design choices such as the speech tokenizer, the
pretrained textual model, and the dataset size. We find that model and dataset
scale both play an important role in constructing better-performing SpeechLMs.
Based on our observations, we present the largest (to the best of our
knowledge) SpeechLM both in terms of number of parameters and training data. We
additionally introduce two spoken versions of the StoryCloze textual benchmark
to further improve model evaluation and advance future research in the field.
We make speech samples, code and models publicly available:
https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13175">Discovering Universal Geometry in Embeddings with ICA. (arXiv:2305.13175v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1">Hiroaki Yamagiwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyama_M/0/1/0/all/0/1">Momose Oyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1">Hidetoshi Shimodaira</a></p>
<p>This study utilizes Independent Component Analysis (ICA) to unveil a
consistent semantic structure within embeddings of words or images. Our
approach extracts independent semantic components from the embeddings of a
pre-trained model by leveraging anisotropic information that remains after the
whitening process in Principal Component Analysis (PCA). We demonstrate that
each embedding can be expressed as a composition of a few intrinsic
interpretable axes and that these semantic axes remain consistent across
different languages, algorithms, and modalities. The discovery of a universal
semantic structure in the geometric patterns of embeddings enhances our
understanding of the representations in embeddings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13194">SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation. (arXiv:2305.13194v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1">Elizabeth Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1">Shruti Rijhwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1">Sebastian Gehrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1">Joshua Maynez</a>, <a href="http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1">Roee Aharoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1">Vitaly Nikolaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1">Thibault Sellam</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1">Aditya Siddhant</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Dipanjan Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1">Ankur P. Parikh</a></p>
<p>Reliable automatic evaluation of summarization systems is challenging due to
the multifaceted and subjective nature of the task. This is especially the case
for languages other than English, where human evaluations are scarce. In this
work, we introduce SEAHORSE, a dataset for multilingual, multifaceted
summarization evaluation. SEAHORSE consists of 96K summaries with human ratings
along 6 dimensions of text quality: comprehensibility, repetition, grammar,
attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4
datasets. As a result of its size and scope, SEAHORSE can serve both as a
benchmark to evaluate learnt metrics, as well as a large-scale resource for
training such metrics. We show that metrics trained with SEAHORSE achieve
strong performance on the out-of-domain meta-evaluation benchmarks TRUE
(Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE
dataset and metrics publicly available for future research on multilingual and
multifaceted summarization evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13533">Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting. (arXiv:2305.13533v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1">William Hogan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiacheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a></p>
<p>Open-world Relation Extraction (OpenRE) has recently garnered significant
attention. However, existing approaches tend to oversimplify the problem by
assuming that all unlabeled texts belong to novel classes, thereby limiting the
practicality of these methods. We argue that the OpenRE setting should be more
aligned with the characteristics of real-world data. Specifically, we propose
two key improvements: (a) unlabeled data should encompass known and novel
classes, including hard-negative instances; and (b) the set of novel classes
should represent long-tail relation types. Furthermore, we observe that popular
relations such as titles and locations can often be implicitly inferred through
specific patterns, while long-tail relations tend to be explicitly expressed in
sentences. Motivated by these insights, we present a novel method called KNoRD
(Known and Novel Relation Discovery), which effectively classifies explicitly
and implicitly expressed relations from known and novel classes within
unlabeled data. Experimental evaluations on several Open-world RE benchmarks
demonstrate that KNoRD consistently outperforms other existing methods,
achieving significant performance gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13677">Towards Legally Enforceable Hate Speech Detection for Public Forums. (arXiv:2305.13677v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chu Fei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1">Rohan Bhambhoria</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaodan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahan_S/0/1/0/all/0/1">Samuel Dahan</a></p>
<p>Hate speech causes widespread and deep-seated societal issues. Proper
enforcement of hate speech laws is key for protecting groups of people against
harmful and discriminatory language. However, determining what constitutes hate
speech is a complex task that is highly open to subjective interpretations.
Existing works do not align their systems with enforceable definitions of hate
speech, which can make their outputs inconsistent with the goals of regulators.
This research introduces a new perspective and task for enforceable hate speech
detection centred around legal definitions, and a dataset annotated on
violations of eleven possible definitions by legal experts. Given the challenge
of identifying clear, legally enforceable instances of hate speech, we augment
the dataset with expert-generated samples and an automatically mined challenge
set. We experiment with grounding the model decision in these definitions using
zero-shot and few-shot prompting. We then report results on several large
language models (LLMs). With this task definition, automatic hate speech
detection can be more closely aligned to enforceable laws, and hence assist in
more rigorous enforcement of legal protections against harmful speech in public
forums.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14999">The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models. (arXiv:2305.14999v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jingyuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Ying Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minqian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1">Di Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a></p>
<p>Chain-of-Thought (CoT) prompting enables large language models to solve
complex reasoning problems by generating intermediate steps. However, confined
by its inherent single-pass and sequential generation process, CoT heavily
relies on the initial decisions, causing errors in early steps to accumulate
and impact the final answers. In contrast, humans adopt recursive thinking when
tackling complex reasoning problems, i.e., iteratively breaking the original
problem into approachable sub-problems and aggregating their answers to resolve
the original one. Inspired by the human cognitive process, we propose SOCRATIC
QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive
thinking process. Specifically, SOCRATIC QUESTIONING leverages large language
models to raise and answer sub-questions until collecting enough information to
tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly
navigates the thinking space, stimulates effective recursive thinking, and is
more robust towards errors in the thinking process. Extensive experiments on
several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual
question-answering demonstrate significant performance improvements over the
state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The
qualitative analysis clearly shows that the intermediate reasoning steps
elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking
process of complex reasoning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17020">Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lesci_P/0/1/0/all/0/1">Pietro Lesci</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujinuma_Y/0/1/0/all/0/1">Yoshinari Fujinuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1">Momchil Hardalov</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1">Chao Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Benajiba_Y/0/1/0/all/0/1">Yassine Benajiba</a>, <a href="http://arxiv.org/find/cs/1/au:+Marquez_L/0/1/0/all/0/1">Lluis Marquez</a></p>
<p>Sequence-to-sequence state-of-the-art systems for dialogue state tracking
(DST) use the full dialogue history as input, represent the current state as a
list with all the slots, and generate the entire state from scratch at each
dialogue turn. This approach is inefficient, especially when the number of
slots is large and the conversation is long. We propose Diable, a new task
formalisation that simplifies the design and implementation of efficient DST
systems and allows one to easily plug and play large language models. We
represent the dialogue state as a table and formalise DST as a table
manipulation task. At each turn, the system updates the previous state by
generating table operations based on the dialogue context. Extensive
experimentation on the MultiWoz datasets demonstrates that Diable (i)
outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient
than current state-of-the-art methods while retaining competitive Joint Goal
Accuracy, and (iii) is robust to noisy data annotations due to the table
operations approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08129">AVIS: Autonomous Visual Information Seeking with Large Language Model Agent. (arXiv:2306.08129v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Ziniu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1">Ahmet Iscen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yizhou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1">David A Ross</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Fathi_A/0/1/0/all/0/1">Alireza Fathi</a></p>
<p>In this paper, we propose an autonomous information seeking visual question
answering framework, AVIS. Our method leverages a Large Language Model (LLM) to
dynamically strategize the utilization of external tools and to investigate
their outputs, thereby acquiring the indispensable knowledge needed to provide
answers to the posed questions. Responding to visual questions that necessitate
external knowledge, such as "What event is commemorated by the building
depicted in this image?", is a complex task. This task presents a combinatorial
search space that demands a sequence of actions, including invoking APIs,
analyzing their responses, and making informed decisions. We conduct a user
study to collect a variety of instances of human decision-making when faced
with this task. This data is then used to design a system comprised of three
components: an LLM-powered planner that dynamically determines which tool to
use next, an LLM-powered reasoner that analyzes and extracts key information
from the tool outputs, and a working memory component that retains the acquired
information throughout the process. The collected user behavior serves as a
guide for our system in two key ways. First, we create a transition graph by
analyzing the sequence of decisions made by users. This graph delineates
distinct states and confines the set of actions available at each state.
Second, we use examples of user decision-making to provide our LLM-powered
planner and reasoner with relevant contextual instances, enhancing their
capacity to make informed decisions. We show that AVIS achieves
state-of-the-art results on knowledge-intensive visual question answering
benchmarks such as Infoseek and OK-VQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12317">Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shamsi_D/0/1/0/all/0/1">Davood Shamsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wen-yu Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1">Brian Williams</a></p>
<p>In this work, we demonstrate the application of a first-order Taylor
expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times
m}$ and utilize it in language modeling. To enhance the basic Taylor expansion,
we introduce iteration and piecewise modeling, leading us to name the algorithm
the Iterative Piecewise Affine (IPA) approximation. The final algorithm
exhibits interesting resemblances to the Transformers decoder architecture. By
comparing parameter arrangements in IPA and Transformers, we observe a
strikingly similar performance, with IPA outperforming Transformers by 1.5\% in
the next token prediction task with cross-entropy loss for smaller sequence
lengths.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17820">Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuosheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a></p>
<p>Neural-symbolic methods have shown their effectiveness in enhancing the
reasoning abilities of large language models (LLMs). However, existing methods
primarily rely on mapping natural languages to more syntactically complete
formal languages (e.g., Python and SQL). Those approaches necessitate that
reasoning tasks be convertible into programs, which cater more to the computer
execution mindset and deviate from human reasoning habits. To expand the
real-world applicability and flexibility of symbolic methods, we propose
Meta-Reasoning from the scope of linguistics itself. This method empowers LLMs
to deconstruct questions and effectively capture more generalized knowledge
autonomously. We find that Meta-Reasoning achieves improved in-context learning
efficiency, reasoning accuracy, and output stability in six arithmetic and
symbolic reasoning tasks. In particular, when applied to symbolic reasoning
tasks such as Tracking Shuffled Objects, GPT-3 (text-davinci-002) surpasses the
few-shot Chain-of-Thought prompting approach (+37.7%), with 99% accuracy after
a single demonstration of Meta-Reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02028">EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wornow_M/0/1/0/all/0/1">Michael Wornow</a>, <a href="http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1">Rahul Thapa</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1">Ethan Steinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1">Jason A. Fries</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Nigam H. Shah</a></p>
<p>While the general machine learning (ML) community has benefited from public
datasets, tasks, and models, the progress of ML in healthcare has been hampered
by a lack of such shared assets. The success of foundation models creates new
challenges for healthcare ML by requiring access to shared pretrained models to
validate performance benefits. We help address these challenges through three
contributions. First, we publish a new dataset, EHRSHOT, which contains
deidentified structured data from the electronic health records (EHRs) of 6,739
patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR
datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.
Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical
foundation model pretrained on the structured EHR data of 2.57M patients. We
are one of the first to fully release such a model for coded EHR data; in
contrast, most prior models released for clinical data (e.g. GatorTron,
ClinicalBERT) only work with unstructured text and cannot process the rich,
structured data within an EHR. We provide an end-to-end pipeline for the
community to validate and build upon its performance. Third, we define 15
few-shot clinical prediction tasks, enabling evaluation of foundation models on
benefits such as sample efficiency and task adaptation. Our model and dataset
are available via a research data use agreement from the Stanford AIMI Center.
Code to reproduce our results are available at our Github repo:
https://github.com/som-shahlab/ehrshot-benchmark
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02729">Text Alignment Is An Efficient Unified Model for Massive NLP Tasks. (arXiv:2307.02729v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1">Yuheng Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yichi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruichen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiting Hu</a></p>
<p>Large language models (LLMs), typically designed as a function of next-word
prediction, have excelled across extensive NLP tasks. Despite the generality,
next-word prediction is often not an efficient formulation for many of the
tasks, demanding an extreme scale of model parameters (10s or 100s of billions)
and sometimes yielding suboptimal performance. In practice, it is often
desirable to build more efficient models -- despite being less versatile, they
still apply to a substantial subset of problems, delivering on par or even
superior performance with much smaller model sizes. In this paper, we propose
text alignment as an efficient unified model for a wide range of crucial tasks
involving text entailment, similarity, question answering (and answerability),
factual consistency, and so forth. Given a pair of texts, the model measures
the degree of alignment between their information. We instantiate an alignment
model (Align) through lightweight finetuning of RoBERTa (355M parameters) using
5.9M examples from 28 datasets. Despite its compact size, extensive experiments
show the model's efficiency and strong performance: (1) On over 20 datasets of
aforementioned diverse tasks, the model matches or surpasses FLAN-T5 models
that have around 2x or 10x more parameters; the single unified model also
outperforms task-specific models finetuned on individual datasets; (2) When
applied to evaluate factual consistency of language generation on 23 datasets,
our model improves over various baselines, including the much larger GPT-3.5
(ChatGPT) and sometimes even GPT-4; (3) The lightweight model can also serve as
an add-on component for LLMs such as GPT-3.5 in question answering tasks,
improving the average exact match (EM) score by 17.94 and F1 score by 15.05
through identifying unanswerable questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05973">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenlong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunzhu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1">Li Fei-Fei</a></p>
<p>Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a vision-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Videos and code at https://voxposer.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06435">A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1">Humza Naveed</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Asad Ullah Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Shi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1">Muhammad Saqib</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1">Saeed Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1">Muhammad Usman</a>, <a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1">Naveed Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1">Nick Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1">Ajmal Mian</a></p>
<p>Large Language Models (LLMs) have recently demonstrated remarkable
capabilities in natural language processing tasks and beyond. This success of
LLMs has led to a large influx of research contributions in this direction.
These works encompass diverse topics such as architectural innovations of the
underlying neural networks, context length improvements, model alignment,
training datasets, benchmarking, efficiency and more. With the rapid
development of techniques and regular breakthroughs in LLM research, it has
become considerably challenging to perceive the bigger picture of the advances
in this direction. Considering the rapidly emerging plethora of literature on
LLMs, it is imperative that the research community is able to benefit from a
concise yet comprehensive overview of the recent developments in this field.
This article provides that overview to the research community. It not only
focuses on a systematic treatment of the existing literature on a broad range
of LLM related concept, but also pays special attention to providing
comprehensive summaries with extensive details about the individual existing
models, datasets and major insights. We also pay heed to aligning our overview
with the emerging outlook of this research direction by accounting for the
other recently materializing reviews of the broader research direction of LLMs.
Our self-contained comprehensive overview of LLMs discusses relevant background
concepts along with covering the advanced topics at the frontier of this
research direction. This review article is intended to not only provide a
systematic survey, but also a quick comprehensive reference for the researchers
and practitioners to draw insights from extensive informative summaries of the
existing works to advance the LLM research direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01456">LLM and Infrastructure as a Code use case. (arXiv:2309.01456v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chanus_T/0/1/0/all/0/1">Thibault Chanus</a> (ENS Rennes), <a href="http://arxiv.org/find/cs/1/au:+Aubertin_M/0/1/0/all/0/1">Michael Aubertin</a></p>
<p>Cloud computing and the evolution of management methodologies such as Lean
Management or Agile entail a profound transformation in both system
construction and maintenance approaches. These practices are encompassed within
the term "DevOps." This descriptive approach to an information system or
application, alongside the configuration of its constituent components, has
necessitated the development of descriptive languages paired with specialized
engines for automating systems administration tasks. Among these, the tandem of
Ansible (engine) and YAML (descriptive language) stands out as the two most
prevalent tools in the market, facing notable competition mainly from
Terraform. The current document presents an inquiry into a solution for
generating and managing Ansible YAML roles and playbooks, utilizing Generative
LLMs (Language Models) to translate human descriptions into code. Our efforts
are focused on identifying plausible directions and outlining the potential
industrial applications. Note: For the purpose of this experiment, we have
opted against the use of Ansible Lightspeed. This is due to its reliance on an
IBM Watson model, for which we have not found any publicly available
references. Comprehensive information regarding this remarkable technology can
be found [1] directly on our partner's website, RedHat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04691">EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Siyu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kenny Q. Zhu</a></p>
<p>Neural language models are probabilistic models of human text. They are
predominantly trained using maximum likelihood estimation (MLE), which is
equivalent to minimizing the forward cross-entropy between the empirical data
distribution and the model distribution. However, various degeneration
phenomena are still widely observed when decoding from the distributions
learned by such models. We establish that the forward cross-entropy is
suboptimal as a distance metric for aligning human and model distribution due
to its (1) recall-prioritization (2) negative diversity ignorance and (3)
train-test mismatch. In this paper, we propose Earth Mover Distance
Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on
the inherent properties of earth mover distance to address the aforementioned
challenges. Due to the high complexity of direct computation, we further
introduce a feasible upper bound for EMO to ease end-to-end training. Upon
extensive evaluation of language models trained using EMO and MLE. We find that
EMO demonstrates a consistently better language modeling performance than MLE
across domains. Moreover, EMO demonstrates noteworthy enhancements in
downstream performance with minimal fine-tuning on merely 25,000 sentences.
This highlights the tremendous potential of EMO as a lightweight calibration
method for enhancing large-scale pre-trained language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11778">Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qichao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_T/0/1/0/all/0/1">Tian Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yian Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tingyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Helen M. Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bingzhe Wu</a></p>
<p>The recent surge in the research of diffusion models has accelerated the
adoption of text-to-image models in various Artificial Intelligence Generated
Content (AIGC) commercial products. While these exceptional AIGC products are
gaining increasing recognition and sparking enthusiasm among consumers, the
questions regarding whether, when, and how these models might unintentionally
reinforce existing societal stereotypes remain largely unaddressed. Motivated
by recent advancements in language agents, here we introduce a novel agent
architecture tailored for stereotype detection in text-to-image models. This
versatile agent architecture is capable of accommodating free-form detection
tasks and can autonomously invoke various tools to facilitate the entire
process, from generating corresponding instructions and images, to detecting
stereotypes. We build the stereotype-relevant benchmark based on multiple
open-text datasets, and apply this architecture to commercial products and
popular open source text-to-image models. We find that these models often
display serious stereotypes when it comes to certain prompts about personal
characteristics, social cultural context and crime-related aspects. In summary,
these empirical findings underscore the pervasive existence of stereotypes
across social dimensions, including gender, race, and religion, which not only
validate the effectiveness of our proposed approach, but also emphasize the
critical necessity of addressing potential ethical risks in the burgeoning
realm of AIGC. As AIGC continues its rapid expansion trajectory, with new
models and plugins emerging daily in staggering numbers, the challenge lies in
the timely detection and mitigation of potential biases within these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14520">QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing. (arXiv:2310.14520v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yating Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangla_R/0/1/0/all/0/1">Ritika Mangla</a>, <a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1">Greg Durrett</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Jessy Li</a></p>
<p>Questions Under Discussion (QUD) is a versatile linguistic framework in which
discourse progresses as continuously asking questions and answering them.
Automatic parsing of a discourse to produce a QUD structure thus entails a
complex question generation task: given a document and an answer sentence,
generate a question that satisfies linguistic constraints of QUD and can be
grounded in an anchor sentence in prior context. These questions are known to
be curiosity-driven and open-ended. This work introduces the first framework
for the automatic evaluation of QUD parsing, instantiating the theoretical
constraints of QUD in a concrete protocol. We present QUDeval, a dataset of
fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned
systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD
is still challenging for modern LLMs, and that existing evaluation metrics
poorly approximate parser quality. Encouragingly, human-authored QUDs are
scored highly by our human evaluators, suggesting that there is headroom for
further progress on language modeling to improve both QUD parsing and QUD
evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14892">Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation. (arXiv:2310.14892v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Tianqi Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Quan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jingxuan Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongdong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhendong Mao</a></p>
<p>Controllable text generation (CTG) aims to generate text with desired
attributes, and decoding-time-based methods have shown promising performance on
this task. However, in this paper, we identify the phenomenon of Attribute
Collapse for the first time. It causes the fluency of generated text to rapidly
decrease when the control strength exceeds a critical value, rendering the text
completely unusable. This limitation hinders the effectiveness of decoding
methods in achieving high levels of controllability. To address this problem,
we propose a novel lightweight decoding framework named Air-Decoding. Its main
idea is reconstructing the attribute distributions to balance the weights
between attribute words and non-attribute words to generate more fluent text.
Specifically, we train prefixes by prefix-tuning to obtain attribute
distributions. Then we design a novel attribute distribution reconstruction
method to balance the obtained distributions and use the reconstructed
distributions to guide language models for generation, effectively avoiding the
issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our
method achieves a new state-of-the-art control performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17490">Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sukmin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Jeongyeon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Soyeong Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jong C. Park</a></p>
<p>Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18348">Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tian Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1">Alessandro Achille</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1">Pramuditha Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1">Luca Zancato</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00286">JADE: A Linguistics-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xudong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a></p>
<p>In this paper, we present JADE, a targeted linguistic fuzzing platform which
strengthens the linguistic complexity of seed questions to simultaneously and
consistently break a wide range of widely-used LLMs categorized in three
groups: eight open-sourced Chinese, six commercial Chinese and four commercial
English LLMs. JADE generates three safety benchmarks for the three groups of
LLMs, which contain unsafe questions that are highly threatening: the questions
simultaneously trigger harmful generation of multiple LLMs, with an average
unsafe generation ratio of $70\%$ (please see the table below), while are still
natural questions, fluent and preserving the core unsafe semantics. We release
the benchmark demos generated for commercial English LLMs and open-sourced
English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For
readers who are interested in evaluating on more questions generated by JADE,
please contact us.
</p>
<p>JADE is based on Noam Chomsky's seminal theory of transformational-generative
grammar. Given a seed question with unsafe intention, JADE invokes a sequence
of generative and transformational rules to increment the complexity of the
syntactic structure of the original question, until the safety guardrail is
broken. Our key insight is: Due to the complexity of human language, most of
the current best LLMs can hardly recognize the invariant evil from the infinite
number of different syntactic structures which form an unbound example space
that can never be fully covered. Technically, the generative/transformative
rules are constructed by native speakers of the languages, and, once developed,
can be used to automatically grow and transform the parse tree of a given
question, until the guardrail is broken. For more evaluation results and demo,
please check our website: https://whitzard-ai.github.io/jade.html.
</p>
</p>
</div>

    </div>
    </body>
    