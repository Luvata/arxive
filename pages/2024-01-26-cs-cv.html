<!DOCTYPE html>
<html>
<head>
<title>2024-01-26-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.13011">CCA: Collaborative Competitive Agents for Image Editing. (arXiv:2401.13011v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hang_T/0/1/0/all/0/1">Tiankai Hang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shuyang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1">Baining Guo</a></p>
<p>This paper presents a novel generative model, Collaborative Competitive
Agents (CCA), which leverages the capabilities of multiple Large Language
Models (LLMs) based agents to execute complex tasks. Drawing inspiration from
Generative Adversarial Networks (GANs), the CCA system employs two equal-status
generator agents and a discriminator agent. The generators independently
process user instructions and generate results, while the discriminator
evaluates the outputs, and provides feedback for the generator agents to
further reflect and improve the generation results. Unlike the previous
generative model, our system can obtain the intermediate steps of generation.
This allows each generator agent to learn from other successful executions due
to its transparency, enabling a collaborative competition that enhances the
quality and robustness of the system's results. The primary focus of this study
is image editing, demonstrating the CCA's ability to handle intricate
instructions robustly. The paper's main contributions include the introduction
of a multi-agent-based generative model with controllable intermediate steps
and iterative optimization, a detailed examination of agent relationships, and
comprehensive experiments on image editing. Code is available at
\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13049">CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention. (arXiv:2401.13049v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Imran_M/0/1/0/all/0/1">Muhammad Imran</a>, <a href="http://arxiv.org/find/eess/1/au:+Krebs_J/0/1/0/all/0/1">Jonathan R Krebs</a>, <a href="http://arxiv.org/find/eess/1/au:+Gopu_V/0/1/0/all/0/1">Veera Rajasekhar Reddy Gopu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fazzone_B/0/1/0/all/0/1">Brian Fazzone</a>, <a href="http://arxiv.org/find/eess/1/au:+Sivaraman_V/0/1/0/all/0/1">Vishal Balaji Sivaraman</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1">Amarjeet Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Viscardi_C/0/1/0/all/0/1">Chelsea Viscardi</a>, <a href="http://arxiv.org/find/eess/1/au:+Heithaus_R/0/1/0/all/0/1">Robert Evans Heithaus</a>, <a href="http://arxiv.org/find/eess/1/au:+Shickel_B/0/1/0/all/0/1">Benjamin Shickel</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyin Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Cooper_M/0/1/0/all/0/1">Michol A Cooper</a>, <a href="http://arxiv.org/find/eess/1/au:+Shao_W/0/1/0/all/0/1">Wei Shao</a></p>
<p>Advancements in medical imaging and endovascular grafting have facilitated
minimally invasive treatments for aortic diseases. Accurate 3D segmentation of
the aorta and its branches is crucial for interventions, as inaccurate
segmentation can lead to erroneous surgical planning and endograft
construction. Previous methods simplified aortic segmentation as a binary image
segmentation problem, overlooking the necessity of distinguishing between
individual aortic branches. In this paper, we introduce Context Infused
Swin-UNet (CIS-UNet), a deep learning model designed for multi-class
segmentation of the aorta and thirteen aortic branches. Combining the strengths
of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts
a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric
decoder, skip connections, and a novel Context-aware Shifted Window
Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a
unique utilization of the patch merging layer, distinct from conventional Swin
transformers. It efficiently condenses the feature map, providing a global
spatial context and enhancing performance when applied at the bottleneck layer,
offering superior computational efficiency and segmentation accuracy compared
to the Swin transformers. We trained our model on computed tomography (CT)
scans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the
state-of-the-art SwinUNetR segmentation model, which is solely based on Swin
transformers, by achieving a superior mean Dice coefficient of 0.713 compared
to 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm.
CIS-UNet's superior 3D aortic segmentation offers improved precision and
optimization for planning endovascular treatments. Our dataset and code will be
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13051">PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation. (arXiv:2401.13051v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhaozhi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1">Bochen Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Weihao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1">Muyang Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yue Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongtao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>The Segment Anything Model (SAM) has exhibited outstanding performance in
various image segmentation tasks. Despite being trained with over a billion
masks, SAM faces challenges in mask prediction quality in numerous scenarios,
especially in real-world contexts. In this paper, we introduce a novel
prompt-driven adapter into SAM, namely Prompt Adapter Segment Anything Model
(PA-SAM), aiming to enhance the segmentation mask quality of the original SAM.
By exclusively training the prompt adapter, PA-SAM extracts detailed
information from images and optimizes the mask decoder feature at both sparse
and dense prompt levels, improving the segmentation performance of SAM to
produce high-quality masks. Experimental results demonstrate that our PA-SAM
outperforms other SAM-based methods in high-quality, zero-shot, and open-set
segmentation. We're making the source code and models available at
https://github.com/xzz2/pa-sam.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13068">Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images. (arXiv:2401.13068v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jarman_S/0/1/0/all/0/1">Scout Jarman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hampel_Arias_Z/0/1/0/all/0/1">Zigfried Hampel-Arias</a>, <a href="http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1">Adra Carr</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_K/0/1/0/all/0/1">Kevin R. Moon</a></p>
<p>Deep learning identification models have shown promise for identifying gas
plumes in Longwave IR hyperspectral images of urban scenes, particularly when a
large library of gases are being considered. Because many gases have similar
spectral signatures, it is important to properly estimate the signal from a
detected plume. Typically, a scene's global mean spectrum and covariance matrix
are estimated to whiten the plume's signal, which removes the background's
signature from the gas signature. However, urban scenes can have many different
background materials that are spatially and spectrally heterogeneous. This can
lead to poor identification performance when the global background estimate is
not representative of a given local background material. We use image
segmentation, along with an iterative background estimation algorithm, to
create local estimates for the various background materials that reside
underneath a gas plume. Our method outperforms global background estimation on
a set of simulated and real gas plumes. This method shows promise in increasing
deep learning identification confidence, while being simple and easy to tune
when considering diverse plumes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13076">SemanticSLAM: Learning based Semantic Map Construction and Robust Camera Localization. (arXiv:2401.13076v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yue Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1">Qinru Qiu</a></p>
<p>Current techniques in Visual Simultaneous Localization and Mapping (VSLAM)
estimate camera displacement by comparing image features of consecutive scenes.
These algorithms depend on scene continuity, hence requires frequent camera
inputs. However, processing images frequently can lead to significant memory
usage and computation overhead. In this study, we introduce SemanticSLAM, an
end-to-end visual-inertial odometry system that utilizes semantic features
extracted from an RGB-D sensor. This approach enables the creation of a
semantic map of the environment and ensures reliable camera localization.
SemanticSLAM is scene-agnostic, which means it doesn't require retraining for
different environments. It operates effectively in indoor settings, even with
infrequent camera input, without prior knowledge. The strength of SemanticSLAM
lies in its ability to gradually refine the semantic map and improve pose
estimation. This is achieved by a convolutional long-short-term-memory
(ConvLSTM) network, trained to correct errors during map construction. Compared
to existing VSLAM algorithms, SemanticSLAM improves pose estimation by 17%. The
resulting semantic map provides interpretable information about the environment
and can be easily applied to various downstream tasks, such as path planning,
obstacle avoidance, and robot navigation. The code will be publicly available
at https://github.com/Leomingyangli/SemanticSLAM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13081">Free Form Medical Visual Question Answering in Radiology. (arXiv:2401.13081v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1">Abhishek Narayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Musthyala_R/0/1/0/all/0/1">Rushabh Musthyala</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_R/0/1/0/all/0/1">Rahul Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nistala_A/0/1/0/all/0/1">Anirudh Prasad Nistala</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pranav Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1">Jacopo Cirrone</a></p>
<p>Visual Question Answering (VQA) in the medical domain presents a unique,
interdisciplinary challenge, combining fields such as Computer Vision, Natural
Language Processing, and Knowledge Representation. Despite its importance,
research in medical VQA has been scant, only gaining momentum since 2018.
Addressing this gap, our research delves into the effective representation of
radiology images and the joint learning of multimodal representations,
surpassing existing methods. We innovatively augment the SLAKE dataset,
enabling our model to respond to a more diverse array of questions, not limited
to the immediate content of radiology or pathology images. Our model achieves a
top-1 accuracy of 79.55\% with a less complex architecture, demonstrating
comparable performance to current state-of-the-art models. This research not
only advances medical VQA but also opens avenues for practical applications in
diagnostic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13082">PlaceFormer: Transformer-based Visual Place Recognition using Multi-Scale Patch Selection and Fusion. (arXiv:2401.13082v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kannan_S/0/1/0/all/0/1">Shyam Sundar Kannan</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1">Byung-Cheol Min</a></p>
<p>Visual place recognition is a challenging task in the field of computer
vision, and autonomous robotics and vehicles, which aims to identify a location
or a place from visual inputs. Contemporary methods in visual place recognition
employ convolutional neural networks and utilize every region within the image
for the place recognition task. However, the presence of dynamic and
distracting elements in the image may impact the effectiveness of the place
recognition process. Therefore, it is meaningful to focus on task-relevant
regions of the image for improved recognition. In this paper, we present
PlaceFormer, a novel transformer-based approach for visual place recognition.
PlaceFormer employs patch tokens from the transformer to create global image
descriptors, which are then used for image retrieval. To re-rank the retrieved
images, PlaceFormer merges the patch tokens from the transformer to form
multi-scale patches. Utilizing the transformer's self-attention mechanism, it
selects patches that correspond to task-relevant areas in an image. These
selected patches undergo geometric verification, generating similarity scores
across different patch sizes. Subsequently, spatial scores from each patch size
are fused to produce a final similarity score. This score is then used to
re-rank the images initially retrieved using global image descriptors.
Extensive experiments on benchmark datasets demonstrate that PlaceFormer
outperforms several state-of-the-art methods in terms of accuracy and
computational efficiency, requiring less time and memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13087">Open-source data pipeline for street-view images: a case study on community mobility during COVID-19 pandemic. (arXiv:2401.13087v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martell_M/0/1/0/all/0/1">Matthew Martell</a>, <a href="http://arxiv.org/find/cs/1/au:+Terry_N/0/1/0/all/0/1">Nick Terry</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_R/0/1/0/all/0/1">Ribhu Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Salazar_C/0/1/0/all/0/1">Chris Salazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Errett_N/0/1/0/all/0/1">Nicole A. Errett</a>, <a href="http://arxiv.org/find/cs/1/au:+Miles_S/0/1/0/all/0/1">Scott B. Miles</a>, <a href="http://arxiv.org/find/cs/1/au:+Wartman_J/0/1/0/all/0/1">Joseph Wartman</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1">Youngjun Choe</a></p>
<p>Street View Images (SVI) are a common source of valuable data for
researchers. Researchers have used SVI data for estimating pedestrian volumes,
demographic surveillance, and to better understand built and natural
environments in cityscapes. However, the most common source of publicly
available SVI data is Google Street View. Google Street View images are
collected infrequently, making temporal analysis challenging, especially in low
population density areas. Our main contribution is the development of an
open-source data pipeline for processing 360-degree video recorded from a
car-mounted camera. The video data is used to generate SVIs, which then can be
used as an input for temporal analysis. We demonstrate the use of the pipeline
by collecting a SVI dataset over a 38-month longitudinal survey of Seattle, WA,
USA during the COVID-19 pandemic. The output of our pipeline is validated
through statistical analyses of pedestrian traffic in the images. We confirm
known results in the literature and provide new insights into outdoor
pedestrian traffic patterns. This study demonstrates the feasibility and value
of collecting and using SVI for research purposes beyond what is possible with
currently available SVI data. Limitations and future improvements on the data
pipeline and case study are also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13097">Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems. (arXiv:2401.13097v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Greene_M/0/1/0/all/0/1">Michelle R. Greene</a>, <a href="http://arxiv.org/find/cs/1/au:+Josyula_M/0/1/0/all/0/1">Mariam Josyula</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_W/0/1/0/all/0/1">Wentao Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Hart_J/0/1/0/all/0/1">Jennifer A. Hart</a></p>
<p>Computer-based scene understanding has influenced fields ranging from urban
planning to autonomous vehicle performance, yet little is known about how well
these technologies work across social differences. We investigate the biases of
deep convolutional neural networks (dCNNs) in scene classification, using
nearly one million images from global and US sources, including user-submitted
home photographs and Airbnb listings. We applied statistical models to quantify
the impact of socioeconomic indicators such as family income, Human Development
Index (HDI), and demographic factors from public data sources (CIA and US
Census) on dCNN performance. Our analyses revealed significant socioeconomic
bias, where pretrained dCNNs demonstrated lower classification accuracy, lower
classification confidence, and a higher tendency to assign labels that could be
offensive when applied to homes (e.g., "ruin", "slum"), especially in images
from homes with lower socioeconomic status (SES). This trend is consistent
across two datasets of international images and within the diverse economic and
racial landscapes of the United States. This research contributes to
understanding biases in computer vision, emphasizing the need for more
inclusive and representative training datasets. By mitigating the bias in the
computer vision pipelines, we can ensure fairer and more equitable outcomes for
applied computer vision, including home valuation and smart home security
systems. There is urgency in addressing these biases, which can significantly
impact critical decisions in urban development and resource allocation. Our
findings also motivate the development of AI systems that better understand and
serve diverse communities, moving towards technology that equitably benefits
all sectors of society.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13140">Dual-Domain Coarse-to-Fine Progressive Estimation Network for Simultaneous Denoising, Limited-View Reconstruction, and Attenuation Correction of Cardiac SPECT. (arXiv:2401.13140v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiongchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xueqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1">Huidong Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qiong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a>, <a href="http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1">Albert J.Sinusas</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a></p>
<p>Single-Photon Emission Computed Tomography (SPECT) is widely applied for the
diagnosis of coronary artery diseases. Low-dose (LD) SPECT aims to minimize
radiation exposure but leads to increased image noise. Limited-view (LV) SPECT,
such as the latest GE MyoSPECT ES system, enables accelerated scanning and
reduces hardware expenses but degrades reconstruction accuracy. Additionally,
Computed Tomography (CT) is commonly used to derive attenuation maps
($\mu$-maps) for attenuation correction (AC) of cardiac SPECT, but it will
introduce additional radiation exposure and SPECT-CT misalignments. Although
various methods have been developed to solely focus on LD denoising, LV
reconstruction, or CT-free AC in SPECT, the solution for simultaneously
addressing these tasks remains challenging and under-explored. Furthermore, it
is essential to explore the potential of fusing cross-domain and cross-modality
information across these interrelated tasks to further enhance the accuracy of
each task. Thus, we propose a Dual-Domain Coarse-to-Fine Progressive Network
(DuDoCFNet), a multi-task learning method for simultaneous LD denoising, LV
reconstruction, and CT-free $\mu$-map generation of cardiac SPECT. Paired
dual-domain networks in DuDoCFNet are cascaded using a multi-layer fusion
mechanism for cross-domain and cross-modality feature fusion. Two-stage
progressive learning strategies are applied in both projection and image
domains to achieve coarse-to-fine estimations of SPECT projections and
CT-derived $\mu$-maps. Our experiments demonstrate DuDoCFNet's superior
accuracy in estimating projections, generating $\mu$-maps, and AC
reconstructions compared to existing single- or multi-task learning methods,
under various iterations and LD levels. The source code of this work is
available at https://github.com/XiongchaoChen/DuDoCFNet-MultiTask.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13147">Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic Images Using a 3D Convolutional Auto-Encoder. (arXiv:2401.13147v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tabassian_M/0/1/0/all/0/1">Mahdi Tabassian</a>, <a href="http://arxiv.org/find/eess/1/au:+S_S/0/1/0/all/0/1">Somayeh Akbari. S</a>, <a href="http://arxiv.org/find/eess/1/au:+Queiros_S/0/1/0/all/0/1">Sandro Queir&#xf3;s</a>, <a href="http://arxiv.org/find/eess/1/au:+Dhooge_J/0/1/0/all/0/1">Jan D&#x27;hooge</a></p>
<p>This study presents a deep convolutional auto-encoder network for filtering
reverberation artifacts, from transthoracic echocardiographic (TTE) image
sequences. Given the spatiotemporal nature of these artifacts, the filtering
network was built using 3D convolutional layers to suppress the clutter
patterns throughout the cardiac cycle. The network was designed by taking
advantage of: i) an attention mechanism to focus primarily on cluttered regions
and ii) residual learning to preserve fine structures of the image frames. To
train the deep network, a diverse set of artifact patterns was simulated and
the simulated patterns were superimposed onto artifact-free ultra-realistic
synthetic TTE sequences of six ultrasound vendors to generate input of the
filtering network. The artifact-free sequences served as ground-truth.
Performance of the filtering network was evaluated using unseen synthetic as
well as in-vivo artifactual sequences. Satisfactory results obtained using the
latter dataset confirmed the good generalization performance of the proposed
network which was trained using the synthetic sequences and simulated artifact
patterns. Suitability of the clutter-filtered sequences for further processing
was assessed by computing segmental strain curves from them. The results showed
that the large discrepancy between the strain profiles computed from the
cluttered segments and their corresponding segments in the clutter-free images
was significantly reduced after filtering the sequences using the proposed
network. The trained deep network could process an artifactual TTE sequence in
a fraction of a second and can be used for real-time clutter filtering.
Moreover, it can improve the precision of the clinical indexes that are
computed from the TTE sequences. The source code of the proposed method is
available at:
https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13161">A Generalized Multiscale Bundle-Based Hyperspectral Sparse Unmixing Algorithm. (arXiv:2401.13161v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ayres_L/0/1/0/all/0/1">Luciano Carvalho Ayres</a>, <a href="http://arxiv.org/find/cs/1/au:+Borsoi_R/0/1/0/all/0/1">Ricardo Augusto Borsoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermudez_J/0/1/0/all/0/1">Jos&#xe9; Carlos Moreira Bermudez</a>, <a href="http://arxiv.org/find/cs/1/au:+Almeida_S/0/1/0/all/0/1">S&#xe9;rgio Jos&#xe9; Melo de Almeida</a></p>
<p>In hyperspectral sparse unmixing, a successful approach employs spectral
bundles to address the variability of the endmembers in the spatial domain.
However, the regularization penalties usually employed aggregate substantial
computational complexity, and the solutions are very noise-sensitive. We
generalize a multiscale spatial regularization approach to solve the unmixing
problem by incorporating group sparsity-inducing mixed norms. Then, we propose
a noise-robust method that can take advantage of the bundle structure to deal
with endmember variability while ensuring inter- and intra-class sparsity in
abundance estimation with reasonable computational cost. We also present a
general heuristic to select the \emph{most representative} abundance estimation
over multiple runs of the unmixing process, yielding a solution that is robust
and highly reproducible. Experiments illustrate the robustness and consistency
of the results when compared to related methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13172">ADMap: Anti-disturbance framework for reconstructing online vectorized HD map. (arXiv:2401.13172v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Haotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fanyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaonong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Laifeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiwang Zhang</a></p>
<p>In the field of autonomous driving, online high-definition (HD) map
reconstruction is crucial for planning tasks. Recent research has developed
several high-performance HD map reconstruction models to meet this necessity.
However, the point sequences within the instance vectors may be jittery or
jagged due to prediction bias, which can impact subsequent tasks. Therefore,
this paper proposes the Anti-disturbance Map reconstruction framework (ADMap).
To mitigate point-order jitter, the framework consists of three modules:
Multi-Scale Perception Neck, Instance Interactive Attention (IIA), and Vector
Direction Difference Loss (VDDL). By exploring the point-order relationships
between and within instances in a cascading manner, the model can monitor the
point-order prediction process more effectively. ADMap achieves
state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive
results demonstrate its ability to produce stable and reliable map elements in
complex and changing driving scenarios. Code and more demos are available at
https://github.com/hht1996ok/ADMap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13174">Boundary and Relation Distillation for Semantic Segmentation. (arXiv:2401.13174v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1">Pingcheng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinting Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kwang-Ting Cheng</a></p>
<p>Recently, it has been revealed that small semantic segmentation (SS) models
exhibit a tendency to make errors in maintaining boundary region completeness
and preserving target region connectivity, despite their effective segmentation
of the main object regions. To address these errors, we propose a targeted
boundary and relation distillation (BRD) strategy using knowledge distillation
from large teacher models to small student models. Specifically, the boundary
distillation extracts explicit object boundaries from the hierarchical feature
maps of the backbone network, subsequently enhancing the student model's mask
quality in boundary regions. Concurrently, the relation distillation transfers
implicit relations from the teacher model to the student model using
pixel-level self-relation as a bridge, ensuring that the student's mask has
strong target region connectivity. The proposed BRD is designed concretely for
SS and is characterized by simplicity and efficiency. Through experimental
evaluations on multiple SS datasets, including Pascal VOC 2012, Cityscapes,
ADE20K, and COCO-Stuff 10K, we demonstrated that BRD significantly surpasses
the current methods without increasing the inference costs, generating crisp
region boundaries and smooth connecting regions that are challenging for small
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13191">Towards Multi-domain Face Landmark Detection with Synthetic Data from Diffusion model. (arXiv:2401.13191v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gwantae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1">Jeong-gi Kwak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ku_B/0/1/0/all/0/1">Bon-hwa Ku</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1">Hanseok Ko</a></p>
<p>Recently, deep learning-based facial landmark detection for in-the-wild faces
has achieved significant improvement. However, there are still challenges in
face landmark detection in other domains (e.g. cartoon, caricature, etc). This
is due to the scarcity of extensively annotated training data. To tackle this
concern, we design a two-stage training approach that effectively leverages
limited datasets and the pre-trained diffusion model to obtain aligned pairs of
landmarks and face in multiple domains. In the first stage, we train a
landmark-conditioned face generation model on a large dataset of real faces. In
the second stage, we fine-tune the above model on a small dataset of
image-landmark pairs with text prompts for controlling the domain. Our new
designs enable our method to generate high-quality synthetic paired datasets
from multiple domains while preserving the alignment between landmarks and
facial features. Finally, we fine-tuned a pre-trained face landmark detection
model on the synthetic dataset to achieve multi-domain face landmark detection.
Our qualitative and quantitative results demonstrate that our method
outperforms existing methods on multi-domain face landmark detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13193">Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN. (arXiv:2401.13193v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minsoo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minkoo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Suhyun Kim</a></p>
<p>Deep learning has made significant advances in computer vision, particularly
in image classification tasks. Despite their high accuracy on training data,
deep learning models often face challenges related to complexity and
overfitting. One notable concern is that the model often relies heavily on a
limited subset of filters for making predictions. This dependency can result in
compromised generalization and an increased vulnerability to minor variations.
While regularization techniques like weight decay, dropout, and data
augmentation are commonly used to address this issue, they may not directly
tackle the reliance on specific filters. Our observations reveal that the heavy
reliance problem gets severe when slow-learning filters are deprived of
learning opportunities due to fast-learning filters. Drawing inspiration from
image augmentation research that combats over-reliance on specific image
regions by removing and replacing parts of images, our idea is to mitigate the
problem of over-reliance on strong filters by substituting highly activated
features. To this end, we present a novel method called Catch-up Mix, which
provides learning opportunities to a wide range of filters during training,
focusing on filters that may lag behind. By mixing activation maps with
relatively lower norms, Catch-up Mix promotes the development of more diverse
representations and reduces reliance on a small subset of filters. Experimental
results demonstrate the superiority of our method in various vision
classification datasets, providing enhanced robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13197">Predicting Mitral Valve mTEER Surgery Outcomes Using Machine Learning and Deep Learning Techniques. (arXiv:2401.13197v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Vyas_T/0/1/0/all/0/1">Tejas Vyas</a>, <a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1">Mohsena Chowdhury</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1">Xiaojiao Xiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Claeys_M/0/1/0/all/0/1">Mathias Claeys</a>, <a href="http://arxiv.org/find/eess/1/au:+Ong_G/0/1/0/all/0/1">G&#xe9;raldine Ong</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Guanghui Wang</a></p>
<p>Mitral Transcatheter Edge-to-Edge Repair (mTEER) is a medical procedure
utilized for the treatment of mitral valve disorders. However, predicting the
outcome of the procedure poses a significant challenge. This paper makes the
first attempt to harness classical machine learning (ML) and deep learning (DL)
techniques for predicting mitral valve mTEER surgery outcomes. To achieve this,
we compiled a dataset from 467 patients, encompassing labeled echocardiogram
videos and patient reports containing Transesophageal Echocardiography (TEE)
measurements detailing Mitral Valve Repair (MVR) treatment outcomes. Leveraging
this dataset, we conducted a benchmark evaluation of six ML algorithms and two
DL models. The results underscore the potential of ML and DL in predicting
mTEER surgery outcomes, providing insight for future investigation and
advancements in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13201">MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfei Zhang</a></p>
<p>Multimodal large language models (MLLM) have achieved satisfactory results in
many tasks. However, their performance in the task of person re-identification
(ReID) has not been explored to date. This paper will investigate how to adapt
them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID
image-text datasets, and then use their visual encoder as a backbone for ReID.
However, there still exist two apparent issues: (1) Designing instructions for
ReID, MLLMs may overfit specific instructions, and designing a variety of
instructions will lead to higher costs. (2) Latent image feature vectors from
LLMs are not involved in loss computation. Instructional learning, aligning
image-text features, results in indirect optimization and a learning objective
that inadequately utilizes features, limiting effectiveness in person feature
learning. To address these problems, this paper proposes MLLMReID: Multimodal
Large Language Model-based ReID. Firstly, we proposed Common Instruction, a
simple approach that leverages the essence ability of LLMs to continue writing,
avoiding complex and diverse instruction design. Secondly, we proposed
DirectReID, which effectively employs the latent image feature vectors of
images outputted by LLMs in ReID tasks. The experimental results demonstrate
the superiority of our method. We will open-source the code on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13203">Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects. (arXiv:2401.13203v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunfan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhiwei Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhiqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vun_N/0/1/0/all/0/1">Nicholas Vun</a></p>
<p>Controllable 3D indoor scene synthesis stands at the forefront of
technological progress, offering various applications like gaming, film, and
augmented/virtual reality. The capability to stylize and de-couple objects
within these scenarios is a crucial factor, providing an advanced level of
control throughout the editing process. This control extends not just to
manipulating geometric attributes like translation and scaling but also
includes managing appearances, such as stylization. Current methods for scene
stylization are limited to applying styles to the entire scene, without the
ability to separate and customize individual objects. Addressing the
intricacies of this challenge, we introduce a unique pipeline designed for
synthesis 3D indoor scenes. Our approach involves strategically placing objects
within the scene, utilizing information from professionally designed bounding
boxes. Significantly, our pipeline prioritizes maintaining style consistency
across multiple objects within the scene, ensuring a cohesive and visually
appealing result aligned with the desired aesthetic. The core strength of our
pipeline lies in its ability to generate 3D scenes that are not only visually
impressive but also exhibit features like photorealism, multi-view consistency,
and diversity. These scenes are crafted in response to various natural language
prompts, demonstrating the versatility and adaptability of our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13205">Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size. (arXiv:2401.13205v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junlin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xinchen Lyu</a></p>
<p>Adversarial examples are one critical security threat to various visual
applications, where injected human-imperceptible perturbations can confuse the
output.Generating transferable adversarial examples in the black-box setting is
crucial but challenging in practice. Existing input-diversity-based methods
adopt different image transformations, but may be inefficient due to
insufficient input diversity and an identical perturbation step size. Motivated
by the fact that different image regions have distinctive weights in
classification, this paper proposes a black-box adversarial generative
framework by jointly designing enhanced input diversity and adaptive step
sizes. We design local mixup to randomly mix a group of transformed adversarial
images, strengthening the input diversity. For precise adversarial generation,
we project the perturbation into the $tanh$ space to relax the boundary
constraint. Moreover, the step sizes of different regions can be dynamically
adjusted by integrating a second-order momentum.Extensive experiments on
ImageNet validate that our framework can achieve superior transferability
compared to state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13212">AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation. (arXiv:2401.13212v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lulan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Edalati_A/0/1/0/all/0/1">Ali Edalati</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1">Brett Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gross_W/0/1/0/all/0/1">Warren Gross</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">James J. Clark</a></p>
<p>This paper describes a simple yet effective technique for refining a
pretrained classifier network. The proposed AdCorDA method is based on
modification of the training set and making use of the duality between network
weights and layer inputs. We call this input space training. The method
consists of two stages - adversarial correction followed by domain adaptation.
Adversarial correction uses adversarial attacks to correct incorrect
training-set classifications. The incorrectly classified samples of the
training set are removed and replaced with the adversarially corrected samples
to form a new training set, and then, in the second stage, domain adaptation is
performed back to the original training set. Extensive experimental validations
show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The
technique can be straightforwardly applied to refinement of weight-quantized
neural networks, where experiments show substantial enhancement in performance
over the baseline. The adversarial correction technique also results in
enhanced robustness to adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13213">Common-Sense Bias Discovery and Mitigation for Classification Tasks. (arXiv:2401.13213v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Miao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+fryer_Z/0/1/0/all/0/1">Zee fryer</a>, <a href="http://arxiv.org/find/cs/1/au:+Colman_B/0/1/0/all/0/1">Ben Colman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahriyari_A/0/1/0/all/0/1">Ali Shahriyari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bharaj_G/0/1/0/all/0/1">Gaurav Bharaj</a></p>
<p>Machine learning model bias can arise from dataset composition: sensitive
features correlated to the learning target disturb the model decision rule and
lead to performance differences along the features. Existing de-biasing work
captures prominent and delicate image features which are traceable in model
latent space, like colors of digits or background of animals. However, using
the latent space is not sufficient to understand all dataset feature
correlations. In this work, we propose a framework to extract feature clusters
in a dataset based on image descriptions, allowing us to capture both subtle
and coarse features of the images. The feature co-occurrence pattern is
formulated and correlation is measured, utilizing a human-in-the-loop for
examination. The analyzed features and correlations are human-interpretable, so
we name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive
correlations in a dataset, we demonstrate that downstream model bias can be
mitigated by adjusting image sampling weights, without requiring a sensitive
group label supervision. Experiments show that our method discovers novel
biases on multiple classification tasks for two benchmark image datasets, and
the intervention outperforms state-of-the-art unsupervised bias mitigation
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13214">AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network. (arXiv:2401.13214v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaolin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Junkai Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aihua Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhilong Lin</a></p>
<p>Recently, methods based on deep learning have been successfully applied to
ship detection for synthetic aperture radar (SAR) images. Despite the
development of numerous ship detection methodologies, detecting small and
coastal ships remains a significant challenge due to the limited features and
clutter in coastal environments. For that, a novel adaptive multi-hierarchical
attention module (AMAM) is proposed to learn multi-scale features and
adaptively aggregate salient features from various feature layers, even in
complex environments. Specifically, we first fuse information from adjacent
feature layers to enhance the detection of smaller targets, thereby achieving
multi-scale feature enhancement. Then, to filter out the adverse effects of
complex backgrounds, we dissect the previously fused multi-level features on
the channel, individually excavate the salient regions, and adaptively
amalgamate features originating from different channels. Thirdly, we present a
novel adaptive multi-hierarchical attention network (AMANet) by embedding the
AMAM between the backbone network and the feature pyramid network (FPN).
Besides, the AMAM can be readily inserted between different frameworks to
improve object detection. Lastly, extensive experiments on two large-scale SAR
ship detection datasets demonstrate that our AMANet method is superior to
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13220">Segment Any Cell: A SAM-based Auto-prompting Fine-tuning Framework for Nuclei Segmentation. (arXiv:2401.13220v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Na_S/0/1/0/all/0/1">Saiyang Na</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yuzhi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_F/0/1/0/all/0/1">Feng Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1">Hehuan Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1">Junzhou Huang</a></p>
<p>In the rapidly evolving field of AI research, foundational models like BERT
and GPT have significantly advanced language and vision tasks. The advent of
pretrain-prompting models such as ChatGPT and Segmentation Anything Model (SAM)
has further revolutionized image segmentation. However, their applications in
specialized areas, particularly in nuclei segmentation within medical imaging,
reveal a key challenge: the generation of high-quality, informative prompts is
as crucial as applying state-of-the-art (SOTA) fine-tuning techniques on
foundation models. To address this, we introduce Segment Any Cell (SAC), an
innovative framework that enhances SAM specifically for nuclei segmentation.
SAC integrates a Low-Rank Adaptation (LoRA) within the attention layer of the
Transformer to improve the fine-tuning process, outperforming existing SOTA
methods. It also introduces an innovative auto-prompt generator that produces
effective prompts to guide segmentation, a critical factor in handling the
complexities of nuclei segmentation in biomedical imaging. Our extensive
experiments demonstrate the superiority of SAC in nuclei segmentation tasks,
proving its effectiveness as a tool for pathologists and researchers. Our
contributions include a novel prompt generation strategy, automated
adaptability for diverse segmentation tasks, the innovative application of
Low-Rank Attention Adaptation in SAM, and a versatile framework for semantic
segmentation challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13221">Unified-Width Adaptive Dynamic Network for All-In-One Image Restoration. (arXiv:2401.13221v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yimin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1">Nanxi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1">Zhongyun Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1">Fei Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>In contrast to traditional image restoration methods, all-in-one image
restoration techniques are gaining increased attention for their ability to
restore images affected by diverse and unknown corruption types and levels.
However, contemporary all-in-one image restoration methods omit task-wise
difficulties and employ the same networks to reconstruct images afflicted by
diverse degradations. This practice leads to an underestimation of the task
correlations and suboptimal allocation of computational resources. To elucidate
task-wise complexities, we introduce a novel concept positing that intricate
image degradation can be represented in terms of elementary degradation.
Building upon this foundation, we propose an innovative approach, termed the
Unified-Width Adaptive Dynamic Network (U-WADN), consisting of two pivotal
components: a Width Adaptive Backbone (WAB) and a Width Selector (WS). The WAB
incorporates several nested sub-networks with varying widths, which facilitates
the selection of the most apt computations tailored to each task, thereby
striking a balance between accuracy and computational efficiency during
runtime. For different inputs, the WS automatically selects the most
appropriate sub-network width, taking into account both task-specific and
sample-specific complexities. Extensive experiments across a variety of image
restoration tasks demonstrate that the proposed U-WADN achieves better
performance while simultaneously reducing up to 32.3\% of FLOPs and providing
approximately 15.7\% real-time acceleration. The code has been made available
at \url{https://github.com/xuyimin0926/U-WADN}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13264">Enhancing cross-domain detection: adaptive class-aware contrastive transformer. (arXiv:2401.13264v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Ziru Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yue Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongtao Lu</a></p>
<p>Recently,the detection transformer has gained substantial attention for its
inherent minimal post-processing requirement.However,this paradigm relies on
abundant training data,yet in the context of the cross-domain
adaptation,insufficient labels in the target domain exacerbate issues of class
imbalance and model performance degradation.To address these challenges, we
propose a novel class-aware cross domain detection transformer based on the
adversarial learning and mean-teacher framework.First,considering the
inconsistencies between the classification and regression tasks,we introduce an
IoU-aware prediction branch and exploit the consistency of classification and
location scores to filter and reweight pseudo labels.Second, we devise a
dynamic category threshold refinement to adaptively manage model
confidence.Third,to alleviate the class imbalance,an instance-level class-aware
contrastive learning module is presented to encourage the generation of
discriminative features for each class,particularly benefiting minority
classes.Experimental results across diverse domain-adaptive scenarios validate
our method's effectiveness in improving performance and alleviating class
imbalance issues,which outperforms the state-of-the-art transformer based
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13267">Dual-modal Dynamic Traceback Learning for Medical Report Generation. (arXiv:2401.13267v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1">Shuchang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1">Mingyuan Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingjian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1">Dagan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinman Kim</a></p>
<p>With increasing reliance on medical imaging in clinical practices, automated
report generation from medical images is in great demand. Existing report
generation methods typically adopt an encoder-decoder deep learning framework
to build a uni-directional image-to-report mapping. However, such a framework
ignores the bi-directional mutual associations between images and reports, thus
incurring difficulties in associating the intrinsic medical meanings between
them. Recent generative representation learning methods have demonstrated the
benefits of dual-modal learning from both image and text modalities. However,
these methods exhibit two major drawbacks for medical report generation: 1)
they tend to capture morphological information and have difficulties in
capturing subtle pathological semantic information, and 2) they predict masked
text rely on both unmasked images and text, inevitably degrading performance
when inference is based solely on images. In this study, we propose a new
report generation framework with dual-modal dynamic traceback learning (DTrace)
to overcome the two identified drawbacks and enable dual-modal learning for
medical report generation. To achieve this, our DTrace introduces a traceback
mechanism to control the semantic validity of generated content via
self-assessment. Further, our DTrace introduces a dynamic learning strategy to
adapt to various proportions of image and text input, enabling report
generation without reliance on textual input during inference. Extensive
experiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that
our DTrace outperforms state-of-the-art medical report generation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13270">Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Pengcheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1">Wei Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ronggang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1">Richang Hong</a></p>
<p>Automatic image colorization is inherently an ill-posed problem with
uncertainty, which requires an accurate semantic understanding of scenes to
estimate reasonable colors for grayscale images. Although recent
interaction-based methods have achieved impressive performance, it is still a
very difficult task to infer realistic and accurate colors for automatic
colorization. To reduce the difficulty of semantic understanding of grayscale
scenes, this paper tries to utilize corresponding audio, which naturally
contains extra semantic information about the same scene. Specifically, a novel
audio-infused automatic image colorization (AIAIC) network is proposed, which
consists of three stages. First, we take color image semantics as a bridge and
pretrain a colorization network guided by color image semantics. Second, the
natural co-occurrence of audio and video is utilized to learn the color
semantic correlations between audio and visual scenes. Third, the implicit
audio semantic representation is fed into the pretrained network to finally
realize the audio-guided colorization. The whole process is trained in a
self-supervised manner without human annotation. In addition, an audiovisual
colorization dataset is established for training and testing. Experiments
demonstrate that audio guidance can effectively improve the performance of
automatic colorization, especially for some scenes that are difficult to
understand only from visual modality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13280">DDI-CoCo: A Dataset For Understanding The Effect Of Color Contrast In Machine-Assisted Skin Disease Detection. (arXiv:2401.13280v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiu_M/0/1/0/all/0/1">Ming-Chang Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingfei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1">Yen-Ju Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a></p>
<p>Skin tone as a demographic bias and inconsistent human labeling poses
challenges in dermatology AI. We take another angle to investigate color
contrast's impact, beyond skin tones, on malignancy detection in skin disease
datasets: We hypothesize that in addition to skin tones, the color difference
between the lesion area and skin also plays a role in malignancy detection
performance of dermatology AI models. To study this, we first propose a robust
labeling method to quantify color contrast scores of each image and validate
our method by showing small labeling variations. More importantly, applying our
method to \textit{the only} diverse-skin tone and pathologically-confirmed skin
disease dataset DDI, yields \textbf{DDI-CoCo Dataset}, and we observe a
performance gap between the high and low color difference groups. This
disparity remains consistent across various state-of-the-art (SoTA) image
classification models, which supports our hypothesis. Furthermore, we study the
interaction between skin tone and color difference effects and suggest that
color difference can be an additional reason behind model performance bias
between skin tones. Our work provides a complementary angle to dermatology AI
for improving skin disease detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13285">Small Object Tracking in LiDAR Point Cloud: Learning the Target-awareness Prototype and Fine-grained Search Region. (arXiv:2401.13285v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Shengjing Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yinan Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiuping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiantong Zhao</a></p>
<p>Single Object Tracking in LiDAR point cloud is one of the most essential
parts of environmental perception, in which small objects are inevitable in
real-world scenarios and will bring a significant barrier to the accurate
location. However, the existing methods concentrate more on exploring universal
architectures for common categories and overlook the challenges that small
objects have long been thorny due to the relative deficiency of foreground
points and a low tolerance for disturbances. To this end, we propose a Siamese
network-based method for small object tracking in the LiDAR point cloud, which
is composed of the target-awareness prototype mining (TAPM) module and the
regional grid subdivision (RGS) module. The TAPM module adopts the
reconstruction mechanism of the masked decoder to learn the prototype in the
feature space, aiming to highlight the presence of foreground points that will
facilitate the subsequent location of small objects. Through the above
prototype is capable of accentuating the small object of interest, the
positioning deviation in feature maps still leads to high tracking errors. To
alleviate this issue, the RGS module is proposed to recover the fine-grained
features of the search region based on ViT and pixel shuffle layers. In
addition, apart from the normal settings, we elaborately design a scaling
experiment to evaluate the robustness of the different trackers on small
objects. Extensive experiments on KITTI and nuScenes demonstrate that our
method can effectively improve the tracking performance of small targets
without affecting normal-sized objects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13296">Visual Objectification in Films: Towards a New AI Task for Video Interpretation. (arXiv:2401.13296v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tores_J/0/1/0/all/0/1">Julie Tores</a>, <a href="http://arxiv.org/find/cs/1/au:+Sassatelli_L/0/1/0/all/0/1">Lucile Sassatelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hui-Yin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergman_C/0/1/0/all/0/1">Clement Bergman</a>, <a href="http://arxiv.org/find/cs/1/au:+Andolfi_L/0/1/0/all/0/1">Lea Andolfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ecrement_V/0/1/0/all/0/1">Victor Ecrement</a>, <a href="http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1">Frederic Precioso</a>, <a href="http://arxiv.org/find/cs/1/au:+Devars_T/0/1/0/all/0/1">Thierry Devars</a>, <a href="http://arxiv.org/find/cs/1/au:+Guaresi_M/0/1/0/all/0/1">Magali Guaresi</a>, <a href="http://arxiv.org/find/cs/1/au:+Julliard_V/0/1/0/all/0/1">Virginie Julliard</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecossais_S/0/1/0/all/0/1">Sarah Lecossais</a></p>
<p>In film gender studies, the concept of 'male gaze' refers to the way the
characters are portrayed on-screen as objects of desire rather than subjects.
In this article, we introduce a novel video-interpretation task, to detect
character objectification in films. The purpose is to reveal and quantify the
usage of complex temporal patterns operated in cinema to produce the cognitive
perception of objectification. We introduce the ObyGaze12 dataset, made of 1914
movie clips densely annotated by experts for objectification concepts
identified in film studies and psychology. We evaluate recent vision models,
show the feasibility of the task and where the challenges remain with concept
bottleneck models. Our new dataset and code are made available to the
community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13307">ChatterBox: Multi-round Multimodal Referring and Grounding. (arXiv:2401.13307v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yunjie Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tianren Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lingxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jihao Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jianbin Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qixiang Ye</a></p>
<p>In this study, we establish a baseline for a new task named multimodal
multi-round referring and grounding (MRG), opening up a promising direction for
instance-level multimodal dialogues. We present a new benchmark and an
efficient vision-language model for this purpose. The new benchmark, named
CB-300K, spans challenges including multi-round dialogue, complex spatial
relationships among multiple instances, and consistent reasoning, which are
beyond those shown in existing benchmarks. The proposed model, named
ChatterBox, utilizes a two-branch architecture to collaboratively handle vision
and language tasks. By tokenizing instance regions, the language branch
acquires the ability to perceive referential information. Meanwhile, ChatterBox
feeds a query embedding in the vision branch to a token receiver for visual
grounding. A two-stage optimization strategy is devised, making use of both
CB-300K and auxiliary external data to improve the model's stability and
capacity for instance-level understanding. Experiments show that ChatterBox
outperforms existing models in MRG both quantitatively and qualitatively,
paving a new path towards multimodal dialogue scenarios with complicated and
precise interactions. Code, data, and model are available at:
https://github.com/sunsmarterjie/ChatterBox.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13311">ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1">Rohan Wadhawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Recent advancements in AI have led to the development of large multimodal
models (LMMs) capable of processing complex tasks involving joint reasoning
over text and visual content in the image (e.g., navigating maps in public
places). This paper introduces ConTextual, a novel benchmark comprising
instructions designed explicitly to evaluate LMMs' ability to perform
context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse
real-world scenarios (e.g., time-reading, navigation, shopping and more)
demanding a deeper understanding of the interactions between textual and visual
elements. Our findings reveal a significant performance gap of 30.8% between
the best-performing LMM, GPT-4V(ision), and human capabilities using human
evaluation indicating substantial room for improvement in context-sensitive
text-rich visual reasoning. Notably, while GPT-4V excelled in abstract
categories like meme and quote interpretation, its overall performance still
lagged behind humans. In addition to human evaluations, we also employed
automatic evaluation metrics using GPT-4, uncovering similar trends in
performance disparities. We also perform a fine-grained evaluation across
diverse visual contexts and provide qualitative analysis which provides a
robust framework for future advancements in the LMM design.
https://con-textual.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13313">InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions. (arXiv:2401.13313v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1">Ryota Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Iki_T/0/1/0/all/0/1">Taichi Iki</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1">Kyosuke Nishida</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1">Kuniko Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1">Jun Suzuki</a></p>
<p>We study the problem of completing various visual document understanding
(VDU) tasks, e.g., question answering and information extraction, on real-world
documents through human-written instructions. To this end, we propose
InstructDoc, the first large-scale collection of 30 publicly available VDU
datasets, each with diverse instructions in a unified format, which covers a
wide range of 12 tasks and includes open document types/formats. Furthermore,
to enhance the generalization performance on VDU tasks, we design a new
instruction-based document reading and understanding model, InstructDr, that
connects document images, image encoders, and large language models (LLMs)
through a trainable bridging module. Experiments demonstrate that InstructDr
can effectively adapt to new VDU datasets, tasks, and domains via given
instructions and outperforms existing multimodal LLMs and ChatGPT without
specific training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13315">Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging. (arXiv:2401.13315v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Haugland_M/0/1/0/all/0/1">Mathias Ramm Haugland</a>, <a href="http://arxiv.org/find/eess/1/au:+Qadir_H/0/1/0/all/0/1">Hemin Ali Qadir</a>, <a href="http://arxiv.org/find/eess/1/au:+Balasingham_I/0/1/0/all/0/1">Ilangko Balasingham</a></p>
<p>To cope with the growing prevalence of colorectal cancer (CRC), screening
programs for polyp detection and removal have proven their usefulness.
Colonoscopy is considered the best-performing procedure for CRC screening. To
ease the examination, deep learning based methods for automatic polyp detection
have been developed for conventional white-light imaging (WLI). Compared with
WLI, narrow-band imaging (NBI) can improve polyp classification during
colonoscopy but requires special equipment. We propose a CycleGAN-based
framework to convert images captured with regular WLI to synthetic NBI (SNBI)
as a pre-processing method for improving object detection on WLI when NBI is
unavailable. This paper first shows that better results for polyp detection can
be achieved on NBI compared to a relatively similar dataset of WLI. Secondly,
experimental results demonstrate that our proposed modality translation can
achieve improved polyp detection on SNBI images generated from WLI compared to
the original WLI. This is because our WLI-to-SNBI translation model can enhance
the observation of polyp surface patterns in the generated SNBI images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13325">Memory Consistency Guided Divide-and-Conquer Learning for Generalized Category Discovery. (arXiv:2401.13325v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1">Yuanpeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zhun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuxi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a></p>
<p>Generalized category discovery (GCD) aims at addressing a more realistic and
challenging setting of semi-supervised learning, where only part of the
category labels are assigned to certain training samples. Previous methods
generally employ naive contrastive learning or unsupervised clustering scheme
for all the samples. Nevertheless, they usually ignore the inherent critical
information within the historical predictions of the model being trained.
Specifically, we empirically reveal that a significant number of salient
unlabeled samples yield consistent historical predictions corresponding to
their ground truth category. From this observation, we propose a Memory
Consistency guided Divide-and-conquer Learning framework (MCDL). In this
framework, we introduce two memory banks to record historical prediction of
unlabeled data, which are exploited to measure the credibility of each sample
in terms of its prediction consistency. With the guidance of credibility, we
can design a divide-and-conquer learning strategy to fully utilize the
discriminative information of unlabeled data while alleviating the negative
influence of noisy labels. Extensive experimental results on multiple
benchmarks demonstrate the generality and superiority of our method, where our
method outperforms state-of-the-art models by a large margin on both seen and
unseen classes of the generic image recognition and challenging semantic shift
settings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13329">Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval. (arXiv:2401.13329v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Dezhao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiabo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1">Shaogang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hailin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Video Moment Retrieval (VMR) requires precise modelling of fine-grained
moment-text associations to capture intricate visual-language relationships.
Due to the lack of a diverse and generalisable VMR dataset to facilitate
learning scalable moment-text associations, existing methods resort to joint
training on both source and target domain videos for cross-domain applications.
Meanwhile, recent developments in vision-language multimodal models pre-trained
on large-scale image-text and/or video-text pairs are only based on coarse
associations (weakly labelled). They are inadequate to provide fine-grained
moment-text correlations required for cross-domain VMR. In this work, we solve
the problem of unseen cross-domain VMR, where certain visual and textual
concepts do not overlap across domains, by only utilising target domain
sentences (text prompts) without accessing their videos. To that end, we
explore generative video diffusion for fine-grained editing of source videos
controlled by the target sentences, enabling us to simulate target domain
videos. We address two problems in video editing for optimising unseen domain
VMR: (1) generation of high-quality simulation videos of different moments with
subtle distinctions, (2) selection of simulation videos that complement
existing source training videos without introducing harmful noise or
unnecessary repetitions. On the first problem, we formulate a two-stage video
diffusion generation controlled simultaneously by (1) the original video
structure of a source video, (2) subject specifics, and (3) a target sentence
prompt. This ensures fine-grained variations between video moments. On the
second problem, we introduce a hybrid selection mechanism that combines two
quantitative metrics for noise filtering and one qualitative metric for
leveraging VMR prediction on simulation video selection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13330">NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks. (arXiv:2401.13330v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gambella_M/0/1/0/all/0/1">Matteo Gambella</a>, <a href="http://arxiv.org/find/cs/1/au:+Pomponi_J/0/1/0/all/0/1">Jary Pomponi</a>, <a href="http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1">Simone Scardapane</a>, <a href="http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1">Manuel Roveri</a></p>
<p>Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)
with Early Exit Classifiers (EECs), to provide predictions at intermediate
points of the processing when enough confidence in classification is achieved.
This leads to many benefits in terms of effectiveness and efficiency.
Currently, the design of EENNs is carried out manually by experts, a complex
and time-consuming task that requires accounting for many aspects, including
the correct placement, the thresholding, and the computational overhead of the
EECs. For this reason, the research is exploring the use of Neural Architecture
Search (NAS) to automatize the design of EENNs. Currently, few comprehensive
NAS solutions for EENNs have been proposed in the literature, and a fully
automated, joint design strategy taking into consideration both the backbone
and the EECs remains an open problem. To this end, this work presents Neural
Architecture Search for Hardware Constrained Early Exit Neural Networks
(NACHOS), the first NAS framework for the design of optimal EENNs satisfying
constraints on the accuracy and the number of Multiply and Accumulate (MAC)
operations performed by the EENNs at inference time. In particular, this
provides the joint design of backbone and EECs to select a set of admissible
(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best
tradeoff between the accuracy and number of MACs. The results show that the
models designed by NACHOS are competitive with the state-of-the-art EENNs.
Additionally, this work investigates the effectiveness of two novel
regularization terms designed for the optimization of the auxiliary classifiers
of the EENN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13352">EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction. (arXiv:2401.13352v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangsen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>The accurate 3D reconstruction of deformable soft body tissues from
endoscopic videos is a pivotal challenge in medical applications such as VR
surgery and medical image analysis. Existing methods often struggle with
accuracy and the ambiguity of hallucinated tissue parts, limiting their
practical utility. In this work, we introduce EndoGaussians, a novel approach
that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This
method marks the first use of Gaussian Splatting in this context, overcoming
the limitations of previous NeRF-based techniques. Our method sets new
state-of-the-art standards, as demonstrated by quantitative assessments on
various endoscope datasets. These advancements make our method a promising tool
for medical professionals, offering more reliable and efficient 3D
reconstructions for practical applications in the medical field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13357">Linear Relative Pose Estimation Founded on Pose-only Imaging Geometry. (arXiv:2401.13357v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1">Qi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuanxin Wu</a></p>
<p>How to efficiently and accurately handle image matching outliers is a
critical issue in two-view relative estimation. The prevailing RANSAC method
necessitates that the minimal point pairs be inliers. This paper introduces a
linear relative pose estimation algorithm for n $( n \geq 6$) point pairs,
which is founded on the recent pose-only imaging geometry to filter out
outliers by proper reweighting. The proposed algorithm is able to handle planar
degenerate scenes, and enhance robustness and accuracy in the presence of a
substantial ratio of outliers. Specifically, we embed the linear global
translation (LiGT) constraint into the strategies of iteratively reweighted
least-squares (IRLS) and RANSAC so as to realize robust outlier removal.
Simulations and real tests of the Strecha dataset show that the proposed
algorithm achieves relative rotation accuracy improvement of 2 $\sim$ 10 times
in face of as large as 80% outliers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13363">Do You Guys Want to Dance: Zero-Shot Compositional Human Dance Generation with Multiple Persons. (arXiv:2401.13363v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1">Kun Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Cheng Deng</a></p>
<p>Human dance generation (HDG) aims to synthesize realistic videos from images
and sequences of driving poses. Despite great success, existing methods are
limited to generating videos of a single person with specific backgrounds,
while the generalizability for real-world scenarios with multiple persons and
complex backgrounds remains unclear. To systematically measure the
generalizability of HDG models, we introduce a new task, dataset, and
evaluation protocol of compositional human dance generation (cHDG). Evaluating
the state-of-the-art methods on cHDG, we empirically find that they fail to
generalize to real-world scenarios. To tackle the issue, we propose a novel
zero-shot framework, dubbed MultiDance-Zero, that can synthesize videos
consistent with arbitrary multiple persons and background while precisely
following the driving poses. Specifically, in contrast to straightforward DDIM
or null-text inversion, we first present a pose-aware inversion method to
obtain the noisy latent code and initialization text embeddings, which can
accurately reconstruct the composed reference image. Since directly generating
videos from them will lead to severe appearance inconsistency, we propose a
compositional augmentation strategy to generate augmented images and utilize
them to optimize a set of generalizable text embeddings. In addition,
consistency-guided sampling is elaborated to encourage the background and
keypoints of the estimated clean image at each reverse step to be close to
those of the reference image, further improving the temporal consistency of
generated videos. Extensive qualitative and quantitative results demonstrate
the effectiveness and superiority of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13386">Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain. (arXiv:2401.13386v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1">Joachim Denzler</a></p>
<p>Face recognition technology has been deployed in various real-life
applications. The most sophisticated deep learning-based face recognition
systems rely on training millions of face images through complex deep neural
networks to achieve high accuracy. It is quite common for clients to upload
face images to the service provider in order to access the model inference.
However, the face image is a type of sensitive biometric attribute tied to the
identity information of each user. Directly exposing the raw face image to the
service provider poses a threat to the user's privacy. Current
privacy-preserving approaches to face recognition focus on either concealing
visual information on model input or protecting model output face embedding.
The noticeable drop in recognition accuracy is a pitfall for most methods. This
paper proposes a hybrid frequency-color fusion approach to reduce the input
dimensionality of face recognition in the frequency domain. Moreover, sparse
color information is also introduced to alleviate significant accuracy
degradation after adding differential privacy noise. Besides, an
identity-specific embedding mapping scheme is applied to protect original face
embedding by enlarging the distance among identities. Lastly, secure multiparty
computation is implemented for safely computing the embedding distance during
model inference. The proposed method performs well on multiple widely used
verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy
than the state-of-the-art in the 1:N verification scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13388">UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion. (arXiv:2401.13388v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xue Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiachen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xinyan Xiao</a></p>
<p>Existing text-to-image diffusion models primarily generate images from text
prompts. However, the inherent conciseness of textual descriptions poses
challenges in faithfully synthesizing images with intricate details, such as
specific entities or scenes. This paper presents \textbf{UNIMO-G}, a simple
multimodal conditional diffusion framework that operates on multimodal prompts
with interleaved textual and visual inputs, which demonstrates a unified
ability for both text-driven and subject-driven image generation. UNIMO-G
comprises two core components: a Multimodal Large Language Model (MLLM) for
encoding multimodal prompts, and a conditional denoising diffusion network for
generating images based on the encoded multimodal input. We leverage a
two-stage training strategy to effectively train the framework: firstly
pre-training on large-scale text-image pairs to develop conditional image
generation capabilities, and then instruction tuning with multimodal prompts to
achieve unified image generation proficiency. A well-designed data processing
pipeline involving language grounding and image segmentation is employed to
construct multi-modal prompts. UNIMO-G excels in both text-to-image generation
and zero-shot subject-driven synthesis, and is notably effective in generating
high-fidelity images from complex multimodal prompts involving multiple image
entities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13403">SEDNet: Shallow Encoder-Decoder Network for Brain Tumor Segmentation. (arXiv:2401.13403v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Olisah_C/0/1/0/all/0/1">Chollette C. Olisah</a></p>
<p>Despite the advancement in computational modeling towards brain tumor
segmentation, of which several models have been developed, it is evident from
the computational complexity of existing models which are still at an all-time
high, that performance and efficiency under clinical application scenarios are
limited. Therefore, this paper proposes a shallow encoder and decoder network
named SEDNet for brain tumor segmentation. The proposed network is adapted from
the U-Net structure. Though brain tumors do not assume complex structures like
the task the traditional U-Net was designed for, their variance in appearance,
shape, and ambiguity of boundaries makes it a compelling complex task to solve.
SEDNet architecture design is inspired by the localized nature of brain tumors
in brain images, thus consists of sufficient hierarchical convolutional blocks
in the encoding pathway capable of learning the intrinsic features of brain
tumors in brain slices, and a decoding pathway with selective skip path
sufficient for capturing miniature local-level spatial features alongside the
global-level features of brain tumor. SEDNet with the integration of the
proposed preprocessing algorithm and optimization function on the BraTS2020 set
reserved for testing achieves impressive dice and Hausdorff scores of 0.9308,
0.9451, 0.9026, and 0.7040, 1.2866, 0.7762 for non-enhancing tumor core (NTC),
peritumoral edema (ED), and enhancing tumor (ET), respectively. Furthermore,
through transfer learning with initialized SEDNet pre-trained weights, termed
SEDNetX, a performance increase is observed. The dice and Hausdorff scores
recorded are 0.9336, 0.9478, 0.9061, 0.6983, 1.2691, and 0.7711 for NTC, ED,
and ET, respectively. With about 1.3 million parameters and impressive
performance in comparison to the state-of-the-art, SEDNet(X) is shown to be
computationally efficient for real-time clinical diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13405">Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter. (arXiv:2401.13405v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongmyoung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojas_N/0/1/0/all/0/1">Nicolas Rojas</a></p>
<p>Object recognition and object pose estimation in robotic grasping continue to
be significant challenges, since building a labelled dataset can be time
consuming and financially costly in terms of data collection and annotation. In
this work, we propose a synthetic data generation method that minimizes human
intervention and makes downstream image segmentation algorithms more robust by
combining a generated synthetic dataset with a smaller real-world dataset
(hybrid dataset). Annotation experiments show that the proposed synthetic scene
generation can diminish labelling time dramatically. RGB image segmentation is
trained with hybrid dataset and combined with depth information to produce
pixel-to-point correspondence of individual segmented objects. The object to
grasp is then determined by the confidence score of the segmentation algorithm.
Pick-and-place experiments demonstrate that segmentation trained on our hybrid
dataset (98.9%, 70%) outperforms the real dataset and a publicly available
dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping
success rate, respectively. Supplementary material is available at
https://sites.google.com/view/synthetic-dataset-generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13414">GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition. (arXiv:2401.13414v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xingyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Demachi_K/0/1/0/all/0/1">Kazuyuki Demachi</a></p>
<p>Current datasets for action recognition tasks face limitations stemming from
traditional collection and generation methods, including the constrained range
of action classes, absence of multi-viewpoint recordings, limited diversity,
poor video quality, and labor-intensive manually collection. To address these
challenges, we introduce GTAutoAct, a innovative dataset generation framework
leveraging game engine technology to facilitate advancements in action
recognition. GTAutoAct excels in automatically creating large-scale,
well-annotated datasets with extensive action classes and superior video
quality. Our framework's distinctive contributions encompass: (1) it
innovatively transforms readily available coordinate-based 3D human motion into
rotation-orientated representation with enhanced suitability in multiple
viewpoints; (2) it employs dynamic segmentation and interpolation of rotation
sequences to create smooth and realistic animations of action; (3) it offers
extensively customizable animation scenes; (4) it implements an autonomous
video capture and processing pipeline, featuring a randomly navigating camera,
with auto-trimming and labeling functionalities. Experimental results
underscore the framework's robustness and highlights its potential to
significantly improve action recognition model training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13418">Serial fusion of multi-modal biometric systems. (arXiv:2401.13418v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marcialis_G/0/1/0/all/0/1">Gian Luca Marcialis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastinu_P/0/1/0/all/0/1">Paolo Mastinu</a>, <a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1">Fabio Roli</a></p>
<p>Serial, or sequential, fusion of multiple biometric matchers has been not
thoroughly investigated so far. However, this approach exhibits some advantages
with respect to the widely adopted parallel approaches. In this paper, we
propose a novel theoretical framework for the assessment of performance of such
systems, based on a previous work of the authors. Benefits in terms of
performance are theoretically evaluated, as well as estimation errors in the
model parameters computation. Model is analyzed from the viewpoint of its pros
and cons, by mean of preliminary experiments performed on NIST Biometric Score
Set 1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13432">Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond. (arXiv:2401.13432v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1">Lang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chunyu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1">Kang Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shuaicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a></p>
<p>Thin-plate spline (TPS) is a principal warp that allows for representing
elastic, nonlinear transformation with control point motions. With the increase
of control points, the warp becomes increasingly flexible but usually
encounters a bottleneck caused by undesired issues, e.g., content distortion.
In this paper, we explore generic applications of TPS in single-image-based
warping tasks, such as rotation correction, rectangling, and portrait
correction. To break this bottleneck, we propose the coupled thin-plate spline
model (CoupledTPS), which iteratively couples multiple TPS with limited control
points into a more flexible and powerful transformation. Concretely, we first
design an iterative search to predict new control points according to the
current latent condition. Then, we present the warping flow as a bridge for the
coupling of different TPS transformations, effectively eliminating
interpolation errors caused by multiple warps. Besides, in light of the
laborious annotation cost, we develop a semi-supervised learning scheme to
improve warping quality by exploiting unlabeled data. It is formulated through
dual transformation between the searched control points of unlabeled data and
its graphic augmentation, yielding an implicit correction consistency
constraint. Finally, we collect massive unlabeled data to exhibit the benefit
of our semi-supervised scheme in rotation correction. Extensive experiments
demonstrate the superiority and universality of CoupledTPS over the existing
state-of-the-art (SoTA) solutions for rotation correction and beyond. The code
and data will be available at https://github.com/nie-lang/CoupledTPS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13472">Segmenting Cardiac Muscle Z-disks with Deep Neural Networks. (arXiv:2401.13472v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ibrahim_M/0/1/0/all/0/1">Mihaela Croitor Ibrahim</a>, <a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1">Nishant Ravikumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Curd_A/0/1/0/all/0/1">Alistair Curd</a>, <a href="http://arxiv.org/find/eess/1/au:+Leng_J/0/1/0/all/0/1">Joanna Leng</a>, <a href="http://arxiv.org/find/eess/1/au:+Umney_O/0/1/0/all/0/1">Oliver Umney</a>, <a href="http://arxiv.org/find/eess/1/au:+Peckham_M/0/1/0/all/0/1">Michelle Peckham</a></p>
<p>Z-disks are complex structures that delineate repeating sarcomeres in
striated muscle. They play significant roles in cardiomyocytes such as
providing mechanical stability for the contracting sarcomere, cell signalling
and autophagy. Changes in Z-disk architecture have been associated with
impaired cardiac function. Hence, there is a strong need to create tools to
segment Z-disks from microscopy images, that overcome traditional limitations
such as variability in image brightness and staining technique. In this study,
we apply deep learning based segmentation models to extract Z-disks in images
of striated muscle tissue. We leverage a novel Airyscan confocal dataset, which
comprises high resolution images of Z-disks of healthy heart tissue, stained
with Affimers for specific Z-disk proteins. We employed an interactive
labelling tool, Ilastik to obtain ground truth segmentation masks and use the
resulting data set to train and evaluate the performance of several
state-of-the-art segmentation networks. On the test set, UNet++ achieves best
segmentation performance for Z-disks in cardiomyocytes, with an average Dice
score of 0.91 and outperforms other established segmentation methods including
UNet, FPN, DeepLabv3+ and pix2pix. However, pix2pix demonstrates improved
generalisation, when tested on an additional dataset of cardiomyocytes with a
titin mutation. This is the first study to demonstrate that automated machine
learning-based segmentation approaches may be used effectively to segment
Z-disks in confocal microscopy images. Automated segmentation approaches and
predicted segmentation masks could be used to derive morphological features of
Z-disks (e.g. width and orientation), and subsequently, to quantify
disease-related changes to cardiac microstructure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13478">SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yiming Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaijing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chenghao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoran Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bohao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1">Noura Al Moubayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Multi-modal information retrieval (MMIR) is a rapidly evolving field, where
significant progress, particularly in image-text pairing, has been made through
advanced representation learning and cross-modality alignment research.
However, current benchmarks for evaluating MMIR performance in image-text
pairing within the scientific domain show a notable gap, where chart and table
images described in scholarly language usually do not play a significant role.
To bridge this gap, we develop a specialised scientific MMIR (SciMMIR)
benchmark by leveraging open-access paper collections to extract data relevant
to the scientific domain. This benchmark comprises 530K meticulously curated
image-text pairs, extracted from figures and tables with detailed captions in
scientific documents. We further annotate the image-text pairs with two-level
subset-subcategory hierarchy annotations to facilitate a more comprehensive
evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations
on prominent multi-modal image-captioning and visual language models, such as
CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific
domain, including the impact of pre-training and fine-tuning settings and the
influence of the visual and textual encoders. All our data and checkpoints are
publicly available at https://github.com/Wusiwei0410/SciMMIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13499">LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning. (arXiv:2401.13499v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Maofa Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bingchen Yan</a></p>
<p>Few-shot image classification has emerged as a key challenge in the field of
computer vision, highlighting the capability to rapidly adapt to new tasks with
minimal labeled data. Existing methods predominantly rely on image-level
features or local descriptors, often overlooking the holistic context
surrounding these descriptors. In this work, we introduce a novel approach
termed "Local Descriptor with Contextual Augmentation (LDCA)". Specifically,
this method bridges the gap between local and global understanding uniquely by
leveraging an adaptive global contextual enhancement module. This module
incorporates a visual transformer, endowing local descriptors with contextual
awareness capabilities, ranging from broad global perspectives to intricate
surrounding nuances. By doing so, LDCA transcends traditional descriptor-based
approaches, ensuring each local feature is interpreted within its larger visual
narrative. Extensive experiments underscore the efficacy of our method, showing
a maximal absolute improvement of 20\% over the next-best on fine-grained
classification datasets, thus demonstrating significant advancements in
few-shot classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13503">Learning Representations for Clustering via Partial Information Discrimination and Cross-Level Interaction. (arXiv:2401.13503v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hai-Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Hua-Bao Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guang-Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wei-jun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zi-hao Wen</a></p>
<p>In this paper, we present a novel deep image clustering approach termed PICI,
which enforces the partial information discrimination and the cross-level
interaction in a joint learning framework. In particular, we leverage a
Transformer encoder as the backbone, through which the masked image modeling
with two paralleled augmented views is formulated. After deriving the class
tokens from the masked images by the Transformer encoder, three partial
information learning modules are further incorporated, including the PISD
module for training the auto-encoder via masked image reconstruction, the PICD
module for employing two levels of contrastive learning, and the CLI module for
mutual interaction between the instance-level and cluster-level subspaces.
Extensive experiments have been conducted on six real-world image datasets,
which demononstrate the superior clustering performance of the proposed PICI
approach over the state-of-the-art deep clustering approaches. The source code
is available at https://github.com/Regan-Zhang/PICI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13504">Research about the Ability of LLM in the Tamper-Detection Area. (arXiv:2401.13504v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jizhe Zhou</a></p>
<p>In recent years, particularly since the early 2020s, Large Language Models
(LLMs) have emerged as the most powerful AI tools in addressing a diverse range
of challenges, from natural language processing to complex problem-solving in
various domains. In the field of tamper detection, LLMs are capable of
identifying basic tampering activities.To assess the capabilities of LLMs in
more specialized domains, we have collected five different LLMs developed by
various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This
diverse range of models allows for a comprehensive evaluation of their
performance in detecting sophisticated tampering instances.We devised two
domains of detection: AI-Generated Content (AIGC) detection and manipulation
detection. AIGC detection aims to test the ability to distinguish whether an
image is real or AI-generated. Manipulation detection, on the other hand,
focuses on identifying tampered images. According to our experiments, most LLMs
can identify composite pictures that are inconsistent with logic, and only more
powerful LLMs can distinguish logical, but visible signs of tampering to the
human eye. All of the LLMs can't identify carefully forged images and very
realistic images generated by AI. In the area of tamper detection, LLMs still
have a long way to go, particularly in reliably identifying highly
sophisticated forgeries and AI-generated images that closely mimic reality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13505">Generative Human Motion Stylization in Latent Space. (arXiv:2401.13505v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1">Yuxuan Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1">Xinxin Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1">Peng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Youliang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Juwei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Li Cheng</a></p>
<p>Human motion stylization aims to revise the style of an input motion while
keeping its content unaltered. Unlike existing works that operate directly in
pose space, we leverage the latent space of pretrained autoencoders as a more
expressive and robust representation for motion extraction and infusion.
Building upon this, we present a novel generative model that produces diverse
stylization results of a single motion (latent) code. During training, a motion
code is decomposed into two coding components: a deterministic content code,
and a probabilistic style code adhering to a prior distribution; then a
generator massages the random combination of content and style codes to
reconstruct the corresponding motion codes. Our approach is versatile, allowing
the learning of probabilistic style space from either style labeled or
unlabeled motions, providing notable flexibility in stylization as well. In
inference, users can opt to stylize a motion using style cues from a reference
motion or a label. Even in the absence of explicit style input, our model
facilitates novel re-stylization by sampling from the unconditional style prior
distribution. Experimental results show that our proposed stylization models,
despite their lightweight design, outperform the state-of-the-arts in style
reeanactment, content preservation, and generalization across various
applications and settings. Project Page: https://yxmu.foo/GenMoStyle
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13511">Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images. (arXiv:2401.13511v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lucassen_R/0/1/0/all/0/1">Ruben T. Lucassen</a>, <a href="http://arxiv.org/find/eess/1/au:+Blokx_W/0/1/0/all/0/1">Willeke A. M. Blokx</a>, <a href="http://arxiv.org/find/eess/1/au:+Veta_M/0/1/0/all/0/1">Mitko Veta</a></p>
<p>Tissue segmentation is a routine preprocessing step to reduce the
computational cost of whole slide image (WSI) analysis by excluding background
regions. Traditional image processing techniques are commonly used for tissue
segmentation, but often require manual adjustments to parameter values for
atypical cases, fail to exclude all slide and scanning artifacts from the
background, and are unable to segment adipose tissue. Pen marking artifacts in
particular can be a potential source of bias for subsequent analyses if not
removed. In addition, several applications require the separation of individual
cross-sections, which can be challenging due to tissue fragmentation and
adjacent positioning. To address these problems, we develop a convolutional
neural network for tissue and pen marking segmentation using a dataset of 200
H&amp;E stained WSIs. For separating tissue cross-sections, we propose a novel
post-processing method based on clustering predicted centroid locations of the
cross-sections in a 2D histogram. On an independent test set, the model
achieved a mean Dice score of 0.981$\pm$0.033 for tissue segmentation and a
mean Dice score of 0.912$\pm$0.090 for pen marking segmentation. The mean
absolute difference between the number of annotated and separated
cross-sections was 0.075$\pm$0.350. Our results demonstrate that the proposed
model can accurately segment H&amp;E stained tissue cross-sections and pen markings
in WSIs while being robust to many common slide and scanning artifacts. The
model with trained model parameters and post-processing method are made
publicly available as a Python package called SlideSegmenter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13516">Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces. (arXiv:2401.13516v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Juan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1">Xin Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1">Difei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1">Satoshi Tsutsui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>Deepfake videos are becoming increasingly realistic, showing subtle tampering
traces on facial areasthat vary between frames. Consequently, many existing
Deepfake detection methods struggle to detect unknown domain Deepfake videos
while accurately locating the tampered region. To address thislimitation, we
propose Delocate, a novel Deepfake detection model that can both recognize
andlocalize unknown domain Deepfake videos. Ourmethod consists of two stages
named recoveringand localization. In the recovering stage, the modelrandomly
masks regions of interest (ROIs) and reconstructs real faces without tampering
traces, resulting in a relatively good recovery effect for realfaces and a poor
recovery effect for fake faces. Inthe localization stage, the output of the
recoveryphase and the forgery ground truth mask serve assupervision to guide
the forgery localization process. This process strategically emphasizes the
recovery phase of fake faces with poor recovery, facilitating the localization
of tampered regions. Ourextensive experiments on four widely used benchmark
datasets demonstrate that Delocate not onlyexcels in localizing tampered areas
but also enhances cross-domain detection performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13531">QAGait: Revisit Gait Recognition from a Quality Perspective. (arXiv:2401.13531v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zengbin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1">Saihui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Man Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Chunshui Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongzhen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peipei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shibiao Xu</a></p>
<p>Gait recognition is a promising biometric method that aims to identify
pedestrians from their unique walking patterns. Silhouette modality, renowned
for its easy acquisition, simple structure, sparse representation, and
convenient modeling, has been widely employed in controlled in-the-lab
research. However, as gait recognition rapidly advances from in-the-lab to
in-the-wild scenarios, various conditions raise significant challenges for
silhouette modality, including 1) unidentifiable low-quality silhouettes
(abnormal segmentation, severe occlusion, or even non-human shape), and 2)
identifiable but challenging silhouettes (background noise, non-standard
posture, slight occlusion). To address these challenges, we revisit gait
recognition pipeline and approach gait recognition from a quality perspective,
namely QAGait. Specifically, we propose a series of cost-effective quality
assessment strategies, including Maxmial Connect Area and Template Match to
eliminate background noises and unidentifiable silhouettes, Alignment strategy
to handle non-standard postures. We also propose two quality-aware loss
functions to integrate silhouette quality into optimization within the
embedding space. Extensive experiments demonstrate our QAGait can guarantee
both gait reliability and performance enhancement. Furthermore, our quality
assessment strategies can seamlessly integrate with existing gait datasets,
showcasing our superiority. Code is available at
https://github.com/wzb-bupt/QAGait.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13551">Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection. (arXiv:2401.13551v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Yongwei Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1">Chengjiang Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1">Pradipta Maji</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hongmin Cai</a></p>
<p>Without human annotations, a typical Unsupervised Video Anomaly Detection
(UVAD) method needs to train two models that generate pseudo labels for each
other. In previous work, the two models are closely entangled with each other,
and it is not known how to upgrade their method without modifying their
training framework significantly. Second, previous work usually adopts fixed
thresholding to obtain pseudo labels, however the user-specified threshold is
not reliable which inevitably introduces errors into the training process. To
alleviate these two problems, we propose a novel interleaved framework that
alternately trains a One-Class Classification (OCC) model and a
Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can
be easily replaced with other OCC or WS models, which facilitates our method to
upgrade with the most recent developments in both fields. For handling the
fixed thresholding problem, we break through the conventional cognitive
boundary and propose a weighted OCC model that can be trained on both normal
and abnormal data. We also propose an adaptive mechanism for automatically
finding the optimal threshold for the WS model in a loose to strict manner.
Experiments demonstrate that the proposed UVAD method outperforms previous
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13554">PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition. (arXiv:2401.13554v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brookes_O/0/1/0/all/0/1">Otto Brookes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1">Majid Mirmehdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Stephens_C/0/1/0/all/0/1">Colleen Stephens</a>, <a href="http://arxiv.org/find/cs/1/au:+Angedakin_S/0/1/0/all/0/1">Samuel Angedakin</a>, <a href="http://arxiv.org/find/cs/1/au:+Corogenes_K/0/1/0/all/0/1">Katherine Corogenes</a>, <a href="http://arxiv.org/find/cs/1/au:+Dowd_D/0/1/0/all/0/1">Dervla Dowd</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieguez_P/0/1/0/all/0/1">Paula Dieguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hicks_T/0/1/0/all/0/1">Thurston C. Hicks</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_S/0/1/0/all/0/1">Sorrel Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kevin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Leinert_V/0/1/0/all/0/1">Vera Leinert</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapuente_J/0/1/0/all/0/1">Juan Lapuente</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarthy_M/0/1/0/all/0/1">Maureen S. McCarthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Meier_A/0/1/0/all/0/1">Amelia Meier</a>, <a href="http://arxiv.org/find/cs/1/au:+Murai_M/0/1/0/all/0/1">Mizuki Murai</a>, <a href="http://arxiv.org/find/cs/1/au:+Normand_E/0/1/0/all/0/1">Emmanuelle Normand</a>, <a href="http://arxiv.org/find/cs/1/au:+Vergnes_V/0/1/0/all/0/1">Virginie Vergnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Wessling_E/0/1/0/all/0/1">Erin G. Wessling</a>, <a href="http://arxiv.org/find/cs/1/au:+Wittig_R/0/1/0/all/0/1">Roman M. Wittig</a>, <a href="http://arxiv.org/find/cs/1/au:+Langergraber_K/0/1/0/all/0/1">Kevin Langergraber</a>, <a href="http://arxiv.org/find/cs/1/au:+Maldonado_N/0/1/0/all/0/1">Nuria Maldonado</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuberbuhler_K/0/1/0/all/0/1">Klaus Zuberbuhler</a>, <a href="http://arxiv.org/find/cs/1/au:+Boesch_C/0/1/0/all/0/1">Christophe Boesch</a>, <a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_M/0/1/0/all/0/1">Mimi Arandjelovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_H/0/1/0/all/0/1">Hjalmar Kuhl</a>, <a href="http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1">Tilo Burghardt</a></p>
<p>We present the PanAf20K dataset, the largest and most diverse open-access
annotated video dataset of great apes in their natural environment. It
comprises more than 7 million frames across ~20,000 camera trap videos of
chimpanzees and gorillas collected at 18 field sites in tropical Africa as part
of the Pan African Programme: The Cultured Chimpanzee. The footage is
accompanied by a rich set of annotations and benchmarks making it suitable for
training and testing a variety of challenging and ecologically important
computer vision tasks including ape detection and behaviour recognition.
Furthering AI analysis of camera trap information is critical given the
International Union for Conservation of Nature now lists all species in the
great ape family as either Endangered or Critically Endangered. We hope the
dataset can form a solid basis for engagement of the AI community to improve
performance, efficiency, and result interpretation in order to support
assessments of great ape presence, abundance, distribution, and behaviour and
thereby aid conservation efforts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13555">Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1">Mike Laszkiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1">Imant Daunhawer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1">Asja Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1">Johannes Lederer</a></p>
<p>Recent years have witnessed a rapid development of deep generative models for
creating synthetic media, such as images and videos. While the practical
applications of these models in everyday tasks are enticing, it is crucial to
assess the inherent risks regarding their fairness. In this work, we introduce
a comprehensive framework for benchmarking the performance and fairness of
conditional generative models. We develop a set of
metrics$\unicode{x2013}$inspired by their supervised fairness
counterparts$\unicode{x2013}$to evaluate the models on their fairness and
diversity. Focusing on the specific application of image upsampling, we create
a benchmark covering a wide variety of modern upsampling methods. As part of
the benchmark, we introduce UnfairFace, a subset of FairFace that replicates
the racial distribution of common large-scale face datasets. Our empirical
study highlights the importance of using an unbiased training set and reveals
variations in how the algorithms respond to dataset imbalances. Alarmingly, we
find that none of the considered methods produces statistically fair and
diverse results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13560">SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation. (arXiv:2401.13560v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhaohu Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Tian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yijun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a></p>
<p>The Transformer architecture has shown a remarkable ability in modeling
global relationships. However, it poses a significant computational challenge
when processing high-dimensional medical images. This hinders its development
and widespread adoption in this task. Mamba, as a State Space Model (SSM),
recently emerged as a notable manner for long-range dependencies in sequential
modeling, excelling in natural language processing filed with its remarkable
memory efficiency and computational speed. Inspired by its success, we
introduce SegMamba, a novel 3D medical image \textbf{Seg}mentation
\textbf{Mamba} model, designed to effectively capture long-range dependencies
within whole volume features at every scale. Our SegMamba, in contrast to
Transformer-based methods, excels in whole volume feature modeling from a state
space model standpoint, maintaining superior processing speed, even with volume
features at a resolution of {$64\times 64\times 64$}. Comprehensive experiments
on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our
SegMamba. The code for SegMamba is available at:
https://github.com/ge-xing/SegMamba
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13581">Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation. (arXiv:2401.13581v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a></p>
<p>Previous contrastive deep clustering methods mostly focus on instance-level
information while overlooking the member relationship within groups/clusters,
which may significantly undermine their representation learning and clustering
capability. Recently, some group-contrastive methods have been developed,
which, however, typically rely on the samples of the entire dataset to obtain
pseudo labels and lack the ability to efficiently update the group assignments
in a batch-wise manner. To tackle these critical issues, we present a novel
end-to-end deep clustering framework with dynamic grouping and prototype
aggregation, termed as DigPro. Specifically, the proposed dynamic grouping
extends contrastive learning from instance-level to group-level, which is
effective and efficient for timely updating groups. Meanwhile, we perform
contrastive learning on prototypes in a spherical feature space, termed as
prototype aggregation, which aims to maximize the inter-cluster distance.
Notably, with an expectation-maximization framework, DigPro simultaneously
takes advantage of compact intra-cluster connections, well-separated clusters,
and efficient group updating during the self-supervised training. Extensive
experiments on six image benchmarks demonstrate the superior performance of our
approach over the state-of-the-art. Code is available at
https://github.com/Regan-Zhang/DigPro.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13596">PLATE: A perception-latency aware estimator,. (arXiv:2401.13596v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Aldana_Lopez_R/0/1/0/all/0/1">Rodrigo Aldana-L&#xf3;pez</a>, <a href="http://arxiv.org/find/eess/1/au:+Aragues_R/0/1/0/all/0/1">Rosario Arag&#xfc;&#xe9;s</a>, <a href="http://arxiv.org/find/eess/1/au:+Sagues_C/0/1/0/all/0/1">Carlos Sag&#xfc;&#xe9;s</a></p>
<p>Target tracking is a popular problem with many potential applications. There
has been a lot of effort on improving the quality of the detection of targets
using cameras through different techniques. In general, with higher
computational effort applied, i.e., a longer perception-latency, a better
detection accuracy is obtained. However, it is not always useful to apply the
longest perception-latency allowed, particularly when the environment doesn't
require to and when the computational resources are shared between other tasks.
In this work, we propose a new Perception-LATency aware Estimator (PLATE),
which uses different perception configurations in different moments of time in
order to optimize a certain performance measure. This measure takes into
account a perception-latency and accuracy trade-off aiming for a good
compromise between quality and resource usage. Compared to other heuristic
frame-skipping techniques, PLATE comes with a formal complexity and optimality
analysis. The advantages of PLATE are verified by several experiments including
an evaluation over a standard benchmark with real data and using state of the
art deep learning object detection methods for the perception stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13613">Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode. (arXiv:2401.13613v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lahajal_N/0/1/0/all/0/1">Naresh Kumar Lahajal</a>, <a href="http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1">Harini S</a></p>
<p>Photo search, the task of retrieving images based on textual queries, has
witnessed significant advancements with the introduction of CLIP (Contrastive
Language-Image Pretraining) model. CLIP leverages a vision-language pre
training approach, wherein it learns a shared representation space for images
and text, enabling cross-modal understanding. This model demonstrates the
capability to understand the semantic relationships between diverse image and
text pairs, allowing for efficient and accurate retrieval of images based on
natural language queries. By training on a large-scale dataset containing
images and their associated textual descriptions, CLIP achieves remarkable
generalization, providing a powerful tool for tasks such as zero-shot learning
and few-shot classification. This abstract summarizes the foundational
principles of CLIP and highlights its potential impact on advancing the field
of photo search, fostering a seamless integration of natural language
understanding and computer vision for improved information retrieval in
multimedia applications
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13616">FLLIC: Functionally Lossless Image Compression. (arXiv:2401.13616v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xiaolin Wu</a></p>
<p>Recently, DNN models for lossless image coding have surpassed their
traditional counterparts in compression performance, reducing the bit rate by
about ten percent for natural color images. But even with these advances,
mathematically lossless image compression (MLLIC) ratios for natural images
still fall short of the bandwidth and cost-effectiveness requirements of most
practical imaging and vision systems at present and beyond. To break the
bottleneck of MLLIC in compression performance, we question the necessity of
MLLIC, as almost all digital sensors inherently introduce acquisition noises,
making mathematically lossless compression counterproductive. Therefore, in
contrast to MLLIC, we propose a new paradigm of joint denoising and compression
called functionally lossless image compression (FLLIC), which performs lossless
compression of optimally denoised images (the optimality may be task-specific).
Although not literally lossless with respect to the noisy input, FLLIC aims to
achieve the best possible reconstruction of the latent noise-free original
image. Extensive experiments show that FLLIC achieves state-of-the-art
performance in joint denoising and compression of noisy images and does so at a
lower computational cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13627">Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild. (arXiv:2401.13627v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fanghua Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jinfan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xiangtao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingwen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chao Dong</a></p>
<p>We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image
restoration method that harnesses generative prior and the power of model
scaling up. Leveraging multi-modal techniques and advanced generative prior,
SUPIR marks a significant advance in intelligent and realistic image
restoration. As a pivotal catalyst within SUPIR, model scaling dramatically
enhances its capabilities and demonstrates new potential for image restoration.
We collect a dataset comprising 20 million high-resolution, high-quality images
for model training, each enriched with descriptive text annotations. SUPIR
provides the capability to restore images guided by textual prompts, broadening
its application scope and potential. Moreover, we introduce negative-quality
prompts to further improve perceptual quality. We also develop a
restoration-guided sampling method to suppress the fidelity issue encountered
in generative-based restoration. Experiments demonstrate SUPIR's exceptional
restoration effects and its novel capacity to manipulate restoration through
textual prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13641">How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DeAndres_Tame_I/0/1/0/all/0/1">Ivan DeAndres-Tame</a>, <a href="http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1">Ruben Tolosana</a>, <a href="http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1">Ruben Vera-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1">Aythami Morales</a>, <a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1">Julian Fierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1">Javier Ortega-Garcia</a></p>
<p>Large Language Models (LLMs) such as GPT developed by OpenAI, have already
shown astonishing results, introducing quick changes in our society. This has
been intensified by the release of ChatGPT which allows anyone to interact in a
simple conversational way with LLMs, without any experience in the field
needed. As a result, ChatGPT has been rapidly applied to many different tasks
such as code- and song-writer, education, virtual assistants, etc., showing
impressive results for tasks for which it was not trained (zero-shot learning).
</p>
<p>The present study aims to explore the ability of ChatGPT, based on the recent
GPT-4 multimodal LLM, for the task of face biometrics. In particular, we
analyze the ability of ChatGPT to perform tasks such as face verification,
soft-biometrics estimation, and explainability of the results. ChatGPT could be
very valuable to further increase the explainability and transparency of the
automatic decisions in human scenarios. Experiments are carried out in order to
evaluate the performance and robustness of ChatGPT, using popular public
benchmarks and comparing the results with state-of-the-art methods in the
field. The results achieved in this study show the potential of LLMs such as
ChatGPT for face biometrics, especially to enhance explainability. For
reproducibility reasons, we release all the code in GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13649">VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1">Jing Yu Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_R/0/1/0/all/0/1">Robert Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1">Lawrence Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duvvur_V/0/1/0/all/0/1">Vikram Duvvur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1">Ming Chong Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Po-Yu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shuyan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1">Daniel Fried</a></p>
<p>Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13650">Tyche: Stochastic In-Context Learning for Medical Image Segmentation. (arXiv:2401.13650v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rakic_M/0/1/0/all/0/1">Marianne Rakic</a>, <a href="http://arxiv.org/find/eess/1/au:+Wong_H/0/1/0/all/0/1">Hallee E. Wong</a>, <a href="http://arxiv.org/find/eess/1/au:+Ortiz_J/0/1/0/all/0/1">Jose Javier Gonzalez Ortiz</a>, <a href="http://arxiv.org/find/eess/1/au:+Cimini_B/0/1/0/all/0/1">Beth Cimini</a>, <a href="http://arxiv.org/find/eess/1/au:+Guttag_J/0/1/0/all/0/1">John Guttag</a>, <a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1">Adrian V. Dalca</a></p>
<p>Existing learning-based solutions to medical image segmentation have two
important shortcomings. First, for most new segmentation task, a new model has
to be trained or fine-tuned. This requires extensive resources and machine
learning expertise, and is therefore often infeasible for medical researchers
and clinicians. Second, most existing segmentation methods produce a single
deterministic segmentation mask for a given image. In practice however, there
is often considerable uncertainty about what constitutes the correct
segmentation, and different expert annotators will often segment the same image
differently. We tackle both of these problems with Tyche, a model that uses a
context set to generate stochastic predictions for previously unseen tasks
without the need to retrain. Tyche differs from other in-context segmentation
methods in two important ways. (1) We introduce a novel convolution block
architecture that enables interactions among predictions. (2) We introduce
in-context test-time augmentation, a new mechanism to provide prediction
stochasticity. When combined with appropriate model design and loss functions,
Tyche can predict a set of plausible diverse segmentation candidates for new or
unseen medical images and segmentation tasks without the need to retrain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13666">Algebraic methods for solving recognition problems with non-crossing classes. (arXiv:2401.13666v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabulov_A/0/1/0/all/0/1">Anvar Kabulov</a>, <a href="http://arxiv.org/find/cs/1/au:+Babadzhanov_A/0/1/0/all/0/1">Alimdzhan Babadzhanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Saymanov_I/0/1/0/all/0/1">Islambek Saymanov</a></p>
<p>In this paper, we propose to consider various models of pattern recognition.
At the same time, it is proposed to consider models in the form of two
operators: a recognizing operator and a decision rule. Algebraic operations are
introduced on recognizing operators, and based on the application of these
operators, a family of recognizing algorithms is created. An upper estimate is
constructed for the model, which guarantees the completeness of the extension.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1811.08075">Scene Graph Generation via Conditional Random Fields. (arXiv:1811.08075v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1">Weilin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wang-Chien Lee</a></p>
<p>Despite the great success object detection and segmentation models have
achieved in recognizing individual objects in images, performance on cognitive
tasks such as image caption, semantic image retrieval, and visual QA is far
from satisfactory. To achieve better performance on these cognitive tasks,
merely recognizing individual object instances is insufficient. Instead, the
interactions between object instances need to be captured in order to
facilitate reasoning and understanding of the visual scenes in an image. Scene
graph, a graph representation of images that captures object instances and
their relationships, offers a comprehensive understanding of an image. However,
existing techniques on scene graph generation fail to distinguish subjects and
objects in the visual scenes of images and thus do not perform well with
real-world datasets where exist ambiguous object instances. In this work, we
propose a novel scene graph generation model for predicting object instances
and its corresponding relationships in an image. Our model, SG-CRF, learns the
sequential order of subject and object in a relationship triplet, and the
semantic compatibility of object instance nodes and relationship nodes in a
scene graph efficiently. Experiments empirically show that SG-CRF outperforms
the state-of-the-art methods, on three different datasets, i.e., CLEVR, VRD,
and Visual Genome, raising the Recall@100 from 24.99% to 49.95%, from 41.92% to
50.47%, and from 54.69% to 54.77%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.13694">A fast horizon detector and a new annotated dataset for maritime video processing. (arXiv:2110.13694v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zardoua_Y/0/1/0/all/0/1">Yassir Zardoua</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammed_B/0/1/0/all/0/1">Boulaala Mohammed</a>, <a href="http://arxiv.org/find/cs/1/au:+Mrabet_M/0/1/0/all/0/1">Mhamed El Mrabet</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1">Astito Abdelali</a></p>
<p>Accurate and fast sea horizon detection is vital for tasks in autonomous
navigation and maritime security, such as video stabilization, target region
reduction, precise tracking, and obstacle avoidance. This paper introduces a
novel sea horizon detector from RGB videos, focusing on rapid and effective sea
noise suppression while preserving weak horizon edges. Line fitting methods are
subsequently employed on filtered edges for horizon detection. We address the
filtering problem by extracting line segments with a very low edge threshold,
ensuring the detection of line segments even in low-contrast horizon
conditions. We show that horizon line segments have simple and relevant
properties in RGB images, which we exploit to suppress noisy segments. Then we
use the surviving segments to construct a filtered edge map and infer the
horizon from the filtered edges. We propose a careful incorporation of temporal
information for horizon inference and experimentally show its effectiveness. We
address the computational constraint by providing a vectorized implementation
for efficient CPU execution, and leveraging image downsizing with minimal loss
of accuracy on the original size. Moreover, we contribute a public horizon line
dataset to enrich existing data resources. Our algorithm's performance is
rigorously evaluated against state-of-the-art methods, and its components are
validated through ablation experiments. Source code and dataset files are
available at:
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.13883">Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1">Sara Abdali</a>, <a href="http://arxiv.org/find/cs/1/au:+shaham_S/0/1/0/all/0/1">Sina shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1">Bhaskar Krishnamachari</a></p>
<p>As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.05967">Target Aware Network Architecture Search and Compression for Efficient Knowledge Transfer. (arXiv:2205.05967v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Basha_S/0/1/0/all/0/1">S.H.Shabbeer Basha</a>, <a href="http://arxiv.org/find/cs/1/au:+Tula_D/0/1/0/all/0/1">Debapriya Tula</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinakota_S/0/1/0/all/0/1">Sravan Kumar Vinakota</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1">Shiv Ram Dubey</a></p>
<p>Transfer Learning enables Convolutional Neural Networks (CNN) to acquire
knowledge from a source domain and transfer it to a target domain, where
collecting large-scale annotated examples is time-consuming and expensive.
Conventionally, while transferring the knowledge learned from one task to
another task, the deeper layers of a pre-trained CNN are finetuned over the
target dataset. However, these layers are originally designed for the source
task which may be over-parameterized for the target task. Thus, finetuning
these layers over the target dataset may affect the generalization ability of
the CNN due to high network complexity. To tackle this problem, we propose a
two-stage framework called TASCNet which enables efficient knowledge transfer.
In the first stage, the configuration of the deeper layers is learned
automatically and finetuned over the target dataset. Later, in the second
stage, the redundant filters are pruned from the fine-tuned CNN to decrease the
network's complexity for the target task while preserving the performance. This
two-stage mechanism finds a compact version of the pre-trained CNN with optimal
structure (number of filters in a convolutional layer, number of neurons in a
dense layer, and so on) from the hypothesis space. The efficacy of the proposed
method is evaluated using VGG-16, ResNet-50, and DenseNet-121 on CalTech-101,
CalTech-256, and Stanford Dogs datasets. Similar to computer vision tasks, we
have also conducted experiments on Movie Review Sentiment Analysis task. The
proposed TASCNet reduces the computational complexity of pre-trained CNNs over
the target task by reducing both trainable parameters and FLOPs which enables
resource-efficient knowledge transfer. The source code is available at:
https://github.com/Debapriya-Tula/TASCNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04625">Soft Augmentation for Image Classification. (arXiv:2211.04625v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1">Laura Leal-Taix&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1">James Hays</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a></p>
<p>Modern neural networks are over-parameterized and thus rely on strong
regularization such as data augmentation and weight decay to reduce overfitting
and improve generalization. The dominant form of data augmentation applies
invariant transforms, where the learning target of a sample is invariant to the
transform applied to that sample. We draw inspiration from human visual
classification studies and propose generalizing augmentation with invariant
transforms to soft augmentation where the learning target softens non-linearly
as a function of the degree of the transform applied to the sample: e.g., more
aggressive image crop augmentations produce less confident learning targets. We
demonstrate that soft targets allow for more aggressive data augmentation,
offer more robust performance boosts, work with other augmentation policies,
and interestingly, produce better calibrated models (since they are trained to
be less confident on aggressively cropped/occluded examples). Combined with
existing aggressive augmentation strategies, soft target 1) doubles the top-1
accuracy boost across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2, 2)
improves model occlusion performance by up to $4\times$, and 3) halves the
expected calibration error (ECE). Finally, we show that soft augmentation
generalizes to self-supervised classification tasks. Code available at
https://github.com/youngleox/soft_augmentation
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10227">Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Windeatt_T/0/1/0/all/0/1">T. Windeatt</a></p>
<p>A new method of detecting adversarial attacks is proposed for an ensemble of
Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The
ensemble is combined using Walsh coefficients which are capable of
approximating Boolean functions and thereby controlling the complexity of the
ensemble decision boundary. The hypothesis in this paper is that decision
boundaries with high curvature allow adversarial perturbations to be found, but
change the curvature of the decision boundary, which is then approximated in a
different way by Walsh coefficients compared to the clean images. By observing
the difference in Walsh coefficient approximation between clean and adversarial
images, it is shown experimentally that transferability of attack may be used
for detection. Furthermore, approximating the decision boundary may aid in
understanding the learning and transferability properties of DNNs. While the
experiments here use images, the proposed approach of modelling two-class
ensemble decision boundaries could in principle be applied to any application
area. Code for approximating Boolean functions using Walsh coefficients:
https://doi.org/10.24433/CO.3695905.v1
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.12343">Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems. (arXiv:2211.12343v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1">Xiangming Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabashima_Y/0/1/0/all/0/1">Yoshiyuki Kabashima</a></p>
<p>We consider the ubiquitous linear inverse problems with additive Gaussian
noise and propose an unsupervised sampling approach called diffusion model
based posterior sampling (DMPS) to reconstruct the unknown signal from noisy
linear measurements. Specifically, using one diffusion model (DM) as an
implicit prior, the fundamental difficulty in performing posterior sampling is
that the noise-perturbed likelihood score, i.e., gradient of an annealed
likelihood function, is intractable. To circumvent this problem, we introduce a
simple yet effective closed-form approximation using an uninformative prior
assumption. Extensive experiments are conducted on a variety of noisy linear
inverse problems such as noisy super-resolution, denoising, deblurring, and
colorization. In all tasks, the proposed DMPS demonstrates highly competitive
or even better performances on various tasks while being 3 times faster than
the state-of-the-art competitor diffusion posterior sampling (DPS).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02803">Tensor PCA from basis in tensor space. (arXiv:2305.02803v3 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Turchetti_C/0/1/0/all/0/1">Claudio Turchetti</a>, <a href="http://arxiv.org/find/math/1/au:+Falaschetti_L/0/1/0/all/0/1">Laura Falaschetti</a></p>
<p>The aim of this paper is to present a mathematical framework for tensor PCA.
The proposed approach is able to overcome the limitations of previous methods
that extract a low dimensional subspace by iteratively solving an optimization
problem. The core of the proposed approach is the derivation of a basis in
tensor space from a real self-adjoint tensor operator, thus reducing the
problem of deriving a basis to an eigenvalue problem. Three different cases
have been studied to derive: i) a basis from a self-adjoint tensor operator;
ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence
between eigenvalue equation for a real self-adjoint tensor operator and
standard matrix eigenvalue equation has been proven. For all the three cases
considered, a subspace approach has been adopted to derive a tensor PCA.
Experiments on image datasets validate the proposed mathematical framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08891">Common Diffusion Noise Schedules and Sample Steps are Flawed. (arXiv:2305.08891v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shanchuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiashi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a></p>
<p>We discover that common diffusion noise schedules do not enforce the last
timestep to have zero signal-to-noise ratio (SNR), and some implementations of
diffusion samplers do not start from the last timestep. Such designs are flawed
and do not reflect the fact that the model is given pure Gaussian noise at
inference, creating a discrepancy between training and inference. We show that
the flawed design causes real problems in existing implementations. In Stable
Diffusion, it severely limits the model to only generate images with medium
brightness and prevents it from generating very bright and dark samples. We
propose a few simple fixes: (1) rescale the noise schedule to enforce zero
terminal SNR; (2) train the model with v prediction; (3) change the sampler to
always start from the last timestep; (4) rescale classifier-free guidance to
prevent over-exposure. These simple changes ensure the diffusion process is
congruent between training and inference and allow the model to generate
samples more faithful to the original data distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17369">Modularized Zero-shot VQA with Pre-trained Models. (arXiv:2305.17369v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1">Rui Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a></p>
<p>Large-scale pre-trained models (PTMs) show great zero-shot capabilities. In
this paper, we study how to leverage them for zero-shot visual question
answering (VQA). Our approach is motivated by a few observations. First, VQA
questions often require multiple steps of reasoning, which is still a
capability that most PTMs lack. Second, different steps in VQA reasoning chains
require different skills such as object detection and relational reasoning, but
a single PTM may not possess all these skills. Third, recent work on zero-shot
VQA does not explicitly consider multi-step reasoning chains, which makes them
less interpretable compared with a decomposition-based approach. We propose a
modularized zero-shot network that explicitly decomposes questions into sub
reasoning steps and is highly interpretable. We convert sub reasoning tasks to
acceptable objectives of PTMs and assign tasks to proper PTMs without any
adaptation. Our experiments on two VQA benchmarks under the zero-shot setting
demonstrate the effectiveness of our method and better interpretability
compared with several baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08013">TopP&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1">Pum Jun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yoojin Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jisu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1">Jaejun Yoo</a></p>
<p>We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&amp;R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&amp;R, pronounced 'topper'), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&amp;R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&amp;R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08877">Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. (arXiv:2306.08877v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rassin_R/0/1/0/all/0/1">Royi Rassin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1">Eran Hirsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Glickman_D/0/1/0/all/0/1">Daniel Glickman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1">Shauli Ravfogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a></p>
<p>Text-conditioned image generation models often generate incorrect
associations between entities and their visual attributes. This reflects an
impaired mapping between linguistic binding of entities and modifiers in the
prompt and visual binding of the corresponding elements in the generated image.
As one notable example, a query like "a pink sunflower and a yellow flamingo"
may incorrectly produce an image of a yellow sunflower and a pink flamingo. To
remedy this issue, we propose SynGen, an approach which first syntactically
analyses the prompt to identify entities and their modifiers, and then uses a
novel loss function that encourages the cross-attention maps to agree with the
linguistic binding reflected by the syntax. Specifically, we encourage large
overlap between attention maps of entities and their modifiers, and small
overlap with other entities and modifier words. The loss is optimized during
inference, without retraining or fine-tuning the model. Human evaluation on
three datasets, including one new and challenging set, demonstrate significant
improvements of SynGen compared with current state of the art methods. This
work highlights how making use of sentence structure during inference can
efficiently and substantially improve the faithfulness of text-to-image
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15142">LRANet: Towards Accurate and Efficient Scene Text Detection with Low-Rank Approximation Network. (arXiv:2306.15142v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yuchen Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhineng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhiwen Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yuning Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Zhilong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jinfeng Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Recently, regression-based methods, which predict parameterized text shapes
for text localization, have gained popularity in scene text detection. However,
the existing parameterized text shape methods still have limitations in
modeling arbitrary-shaped texts due to ignoring the utilization of
text-specific shape information. Moreover, the time consumption of the entire
pipeline has been largely overlooked, leading to a suboptimal overall inference
speed. To address these issues, we first propose a novel parameterized text
shape method based on low-rank approximation. Unlike other shape representation
methods that employ data-irrelevant parameterization, our approach utilizes
singular value decomposition and reconstructs the text shape using a few
eigenvectors learned from labeled text contours. By exploring the shape
correlation among different text contours, our method achieves consistency,
compactness, simplicity, and robustness in shape representation. Next, we
propose a dual assignment scheme for speed acceleration. It adopts a sparse
assignment branch to accelerate the inference speed, and meanwhile, provides
ample supervised signals for training through a dense assignment branch.
Building upon these designs, we implement an accurate and efficient
arbitrary-shaped text detector named LRANet. Extensive experiments are
conducted on several challenging benchmarks, demonstrating the superior
accuracy and efficiency of LRANet compared to state-of-the-art methods. Code is
available at: \url{https://github.com/ychensu/LRANet.git}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06082">VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1">Raphael Schumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wanrong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Weixi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1">Tsu-Jui Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1">Stefan Riezler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10623">GaitPT: Skeletons Are All You Need For Gait Recognition. (arXiv:2308.10623v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Catruna_A/0/1/0/all/0/1">Andy Catruna</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1">Adrian Cosma</a>, <a href="http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1">Emilian Radoi</a></p>
<p>The analysis of patterns of walking is an important area of research that has
numerous applications in security, healthcare, sports and human-computer
interaction. Lately, walking patterns have been regarded as a unique
fingerprinting method for automatic person identification at a distance. In
this work, we propose a novel gait recognition architecture called Gait Pyramid
Transformer (GaitPT) that leverages pose estimation skeletons to capture unique
walking patterns, without relying on appearance information. GaitPT adopts a
hierarchical transformer architecture that effectively extracts both spatial
and temporal features of movement in an anatomically consistent manner, guided
by the structure of the human skeleton. Our results show that GaitPT achieves
state-of-the-art performance compared to other skeleton-based gait recognition
works, in both controlled and in-the-wild scenarios. GaitPT obtains 82.6%
average accuracy on CASIA-B, surpassing other works by a margin of 6%.
Moreover, it obtains 52.16% Rank-1 accuracy on GREW, outperforming both
skeleton-based and appearance-based approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01171">Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction. (arXiv:2309.01171v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lei_P/0/1/0/all/0/1">Pengcheng Lei</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_F/0/1/0/all/0/1">Faming Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1">Guixu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Ming Xu</a></p>
<p>Magnetic resonance imaging (MRI) tasks often involve multiple contrasts.
Recently, numerous deep learning-based multi-contrast MRI super-resolution (SR)
and reconstruction methods have been proposed to explore the complementary
information from the multi-contrast images. However, these methods either
construct parameter-sharing networks or manually design fusion rules, failing
to accurately model the correlations between multi-contrast images and lacking
certain interpretations. In this paper, we propose a multi-contrast
convolutional dictionary (MC-CDic) model under the guidance of the optimization
algorithm with a well-designed data fidelity term. Specifically, we bulid an
observation model for the multi-contrast MR images to explicitly model the
multi-contrast images as common features and unique features. In this way, only
the useful information in the reference image can be transferred to the target
image, while the inconsistent information will be ignored. We employ the
proximal gradient algorithm to optimize the model and unroll the iterative
steps into a deep CDic model. Especially, the proximal operators are replaced
by learnable ResNet. In addition, multi-scale dictionaries are introduced to
further improve the model performance. We test our MC-CDic model on
multi-contrast MRI SR and reconstruction tasks. Experimental results
demonstrate the superior performance of the proposed MC-CDic model against
existing SOTA methods. Code is available at
https://github.com/lpcccc-cv/MC-CDic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04447">Impact of Blur and Resolution on Demographic Disparities in 1-to-Many Facial Identification. (arXiv:2309.04447v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhatta_A/0/1/0/all/0/1">Aman Bhatta</a>, <a href="http://arxiv.org/find/cs/1/au:+Pangelinan_G/0/1/0/all/0/1">Gabriella Pangelinan</a>, <a href="http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1">Michael C. King</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1">Kevin W. Bowyer</a></p>
<p>Most studies to date that have examined demographic variations in face
recognition accuracy have analyzed 1-to-1 matching accuracy, using images that
could be described as "government ID quality". This paper analyzes the accuracy
of 1-to-many facial identification across demographic groups, and in the
presence of blur and reduced resolution in the probe image as might occur in
"surveillance camera quality" images. Cumulative match characteristic curves
(CMC) are not appropriate for comparing propensity for rank-one recognition
errors across demographics, and so we use three metrics for our analysis: (1)
the well-known d' metric between mated and non-mated score distributions, and
introduced in this work, (2) absolute score difference between thresholds in
the high-similarity tail of the non-mated and the low-similarity tail of the
mated distribution, and (3) distribution of (mated - non-mated rank-one scores)
across the set of probe images. We find that demographic variation in 1-to-many
accuracy does not entirely follow what has been observed in 1-to-1 matching
accuracy. Also, different from 1-to-1 accuracy, demographic comparison of
1-to-many accuracy can be affected by different numbers of identities and
images across demographics. More importantly, we show that increased blur in
the probe image, or reduced resolution of the face in the probe image, can
significantly increase the false positive identification rate. And we show that
the demographic variation in these high blur or low resolution conditions is
much larger for male / female than for African-American / Caucasian. The point
that 1-to-many accuracy can potentially collapse in the context of processing
"surveillance camera quality" probe images against a "government ID quality"
gallery is an important one.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07254">Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement. (arXiv:2309.07254v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dake Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1">Peter A. Beerel</a></p>
<p>While diffusion models demonstrate a remarkable capability for generating
high-quality images, their tendency to `replicate' training data raises privacy
concerns. Although recent research suggests that this replication may stem from
the insufficient generalization of training data captions and duplication of
training images, effective mitigation strategies remain elusive. To address
this gap, our paper first introduces a generality score that measures the
caption generality and employ large language model (LLM) to generalize training
captions. Subsequently, we leverage generalized captions and propose a novel
dual fusion enhancement approach to mitigate the replication of diffusion
models. Our empirical results demonstrate that our proposed methods can
significantly reduce replication by 43.5% compared to the original diffusion
model while maintaining the diversity and quality of generations. Code is
available at https://github.com/HowardLi0816/dual-fusion-diffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15508">DreamCom: Finetuning Text-guided Inpainting Model for Image Composition. (arXiv:2309.15508v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Lingxiao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangtong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Li Niu</a></p>
<p>The goal of image composition is merging a foreground object into a
background image to obtain a realistic composite image. Recently, generative
composition methods are built on large pretrained diffusion models, due to
their unprecedented image generation ability. However, they are weak in
preserving the foreground object details. Inspired by recent text-to-image
generation customized for certain object, we propose DreamCom by treating image
composition as text-guided image inpainting customized for certain object.
Specifically , we finetune pretrained text-guided image inpainting model based
on a few reference images containing the same object, during which the text
prompt contains a special token associated with this object. Then, given a new
background, we can insert this object into the background with the text prompt
containing the special token. In practice, the inserted object may be adversely
affected by the background, so we propose masked attention mechanisms to avoid
negative background interference. Experimental results on DreamEditBench and
our contributed MureCom dataset show the outstanding performance of our
DreamCom.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01403">CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction. (arXiv:2310.01403v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Size Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lumin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Sheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wentao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>Open-vocabulary dense prediction tasks including object detection and image
segmentation have been advanced by the success of Contrastive Language-Image
Pre-training (CLIP). CLIP models, particularly those incorporating vision
transformers (ViTs), have exhibited remarkable generalization ability in
zero-shot image classification. However, when transferring the vision-language
alignment of CLIP from global image representation to local region
representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer
from the domain shift from full images to local image regions. In this paper,
we embark on an in-depth analysis of the region-language alignment in CLIP
models, which is essential for downstream open-vocabulary dense prediction
tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the
image-level recognition ability of CLIP ViT to local image regions without
needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by
aligning a region representation extracted from its dense feature map with the
image-level representation of the corresponding image crop. With the enhanced
CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary
object detection, semantic segmentation, and panoptic segmentation across
various benchmarks. Models and code are released at
https://github.com/wusize/CLIPSelf.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03658">Visual inspection for illicit items in X-ray images using Deep Learning. (arXiv:2310.03658v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1">Ioannis Mademlis</a>, <a href="http://arxiv.org/find/cs/1/au:+Batsis_G/0/1/0/all/0/1">Georgios Batsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chrysochoou_A/0/1/0/all/0/1">Adamantia Anna Rebolledo Chrysochoou</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_G/0/1/0/all/0/1">Georgios Th. Papadopoulos</a></p>
<p>Automated detection of contraband items in X-ray images can significantly
increase public safety, by enhancing the productivity and alleviating the
mental load of security officers in airports, subways, customs/post offices,
etc. The large volume and high throughput of passengers, mailed parcels, etc.,
during rush hours practically make it a Big Data problem. Modern computer
vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of
undertaking this task even under resource-constrained and embedded execution
scenarios, e.g., as is the case with fast, single-stage object detectors.
However, no comparative experimental assessment of the various relevant DNN
components/methods has been performed under a common evaluation protocol, which
means that reliable cross-method comparisons are missing. This paper presents
exactly such a comparative assessment, utilizing a public relevant dataset and
a well-defined methodology for selecting the specific DNN components/modules
that are being evaluated. The results indicate the superiority of Transformer
detectors, the obsolete nature of auxiliary neural modules that have been
developed in the past few years for security applications and the efficiency of
the CSP-DarkNet backbone CNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07223">Bidirectional recurrent imputation and abundance estimation of LULC classes with MODIS multispectral time series and geo-topographic and climatic data. (arXiv:2310.07223v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Ortega_J/0/1/0/all/0/1">Jos&#xe9; Rodr&#xed;guez-Ortega</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Khaldi_R/0/1/0/all/0/1">Rohaifa Khaldi</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Alcaraz_Segura_D/0/1/0/all/0/1">Domingo Alcaraz-Segura</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Tabik_S/0/1/0/all/0/1">Siham Tabik</a> (1) ((1) Department of Computer Science and Artificial Intelligence, DaSCI, University of Granada, Granada, Spain, (2) LifeWatch-ERIC ICT Core, Seville, Spain, (3) Department of Botany, Faculty of Science, University of Granada, Granada, Spain)</p>
<p>Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)
types. Spectral unmixing (SU) is a key technique that disentangles mixed pixels
into constituent LULC types and their abundance fractions. While existing
studies on Deep Learning (DL) for SU typically focus on single time-step
hyperspectral (HS) or multispectral (MS) data, our work pioneers SU using MODIS
MS time series, addressing missing data with end-to-end DL models. Our approach
enhances a Long-Short Term Memory (LSTM)-based model by incorporating
geographic, topographic (geo-topographic), and climatic ancillary information.
Notably, our method eliminates the need for explicit endmember extraction,
instead learning the input-output relationship between mixed spectra and LULC
abundances through supervised learning. Experimental results demonstrate that
integrating spectral-temporal input data with geo-topographic and climatic
information significantly improves the estimation of LULC abundances in mixed
pixels. To facilitate this study, we curated a novel labeled dataset for
Andalusia (Spain) with monthly MODIS multispectral time series at 460m
resolution for 2013. Named Andalusia MultiSpectral MultiTemporal Unmixing
(Andalusia-MSMTU), this dataset provides pixel-level annotations of LULC
abundances along with ancillary information. The dataset
(https://zenodo.org/records/7752348) and code
(https://github.com/jrodriguezortega/MSMTU) are available to the public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09099">Vision Transformers increase efficiency of 3D cardiac CT multi-label segmentation. (arXiv:2310.09099v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jollans_L/0/1/0/all/0/1">Lee Jollans</a>, <a href="http://arxiv.org/find/eess/1/au:+Bustamante_M/0/1/0/all/0/1">Mariana Bustamante</a>, <a href="http://arxiv.org/find/eess/1/au:+Henriksson_L/0/1/0/all/0/1">Lilian Henriksson</a>, <a href="http://arxiv.org/find/eess/1/au:+Persson_A/0/1/0/all/0/1">Anders Persson</a>, <a href="http://arxiv.org/find/eess/1/au:+Ebbers_T/0/1/0/all/0/1">Tino Ebbers</a></p>
<p>Accurate segmentation of the heart is essential for personalized blood flow
simulations and surgical intervention planning. Segmentations need to be
accurate in every spatial dimension, which is not ensured by segmenting data
slice by slice. Two cardiac computed tomography (CT) datasets consisting of 760
volumes across the whole cardiac cycle from 39 patients, and of 60 volumes from
60 patients respectively were used to train networks to simultaneously segment
multiple regions representing the whole heart in 3D. The segmented regions
included the left and right atrium and ventricle, left ventricular myocardium,
ascending aorta, pulmonary arteries, pulmonary veins, and left atrial
appendage. The widely used 3D U-Net and the UNETR architecture were compared to
our proposed method optimized for large volumetric inputs. The proposed network
architecture, termed Transformer Residual U-Net (TRUNet), maintains the cascade
downsampling encoder, cascade upsampling decoder and skip connections from
U-Net, while incorporating a Vision Transformer (ViT) block in the encoder
alongside a modified ResNet50 block. TRUNet reached higher segmentation
performance for all structures within approximately half the training time
needed for 3D U-Net and UNETR. The proposed method achieved more precise vessel
boundary segmentations and better captured the heart's overall anatomical
structure compared to the other methods. The fast training time and accurate
delineation of adjacent structures makes TRUNet a promising candidate for
medical image segmentation tasks. The code for TRUNet is available at
github.com/ljollans/TRUNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11881">A Comparative Study of Image Restoration Networks for General Backbone Network Design. (arXiv:2310.11881v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yuandong Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yihao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiantao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chao Dong</a></p>
<p>Despite the significant progress made by deep models in various image
restoration tasks, existing image restoration networks still face challenges in
terms of task generality. An intuitive manifestation is that networks which
excel in certain tasks often fail to deliver satisfactory results in others. To
illustrate this point, we select five representative networks and conduct a
comparative study on five classic image restoration tasks. First, we provide a
detailed explanation of the characteristics of different image restoration
tasks and backbone networks. Following this, we present the benchmark results
and analyze the reasons behind the performance disparity of different models
across various tasks. Drawing from this comparative study, we propose that a
general image restoration backbone network needs to meet the functional
requirements of diverse tasks. Based on this principle, we design a new general
image restoration backbone network, X-Restormer. Extensive experiments
demonstrate that X-Restormer possesses good task generality and achieves
state-of-the-art performance across a variety of tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05836">UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1">Jing Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1">Qinrui Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1">Shu Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1">Siwei Lyu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xi Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a></p>
<p>In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15939">Unleashing the Power of Prompt-driven Nucleus Instance Segmentation. (arXiv:2311.15939v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shui_Z/0/1/0/all/0/1">Zhongyi Shui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunlong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chenglu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Sunyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jingxiong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Honglin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Ruizhe Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin Yang</a></p>
<p>Nucleus instance segmentation in histology images is crucial for a broad
spectrum of clinical applications. Current dominant algorithms rely on
regression of nuclear proxy maps. Distinguishing nucleus instances from the
estimated maps requires carefully curated post-processing, which is error-prone
and parameter-sensitive. Recently, the Segment Anything Model (SAM) has earned
huge attention in medical image segmentation, owing to its impressive
generalization ability and promptable property. Nevertheless, its potential on
nucleus instance segmentation remains largely underexplored. In this paper, we
present a novel prompt-driven framework that consists of a nucleus prompter and
SAM for automatic nucleus instance segmentation. Specifically, the prompter
learns to generate a unique point prompt for each nucleus while the SAM is
fine-tuned to output the corresponding mask for the prompted nucleus.
Furthermore, we propose the inclusion of adjacent nuclei as negative prompts to
enhance the model's capability to identify overlapping nuclei. Without
complicated post-processing, our proposed method sets a new state-of-the-art
performance on three challenging benchmarks. Code is available at
\url{github.com/windygoo/PromptNucSeg}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17425">SpeechAct: Towards Generating Whole-body Motion from Speech. (arXiv:2311.17425v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinsong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a></p>
<p>This paper addresses the problem of generating whole-body motion from speech.
Despite great successes, prior methods still struggle to produce reasonable and
diverse whole-body motions from speech. This is due to their reliance on
suboptimal representations and a lack of strategies for generating diverse
results. To address these challenges, we present a novel hybrid point
representation to achieve accurate and continuous motion generation, e.g.,
avoiding foot skating, and this representation can be transformed into an
easy-to-use representation, i.e., SMPL-X body mesh, for many applications. To
generate whole-body motion from speech, for facial motion, closely tied to the
audio signal, we introduce an encoder-decoder architecture to achieve
deterministic outcomes. However, for the body and hands, which have weaker
connections to the audio signal, we aim to generate diverse yet reasonable
motions. To boost diversity in motion generation, we propose a contrastive
motion learning method to encourage the model to produce more distinctive
representations. Specifically, we design a robust VQ-VAE to learn a quantized
motion codebook using our hybrid representation. Then, we regress the motion
representation from the audio signal by a translation model employing our
contrastive motion learning method. Experimental results validate the superior
performance and the correctness of our model. The project page is available for
research purposes at <a href="http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03408">Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future. (arXiv:2312.03408v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jia Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huilin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Li Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Lu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Futang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chunjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tiancai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1">Beipeng Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhihui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>With the continuous maturation and application of autonomous driving
technology, a systematic examination of open-source autonomous driving datasets
becomes instrumental in fostering the robust evolution of the industry
ecosystem. Current autonomous driving datasets can broadly be categorized into
two generations. The first-generation autonomous driving datasets are
characterized by relatively simpler sensor modalities, smaller data scale, and
is limited to perception-level tasks. KITTI, introduced in 2012, serves as a
prominent representative of this initial wave. In contrast, the
second-generation datasets exhibit heightened complexity in sensor modalities,
greater data scale and diversity, and an expansion of tasks from perception to
encompass prediction and control. Leading examples of the second generation
include nuScenes and Waymo, introduced around 2019. This comprehensive review,
conducted in collaboration with esteemed colleagues from both academia and
industry, systematically assesses over seventy open-source autonomous driving
datasets from domestic and international sources. It offers insights into
various aspects, such as the principles underlying the creation of high-quality
datasets, the pivotal role of data engine systems, and the utilization of
generative foundation models to facilitate scalable data generation.
Furthermore, this review undertakes an exhaustive analysis and discourse
regarding the characteristics and data scales that future third-generation
autonomous driving datasets should possess. It also delves into the scientific
and technical challenges that warrant resolution. These endeavors are pivotal
in advancing autonomous innovation and fostering technological enhancement in
critical domains. For further details, please refer to
https://github.com/OpenDriveLab/DriveAGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07630">Building Universal Foundation Models for Medical Image Analysis with Spatially Adaptive Networks. (arXiv:2312.07630v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Lingxiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuanzhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1">Bingda Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinsheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1">Rong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengpeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yujiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a></p>
<p>Recent advancements in foundation models, typically trained with
self-supervised learning on large-scale and diverse datasets, have shown great
potential in medical image analysis. However, due to the significant spatial
heterogeneity of medical imaging data, current models must tailor specific
structures for different datasets, making it challenging to leverage the
abundant unlabeled data. In this work, we propose a universal foundation model
for medical image analysis that processes images with heterogeneous spatial
properties using a unified structure. To accomplish this, we propose spatially
adaptive networks (SPAD-Nets), a family of networks that dynamically adjust the
structures to adapt to the spatial properties of input images, to build such a
universal foundation model. We pre-train a spatial adaptive visual tokenizer
(SPAD-VT) and then a spatial adaptive Vision Transformer (SPAD-ViT) via masked
image modeling (MIM) on 55 public medical image datasets. The pre-training data
comprises over 9 million image slices, representing the largest, most
comprehensive, and most diverse dataset to our knowledge for pre-training
universal foundation models for medical image analysis. The experimental
results on downstream medical image classification and segmentation tasks
demonstrate the superior performance and label efficiency of our model. Our
code is available at https://github.com/function2-llx/PUMIT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08917">An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiaqi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruizheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Sixing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1">Tsz Wa Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1">Ming Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Ying-Cong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsung_F/0/1/0/all/0/1">Fugee Tsung</a></p>
<p>Artificial Intelligence (AI)-driven defect inspection is pivotal in
industrial manufacturing. Yet, many methods, tailored to specific pipelines,
grapple with diverse product portfolios and evolving processes. Addressing
this, we present the Incremental Unified Framework (IUF), which can reduce the
feature conflict problem when continuously integrating new objects in the
pipeline, making it advantageous in object-incremental learning scenarios.
Employing a state-of-the-art transformer, we introduce Object-Aware
Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic
Compression Loss (SCL) is integrated to optimize non-primary semantic space,
enhancing network adaptability for novel objects. Additionally, we prioritize
retaining the features of established objects during weight updates.
Demonstrating prowess in both image and pixel-level defect inspection, our
approach achieves state-of-the-art performance, proving indispensable for
dynamic and scalable industrial inspections. Our code will be released at
\url{https://github.com/jqtangust/IUF}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09093">Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption. (arXiv:2312.09093v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1">Ziteng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1">Lin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xianzheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1">Tatsuya Harada</a></p>
<p>The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered
methodology, entangling the aspects of illumination and material reflectance
into emission solely from 3D points. This simplified rendering approach
presents challenges in accurately modeling images captured under adverse
lighting conditions, such as low light or over-exposure. Motivated by the
ancient Greek emission theory that posits visual perception as a result of rays
emanating from the eyes, we slightly refine the conventional NeRF framework to
train NeRF under challenging light conditions and generate normal-light
condition novel views unsupervised. We introduce the concept of a "Concealing
Field," which assigns transmittance values to the surrounding air to account
for illumination effects. In dark scenarios, we assume that object emissions
maintain a standard lighting level but are attenuated as they traverse the air
during the rendering process. Concealing Field thus compel NeRF to learn
reasonable density and colour estimations for objects even in dimly lit
situations. Similarly, the Concealing Field can mitigate over-exposed emissions
during the rendering stage. Furthermore, we present a comprehensive multi-view
dataset captured under challenging illumination conditions for evaluation. Our
code and dataset available at https://github.com/cuiziteng/Aleth-NeRF
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00496">SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Psychogyios_D/0/1/0/all/0/1">Dimitrios Psychogyios</a>, <a href="http://arxiv.org/find/cs/1/au:+Colleoni_E/0/1/0/all/0/1">Emanuele Colleoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Amsterdam_B/0/1/0/all/0/1">Beatrice Van Amsterdam</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chih-Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shu-Yu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuchong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1">Fucang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1">Baosheng Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guotai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Boels_M/0/1/0/all/0/1">Maxence Boels</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1">Jiayu Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1">Rachel Sparks</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1">Prokar Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Granados_A/0/1/0/all/0/1">Alejandro Granados</a>, <a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1">Sebastien Ourselin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengya Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">An Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yanan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1">Long Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Hongliang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamada_A/0/1/0/all/0/1">Atsushi Yamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Harai_Y/0/1/0/all/0/1">Yuriko Harai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_Y/0/1/0/all/0/1">Yuto Ishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1">Kazuyuki Hayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Simoens_J/0/1/0/all/0/1">Jente Simoens</a>, <a href="http://arxiv.org/find/cs/1/au:+DeBacker_P/0/1/0/all/0/1">Pieter DeBacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Cisternino_F/0/1/0/all/0/1">Francesco Cisternino</a>, <a href="http://arxiv.org/find/cs/1/au:+Furnari_G/0/1/0/all/0/1">Gabriele Furnari</a>, <a href="http://arxiv.org/find/cs/1/au:+Mottrie_A/0/1/0/all/0/1">Alex Mottrie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferraguti_F/0/1/0/all/0/1">Federica Ferraguti</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondo_S/0/1/0/all/0/1">Satoshi Kondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasai_S/0/1/0/all/0/1">Satoshi Kasai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirasawa_K/0/1/0/all/0/1">Kousuke Hirasawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Soohee Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seung Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyu Eun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1">Hyoun-Joong Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1">Kui Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1">Shan An</a>, <a href="http://arxiv.org/find/cs/1/au:+Krell_S/0/1/0/all/0/1">Stefanie Krell</a>, <a href="http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1">Sebastian Bodenstedt</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1">Nicolas Ayobi</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1">Alejandra Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1">Santiago Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Puentes_J/0/1/0/all/0/1">Juanita Puentes</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1">Pablo Arbelaez</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1">Omid Mohareri</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1">Danail Stoyanov</a></p>
<p>Surgical tool segmentation and action recognition are fundamental building
blocks in many computer-assisted intervention applications, ranging from
surgical skills assessment to decision support systems. Nowadays,
learning-based action recognition and segmentation approaches outperform
classical methods, relying, however, on large, annotated datasets. Furthermore,
action recognition and tool segmentation algorithms are often trained and make
predictions in isolation from each other, without exploiting potential
cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we
release the first multimodal, publicly available, in-vivo, dataset for surgical
action recognition and semantic instrumentation segmentation, containing 50
suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The
aim of the challenge is twofold. First, to enable researchers to leverage the
scale of the provided dataset and develop robust and highly accurate
single-task action recognition and tool segmentation approaches in the surgical
domain. Second, to further explore the potential of multitask-based learning
approaches and determine their comparative advantage against their single-task
counterparts. A total of 12 teams participated in the challenge, contributing 7
action recognition methods, 9 instrument segmentation techniques, and 4
multitask approaches that integrated both action recognition and instrument
segmentation. The complete SAR-RARP50 dataset is available at:
https://rdr.ucl.ac.uk/projects/SARRARP50_Segmentation_of_surgical_instrumentation_and_Action_Recognition_on_Robot-Assisted_Radical_Prostatectomy_Challenge/191091
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08396">Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fangyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiliang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Justin M. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Robert Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1">Ronald M. Summers</a>, <a href="http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1">Justin F. Rousseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_P/0/1/0/all/0/1">Peiyun Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Landsman_M/0/1/0/all/0/1">Marc J Landsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Baxter_S/0/1/0/all/0/1">Sally L. Baxter</a>, <a href="http://arxiv.org/find/cs/1/au:+AlAref_S/0/1/0/all/0/1">Subhi J. Al&#x27;Aref</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1">Michael F. Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yifan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>Recent studies indicate that Generative Pre-trained Transformer 4 with Vision
(GPT-4V) outperforms human physicians in medical challenge tasks. However,
these evaluations primarily focused on the accuracy of multi-choice questions
alone. Our study extends the current scope by conducting a comprehensive
analysis of GPT-4V's rationales of image comprehension, recall of medical
knowledge, and step-by-step multimodal reasoning when solving New England
Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test
the knowledge and diagnostic capabilities of medical professionals. Evaluation
results confirmed that GPT-4V outperforms human physicians regarding
multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in
cases where physicians incorrectly answer, with over 80% accuracy. However, we
discovered that GPT-4V frequently presents flawed rationales in cases where it
makes the correct final choices (27.3%), most prominent in image comprehension
(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our
findings emphasize the necessity for further in-depth evaluations of its
rationales before integrating such models into clinical workflows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10110">VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition. (arXiv:2401.10110v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xianfu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Weixiao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tongliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhoujun Li</a></p>
<p>Scene Text Recognition (STR) is a challenging task that involves recognizing
text within images of natural scenes. Although current state-of-the-art models
for STR exhibit high performance, they typically suffer from low inference
efficiency due to their reliance on hybrid architectures comprised of visual
encoders and sequence decoders. In this work, we propose the VIsion Permutable
extractor for fast and efficient scene Text Recognition (VIPTR), which achieves
an impressive balance between high performance and rapid inference speeds in
the domain of STR. Specifically, VIPTR leverages a visual-semantic extractor
with a pyramid structure, characterized by multiple self-attention layers,
while eschewing the traditional sequence decoder. This design choice results in
a lightweight and efficient model capable of handling inputs of varying sizes.
Extensive experimental results on various standard datasets for both Chinese
and English scene text recognition validate the superiority of VIPTR. Notably,
the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par with
other lightweight models and achieves SOTA inference speeds. Meanwhile, the
VIPTR-L (Large) variant attains greater recognition accuracy, while maintaining
a low parameter count and favorable inference speed. Our proposed method
provides a compelling solution for the STR challenge, which blends high
accuracy with efficiency and greatly benefits real-world applications requiring
fast and reliable text recognition. The code is publicly available at
https://github.com/cxfyxl/VIPTR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10608">M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images. (arXiv:2401.10608v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiuju Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Shuyi Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yen-Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lanfen Lin</a></p>
<p>The advancement of Spatial Transcriptomics (ST) has facilitated the
spatially-aware profiling of gene expressions based on histopathology images.
Although ST data offers valuable insights into the micro-environment of tumors,
its acquisition cost remains expensive. Therefore, directly predicting the ST
expressions from digital pathology images is desired. Current methods usually
adopt existing regression backbones for this task, which ignore the inherent
multi-scale hierarchical data structure of digital pathology images. To address
this limit, we propose M2ORT, a many-to-one regression Transformer that can
accommodate the hierarchical structure of the pathology images through a
decoupled multi-scale feature extractor. Different from traditional models that
are trained with one-to-one image-label pairs, M2ORT accepts multiple pathology
images of different magnifications at a time to jointly predict the gene
expressions at their corresponding common ST spot, aiming at learning a
many-to-one relationship through training. We have tested M2ORT on three public
ST datasets and the experimental results show that M2ORT can achieve
state-of-the-art performance with fewer parameters and floating-point
operations (FLOPs). The code is available at:
https://github.com/Dootmaan/M2ORT/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10727">MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning. (arXiv:2401.10727v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weixin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qianyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1">Haonan Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jindi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Sixun Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiaohua/0/1/0/all/0/1">Xiaohua</a> (Michael) <a href="http://arxiv.org/find/cs/1/au:+Xuan/0/1/0/all/0/1">Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shenghua Gao</a></p>
<p>Recently, the astonishing performance of large language models (LLMs) in
natural language comprehension and generation tasks triggered lots of
exploration of using them as central controllers to build agent systems.
Multiple studies focus on bridging the LLMs to external tools to extend the
application scenarios. However, the current LLMs' perceiving tool-use ability
is limited to a single text query, which may result in ambiguity in
understanding the users' real intentions. LLMs are expected to eliminate that
by perceiving the visual- or auditory-grounded instructions' information.
Therefore, in this paper, we propose MLLM-Tool, a system incorporating
open-source LLMs and multi-modal encoders so that the learnt LLMs can be
conscious of multi-modal input instruction and then select the function-matched
tool correctly. To facilitate the evaluation of the model's capability, we
collect a dataset featured by consisting of multi-modal input tools from
HuggingFace. Another important feature of our dataset is that our dataset also
contains multiple potential choices for the same instruction due to the
existence of identical functions and synonymous functions, which provides more
potential solutions for the same query. The experiments reveal that our
MLLM-Tool is capable of recommending appropriate tools for multi-modal
instructions. Codes and data are available at
https://github.com/MLLM-Tool/MLLM-Tool.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11115">MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation. (arXiv:2401.11115v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1">Nhat M. Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1">Kehong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1">Michael Bi Mi</a></p>
<p>Controllable generation of 3D human motions becomes an important topic as the
world embraces digital transformation. Existing works, though making promising
progress with the advent of diffusion models, heavily rely on meticulously
captured and annotated (e.g., text) high-quality motion corpus, a
resource-intensive endeavor in the real world. This motivates our proposed
MotionMix, a simple yet effective weakly-supervised diffusion model that
leverages both noisy and unannotated motion sequences. Specifically, we
separate the denoising objectives of a diffusion model into two stages:
obtaining conditional rough motion approximations in the initial $T-T^*$ steps
by learning the noisy annotated motions, followed by the unconditional
refinement of these preliminary motions during the last $T^*$ steps using
unannotated motions. Notably, though learning from two sources of imperfect
data, our model does not compromise motion generation quality compared to fully
supervised approaches that access gold data. Extensive experiments on several
benchmarks demonstrate that our MotionMix, as a versatile framework,
consistently achieves state-of-the-art performances on text-to-motion,
action-to-motion, and music-to-dance tasks. Project page:
https://nhathoang2002.github.io/MotionMix-page/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11751">Boosting Multi-view Stereo with Late Cost Aggregation. (arXiv:2401.11751v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenxun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jinqiu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanning Zhang</a></p>
<p>Pairwise matching cost aggregation is a crucial step for modern
learning-based Multi-view Stereo (MVS). Prior works adopt an early aggregation
scheme, which adds up pairwise costs into an intermediate cost. However, we
analyze that this process can degrade informative pairwise matchings, thereby
blocking the depth network from fully utilizing the original geometric matching
cues. To address this challenge, we present a late aggregation approach that
allows for aggregating pairwise costs throughout the network feed-forward
process, achieving accurate estimations with only minor changes of the plain
CasMVSNet. Instead of building an intermediate cost by weighted sum, late
aggregation preserves all pairwise costs along a distinct view channel. This
enables the succeeding depth network to fully utilize the crucial geometric
cues without loss of cost fidelity. Grounded in the new aggregation scheme, we
propose further techniques addressing view order dependence inside the
preserved cost, handling flexible testing views, and improving the depth
filtering process. Despite its technical simplicity, our method improves
significantly upon the baseline cascade-based approach, achieving comparable
results with state-of-the-art methods with favorable computation overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11859">LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution. (arXiv:2401.11859v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Qin_F/0/1/0/all/0/1">Feiwei Qin</a>, <a href="http://arxiv.org/find/eess/1/au:+Yan_K/0/1/0/all/0/1">Kang Yan</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Changmiao Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ge_R/0/1/0/all/0/1">Ruiquan Ge</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1">Yong Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a></p>
<p>Given the broad application of infrared technology across diverse fields,
there is an increasing emphasis on investigating super-resolution techniques
for infrared images within the realm of deep learning. Despite the impressive
results of current Transformer-based methods in image super-resolution tasks,
their reliance on the self-attentive mechanism intrinsic to the Transformer
architecture results in images being treated as one-dimensional sequences,
thereby neglecting their inherent two-dimensional structure. Moreover, infrared
images exhibit a uniform pixel distribution and a limited gradient range,
posing challenges for the model to capture effective feature information.
Consequently, we suggest a potent Transformer model, termed Large Kernel
Transformer (LKFormer), to address this issue. Specifically, we have designed a
Large Kernel Residual Attention (LKRA) module with linear complexity. This
mainly employs depth-wise convolution with large kernels to execute non-local
feature modeling, thereby substituting the standard self-attentive layer.
Additionally, we have devised a novel feed-forward network structure called
Gated-Pixel Feed-Forward Network (GPFN) to augment the LKFormer's capacity to
manage the information flow within the network. Comprehensive experimental
results reveal that our method surpasses the most advanced techniques
available, using fewer parameters and yielding considerably superior
performance.The source code will be available at
https://github.com/sad192/large-kernel-Transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12592">RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos. (arXiv:2401.12592v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Hongchi Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sifei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>We introduce a new RGB-D object dataset captured in the wild called
WildRGB-D. Unlike most existing real-world object-centric datasets which only
come with RGB capturing, the direct capture of the depth channel allows better
3D annotations and broader downstream applications. WildRGB-D comprises
large-scale category-level RGB-D object videos, which are taken using an iPhone
to go around the objects in 360 degrees. It contains around 8500 recorded
objects and nearly 20000 RGB-D videos across 46 common object categories. These
videos are taken with diverse cluttered backgrounds with three setups to cover
as many real-world scenarios as possible: (i) a single object in one video;
(ii) multiple objects in one video; and (iii) an object with a static hand in
one video. The dataset is annotated with object masks, real-world scale camera
poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark
four tasks with WildRGB-D including novel view synthesis, camera pose
estimation, object 6d pose estimation, and object surface reconstruction. Our
experiments show that the large-scale capture of RGB-D objects provides a large
potential to advance 3D object learning. Our project page is
https://wildrgbd.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12900">PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting. (arXiv:2401.12900v2 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhongyuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1">Zhenyu Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1">Guoping Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kanglin Liu</a></p>
<p>Despite much progress, achieving real-time high-fidelity head avatar
animation is still difficult and existing methods have to trade-off between
speed and quality. 3DMM based methods often fail to model non-facial structures
such as eyeglasses and hairstyles, while neural implicit models suffer from
deformation inflexibility and rendering inefficiency. Although 3D Gaussian has
been demonstrated to possess promising capability for geometry representation
and radiance field reconstruction, applying 3D Gaussian in head avatar creation
remains a major challenge since it is difficult for 3D Gaussian to model the
head shape variations caused by changing poses and expressions. In this paper,
we introduce PSAvatar, a novel framework for animatable head avatar creation
that utilizes discrete geometric primitive to create a parametric morphable
shape model and employs 3D Gaussian for fine detail representation and high
fidelity rendering. The parametric morphable shape model is a Point-based
Morphable Shape Model (PMSM) which uses points instead of meshes for 3D
representation to achieve enhanced representation flexibility. The PMSM first
converts the FLAME mesh to points by sampling on the surfaces as well as off
the meshes to enable the reconstruction of not only surface-like structures but
also complex geometries such as eyeglasses and hairstyles. By aligning these
points with the head shape in an analysis-by-synthesis manner, the PMSM makes
it possible to utilize 3D Gaussian for fine detail representation and
appearance modeling, thus enabling the creation of high-fidelity avatars. We
show that PSAvatar can reconstruct high-fidelity head avatars of a variety of
subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a
resolution of 512 $\times$ 512 ).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12946">Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization. (arXiv:2401.12946v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zimeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhiyang Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Cheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiaoxiao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1">Shiqing Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoming Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>We introduce Coverage Axis++, a novel and efficient approach to 3D shape
skeletonization. The current state-of-the-art approaches for this task often
rely on the watertightness of the input or suffer from substantial
computational costs, thereby limiting their practicality. To address this
challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal
points, offering a high-accuracy approximation of the Medial Axis Transform
(MAT) while significantly mitigating computational intensity for various shape
representations. We introduce a simple yet effective strategy that considers
both shape coverage and uniformity to derive skeletal points. The selection
procedure enforces consistency with the shape structure while favoring the
dominant medial balls, which thus introduces a compact underlying shape
representation in terms of MAT. As a result, Coverage Axis++ allows for
skeletonization for various shape representations (e.g., water-tight meshes,
triangle soups, point clouds), specification of the number of skeletal points,
few hyperparameters, and highly efficient computation with improved
reconstruction accuracy. Extensive experiments across a wide range of 3D shapes
validate the efficiency and effectiveness of Coverage Axis++. The code will be
publicly available once the paper is published.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12978">Zero-Shot Learning for the Primitives of 3D Affordance in General Objects. (arXiv:2401.12978v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyeonwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Sookwan Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_P/0/1/0/all/0/1">Patrick Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1">Hanbyul Joo</a></p>
<p>One of the major challenges in AI is teaching machines to precisely respond
and utilize environmental functionalities, thereby achieving the affordance
awareness that humans possess. Despite its importance, the field has been
lagging in terms of learning, especially in 3D, as annotating affordance
accompanies a laborious process due to the numerous variations of human-object
interaction. The low availability of affordance data limits the learning in
terms of generalization for object categories, and also simplifies the
representation of affordance, capturing only a fraction of the affordance. To
overcome these challenges, we propose a novel, self-supervised method to
generate the 3D affordance examples given only a 3D object, without any manual
annotations. The method starts by capturing the 3D object into images and
creating 2D affordance images by inserting humans into the image via inpainting
diffusion models, where we present the Adaptive Mask algorithm to enable human
insertion without altering the original details of the object. The method
consequently lifts inserted humans back to 3D to create 3D human-object pairs,
where the depth ambiguity is resolved within a depth optimization framework
that utilizes pre-generated human postures from multiple viewpoints. We also
provide a novel affordance representation defined on relative orientations and
proximity between dense human and object points, that can be easily aggregated
from any 3D HOI datasets. The proposed representation serves as a primitive
that can be manifested to conventional affordance representations via simple
transformations, ranging from physically exerted affordances to nonphysical
ones. We demonstrate the efficacy of our method and representation by
generating the 3D affordance samples and deriving high-quality affordance
examples from the representation, including contact, orientation, and spatial
occupancies.
</p>
</p>
</div>

    </div>
    </body>
    