<!DOCTYPE html>
<html>
<head>
<title>2023-07-06-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.01202">Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Stephen Yang</a></p>
<p>Analysis of innovation has been fundamentally limited by conventional
approaches to broad, structural variables. This paper pushes the boundaries,
taking an LLM approach to patent analysis with the groundbreaking ChatGPT
technology. OpenAI's state-of-the-art textual embedding accesses complex
information about the quality and impact of each invention to power deep
learning predictive models. The nuanced embedding drives a 24% incremental
improvement in R-squared predicting patent value and clearly isolates the worst
and best applications. These models enable a revision of the contemporary
Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median
deviation of 1.5 times, accounting for potential institutional predictions.
Furthermore, the market fails to incorporate timely information about
applications; a long-short portfolio based on predicted acceptance rates
achieves significant abnormal returns of 3.3% annually. The models provide an
opportunity to revolutionize startup and small-firm corporate policy vis-a-vis
patenting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01204">Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach. (arXiv:2307.01204v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zicheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Linhao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quoc Viet Hung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a></p>
<p>Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict
missing links for unseen entities with few-shot links observed. Previous
methods are limited to transductive scenarios, where entities exist in the
knowledge graphs, so they are unable to handle unseen entities. Therefore,
recent inductive methods utilize the sub-graphs around unseen entities to
obtain the semantics and predict links inductively. However, in the few-shot
setting, the sub-graphs are often sparse and cannot provide meaningful
inductive patterns. In this paper, we propose a novel relational anonymous
walk-guided neural process for few-shot inductive link prediction on knowledge
graphs, denoted as RawNP. Specifically, we develop a neural process-based
method to model a flexible distribution over link prediction functions. This
enables the model to quickly adapt to new entities and estimate the uncertainty
when making predictions. To capture general inductive patterns, we present a
relational anonymous walk to extract a series of relational motifs from
few-shot observations. These motifs reveal the distinctive semantic patterns on
KGs that support inductive predictions. Extensive experiments on typical
benchmark datasets demonstrate that our model derives new state-of-the-art
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01206">Confidence Ranking for CTR Prediction. (arXiv:2307.01206v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Congcong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhangang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jingping Shao</a></p>
<p>Model evolution and constant availability of data are two common phenomena in
large-scale real-world machine learning applications, e.g. ads and
recommendation systems. To adapt, the real-world system typically retrain with
all available data and online learn with recently available data to update the
models periodically with the goal of better serving performance. In this paper,
we propose a novel framework, named Confidence Ranking, which designs the
optimization objective as a ranking function with two different models. Our
confidence ranking loss allows direct optimization of the logits output for
different convex surrogate functions of metrics, e.g. AUC and Accuracy
depending on the target task and dataset. Armed with our proposed methods, our
experiments show that the introduction of confidence ranking loss can
outperform all baselines on the CTR prediction tasks of public and industrial
datasets. This framework has been deployed in the advertisement system of
JD.com to serve the main traffic in the fine-rank stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01207">Recommender Systems for Online and Mobile Social Networks: A survey. (arXiv:2307.01207v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campana_M/0/1/0/all/0/1">Mattia Giovanni Campana</a>, <a href="http://arxiv.org/find/cs/1/au:+Delmastro_F/0/1/0/all/0/1">Franca Delmastro</a></p>
<p>Recommender Systems (RS) currently represent a fundamental tool in online
services, especially with the advent of Online Social Networks (OSN). In this
case, users generate huge amounts of contents and they can be quickly
overloaded by useless information. At the same time, social media represent an
important source of information to characterize contents and users' interests.
RS can exploit this information to further personalize suggestions and improve
the recommendation process. In this paper we present a survey of Recommender
Systems designed and implemented for Online and Mobile Social Networks,
highlighting how the use of social context information improves the
recommendation task, and how standard algorithms must be enhanced and optimized
to run in a fully distributed environment, as opportunistic networks. We
describe advantages and drawbacks of these systems in terms of algorithms,
target domains, evaluation metrics and performance evaluations. Eventually, we
present some open research challenges in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01209">Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhibai Jia</a></p>
<p>Machine learning techniques have shown their competence for representing and
reasoning in symbolic systems such as language and phonology. In Sinitic
Historical Phonology, notable tasks that could benefit from machine learning
include the comparison of dialects and reconstruction of proto-languages
systems. Motivated by this, this paper provides an approach for obtaining
multi-dialectal representations of Sinitic syllables, by constructing a
knowledge graph from structured phonological data, then applying the BoxE
technique from knowledge base learning. We applied unsupervised clustering
techniques to the obtained representations to observe that the representations
capture phonemic contrast from the input dialects. Furthermore, we trained
classifiers to perform inference of unobserved Middle Chinese labels, showing
the representations' potential for indicating archaic, proto-language features.
The representations can be used for performing completion of fragmented Sinitic
phonological knowledge bases, estimating divergences between different
characters, or aiding the exploration and reconstruction of archaic features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01212">Of Spiky SVDs and Music Recommendation. (arXiv:2307.01212v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Afchar_D/0/1/0/all/0/1">Darius Afchar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1">Romain Hennequin</a>, <a href="http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1">Vincent Guigue</a></p>
<p>The truncated singular value decomposition is a widely used methodology in
music recommendation for direct similar-item retrieval or embedding musical
items for downstream tasks. This paper investigates a curious effect that we
show naturally occurring on many recommendation datasets: spiking formations in
the embedding space. We first propose a metric to quantify this spiking
organization's strength, then mathematically prove its origin tied to
underlying communities of items of varying internal popularity. With this
new-found theoretical understanding, we finally open the topic with an
industrial use case of estimating how music embeddings' top-k similar items
will change over time under the addition of data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01217">FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yang Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zhengui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruhui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Haibing Guan</a></p>
<p>Recently, personalized federated learning (pFL) has attracted increasing
attention in privacy protection, collaborative learning, and tackling
statistical heterogeneity among clients, e.g., hospitals, mobile smartphones,
etc. Most existing pFL methods focus on exploiting the global information and
personalized information in the client-level model parameters while neglecting
that data is the source of these two kinds of information. To address this, we
propose the Federated Conditional Policy (FedCP) method, which generates a
conditional policy for each sample to separate the global information and
personalized information in its features and then processes them by a global
head and a personalized head, respectively. FedCP is more fine-grained to
consider personalization in a sample-specific manner than existing pFL methods.
Extensive experiments in computer vision and natural language processing
domains show that FedCP outperforms eleven state-of-the-art methods by up to
6.69%. Furthermore, FedCP maintains its superiority when some clients
accidentally drop out, which frequently happens in mobile settings. Our code is
public at https://github.com/TsingZ0/FedCP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01225">Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sabir_B/0/1/0/all/0/1">Bushra Sabir</a>, <a href="http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1">M. Ali Babar</a>, <a href="http://arxiv.org/find/cs/1/au:+Abuadbba_S/0/1/0/all/0/1">Sharif Abuadbba</a></p>
<p>Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have
shown impressive performance in NLP. However, their vulnerability to
adversarial examples poses a security risk. Existing defense methods lack
interpretability, making it hard to understand adversarial classifications and
identify model vulnerabilities. To address this, we propose the
Interpretability and Transparency-Driven Detection and Transformation (IT-DT)
framework. It focuses on interpretability and transparency in detecting and
transforming textual adversarial examples. IT-DT utilizes techniques like
attention maps, integrated gradients, and model feedback for interpretability
during detection. This helps identify salient features and perturbed words
contributing to adversarial classifications. In the transformation phase, IT-DT
uses pre-trained embeddings and model feedback to generate optimal replacements
for perturbed words. By finding suitable substitutions, we aim to convert
adversarial examples into non-adversarial counterparts that align with the
model's intended behavior while preserving the text's meaning. Transparency is
emphasized through human expert involvement. Experts review and provide
feedback on detection and transformation results, enhancing decision-making,
especially in complex scenarios. The framework generates insights and threat
intelligence empowering analysts to identify vulnerabilities and improve model
robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in
detecting and transforming adversarial examples. The approach enhances
interpretability, provides transparency, and enables accurate identification
and successful transformation of adversarial inputs. By combining technical
analysis and human expertise, IT-DT significantly improves the resilience and
trustworthiness of transformer-based text classifiers against adversarial
attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01226">vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weijie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaoyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1">Srinivasan H. Sengamedu</a>, <a href="http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1">Francis Iannacci</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinjin Zhao</a></p>
<p>Recently, Neural Topic Models (NTM), inspired by variational autoencoders,
have attracted a lot of research interest; however, these methods have limited
applications in the real world due to the challenge of incorporating human
knowledge. This work presents a semi-supervised neural topic modeling method,
vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and
optimal transport. When a few keywords per topic are provided, vONTSS in the
semi-supervised setting generates potential topics and optimizes topic-keyword
quality and topic classification. Experiments show that vONTSS outperforms
existing semi-supervised topic modeling methods in classification accuracy and
diversity. vONTSS also supports unsupervised topic modeling. Quantitative and
qualitative experiments show that vONTSS in the unsupervised setting
outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered
and coherent topics on benchmark datasets. It is also much faster than the
state-of-the-art weakly supervised text classification method while achieving
similar classification performance. We further prove the equivalence of optimal
transport loss and cross-entropy loss at the global minimum.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01227">ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangrok Lee</a></p>
<p>Traffic forecasting is a highly challenging task owing to the dynamical
spatio-temporal dependencies of traffic flows. To handle this, we focus on
modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze
Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple
regions. ESGCN consists of two modules: W-module and ES module. W-module is a
fully node-wise convolutional network. It encodes the time-series of each
traffic region separately and decomposes the time-series at various scales to
capture fine and coarse features. The ES module models the spatio-temporal
dynamics using Graph Convolutional Network (GCN) and generates an Adaptive
Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM,
we introduce three key concepts. 1) Using edge features to directly capture the
spatiotemporal flow representation among regions. 2) Applying an edge attention
mechanism to GCN to extract the AAM from the edge features. Here, the attention
mechanism can effectively determine important spatio-temporal adjacency
relations. 3) Proposing a novel node contrastive loss to suppress obstructed
connections and emphasize related connections. Experimental results show that
ESGCN achieves state-of-the-art performance by a large margin on four
real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01229">EmoGen: Eliminating Subjective Bias in Emotional Music Generation. (arXiv:2307.01229v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1">Chenfei Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Peiling Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Botao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wei Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Music is used to convey emotions, and thus generating emotional music is
important in automatic music generation. Previous work on emotional music
generation directly uses annotated emotion labels as control signals, which
suffers from subjective bias: different people may annotate different emotions
on the same music, and one person may feel different emotions under different
situations. Therefore, directly mapping emotion labels to music sequences in an
end-to-end way would confuse the learning process and hinder the model from
generating music with general emotions. In this paper, we propose EmoGen, an
emotional music generation system that leverages a set of emotion-related music
attributes as the bridge between emotion and music, and divides the generation
into two stages: emotion-to-attribute mapping with supervised clustering, and
attribute-to-music generation with self-supervised learning. Both stages are
beneficial: in the first stage, the attribute values around the clustering
center represent the general emotions of these samples, which help eliminate
the impacts of the subjective bias of emotion labels; in the second stage, the
generation is completely disentangled from emotion labels and thus free from
the subjective bias. Both subjective and objective evaluations show that EmoGen
outperforms previous methods on emotion control accuracy and music quality
respectively, which demonstrate our superiority in generating emotional music.
Music samples generated by EmoGen are available via this
link:https://ai-muzic.github.io/emogen/, and the code is available at this
link:https://github.com/microsoft/muzic/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01230">Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rios_T/0/1/0/all/0/1">Thiago Rios</a>, <a href="http://arxiv.org/find/cs/1/au:+Menzel_S/0/1/0/all/0/1">Stefan Menzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sendhoff_B/0/1/0/all/0/1">Bernhard Sendhoff</a> (Honda Research Institute Europe)</p>
<p>The current advances in generative AI for learning large neural network
models with the capability to produce essays, images, music and even 3D assets
from text prompts create opportunities for a manifold of disciplines. In the
present paper, we study the potential of deep text-to-3D models in the
engineering domain, with focus on the chances and challenges when integrating
and interacting with 3D assets in computational simulation-based design
optimization. In contrast to traditional design optimization of 3D geometries
that often searches for the optimum designs using numerical representations,
such as B-Spline surface or deformation parameters in vehicle aerodynamic
optimization, natural language challenges the optimization framework by
requiring a different interpretation of variation operators while at the same
time may ease and motivate the human user interaction. Here, we propose and
realize a fully automated evolutionary design optimization framework using
Shap-E, a recently published text-to-3D asset network by OpenAI, in the context
of aerodynamic vehicle optimization. For representing text prompts in the
evolutionary optimization, we evaluate (a) a bag-of-words approach based on
prompt templates and Wordnet samples, and (b) a tokenisation approach based on
prompt templates and the byte pair encoding method from GPT4. Our main findings
from the optimizations indicate that, first, it is important to ensure that the
designs generated from prompts are within the object class of application, i.e.
diverse and novel designs need to be realistic, and, second, that more research
is required to develop methods where the strength of text prompt variations and
the resulting variations of the 3D designs share causal relations to some
degree to improve the optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01231">A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papadakis_G/0/1/0/all/0/1">George Papadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirielle_N/0/1/0/all/0/1">Nishadi Kirielle</a>, <a href="http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1">Peter Christen</a>, <a href="http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Entity resolution (ER) is the process of identifying records that refer to
the same entities within one or across multiple databases. Numerous techniques
have been developed to tackle ER challenges over the years, with recent
emphasis placed on machine and deep learning methods for the matching phase.
However, the quality of the benchmark datasets typically used in the
experimental evaluations of learning-based matching algorithms has not been
examined in the literature. To cover this gap, we propose four different
approaches to assessing the difficulty and appropriateness of 13 established
datasets: two theoretical approaches, which involve new measures of linearity
and existing measures of complexity, and two practical approaches: the
difference between the best non-linear and linear matchers, as well as the
difference between the best learning-based matcher and the perfect oracle. Our
analysis demonstrates that most of the popular datasets pose rather easy
classification tasks. As a result, they are not suitable for properly
evaluating learning-based matching algorithms. To address this issue, we
propose a new methodology for yielding benchmark datasets. We put it into
practice by creating four new matching tasks, and we verify that these new
benchmarks are more challenging and therefore more suitable for further
advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01232">Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data. (arXiv:2307.01232v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Qayyum_A/0/1/0/all/0/1">Adnan Qayyum</a>, <a href="http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1">Hassan Ali</a>, <a href="http://arxiv.org/find/eess/1/au:+Caputo_M/0/1/0/all/0/1">Massimo Caputo</a>, <a href="http://arxiv.org/find/eess/1/au:+Vohra_H/0/1/0/all/0/1">Hunaid Vohra</a>, <a href="http://arxiv.org/find/eess/1/au:+Akinosho_T/0/1/0/all/0/1">Taofeek Akinosho</a>, <a href="http://arxiv.org/find/eess/1/au:+Abioye_S/0/1/0/all/0/1">Sofiat Abioye</a>, <a href="http://arxiv.org/find/eess/1/au:+Berrou_I/0/1/0/all/0/1">Ilhem Berrou</a>, <a href="http://arxiv.org/find/eess/1/au:+Capik_P/0/1/0/all/0/1">Pawe&#x142; Capik</a>, <a href="http://arxiv.org/find/eess/1/au:+Qadir_J/0/1/0/all/0/1">Junaid Qadir</a>, <a href="http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1">Muhammad Bilal</a></p>
<p>Over the past few years, surgical data science has attracted substantial
interest from the machine learning (ML) community. Various studies have
demonstrated the efficacy of emerging ML techniques in analysing surgical data,
particularly recordings of procedures, for digitizing clinical and non-clinical
functions like preoperative planning, context-aware decision-making, and
operating skill assessment. However, this field is still in its infancy and
lacks representative, well-annotated datasets for training robust models in
intermediate ML tasks. Also, existing datasets suffer from inaccurate labels,
hindering the development of reliable models. In this paper, we propose a
systematic methodology for developing robust models for surgical tool detection
using noisy data. Our methodology introduces two key innovations: (1) an
intelligent active learning strategy for minimal dataset identification and
label correction by human experts; and (2) an assembling strategy for a
student-teacher model-based self-training framework to achieve the robust
classification of 14 surgical tools in a semi-supervised fashion. Furthermore,
we employ weighted data loaders to handle difficult class labels and address
class imbalance issues. The proposed methodology achieves an average F1-score
of 85.88\% for the ensemble model-based self-training with class weights, and
80.88\% without class weights for noisy labels. Also, our proposed method
significantly outperforms existing approaches, which effectively demonstrates
its effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01233">RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations. (arXiv:2307.01233v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sahipjohn_N/0/1/0/all/0/1">Neha Sahipjohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Neil Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambrahalli_V/0/1/0/all/0/1">Vishal Tambrahalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1">Vineet Gandhi</a></p>
<p>Significant progress has been made in speaker dependent Lip-to-Speech
synthesis, which aims to generate speech from silent videos of talking faces.
Current state-of-the-art approaches primarily employ non-autoregressive
sequence-to-sequence architectures to directly predict mel-spectrograms or
audio waveforms from lip representations. We hypothesize that the direct
mel-prediction hampers training/model efficiency due to the entanglement of
speech content with ambient information and speaker characteristics. To this
end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis.
First, a non-autoregressive sequence-to-sequence model maps self-supervised
visual features to a representation of disentangled speech content. A vocoder
then converts the speech features into raw waveforms. Extensive evaluations
confirm the effectiveness of our setup, achieving state-of-the-art performance
on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT
datasets. Speech samples from RobustL2S can be found at
https://neha-sherin.github.io/RobustL2S/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01234">Internet of Things Fault Detection and Classification via Multitask Learning. (arXiv:2307.01234v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Mohammad Arif Ul Alam</a></p>
<p>This paper presents a comprehensive investigation into developing a fault
detection and classification system for real-world IIoT applications. The study
addresses challenges in data collection, annotation, algorithm development, and
deployment. Using a real-world IIoT system, three phases of data collection
simulate 11 predefined fault categories. We propose SMTCNN for fault detection
and category classification in IIoT, evaluating its performance on real-world
data. SMTCNN achieves superior specificity (3.5%) and shows significant
improvements in precision, recall, and F1 measures compared to existing
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01236">Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch. (arXiv:2307.01236v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xunyi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hellard_T/0/1/0/all/0/1">Th&#xe9;otime Le Hellard</a>, <a href="http://arxiv.org/find/cs/1/au:+Eyraud_L/0/1/0/all/0/1">Lionel Eyraud</a>, <a href="http://arxiv.org/find/cs/1/au:+Gusak_J/0/1/0/all/0/1">Julia Gusak</a>, <a href="http://arxiv.org/find/cs/1/au:+Beaumont_O/0/1/0/all/0/1">Olivier Beaumont</a></p>
<p>We propose Rockmate to control the memory requirements when training PyTorch
DNN models. Rockmate is an automatic tool that starts from the model code and
generates an equivalent model, using a predefined amount of memory for
activations, at the cost of a few re-computations. Rockmate automatically
detects the structure of computational and data dependencies and rewrites the
initial model as a sequence of complex blocks. We show that such a structure is
widespread and can be found in many models in the literature (Transformer based
models, ResNet, RegNets,...). This structure allows us to solve the problem in
a fast and efficient way, using an adaptation of Checkmate (too slow on the
whole model but general) at the level of individual blocks and an adaptation of
Rotor (fast but limited to sequential models) at the level of the sequence
itself. We show through experiments on many models that Rockmate is as fast as
Rotor and as efficient as Checkmate, and that it allows in many cases to obtain
a significantly lower memory consumption for activations (by a factor of 2 to
5) for a rather negligible overhead (of the order of 10% to 20%). Rockmate is
open source and available at https://github.com/topal-team/rockmate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01237">Dynamical Graph Echo State Networks with Snapshot Merging for Dissemination Process Classification. (arXiv:2307.01237v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujiwara_K/0/1/0/all/0/1">Kantaro Fujiwara</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_G/0/1/0/all/0/1">Gouhei Tanaka</a></p>
<p>The Dissemination Process Classification (DPC) is a popular application of
temporal graph classification. The aim of DPC is to classify different
spreading patterns of information or pestilence within a community represented
by discrete-time temporal graphs. Recently, a reservoir computing-based model
named Dynamical Graph Echo State Network (DynGESN) has been proposed for
processing temporal graphs with relatively high effectiveness and low
computational costs. In this study, we propose a novel model which combines a
novel data augmentation strategy called snapshot merging with the DynGESN for
dealing with DPC tasks. In our model, the snapshot merging strategy is designed
for forming new snapshots by merging neighboring snapshots over time, and then
multiple reservoir encoders are set for capturing spatiotemporal features from
merged snapshots. After those, the logistic regression is adopted for decoding
the sum-pooled embeddings into the classification results. Experimental results
on six benchmark DPC datasets show that our proposed model has better
classification performances than the DynGESN and several kernel-based models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01238">Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction. (arXiv:2307.01238v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1">Daniel Parra</a>, <a href="http://arxiv.org/find/cs/1/au:+Joedicke_D/0/1/0/all/0/1">David Joedicke</a>, <a href="http://arxiv.org/find/cs/1/au:+Velasco_J/0/1/0/all/0/1">J. Manuel Velasco</a>, <a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1">Gabriel Kronberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Hidalgo_J/0/1/0/all/0/1">J. Ignacio Hidalgo</a></p>
<p>People with diabetes must carefully monitor their blood glucose levels,
especially after eating. Blood glucose regulation requires a proper combination
of food intake and insulin boluses. Glucose prediction is vital to avoid
dangerous post-meal complications in treating individuals with diabetes.
Although traditional methods, such as artificial neural networks, have shown
high accuracy rates, sometimes they are not suitable for developing
personalised treatments by physicians due to their lack of interpretability. In
this study, we propose a novel glucose prediction method emphasising
interpretability: Interpretable Sparse Identification by Grammatical Evolution.
Combined with a previous clustering stage, our approach provides finite
difference equations to predict postprandial glucose levels up to two hours
after meals. We divide the dataset into four-hour segments and perform
clustering based on blood glucose values for the twohour window before the
meal. Prediction models are trained for each cluster for the two-hour windows
after meals, allowing predictions in 15-minute steps, yielding up to eight
predictions at different time horizons. Prediction safety was evaluated based
on Parkes Error Grid regions. Our technique produces safe predictions through
explainable expressions, avoiding zones D (0.2% average) and E (0%) and
reducing predictions on zone C (6.2%). In addition, our proposal has slightly
better accuracy than other techniques, including sparse identification of
non-linear dynamics and artificial neural networks. The results demonstrate
that our proposal provides interpretable solutions without sacrificing
prediction accuracy, offering a promising approach to glucose prediction in
diabetes management that balances accuracy, interpretability, and computational
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01288">Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banegas_Luna_A/0/1/0/all/0/1">Antonio Jesus Banegas-Luna</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_Cortes_C/0/1/0/all/0/1">Carlos Mart&#x131;nez-Cortes</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Sanchez_H/0/1/0/all/0/1">Horacio Perez-Sanchez</a></p>
<p>Machine learning (ML) models are often valued by the accuracy of their
predictions. However, in some areas of science, the inner workings of models
are as relevant as their accuracy. To understand how ML models work internally,
the use of interpretability algorithms is the preferred option. Unfortunately,
despite the diversity of algorithms available, they often disagree in
explaining a model, leading to contradictory explanations. To cope with this
issue, consensus functions can be applied once the models have been explained.
Nevertheless, the problem is not completely solved because the final result
will depend on the selected consensus function and other factors. In this
paper, six consensus functions have been evaluated for the explanation of five
ML models. The models were previously trained on four synthetic datasets whose
internal rules were known in advance. The models were then explained with
model-agnostic local and global interpretability algorithms. Finally, consensus
was calculated with six different functions, including one developed by the
authors. The results demonstrated that the proposed function is fairer than the
others and provides more consistent and accurate explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01292">Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1">Debopam Sanyal</a> (Georgia Institute of Technology), <a href="http://arxiv.org/find/cs/1/au:+Hung_J/0/1/0/all/0/1">Jui-Tse Hung</a> (Georgia Institute of Technology), <a href="http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1">Manav Agrawal</a> (Georgia Institute of Technology), <a href="http://arxiv.org/find/cs/1/au:+Jasti_P/0/1/0/all/0/1">Prahlad Jasti</a> (Georgia Institute of Technology), <a href="http://arxiv.org/find/cs/1/au:+Nikkhoo_S/0/1/0/all/0/1">Shahab Nikkhoo</a> (University of California, Riverside), <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a> (University of Wisconsin, Madison), <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a> (University of Virginia), <a href="http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1">Sibin Mohan</a> (The George Washington University), <a href="http://arxiv.org/find/cs/1/au:+Tumanov_A/0/1/0/all/0/1">Alexey Tumanov</a> (Georgia Institute of Technology)</p>
<p>With the emergence of large foundational models, model-serving systems are
becoming popular. In such a system, users send the queries to the server and
specify the desired performance metrics (e.g., accuracy, latency, etc.). The
server maintains a set of models (model zoo) in the back-end and serves the
queries based on the specified metrics. This paper examines the security,
specifically robustness against model extraction attacks, of such systems.
Existing black-box attacks cannot be directly applied to extract a victim
model, as models hide among the model zoo behind the inference serving
interface, and attackers cannot identify which model is being used. An
intermediate step is required to ensure that every input query gets the output
from the victim model. To this end, we propose a query-efficient fingerprinting
algorithm to enable the attacker to trigger any desired model consistently. We
show that by using our fingerprinting algorithm, model extraction can have
fidelity and accuracy scores within $1\%$ of the scores obtained if attacking
in a single-model setting and up to $14.6\%$ gain in accuracy and up to $7.7\%$
gain in fidelity compared to the naive attack. Finally, we counter the proposed
attack with a noise-based defense mechanism that thwarts fingerprinting by
adding noise to the specified performance metrics. Our defense strategy reduces
the attack's accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively
(on medium-sized model extraction). We show that the proposed defense induces a
fundamental trade-off between the level of protection and system goodput,
achieving configurable and significant victim model extraction protection while
maintaining acceptable goodput ($&gt;80\%$). We provide anonymous access to our
code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01304">A numerical algorithm for attaining the Chebyshev bound in optimal learning. (arXiv:2307.01304v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Paruchuri_P/0/1/0/all/0/1">Pradyumna Paruchuri</a>, <a href="http://arxiv.org/find/math/1/au:+Chatterjee_D/0/1/0/all/0/1">Debasish Chatterjee</a></p>
<p>Given a compact subset of a Banach space, the Chebyshev center problem
consists of finding a minimal circumscribing ball containing the set. In this
article we establish a numerically tractable algorithm for solving the
Chebyshev center problem in the context of optimal learning from a finite set
of data points. For a hypothesis space realized as a compact but not
necessarily convex subset of a finite-dimensional subspace of some underlying
Banach space, this algorithm computes the Chebyshev radius and the Chebyshev
center of the hypothesis space, thereby solving the problem of optimal recovery
of functions from data. The algorithm itself is based on, and significantly
extends, recent results for near-optimal solutions of convex semi-infinite
problems by means of targeted sampling, and it is of independent interest.
Several examples of numerical computations of Chebyshev centers are included in
order to illustrate the effectiveness of the algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01316">Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharifi_I/0/1/0/all/0/1">Iman Sharifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yildirim_M/0/1/0/all/0/1">Mustafa Yildirim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1">Saber Fallah</a></p>
<p>The dynamic nature of driving environments and the presence of diverse road
users pose significant challenges for decision-making in autonomous driving.
Deep reinforcement learning (DRL) has emerged as a popular approach to tackle
this problem. However, the application of existing DRL solutions is mainly
confined to simulated environments due to safety concerns, impeding their
deployment in real-world. To overcome this limitation, this paper introduces a
novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics
(DRLSL) that combines the strengths of DRL (learning from experience) and
symbolic first-order logics knowledge-driven reasoning) to enable safe learning
in real-time interactions of autonomous driving within real environments. This
innovative approach provides a means to learn autonomous driving policies by
actively engaging with the physical environment while ensuring safety. We have
implemented the DRLSL framework in autonomous driving using the highD dataset
and demonstrated that our method successfully avoids unsafe actions during both
the training and testing phases. Furthermore, our results indicate that DRLSL
achieves faster convergence during training and exhibits better
generalizability to new driving scenarios compared to traditional DRL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01317">Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly. (arXiv:2307.01317v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianxiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Atad_M/0/1/0/all/0/1">Matan Atad</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1">Ismael Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Durner_M/0/1/0/all/0/1">Maximilian Durner</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1">Rudolph Triebel</a></p>
<p>Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP)
need to be introspective on the predicted solutions, i.e. whether they are
feasible or not, to circumvent potential efficiency degradation. Previous works
need both feasible and infeasible examples during training. However, the
infeasible ones are hard to collect sufficiently when re-training is required
for swift adaptation to new product variants. In this work, we propose a
density-based feasibility learning method that requires only feasible examples.
Concretely, we formulate the feasibility learning problem as
Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are
powerful generative models for estimating complex probability distributions.
Empirically, the proposed method is demonstrated on robotic assembly use cases
and outperforms other single-class baselines in detecting infeasible
assemblies. We further investigate the internal working mechanism of our method
and show that a large memory saving can be obtained based on an advanced
variant of NF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01325">Robust Uncertainty Estimation for Classification of Maritime Objects. (arXiv:2307.01325v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Becktor_J/0/1/0/all/0/1">Jonathan Becktor</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholler_F/0/1/0/all/0/1">Frederik Scholler</a>, <a href="http://arxiv.org/find/cs/1/au:+Boukas_E/0/1/0/all/0/1">Evangelos Boukas</a>, <a href="http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1">Lazaros Nalpantidis</a></p>
<p>We explore the use of uncertainty estimation in the maritime domain, showing
the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset,
SHIPS. We present a method joining the intra-class uncertainty achieved using
Monte Carlo Dropout, with recent discoveries in the field of outlier detection,
to gain more holistic uncertainty measures. We explore the relationship between
the introduced uncertainty measures and examine how well they work on CIFAR10
and in a real-life setting. Our work improves the FPR95 by 8% compared to the
current highest-performing work when the models are trained without
out-of-distribution data. We increase the performance by 77% compared to a
vanilla implementation of the Wide ResNet. We release the SHIPS dataset and
show the effectiveness of our method by improving the FPR95 by 44.2% with
respect to the baseline. Our approach is model agnostic, easy to implement, and
often does not require model retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01346">Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols. (arXiv:2307.01346v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goodwin_Allcock_T/0/1/0/all/0/1">Tobias Goodwin-Allcock</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1">Ting Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1">Robert Gray</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1">Parashkev Nachev</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hui Zhang</a></p>
<p>We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from
only six-direction diffusion weighted images (DWI). Deep learning-based methods
have been recently proposed for dMRI parameter estimation, using either
voxel-wise fully-connected neural networks (FCN) or image-wise convolutional
neural networks (CNN). In the acute clinical context -- where pressure of time
limits the number of imaged directions to a minimum -- existing approaches
either require an infeasible number of training images volumes (image-wise
CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for
tractogram estimation. To overcome these limitations, we propose Patch-CNN, a
neural network with a minimal (non-voxel-wise) convolutional kernel
(3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of
allowing the network to leverage local anatomical information. Compared with
image-wise CNNs, the minimal kernel vastly reduces training data demand.
Evaluated against both conventional model fitting and a voxel-wise FCN,
Patch-CNN, trained with a single subject is shown to improve the estimation of
both scalar dMRI parameters and fibre orientation from six-direction DWIs. The
improved fibre orientation estimation is shown to produce improved tractogram.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01354">Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator. (arXiv:2307.01354v1 [physics.comp-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Diab_W/0/1/0/all/0/1">Waleed Diab</a>, <a href="http://arxiv.org/find/physics/1/au:+Chaabi_O/0/1/0/all/0/1">Omar Chaabi</a>, <a href="http://arxiv.org/find/physics/1/au:+Alkobaisi_S/0/1/0/all/0/1">Shayma Alkobaisi</a>, <a href="http://arxiv.org/find/physics/1/au:+Awotunde_A/0/1/0/all/0/1">Abeeb Awotunde</a>, <a href="http://arxiv.org/find/physics/1/au:+Kobaisi_M/0/1/0/all/0/1">Mohammed Al Kobaisi</a></p>
<p>Traditional numerical schemes for simulating fluid flow and transport in
porous media can be computationally expensive. Advances in machine learning for
scientific computing have the potential to help speed up the simulation time in
many scientific and engineering fields. DeepONet has recently emerged as a
powerful tool for accelerating the solution of partial differential equations
(PDEs) by learning operators (mapping between function spaces) of PDEs. In this
work, we learn the mapping between the space of flux functions of the
Buckley-Leverett PDE and the space of solutions (saturations). We use
Physics-Informed DeepONets (PI-DeepONets) to achieve this mapping without any
paired input-output observations, except for a set of given initial or boundary
conditions; ergo, eliminating the expensive data generation process. By
leveraging the underlying physical laws via soft penalty constraints during
model training, in a manner similar to Physics-Informed Neural Networks
(PINNs), and a unique deep neural network architecture, the proposed
PI-DeepONet model can predict the solution accurately given any type of flux
function (concave, convex, or non-convex) while achieving up to four orders of
magnitude improvements in speed over traditional numerical solvers. Moreover,
the trained PI-DeepONet model demonstrates excellent generalization qualities,
rendering it a promising tool for accelerating the solution of transport
problems in porous media.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01357">Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Anish Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_K/0/1/0/all/0/1">Keegan Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Whitehouse_J/0/1/0/all/0/1">Justin Whitehouse</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a></p>
<p>Principal component regression (PCR) is a popular technique for fixed-design
error-in-variables regression, a generalization of the linear regression
setting in which the observed covariates are corrupted with random noise. We
provide the first time-uniform finite sample guarantees for online
(regularized) PCR whenever data is collected adaptively. Since the proof
techniques for analyzing PCR in the fixed design setting do not readily extend
to the online setting, our results rely on adapting tools from modern
martingale concentration to the error-in-variables setting. As an application
of our bounds, we provide a framework for experiment design in panel data
settings when interventions are assigned adaptively. Our framework may be
thought of as a generalization of the synthetic control and synthetic
interventions frameworks, where data is collected via an adaptive intervention
assignment policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01377">Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1">Matthew Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Penney_D/0/1/0/all/0/1">Drew Penney</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhong Chen</a></p>
<p>Transformer models using segment-based processing have been an effective
architecture for simultaneous speech translation. However, such models create a
context mismatch between training and inference environments, hindering
potential translation accuracy. We solve this issue by proposing Shiftable
Context, a simple yet effective scheme to ensure that consistent segment and
context sizes are maintained throughout training and inference, even with the
presence of partially filled segments due to the streaming nature of
simultaneous translation. Shiftable Context is also broadly applicable to
segment-based transformers for streaming tasks. Our experiments on the
English-German, English-French, and English-Spanish language pairs from the
MUST-C dataset demonstrate that when applied to the Augmented Memory
Transformer, a state-of-the-art model for simultaneous speech translation, the
proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU
scores across each wait-k value for the three language pairs, respectively,
with a minimal impact on computation-aware Average Lagging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01379">Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jinhao Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zavalny_A/0/1/0/all/0/1">Alex Zavalny</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Renjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1">Bhavya Kailkhura</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a></p>
<p>Although Large Language Models (LLMs) have shown great potential in Natural
Language Generation, it is still challenging to characterize the uncertainty of
model generations, i.e., when users could trust model outputs. Our research is
derived from the heuristic facts that tokens are created unequally in
reflecting the meaning of generations by auto-regressive LLMs, i.e., some
tokens are more relevant (or representative) than others, yet all the tokens
are equally valued when estimating uncertainty. It is because of the linguistic
redundancy where mostly a few keywords are sufficient to convey the meaning of
a long sentence. We name these inequalities as generative inequalities and
investigate how they affect uncertainty estimation. Our results reveal that
considerable tokens and sentences containing limited semantics are weighted
equally or even heavily when estimating uncertainty. To tackle these biases
posed by generative inequalities, we propose to jointly Shifting Attention to
more Relevant (SAR) components from both the token level and the sentence level
while estimating uncertainty. We conduct experiments over popular
"off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful
commercial LLMs (e.g., Davinci from OpenAI), across various free-form
question-answering tasks. Experimental results and detailed demographic
analysis indicate the superior performance of SAR. Code is available at
https://github.com/jinhaoduan/shifting-attention-to-relevance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01381">Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1">Matthew Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhong Chen</a></p>
<p>Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment's attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01384">Systematic Bias in Sample Inference and its Effect on Machine Learning. (arXiv:2307.01384v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+ONeill_O/0/1/0/all/0/1">Owen O&#x27;Neill</a>, <a href="http://arxiv.org/find/cs/1/au:+Costello_F/0/1/0/all/0/1">Fintan Costello</a></p>
<p>A commonly observed pattern in machine learning models is an underprediction
of the target feature, with the model's predicted target rate for members of a
given category typically being lower than the actual target rate for members of
that category in the training set. This underprediction is usually larger for
members of minority groups; while income level is underpredicted for both men
and women in the 'adult' dataset, for example, the degree of underprediction is
significantly higher for women (a minority in that dataset). We propose that
this pattern of underprediction for minorities arises as a predictable
consequence of statistical inference on small samples. When presented with a
new individual for classification, an ML model performs inference not on the
entire training set, but on a subset that is in some way similar to the new
individual, with sizes of these subsets typically following a power law
distribution so that most are small (and with these subsets being necessarily
smaller for the minority group). We show that such inference on small samples
is subject to systematic and directional statistical bias, and that this bias
produces the observed patterns of underprediction seen in ML models. Analysing
a standard sklearn decision tree model's predictions on a set of over 70
subsets of the 'adult' and COMPAS datasets, we found that a bias prediction
measure based on small-sample inference had a significant positive correlations
(0.56 and 0.85) with the observed underprediction rate for these subsets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01389">Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer&#x27;s Disease Progression via Counterfactual Inference. (arXiv:2307.01389v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1">Haixing Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Mengxuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dajiang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Diez_I/0/1/0/all/0/1">Ibai Diez</a>, <a href="http://arxiv.org/find/cs/1/au:+Sepulcre_J/0/1/0/all/0/1">Jorge Sepulcre</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xingyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Manhua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanzheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a></p>
<p>Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning
with amyloidosis, followed by neuronal loss and deterioration in structure,
function, and cognition. The accumulation of amyloid-beta in the brain,
measured through 18F-florbetapir (AV45) positron emission tomography (PET)
imaging, has been widely used for early diagnosis of AD. However, the
relationship between amyloid-beta accumulation and AD pathophysiology remains
unclear, and causal inference approaches are needed to uncover how amyloid-beta
levels can impact AD development. In this paper, we propose a graph varying
coefficient neural network (GVCNet) for estimating the individual treatment
effect with continuous treatment levels using a graph convolutional neural
network. We highlight the potential of causal inference approaches, including
GVCNet, for measuring the regional causal connections between amyloid-beta
accumulation and AD pathophysiology, which may serve as a robust tool for early
diagnosis and tailored care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01390">Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives. (arXiv:2307.01390v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lunghi_D/0/1/0/all/0/1">Danele Lunghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Simitsis_A/0/1/0/all/0/1">Alkis Simitsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Caelen_O/0/1/0/all/0/1">Olivier Caelen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1">Gianluca Bontempi</a></p>
<p>Data economy relies on data-driven systems and complex machine learning
applications are fueled by them. Unfortunately, however, machine learning
models are exposed to fraudulent activities and adversarial attacks, which
threaten their security and trustworthiness. In the last decade or so, the
research interest on adversarial machine learning has grown significantly,
revealing how learning applications could be severely impacted by effective
attacks. Although early results of adversarial machine learning indicate the
huge potential of the approach to specific domains such as image processing,
still there is a gap in both the research literature and practice regarding how
to generalize adversarial techniques in other domains and applications. Fraud
detection is a critical defense mechanism for data economy, as it is for other
applications as well, which poses several challenges for machine learning. In
this work, we describe how attacks against fraud detection systems differ from
other applications of adversarial machine learning, and propose a number of
interesting directions to bridge this gap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01393">Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I -- Analysis with a Small Sample Size. (arXiv:2307.01393v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamath_C/0/1/0/all/0/1">Chandrika Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Franzman_J/0/1/0/all/0/1">Juliette S. Franzman</a>, <a href="http://arxiv.org/find/cs/1/au:+Daub_B/0/1/0/all/0/1">Brian H. Daub</a></p>
<p>Computer simulations, especially of complex phenomena, can be expensive,
requiring high-performance computing resources. Often, to understand a
phenomenon, multiple simulations are run, each with a different set of
simulation input parameters. These data are then used to create an interpolant,
or surrogate, relating the simulation outputs to the corresponding inputs. When
the inputs and outputs are scalars, a simple machine learning model can
suffice. However, when the simulation outputs are vector valued, available at
locations in two or three spatial dimensions, often with a temporal component,
creating a surrogate is more challenging. In this report, we use a
two-dimensional problem of a jet interacting with high explosives to understand
how we can build high-quality surrogates. The characteristics of our data set
are unique - the vector-valued outputs from each simulation are available at
over two million spatial locations; each simulation is run for a relatively
small number of time steps; the size of the computational domain varies with
each simulation; and resource constraints limit the number of simulations we
can run. We show how we analyze these extremely large data-sets, set the
parameters for the algorithms used in the analysis, and use simple ways to
improve the accuracy of the spatio-temporal surrogates without substantially
increasing the number of simulations required.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01394">In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes. (arXiv:2307.01394v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perera_N/0/1/0/all/0/1">Niranda Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1">Arup Kumar Sarker</a>, <a href="http://arxiv.org/find/cs/1/au:+Staylor_M/0/1/0/all/0/1">Mills Staylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Laszewski_G/0/1/0/all/0/1">Gregor von Laszewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_K/0/1/0/all/0/1">Kaiying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamburugamuve_S/0/1/0/all/0/1">Supun Kamburugamuve</a>, <a href="http://arxiv.org/find/cs/1/au:+Widanage_C/0/1/0/all/0/1">Chathura Widanage</a>, <a href="http://arxiv.org/find/cs/1/au:+Abeykoon_V/0/1/0/all/0/1">Vibhatha Abeykoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanewela_T/0/1/0/all/0/1">Thejaka Amila Kanewela</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1">Geoffrey Fox</a></p>
<p>The Data Science domain has expanded monumentally in both research and
industry communities during the past decade, predominantly owing to the Big
Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are
bringing more complexities to data engineering applications, which are now
integrated into data processing pipelines to process terabytes of data.
Typically, a significant amount of time is spent on data preprocessing in these
pipelines, and hence improving its e fficiency directly impacts the overall
pipeline performance. The community has recently embraced the concept of
Dataframes as the de-facto data structure for data representation and
manipulation. However, the most widely used serial Dataframes today (R, pandas)
experience performance limitations while working on even moderately large data
sets. We believe that there is plenty of room for improvement by taking a look
at this problem from a high-performance computing point of view. In a prior
publication, we presented a set of parallel processing patterns for distributed
dataframe operators and the reference runtime implementation, Cylon [1]. In
this paper, we are expanding on the initial concept by introducing a cost model
for evaluating the said patterns. Furthermore, we evaluate the performance of
Cylon on the ORNL Summit supercomputer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01400">Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II -- Clustering Extremely High-Dimensional Grid-Based Data. (arXiv:2307.01400v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamath_C/0/1/0/all/0/1">Chandrika Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Franzman_J/0/1/0/all/0/1">Juliette S. Franzman</a></p>
<p>Building an accurate surrogate model for the spatio-temporal outputs of a
computer simulation is a challenging task. A simple approach to improve the
accuracy of the surrogate is to cluster the outputs based on similarity and
build a separate surrogate model for each cluster. This clustering is
relatively straightforward when the output at each time step is of moderate
size. However, when the spatial domain is represented by a large number of grid
points, numbering in the millions, the clustering of the data becomes more
challenging. In this report, we consider output data from simulations of a jet
interacting with high explosives. These data are available on spatial domains
of different sizes, at grid points that vary in their spatial coordinates, and
in a format that distributes the output across multiple files at each time step
of the simulation. We first describe how we bring these data into a consistent
format prior to clustering. Borrowing the idea of random projections from data
mining, we reduce the dimension of our data by a factor of thousand, making it
possible to use the iterative k-means method for clustering. We show how we can
use the randomness of both the random projections, and the choice of initial
centroids in k-means clustering, to determine the number of clusters in our
data set. Our approach makes clustering of extremely high dimensional data
tractable, generating meaningful cluster assignments for our problem, despite
the approximation introduced in the random projections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01403">Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1">Yat Long Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1">Biswa Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a>, <a href="http://arxiv.org/find/cs/1/au:+Noukhovitch_M/0/1/0/all/0/1">Michael Noukhovitch</a></p>
<p>Communication is a powerful tool for coordination in multi-agent RL. But
inducing an effective, common language is a difficult challenge, particularly
in the decentralized setting. In this work, we introduce an alternative
perspective where communicative messages sent between agents are considered as
different incomplete views of the environment state. By examining the
relationship between messages sent and received, we propose to learn to
communicate using contrastive learning to maximize the mutual information
between messages of a given trajectory. In communication-essential
environments, our method outperforms previous work in both performance and
learning speed. Using qualitative metrics and representation probing, we show
that our method induces more symmetric communication and captures global state
information from the environment. Overall, we show the power of contrastive
learning and the importance of leveraging messages as encodings for effective
communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01408">Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors. (arXiv:2307.01408v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1">Sushant Veer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Apoorva Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>Trajectory prediction modules are key enablers for safe and efficient
planning of autonomous vehicles (AVs), particularly in highly interactive
traffic scenarios. Recently, learning-based trajectory predictors have
experienced considerable success in providing state-of-the-art performance due
to their ability to learn multimodal behaviors of other agents from data. In
this paper, we present an algorithm called multi-predictor fusion (MPF) that
augments the performance of learning-based predictors by imbuing them with
motion planners that are tasked with satisfying logic-based rules. MPF
probabilistically combines learning- and rule-based predictors by mixing
trajectories from both standalone predictors in accordance with a belief
distribution that reflects the online performance of each predictor. In our
results, we show that MPF outperforms the two standalone predictors on various
metrics and delivers the most consistent performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01417">Free energy of Bayesian Convolutional Neural Network with Skip Connection. (arXiv:2307.01417v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagayasu_S/0/1/0/all/0/1">Shuya Nagayasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1">Sumio Watanabe</a></p>
<p>Since the success of Residual Network(ResNet), many of architectures of
Convolutional Neural Networks(CNNs) have adopted skip connection. While the
generalization performance of CNN with skip connection has been explained
within the framework of Ensemble Learning, the dependency on the number of
parameters have not been revealed. In this paper, we show that Bayesian free
energy of Convolutional Neural Network both with and without skip connection in
Bayesian learning. The upper bound of free energy of Bayesian CNN with skip
connection does not depend on the oveparametrization and, the generalization
error of Bayesian CNN has similar property.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01422">Generative Flow Networks: a Markov Chain Perspective. (arXiv:2307.01422v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1">Tristan Deleu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>While Markov chain Monte Carlo methods (MCMC) provide a general framework to
sample from a probability distribution defined up to normalization, they often
suffer from slow convergence to the target distribution when the latter is
highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been
proposed as an alternative framework to mitigate this issue when samples have a
clear compositional structure, by treating sampling as a sequential decision
making problem. Although they were initially introduced from the perspective of
flow networks, the recent advances of GFlowNets draw more and more inspiration
from the Markov chain literature, bypassing completely the need for flows. In
this paper, we formalize this connection and offer a new perspective for
GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless
of the nature of the state space as recurrent Markov chains. Positioning
GFlowNets under the same theoretical framework as MCMC methods also allows us
to identify the similarities between both frameworks, and most importantly to
highlight their
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01429">Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios. (arXiv:2307.01429v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dai_B/0/1/0/all/0/1">Baorui Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Frusque_G/0/1/0/all/0/1">Ga&#xeb;tan Frusque</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tianfu Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1">Qi Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>The application of unsupervised domain adaptation (UDA)-based fault diagnosis
methods has shown significant efficacy in industrial settings, facilitating the
transfer of operational experience and fault signatures between different
operating conditions, different units of a fleet or between simulated and real
data. However, in real industrial scenarios, unknown levels and types of noise
can amplify the difficulty of domain alignment, thus severely affecting the
diagnostic performance of deep learning models. To address this issue, we
propose an UDA method called Smart Filter-Aided Domain Adversarial Neural
Network (SFDANN) for fault diagnosis in noisy industrial scenarios. The
proposed methodology comprises two steps. In the first step, we develop a smart
filter that dynamically enforces similarity between the source and target
domain data in the time-frequency domain. This is achieved by combining a
learnable wavelet packet transform network (LWPT) and a traditional wavelet
packet transform module. In the second step, we input the data reconstructed by
the smart filter into a domain adversarial neural network (DANN). To learn
domain-invariant and discriminative features, the learnable modules of SFDANN
are trained in a unified manner with three objectives: time-frequency feature
proximity, domain alignment, and fault classification. We validate the
effectiveness of the proposed SFDANN method based on two fault diagnosis cases:
one involving fault diagnosis of bearings in noisy environments and another
involving fault diagnosis of slab tracks in a train-track-bridge coupling
vibration system, where the transfer task involves transferring from numerical
simulations to field measurements. Results show that compared to other
representative state of the art UDA methods, SFDANN exhibits superior
performance and remarkable stability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01434">Learning to Branch in Combinatorial Optimization with Graph Pointer Networks. (arXiv:2307.01434v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhiming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Ling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1">Xiangke Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kaiwen Li</a></p>
<p>Branch-and-bound is a typical way to solve combinatorial optimization
problems. This paper proposes a graph pointer network model for learning the
variable selection policy in the branch-and-bound. We extract the graph
features, global features and historical features to represent the solver
state. The proposed model, which combines the graph neural network and the
pointer mechanism, can effectively map from the solver state to the branching
variable decisions. The model is trained to imitate the classic strong
branching expert rule by a designed top-k Kullback-Leibler divergence loss
function. Experiments on a series of benchmark problems demonstrate that the
proposed approach significantly outperforms the widely used expert-designed
branching rules. Our approach also outperforms the state-of-the-art
machine-learning-based branch-and-bound methods in terms of solving speed and
search tree size on all the test instances. In addition, the model can
generalize to unseen instances and scale to larger instances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01446">On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1">Jonathan Pilault</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Can Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1">Markus Dreyer</a></p>
<p>Prompts have been shown to be an effective method to adapt a frozen
Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts
can be represented by a human-engineered word sequence or by a learned
continuous embedding. In this work, we investigate conditional and
compositional differentiable prompting. We propose a new model, Prompt
Production System (PRopS), which learns to transform task instructions or input
metadata, into continuous prompts that elicit task-specific outputs from the
PLM. Our model uses a modular network structure based on our neural formulation
of Production Systems, which allows the model to learn discrete rules -- neural
functions that learn to specialize in transforming particular prompt input
patterns, making it suitable for compositional transfer learning and few-shot
learning. We present extensive empirical and theoretical analysis and show that
PRopS consistently surpasses other PLM adaptation techniques, and often
improves upon fully fine-tuned models, on compositional generalization tasks,
controllable summarization and multilingual translation, while needing fewer
trainable parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01449">A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Morucci_M/0/1/0/all/0/1">Marco Morucci</a>, <a href="http://arxiv.org/find/stat/1/au:+Orlandi_V/0/1/0/all/0/1">Vittorio Orlandi</a>, <a href="http://arxiv.org/find/stat/1/au:+Parikh_H/0/1/0/all/0/1">Harsh Parikh</a>, <a href="http://arxiv.org/find/stat/1/au:+Roy_S/0/1/0/all/0/1">Sudeepa Roy</a>, <a href="http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a>, <a href="http://arxiv.org/find/stat/1/au:+Volfovsky_A/0/1/0/all/0/1">Alexander Volfovsky</a></p>
<p>Experimental and observational studies often lack validity due to untestable
assumptions. We propose a double machine learning approach to combine
experimental and observational studies, allowing practitioners to test for
assumption violations and estimate treatment effects consistently. Our
framework tests for violations of external validity and ignorability under
milder assumptions. When only one assumption is violated, we provide
semi-parametrically efficient treatment effect estimators. However, our
no-free-lunch theorem highlights the necessity of accurately identifying the
violated assumption for consistent treatment effect estimation. We demonstrate
the applicability of our approach in three real-world case studies,
highlighting its relevance for practical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01452">Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhihong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">Guodong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengqi Zhang</a></p>
<p>Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01470">A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding. (arXiv:2307.01470v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pavan Kumar Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1">Pranamesh Chakraborty</a></p>
<p>Driver gaze plays an important role in different gaze-based applications such
as driver attentiveness detection, visual distraction detection, gaze behavior
understanding, and building driver assistance system. The main objective of
this study is to perform a comprehensive summary of driver gaze fundamentals,
methods to estimate driver gaze, and it's applications in real world driving
scenarios. We first discuss the fundamentals related to driver gaze, involving
head-mounted and remote setup based gaze estimation and the terminologies used
for each of these data collection methods. Next, we list out the existing
benchmark driver gaze datasets, highlighting the collection methodology and the
equipment used for such data collection. This is followed by a discussion of
the algorithms used for driver gaze estimation, which primarily involves
traditional machine learning and deep learning based techniques. The estimated
driver gaze is then used for understanding gaze behavior while maneuvering
through intersections, on-ramps, off-ramps, lane changing, and determining the
effect of roadside advertising structures. Finally, we have discussed the
limitations in the existing literature, challenges, and the future scope in
driver gaze estimation and gaze-based applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01472">Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning. (arXiv:2307.01472v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Ling Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longbo Huang</a></p>
<p>We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline
Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms
that rely mainly on conservatism in policy design, DOM2 enhances policy
expressiveness and diversity based on diffusion. Specifically, we incorporate a
diffusion model into the policy network and propose a trajectory-based
data-augmentation scheme in training. These key ingredients make our algorithm
more robust to environment changes and achieve significant improvements in
performance, generalization and data-efficiency. Our extensive experimental
results demonstrate that DOM2 outperforms existing state-of-the-art methods in
multi-agent particle and multi-agent MuJoCo environments, and generalizes
significantly better in shifted environments thanks to its high expressiveness
and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve
state-of-the-art performance with $20+$ times less data compared to existing
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01482">Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_T/0/1/0/all/0/1">Tong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1">Guoyang Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jian Sun</a></p>
<p>Modeling and forecasting multivariate time series not only facilitates the
decision making of practitioners, but also deepens our scientific understanding
of the underlying dynamical systems. Spatial-temporal graph neural networks
(STGNNs) are emerged as powerful predictors and have become the de facto models
for learning spatiotemporal representations in recent years. However, existing
architectures of STGNNs tend to be complicated by stacking a series of fancy
layers. The designed models could be either redundant or enigmatic, which pose
great challenges on their complexity and scalability. Such concerns prompt us
to re-examine the designs of modern STGNNs and identify core principles that
contribute to a powerful and efficient neural predictor. Here we present a
compact predictive model that is fully defined by a dense encoder-decoder and a
message-passing layer, powered by node identifications, without any complex
sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical results
demonstrate how a simple and elegant model with proper inductive basis can
compare favorably w.r.t. the state of the art with elaborate designs, while
being much more interpretable and computationally efficient for
spatial-temporal forecasting problem. We hope our findings would open new
horizons for future studies to revisit the design of more concise neural
forecasting architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01494">Review of Deep Learning-based Malware Detection for Android and Windows System. (arXiv:2307.01494v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_N/0/1/0/all/0/1">Nazmul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seokjoo Shin</a></p>
<p>Differentiating malware is important to determine their behaviors and level
of threat; as well as to devise defensive strategy against them. In response,
various anti-malware systems have been developed to distinguish between
different malwares. However, most of the recent malware families are Artificial
Intelligence (AI) enable and can deceive traditional anti-malware systems using
different obfuscation techniques. Therefore, only AI-enabled anti-malware
system is robust against these techniques and can detect different features in
the malware files that aid in malicious activities. In this study we review two
AI-enabled techniques for detecting malware in Windows and Android operating
system, respectively. Both the techniques achieved perfect accuracy in
detecting various malware families.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01497">Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Ilandarideva_S/0/1/0/all/0/1">Sasila Ilandarideva</a>, <a href="http://arxiv.org/find/math/1/au:+Juditsky_A/0/1/0/all/0/1">Anatoli Juditsky</a>, <a href="http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1">Guanghui Lan</a>, <a href="http://arxiv.org/find/math/1/au:+Li_T/0/1/0/all/0/1">Tianjiao Li</a></p>
<p>We consider a class of stochastic smooth convex optimization problems under
rather general assumptions on the noise in the stochastic gradient observation.
As opposed to the classical problem setting in which the variance of noise is
assumed to be uniformly bounded, herein we assume that the variance of
stochastic gradients is related to the "sub-optimality" of the approximate
solutions delivered by the algorithm. Such problems naturally arise in a
variety of applications, in particular, in the well-known generalized linear
regression problem in statistics. However, to the best of our knowledge, none
of the existing stochastic approximation algorithms for solving this class of
problems attain optimality in terms of the dependence on accuracy, problem
parameters, and mini-batch size.
</p>
<p>We discuss two non-Euclidean accelerated stochastic approximation
routines--stochastic accelerated gradient descent (SAGD) and stochastic
gradient extrapolation (SGE)--which carry a particular duality relationship. We
show that both SAGD and SGE, under appropriate conditions, achieve the optimal
convergence rate, attaining the optimal iteration and sample complexities
simultaneously. However, corresponding assumptions for the SGE algorithm are
more general; they allow, for instance, for efficient application of the SGE to
statistical estimation problems under heavy tail noises and discontinuous score
functions. We also discuss the application of the SGE to problems satisfying
quadratic growth conditions, and show how it can be used to recover sparse
solutions. Finally, we report on some simulation experiments to illustrate
numerical performance of our proposed algorithms in high-dimensional settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01504">All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangguo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1">Jihong Guan</a></p>
<p>Recently, ''pre-training and fine-tuning'' has been adopted as a standard
workflow for many graph tasks since it can take general graph knowledge to
relieve the lack of graph annotations from each application. However, graph
tasks with node level, edge level, and graph level are far diversified, making
the pre-training pretext often incompatible with these multiple tasks. This gap
may even cause a ''negative transfer'' to the specific application, leading to
poor results. Inspired by the prompt learning in natural language processing
(NLP), which has presented significant effectiveness in leveraging prior
knowledge for various NLP tasks, we study the prompting topic for graphs with
the motivation of filling the gap between pre-trained models and various graph
tasks. In this paper, we propose a novel multi-task prompting method for graph
models. Specifically, we first unify the format of graph prompts and language
prompts with the prompt token, token structure, and inserting pattern. In this
way, the prompting idea from NLP can be seamlessly introduced to the graph
area. Then, to further narrow the gap between various graph tasks and
state-of-the-art pre-training strategies, we further study the task space of
various graph applications and reformulate downstream problems to the
graph-level task. Afterward, we introduce meta-learning to efficiently learn a
better initialization for the multi-task prompt of graphs so that our prompting
framework can be more reliable and general for different tasks. We conduct
extensive experiments, results from which demonstrate the superiority of our
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01507">Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction. (arXiv:2307.01507v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Mengying Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guizhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Biao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yuanchao Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Weiqiang Jin</a></p>
<p>Relation-aware subgraph embedding is promising for predicting
multi-relational drug-drug interactions (DDIs). Typically, most existing
methods begin by constructing a multi-relational DDI graph and then learning
relation-aware subgraph embeddings (RaSEs) of drugs from the DDI graph.
However, most existing approaches are usually limited in learning RaSEs of new
drugs, leading to serious over-fitting when the test DDIs involve such drugs.
To alleviate this issue, We propose a novel DDI prediction method based on
relation-aware subgraph embedding with co-contrastive learning, RaSECo. RaSECo
constructs two heterogeneous drug graphs: a multi-relational DDI graph and a
multi-attributes-based drug-drug similarity (DDS) graph. The two graphs are
used respectively for learning and propagating the RaSEs of drugs, thereby
ensuring that all drugs, including new ones, can aggregate effective RaSEs.
Additionally, we employ a cross-view contrastive mechanism to enhance drug-pair
(DP) embedding. RaSECo learns DP embeddings from two distinct views
(interaction and similarity views) and encourages these views to supervise each
other collaboratively to obtain more discriminative DP embeddings. We evaluate
the effectiveness of our RaSECo on three different tasks using two real
datasets. The experimental results demonstrate that RaSECo outperforms existing
state-of-the-art prediction methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01514">SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1">Sunder Ali Khowaja</a>, <a href="http://arxiv.org/find/cs/1/au:+Dev_K/0/1/0/all/0/1">Kapal Dev</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1">Syed Muhammad Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Linguraru_M/0/1/0/all/0/1">Marius George Linguraru</a></p>
<p>Self-supervised learning in federated learning paradigm has been gaining a
lot of interest both in industry and research due to the collaborative learning
capability on unlabeled yet isolated data. However, self-supervised based
federated learning strategies suffer from performance degradation due to label
scarcity and diverse data distributions, i.e., data heterogeneity. In this
paper, we propose the SelfFed framework for Internet of Medical Things (IoMT).
Our proposed SelfFed framework works in two phases. The first phase is the
pre-training paradigm that performs augmentive modeling using Swin Transformer
based encoder in a decentralized manner. The first phase of SelfFed framework
helps to overcome the data heterogeneity issue. The second phase is the
fine-tuning paradigm that introduces contrastive network and a novel
aggregation strategy that is trained on limited labeled data for a target task
in a decentralized manner. This fine-tuning stage overcomes the label scarcity
problem. We perform our experimental analysis on publicly available medical
imaging datasets and show that our proposed SelfFed framework performs better
when compared to existing baselines concerning non-independent and identically
distributed (IID) data and label scarcity. Our method achieves a maximum
improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID
dataset. Further, our proposed method outperforms existing baselines even when
trained on a few (10%) labeled instances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01519">Deep Attention Q-Network for Personalized Treatment Recommendation. (arXiv:2307.01519v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Simin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Junghwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Serban_N/0/1/0/all/0/1">Nicoleta Serban</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shihao Yang</a></p>
<p>Tailoring treatment for individual patients is crucial yet challenging in
order to achieve optimal healthcare outcomes. Recent advances in reinforcement
learning offer promising personalized treatment recommendations; however, they
rely solely on current patient observations (vital signs, demographics) as the
patient's state, which may not accurately represent the true health status of
the patient. This limitation hampers policy learning and evaluation, ultimately
limiting treatment effectiveness. In this study, we propose the Deep Attention
Q-Network for personalized treatment recommendations, utilizing the Transformer
architecture within a deep reinforcement learning framework to efficiently
incorporate all past patient observations. We evaluated the model on real-world
sepsis and acute hypotension cohorts, demonstrating its superiority to
state-of-the-art models. The source code for our model is available at
https://github.com/stevenmsm/RL-ICU-DAQN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01524">Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kakaiya_R/0/1/0/all/0/1">Ravi Kakaiya</a>, <a href="http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1">Rakshith Sathish</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethuraman_R/0/1/0/all/0/1">Ramanathan Sethuraman</a></p>
<p>Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the
potential to radically change the way we travel. Many such vehicles currently
rely on segmentation and object detection algorithms to detect and track
objects around its surrounding. The data collected from the vehicles are often
sent to cloud servers to facilitate continual/life-long learning of these
algorithms. Considering the bandwidth constraints, the data is compressed
before sending it to servers, where it is typically decompressed for training
and analysis. In this work, we propose the use of a learning-based compression
Codec to reduce the overhead in latency incurred for the decompression
operation in the standard pipeline. We demonstrate that the learned compressed
representation can also be used to perform tasks like semantic segmentation in
addition to decompression to obtain the images. We experimentally validate the
proposed pipeline on the Cityscapes dataset, where we achieve a compression
factor up to $66 \times$ while preserving the information required to perform
segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved
using decompressed images while reducing the overall compute by $11\%$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01558">Scalable variable selection for two-view learning tasks with projection operators. (arXiv:2307.01558v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Szedmak_S/0/1/0/all/0/1">Sandor Szedmak</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Huusari_R/0/1/0/all/0/1">Riikka Huusari</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Tat Hong Duong Le</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Rousu_J/0/1/0/all/0/1">Juho Rousu</a> (1) ((1) Department of Computer Science, Aalto University, Espoo, Finland)</p>
<p>In this paper we propose a novel variable selection method for two-view
settings, or for vector-valued supervised learning problems. Our framework is
able to handle extremely large scale selection tasks, where number of data
samples could be even millions. In a nutshell, our method performs variable
selection by iteratively selecting variables that are highly correlated with
the output variables, but which are not correlated with the previously chosen
variables. To measure the correlation, our method uses the concept of
projection operators and their algebra. With the projection operators the
relationship, correlation, between sets of input and output variables can also
be expressed by kernel functions, thus nonlinear correlation models can be
exploited as well. We experimentally validate our approach, showing on both
synthetic and real data its scalability and the relevance of the selected
features. Keywords: Supervised variable selection, vector-valued learning,
projection-valued measure, reproducing kernel Hilbert space
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01559">Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones. (arXiv:2307.01559v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cereda_E/0/1/0/all/0/1">Elia Cereda</a>, <a href="http://arxiv.org/find/cs/1/au:+Giusti_A/0/1/0/all/0/1">Alessandro Giusti</a>, <a href="http://arxiv.org/find/cs/1/au:+Palossi_D/0/1/0/all/0/1">Daniele Palossi</a></p>
<p>Palm-sized nano-drones are an appealing class of edge nodes, but their
limited computational resources prevent running large deep-learning models
onboard. Adopting an edge-fog computational paradigm, we can offload part of
the computation to the fog; however, this poses security concerns if the fog
node, or the communication link, can not be trusted. To tackle this concern, we
propose a novel distributed edge-fog execution scheme that validates fog
computation by redundantly executing a random subnetwork aboard our nano-drone.
Compared to a State-of-the-Art visual pose estimation network that entirely
runs onboard, a larger network executed in a distributed way improves the $R^2$
score by +0.19; in case of attack, our approach detects it within 2s with 95%
probability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01563">Approximate information for efficient exploration-exploitation strategies. (arXiv:2307.01563v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Barbier_Chebbah_A/0/1/0/all/0/1">Alex Barbier-Chebbah</a> (IP, CNRS, UPCit&#xe9;), <a href="http://arxiv.org/find/stat/1/au:+Vestergaard_C/0/1/0/all/0/1">Christian L. Vestergaard</a> (IP, CNRS, UPCit&#xe9;), <a href="http://arxiv.org/find/stat/1/au:+Masson_J/0/1/0/all/0/1">Jean-Baptiste Masson</a> (IP, CNRS, UPCit&#xe9;)</p>
<p>This paper addresses the exploration-exploitation dilemma inherent in
decision-making, focusing on multi-armed bandit problems. The problems involve
an agent deciding whether to exploit current knowledge for immediate gains or
explore new avenues for potential long-term rewards. We here introduce a novel
algorithm, approximate information maximization (AIM), which employs an
analytical approximation of the entropy gradient to choose which arm to pull at
each point in time. AIM matches the performance of Infomax and Thompson
sampling while also offering enhanced computational speed, determinism, and
tractability. Empirical evaluation of AIM indicates its compliance with the
Lai-Robbins asymptotic bound and demonstrates its robustness for a range of
priors. Its expression is tunable, which allows for specific optimization in
various settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01578">Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchesoni_Acland_F/0/1/0/all/0/1">Franco Marchesoni-Acland</a>, <a href="http://arxiv.org/find/cs/1/au:+Morel_J/0/1/0/all/0/1">Jean-Michel Morel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kherroubi_J/0/1/0/all/0/1">Josselin Kherroubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1">Gabriele Facciolo</a></p>
<p>Even though data annotation is extremely important for interpretability,
research and development of artificial intelligence solutions, most research
efforts such as active learning or few-shot learning focus on the sample
efficiency problem. This paper studies the neglected complementary problem of
getting annotated data given a predictor. For the simple binary classification
setting, we present the spectrum ranging from optimal general solutions to
practical efficient methods. The problem is framed as the full annotation of a
binary classification dataset with the minimal number of yes/no questions when
a predictor is available. For the case of general binary questions the solution
is found in coding theory, where the optimal questioning strategy is given by
the Huffman encoding of the possible labelings. However, this approach is
computationally intractable even for small dataset sizes. We propose an
alternative practical solution based on several heuristics and lookahead
minimization of proxy cost functions. The proposed solution is analysed,
compared with optimal solutions and evaluated on several synthetic and
real-world datasets. On these datasets, the method allows a significant
improvement ($23-86\%$) in annotation efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01582">IAdet: Simplest human-in-the-loop object detection. (arXiv:2307.01582v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchesoni_Acland_F/0/1/0/all/0/1">Franco Marchesoni-Acland</a>, <a href="http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1">Gabriele Facciolo</a></p>
<p>This work proposes a strategy for training models while annotating data named
Intelligent Annotation (IA). IA involves three modules: (1) assisted data
annotation, (2) background model training, and (3) active selection of the next
datapoints. Under this framework, we open-source the IAdet tool, which is
specific for single-class object detection. Additionally, we devise a method
for automatically evaluating such a human-in-the-loop system. For the PASCAL
VOC dataset, the IAdet tool reduces the database annotation time by $25\%$
while providing a trained model for free. These results are obtained for a
deliberately very simple IAdet design. As a consequence, IAdet is susceptible
to multiple easy improvements, paving the way for powerful human-in-the-loop
object detection systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01583">Learning Lie Group Symmetry Transformations with Neural Networks. (arXiv:2307.01583v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gabel_A/0/1/0/all/0/1">Alex Gabel</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_V/0/1/0/all/0/1">Victoria Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Valperga_R/0/1/0/all/0/1">Riccardo Valperga</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamb_J/0/1/0/all/0/1">Jeroen S. W. Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1">Kevin Webster</a>, <a href="http://arxiv.org/find/cs/1/au:+Quax_R/0/1/0/all/0/1">Rick Quax</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1">Efstratios Gavves</a></p>
<p>The problem of detecting and quantifying the presence of symmetries in
datasets is useful for model selection, generative modeling, and data analysis,
amongst others. While existing methods for hard-coding transformations in
neural networks require prior knowledge of the symmetries of the task at hand,
this work focuses on discovering and characterizing unknown symmetries present
in the dataset, namely, Lie group symmetry transformations beyond the
traditional ones usually considered in the field (rotation, scaling, and
translation). Specifically, we consider a scenario in which a dataset has been
transformed by a one-parameter subgroup of transformations with different
parameter values for each data point. Our goal is to characterize the
transformation group and the distribution of the parameter values. The results
showcase the effectiveness of the approach in both these settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01593">Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising. (arXiv:2307.01593v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Ping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jian Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongkang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengye Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingxing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a></p>
<p>The effectiveness of ad creatives is greatly influenced by their visual
appearance. Advertising platforms can generate ad creatives with different
appearances by combining creative elements provided by advertisers. However,
with the increasing number of ad creative elements, it becomes challenging to
select a suitable combination from the countless possibilities. The industry's
mainstream approach is to select individual creative elements independently,
which often overlooks the importance of interaction between creative elements
during the modeling process. In response, this paper proposes a Cross-Element
Combinatorial Selection framework for multiple creative elements, termed CECS.
In the encoder process, a cross-element interaction is adopted to dynamically
adjust the expression of a single creative element based on the current
candidate creatives. In the decoder process, the creative combination problem
is transformed into a cascade selection problem of multiple creative elements.
A pointer mechanism with a cascade design is used to model the associations
among candidates. Comprehensive experiments on real-world datasets show that
CECS achieved the SOTA score on offline metrics. Moreover, the CECS algorithm
has been deployed in our industrial application, resulting in a significant
6.02% CTR and 10.37% GMV lift, which is beneficial to the business.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01597">Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jingyuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Heling Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuantao Gu</a></p>
<p>Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in
various domains. While state-of-the-art deep learning models excel in regular
Time Series Forecasting (TSF), they struggle to achieve comparable results in
PHSF. This can be attributed to the challenges posed by the high degree of
non-stationarity in peak-hour series, which makes direct forecasting more
difficult than standard TSF. Additionally, manually extracting the maximum
value from regular forecasting results leads to suboptimal performance due to
models minimizing the mean deficit. To address these issues, this paper
presents Seq2Peak, a novel framework designed specifically for PHSF tasks,
bridging the performance gap observed in TSF models. Seq2Peak offers two key
components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and
a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid
loss function that utilizes both the original series and peak-hour series as
supervised signals. Extensive experimentation on publicly available time series
datasets demonstrates the effectiveness of the proposed framework, yielding a
remarkable average relative improvement of 37.7\% across four real-world
datasets for both transformer- and non-transformer-based TSF models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01599">A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management. (arXiv:2307.01599v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Huang_Z/0/1/0/all/0/1">Zhenhan Huang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Tanaka_F/0/1/0/all/0/1">Fumihide Tanaka</a></p>
<p>On-chain data (metrics) of blockchain networks, akin to company fundamentals,
provide crucial and comprehensive insights into the networks. Despite their
informative nature, on-chain data have not been utilized in reinforcement
learning (RL)-based systems for cryptocurrency (crypto) portfolio management
(PM). An intriguing subject is the extent to which the utilization of on-chain
data can enhance an RL-based system's return performance compared to baselines.
Therefore, in this study, we propose CryptoRLPM, a novel RL-based system
incorporating on-chain data for end-to-end crypto PM. CryptoRLPM consists of
five units, spanning from information comprehension to trading order execution.
In CryptoRLPM, the on-chain data are tested and specified for each crypto to
solve the issue of ineffectiveness of metrics. Moreover, the scalable nature of
CryptoRLPM allows changes in the portfolios' cryptos at any time. Backtesting
results on three portfolios indicate that CryptoRLPM outperforms all the
baselines in terms of accumulated rate of return (ARR), daily rate of return
(DRR), and Sortino ratio (SR). Particularly, when compared to Bitcoin,
CryptoRLPM enhances the ARR, DRR, and SR by at least 83.14%, 0.5603%, and
2.1767 respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01601">Prototypes as Explanation for Time Series Anomaly Detection. (arXiv:2307.01601v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jentsch_C/0/1/0/all/0/1">Carsten Jentsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_E/0/1/0/all/0/1">Emmanuel M&#xfc;ller</a></p>
<p>Detecting abnormal patterns that deviate from a certain regular repeating
pattern in time series is essential in many big data applications. However, the
lack of labels, the dynamic nature of time series data, and unforeseeable
abnormal behaviors make the detection process challenging. Despite the success
of recent deep anomaly detection approaches, the mystical mechanisms in such
black-box models have become a new challenge in safety-critical applications.
The lack of model transparency and prediction reliability hinders further
breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as
the example-based explanation for the state of regular patterns during anomaly
detection. Without significant impact on the detection performance, prototypes
shed light on the deep black-box models and provide intuitive understanding for
domain experts and stakeholders. We extend the widely used prototype learning
in classification problems into anomaly detection. By visualizing both the
latent space and input space prototypes, we intuitively demonstrate how regular
data are modeled and why specific patterns are considered abnormal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01610">Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zitao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattabiraman_K/0/1/0/all/0/1">Karthik Pattabiraman</a></p>
<p>Machine learning (ML) models are vulnerable to membership inference attacks
(MIAs), which determine whether a given input is used for training the target
model. While there have been many efforts to mitigate MIAs, they often suffer
from limited privacy protection, large accuracy drop, and/or requiring
additional data that may be difficult to acquire. This work proposes a defense
technique, HAMP that can achieve both strong membership privacy and high
accuracy, without requiring extra data. To mitigate MIAs in different forms, we
observe that they can be unified as they all exploit the ML model's
overconfidence in predicting training samples through different proxies. This
motivates our design to enforce less confident prediction by the model, hence
forcing the model to behave similarly on the training and testing samples. HAMP
consists of a novel training framework with high-entropy soft labels and an
entropy-based regularizer to constrain the model's prediction while still
achieving high accuracy. To further reduce privacy risk, HAMP uniformly
modifies all the prediction outputs to become low-confidence outputs while
preserving the accuracy, which effectively obscures the differences between the
prediction on members and non-members. We conduct extensive evaluation on five
benchmark datasets, and show that HAMP provides consistently high accuracy and
strong membership privacy. Our comparison with seven state-of-the-art defenses
shows that HAMP achieves a superior privacy-utility trade off than those
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01616">SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuantao Gu</a></p>
<p>Multivariate time series forecasting plays a critical role in diverse
domains. While recent advancements in deep learning methods, especially
Transformers, have shown promise, there remains a gap in addressing the
significance of inter-series dependencies. This paper introduces SageFormer, a
Series-aware Graph-enhanced Transformer model designed to effectively capture
and model dependencies between series using graph structures. SageFormer
tackles two key challenges: effectively representing diverse temporal patterns
across series and mitigating redundant information among series. Importantly,
the proposed series-aware framework seamlessly integrates with existing
Transformer-based models, augmenting their ability to model inter-series
dependencies. Through extensive experiments on real-world and synthetic
datasets, we showcase the superior performance of SageFormer compared to
previous state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01622">Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network. (arXiv:2307.01622v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakip_M/0/1/0/all/0/1">Mert Nak&#x131;p</a>, <a href="http://arxiv.org/find/cs/1/au:+Copur_O/0/1/0/all/0/1">Onur &#xc7;opur</a>, <a href="http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1">Emrah Biyik</a>, <a href="http://arxiv.org/find/cs/1/au:+Guzelis_C/0/1/0/all/0/1">C&#xfc;neyt G&#xfc;zeli&#x15f;</a></p>
<p>Smart home energy management systems help the distribution grid operate more
efficiently and reliably, and enable effective penetration of distributed
renewable energy sources. These systems rely on robust forecasting,
optimization, and control/scheduling algorithms that can handle the uncertain
nature of demand and renewable generation. This paper proposes an advanced ML
algorithm, called Recurrent Trend Predictive Neural Network based Forecast
Embedded Scheduling (rTPNN-FES), to provide efficient residential demand
control. rTPNN-FES is a novel neural network architecture that simultaneously
forecasts renewable energy generation and schedules household appliances. By
its embedded structure, rTPNN-FES eliminates the utilization of separate
algorithms for forecasting and scheduling and generates a schedule that is
robust against forecasting errors. This paper also evaluates the performance of
the proposed algorithm for an IoT-enabled smart home. The evaluation results
reveal that rTPNN-FES provides near-optimal scheduling $37.5$ times faster than
the optimization while outperforming state-of-the-art forecasting techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01636">HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks. (arXiv:2307.01636v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guanghui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhennan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chunfeng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yihua Huang</a></p>
<p>Heterogeneous graph neural networks (GNNs) have been successful in handling
heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an
essential role. However, recent work pointed out that simple homogeneous graph
model without meta-path can also achieve comparable results, which calls into
question the necessity of meta-path. In this paper, we first present the
intrinsic difference about meta-path-based and meta-path-free models, i.e., how
to select neighbors for node aggregation. Then, we propose a novel framework to
utilize the rich type semantic information in heterogeneous graphs
comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The
core of HAGNN is to leverage the meta-path neighbors and the directly connected
neighbors simultaneously for node aggregations. HAGNN divides the overall
aggregation process into two phases: meta-path-based intra-type aggregation and
meta-path-free inter-type aggregation. During the intra-type aggregation phase,
we propose a new data structure called fused meta-path graph and perform
structural semantic aware aggregation on it. Finally, we combine the embeddings
generated by each phase. Compared with existing heterogeneous GNN models, HAGNN
can take full advantage of the heterogeneity in heterogeneous graphs. Extensive
experimental results on node classification, node clustering, and link
prediction tasks show that HAGNN outperforms the existing modes, demonstrating
the effectiveness of HAGNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01639">Heuristic Algorithms for the Approximation of Mutual Coherence. (arXiv:2307.01639v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Betz_G/0/1/0/all/0/1">Gregor Betz</a>, <a href="http://arxiv.org/find/cs/1/au:+Chekan_V/0/1/0/all/0/1">Vera Chekan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mchedlidze_T/0/1/0/all/0/1">Tamara Mchedlidze</a></p>
<p>Mutual coherence is a measure of similarity between two opinions. Although
the notion comes from philosophy, it is essential for a wide range of
technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters
to find candidates that are the closest to their political preferences. The
exact computation of mutual coherence is highly time-consuming due to the
iteration over all subsets of an opinion. Moreover, for every subset, an
instance of the SAT model counting problem has to be solved which is known to
be a hard problem in computer science. This work is the first study to
accelerate this computation. We model the distribution of the so-called
confirmation values as a mixture of three Gaussians and present efficient
heuristics to estimate its model parameters. The mutual coherence is then
approximated with the expected value of the distribution. Some of the presented
algorithms are fully polynomial-time, others only require solving a small
number of instances of the SAT model counting problem. The average squared
error of our best algorithm lies below 0.0035 which is insignificant if the
efficiency is taken into account. Furthermore, the accuracy is precise enough
to be used in Wahl-O-Mat-like systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01646">SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhengyang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Renjie Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lele Wang</a></p>
<p>Diffusion models based on permutation-equivariant networks can learn
permutation-invariant distributions for graph data. However, in comparison to
their non-invariant counterparts, we have found that these invariant models
encounter greater learning challenges since 1) their effective target
distributions exhibit more modes; 2) their optimal one-step denoising scores
are the score functions of Gaussian mixtures with more components. Motivated by
this analysis, we propose a non-invariant diffusion model, called
$\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message
passing network and utilizes shifted window based self-attention inspired by
SwinTransformers. Further, through systematic ablations, we identify several
critical training and sampling techniques that significantly improve the sample
quality of graph generation. At last, we introduce a simple post-processing
trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably
converts any graph generative model to a permutation-invariant one. Extensive
experiments on synthetic and real-world protein and molecule datasets show that
our SwinGNN achieves state-of-the-art performances. Our code is released at
https://github.com/qiyan98/SwinGNN .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01649">Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minshuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengdi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Xiang Wang</a></p>
<p>Convolutional residual neural networks (ConvResNets), though
overparameterized, can achieve remarkable prediction performance in practice,
which cannot be well explained by conventional wisdom. To bridge this gap, we
study the performance of ConvResNeXts, which cover ConvResNets as a special
case, trained with weight decay from the perspective of nonparametric
classification. Our analysis allows for infinitely many building blocks in
ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these
blocks. Specifically, we consider a smooth target function supported on a
low-dimensional manifold, then prove that ConvResNeXts can adapt to the
function smoothness and low-dimensional structures and efficiently learn the
function without suffering from the curse of dimensionality. Our findings
partially justify the advantage of overparameterized ConvResNeXts over
conventional machine learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01668">Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weijian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Tianyang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiacheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihua Zhang</a></p>
<p>Energy-Based Models (EBMs) have been widely used for generative modeling.
Contrastive Divergence (CD), a prevailing training objective for EBMs, requires
sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which
leads to an irreconcilable trade-off between the computational burden and the
validity of the CD. Running MCMCs till convergence is computationally
intensive. On the other hand, short-run MCMC brings in an extra non-negligible
parameter gradient term that is difficult to handle. In this paper, we provide
a general interpretation of CD, viewing it as a special instance of our
proposed Diffusion Contrastive Divergence (DCD) family. By replacing the
Langevin dynamic used in CD with other EBM-parameter-free diffusion processes,
we propose a more efficient divergence. We show that the proposed DCDs are both
more computationally efficient than the CD and are not limited to a
non-negligible gradient term. We conduct intensive experiments, including both
synthesis data modeling and high-dimensional image denoising and generation, to
show the advantages of the proposed DCDs. On the synthetic data learning and
image denoising experiments, our proposed DCD outperforms CD by a large margin.
In image generation experiments, the proposed DCD is capable of training an
energy-based model for generating the Celab-A $32\times 32$ dataset, which is
comparable to existing EBMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01683">Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berger_G/0/1/0/all/0/1">Guy Berger</a>, <a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1">Aviv Navon</a>, <a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1">Ethan Fetaya</a></p>
<p>In computer vision and machine learning, a crucial challenge is to lower the
computation and memory demands for neural network inference. A commonplace
solution to address this challenge is through the use of binarization. By
binarizing the network weights and activations, one can significantly reduce
computational complexity by substituting the computationally expensive floating
operations with faster bitwise operations. This leads to a more efficient
neural network inference that can be deployed on low-resource devices. In this
work, we extend previous approaches that trained networks with discrete weights
using the local reparameterization trick to also allow for discrete
activations. The original approach optimized a distribution over the discrete
weights and uses the central limit theorem to approximate the pre-activation
with a continuous Gaussian distribution. Here we show that the probabilistic
modeling can also allow effective training of networks with discrete activation
as well. This further reduces runtime and memory footprint at inference time
with state-of-the-art results for networks with binary activations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01684">Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services. (arXiv:2307.01684v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Liekang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Peng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Ke Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoxi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhi Zhou</a></p>
<p>Graph Neural Networks (GNNs) have gained growing interest in miscellaneous
applications owing to their outstanding ability in extracting latent
representation on graph structures. To render GNN-based service for IoT-driven
smart applications, traditional model serving paradigms usually resort to the
cloud by fully uploading geo-distributed input data to remote datacenters.
However, our empirical measurements reveal the significant communication
overhead of such cloud-based serving and highlight the profound potential in
applying the emerging fog computing. To maximize the architectural benefits
brought by fog computing, in this paper, we present Fograph, a novel
distributed real-time GNN inference framework that leverages diverse and
dynamic resources of multiple fog nodes in proximity to IoT data sources. By
introducing heterogeneity-aware execution planning and GNN-specific compression
techniques, Fograph tailors its design to well accommodate the unique
characteristics of GNN serving in fog environments. Prototype-based evaluation
and case study demonstrate that Fograph significantly outperforms the
state-of-the-art cloud serving and fog deployment by up to 5.39x execution
speedup and 6.84x throughput improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01689">Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Assos_A/0/1/0/all/0/1">Angelos Assos</a>, <a href="http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1">Idan Attias</a>, <a href="http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1">Yuval Dagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1">Constantinos Daskalakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1">Maxwell Fishelson</a></p>
<p>While ERM suffices to attain near-optimal generalization error in the
stochastic learning setting, this is not known to be the case in the online
learning setting, where algorithms for general concept classes rely on
computationally inefficient oracles such as the Standard Optimal Algorithm
(SOA). In this work, we propose an algorithm for online binary classification
setting that relies solely on ERM oracle calls, and show that it has finite
regret in the realizable setting and sublinearly growing regret in the agnostic
setting. We bound the regret in terms of the Littlestone and threshold
dimensions of the underlying concept class.
</p>
<p>We obtain similar results for nonparametric games, where the ERM oracle can
be interpreted as a best response oracle, finding the best response of a player
to a given history of play of the other players. In this setting, we provide
learning algorithms that only rely on best response oracles and converge to
approximate-minimax equilibria in two-player zero-sum games and approximate
coarse correlated equilibria in multi-player general-sum games, as long as the
game has a bounded fat-threshold dimension. Our algorithms apply to both
binary-valued and real-valued games and can be viewed as providing
justification for the wide use of double oracle and multiple oracle algorithms
in the practice of solving large games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01708">Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1">Tyler Kastner</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1">Murat A. Erdogdu</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1">Amir-massoud Farahmand</a></p>
<p>We consider the problem of learning models for risk-sensitive reinforcement
learning. We theoretically demonstrate that proper value equivalence, a method
of learning models which can be used to plan optimally in the risk-neutral
setting, is not sufficient to plan optimally in the risk-sensitive setting. We
leverage distributional reinforcement learning to introduce two new notions of
model equivalence, one which is general and can be used to plan for any risk
measure, but is intractable; and a practical variation which allows one to
choose which risk measures they may plan optimally for. We demonstrate how our
framework can be used to augment any model-free risk-sensitive algorithm, and
provide both tabular and large-scale experiments to demonstrate its ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01715">Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Segev_E/0/1/0/all/0/1">Eliya Segev</a>, <a href="http://arxiv.org/find/cs/1/au:+Alroy_M/0/1/0/all/0/1">Maya Alroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Katsir_R/0/1/0/all/0/1">Ronen Katsir</a>, <a href="http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1">Noam Wies</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenhav_A/0/1/0/all/0/1">Ayana Shenhav</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Oren_Y/0/1/0/all/0/1">Yael Ben-Oren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zar_D/0/1/0/all/0/1">David Zar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tadmor_O/0/1/0/all/0/1">Oren Tadmor</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitterman_J/0/1/0/all/0/1">Jacob Bitterman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1">Amnon Shashua</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenwein_T/0/1/0/all/0/1">Tal Rosenwein</a></p>
<p>Connectionist Temporal Classification (CTC) is a widely used criterion for
training supervised sequence-to-sequence (seq2seq) models. It enables learning
the relations between input and output sequences, termed alignments, by
marginalizing over perfect alignments (that yield the ground truth), at the
expense of imperfect alignments. This binary differentiation of perfect and
imperfect alignments falls short of capturing other essential alignment
properties that hold significance in other real-world applications. Here we
propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play
framework}$ for enhancing a desired property in models trained with the CTC
criterion. We do that by complementing the CTC with an additional loss term
that prioritizes alignments according to a desired property. Our method does
not require any intervention in the CTC loss function, enables easy
optimization of a variety of properties, and allows differentiation between
both perfect and imperfect alignments. We apply our framework in the domain of
Automatic Speech Recognition (ASR) and show its generality in terms of property
selection, architectural choice, and scale of training dataset (up to 280,000
hours). To demonstrate the effectiveness of our framework, we apply it to two
unrelated properties: emission time and word error rate (WER). For the former,
we report an improvement of up to 570ms in latency optimization with a minor
reduction in WER, and for the latter, we report a relative improvement of 4.5%
WER over the baseline models. To the best of our knowledge, these applications
have never been demonstrated to work on a scale of data as large as ours.
Notably, our method can be implemented using only a few lines of code, and can
be extended to other alignment-free loss functions and to domains other than
ASR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01717">On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coletta_A/0/1/0/all/0/1">Andrea Coletta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishan_S/0/1/0/all/0/1">Sriram Gopalakrishan</a>, <a href="http://arxiv.org/find/cs/1/au:+Borrajo_D/0/1/0/all/0/1">Daniel Borrajo</a>, <a href="http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1">Svitlana Vyetrenko</a></p>
<p>Synthetic time series are often used in practical applications to augment the
historical time series dataset for better performance of machine learning
algorithms, amplify the occurrence of rare events, and also create
counterfactual scenarios described by the time series.
Distributional-similarity (which we refer to as realism) as well as the
satisfaction of certain numerical constraints are common requirements in
counterfactual time series scenario generation requests. For instance, the US
Federal Reserve publishes synthetic market stress scenarios given by the
constrained time series for financial institutions to assess their performance
in hypothetical recessions. Existing approaches for generating constrained time
series usually penalize training loss to enforce constraints, and reject
non-conforming samples. However, these approaches would require re-training if
we change constraints, and rejection sampling can be computationally expensive,
or impractical for complex constraints. In this paper, we propose a novel set
of methods to tackle the constrained time series generation problem and provide
efficient sampling while ensuring the realism of generated time series. In
particular, we frame the problem using a constrained optimization framework and
then we propose a set of generative methods including ``GuidedDiffTime'', a
guided diffusion model to generate realistic time series. Empirically, we
evaluate our work on several datasets for financial and energy data, where
incorporating constraints is critical. We show that our approaches outperform
existing work both qualitatively and quantitatively. Most importantly, we show
that our ``GuidedDiffTime'' model is the only solution where re-training is not
necessary for new constraints, resulting in a significant carbon footprint
reduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01719">MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Zheng_Y/0/1/0/all/0/1">Yong Zheng</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Shukla_K/0/1/0/all/0/1">Kumar Neelotpal Shukla</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Xu_J/0/1/0/all/0/1">Jasmine Xu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+David/0/1/0/all/0/1">David</a> (Xuejun) <a href="http://arxiv.org/find/q-fin/1/au:+Wang/0/1/0/all/0/1">Wang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+OLeary_M/0/1/0/all/0/1">Michael O&#x27;Leary</a></p>
<p>MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for
Sustainable Investments. This document provides a user guide for MOPO-LSI
version 1.0, including problem setup, workflow and the hyper-parameters in
configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01725">RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network. (arXiv:2307.01725v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Feng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cicone_A/0/1/0/all/0/1">Antonio Cicone</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Haomin Zhou</a></p>
<p>The decomposition of non-stationary signals is an important and challenging
task in the field of signal time-frequency analysis. In the recent two decades,
many signal decomposition methods led by the empirical mode decomposition,
which was pioneered by Huang et al. in 1998, have been proposed by different
research groups. However, they still have some limitations. For example, they
are generally prone to boundary and mode mixing effects and are not very robust
to noise. Inspired by the successful applications of deep learning in fields
like image processing and natural language processing, and given the lack in
the literature of works in which deep learning techniques are used directly to
decompose non-stationary signals into simple oscillatory components, we use the
convolutional neural network, residual structure and nonlinear activation
function to compute in an innovative way the local average of the signal, and
study a new non-stationary signal decomposition method under the framework of
deep learning. We discuss the training process of the proposed model and study
the convergence analysis of the learning algorithm. In the experiments, we
evaluate the performance of the proposed model from two points of view: the
calculation of the local average and the signal decomposition. Furthermore, we
study the mode mixing, noise interference, and orthogonality properties of the
decomposed components produced by the proposed method. All results show that
the proposed model allows for better handling boundary effect, mode mixing
effect, robustness, and the orthogonality of the decomposed components than
existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01750">SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1">Zhijie Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jingcai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Luyao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xinghao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Song Guo</a></p>
<p>This paper provides a novel framework for single-domain generalized object
detection (i.e., Single-DGOD), where we are interested in learning and
maintaining the semantic structures of self-augmented compound cross-domain
samples to enhance the model's generalization ability. Different from DGOD
trained on multiple source domains, Single-DGOD is far more challenging to
generalize well to multiple target domains with only one single source domain.
Existing methods mostly adopt a similar treatment from DGOD to learn
domain-invariant features by decoupling or compressing the semantic space.
However, there may have two potential limitations: 1) pseudo attribute-label
correlation, due to extremely scarce single-domain data; and 2) the semantic
structural information is usually ignored, i.e., we found the affinities of
instance-level semantic relations in samples are crucial to model
generalization. In this paper, we introduce Semantic Reasoning with Compound
Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main
components, namely, the texture-based self-augmentation (TBSA) module, and the
local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the
effects of irrelevant attributes associated with labels, such as light, shadow,
color, etc., at the image level by a light-yet-efficient self-augmentation.
Moreover, LGSR is used to further model the semantic relationships on instance
features to uncover and maintain the intrinsic semantic structures. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of the
proposed SRCD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01753">Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies. (arXiv:2307.01753v1 [astro-ph.CO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Rezaie_M/0/1/0/all/0/1">Mehdi Rezaie</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ross_A/0/1/0/all/0/1">Ashley J. Ross</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Seo_H/0/1/0/all/0/1">Hee-Jong Seo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kong_H/0/1/0/all/0/1">Hui Kong</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Porredon_A/0/1/0/all/0/1">Anna Porredon</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Samushia_L/0/1/0/all/0/1">Lado Samushia</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Chaussidon_E/0/1/0/all/0/1">Edmond Chaussidon</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Krolewski_A/0/1/0/all/0/1">Alex Krolewski</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Mattia_A/0/1/0/all/0/1">Arnaud de Mattia</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Beutler_F/0/1/0/all/0/1">Florian Beutler</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Aguilar_J/0/1/0/all/0/1">Jessica Nicole Aguilar</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ahlen_S/0/1/0/all/0/1">Steven Ahlen</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Alam_S/0/1/0/all/0/1">Shadab Alam</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Avila_S/0/1/0/all/0/1">Santiago Avila</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bahr_Kalus_B/0/1/0/all/0/1">Benedict Bahr-Kalus</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bermejo_Climent_J/0/1/0/all/0/1">Jose Bermejo-Climent</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Brooks_D/0/1/0/all/0/1">David Brooks</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Claybaugh_T/0/1/0/all/0/1">Todd Claybaugh</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Cole_S/0/1/0/all/0/1">Shaun Cole</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Dawson_K/0/1/0/all/0/1">Kyle Dawson</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Macorra_A/0/1/0/all/0/1">Axel de la Macorra</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Doel_P/0/1/0/all/0/1">Peter Doel</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Font_Ribera_A/0/1/0/all/0/1">Andreu Font-Ribera</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Forero_Romero_J/0/1/0/all/0/1">Jaime E. Forero-Romero</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Gontcho_S/0/1/0/all/0/1">Satya Gontcho A Gontcho</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Guy_J/0/1/0/all/0/1">Julien Guy</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Honscheid_K/0/1/0/all/0/1">Klaus Honscheid</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kisner_T/0/1/0/all/0/1">Theodore Kisner</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Landriau_M/0/1/0/all/0/1">Martin Landriau</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Levi_M/0/1/0/all/0/1">Michael Levi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Manera_M/0/1/0/all/0/1">Marc Manera</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Meisner_A/0/1/0/all/0/1">Aaron Meisner</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Miquel_R/0/1/0/all/0/1">Ramon Miquel</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Mueller_E/0/1/0/all/0/1">Eva-Maria Mueller</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Myers_A/0/1/0/all/0/1">Adam Myers</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Newman_J/0/1/0/all/0/1">Jeffrey A. Newman</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Nie_J/0/1/0/all/0/1">Jundan Nie</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Palanque_Delabrouille_N/0/1/0/all/0/1">Nathalie Palanque-Delabrouille</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Percival_W/0/1/0/all/0/1">Will Percival</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Poppett_C/0/1/0/all/0/1">Claire Poppett</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Rossi_G/0/1/0/all/0/1">Graziano Rossi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Sanchez_E/0/1/0/all/0/1">Eusebio Sanchez</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Schubnell_M/0/1/0/all/0/1">Michael Schubnell</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Tarle_G/0/1/0/all/0/1">Gregory Tarl&#xe9;</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Weaver_B/0/1/0/all/0/1">Benjamin Alan Weaver</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Yeche_C/0/1/0/all/0/1">Christophe Y&#xe8;che</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zhou_Z/0/1/0/all/0/1">Zhimin Zhou</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zou_H/0/1/0/all/0/1">Hu Zou</a></p>
<p>We use angular clustering of luminous red galaxies from the Dark Energy
Spectroscopic Instrument (DESI) imaging surveys to constrain the local
primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million
targets, covering 14,000 square degrees of the sky, with redshifts in the range
0.2&lt; z &lt; 1.35. We identify Galactic extinction, survey depth, and astronomical
seeing as the primary sources of systematic error, and employ linear regression
and artificial neural networks to alleviate non-cosmological excess clustering
on large scales. Our methods are tested against log-normal simulations with and
without fNL and systematics, showing superior performance of the neural network
treatment in reducing remaining systematics. Assuming the universality
relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence.
With a more aggressive treatment, including regression against the full set of
imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 50$ and
the uncertainty on fNL increases due to the removal of large-scale clustering
information. We apply a series of robustness tests (e.g., cuts on imaging,
declination, or scales used) that show consistency in the obtained constraints.
Despite extensive efforts to mitigate systematics, our measurements indicate
fNL &gt; 0 with a 99.9 percent confidence level. This outcome raises concerns as
it could be attributed to unforeseen systematics, including calibration errors
or uncertainties associated with low-\ell systematics in the extinction
template. Alternatively, it could suggest a scale-dependent fNL model--causing
significant non-Gaussianity around large-scale structure while leaving cosmic
microwave background scales unaffected. Our results encourage further studies
of fNL with DESI spectroscopic samples, where the inclusion of 3D clustering
modes should help separate imaging systematics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01759">Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification. (arXiv:2307.01759v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahler_L/0/1/0/all/0/1">Lucas Mahler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiglechner_J/0/1/0/all/0/1">Julius Steiglechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Birk_F/0/1/0/all/0/1">Florian Birk</a>, <a href="http://arxiv.org/find/cs/1/au:+Heczko_S/0/1/0/all/0/1">Samuel Heczko</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheffler_K/0/1/0/all/0/1">Klaus Scheffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lohmann_G/0/1/0/all/0/1">Gabriele Lohmann</a></p>
<p>Autism spectrum disorder (ASD) is a prevalent psychiatric condition
characterized by atypical cognitive, emotional, and social patterns. Timely and
accurate diagnosis is crucial for effective interventions and improved outcomes
in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced
Transformer framework, METAFormer, ASD classification. Our framework utilizes
resting-state functional magnetic resonance imaging data from the ABIDE I
dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer
employs a multi-atlas approach, where flattened connectivity matrices from the
AAL, CC200, and DOS160 atlases serve as input to the transformer encoder.
Notably, we demonstrate that self-supervised pretraining, involving the
reconstruction of masked values from the input, significantly enhances
classification performance without the need for additional or separate training
data. Through stratified cross-validation, we evaluate the proposed framework
and show that it surpasses state-of-the-art performance on the ABIDE I dataset,
with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our
framework is available at https://github.com/Lugges991/METAFormer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01767">Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana. (arXiv:2307.01767v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akogo_D/0/1/0/all/0/1">Darlington Akogo</a>, <a href="http://arxiv.org/find/cs/1/au:+Samori_I/0/1/0/all/0/1">Issah Samori</a>, <a href="http://arxiv.org/find/cs/1/au:+Akafia_C/0/1/0/all/0/1">Cyril Akafia</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiagbor_H/0/1/0/all/0/1">Harriet Fiagbor</a>, <a href="http://arxiv.org/find/cs/1/au:+Kangah_A/0/1/0/all/0/1">Andrews Kangah</a>, <a href="http://arxiv.org/find/cs/1/au:+Asiedu_D/0/1/0/all/0/1">Donald Kwame Asiedu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuachie_K/0/1/0/all/0/1">Kwabena Fuachie</a>, <a href="http://arxiv.org/find/cs/1/au:+Oala_L/0/1/0/all/0/1">Luis Oala</a></p>
<p>The Ghana Cashew Disease Identification with Artificial Intelligence (CADI
AI) project demonstrates the importance of sound data work as a precondition
for the delivery of useful, localized datacentric solutions for public good
tasks such as agricultural productivity and food security. Drone collected data
and machine learning are utilized to determine crop stressors. Data, model and
the final app are developed jointly and made available to local farmers via a
desktop application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01770">Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics. (arXiv:2307.01770v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Mahey_G/0/1/0/all/0/1">Guillaume Mahey</a>, <a href="http://arxiv.org/find/stat/1/au:+Chapel_L/0/1/0/all/0/1">Laetitia Chapel</a>, <a href="http://arxiv.org/find/stat/1/au:+Gasso_G/0/1/0/all/0/1">Gilles Gasso</a>, <a href="http://arxiv.org/find/stat/1/au:+Bonet_C/0/1/0/all/0/1">Cl&#xe9;ment Bonet</a>, <a href="http://arxiv.org/find/stat/1/au:+Courty_N/0/1/0/all/0/1">Nicolas Courty</a></p>
<p>Wasserstein distance (WD) and the associated optimal transport plan have been
proven useful in many applications where probability measures are at stake. In
this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is
based on the transport map induced by an optimal one-dimensional projection of
the two input distributions. We draw connections between min-SWGG and
Wasserstein generalized geodesics in which the pivot measure is supported on a
line. We notably provide a new closed form for the exact Wasserstein distance
in the particular case of one of the distributions supported on a line allowing
us to derive a fast computational scheme that is amenable to gradient descent
optimization. We show that min-SWGG is an upper bound of WD and that it has a
complexity similar to as Sliced-Wasserstein, with the additional feature of
providing an associated transport plan. We also investigate some theoretical
properties such as metricity, weak convergence, computational and topological
properties. Empirical evidences support the benefits of min-SWGG in various
contexts, from gradient flows, shape matching and image colorization, among
others.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01777">Shapley Sets: Feature Attribution via Recursive Function Decomposition. (arXiv:2307.01777v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sivill_T/0/1/0/all/0/1">Torty Sivill</a>, <a href="http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1">Peter Flach</a></p>
<p>Despite their ubiquitous use, Shapley value feature attributions can be
misleading due to feature interaction in both model and data. We propose an
alternative attribution approach, Shapley Sets, which awards value to sets of
features. Shapley Sets decomposes the underlying model into non-separable
variable groups using a recursive function decomposition algorithm with log
linear complexity in the number of variables. Shapley Sets attributes to each
non-separable variable group their combined value for a particular prediction.
We show that Shapley Sets is equivalent to the Shapley value over the
transformed feature set and thus benefits from the same axioms of fairness.
Shapley Sets is value function agnostic and we show theoretically and
experimentally how Shapley Sets avoids pitfalls associated with Shapley value
based alternatives and are particularly advantageous for data types with
complex dependency structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01780">FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices. (arXiv:2307.01780v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gufran_D/0/1/0/all/0/1">Danish Gufran</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1">Sudeep Pasricha</a></p>
<p>Indoor localization plays a vital role in applications such as emergency
response, warehouse management, and augmented reality experiences. By deploying
machine learning (ML) based indoor localization frameworks on their mobile
devices, users can localize themselves in a variety of indoor and subterranean
environments. However, achieving accurate indoor localization can be
challenging due to heterogeneity in the hardware and software stacks of mobile
devices, which can result in inconsistent and inaccurate location estimates.
Traditional ML models also heavily rely on initial training data, making them
vulnerable to degradation in performance with dynamic changes across indoor
environments. To address the challenges due to device heterogeneity and lack of
adaptivity, we propose a novel embedded ML framework called FedHIL. Our
framework combines indoor localization and federated learning (FL) to improve
indoor localization accuracy in device-heterogeneous environments while also
preserving user data privacy. FedHIL integrates a domain-specific selective
weight adjustment approach to preserve the ML model's performance for indoor
localization during FL, even in the presence of extremely noisy data.
Experimental evaluations in diverse real-world indoor environments and with
heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL
and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62x
better localization accuracy on average than the best performing FL-based
indoor localization framework from prior work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01782">GHOST: A Graph Neural Network Accelerator using Silicon Photonics. (arXiv:2307.01782v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Afifi_S/0/1/0/all/0/1">Salma Afifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunny_F/0/1/0/all/0/1">Febin Sunny</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiee_A/0/1/0/all/0/1">Amin Shafiee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikdast_M/0/1/0/all/0/1">Mahdi Nikdast</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1">Sudeep Pasricha</a></p>
<p>Graph neural networks (GNNs) have emerged as a powerful approach for
modelling and learning from graph-structured data. Multiple fields have since
benefitted enormously from the capabilities of GNNs, such as recommendation
systems, social network analysis, drug discovery, and robotics. However,
accelerating and efficiently processing GNNs require a unique approach that
goes beyond conventional artificial neural network accelerators, due to the
substantial computational and memory requirements of GNNs. The slowdown of
scaling in CMOS platforms also motivates a search for alternative
implementation substrates. In this paper, we present GHOST, the first
silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates
the costs associated with both vertex-centric and edge-centric operations. It
implements separately the three main stages involved in running GNNs in the
optical domain, allowing it to be used for the inference of various widely used
GNN models and architectures, such as graph convolution networks and graph
attention networks. Our simulation studies indicate that GHOST exhibits at
least 10.2x better throughput and 3.8x better energy efficiency when compared
to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01798">Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI. (arXiv:2307.01798v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1">Xiaojiao Xiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Q/0/1/0/all/0/1">Qinmin Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Guanghui Wang</a></p>
<p>Simultaneous multi-index quantification, segmentation, and uncertainty
estimation of liver tumors on multi-modality non-contrast magnetic resonance
imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods
lack an effective mechanism for multi-modality NCMRI fusion and accurate
boundary information capture, making these tasks challenging. To address these
issues, this paper proposes a unified framework, namely edge-aware multi-task
network (EaMtNet), to associate multi-index quantification, segmentation, and
uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs
two parallel CNN encoders and the Sobel filters to extract local features and
edge maps, respectively. The newly designed edge-aware feature aggregation
module (EaFA) is used for feature fusion and selection, making the network
edge-aware by capturing long-range dependency between feature and edge maps.
Multi-tasking leverages prediction discrepancy to estimate uncertainty and
improve segmentation and quantification performance. Extensive experiments are
performed on multi-modality NCMRI with 250 clinical subjects. The proposed
model outperforms the state-of-the-art by a large margin, achieving a dice
similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of
2.72$\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a
reliable clinical-aided tool for medical image analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01804">Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators. (arXiv:2307.01804v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiangce Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenzhuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldwin_M/0/1/0/all/0/1">Martha Baldwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nijhuis_B/0/1/0/all/0/1">Bj&#xf6;rn Nijhuis</a>, <a href="http://arxiv.org/find/cs/1/au:+Boogaard_T/0/1/0/all/0/1">Ton van den Boogaard</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_N/0/1/0/all/0/1">Noelia Grande Guti&#xe9;rrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Narra_S/0/1/0/all/0/1">Sneha Prabha Narra</a>, <a href="http://arxiv.org/find/cs/1/au:+McComb_C/0/1/0/all/0/1">Christopher McComb</a></p>
<p>High-fidelity, data-driven models that can quickly simulate thermal behavior
during additive manufacturing (AM) are crucial for improving the performance of
AM technologies in multiple areas, such as part design, process planning,
monitoring, and control. However, the complexities of part geometries make it
challenging for current models to maintain high accuracy across a wide range of
geometries. Additionally, many models report a low mean square error (MSE)
across the entire domain (part). However, in each time step, most areas of the
domain do not experience significant changes in temperature, except for the
heat-affected zones near recent depositions. Therefore, the MSE-based fidelity
measurement of the models may be overestimated.
</p>
<p>This paper presents a data-driven model that uses Fourier Neural Operator to
capture the local temperature evolution during the additive manufacturing
process. In addition, the authors propose to evaluate the model using the $R^2$
metric, which provides a relative measure of the model's performance compared
to using mean temperature as a prediction. The model was tested on numerical
simulations based on the Discontinuous Galerkin Finite Element Method for the
Direct Energy Deposition process, and the results demonstrate that the model
achieves high fidelity as measured by $R^2$ and maintains generalizability to
geometries that were not included in the training process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01813">Structural Balance and Random Walks on Complex Networks with Complex Weights. (arXiv:2307.01813v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambiotte_R/0/1/0/all/0/1">Renaud Lambiotte</a></p>
<p>Complex numbers define the relationship between entities in many situations.
A canonical example would be the off-diagonal terms in a Hamiltonian matrix in
quantum physics. Recent years have seen an increasing interest to extend the
tools of network science when the weight of edges are complex numbers. Here, we
focus on the case when the weight matrix is Hermitian, a reasonable assumption
in many applications, and investigate both structural and dynamical properties
of the complex-weighted networks. Building on concepts from signed graphs, we
introduce a classification of complex-weighted networks based on the notion of
structural balance, and illustrate the shared spectral properties within each
type. We then apply the results to characterise the dynamics of random walks on
complex-weighted networks, where local consensus can be achieved asymptotically
when the graph is structurally balanced, while global consensus will be
obtained when it is strictly unbalanced. Finally, we explore potential
applications of our findings by generalising the notion of cut, and propose an
associated spectral clustering algorithm. We also provide further
characteristics of the magnetic Laplacian, associating directed networks to
complex-weighted ones. The performance of the algorithm is verified on both
synthetic and real networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01827">Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buzaglo_G/0/1/0/all/0/1">Gon Buzaglo</a>, <a href="http://arxiv.org/find/cs/1/au:+Haim_N/0/1/0/all/0/1">Niv Haim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1">Gilad Yehudai</a>, <a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1">Gal Vardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Oz_Y/0/1/0/all/0/1">Yakir Oz</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikankin_Y/0/1/0/all/0/1">Yaniv Nikankin</a>, <a href="http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1">Michal Irani</a></p>
<p>Memorization of training data is an active research area, yet our
understanding of the inner workings of neural networks is still in its infancy.
Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples
from multilayer perceptron binary classifiers, effectively demonstrating that a
large portion of training samples are encoded in the parameters of such
networks. In this work, we extend their findings in several directions,
including reconstruction from multiclass and convolutional neural networks. We
derive a more general reconstruction scheme which is applicable to a wider
range of loss functions such as regression losses. Moreover, we study the
various factors that contribute to networks' susceptibility to such
reconstruction schemes. Intriguingly, we observe that using weight decay during
training increases reconstructability both in terms of quantity and quality.
Additionally, we examine the influence of the number of neurons relative to the
number of training samples on the reconstructability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01831">DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1">Ruihang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Lewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lanqing Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1">Matthias Nie&#xdf;ner</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a></p>
<p>Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful
effectiveness in generating high-quality 2D images. However, it is still being
determined whether the Transformer architecture performs equally well in 3D
shape generation, as previous 3D diffusion methods mostly adopted the U-Net
architecture. To bridge this gap, we propose a novel Diffusion Transformer for
3D shape generation, namely DiT-3D, which can directly operate the denoising
process on voxelized point clouds using plain Transformers. Compared to
existing U-Net approaches, our DiT-3D is more scalable in model size and
produces much higher quality generations. Specifically, the DiT-3D adopts the
design philosophy of DiT but modifies it by incorporating 3D positional and
patch embeddings to adaptively aggregate input from voxelized point clouds. To
reduce the computational cost of self-attention in 3D shape generation, we
incorporate 3D window attention into Transformer blocks, as the increased 3D
token length resulting from the additional dimension of voxels can lead to high
computation. Finally, linear and devoxelization layers are used to predict the
denoised point clouds. In addition, our transformer architecture supports
efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on
ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on
the ShapeNet dataset demonstrate that the proposed DiT-3D achieves
state-of-the-art performance in high-fidelity and diverse 3D point cloud
generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy
of the state-of-the-art method by 4.59 and increases the Coverage metric by
3.51 when evaluated on Chamfer Distance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01840">Empirical Sample Complexity of Neural Network Mixed State Reconstruction. (arXiv:2307.01840v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Zhao_H/0/1/0/all/0/1">Haimeng Zhao</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1">Giuseppe Carleo</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Vicentini_F/0/1/0/all/0/1">Filippo Vicentini</a></p>
<p>Quantum state reconstruction using Neural Quantum States has been proposed as
a viable tool to reduce quantum shot complexity in practical applications, and
its advantage over competing techniques has been shown in numerical experiments
focusing mainly on the noiseless case. In this work, we numerically investigate
the performance of different quantum state reconstruction techniques for mixed
states: the finite-temperature Ising model. We show how to systematically
reduce the quantum resource requirement of the algorithms by applying variance
reduction techniques. Then, we compare the two leading neural quantum state
encodings of the state, namely, the Neural Density Operator and the positive
operator-valued measurement representation, and illustrate their different
performance as the mixedness of the target state varies. We find that certain
encodings are more efficient in different regimes of mixedness and point out
the need for designing more efficient encodings in terms of both classical and
quantum resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01849">Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Belagali_V/0/1/0/all/0/1">Varun Belagali</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jinghuan Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael S. Ryoo</a></p>
<p>Sequence modeling approaches have shown promising results in robot imitation
learning. Recently, diffusion models have been adopted for behavioral cloning,
benefiting from their exceptional capabilities in modeling complex data
distribution. In this work, we propose Crossway Diffusion, a method to enhance
diffusion-based visuomotor policy learning by using an extra self-supervised
learning (SSL) objective. The standard diffusion-based policy generates action
sequences from random noise conditioned on visual observations and other
low-dimensional states. We further extend this by introducing a new decoder
that reconstructs raw image pixels (and other state information) from the
intermediate representations of the reverse diffusion process, and train the
model jointly using the SSL loss. Our experiments demonstrate the effectiveness
of Crossway Diffusion in various simulated and real-world robot tasks,
confirming its advantages over the standard diffusion-based policy. We
demonstrate that such self-supervised reconstruction enables better
representation for policy learning, especially when the demonstrations have
different proficiencies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1705.03387">Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN. (arXiv:1705.03387v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyeungill Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Sungyeob Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungwoo Lee</a></p>
<p>We propose a novel technique to make neural network robust to adversarial
examples using a generative adversarial network. We alternately train both
classifier and generator networks. The generator network generates an
adversarial perturbation that can easily fool the classifier network by using a
gradient of each image. Simultaneously, the classifier network is trained to
classify correctly both original and adversarial images generated by the
generator. These procedures help the classifier network to become more robust
to adversarial perturbations. Furthermore, our adversarial training framework
efficiently reduces overfitting and outperforms other regularization methods
such as Dropout. We applied our method to supervised learning for CIFAR
datasets, and experimantal results show that our method significantly lowers
the generalization error of the network. To the best of our knowledge, this is
the first method which uses GAN to improve supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1802.00810">Deep Learning for Genomics: A Concise Overview. (arXiv:1802.00810v3 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Yue_T/0/1/0/all/0/1">Tianwei Yue</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a></p>
<p>Advancements in genomic research such as high-throughput sequencing
techniques have driven modern genomic studies into "big data" disciplines. This
data explosion is constantly challenging conventional methods used in genomics.
In parallel with the urgent demand for robust algorithms, deep learning has
succeeded in a variety of fields such as vision, speech, and text processing.
Yet genomics entails unique challenges to deep learning since we are expecting
from deep learning a superhuman intelligence that explores beyond our knowledge
to interpret the genome. A powerful deep learning model should rely on
insightful utilization of task-specific knowledge. In this paper, we briefly
discuss the strengths of different deep learning models from a genomic
perspective so as to fit each particular task with a proper deep architecture,
and remark on practical considerations of developing modern deep learning
architectures for genomics. We also provide a concise review of deep learning
applications in various aspects of genomic research, as well as pointing out
potential opportunities and obstacles for future genomics applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2003.09053">Cross-Shape Attention for Part Segmentation of 3D Point Clouds. (arXiv:2003.09053v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1">Marios Loizou</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Siddhant Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrov_D/0/1/0/all/0/1">Dmitry Petrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1">Melinos Averkiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1">Evangelos Kalogerakis</a></p>
<p>We present a deep learning method that propagates point-wise feature
representations across shapes within a collection for the purpose of 3D shape
segmentation. We propose a cross-shape attention mechanism to enable
interactions between a shape's point-wise features and those of other shapes.
The mechanism assesses both the degree of interaction between points and also
mediates feature propagation across shapes, improving the accuracy and
consistency of the resulting point-wise feature representations for shape
segmentation. Our method also proposes a shape retrieval measure to select
suitable shapes for cross-shape attention operations for each test shape. Our
experiments demonstrate that our approach yields state-of-the-art results in
the popular PartNet dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.07888">Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhuangdi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1">Kaixiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Anil K. Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiayu Zhou</a></p>
<p>Reinforcement learning is a learning paradigm for solving sequential
decision-making problems. Recent years have witnessed remarkable progress in
reinforcement learning upon the fast development of deep neural networks. Along
with the promising prospects of reinforcement learning in numerous domains such
as robotics and game-playing, transfer learning has arisen to tackle various
challenges faced by reinforcement learning, by transferring knowledge from
external expertise to facilitate the efficiency and effectiveness of the
learning process. In this survey, we systematically investigate the recent
progress of transfer learning approaches in the context of deep reinforcement
learning. Specifically, we provide a framework for categorizing the
state-of-the-art transfer learning approaches, under which we analyze their
goals, methodologies, compatible reinforcement learning backbones, and
practical applications. We also draw connections between transfer learning and
other relevant topics from the reinforcement learning perspective and explore
their potential challenges that await future research progress.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.04106">Ensemble Knowledge Distillation for CTR Prediction. (arXiv:2011.04106v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jieming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1">Jincai Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiuqiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a></p>
<p>Recently, deep learning-based models have been widely studied for
click-through rate (CTR) prediction and lead to improved prediction accuracy in
many industrial applications. However, current research focuses primarily on
building complex network architectures to better capture sophisticated feature
interactions and dynamic user behaviors. The increased model complexity may
slow down online inference and hinder its adoption in real-time applications.
Instead, our work targets at a new model training strategy based on knowledge
distillation (KD). KD is a teacher-student learning framework to transfer
knowledge learned from a teacher model to a student model. The KD strategy not
only allows us to simplify the student model as a vanilla DNN model but also
achieves significant accuracy improvements over the state-of-the-art teacher
models. The benefits thus motivate us to further explore the use of a powerful
ensemble of teachers for more accurate student model training. We also propose
some novel techniques to facilitate ensembled CTR prediction, including teacher
gating and early stopping by distillation loss. We conduct comprehensive
experiments against 12 existing models and across three industrial datasets.
Both offline and online A/B testing results show the effectiveness of our
KD-based training strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.14956">Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongquan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiayi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhongxi Zheng</a></p>
<p>Learning from noisy labels is an important concern because of the lack of
accurate ground-truth labels in plenty of real-world scenarios. In practice,
various approaches for this concern first make some corrections corresponding
to potentially noisy-labeled instances, and then update predictive model with
information of the made corrections. However, in specific areas, such as
medical histopathology whole slide image analysis (MHWSIA), it is often
difficult or even impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. In this paper, we
focus on alleviating these two problems. For the problem 1), we present
one-step abductive multi-target learning (OSAMTL) that imposes a one-step
logical reasoning upon machine learning via a multi-target learning procedure
to constrain the predictions of the learning model to be subject to our prior
knowledge about the true target. For the problem 2), we propose a logical
assessment formula (LAF) that evaluates the logical rationality of the outputs
of an approach by estimating the consistencies between the predictions of the
learning model and the logical facts narrated from the results of the one-step
logical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori
(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable
the machine learning model achieving logically more rational predictions, which
is beyond various state-of-the-art approaches in handling complex noisy labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.07050">SCEI: A Smart-Contract Driven Edge Intelligence Framework for IoT Systems. (arXiv:2103.07050v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenhao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiaqi Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Longxiang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengshi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yong Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xi Zheng</a></p>
<p>Federated learning (FL) enables collaborative training of a shared model on
edge devices while maintaining data privacy. FL is effective when dealing with
independent and identically distributed (iid) datasets, but struggles with
non-iid datasets. Various personalized approaches have been proposed, but such
approaches fail to handle underlying shifts in data distribution, such as data
distribution skew commonly observed in real-world scenarios (e.g., driver
behavior in smart transportation systems changing across time and location).
Additionally, trust concerns among unacquainted devices and security concerns
with the centralized aggregator pose additional challenges. To address these
challenges, this paper presents a dynamically optimized personal deep learning
scheme based on blockchain and federated learning. Specifically, the innovative
smart contract implemented in the blockchain allows distributed edge devices to
reach a consensus on the optimal weights of personalized models. Experimental
evaluations using multiple models and real-world datasets demonstrate that the
proposed scheme achieves higher accuracy and faster convergence compared to
traditional federated and personalized learning approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.04140">Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Byeonggeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Simyung Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jinkyu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1">Dooyong Sung</a></p>
<p>Keyword spotting is an important research field because it plays a key role
in device wake-up and user interaction on smart devices. However, it is
challenging to minimize errors while operating efficiently in devices with
limited resources such as mobile phones. We present a broadcasted residual
learning method to achieve high accuracy with small model size and
computational load. Our method configures most of the residual functions as 1D
temporal convolution while still allows 2D convolution together using a
broadcasted-residual connection that expands temporal output to
frequency-temporal dimension. This residual mapping enables the network to
effectively represent useful audio features with much less computation than
conventional convolutional neural networks. We also propose a novel network
architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted
residual learning and describe how to scale up the model according to the
target device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%
top-1 accuracy on Google speech command datasets v1 and v2, respectively, and
consistently outperform previous approaches, using fewer computations and
parameters. Code is available at
https://github.com/Qualcomm-AI-research/bcresnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.04763">Fixed-Budget Best-Arm Identification in Structured Bandits. (arXiv:2106.04763v8 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1">Mohammad Javad Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a></p>
<p>Best-arm identification (BAI) in a fixed-budget setting is a bandit problem
where the learning agent maximizes the probability of identifying the optimal
(best) arm after a fixed number of observations. Most works on this topic study
unstructured problems with a small number of arms, which limits their
applicability. We propose a general tractable algorithm that incorporates the
structure, by successively eliminating suboptimal arms based on their mean
reward estimates from a joint generalization model. We analyze our algorithm in
linear and generalized linear models (GLMs), and propose a practical
implementation based on a G-optimal design. In linear models, our algorithm has
competitive error guarantees to prior works and performs at least as well
empirically. In GLMs, this is the first practical algorithm with analysis for
fixed-budget BAI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.09973">The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chenjun Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Ilbin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1">Bo Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1">Dale Schuurmans</a>, <a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1">Csaba Szepesvari</a></p>
<p>In high stake applications, active experimentation may be considered too
risky and thus data are often collected passively. While in simple cases, such
as in bandits, passive and active data collection are similarly effective, the
price of passive sampling can be much higher when collecting data from a system
with controlled states. The main focus of the current paper is the
characterization of this price. For example, when learning in episodic finite
state-action Markov decision processes (MDPs) with $\mathrm{S}$ states and
$\mathrm{A}$ actions, we show that even with the best (but passively chosen)
logging policy, $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$
episodes are necessary (and sufficient) to obtain an $\epsilon$-optimal policy,
where $H$ is the length of episodes. Note that this shows that the sample
complexity blows up exponentially compared to the case of active data
collection, a result which is not unexpected, but, as far as we know, have not
been published beforehand and perhaps the form of the exact expression is a
little surprising. We also extend these results in various directions, such as
other criteria or learning in the presence of function approximation, with
similar conclusions. A remarkable feature of our result is the sharp
characterization of the exponent that appears, which is critical for
understanding what makes passive learning hard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.10933">On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods. (arXiv:2109.10933v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Espath_L/0/1/0/all/0/1">Luis Espath</a>, <a href="http://arxiv.org/find/math/1/au:+Krumscheid_S/0/1/0/all/0/1">Sebastian Krumscheid</a>, <a href="http://arxiv.org/find/math/1/au:+Tempone_R/0/1/0/all/0/1">Ra&#xfa;l Tempone</a>, <a href="http://arxiv.org/find/math/1/au:+Vilanova_P/0/1/0/all/0/1">Pedro Vilanova</a></p>
<p>In this study, we demonstrate that the norm test and inner
product/orthogonality test presented in \cite{Bol18} are equivalent in terms of
the convergence rates associated with Stochastic Gradient Descent (SGD) methods
if $\epsilon^2=\theta^2+\nu^2$ with specific choices of $\theta$ and $\nu$.
Here, $\epsilon$ controls the relative statistical error of the norm of the
gradient while $\theta$ and $\nu$ control the relative statistical error of the
gradient in the direction of the gradient and in the direction orthogonal to
the gradient, respectively. Furthermore, we demonstrate that the inner
product/orthogonality test can be as inexpensive as the norm test in the best
case scenario if $\theta$ and $\nu$ are optimally selected, but the inner
product/orthogonality test will never be more computationally affordable than
the norm test if $\epsilon^2=\theta^2+\nu^2$. Finally, we present two
stochastic optimization problems to illustrate our results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.15824">Tractability from overparametrization: The example of the negative perceptron. (arXiv:2110.15824v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1">Andrea Montanari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yiqiao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kangjie Zhou</a></p>
<p>In the negative perceptron problem we are given $n$ data points
$({\boldsymbol x}_i,y_i)$, where ${\boldsymbol x}_i$ is a $d$-dimensional
vector and $y_i\in\{+1,-1\}$ is a binary label. The data are not linearly
separable and hence we content ourselves to find a linear classifier with the
largest possible \emph{negative} margin. In other words, we want to find a unit
norm vector ${\boldsymbol \theta}$ that maximizes $\min_{i\le n}y_i\langle
{\boldsymbol \theta},{\boldsymbol x}_i\rangle$. This is a non-convex
optimization problem (it is equivalent to finding a maximum norm vector in a
polytope), and we study its typical properties under two random models for the
data.
</p>
<p>We consider the proportional asymptotics in which $n,d\to \infty$ with
$n/d\to\delta$, and prove upper and lower bounds on the maximum margin
$\kappa_{\text{s}}(\delta)$ or -- equivalently -- on its inverse function
$\delta_{\text{s}}(\kappa)$. In other words, $\delta_{\text{s}}(\kappa)$ is the
overparametrization threshold: for $n/d\le
\delta_{\text{s}}(\kappa)-\varepsilon$ a classifier achieving vanishing
training error exists with high probability, while for $n/d\ge
\delta_{\text{s}}(\kappa)+\varepsilon$ it does not. Our bounds on
$\delta_{\text{s}}(\kappa)$ match to the leading order as $\kappa\to -\infty$.
We then analyze a linear programming algorithm to find a solution, and
characterize the corresponding threshold $\delta_{\text{lin}}(\kappa)$. We
observe a gap between the interpolation threshold $\delta_{\text{s}}(\kappa)$
and the linear programming threshold $\delta_{\text{lin}}(\kappa)$, raising the
question of the behavior of other algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.15430">The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1">Adrian Galdran</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1">Jose Dolz</a></p>
<p>In spite of the dominant performances of deep neural networks, recent works
have shown that they are poorly calibrated, resulting in over-confident
predictions. Miscalibration can be exacerbated by overfitting due to the
minimization of the cross-entropy during training, as it promotes the predicted
softmax probabilities to match the one-hot label assignments. This yields a
pre-softmax activation of the correct class that is significantly larger than
the remaining activations. Recent evidence from the literature suggests that
loss functions that embed implicit or explicit maximization of the entropy of
predictions yield state-of-the-art calibration performances. We provide a
unifying constrained-optimization perspective of current state-of-the-art
calibration losses. Specifically, these losses could be viewed as
approximations of a linear penalty (or a Lagrangian) imposing equality
constraints on logit distances. This points to an important limitation of such
underlying equality constraints, whose ensuing gradients constantly push
towards a non-informative solution, which might prevent from reaching the best
compromise between the discriminative performance and calibration of the model
during gradient-based optimization. Following our observations, we propose a
simple and flexible generalization based on inequality constraints, which
imposes a controllable margin on logit distances. Comprehensive experiments on
a variety of image classification, semantic segmentation and NLP benchmarks
demonstrate that our method sets novel state-of-the-art results on these tasks
in terms of network calibration, without affecting the discriminative
performance. The code is available at https://github.com/by-liu/MbLS .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.01021">Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation. (arXiv:2112.01021v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1">Yeonsung Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1">Hajin Shim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">June Yong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Eunho Yang</a></p>
<p>Deep neural networks (DNNs), despite their impressive ability to generalize
over-capacity networks, often rely heavily on malignant bias as shortcuts
instead of task-related information for discriminative tasks. To address this
problem, recent studies utilize auxiliary information related to the bias,
which is rarely obtainable in practice, or sift through a handful of bias-free
samples for debiasing. However, the success of these methods is not always
guaranteed due to the unfulfilled presumptions. In this paper, we propose a
novel method, Contrastive Debiasing via Generative Bias-transformation (CDvG),
which works without explicit bias labels or bias-free samples. Motivated by our
observation that not only discriminative models but also image translation
models tend to focus on the malignant bias, CDvG employs an image translation
model to transform one bias mode into another while preserving the
task-relevant information. Additionally, the bias-transformed views are set
against each other through contrastive learning to learn bias-invariant
representations. Our method demonstrates superior performance compared to prior
approaches, especially when bias-free samples are scarce or absent.
Furthermore, CDvG can be integrated with the methods that focus on bias-free
samples in a plug-and-play manner for additional enhancements, as demonstrated
by diverse experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08645">Learning Interpretable Models Through Multi-Objective Neural Architecture Search. (arXiv:2112.08645v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1">Zachariah Carmichael</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1">Tim Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1">Sam Ade Jacobs</a></p>
<p>Monumental advances in deep learning have led to unprecedented achievements
across various domains. While the performance of deep neural networks is
indubitable, the architectural design and interpretability of such models are
nontrivial. Research has been introduced to automate the design of neural
network architectures through neural architecture search (NAS). Recent progress
has made these methods more pragmatic by exploiting distributed computation and
novel optimization algorithms. However, there is little work in optimizing
architectures for interpretability. To this end, we propose a multi-objective
distributed NAS framework that optimizes for both task performance and
"introspectability," a surrogate metric for aspects of interpretability. We
leverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable
AI (XAI) techniques to reward architectures that can be better comprehended by
domain experts. The framework is evaluated on several image classification
datasets. We demonstrate that jointly optimizing for task error and
introspectability leads to more disentangled and debuggable architectures that
perform within tolerable error.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.04786">A Non-Classical Parameterization for Density Estimation Using Sample Moments. (arXiv:2201.04786v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1">Guangyu Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Lindquist_A/0/1/0/all/0/1">Anders Lindquist</a></p>
<p>Probability density estimation is a core problem of statistics and signal
processing. Moment methods are an important means of density estimation, but
they are generally strongly dependent on the choice of feasible functions,
which severely affects the performance. In this paper, we propose a
non-classical parametrization for density estimation using sample moments,
which does not require the choice of such functions. The parametrization is
induced by the squared Hellinger distance, and the solution of it, which is
proved to exist and be unique subject to a simple prior that does not depend on
data, and can be obtained by convex optimization. Statistical properties of the
density estimator, together with an asymptotic error upper bound are proposed
for the estimator by power moments. Applications of the proposed density
estimator in signal processing tasks are given. Simulation results validate the
performance of the estimator by a comparison to several prevailing methods. To
the best of our knowledge, the proposed estimator is the first one in the
literature for which the power moments up to an arbitrary even order exactly
match the sample moments, while the true density is not assumed to fall within
specific function classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.12926">Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1">Ekin Aky&#xfc;rek</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a></p>
<p>In tasks like semantic parsing, instruction following, and question
answering, standard deep networks fail to generalize compositionally from small
datasets. Many existing approaches overcome this limitation with model
architectures that enforce a compositional process of sentence interpretation.
In this paper, we present a domain-general and model-agnostic formulation of
compositionality as a constraint on symmetries of data distributions rather
than models. Informally, we prove that whenever a task can be solved by a
compositional model, there is a corresponding data augmentation scheme -- a
procedure for transforming examples into other well formed examples -- that
imparts compositional inductive bias on any model trained to solve the same
task. We describe a procedure called LEXSYM that discovers these
transformations automatically, then applies them to training data for ordinary
neural sequence models. Unlike existing compositional data augmentation
procedures, LEXSYM can be deployed agnostically across text, structured data,
and even images. It matches or surpasses state-of-the-art, task-specific models
on COGS semantic parsing, SCAN and ALCHEMY instruction following, and
CLEVR-COGENT visual question answering datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.08516">SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Wenjie Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Cote_D/0/1/0/all/0/1">David Cote</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a></p>
<p>Missing data in time series is a pervasive problem that puts obstacles in the
way of advanced analysis. A popular solution is imputation, where the
fundamental challenge is to determine what values should be filled in. This
paper proposes SAITS, a novel method based on the self-attention mechanism for
missing value imputation in multivariate time series. Trained by a
joint-optimization approach, SAITS learns missing values from a weighted
combination of two diagonally-masked self-attention (DMSA) blocks. DMSA
explicitly captures both the temporal dependencies and feature correlations
between time steps, which improves imputation accuracy and training speed.
Meanwhile, the weighted-combination design enables SAITS to dynamically assign
weights to the learned representations from two DMSA blocks according to the
attention map and the missingness information. Extensive experiments
quantitatively and qualitatively demonstrate that SAITS outperforms the
state-of-the-art methods on the time-series imputation task efficiently and
reveal SAITS' potential to improve the learning performance of pattern
recognition models on incomplete time-series data from the real world. The code
is open source on GitHub at https://github.com/WenjieDu/SAITS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.09826">Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1">Thang Doan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirzadeh_S/0/1/0/all/0/1">Seyed Iman Mirzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1">Mehrdad Farajtabar</a></p>
<p>A growing body of research in continual learning focuses on the catastrophic
forgetting problem. While many attempts have been made to alleviate this
problem, the majority of the methods assume a single model in the continual
learning setup. In this work, we question this assumption and show that
employing ensemble models can be a simple yet effective method to improve
continual performance. However, ensembles' training and inference costs can
increase significantly as the number of models grows. Motivated by this
limitation, we study different ensemble models to understand their benefits and
drawbacks in continual learning scenarios. Finally, to overcome the high
compute cost of ensembles, we leverage recent advances in neural network
subspace to propose a computationally cheap algorithm with similar runtime to a
single model yet enjoying the performance benefits of ensembles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.12888">Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1">Mohammadjavad Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Katariya_S/0/1/0/all/0/1">Sumeet Katariya</a></p>
<p>We develop a meta-learning framework for simple regret minimization in
bandits. In this framework, a learning agent interacts with a sequence of
bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and
learns its meta-parameters to perform better on future tasks. We propose the
first Bayesian and frequentist meta-learning algorithms for this setting. The
Bayesian algorithm has access to a prior distribution over the meta-parameters
and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere
$\tilde{O}(m / \sqrt{n})$. On the other hand, the meta simple regret of the
frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$. While its
regret is worse, the frequentist algorithm is more general because it does not
need a prior distribution over the meta-parameters. It can also be analyzed in
more settings. We instantiate our algorithms for several classes of bandit
problems. Our algorithms are general and we complement our theory by evaluating
them empirically in several environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.11076">Collaborative Learning for Cyberattack Detection in Blockchain Networks. (arXiv:2203.11076v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khoa_T/0/1/0/all/0/1">Tran Viet Khoa</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_D/0/1/0/all/0/1">Do Hai Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1">Dinh Thai Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Trung_N/0/1/0/all/0/1">Nguyen Linh Trung</a>, <a href="http://arxiv.org/find/cs/1/au:+Quynh_T/0/1/0/all/0/1">Tran Thi Thuy Quynh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Diep N. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_N/0/1/0/all/0/1">Nguyen Viet Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1">Eryk Dutkiewicz</a></p>
<p>This article aims to study intrusion attacks and then develop a novel
cyberattack detection framework for blockchain networks. Specifically, we first
design and implement a blockchain network in our laboratory. This blockchain
network will serve two purposes, i.e., to generate the real traffic data
(including both normal data and attack data) for our learning models and
implement real-time experiments to evaluate the performance of our proposed
intrusion detection framework. To the best of our knowledge, this is the first
dataset that is synthesized in a laboratory for cyberattacks in a blockchain
network. We then propose a novel collaborative learning model that allows
efficient deployment in the blockchain network to detect attacks. The main idea
of the proposed learning model is to enable blockchain nodes to actively
collect data, share the knowledge learned from its data, and then exchange the
knowledge with other blockchain nodes in the network. In this way, we can not
only leverage the knowledge from all the nodes in the network but also do not
need to gather all raw data for training at a centralized node like
conventional centralized learning solutions. Such a framework can also avoid
the risk of exposing local data's privacy as well as the excessive network
overhead/congestion. Both intensive simulations and real-time experiments
clearly show that our proposed collaborative learning-based intrusion detection
framework can achieve an accuracy of up to 97.7% in detecting attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.15150">Tight Bounds on the Hardness of Learning Simple Nonparametric Mixtures. (arXiv:2203.15150v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1">Bryon Aragam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_W/0/1/0/all/0/1">Wai Ming Tai</a></p>
<p>We study the problem of learning nonparametric distributions in a finite
mixture, and establish tight bounds on the sample complexity for learning the
component distributions in such models. Namely, we are given i.i.d. samples
from a pdf $f$ where $$ f=w_1f_1+w_2f_2, \quad w_1+w_2=1, \quad w_1,w_2&gt;0 $$
and we are interested in learning each component $f_i$. Without any assumptions
on $f_i$, this problem is ill-posed. In order to identify the components $f_i$,
we assume that each $f_i$ can be written as a convolution of a Gaussian and a
compactly supported density $\nu_i$ with $\text{supp}(\nu_1)\cap
\text{supp}(\nu_2)=\emptyset$.
</p>
<p>Our main result shows that $(\frac{1}{\varepsilon})^{\Omega(\log\log
\frac{1}{\varepsilon})}$ samples are required for estimating each $f_i$. The
proof relies on a quantitative Tauberian theorem that yields a fast rate of
approximation with Gaussians, which may be of independent interest. To show
this is tight, we also propose an algorithm that uses
$(\frac{1}{\varepsilon})^{O(\log\log \frac{1}{\varepsilon})}$ samples to
estimate each $f_i$. Unlike existing approaches to learning latent variable
models based on moment-matching and tensor methods, our proof instead involves
a delicate analysis of an ill-conditioned linear system via orthogonal
functions. Combining these bounds, we conclude that the optimal sample
complexity of this problem properly lies in between polynomial and exponential,
which is not common in learning theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.10652">Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juanhui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shomer_H/0/1/0/all/0/1">Harry Shomer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jiayuan Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Neil Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a></p>
<p>Knowledge graphs (KGs) facilitate a wide variety of applications. Despite
great efforts in creation and maintenance, even the largest KGs are far from
complete. Hence, KG completion (KGC) has become one of the most crucial tasks
for KG research. Recently, considerable literature in this space has centered
around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn
powerful embeddings. The success of these methods is naturally attributed to
the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their
additional message passing (MP) component. In this work, we find that
surprisingly, simple MLP models are able to achieve comparable performance to
MPNNs, suggesting that MP may not be as crucial as previously believed. With
further exploration, we show careful scoring function and loss function design
has a much stronger influence on KGC model performance. This suggests a
conflation of scoring function design, loss function design, and MP in prior
work, with promising insights regarding the scalability of state-of-the-art KGC
methods today, as well as careful attention to more suitable MP designs for KGC
tasks tomorrow. Our codes are publicly available at:
https://github.com/Juanhui28/Are_MPNNs_helpful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03151">Privacy Amplification via Shuffled Check-Ins. (arXiv:2206.03151v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1">Seng Pei Liew</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1">Satoshi Hasegawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1">Tsubasa Takahashi</a></p>
<p>We study a protocol for distributed computation called shuffled check-in,
which achieves strong privacy guarantees without requiring any further trust
assumptions beyond a trusted shuffler. Unlike most existing work, shuffled
check-in allows clients to make independent and random decisions to participate
in the computation, removing the need for server-initiated subsampling.
Leveraging differential privacy, we show that shuffled check-in achieves tight
privacy guarantees through privacy amplification, with a novel analysis based
on R{\'e}nyi differential privacy that improves privacy accounting over
existing work. We also introduce a numerical approach to track the privacy of
generic shuffling mechanisms, including Gaussian mechanism, which is the first
evaluation of a generic mechanism under the distributed setting within the
local/shuffle model in the literature. Empirical studies are also given to
demonstrate the efficacy of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03341">Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiachen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianfeng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a></p>
<p>Vision transformers (ViTs) have pushed the state-of-the-art for visual
perception tasks. The self-attention mechanism underpinning the strength of
ViTs has a quadratic complexity in both computation and memory usage. This
motivates the development of approximating the self-attention at linear
complexity. However, an in-depth analysis in this work reveals that existing
methods are either theoretically flawed or empirically ineffective for visual
recognition. We identify that their limitations are rooted in the inheritance
of softmax-based self-attention during approximations, that is, normalizing the
scaled dot-product between token feature vectors using the softmax function. As
preserving the softmax operation challenges any subsequent linearization
efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are
proposed. Specifically, a Gaussian kernel function is adopted to replace the
dot-product similarity, enabling a full self-attention matrix to be
approximated under low-rank matrix decomposition. For computational robustness,
we estimate the Moore-Penrose inverse using an iterative Newton-Raphson method
in the forward process only, while calculating its theoretical gradients only
once in the backward process. To further expand applicability (e.g., dense
prediction tasks), an efficient symmetric normalization technique is
introduced. Extensive experiments on ImageNet, COCO, and ADE20K show that our
SOFT significantly improves the computational efficiency of existing ViT
variants. With linear complexity, much longer token sequences are permitted by
SOFT, resulting in superior trade-off between accuracy and complexity. Code and
models are available at https://github.com/fudan-zvg/SOFT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06339">On establishing learning separations between classical and quantum machine learning with classical data. (arXiv:2208.06339v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Gyurik_C/0/1/0/all/0/1">Casper Gyurik</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1">Vedran Dunjko</a></p>
<p>Despite years of effort, the quantum machine learning community has only been
able to show quantum learning advantages for certain contrived
cryptography-inspired datasets in the case of classical data. In this note, we
discuss the challenges of finding learning problems that quantum learning
algorithms can learn much faster than any classical learning algorithm, and we
study how to identify such learning problems. Specifically, we reflect on the
main concepts in computational learning theory pertaining to this question, and
we discuss how subtle changes in definitions can mean conceptually
significantly different tasks, which can either lead to a separation or no
separation at all. Moreover, we study existing learning problems with a
provable quantum speedup to distill sets of more general and sufficient
conditions (i.e., ``checklists'') for a learning problem to exhibit a
separation between classical and quantum learners. These checklists are
intended to streamline one's approach to proving quantum speedups for learning
problems, or to elucidate bottlenecks. Finally, to illustrate its application,
we analyze examples of potential separations (i.e., when the learning problem
is build from computational separations, or when the data comes from a quantum
experiment) through the lens of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.01857">Statistical Comparisons of Classifiers by Generalized Stochastic Dominance. (arXiv:2209.01857v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Jansen_C/0/1/0/all/0/1">Christoph Jansen</a> (1), <a href="http://arxiv.org/find/stat/1/au:+Nalenz_M/0/1/0/all/0/1">Malte Nalenz</a> (1), <a href="http://arxiv.org/find/stat/1/au:+Schollmeyer_G/0/1/0/all/0/1">Georg Schollmeyer</a> (1), <a href="http://arxiv.org/find/stat/1/au:+Augustin_T/0/1/0/all/0/1">Thomas Augustin</a> (1) ((1) Ludwig-Maximilians-Universit&#xe4;t Munich)</p>
<p>Although being a crucial question for the development of machine learning
algorithms, there is still no consensus on how to compare classifiers over
multiple data sets with respect to several criteria. Every comparison framework
is confronted with (at least) three fundamental challenges: the multiplicity of
quality criteria, the multiplicity of data sets and the randomness of the
selection of data sets. In this paper, we add a fresh view to the vivid debate
by adopting recent developments in decision theory. Based on so-called
preference systems, our framework ranks classifiers by a generalized concept of
stochastic dominance, which powerfully circumvents the cumbersome, and often
even self-contradictory, reliance on aggregates. Moreover, we show that
generalized stochastic dominance can be operationalized by solving
easy-to-handle linear programs and moreover statistically tested employing an
adapted two-sample observation-randomization test. This yields indeed a
powerful framework for the statistical comparison of classifiers over multiple
data sets with respect to multiple quality criteria simultaneously. We
illustrate and investigate our framework in a simulation study and with a set
of standard benchmark data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.06356">A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mavor_Parker_A/0/1/0/all/0/1">Augustine N. Mavor-Parker</a>, <a href="http://arxiv.org/find/cs/1/au:+Sargent_M/0/1/0/all/0/1">Matthew J. Sargent</a>, <a href="http://arxiv.org/find/cs/1/au:+Banino_A/0/1/0/all/0/1">Andrea Banino</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1">Lewis D. Griffin</a>, <a href="http://arxiv.org/find/cs/1/au:+Barry_C/0/1/0/all/0/1">Caswell Barry</a></p>
<p>Animals are able to rapidly infer from limited experience when sets of state
action pairs have equivalent reward and transition dynamics. On the other hand,
modern reinforcement learning systems must painstakingly learn through trial
and error that sets of state action pairs are value equivalent -- requiring an
often prohibitively large amount of samples from their environment. MDP
homomorphisms have been proposed that reduce the observed MDP of an environment
to an abstract MDP, which can enable more sample efficient policy learning.
Consequently, impressive improvements in sample efficiency have been achieved
when a suitable MDP homomorphism can be constructed a priori -- usually by
exploiting a practioner's knowledge of environment symmetries. We propose a
novel approach to constructing a homomorphism in discrete action spaces, which
uses a partial model of environment dynamics to infer which state action pairs
lead to the same state -- reducing the size of the state-action space by a
factor equal to the cardinality of the action space. We call this method
equivalent effect abstraction. In a gridworld setting, we demonstrate
empirically that equivalent effect abstraction can improve sample efficiency in
a model-free setting and planning efficiency for modelbased approaches.
Furthermore, we show on cartpole that our approach outperforms an existing
method for learning homomorphisms, while using 33x less training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.06606">PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning. (arXiv:2209.06606v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petit_G/0/1/0/all/0/1">Gr&#xe9;goire Petit</a>, <a href="http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1">Adrian Popescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Belouadah_E/0/1/0/all/0/1">Eden Belouadah</a>, <a href="http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1">David Picard</a>, <a href="http://arxiv.org/find/cs/1/au:+Delezoide_B/0/1/0/all/0/1">Bertrand Delezoide</a></p>
<p>Plasticity and stability are needed in class-incremental learning in order to
learn from new data while preserving past knowledge. Due to catastrophic
forgetting, finding a compromise between these two properties is particularly
challenging when no memory buffer is available. Mainstream methods need to
store two deep models since they integrate new classes using fine-tuning with
knowledge distillation from the previous incremental state. We propose a method
which has similar number of parameters but distributes them differently in
order to find a better balance between plasticity and stability. Following an
approach already deployed by transfer-based incremental methods, we freeze the
feature extractor after the initial state. Classes in the oldest incremental
states are trained with this frozen extractor to ensure stability. Recent
classes are predicted using partially fine-tuned models in order to introduce
plasticity. Our proposed plasticity layer can be incorporated to any
transfer-based method designed for exemplar-free incremental learning, and we
apply it to two such methods. Evaluation is done with three large-scale
datasets. Results show that performance gains are obtained in all tested
configurations compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.06947">Metrics to guide development of machine learning algorithms for malaria diagnosis. (arXiv:2209.06947v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delahunt_C/0/1/0/all/0/1">Charles B. Delahunt</a>, <a href="http://arxiv.org/find/cs/1/au:+Gachuhi_N/0/1/0/all/0/1">Noni Gachuhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Horning_M/0/1/0/all/0/1">Matthew P. Horning</a></p>
<p>Automated malaria diagnosis is a difficult but high-value target for machine
learning (ML), and effective algorithms could save many thousands of children's
lives. However, current ML efforts largely neglect crucial use case constraints
and are thus not clinically useful. Two factors in particular are crucial to
developing algorithms translatable to clinical field settings: (i) Clear
understanding of the clinical needs that ML solutions must accommodate; and
(ii) task-relevant metrics for guiding and evaluating ML models. Neglect of
these factors has seriously hampered past ML work on malaria, because the
resulting algorithms do not align with clinical needs.
</p>
<p>In this paper we address these two issues in the context of automated malaria
diagnosis via microscopy on Giemsa-stained blood films. First, we describe why
domain expertise is crucial to effectively apply ML to malaria, and list
technical documents and other resources that provide this domain knowledge.
Second, we detail performance metrics tailored to the clinical requirements of
malaria diagnosis, to guide development of ML models and evaluate model
performance through the lens of clinical needs (versus a generic ML lens). We
highlight the importance of a patient-level perspective, interpatient
variability, false positive rates, limit of detection, and different types of
error. We also discuss reasons why ROC curves, AUC, and F1, as commonly used in
ML work, are poorly suited to this context. These findings also apply to other
diseases involving parasite loads, including neglected tropical diseases (NTDs)
such as schistosomiasis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15240">Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Taoran Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunchao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a></p>
<p>In recent years, prompt tuning has sparked a research surge in adapting
pre-trained models. Unlike the unified pre-training strategy employed in the
language field, the graph field exhibits diverse pre-training strategies,
posing challenges in designing appropriate prompt-based tuning methods for
graph neural networks. While some pioneering work has devised specialized
prompting functions for models that employ edge prediction as their
pre-training tasks, these methods are limited to specific pre-trained GNN
models and lack broader applicability. In this paper, we introduce a universal
prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained
GNN models under any pre-training strategy. GPF operates on the input graph's
feature space and can theoretically achieve an equivalent effect to any form of
prompting function. Consequently, we no longer need to illustrate the prompting
function corresponding to each pre-training strategy explicitly. Instead, we
employ GPF to obtain the prompted graph for the downstream task in an adaptive
manner. We provide rigorous derivations to demonstrate the universality of GPF
and make guarantee of its effectiveness. The experimental results under various
pre-training strategies indicate that our method performs better than
fine-tuning, with an average improvement of about 1.4% in full-shot scenarios
and about 3.2% in few-shot scenarios. Moreover, our method significantly
outperforms existing specialized prompt-based tuning methods when applied to
models utilizing the pre-training strategy they specialize in. These numerous
advantages position our method as a compelling alternative to fine-tuning for
downstream adaptations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.00708">EraseNet: A Recurrent Residual Network for Supervised Document Cleaning. (arXiv:2210.00708v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shinde_Y/0/1/0/all/0/1">Yashowardhan Shinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1">Kishore Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuberkar_S/0/1/0/all/0/1">Sachin Kuberkar</a></p>
<p>Document denoising is considered one of the most challenging tasks in
computer vision. There exist millions of documents that are still to be
digitized, but problems like document degradation due to natural and man-made
factors make this task very difficult. This paper introduces a supervised
approach for cleaning dirty documents using a new fully convolutional
auto-encoder architecture. This paper focuses on restoring documents with
discrepancies like deformities caused due to aging of a document, creases left
on the pages that were xeroxed, random black patches, lightly visible text,
etc., and also improving the quality of the image for better optical character
recognition system (OCR) performance. Removing noise from scanned documents is
a very important step before the documents as this noise can severely affect
the performance of an OCR system. The experiments in this paper have shown
promising results as the model is able to learn a variety of ordinary as well
as unusual noises and rectify them efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05098">IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1">Kelly Marchisio</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1">Neha Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1">Kevin Duh</a>, <a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1">Philipp Koehn</a></p>
<p>The ability to extract high-quality translation dictionaries from monolingual
word embedding spaces depends critically on the geometric similarity of the
spaces -- their degree of "isomorphism." We address the root-cause of faulty
cross-lingual mapping: that word embedding training resulted in the underlying
spaces being non-isomorphic. We incorporate global measures of isomorphism
directly into the Skip-gram loss function, successfully increasing the relative
isomorphism of trained word embedding spaces and improving their ability to be
mapped to a shared cross-lingual space. The result is improved bilingual
lexicon induction in general data conditions, under domain mismatch, and with
training algorithm dissimilarities. We release IsoVec at
https://github.com/kellymarchisio/isovec.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07722">(1,1)-Cluster Editing is Polynomial-time Solvable. (arXiv:2210.07722v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gutin_G/0/1/0/all/0/1">Gregory Gutin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_A/0/1/0/all/0/1">Anders Yeo</a></p>
<p>A graph $H$ is a clique graph if $H$ is a vertex-disjoin union of cliques.
Abu-Khzam (2017) introduced the $(a,d)$-{Cluster Editing} problem, where for
fixed natural numbers $a,d$, given a graph $G$ and vertex-weights $a^*:\
V(G)\rightarrow \{0,1,\dots, a\}$ and $d^*{}:\ V(G)\rightarrow \{0,1,\dots,
d\}$, we are to decide whether $G$ can be turned into a cluster graph by
deleting at most $d^*(v)$ edges incident to every $v\in V(G)$ and adding at
most $a^*(v)$ edges incident to every $v\in V(G)$. Results by Komusiewicz and
Uhlmann (2012) and Abu-Khzam (2017) provided a dichotomy of complexity (in P or
NP-complete) of $(a,d)$-{Cluster Editing} for all pairs $a,d$ apart from
$a=d=1.$ Abu-Khzam (2017) conjectured that $(1,1)$-{Cluster Editing} is in P.
We resolve Abu-Khzam's conjecture in affirmative by (i) providing a serious of
five polynomial-time reductions to $C_3$-free and $C_4$-free graphs of maximum
degree at most 3, and (ii) designing a polynomial-time algorithm for solving
$(1,1)$-{Cluster Editing} on $C_3$-free and $C_4$-free graphs of maximum degree
at most 3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09477">UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image. (arXiv:2210.09477v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valevski_D/0/1/0/all/0/1">Dani Valevski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalman_M/0/1/0/all/0/1">Matan Kalman</a>, <a href="http://arxiv.org/find/cs/1/au:+Molad_E/0/1/0/all/0/1">Eyal Molad</a>, <a href="http://arxiv.org/find/cs/1/au:+Segalis_E/0/1/0/all/0/1">Eyal Segalis</a>, <a href="http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1">Yossi Matias</a>, <a href="http://arxiv.org/find/cs/1/au:+Leviathan_Y/0/1/0/all/0/1">Yaniv Leviathan</a></p>
<p>Text-driven image generation methods have shown impressive results recently,
allowing casual users to generate high quality images by providing textual
descriptions. However, similar capabilities for editing existing images are
still out of reach. Text-driven image editing methods usually need edit masks,
struggle with edits that require significant visual changes and cannot easily
keep specific details of the edited portion. In this paper we make the
observation that image-generation models can be converted to image-editing
models simply by fine-tuning them on a single image. We also show that
initializing the stochastic sampler with a noised version of the base image
before the sampling and interpolating relevant details from the base image
after sampling further increase the quality of the edit operation. Combining
these observations, we propose UniTune, a novel image editing method. UniTune
gets as input an arbitrary image and a textual edit description, and carries
out the edit while maintaining high fidelity to the input image. UniTune does
not require additional inputs, like masks or sketches, and can perform multiple
edits on the same image without retraining. We test our method using the Imagen
model in a range of different use cases. We demonstrate that it is broadly
applicable and can perform a surprisingly wide range of expressive editing
operations, including those requiring significant visual changes that were
previously impossible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09643">Improving Adversarial Robustness by Contrastive Guided Diffusion Process. (arXiv:2210.09643v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1">Yidong Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Liyan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Guang Cheng</a></p>
<p>Synthetic data generation has become an emerging tool to help improve the
adversarial robustness in classification tasks since robust learning requires a
significantly larger amount of training samples compared with standard
classification tasks. Among various deep generative models, the diffusion model
has been shown to produce high-quality synthetic images and has achieved good
performance in improving the adversarial robustness. However, diffusion-type
methods are typically slow in data generation as compared with other generative
models. Although different acceleration techniques have been proposed recently,
it is also of great importance to study how to improve the sample efficiency of
generated data for the downstream task. In this paper, we first analyze the
optimality condition of synthetic distribution for achieving non-trivial robust
accuracy. We show that enhancing the distinguishability among the generated
data is critical for improving adversarial robustness. Thus, we propose the
Contrastive-Guided Diffusion Process (Contrastive-DP), which adopts the
contrastive loss to guide the diffusion model in data generation. We verify our
theoretical results using simulations and demonstrate the good performance of
Contrastive-DP on image datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10209">Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Prateek Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Continual Learning (CL) methods focus on accumulating knowledge over time
while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)
proposed a CL method, SupSup, which uses a randomly initialized, fixed base
network (model) and finds a supermask for each new task that selectively keeps
or removes each weight to produce a subnetwork. They prevent forgetting as the
network weights are not being updated. Although there is no forgetting, the
performance of SupSup is sub-optimal because fixed weights restrict its
representational power. Furthermore, there is no accumulation or transfer of
knowledge inside the model when new tasks are learned. Hence, we propose
ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and
non-overlapping subnetwork weight training. This avoids conflicting updates to
the shared weights by subsequent tasks to improve performance while still
preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge
Transfer (KKT) module that utilizes previously acquired knowledge to learn new
tasks better and faster. We demonstrate that ExSSNeT outperforms strong
previous methods on both NLP and Vision domains while preventing forgetting.
Moreover, ExSSNeT is particularly advantageous for sparse masks that activate
2-10% of the model parameters, resulting in an average improvement of 8.3% over
SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code
is available at https://github.com/prateeky2806/exessnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10264">SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Han Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Ding-Xuan Zhou</a></p>
<p>Deep neural networks (DNNs) have garnered significant attention in various
fields of science and technology in recent years. Activation functions define
how neurons in DNNs process incoming signals for them. They are essential for
learning non-linear transformations and for performing diverse computations
among successive neuron layers. In the last few years, researchers have
investigated the approximation ability of DNNs to explain their power and
success. In this paper, we explore the approximation ability of DNNs using a
different activation function, called SignReLU. Our theoretical results
demonstrate that SignReLU networks outperform rational and ReLU networks in
terms of approximation performance. Numerical experiments are conducted
comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and
ELU, which illustrate the competitive practical performance of SignReLU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14816">Deep Subspace Encoders for Nonlinear System Identification. (arXiv:2210.14816v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Beintema_G/0/1/0/all/0/1">Gerben I. Beintema</a>, <a href="http://arxiv.org/find/eess/1/au:+Schoukens_M/0/1/0/all/0/1">Maarten Schoukens</a>, <a href="http://arxiv.org/find/eess/1/au:+Toth_R/0/1/0/all/0/1">Roland T&#xf3;th</a></p>
<p>Using Artificial Neural Networks (ANN) for nonlinear system identification
has proven to be a promising approach, but despite of all recent research
efforts, many practical and theoretical problems still remain open.
Specifically, noise handling and models, issues of consistency and reliable
estimation under minimisation of the prediction error are the most severe
problems. The latter comes with numerous practical challenges such as explosion
of the computational cost in terms of the number of data samples and the
occurrence of instabilities during optimization. In this paper, we aim to
overcome these issues by proposing a method which uses a truncated prediction
loss and a subspace encoder for state estimation. The truncated prediction loss
is computed by selecting multiple truncated subsections from the time series
and computing the average prediction loss. To obtain a computationally
efficient estimation method that minimizes the truncated prediction loss, a
subspace encoder represented by an artificial neural network is introduced.
This encoder aims to approximate the state reconstructability map of the
estimated model to provide an initial state for each truncated subsection given
past inputs and outputs. By theoretical analysis, we show that, under mild
conditions, the proposed method is locally consistent, increases optimization
stability, and achieves increased data efficiency by allowing for overlap
between the subsections. Lastly, we provide practical insights and user
guidelines employing a numerical example and state-of-the-art benchmark
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16940">FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs. (arXiv:2210.16940v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yujia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1">Ivan Dario Jimenez Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuanyuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a></p>
<p>Forward invariance is a long-studied property in control theory that is used
to certify that a dynamical system stays within some pre-specified set of
states for all time, and also admits robustness guarantees (e.g., the
certificate holds under perturbations). We propose a general framework for
training and provably certifying robust forward invariance in Neural ODEs. We
apply this framework in two settings: certified safety in robust continuous
control, and certified adversarial robustness for image classification. To our
knowledge, this is the first instance of training NODE policies with such
non-vacuous certified guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.01407">On the Informativeness of Supervision Signals. (arXiv:2211.01407v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1">Ilia Sucholutsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Battleday_R/0/1/0/all/0/1">Ruairidh M. Battleday</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1">Katherine M. Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1">Raja Marjieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Peterson_J/0/1/0/all/0/1">Joshua C. Peterson</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pulkit Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1">Umang Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1">Nori Jacoby</a>, <a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1">Adrian Weller</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a></p>
<p>Supervised learning typically focuses on learning transferable
representations from training examples annotated by humans. While rich
annotations (like soft labels) carry more information than sparse annotations
(like hard labels), they are also more expensive to collect. For example, while
hard labels only provide information about the closest class an object belongs
to (e.g., "this is a dog"), soft labels provide information about the object's
relationship with multiple classes (e.g., "this is most likely a dog, but it
could also be a wolf or a coyote"). We use information theory to compare how a
number of commonly-used supervision signals contribute to
representation-learning performance, as well as how their capacity is affected
by factors such as the number of labels, classes, dimensions, and noise. Our
framework provides theoretical justification for using hard labels in the
big-data regime, but richer supervision signals for few-shot learning and
out-of-distribution generalization. We validate these results empirically in a
series of experiments with over 1 million crowdsourced image annotations and
conduct a cost-benefit analysis to establish a tradeoff curve that enables
users to optimize the cost of supervising representation learning on their own
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06757">DriftRec: Adapting diffusion models to blind JPEG restoration. (arXiv:2211.06757v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Welker_S/0/1/0/all/0/1">Simon Welker</a>, <a href="http://arxiv.org/find/eess/1/au:+Chapman_H/0/1/0/all/0/1">Henry N. Chapman</a>, <a href="http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1">Timo Gerkmann</a></p>
<p>In this work, we utilize the high-fidelity generation abilities of diffusion
models to solve blind JPEG restoration at high compression levels. We propose
an elegant modification of the forward stochastic differential equation of
diffusion models to adapt them to this restoration task and name our method
DriftRec. Comparing DriftRec against an $L_2$ regression baseline with the same
network architecture and two state-of-the-art techniques for JPEG restoration,
we show that our approach can escape the tendency of other methods to generate
blurry images, and recovers the distribution of clean images significantly more
faithfully. For this, only a dataset of clean/corrupted image pairs and no
knowledge about the corruption operation is required, enabling wider
applicability to other restoration tasks. In contrast to other conditional and
unconditional diffusion models, we utilize the idea that the distributions of
clean and corrupted images are much closer to each other than each is to the
usual Gaussian prior of the reverse process in diffusion models. Our approach
therefore requires only low levels of added noise, and needs comparatively few
sampling steps even without further optimizations. We show that DriftRec
naturally generalizes to realistic and difficult scenarios such as unaligned
double JPEG compression and blind restoration of JPEGs found online, without
having encountered such examples during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07398">Age Prediction Performance Varies Across Deep, Superficial, and Cerebellar White Matter Connections. (arXiv:2211.07398v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Wei_Y/0/1/0/all/0/1">Yuxiang Wei</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xue_T/0/1/0/all/0/1">Tengfei Xue</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Rathi_Y/0/1/0/all/0/1">Yogesh Rathi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Makris_N/0/1/0/all/0/1">Nikos Makris</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+ODonnell_L/0/1/0/all/0/1">Lauren J. O&#x27;Donnell</a></p>
<p>The brain's white matter (WM) undergoes developmental and degenerative
processes during the human lifespan. To investigate the relationship between WM
anatomical regions and age, we study diffusion magnetic resonance imaging
tractography that is finely parcellated into fiber clusters in the deep,
superficial, and cerebellar WM. We propose a deep-learning-based age prediction
model that leverages large convolutional kernels and inverted bottlenecks. We
improve performance using novel discrete multi-faceted mix data augmentation
and a novel prior-knowledge-based loss function that encourages age predictions
in the expected range. We study a dataset of 965 healthy young adults (22-37
years) derived from the Human Connectome Project (HCP). Experimental results
demonstrate that the proposed model achieves a mean absolute error of 2.59
years and outperforms compared methods. We find that the deep WM is the most
informative for age prediction in this cohort, while the superficial WM is the
least informative. Overall, the most predictive WM tracts are the
thalamo-frontal tract from the deep WM and the intracerebellar input and
Purkinje tract from the cerebellar WM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08416">Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment. (arXiv:2211.08416v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huihan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasiriany_S/0/1/0/all/0/1">Soroush Nasiriany</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lance Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1">Zhiyao Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuke Zhu</a></p>
<p>With the rapid growth of computing powers and recent advances in deep
learning, we have witnessed impressive demonstrations of novel robot
capabilities in research settings. Nonetheless, these learning systems exhibit
brittle generalization and require excessive training data for practical tasks.
To harness the capabilities of state-of-the-art robot learning models while
embracing their imperfections, we present Sirius, a principled framework for
humans and robots to collaborate through a division of work. In this framework,
partially autonomous robots are tasked with handling a major portion of
decision-making where they work reliably; meanwhile, human operators monitor
the process and intervene in challenging situations. Such a human-robot team
ensures safe deployments in complex tasks. Further, we introduce a new learning
algorithm to improve the policy's performance on the data collected from the
task executions. The core idea is re-weighing training samples with
approximated human trust and optimizing the policies with weighted behavioral
cloning. We evaluate Sirius in simulation and on real hardware, showing that
Sirius consistently outperforms baselines over a collection of contact-rich
manipulation tasks, achieving an 8% boost in simulation and 27% on real
hardware than the state-of-the-art methods in policy success rate, with twice
faster convergence and 85% memory size reduction. Videos and more details are
available at https://ut-austin-rpl.github.io/sirius/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.15536">Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anny_F/0/1/0/all/0/1">Fatema Tuz Zohra Anny</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_O/0/1/0/all/0/1">Oahidul Islam</a></p>
<p>Sentiment analysis or opinion mining help to illustrate the phrase NLP
(Natural Language Processing). Sentiment analysis has been the most significant
topic in recent years. The goal of this study is to solve the sentiment
polarity classification challenges in sentiment analysis. A broad technique for
categorizing sentiment opposition is presented, along with comprehensive
process explanations. With the results of the analysis, both sentence-level
classification and review-level categorization are conducted. Finally, we
discuss our plans for future sentiment analysis research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02090">Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling. (arXiv:2212.02090v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1">Junhyun Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Sangwoo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaeho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a></p>
<p>To capture the relationship between samples and labels, conditional
generative models often inherit spurious correlations from the training
dataset. This can result in label-conditional distributions that are imbalanced
with respect to another latent attribute. To mitigate this issue, which we call
spurious causality of conditional generation, we propose a general two-step
strategy. (a) Fairness Intervention (FI): emphasize the minority samples that
are hard to generate due to the spurious correlation in the training dataset.
(b) Corrective Sampling (CS): explicitly filter the generated samples and
ensure that they follow the desired latent attribute distribution. We have
designed the fairness intervention to work for various degrees of supervision
on the spurious attribute, including unsupervised, weakly-supervised, and
semi-supervised scenarios. Our experimental results demonstrate that FICS can
effectively resolve spurious causality of conditional generation across various
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03369">Exploring Randomly Wired Neural Networks for Climate Model Emulation. (arXiv:2212.03369v3 [physics.ao-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Yik_W/0/1/0/all/0/1">William Yik</a>, <a href="http://arxiv.org/find/physics/1/au:+Silva_S/0/1/0/all/0/1">Sam J. Silva</a>, <a href="http://arxiv.org/find/physics/1/au:+Geiss_A/0/1/0/all/0/1">Andrew Geiss</a>, <a href="http://arxiv.org/find/physics/1/au:+Watson_Parris_D/0/1/0/all/0/1">Duncan Watson-Parris</a></p>
<p>Exploring the climate impacts of various anthropogenic emissions scenarios is
key to making informed decisions for climate change mitigation and adaptation.
State-of-the-art Earth system models can provide detailed insight into these
impacts, but have a large associated computational cost on a per-scenario
basis. This large computational burden has driven recent interest in developing
cheap machine learning models for the task of climate model emulation. In this
manuscript, we explore the efficacy of randomly wired neural networks for this
task. We describe how they can be constructed and compare them to their
standard feedforward counterparts using the ClimateBench dataset. Specifically,
we replace the serially connected dense layers in multilayer perceptrons,
convolutional neural networks, and convolutional long short-term memory
networks with randomly wired dense layers and assess the impact on model
performance for models with 1 million and 10 million parameters. We find that
models with less complex architectures see the greatest performance improvement
with the addition of random wiring (up to 30.4% for multilayer perceptrons).
Furthermore, out of 24 different model architecture, parameter count, and
prediction task combinations, only one saw a statistically significant
performance deficit in randomly wired networks compared to their standard
counterparts, with 14 cases showing statistically significant improvement. We
also find no significant difference in prediction speed between networks with
standard feedforward dense layers and those with randomly wired layers. These
findings indicate that randomly wired neural networks may be suitable direct
replacements for traditional dense layers in many standard models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10503">Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1">Kelly Marchisio</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1">Patrick Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1">Mikel Artetxe</a></p>
<p>Prior work shows that it is possible to expand pretrained Masked Language
Models (MLMs) to new languages by learning a new set of embeddings, while
keeping the transformer body frozen. Despite learning a small subset of
parameters, this approach is not compute-efficient, as training the new
embeddings requires a full forward and backward pass over the entire model. We
propose mini-model adaptation, a compute-efficient alternative that builds a
shallow mini-model from a fraction of a large model's parameters. New
language-specific embeddings can then be efficiently trained over the
mini-model and plugged into the aligned large model for rapid cross-lingual
transfer. We explore two approaches to learn mini-models: MiniJoint, which
jointly pretrains the primary model and the mini-model using a single
transformer with a secondary MLM head at a middle layer; and MiniPost, where we
start from a regular pretrained model, build a mini-model by extracting and
freezing a few layers, and learn a small number of parameters on top.
Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches
the performance of the standard approach using 2.3x less compute on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10549">Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1">Rohan Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1">Rulin Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., "mug in grass") with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from 'mug' to 'grass'
(capturing the semantic relation 'in') to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a 'change of basis' provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06535">Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Islam_J/0/1/0/all/0/1">Jesse Islam</a>, <a href="http://arxiv.org/find/stat/1/au:+Turgeon_M/0/1/0/all/0/1">Maxime Turgeon</a>, <a href="http://arxiv.org/find/stat/1/au:+Sladek_R/0/1/0/all/0/1">Robert Sladek</a>, <a href="http://arxiv.org/find/stat/1/au:+Bhatnagar_S/0/1/0/all/0/1">Sahir Bhatnagar</a></p>
<p>Neural network-based survival methods can model data-driven covariate
interactions. While these methods can provide better predictive performance
than regression-based approaches, not all can model time-varying interactions
and complex baseline hazards. To address this, we propose Case-Base Neural
Networks (CBNNs) as a new approach that combines the case-base sampling
framework with flexible neural network architectures. Using a novel sampling
scheme and data augmentation to naturally account for censoring, we construct a
feed-forward neural network that may take time as an input. CBNNs predict the
probability of an event occurring at a given moment to estimate the hazard
function. We compare the performance of CBNNs to regression and neural
network-based survival methods in a simulation and three case studies using two
time-dependent metrics. First, we examine performance on a simulation involving
a complex baseline hazard and time-varying interactions to assess all methods,
with CBNN outperforming competitors. Then, we apply all methods to three real
data applications, with CBNNs outperforming the competing models in two studies
and showing similar performance in the third. Our results highlight the benefit
of combining case-base sampling with deep learning to provide a simple and
flexible modeling framework for data-driven, time-varying interaction modeling
of single event survival outcomes. An R package is available at
https://github.com/Jesse-Islam/cbnn.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11509">Out-of-distributional risk bounds for neural operators with applications to the Helmholtz equation. (arXiv:2301.11509v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benitez_J/0/1/0/all/0/1">J. Antonio Lara Benitez</a>, <a href="http://arxiv.org/find/cs/1/au:+Furuya_T/0/1/0/all/0/1">Takashi Furuya</a>, <a href="http://arxiv.org/find/cs/1/au:+Faucher_F/0/1/0/all/0/1">Florian Faucher</a>, <a href="http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1">Anastasis Kratsios</a>, <a href="http://arxiv.org/find/cs/1/au:+Tricoche_X/0/1/0/all/0/1">Xavier Tricoche</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoop_M/0/1/0/all/0/1">Maarten V. de Hoop</a></p>
<p>Despite their remarkable success in approximating a wide range of operators
defined by PDEs, existing neural operators (NOs) do not necessarily perform
well for all physics problems. We focus here on high-frequency waves to
highlight possible shortcomings. To resolve these, we propose a subfamily of
NOs enabling an enhanced empirical approximation of the nonlinear operator
mapping wave speed to solution, or boundary values for the Helmholtz equation
on a bounded domain. The latter operator is commonly referred to as the
''forward'' operator in the study of inverse problems. Our methodology draws
inspiration from transformers and techniques such as stochastic depth. Our
experiments reveal certain surprises in the generalization and the relevance of
introducing stochastic depth. Our NOs show superior performance as compared
with standard NOs, not only for testing within the training distribution but
also for out-of-distribution scenarios. To delve into this observation, we
offer an in-depth analysis of the Rademacher complexity associated with our
modified models and prove an upper bound tied to their stochastic depth that
existing NOs do not satisfy. Furthermore, we obtain a novel out-of-distribution
risk bound tailored to Gaussian measures on Banach spaces, again relating
stochastic depth with the bound. We conclude by proposing a hypernetwork
version of the subfamily of NOs as a surrogate model for the mentioned forward
operator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11832">SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces. (arXiv:2301.11832v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adachi_M/0/1/0/all/0/1">Masaki Adachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayakawa_S/0/1/0/all/0/1">Satoshi Hayakawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamid_S/0/1/0/all/0/1">Saad Hamid</a>, <a href="http://arxiv.org/find/cs/1/au:+Jorgensen_M/0/1/0/all/0/1">Martin J&#xf8;rgensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Oberhauser_H/0/1/0/all/0/1">Harald Oberhauser</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1">Micheal A. Osborne</a></p>
<p>Batch Bayesian optimisation and Bayesian quadrature have been shown to be
sample-efficient methods of performing optimisation and quadrature where
expensive-to-evaluate objective functions can be queried in parallel. However,
current methods do not scale to large batch sizes -- a frequent desideratum in
practice (e.g. drug discovery or simulation-based inference). We present a
novel algorithm, SOBER, which permits scalable and diversified batch global
optimisation and quadrature with arbitrary acquisition functions and kernels
over discrete and mixed spaces. The key to our approach is to reformulate batch
selection for global optimisation as a quadrature problem, which relaxes
acquisition function maximisation (non-convex) to kernel recombination
(convex). Bridging global optimisation and quadrature can efficiently solve
both tasks by balancing the merits of exploitative Bayesian optimisation and
explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive
baselines on 12 synthetic and diverse real-world tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12005">EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval. (arXiv:2301.12005v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1">Ankit Singh Rawat</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1">Manzil Zaheer</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1">Sadeep Jayasumana</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadhanala_V/0/1/0/all/0/1">Veeranjaneyulu Sadhanala</a>, <a href="http://arxiv.org/find/cs/1/au:+Jitkrittum_W/0/1/0/all/0/1">Wittawat Jitkrittum</a>, <a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1">Aditya Krishna Menon</a>, <a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1">Rob Fergus</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sanjiv Kumar</a></p>
<p>Large neural models (such as Transformers) achieve state-of-the-art
performance for information retrieval (IR). In this paper, we aim to improve
distillation methods that pave the way for the resource-efficient deployment of
such models in practice. Inspired by our theoretical analysis of the
teacher-student generalization gap for IR models, we propose a novel
distillation approach that leverages the relative geometry among queries and
documents learned by the large teacher model. Unlike existing teacher
score-based distillation methods, our proposed approach employs embedding
matching tasks to provide a stronger signal to align the representations of the
teacher and student models. In addition, it utilizes query generation to
explore the data manifold to reduce the discrepancies between the student and
the teacher where training data is sparse. Furthermore, our analysis also
motivates novel asymmetric architectures for student models which realizes
better embedding alignment without increasing online inference cost. On
standard benchmarks like MSMARCO, we show that our approach successfully
distills from both dual-encoder (DE) and cross-encoder (CE) teacher models to
1/10th size asymmetric students that can retain 95-97% of the teacher
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12040">ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts. (arXiv:2301.12040v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1">Minghao Xu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yuan_X/0/1/0/all/0/1">Xinyu Yuan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Miret_S/0/1/0/all/0/1">Santiago Miret</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a></p>
<p>Current protein language models (PLMs) learn protein representations mainly
based on their sequences, thereby well capturing co-evolutionary information,
but they are unable to explicitly acquire protein functions, which is the end
goal of protein representation learning. Fortunately, for many proteins, their
textual property descriptions are available, where their various functions are
also described. Motivated by this fact, we first build the ProtDescribe dataset
to augment protein sequences with text descriptions of their functions and
other important properties. Based on this dataset, we propose the ProtST
framework to enhance Protein Sequence pre-training and understanding by
biomedical Texts. During pre-training, we design three types of tasks, i.e.,
unimodal mask prediction, multimodal representation alignment and multimodal
mask prediction, to enhance a PLM with protein property information with
different granularities and, at the same time, preserve the PLM's original
representation power. On downstream tasks, ProtST enables both supervised
learning and zero-shot prediction. We verify the superiority of ProtST-induced
PLMs over previous ones on diverse representation learning benchmarks. Under
the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein
classification, and ProtST also enables functional protein retrieval from a
large-scale database without any function annotation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02787">Generative models for two-ground-truth partitions in networks. (arXiv:2302.02787v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mangold_L/0/1/0/all/0/1">Lena Mangold</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_C/0/1/0/all/0/1">Camille Roth</a></p>
<p>A myriad of approaches have been proposed to characterise the mesoscale
structure of networks - most often as a partition based on patterns variously
called communities, blocks, or clusters. Clearly, distinct methods designed to
detect different types of patterns may provide a variety of answers to the
network's mesoscale structure. Yet, even multiple runs of a given method can
sometimes yield diverse and conflicting results, producing entire landscapes of
partitions which potentially include multiple (locally optimal) mesoscale
explanations of the network. Such ambiguity motivates a closer look at the
ability of these methods to find multiple qualitatively different 'ground
truth' partitions in a network. Here, we propose the stochastic cross-block
model (SCBM), a generative model which allows for two distinct partitions to be
built into the mesoscale structure of a single benchmark network. We
demonstrate a use case of the benchmark model by appraising the power of
stochastic block models (SBMs) to detect implicitly planted coexisting
bi-community and core-periphery structures of different strengths. Given our
model design and experimental set-up, we find that the ability to detect the
two partitions individually varies by SBM variant and that coexistence of both
partitions is recovered only in a very limited number of cases. Our findings
suggest that in most instances only one - in some way dominating - structure
can be detected, even in the presence of other partitions. They underline the
need for considering entire landscapes of partitions when different competing
explanations exist and motivate future research to advance partition
coexistence detection methods. Our model also contributes to the field of
benchmark networks more generally by enabling further exploration of the
ability of new and existing methods to detect ambiguity in the mesoscale
structure of networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04391">The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tong Guo</a></p>
<p>In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The experimental results
and human evaluation results verify our idea.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05118">Beyond In-Domain Scenarios: Robust Density-Aware Calibration. (arXiv:2302.05118v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tomani_C/0/1/0/all/0/1">Christian Tomani</a>, <a href="http://arxiv.org/find/cs/1/au:+Waseda_F/0/1/0/all/0/1">Futa Waseda</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yuesong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>Calibrating deep learning models to yield uncertainty-aware predictions is
crucial as deep neural networks get increasingly deployed in safety-critical
applications. While existing post-hoc calibration methods achieve impressive
results on in-domain test datasets, they are limited by their inability to
yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD)
scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving
as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN).
In contrast to existing post-hoc methods, we utilize hidden layers of
classifiers as a source for uncertainty-related information and study their
importance. We show that DAC is a generic method that can readily be combined
with state-of-the-art post-hoc methods. DAC boosts the robustness of
calibration performance in domain-shift and OOD, while maintaining excellent
in-domain predictive uncertainty estimates. We demonstrate that DAC leads to
consistently better calibration across a large number of model architectures,
datasets, and metrics. Additionally, we show that DAC improves calibration
substantially on recent large-scale neural networks pre-trained on vast amounts
of data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07832">Deep Anomaly Detection under Labeling Budget Constraints. (arXiv:2302.07832v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aodong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1">Chen Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1">Marius Kloft</a>, <a href="http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1">Padhraic Smyth</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1">Maja Rudolph</a></p>
<p>Selecting informative data points for expert feedback can significantly
improve the performance of anomaly detection (AD) in various contexts, such as
medical diagnostics or fraud detection. In this paper, we determine a set of
theoretical conditions under which anomaly scores generalize from labeled
queries to unlabeled data. Motivated by these results, we propose a data
labeling strategy with optimal data coverage under labeling budget constraints.
In addition, we propose a new learning framework for semi-supervised AD.
Extensive experiments on image, tabular, and video data sets show that our
approach results in state-of-the-art semi-supervised AD performance under
labeling budget constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07937">The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giannou_A/0/1/0/all/0/1">Angeliki Giannou</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1">Shashank Rajput</a>, <a href="http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1">Dimitris Papailiopoulos</a></p>
<p>Feature normalization transforms such as Batch and Layer-Normalization have
become indispensable ingredients of state-of-the-art deep neural networks.
Recent studies on fine-tuning large pretrained models indicate that just tuning
the parameters of these affine transforms can achieve high accuracy for
downstream tasks. These findings open the questions about the expressive power
of tuning the normalization layers of frozen networks. In this work, we take
the first step towards this question and show that for random ReLU networks,
fine-tuning only its normalization layers can reconstruct any target network
that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even
for randomly sparsified networks, under sufficient overparameterization, in
agreement with prior empirical work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08468">LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1">Ansong Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1">Srini Iyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1">Dragomir Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1">Ves Stoyanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1">Wen-tau Yih</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sida I. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Victoria Lin</a></p>
<p>The advent of large language models trained on code (code LLMs) has led to
significant progress in language-to-code generation. State-of-the-art
approaches in this area combine LLM decoding with sample pruning and reranking
using test cases or heuristics based on the execution results. However, it is
challenging to obtain test cases for many real-world language-to-code
applications, and heuristics cannot well capture the semantic features of the
execution results, such as data type and value range, which often indicates the
correctness of the program. In this work, we propose LEVER, a simple approach
to improve language-to-code generation by learning to verify the generated
programs with their execution results. Specifically, we train verifiers to
determine whether a program sampled from the LLMs is correct or not based on
the natural language input, the program itself and its execution results. The
sampled programs are reranked by combining the verification score with the LLM
generation probability, and marginalizing over programs with the same execution
results. On four datasets across the domains of table QA, math QA and basic
Python programming, LEVER consistently improves over the base code LLMs(4.6% to
10.9% with code-davinci-002) and achieves new state-of-the-art results on all
of them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08652">Minimizing Dynamic Regret on Geodesic Metric Spaces. (arXiv:2302.08652v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zihao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanghui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1">Jacob Abernethy</a></p>
<p>In this paper, we consider the sequential decision problem where the goal is
to minimize the general dynamic regret on a complete Riemannian manifold. The
task of offline optimization on such a domain, also known as a geodesic metric
space, has recently received significant attention. The online setting has
received significantly less attention, and it has remained an open question
whether the body of results that hold in the Euclidean setting can be
transplanted into the land of Riemannian manifolds where new challenges (e.g.,
curvature) come into play. In this paper, we show how to get optimistic regret
bound on manifolds with non-positive curvature whenever improper learning is
allowed and propose an array of adaptive no-regret algorithms. To the best of
our knowledge, this is the first work that considers general dynamic regret and
develops "optimistic" online learning algorithms which can be employed on
geodesic metric spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09532">Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weigang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1">Ziyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1">Yuanhai Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Baosheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Pseudo Labeling is a technique used to improve the performance of
semi-supervised Graph Neural Networks (GNNs) by generating additional
pseudo-labels based on confident predictions. However, the quality of generated
pseudo-labels has been a longstanding concern due to the sensitivity of the
classification objective with respect to the given labels. To avoid the
untrustworthy classification supervision indicating ``a node belongs to a
specific class,'' we favor the fault-tolerant contrasting supervision
demonstrating ``two nodes do not belong to the same class.'' Thus, the problem
of generating high-quality pseudo-labels is then transformed into a relaxed
version, i.e., identifying reliable negative pairs. To achieve this, we propose
a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It
separates two nodes whose positive and negative pseudo-labels target the same
class. To incorporate topological knowledge into learning, we devise a
topologically weighted contrastive loss that spends more effort separating
negative pairs with smaller topological distances. Experimentally, we apply PCL
to various GNNs, which consistently outperform their counterparts using other
popular general techniques on five real-world graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10429">FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy. (arXiv:2302.10429v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tiansheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Federated learning is an emerging distributed machine learning framework
which jointly trains a global model via a large number of local devices with
data privacy protections. Its performance suffers from the non-vanishing biases
introduced by the local inconsistent optimal and the rugged client-drifts by
the local over-fitting. In this paper, we propose a novel and practical method,
FedSpeed, to alleviate the negative impacts posed by these problems.
Concretely, FedSpeed applies the prox-correction term on the current local
updates to efficiently reduce the biases introduced by the prox-term, a
necessary regularizer to maintain the strong local consistency. Furthermore,
FedSpeed merges the vanilla stochastic gradient with a perturbation computed
from an extra gradient ascent step in the neighborhood, thereby alleviating the
issue of local over-fitting. Our theoretical analysis indicates that the
convergence rate is related to both the communication rounds $T$ and local
intervals $K$ with a upper bound $\small \mathcal{O}(1/T)$ if setting a proper
local interval. Moreover, we conduct extensive experiments on the real-world
dataset to demonstrate the efficiency of our proposed FedSpeed, which performs
significantly faster and achieves the state-of-the-art (SOTA) performance on
the general FL experimental settings than several baselines. Our code is
available at \url{https://github.com/woodenchild95/FL-Simulator.git}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10726">Exploring Local Norms in Exp-concave Statistical Learning. (arXiv:2302.10726v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Puchkin_N/0/1/0/all/0/1">Nikita Puchkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1">Nikita Zhivotovskiy</a></p>
<p>We consider the problem of stochastic convex optimization with exp-concave
losses using Empirical Risk Minimization in a convex class. Answering a
question raised in several prior works, we provide a $O( d / n + \log( 1 /
\delta) / n )$ excess risk bound valid for a wide class of bounded exp-concave
losses, where $d$ is the dimension of the convex reference set, $n$ is the
sample size, and $\delta$ is the confidence level. Our result is based on a
unified geometric assumption on the gradient of losses and the notion of local
norms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13259">Can we avoid Double Descent in Deep Neural Networks?. (arXiv:2302.13259v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quetu_V/0/1/0/all/0/1">Victor Qu&#xe9;tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1">Enzo Tartaglione</a></p>
<p>Finding the optimal size of deep learning models is very actual and of broad
impact, especially in energy-saving schemes. Very recently, an unexpected
phenomenon, the ``double descent'', has caught the attention of the deep
learning community. As the model's size grows, the performance gets first
worse, and then goes back to improving. It raises serious questions about the
optimal model's size to maintain high generalization: the model needs to be
sufficiently over-parametrized, but adding too many parameters wastes training
resources. Is it possible to find, in an efficient way, the best trade-off? Our
work shows that the double descent phenomenon is potentially avoidable with
proper conditioning of the learning problem, but a final answer is yet to be
found. We empirically observe that there is hope to dodge the double descent in
complex scenarios with proper regularization, as a simple $\ell_2$
regularization is already positively contributing to such a perspective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13849">Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension. (arXiv:2302.13849v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Filmus_Y/0/1/0/all/0/1">Yuval Filmus</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1">Steve Hanneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehalel_I/0/1/0/all/0/1">Idan Mehalel</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a></p>
<p>A classical result in online learning characterizes the optimal mistake bound
achievable by deterministic learners using the Littlestone dimension
(Littlestone '88). We prove an analogous result for randomized learners: we
show that the optimal expected mistake bound in learning a class $\mathcal{H}$
equals its randomized Littlestone dimension, which is the largest $d$ for which
there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We
further study optimal mistake bounds in the agnostic case, as a function of the
number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$.
We show that the optimal randomized mistake bound for learning a class with
Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies
an optimal deterministic mistake bound of $2k + O (\sqrt{k d} + d )$, thus
resolving an open question which was studied by Auer and Long ['99].
</p>
<p>As an application of our theory, we revisit the classical problem of
prediction using expert advice: about 30 years ago Cesa-Bianchi, Freund,
Haussler, Helmbold, Schapire and Warmuth studied prediction using expert
advice, provided that the best among the $n$ experts makes at most $k$
mistakes, and asked what are the optimal mistake bounds. Cesa-Bianchi, Freund,
Helmbold, and Warmuth ['93, '96] provided a nearly optimal bound for
deterministic learners, and left the randomized case as an open problem. We
resolve this question by providing an optimal learning rule in the randomized
case, and showing that its expected mistake bound equals half of the
deterministic bound, up to negligible additive terms. This improves upon
previous works by Cesa-Bianchi, Freund, Haussler, Helmbold, Schapire and
Warmuth ['93, '97], by Abernethy, Langford, and Warmuth ['06], and by Br\^anzei
and Peres ['19], which handled the regimes $k \ll \log n$ or $k \gg \log n$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04209">Causal Dependence Plots. (arXiv:2303.04209v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loftus_J/0/1/0/all/0/1">Joshua R. Loftus</a>, <a href="http://arxiv.org/find/cs/1/au:+Bynum_L/0/1/0/all/0/1">Lucius E. J. Bynum</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1">Sakina Hansen</a></p>
<p>Explaining artificial intelligence or machine learning models is increasingly
important. To use such data-driven systems wisely we must understand how they
interact with the world, including how they depend causally on data inputs. In
this work we develop Causal Dependence Plots (CDPs) to visualize how one
variable--an outcome--depends on changes in another variable--a
predictor--$\textit{along with any consequent causal changes in other predictor
variables}$. Crucially, CDPs differ from standard methods based on holding
other predictors constant or assuming they are independent. CDPs make use of an
auxiliary causal model because causal conclusions require causal assumptions.
With simulations and real data experiments, we show CDPs can be combined in a
modular way with methods for causal learning or sensitivity analysis. Since
people often think causally about input-output dependence, CDPs can be powerful
tools in the xAI or interpretable machine learning toolkit and contribute to
applications like scientific machine learning and algorithmic fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06455">Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Villaizan_Vallelado_M/0/1/0/all/0/1">Mario Villaiz&#xe1;n-Vallelado</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvatori_M/0/1/0/all/0/1">Matteo Salvatori</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1">Bel&#xe9;n Carro Martinez</a>, <a href="http://arxiv.org/find/cs/1/au:+Esguevillas_A/0/1/0/all/0/1">Antonio Javier Sanchez Esguevillas</a></p>
<p>All industries are trying to leverage Artificial Intelligence (AI) based on
their existing big data which is available in so called tabular form, where
each record is composed of a number of heterogeneous continuous and categorical
columns also known as features. Deep Learning (DL) has constituted a major
breakthrough for AI in fields related to human skills like natural language
processing, but its applicability to tabular data has been more challenging.
More classical Machine Learning (ML) models like tree-based ensemble ones
usually perform better. This paper presents a novel DL model using Graph Neural
Network (GNN) more specifically Interaction Network (IN), for contextual
embedding and modelling interactions among tabular features. Its results
outperform those of a recently published survey with DL benchmark based on five
public datasets, also achieving competitive results when compared to
boosted-tree solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07925">Robust incremental learning pipelines for temporal tabular datasets with distribution shifts. (arXiv:2303.07925v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1">Thomas Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Barahona_M/0/1/0/all/0/1">Mauricio Barahona</a></p>
<p>In this paper, we present a robust deep incremental learning model for
regression tasks on financial temporal tabular datasets. Using commonly
available tabular and time-series prediction models as building blocks, a
machine-learning model is built incrementally to adapt to distributional shifts
in data. Using the concept of self-similarity, the model uses only a basic
building block of machine learning methods, decision trees to build models of
any required complexity. The model is demonstrated to have robust performances
under adverse situations such as regime changes, fat-tailed distributions and
low signal-to-noise ratios which is common in financial datasets. Model
robustness are studied under different hyper-parameters such as model
complexity and data sampling settings using XGBoost models trained on the
Numerai dataset as a detailed case study. The two layer deep ensemble of
XGBoost models over different model snapshots is demonstrated to deliver high
quality predictions under different market regimes. Comparing the XGBoost
models with different number of boosting rounds in three scenarios (small,
standard and large), we demonstrated the model performances are monotonic
increasing with respect to model sizes and converges towards the generalisation
upper bound. Our model is efficient with much lower hardware requirement than
other machine learning models as no specialised neural architectures are used
and each base model can be independently trained in parallel.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08325">FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification. (arXiv:2303.08325v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zikang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1">Quan Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1">Qingsong Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a></p>
<p>Deep learning is becoming increasingly ubiquitous in medical research and
applications while involving sensitive information and even critical diagnosis
decisions. Researchers observe a significant performance disparity among
subgroups with different demographic attributes, which is called model
unfairness, and put lots of effort into carefully designing elegant
architectures to address unfairness, which poses heavy training burden, brings
poor generalization, and reveals the trade-off between model performance and
fairness. To tackle these issues, we propose FairAdaBN by making batch
normalization adaptive to sensitive attribute. This simple but effective design
can be adopted to several classification backbones that are originally unaware
of fairness. Additionally, we derive a novel loss function that restrains
statistical parity between subgroups on mini-batches, encouraging the model to
converge with considerable fairness. In order to evaluate the trade-off between
model performance and fairness, we propose a new metric, named
Fairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness
improvement over accuracy drop. Experiments on two dermatological datasets show
that our proposed method outperforms other methods on fairness criteria and
FATE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11789">Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiwei Zhang</a></p>
<p>We establish a framework of random inverse problems with real-time
observations over graphs, and present a decentralized online learning algorithm
based on online data streams, which unifies the distributed parameter
estimation in Hilbert space and the least mean square problem in reproducing
kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into
the asymptotic stability of randomly time-varying difference equations in
Hilbert space with L2-bounded martingale difference terms and develop the L2
-asymptotic stability theory. It is shown that if the network graph is
connected and the sequence of forward operators satisfies the
infinitedimensional spatio-temporal persistence of excitation condition, then
the estimates of all nodes are mean square and almost surely strongly
consistent. By equivalently transferring the distributed learning problem in
RKHS to the random inverse problem over graphs, we propose a decentralized
online learning algorithm in RKHS based on non-stationary and non-independent
online data streams, and prove that the algorithm is mean square and almost
surely strongly consistent if the operators induced by the random input data
satisfy the infinite-dimensional spatio-temporal persistence of excitation
condition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00133">DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatzimparmpas_A/0/1/0/all/0/1">Angelos Chatzimparmpas</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1">Rafael M. Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1">Alexandru C. Telea</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerren_A/0/1/0/all/0/1">Andreas Kerren</a></p>
<p>As the complexity of machine learning (ML) models increases and the
applications in different (and critical) domains grow, there is a strong demand
for more interpretable and trustworthy ML. One straightforward and
model-agnostic way to interpret complex ML models is to train surrogate models,
such as rule sets and decision trees, that sufficiently approximate the
original ones while being simpler and easier-to-explain. Yet, rule sets can
become very lengthy, with many if-else statements, and decision tree depth
grows rapidly when accurately emulating complex ML models. In such cases, both
approaches can fail to meet their core goal, providing users with model
interpretability. We tackle this by proposing DeforestVis, a visual analytics
tool that offers user-friendly summarization of the behavior of complex ML
models by providing surrogate decision stumps (one-level decision trees)
generated with the adaptive boosting (AdaBoost) technique. Our solution helps
users to explore the complexity vs fidelity trade-off by incrementally
generating more stumps, creating attribute-based explanations with weighted
stumps to justify decision making, and analyzing the impact of rule overriding
on training instance allocation between one or more stumps. An independent test
set allows users to monitor the effectiveness of manual rule changes and form
hypotheses based on case-by-case investigations. We show the applicability and
usefulness of DeforestVis with two use cases and expert interviews with data
analysts and model developers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06808">Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Ting Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1">Kirthevasan Kandasamy</a></p>
<p>We study actively labeling streaming data, where an active learner is faced
with a stream of data points and must carefully choose which of these points to
label via an expensive experiment. Such problems frequently arise in
applications such as healthcare and astronomy. We first study a setting when
the data's inputs belong to one of $K$ discrete distributions and formalize
this problem via a loss that captures the labeling cost and the prediction
error. When the labeling cost is $B$, our algorithm, which chooses to label a
point if the uncertainty is larger than a time and cost dependent threshold,
achieves a worst-case upper bound of $\widetilde{O}(B^{\frac{1}{3}}
K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide
a more nuanced upper bound which demonstrates that the algorithm can adapt to
the arrival pattern, and achieves better performance when the arrival pattern
is more favorable. We complement both upper bounds with matching lower bounds.
We next study this problem when the inputs belong to a continuous domain and
the output of the experiment is a smooth function with bounded RKHS norm. After
$T$ rounds in $d$ dimensions, we show that the loss is bounded by
$\widetilde{O}(B^{\frac{1}{d+3}} T^{\frac{d+2}{d+3}})$ in an RKHS with a
squared exponential kernel and by $\widetilde{O}(B^{\frac{1}{2d+3}}
T^{\frac{2d+2}{2d+3}})$ in an RKHS with a Mat\'ern kernel. Our empirical
evaluation demonstrates that our method outperforms other baselines in several
synthetic experiments and two real experiments in medicine and astronomy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09872">Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms. (arXiv:2304.09872v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blocher_H/0/1/0/all/0/1">Hannah Blocher</a>, <a href="http://arxiv.org/find/cs/1/au:+Schollmeyer_G/0/1/0/all/0/1">Georg Schollmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Jansen_C/0/1/0/all/0/1">Christoph Jansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nalenz_M/0/1/0/all/0/1">Malte Nalenz</a></p>
<p>We propose a framework for descriptively analyzing sets of partial orders
based on the concept of depth functions. Despite intensive studies of depth
functions in linear and metric spaces, there is very little discussion on depth
functions for non-standard data types such as partial orders. We introduce an
adaptation of the well-known simplicial depth to the set of all partial orders,
the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a
comparison of machine learning algorithms based on multidimensional performance
measures. Concretely, we analyze the distribution of different classifier
performances over a sample of standard benchmark data sets. Our results
promisingly demonstrate that our approach differs substantially from existing
benchmarking approaches and, therefore, adds a new perspective to the vivid
debate on the comparison of classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10557">An Introduction to Transformers. (arXiv:2304.10557v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1">Richard E. Turner</a></p>
<p>The transformer is a neural network component that can be used to learn
useful representations of sequences or sets of datapoints. The transformer has
driven recent advances in natural language processing, computer vision, and
spatio-temporal modelling. There are many introductions to transformers, but
most do not contain precise mathematical descriptions of the architecture and
the intuitions behind the design choices are often also missing. Moreover, as
research takes a winding path, the explanations for the components of the
transformer can be idiosyncratic. In this note we aim for a mathematically
precise, intuitive, and clean description of the transformer architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14997">Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Conmy_A/0/1/0/all/0/1">Arthur Conmy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mavor_Parker_A/0/1/0/all/0/1">Augustine N. Mavor-Parker</a>, <a href="http://arxiv.org/find/cs/1/au:+Lynch_A/0/1/0/all/0/1">Aengus Lynch</a>, <a href="http://arxiv.org/find/cs/1/au:+Heimersheim_S/0/1/0/all/0/1">Stefan Heimersheim</a>, <a href="http://arxiv.org/find/cs/1/au:+Garriga_Alonso_A/0/1/0/all/0/1">Adri&#xe0; Garriga-Alonso</a></p>
<p>Through considerable effort and intuition, several recent works have
reverse-engineered nontrivial behaviors of transformer models. This paper
systematizes the mechanistic interpretability process they followed. First,
researchers choose a metric and dataset that elicit the desired model behavior.
Then, they apply activation patching to find which abstract neural network
units are involved in the behavior. By varying the dataset, metric, and units
under investigation, researchers can understand the functionality of each
component. We automate one of the process' steps: to identify the circuit that
implements the specified behavior in the model's computational graph. We
propose several algorithms and reproduce previous interpretability results to
validate them. For example, the ACDC algorithm rediscovered 5/5 of the
component types in a circuit in GPT-2 Small that computes the Greater-Than
operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which
were manually found by previous work. Our code is available at
https://github.com/ArthurConmy/Automatic-Circuit-Discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02301">Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cheng-Yu Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chun-Liang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1">Chih-Kuan Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1">Hootan Nakhost</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1">Yasuhisa Fujii</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1">Alexander Ratner</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chen-Yu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a></p>
<p>Deploying large language models (LLMs) is challenging because they are memory
inefficient and compute-intensive for practical applications. In reaction,
researchers train smaller task-specific models by either finetuning with human
labels or distilling using LLM-generated labels. However, finetuning and
distillation require large amounts of training data to achieve comparable
performance to LLMs. We introduce Distilling step-by-step, a new mechanism that
(a) trains smaller models that outperform LLMs, and (b) achieves so by
leveraging less training data needed by finetuning or distillation. Our method
extracts LLM rationales as additional supervision for training small models
within a multi-task framework. We present three findings across 4 NLP
benchmarks: First, compared to both finetuning and distillation, our mechanism
achieves better performance with much fewer labeled/unlabeled training
examples. Second, compared to few-shot prompted LLMs, we achieve better
performance using substantially smaller model sizes. Third, we reduce both the
model size and the amount of data required to outperform LLMs; our finetuned
770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%
of available data on a benchmark, whereas standard finetuning the same T5 model
struggles to match even by using 100% of the dataset. We release the code at:
https://github.com/google-research/distilling-step-by-step .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02506">String Diagrams with Factorized Densities. (arXiv:2305.02506v4 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sennesh_E/0/1/0/all/0/1">Eli Sennesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1">Jan-Willem van de Meent</a></p>
<p>A growing body of research on probabilistic programs and causal models has
highlighted the need to reason compositionally about model classes that extend
directed graphical models. Both probabilistic programs and causal models define
a joint probability density over a set of random variables, and exhibit sparse
structure that can be used to reason about causation and conditional
independence. This work builds on recent work on Markov categories of
probabilistic mappings to define a category whose morphisms combine a joint
density, factorized over each sample space, with a deterministic mapping from
samples to return values. This is a step towards closing the gap between recent
category-theoretic descriptions of probability measures, and the operational
definitions of factorized densities that are commonly employed in probabilistic
programming and causal inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02562">Conditional and Residual Methods in Scalable Coding for Humans and Machines. (arXiv:2305.02562v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Andrade_A/0/1/0/all/0/1">Anderson de Andrade</a>, <a href="http://arxiv.org/find/eess/1/au:+Harell_A/0/1/0/all/0/1">Alon Harell</a>, <a href="http://arxiv.org/find/eess/1/au:+Foroutan_Y/0/1/0/all/0/1">Yalda Foroutan</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1">Ivan V. Baji&#x107;</a></p>
<p>We present methods for conditional and residual coding in the context of
scalable coding for humans and machines. Our focus is on optimizing the
rate-distortion performance of the reconstruction task using the information
available in the computer vision task. We include an information analysis of
both approaches to provide baselines and also propose an entropy model suitable
for conditional coding with increased modelling capacity and similar
tractability as previous work. We apply these methods to image reconstruction,
using, in one instance, representations created for semantic segmentation on
the Cityscapes dataset, and in another instance, representations created for
object detection on the COCO dataset. In both experiments, we obtain similar
performance between the conditional and residual methods, with the resulting
rate-distortion curves contained within our baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02657">Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1">Yicheng Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1">Zixiong Yu</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_G/0/1/0/all/0/1">Guhan Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Lin_Q/0/1/0/all/0/1">Qian Lin</a></p>
<p>In this paper, we consider the generalization ability of deep wide
feedforward ReLU neural networks defined on a bounded domain $\mathcal X
\subset \mathbb R^{d}$. We first demonstrate that the generalization ability of
the neural network can be fully characterized by that of the corresponding deep
neural tangent kernel (NTK) regression. We then investigate on the spectral
properties of the deep NTK and show that the deep NTK is positive definite on
$\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well
established theories in kernel regression, we then conclude that multilayer
wide neural networks trained by gradient descent with proper early stopping
achieve the minimax rate, provided that the regression function lies in the
reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK.
Finally, we illustrate that the overfitted multilayer wide neural networks can
not generalize well on $\mathbb S^{d}$. We believe our technical contributions
in determining the eigenvalue decay rate of NTK on $\mathbb R^{d}$ might be of
independent interests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08233">Addressing Heterophily in Node Classification with Graph Echo State Networks. (arXiv:2305.08233v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1">Alessio Micheli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tortorella_D/0/1/0/all/0/1">Domenico Tortorella</a></p>
<p>Node classification tasks on graphs are addressed via fully-trained deep
message-passing models that learn a hierarchy of node representations via
multiple aggregations of a node's neighbourhood. While effective on graphs that
exhibit a high ratio of intra-class edges, this approach poses challenges in
the opposite case, i.e. heterophily, where nodes belonging to the same class
are usually further apart. In graphs with a high degree of heterophily, the
smoothed representations based on close neighbours computed by convolutional
models are no longer effective. So far, architectural variations in
message-passing models to reduce excessive smoothing or rewiring the input
graph to improve longer-range message passing have been proposed. In this
paper, we address the challenges of heterophilic graphs with Graph Echo State
Network (GESN) for node classification. GESN is a reservoir computing model for
graphs, where node embeddings are recursively computed by an untrained
message-passing function. Our experiments show that reservoir models are able
to achieve better or comparable accuracy with respect to most fully trained
deep models that implement ad hoc variations in the architectural bias or
perform rewiring as a preprocessing step on the input graph, with an
improvement in terms of efficiency/accuracy trade-off. Furthermore, our
analysis shows that GESN is able to effectively encode the structural
relationships of a graph node, by showing a correlation between iterations of
the recursive embedding function and the distribution of shortest paths in a
graph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12052">Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN). (arXiv:2305.12052v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haces_Garcia_F/0/1/0/all/0/1">Francisco Haces-Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Maslennikova_N/0/1/0/all/0/1">Natalya Maslennikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Glennie_C/0/1/0/all/0/1">Craig L Glennie</a>, <a href="http://arxiv.org/find/cs/1/au:+Rifai_H/0/1/0/all/0/1">Hanadi S Rifai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoskere_V/0/1/0/all/0/1">Vedhus Hoskere</a>, <a href="http://arxiv.org/find/cs/1/au:+Ekhtari_N/0/1/0/all/0/1">Nima Ekhtari</a></p>
<p>Hydrodynamic flood modeling improves hydrologic and hydraulic prediction of
storm events. However, the computationally intensive numerical solutions
required for high-resolution hydrodynamics have historically prevented their
implementation in near-real-time flood forecasting. This study examines whether
several Deep Neural Network (DNN) architectures are suitable for optimizing
hydrodynamic flood models. Several pluvial flooding events were simulated in a
low-relief high-resolution urban environment using a 2D HEC-RAS hydrodynamic
model. These simulations were assembled into a training set for the DNNs, which
were then used to forecast flooding depths and velocities. The DNNs' forecasts
were compared to the hydrodynamic flood models, and showed good agreement, with
a median RMSE of around 2 mm for cell flooding depths in the study area. The
DNNs also improved forecast computation time significantly, with the DNNs
providing forecasts between 34.2 and 72.4 times faster than conventional
hydrodynamic models. The study area showed little change between HEC-RAS' Full
Momentum Equations and Diffusion Equations, however, important numerical
stability considerations were discovered that impact equation selection and DNN
architecture configuration. Overall, the results from this study show that DNNs
can greatly optimize hydrodynamic flood modeling, and enable near-real-time
hydrodynamic flood forecasting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19525">Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v3 [math.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Liu_Z/0/1/0/all/0/1">Ziming Liu</a>, <a href="http://arxiv.org/find/math/1/au:+Sturm_P/0/1/0/all/0/1">Patrick Obin Sturm</a>, <a href="http://arxiv.org/find/math/1/au:+Bharadwaj_S/0/1/0/all/0/1">Saketh Bharadwaj</a>, <a href="http://arxiv.org/find/math/1/au:+Silva_S/0/1/0/all/0/1">Sam Silva</a>, <a href="http://arxiv.org/find/math/1/au:+Tegmark_M/0/1/0/all/0/1">Max Tegmark</a></p>
<p>Discovering conservation laws for a given dynamical system is important but
challenging. In a theorist setup (differential equations and basis functions
are both known), we propose the Sparse Invariant Detector (SID), an algorithm
that auto-discovers conservation laws from differential equations. Its
algorithmic simplicity allows robustness and interpretability of the discovered
conserved quantities. We show that SID is able to rediscover known and even
discover new conservation laws in a variety of systems. For two examples in
fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved
quantities, respectively, where only 12 and 2 were previously known to domain
experts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20030">Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. (arXiv:2305.20030v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yuxin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1">John Kirchenbauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1">Jonas Geiping</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a></p>
<p>Watermarking the outputs of generative models is a crucial technique for
tracing copyright and preventing potential harm from AI-generated content. In
this paper, we introduce a novel technique called Tree-Ring Watermarking that
robustly fingerprints diffusion model outputs. Unlike existing methods that
perform post-hoc modifications to images after sampling, Tree-Ring Watermarking
subtly influences the entire sampling process, resulting in a model fingerprint
that is invisible to humans. The watermark embeds a pattern into the initial
noise vector used for sampling. These patterns are structured in Fourier space
so that they are invariant to convolutions, crops, dilations, flips, and
rotations. After image generation, the watermark signal is detected by
inverting the diffusion process to retrieve the noise vector, which is then
checked for the embedded signal. We demonstrate that this technique can be
easily applied to arbitrary diffusion models, including text-conditioned Stable
Diffusion, as a plug-in with negligible loss in FID. Our watermark is
semantically hidden in the image space and is far more robust than watermarking
alternatives that are currently deployed. Code is available at
https://github.com/YuxinWenRick/tree-ring-watermark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02210">GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tiantian Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1">Samiul Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1">Dimitrios Dimitriadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1">Shrikanth S. Narayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1">Salman Avestimehr</a></p>
<p>In this work, we propose GPT-FL, a generative pre-trained model-assisted
federated learning (FL) framework. At its core, GPT-FL leverages generative
pre-trained models to generate diversified synthetic data. These generated data
are used to train a downstream model on the server, which is then fine-tuned
with private client data under the standard FL framework. We show that GPT-FL
consistently outperforms state-of-the-art FL methods in terms of model test
accuracy, communication efficiency, and client sampling efficiency. Through
comprehensive ablation analysis, we discover that the downstream model
generated by synthetic data plays a crucial role in controlling the direction
of gradient diversity during FL training, which enhances convergence speed and
contributes to the notable accuracy boost observed with GPT-FL. Also,
regardless of whether the target data falls within or outside the domain of the
pre-trained generative model, GPT-FL consistently achieves significant
performance gains, surpassing the results obtained by models trained solely
with FL or synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04719">Don&#x27;t trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1">Robert Geirhos</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1">Roland S. Zimmermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1">Blair Bilodeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1">Wieland Brendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Been Kim</a></p>
<p>How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to "explain" how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07024">DRCFS: Doubly Robust Causal Feature Selection. (arXiv:2306.07024v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quinzan_F/0/1/0/all/0/1">Francesco Quinzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Soleymani_A/0/1/0/all/0/1">Ashkan Soleymani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1">Patrick Jaillet</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojas_C/0/1/0/all/0/1">Cristian R. Rojas</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1">Stefan Bauer</a></p>
<p>Knowing the features of a complex system that are highly relevant to a
particular target variable is of fundamental interest in many areas of science.
Existing approaches are often limited to linear settings, sometimes lack
guarantees, and in most cases, do not scale to the problem at hand, in
particular to images. We propose DRCFS, a doubly robust feature selection
method for identifying the causal features even in nonlinear and high
dimensional settings. We provide theoretical guarantees, illustrate necessary
conditions for our assumptions, and perform extensive experiments across a wide
range of simulated and semi-synthetic datasets. DRCFS significantly outperforms
existing state-of-the-art methods, selecting robust features even in
challenging highly non-linear and high-dimensional problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08149">Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wortwein_T/0/1/0/all/0/1">Torsten W&#xf6;rtwein</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1">Nicholas Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheeber_L/0/1/0/all/0/1">Lisa B. Sheeber</a>, <a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1">Randy P. Auerbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_J/0/1/0/all/0/1">Jeffrey F. Cohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>Personalized prediction is a machine learning approach that predicts a
person's future observations based on their past labeled observations and is
typically used for sequential tasks, e.g., to predict daily mood ratings. When
making personalized predictions, a model can combine two types of trends: (a)
trends shared across people, i.e., person-generic trends, such as being happier
on weekends, and (b) unique trends for each person, i.e., person-specific
trends, such as a stressful weekly meeting. Mixed effect models are popular
statistical models to study both trends by combining person-generic and
person-specific parameters. Though linear mixed effect models are gaining
popularity in machine learning by integrating them with neural networks, these
integrations are currently limited to linear person-specific parameters: ruling
out nonlinear person-specific trends. In this paper, we propose Neural Mixed
Effect (NME) models to optimize nonlinear person-specific parameters anywhere
in a neural network in a scalable manner. NME combines the efficiency of neural
network optimization with nonlinear mixed effects modeling. Empirically, we
observe that NME improves performance across six unimodal and multimodal
datasets, including a smartphone dataset to predict daily mood and a
mother-adolescent dataset to predict affective state sequences where half the
mothers experience at least moderate symptoms of depression. Furthermore, we
evaluate NME for two model architectures, including for neural conditional
random fields (CRF) to predict affective state sequences where the CRF learns
nonlinear person-specific temporal transitions between affective states.
Analysis of these person-specific transitions on the mother-adolescent dataset
shows interpretable trends related to the mother's depression symptoms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09459">Recurrent Memory Decision Transformer. (arXiv:2306.09459v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bessonov_A/0/1/0/all/0/1">Arkadii Bessonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Staroverov_A/0/1/0/all/0/1">Alexey Staroverov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huzhenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovalev_A/0/1/0/all/0/1">Alexey K. Kovalev</a>, <a href="http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1">Dmitry Yudin</a>, <a href="http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1">Aleksandr I. Panov</a></p>
<p>Originally developed for natural language problems, transformer models have
recently been widely used in offline reinforcement learning tasks. This is
because the agent's history can be represented as a sequence, and the whole
task can be reduced to the sequence modeling task. However, the quadratic
complexity of the transformer operation limits the potential increase in
context. Therefore, different versions of the memory mechanism are used to work
with long sequences in a natural language. This paper proposes the Recurrent
Memory Decision Transformer (RMDT), a model that uses a recurrent memory
mechanism for reinforcement learning problems. We conduct thorough experiments
on Atari games and MuJoCo control problems and show that our proposed model is
significantly superior to its counterparts without the recurrent memory
mechanism on Atari games. We also carefully study the effect of memory on the
performance of the proposed model. These findings shed light on the potential
of incorporating recurrent memory mechanisms to improve the performance of
large-scale transformer models in offline reinforcement learning tasks. The
Recurrent Memory Decision Transformer code is publicly available in the
repository \url{https://anonymous.4open.science/r/RMDT-4FE4}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11168">Learning Models of Adversarial Agent Behavior under Partial Observability. (arXiv:2306.11168v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1">Sean Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_M/0/1/0/all/0/1">Manisha Natarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zixuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Paleja_R/0/1/0/all/0/1">Rohan Paleja</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Letian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1">Matthew C. Gombolay</a></p>
<p>The need for opponent modeling and tracking arises in several real-world
scenarios, such as professional sports, video game design, and drug-trafficking
interdiction. In this work, we present Graph based Adversarial Modeling with
Mutal Information (GrAMMI) for modeling the behavior of an adversarial opponent
agent. GrAMMI is a novel graph neural network (GNN) based approach that uses
mutual information maximization as an auxiliary objective to predict the
current and future states of an adversarial opponent with partial
observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion
domains inspired by real-world scenarios, where a team of heterogeneous agents
is tasked with tracking and interdicting a single adversarial agent, and the
adversarial agent must evade detection while achieving its own objectives. With
the mutual information formulation, GrAMMI outperforms all baselines in both
domains and achieves 31.68% higher log-likelihood on average for future
adversarial state predictions across both domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11497">Convergence and concentration properties of constant step-size SGD through Markov chains. (arXiv:2306.11497v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Merad_I/0/1/0/all/0/1">Ibrahim Merad</a>, <a href="http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1">St&#xe9;phane Ga&#xef;ffas</a></p>
<p>We consider the optimization of a smooth and strongly convex objective using
constant step-size stochastic gradient descent (SGD) and study its properties
through the prism of Markov chains. We show that, for unbiased gradient
estimates with mildly controlled variance, the iteration converges to an
invariant distribution in total variation distance. We also establish this
convergence in Wasserstein-2 distance in a more general setting compared to
previous work. Thanks to the invariance property of the limit distribution, our
analysis shows that the latter inherits sub-Gaussian or sub-exponential
concentration properties when these hold true for the gradient. This allows the
derivation of high-confidence bounds for the final estimate. Finally, under
such conditions in the linear case, we obtain a dimension-free deviation bound
for the Polyak-Ruppert average of a tail sequence. All our results are
non-asymptotic and their consequences are discussed through a few applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11730">Segment Anything Model (SAM) for Radiation Oncology. (arXiv:2306.11730v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lian Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1">Zihao Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1">Xiaowei Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Holmes_J/0/1/0/all/0/1">Jason Holmes</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1">Hongying Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1">Haixing Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1">Quanzheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1">Dajiang Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>In this study, we evaluate the performance of the Segment Anything Model
(SAM) in clinical radiotherapy. Our results indicate that SAM's 'segment
anything' mode can achieve clinically acceptable segmentation results in most
organs-at-risk (OARs) with Dice scores higher than 0.7. SAM's 'box prompt' mode
further improves the Dice scores by 0.1 to 0.5. Considering the size of the
organ and the clarity of its boundary, SAM displays better performance for
large organs with clear boundaries but performs worse for smaller organs with
unclear boundaries. Given that SAM, a model pre-trained purely on natural
images, can handle the delineation of OARs from medical images with clinically
acceptable accuracy, these results highlight SAM's robust generalization
capabilities with consistent accuracy in automatic segmentation for
radiotherapy. In other words, SAM can achieve delineation of different OARs at
different sites using a generic automatic segmentation model. SAM's
generalization capabilities across different disease sites suggest that it is
technically feasible to develop a generic model for automatic segmentation in
radiotherapy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12070">Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianghui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xingyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Cong Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a></p>
<p>Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13695">Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ling_H/0/1/0/all/0/1">Hang Jung Ling</a>, <a href="http://arxiv.org/find/eess/1/au:+Bernard_O/0/1/0/all/0/1">Olivier Bernard</a>, <a href="http://arxiv.org/find/eess/1/au:+Ducros_N/0/1/0/all/0/1">Nicolas Ducros</a>, <a href="http://arxiv.org/find/eess/1/au:+Garcia_D/0/1/0/all/0/1">Damien Garcia</a></p>
<p>Color Doppler echocardiography is a widely used non-invasive imaging modality
that provides real-time information about the intracardiac blood flow. In an
apical long-axis view of the left ventricle, color Doppler is subject to phase
wrapping, or aliasing, especially during cardiac filling and ejection. When
setting up quantitative methods based on color Doppler, it is necessary to
correct this wrapping artifact. We developed an unfolded primal-dual network to
unwrap (dealias) color Doppler echocardiographic images and compared its
effectiveness against two state-of-the-art segmentation approaches based on
nnU-Net and transformer models. We trained and evaluated the performance of
each method on an in-house dataset and found that the nnU-Net-based method
provided the best dealiased results, followed by the primal-dual approach and
the transformer-based technique. Noteworthy, the primal-dual network, which had
significantly fewer trainable parameters, performed competitively with respect
to the other two methods, demonstrating the high potential of deep unfolding
methods. Our results suggest that deep learning-based methods can effectively
remove aliasing artifacts in color Doppler echocardiographic images,
outperforming DeAN, a state-of-the-art semi-automatic technique. Overall, our
results show that deep learning-based methods have the potential to effectively
preprocess color Doppler images for downstream quantitative analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14568">Elucidating Interfacial Dynamics of Ti-Al Systems Using Molecular Dynamics Simulation and Markov State Modeling. (arXiv:2306.14568v2 [cond-mat.mes-hall] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Li_T/0/1/0/all/0/1">Tianjiao Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Tian_C/0/1/0/all/0/1">Chenxi Tian</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Moridi_A/0/1/0/all/0/1">Atieh Moridi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yeo_J/0/1/0/all/0/1">Jingjie Yeo</a></p>
<p>Due to their remarkable mechanical and chemical properties, Ti-Al based
materials are attracting considerable interest in numerous fields of
engineering, such as automotive, aerospace, and defense. With their low
density, high strength, and resistance to corrosion and oxidation, these
intermetallic alloys and compound metal-metallic composites have found diverse
applications. The present study delves into the interfacial dynamics of these
Ti-Al systems, particularly focusing on the behavior of Ti and Al atoms in the
presence of TiAl$_3$ grain boundaries under experimental heat treatment
conditions. Using a combination of Molecular Dynamics and Markov State Model
analyses, we scrutinize the kinetic processes involved in the formation of
TiAl$_3$. The Molecular Dynamics simulation indicates that at the early stage
of heat treatment, the predominating process is the diffusion of Al atoms
towards the Ti surface through the TiAl$_3$ grain boundaries. The Markov State
Modeling identifies three distinct dynamic states of Al atoms within the Ti/Al
mixture that forms during the process, each exhibiting a unique spatial
distribution. Using transition timescales as a qualitative measure of the
rapidness of the dynamics, it is observed that the Al dynamics is significantly
less rapid near the Ti surface compared to the Al surface. Put together, the
results offer a comprehensive understanding of the interfacial dynamics and
reveals a three-stage diffusion mechanism. The process initiates with the
premelting of Al, proceeds with the prevalent diffusion of Al atoms towards the
Ti surface, and eventually ceases as the Ti concentration within the mixture
progressively increases. The insights gained from this study could contribute
significantly to the control and optimization of manufacturing processes for
these high-performing Ti-Al based materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16208">Continuous Time q-learning for McKean-Vlasov Control Problems. (arXiv:2306.16208v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaoli Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiang Yu</a></p>
<p>This paper studies the q-learning, recently coined as the continuous time
counterpart of Q-learning by Jia and Zhou (2023), for continuous time
Mckean-Vlasov control problems in the setting of entropy-regularized
reinforcement learning. In contrast to the single agent's control problem in
Jia and Zhou (2023), the mean-field interaction of agents renders the
definition of the q-function more subtle, for which we reveal that two distinct
q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as
the first-order approximation of the integrated Q-function introduced in Gu,
Guo, Wei and Xu (2023), which can be learnt by a weak martingale condition
involving test policies; and (ii) the essential q-function (denoted by $q_e$)
that is employed in the policy improvement iterations. We show that two
q-functions are related via an integral representation under all test policies.
Based on the weak martingale condition and our proposed searching method of
test policies, some model-free learning algorithms are devised. In two
examples, one in LQ control framework and one beyond LQ control framework, we
can obtain the exact parameterization of the optimal value function and
q-functions and illustrate our algorithms with simulation experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16688">SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_Z/0/1/0/all/0/1">Zhiyu Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1">Wei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangju Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huanchen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a></p>
<p>The ever-growing complexity of reinforcement learning (RL) tasks demands a
distributed RL system to efficiently generate and process a massive amount of
data to train intelligent agents. However, existing open-source libraries
suffer from various limitations, which impede their practical use in
challenging scenarios where large-scale training is necessary. While industrial
systems from OpenAI and DeepMind have achieved successful large-scale RL
training, their system architecture and implementation details remain
undisclosed to the community. In this paper, we present a novel abstraction on
the dataflows of RL training, which unifies practical RL training across
diverse applications into a general framework and enables fine-grained
optimizations. Following this abstraction, we develop a scalable, efficient,
and extensible distributed RL system called ReaLly Scalable RL (SRL). The
system architecture of SRL separates major RL computation components and allows
massively parallelized training. Moreover, SRL offers user-friendly and
extensible interfaces for customized algorithms. Our evaluation shows that SRL
outperforms existing academic libraries in both a single machine and a
medium-sized cluster. In a large-scale cluster, the novel architecture of SRL
leads to up to 3.7x speedup compared to the design choices adopted by the
existing libraries. We also conduct a direct benchmark comparison to OpenAI's
industrial system, Rapid, in the challenging hide-and-seek environment. SRL
reproduces the same solution as reported by OpenAI with up to 5x speedup in
wall-clock time. Furthermore, we also examine the performance of SRL in a much
harder variant of the hide-and-seek environment and achieve substantial
learning speedup by scaling SRL to over 15k CPU cores and 32 A100 GPUs.
Notably, SRL is the first in the academic community to perform RL experiments
at such a large scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17010">milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fangqiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peijun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Xiaoxuan Lu</a></p>
<p>Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose \textit{milliFlow}, a novel deep learning method for
scene flow estimation as a complementary motion information for mmWave point
cloud, serving as an intermediate level of features and directly benefiting
downstream human motion sensing tasks. Experimental results demonstrate the
superior performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we provide our codebase and dataset for
open access.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00106">Distance Functions and Normalization Under Stream Scenarios. (arXiv:2307.00106v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barboza_E/0/1/0/all/0/1">Eduardo V. L. Barboza</a>, <a href="http://arxiv.org/find/cs/1/au:+Almeida_P/0/1/0/all/0/1">Paulo R. Lisboa de Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1">Alceu de Souza Britto Jr</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1">Rafael M. O. Cruz</a></p>
<p>Data normalization is an essential task when modeling a classification
system. When dealing with data streams, data normalization becomes especially
challenging since we may not know in advance the properties of the features,
such as their minimum/maximum values, and these properties may change over
time. We compare the accuracies generated by eight well-known distance
functions in data streams without normalization, normalized considering the
statistics of the first batch of data received, and considering the previous
batch received. We argue that experimental protocols for streams that consider
the full stream as normalized are unrealistic and can lead to biased and poor
results. Our results indicate that using the original data stream without
applying normalization, and the Canberra distance, can be a good combination
when no information about the data stream is known beforehand.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00426">Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muthukumar_R/0/1/0/all/0/1">Ramchandran Muthukumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1">Jeremias Sulam</a></p>
<p>Deep artificial neural networks achieve surprising generalization abilities
that remain poorly understood. In this paper, we present a new approach to
analyzing generalization for deep feed-forward ReLU networks that takes
advantage of the degree of sparsity that is achieved in the hidden layer
activations. By developing a framework that accounts for this reduced effective
model size for each input sample, we are able to show fundamental trade-offs
between sparsity and generalization. Importantly, our results make no strong
assumptions about the degree of sparsity achieved by the model, and it improves
over recent norm-based approaches. We illustrate our results numerically,
demonstrating non-vacuous bounds when coupled with data-dependent priors in
specific settings, even in over-parametrized models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00668">Active Sensing with Predictive Coding and Uncertainty Minimization. (arXiv:2307.00668v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharafeldin_A/0/1/0/all/0/1">Abdelrahman Sharafeldin</a>, <a href="http://arxiv.org/find/cs/1/au:+Imam_N/0/1/0/all/0/1">Nabil Imam</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1">Hannah Choi</a></p>
<p>We present an end-to-end procedure for embodied exploration based on two
biologically inspired computations: predictive coding and uncertainty
minimization. The procedure can be applied to any exploration setting in a
task-independent and intrinsically driven manner. We first demonstrate our
approach in a maze navigation task and show that our model is capable of
discovering the underlying transition distribution and reconstructing the
spatial features of the environment. Second, we apply our model to the more
complex task of active vision, where an agent must actively sample its visual
environment to gather information. We show that our model is able to build
unsupervised representations that allow it to actively sample and efficiently
categorize sensory scenes. We further show that using these representations as
input for downstream classification leads to superior data efficiency and
learning speed compared to other baselines, while also maintaining lower
parameter complexity. Finally, the modularity of our model allows us to analyze
its internal mechanisms and to draw insight into the interactions between
perception and action during exploratory behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00677">SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1">Hao Shu</a></p>
<p>Density-based clustering could be the most popular clustering algorithm since
it can identify clusters of arbitrary shape as long as different (high-density)
clusters are separated by low-density regions. However, the requirement of the
separateness of clusters by low-density regions is not trivial since a
high-density region might have different structures which should be clustered
into different groups. Such a situation demonstrates the main flaw of all
previous density-based clustering algorithms we have known--structures in a
high-density cluster could not be detected. Therefore, this paper aims to
provide a density-based clustering scheme that not only has the ability
previous ones have but could also detect structures in a high-density region
not separated by low-density ones. The algorithm employs secondary directed
differential, hierarchy, normalized density, as well as the self-adaption
coefficient, and thus is called Structure Detecting Cluster by Hierarchical
Secondary Directed Differential with Normalized Density and Self-Adaption,
dubbed by SDC-HSDD-NDSA for short. To illustrate its effectiveness, we run the
algorithm in several data sets. The results verify its validity in structure
detection, robustness over noises, as well as independence of granularities,
and demonstrate that it could outperform previous ones. The Python code of the
paper could be found on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00859">CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhijit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1">Arnab Mukherjee</a></p>
<p>In the expansive realm of drug discovery, with approximately 15,000 known
drugs and only around 4,200 approved, the combinatorial nature of the chemical
space presents a formidable challenge. While Artificial Intelligence (AI) has
emerged as a powerful ally, traditional AI frameworks face significant hurdles.
This manuscript introduces CardiGraphormer, a groundbreaking approach that
synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and
Cardinality Preserving Attention to revolutionize drug discovery.
CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving
Attention, leverages SSL to learn potent molecular representations and employs
GNNs to extract molecular fingerprints, enhancing predictive performance and
interpretability while reducing computation time. It excels in handling complex
data like molecular structures and performs tasks associated with nodes, pairs
of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential
applications in drug discovery and drug interactions are vast, from identifying
new drug targets to predicting drug-to-drug interactions and enabling novel
drug discovery. This innovative approach provides an AI-enhanced methodology in
drug development, utilizing SSL combined with GNNs to overcome existing
limitations and pave the way for a richer exploration of the vast combinatorial
chemical space in drug discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01146">AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shengyi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tong Lu</a></p>
<p>The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01163">Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1">Kelly Marchisio</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David Ifeoluwa Adelani</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1">Sebastian Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1">Mikel Artetxe</a></p>
<p>Pretrained language models (PLMs) are today the primary model for natural
language processing. Despite their impressive downstream performance, it can be
difficult to apply PLMs to new languages, a barrier to making their
capabilities universally accessible. While prior work has shown it possible to
address this issue by learning a new embedding layer for the new language,
doing so is both data and compute inefficient. We propose to use an active
forgetting mechanism during pretraining, as a simple way of creating PLMs that
can quickly adapt to new languages. Concretely, by resetting the embedding
layer every K updates during pretraining, we encourage the PLM to improve its
ability of learning new embeddings within a limited number of updates, similar
to a meta-learning effect. Experiments with RoBERTa show that models pretrained
with our forgetting mechanism not only demonstrate faster convergence during
language adaptation but also outperform standard ones in a low-data regime,
particularly for languages that are distant from English.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.09325">TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chuer Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1">Brian Okorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Harry Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1">Ben Eisner</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1">David Held</a></p>
<p>How do we imbue robots with the ability to efficiently manipulate unseen
objects and transfer relevant skills based on demonstrations? End-to-end
learning methods often fail to generalize to novel objects or unseen
configurations. Instead, we focus on the task-specific pose relationship
between relevant parts of interacting objects. We conjecture that this
relationship is a generalizable notion of a manipulation task that can transfer
to new objects in the same category; examples include the relationship between
the pose of a pan relative to an oven or the pose of a mug relative to a mug
rack. We call this task-specific pose relationship "cross-pose" and provide a
mathematical definition of this concept. We propose a vision-based system that
learns to estimate the cross-pose between two objects for a given manipulation
task using learned cross-object correspondences. The estimated cross-pose is
then used to guide a downstream motion planner to manipulate the objects into
the desired pose relationship (placing a pan into the oven or the mug onto the
mug rack). We demonstrate our method's capability to generalize to unseen
objects, in some cases after training on only 10 demonstrations in the real
world. Results show that our system achieves state-of-the-art performance in
both simulated and real-world experiments across a number of tasks.
Supplementary information and videos can be found at
https://sites.google.com/view/tax-pose/home.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12680">Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangbo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Satapathy_R/0/1/0/all/0/1">Ranjan Satapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a></p>
<p>In this technical survey, we comprehensively summarize the latest
advancements in the field of recommender systems. The objective of this study
is to provide an overview of the current state-of-the-art in the field and
highlight the latest trends in the development of recommender systems. The
study starts with a comprehensive summary of the main taxonomy of recommender
systems, including personalized and group recommender systems, and then delves
into the category of knowledge-based recommender systems. In addition, the
survey analyzes the robustness, data bias, and fairness issues in recommender
systems, summarizing the evaluation metrics used to assess the performance of
these systems. Finally, the study provides insights into the latest trends in
the development of recommender systems and highlights the new directions for
future research in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00165">Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jianchao Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zelong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1">Max Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>Causal reasoning and logical reasoning are two important types of reasoning
abilities for human intelligence. However, their relationship has not been
extensively explored under machine intelligence context. In this paper, we
explore how the two reasoning abilities can be jointly modeled to enhance both
accuracy and explainability of machine learning models. More specifically, by
integrating two important types of reasoning ability -- counterfactual
reasoning and (neural) logical reasoning -- we propose Counterfactual
Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to
improve the performance. In particular, we use recommender system as an example
to show how CCR alleviate data scarcity, improve accuracy and enhance
transparency. Technically, we leverage counterfactual reasoning to generate
"difficult" counterfactual training examples for data augmentation, which --
together with the original training examples -- can enhance the model
performance. Since the augmented data is model irrelevant, they can be used to
enhance any model, enabling the wide applicability of the technique. Besides,
most of the existing data augmentation methods focus on "implicit data
augmentation" over users' implicit feedback, while our framework conducts
"explicit data augmentation" over users explicit feedback based on
counterfactual logic reasoning. Experiments on three real-world datasets show
that CCR achieves better performance than non-augmented models and implicitly
augmented models, and also improves model transparency by generating
counterfactual explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01088">Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data. (arXiv:2307.01088v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasa_K/0/1/0/all/0/1">Kevin Kasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">Graham W. Taylor</a></p>
<p>Conformal prediction has emerged as a rigorous means of providing deep
learning models with reliable uncertainty estimates and safety guarantees. Yet,
its performance is known to degrade under distribution shift and long-tailed
class distributions, which are often present in real world applications. Here,
we characterize the performance of several post-hoc and training-based
conformal prediction methods under these settings, providing the first
empirical evaluation on large-scale datasets and models. We show that across
numerous conformal methods and neural network families, performance greatly
degrades under distribution shifts violating safety guarantees. Similarly, we
show that in long-tailed settings the guarantees are frequently violated on
many classes. Understanding the limitations of these methods is necessary for
deployment in real world and safety-critical applications.
</p>
</p>
</div>

    </div>
    </body>
    