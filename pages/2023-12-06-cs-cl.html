<!DOCTYPE html>
<html>
<head>
<title>2023-12-06-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.00789">Do androids dream of fictional references? A bibliographic dialogue with ChatGPT3.5. (arXiv:2312.00789v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vergnas_O/0/1/0/all/0/1">Olivier Las Vergnas</a> (AFA, CIREL)</p>
<p>This article focuses on bibliographic references generated by the ChatGPT3.5
tool. Using this tool based on the trained GPT generation model ChatGPT3.5,
developed by the company OpenAI, we explored six different themes and analyzed
a sample of references generated by the model, in French and English. The
results revealed high percentages of fictitious references in several fields,
underlining the importance of carefully checking these references before using
them in research work. An improvement in results was nevertheless noted between
May and July with regard to English references for themes on which ChatGPR3.5
has been particularly trained, but the situation remains unsatisfactory in
French, for example. It should also be pointed out that much of the text in
this article was generated by ChatGPT in a joint effort with the human author.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00804">Automatic detection of problem-gambling signs from online texts using large language models. (arXiv:2312.00804v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1">Elke Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Reiter_N/0/1/0/all/0/1">Nils Reiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jan Peters</a></p>
<p>Problem gambling is a major public health concern and is associated with
profound psychological distress and economic problems. There are numerous
gambling communities on the internet where users exchange information about
games, gambling tactics, as well as gambling-related problems. Individuals
exhibiting higher levels of problem gambling engage more in such communities.
Online gambling communities may provide insights into problem-gambling
behaviour. Using data scraped from a major German gambling discussion board, we
fine-tuned a large language model, specifically a Bidirectional Encoder
Representations from Transformers (BERT) model, to predict signs of
problem-gambling from forum posts. Training data were generated by manual
annotation and by taking into account diagnostic criteria and gambling-related
cognitive distortions. Using k-fold cross-validation, our models achieved a
precision of 0.95 and F1 score of 0.71, demonstrating that satisfactory
classification performance can be achieved by generating high-quality training
material through manual annotation based on diagnostic criteria. The current
study confirms that a BERT-based model can be reliably used on small data sets
and to detect signatures of problem gambling in online communication data. Such
computational approaches may have potential for the detection of changes in
problem-gambling prevalence among online users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00805">Gender inference: can chatGPT outperform common commercial tools?. (arXiv:2312.00805v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alexopoulos_M/0/1/0/all/0/1">Michelle Alexopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyons_K/0/1/0/all/0/1">Kelly Lyons</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahetaji_K/0/1/0/all/0/1">Kaushar Mahetaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1">Marcus Emmanuel Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutwillinger_R/0/1/0/all/0/1">Rogan Gutwillinger</a></p>
<p>An increasing number of studies use gender information to understand
phenomena such as gender bias, inequity in access and participation, or the
impact of the Covid pandemic response. Unfortunately, most datasets do not
include self-reported gender information, making it necessary for researchers
to infer gender from other information, such as names or names and country
information. An important limitation of these tools is that they fail to
appropriately capture the fact that gender exists on a non-binary scale,
however, it remains important to evaluate and compare how well these tools
perform in a variety of contexts. In this paper, we compare the performance of
a generative Artificial Intelligence (AI) tool ChatGPT with three commercially
available list-based and machine learning-based gender inference tools (Namsor,
Gender-API, and genderize.io) on a unique dataset. Specifically, we use a large
Olympic athlete dataset and report how variations in the input (e.g., first
name and first and last name, with and without country information) impact the
accuracy of their predictions. We report results for the full set, as well as
for the subsets: medal versus non-medal winners, athletes from the largest
English-speaking countries, and athletes from East Asia. On these sets, we find
that Namsor is the best traditional commercially available tool. However,
ChatGPT performs at least as well as Namsor and often outperforms it,
especially for the female sample when country and/or last name information is
available. All tools perform better on medalists versus non-medalists and on
names from English-speaking countries. Although not designed for this purpose,
ChatGPT may be a cost-effective tool for gender prediction. In the future, it
might even be possible for ChatGPT or other large scale language models to
better identify self-reported gender rather than report gender on a binary
scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00819">Large Language Models for Travel Behavior Prediction. (arXiv:2312.00819v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_B/0/1/0/all/0/1">Baichuan Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanyong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1">Dingyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruoyun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaotong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinhua Zhao</a></p>
<p>Travel behavior prediction is a fundamental task in transportation demand
management. The conventional methods for travel behavior prediction rely on
numerical data to construct mathematical models and calibrate model parameters
to represent human preferences. Recent advancement in large language models
(LLMs) has shown great reasoning abilities to solve complex problems. In this
study, we propose to use LLMs to predict travel behavior with prompt
engineering without data-based parameter learning. Specifically, we carefully
design our prompts that include 1) task description, 2) travel characteristics,
3) individual attributes, and 4) guides of thinking with domain knowledge, and
ask the LLMs to predict an individual's travel behavior and explain the
results. We select the travel mode choice task as a case study. Results show
that, though no training samples are provided, LLM-based predictions have
competitive accuracy and F1-score as canonical supervised learning methods such
as multinomial logit, random forest, and neural networks. LLMs can also output
reasons that support their prediction. However, though in most of the cases,
the output explanations are reasonable, we still observe cases that violate
logic or with hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00849">RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback. (arXiv:2312.00849v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tianyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoye Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Taiwen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yifeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1">Ganqu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jinyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hai-Tao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>Multimodal Large Language Models (MLLMs) have recently demonstrated
impressive capabilities in multimodal understanding, reasoning, and
interaction. However, existing MLLMs prevalently suffer from serious
hallucination problems, generating text that is not factually grounded in
associated images. The problem makes existing MLLMs untrustworthy and thus
impractical in real-world (especially high-stakes) applications. To address the
challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior
alignment from fine-grained correctional human feedback. Specifically, RLHF-V
collects human preference in the form of segment-level corrections on
hallucinations, and performs dense direct preference optimization over the
human feedback. Comprehensive experiments on five benchmarks in both automatic
and human evaluation show that, RLHF-V can enable substantially more
trustworthy MLLM behaviors with promising data and computation efficiency.
Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the
hallucination rate of the base MLLM by 34.8%, outperforming the concurrent
LLaVA-RLHF trained on 10k annotated data. The final model achieves
state-of-the-art performance in trustworthiness among open-source MLLMs, and
shows better robustness than GPT-4V in preventing hallucinations aroused from
over-generalization. We open-source our code, model, and data at
https://github.com/RLHF-V/RLHF-V.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00874">Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining. (arXiv:2312.00874v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jingcong Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1">Rong Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1">Meng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1">Ruofei Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhao Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhongyu Wei</a></p>
<p>The knowledge graph is a structure to store and represent knowledge, and
recent studies have discussed its capability to assist language models for
various applications. Some variations of knowledge graphs aim to record
arguments and their relations for computational argumentation tasks. However,
many must simplify semantic types to fit specific schemas, thus losing
flexibility and expression ability. In this paper, we propose the Hierarchical
Argumentation Graph (Hi-ArG), a new structure to organize arguments. We also
introduce two approaches to exploit Hi-ArG, including a text-graph multi-modal
model GreaseArG and a new pre-training framework augmented with graph
information. Experiments on two argumentation tasks have shown that after
further pre-training and fine-tuning, GreaseArG supersedes same-scale language
models on these tasks, while incorporating graph information during further
pre-training can also improve the performance of vanilla language models. Code
for this paper is available at https://github.com/ljcleo/Hi-ArG .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00912">Quick Back-Translation for Unsupervised Machine Translation. (arXiv:2312.00912v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brimacombe_B/0/1/0/all/0/1">Benjamin Brimacombe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiawei Zhou</a></p>
<p>The field of unsupervised machine translation has seen significant
advancement from the marriage of the Transformer and the back-translation
algorithm. The Transformer is a powerful generative model, and back-translation
leverages Transformer's high-quality translations for iterative
self-improvement. However, the Transformer is encumbered by the run-time of
autoregressive inference during back-translation, and back-translation is
limited by a lack of synthetic data efficiency. We propose a two-for-one
improvement to Transformer back-translation: Quick Back-Translation (QBT). QBT
re-purposes the encoder as a generative model, and uses encoder-generated
sequences to train the decoder in conjunction with the original autoregressive
back-translation step, improving data throughput and utilization. Experiments
on various WMT benchmarks demonstrate that a relatively small number of
refining steps of QBT improve current unsupervised machine translation models,
and that QBT dramatically outperforms standard back-translation only method in
terms of training efficiency for comparable translation qualities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00949">Hyperparameter Optimization for Large Language Model Instruction-Tuning. (arXiv:2312.00949v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tribes_C/0/1/0/all/0/1">Christophe Tribes</a>, <a href="http://arxiv.org/find/cs/1/au:+Benarroch_Lelong_S/0/1/0/all/0/1">Sacha Benarroch-Lelong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Peng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1">Ivan Kobyzev</a></p>
<p>The fine-tuning of Large Language Models (LLMs) has enabled them to recently
achieve milestones in natural language processing applications. The emergence
of ever larger LLMs has paved the way for more efficient fine-tuning methods.
Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of
the pre-trained LLM frozen while introducing a low-rank decomposition of the
weight matrix, enabling the tuning of only a very small proportion of the
network. The performance on downstream tasks of models fine-tuned with LoRA
heavily relies on a set of hyperparameters including the rank of the
decomposition. In this work, we investigate the choice of these hyperparameters
through two main blackbox optimization (BBO) techniques. We examine the whole
pipeline of performing fine-tuning and validation on a pre-trained LLM as a
blackbox and efficiently explore the space of hyperparameters with the \nomad
algorithm, achieving a boost in performance and human alignment of the tuned
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00960">The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. (arXiv:2312.00960v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Namburi_S/0/1/0/all/0/1">Satya Sai Srinath Namburi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1">Makesh Sreedhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Srinath Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a></p>
<p>Compressing large language models (LLMs), often consisting of billions of
parameters, provides faster inference, smaller memory footprints, and enables
local deployment. Two standard compression techniques are pruning and
quantization, with the former eliminating redundant connections in model layers
and the latter representing model parameters with fewer bits. The key tradeoff
is between the degree of compression and the impact on the quality of the
compressed model. Existing research on LLM compression primarily focuses on
performance in terms of general metrics like perplexity or downstream task
accuracy. More fine-grained metrics, such as those measuring parametric
knowledge, remain significantly underexplored. To help bridge this gap, we
present a comprehensive analysis across multiple model families (ENCODER,
ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order
to systematically quantify the effect of commonly employed compression
techniques on model performance. A particular focus is on tradeoffs involving
parametric knowledge, with the goal of providing practitioners with practical
insights to help make informed decisions on compression. We release our
codebase1 to enable further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00968">Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts. (arXiv:2312.00968v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jialin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1">Bo Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1">Radu Soricut</a></p>
<p>Large multi-modal models (LMMs) exhibit remarkable performance across
numerous tasks. However, generalist LMMs often suffer from performance
degradation when tuned over a large collection of tasks. Recent research
suggests that Mixture of Experts (MoE) architectures are useful for instruction
tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost
of replicating and storing the expert models severely limits the number of
experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft
MoE approach to (softly) mix many multimodal low rank experts, and avoids
introducing a significant number of new parameters compared to conventional MoE
models. The core intuition here is that the large model provides a foundational
backbone, while different lightweight experts residually learn specialized
knowledge, either per-modality or multimodally. Extensive experiments
demonstrate that the SMoLA approach helps improve the generalist performance
across a broad range of generative vision-and-language tasks, achieving new
SoTA generalist performance that often matches or outperforms single
specialized LMM baselines, as well as new SoTA specialist performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01006">Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake News Detection. (arXiv:2312.01006v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiayang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1">Tianlong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1">Liang Chang</a></p>
<p>Multi-domain fake news detection aims to identify whether various news from
different domains is real or fake and has become urgent and important. However,
existing methods are dedicated to improving the overall performance of fake
news detection, ignoring the fact that unbalanced data leads to disparate
treatment for different domains, i.e., the domain bias problem. To solve this
problem, we propose the Dual-Teacher De-biasing Distillation framework (DTDBD)
to mitigate bias across different domains. Following the knowledge distillation
methods, DTDBD adopts a teacher-student structure, where pre-trained large
teachers instruct a student model. In particular, the DTDBD consists of an
unbiased teacher and a clean teacher that jointly guide the student model in
mitigating domain bias and maintaining performance. For the unbiased teacher,
we introduce an adversarial de-biasing distillation loss to instruct the
student model in learning unbiased domain knowledge. For the clean teacher, we
design domain knowledge distillation loss, which effectively incentivizes the
student model to focus on representing domain features while maintaining
performance. Moreover, we present a momentum-based dynamic adjustment algorithm
to trade off the effects of two teachers. Extensive experiments on Chinese and
English datasets show that the proposed method substantially outperforms the
state-of-the-art baseline methods in terms of bias metrics while guaranteeing
competitive performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01032">Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models. (arXiv:2312.01032v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maity_S/0/1/0/all/0/1">Subhankar Maity</a>, <a href="http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1">Aniket Deroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1">Sudeshna Sarkar</a></p>
<p>Designing high-quality educational questions is a challenging and
time-consuming task. In this work, we propose a novel approach that utilizes
prompt-based techniques to generate descriptive and reasoning-based questions.
However, current question-answering (QA) datasets are inadequate for conducting
our experiments on prompt-based question generation (QG) in an educational
setting. Therefore, we curate a new QG dataset called EduProbe for school-level
subjects, by leveraging the rich content of NCERT textbooks. We carefully
annotate this dataset as quadruples of 1) Context: a segment upon which the
question is formed; 2) Long Prompt: a long textual cue for the question (i.e.,
a longer sequence of words or phrases, covering the main theme of the context);
3) Short Prompt: a short textual cue for the question (i.e., a condensed
representation of the key information or focus of the context); 4) Question: a
deep question that aligns with the context and is coherent with the prompts. We
investigate several prompt-based QG methods by fine-tuning pre-trained
transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and
BART. Moreover, we explore the performance of two general-purpose pre-trained
LLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training.
By performing automatic evaluation, we show that T5 (with long prompt)
outperforms all other models, but still falls short of the human baseline.
Under human evaluation criteria, TextDavinci-003 usually shows better results
than other models under various prompt settings. Even in the case of human
evaluation criteria, QG models mostly fall short of the human baseline. Our
code and dataset are available at: https://github.com/my625/PromptQG
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01037">Eliciting Latent Knowledge from Quirky Language Models. (arXiv:2312.01037v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1">Alex Mallen</a>, <a href="http://arxiv.org/find/cs/1/au:+Belrose_N/0/1/0/all/0/1">Nora Belrose</a></p>
<p>Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's
activations which robustly track the true state of the world, even when the
network's overt output is false or misleading. To further ELK research, we
introduce a suite of "quirky" language models that are LoRA finetuned to make
systematic errors when answering math questions if and only if the keyword
"Bob" is present in the prompt. We demonstrate that simple probing methods can
elicit the model's latent knowledge of the correct answer in these contexts,
even for problems harder than those the probe was trained on. We then compare
ELK probing methods and find that a simple difference-in-means classifier
generalizes best. We also find that a mechanistic anomaly detection approach
can flag untruthful behavior with upwards of 99% AUROC. Our results show
promise for eliciting superhuman knowledge from capable models, and we aim to
facilitate future research that expands on our findings, employing more diverse
and challenging datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01040">From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoyan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1">Mingyuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Sen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yicheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Cong Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wangshu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Teng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Group_Guannan_Zhang_Ant/0/1/0/all/0/1">Guannan Zhang Ant Group</a></p>
<p>Recently, large language model (LLM) based artificial intelligence (AI)
systems have demonstrated remarkable capabilities in natural language
understanding and generation. However, these models face a significant
challenge when it comes to sensitive applications, such as reasoning over
medical knowledge and answering medical questions in a physician-like manner.
Prior studies attempted to overcome this challenge by increasing the model size
(&gt;100B) to learn more general medical knowledge, while there is still room for
improvement in LLMs with smaller-scale model sizes (&lt;100B). In this work, we
start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a
medical beginner towards a medical expert (called AntGLM-Med-10B), which
leverages a 3-stage optimization procedure, \textit{i.e.}, general medical
knowledge injection, medical domain instruction tuning, and specific medical
task adaptation. Our contributions are threefold: (1) We specifically
investigate how to adapt a pre-trained general LLM in medical domain,
especially for a specific medical task. (2) We collect and construct
large-scale medical datasets for each stage of the optimization process. These
datasets encompass various data types and tasks, such as question-answering,
medical reasoning, multi-choice questions, and medical conversations. (3)
Specifically for multi-choice questions in the medical domain, we propose a
novel Verification-of-Choice approach for prompting engineering, which
significantly enhances the reasoning ability of LLMs. Remarkably, by combining
the above approaches, our AntGLM-Med-10B model can outperform the most of LLMs
on PubMedQA, including both general and medical LLMs, even when these LLMs have
larger model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01044">Large Language Models Are Zero-Shot Text Classifiers. (arXiv:2312.01044v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">Yiran Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yanbin Lin</a></p>
<p>Retrained large language models (LLMs) have become extensively used across
various sub-disciplines of natural language processing (NLP). In NLP, text
classification problems have garnered considerable focus, but still faced with
some limitations related to expensive computational cost, time consumption, and
robust performance to unseen classes. With the proposal of chain of thought
prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with
the step by step reasoning prompts, instead of conventional question and answer
formats. The zero-shot LLMs in the text classification problems can alleviate
these limitations by directly utilizing pretrained models to predict both seen
and unseen classes. Our research primarily validates the capability of GPT
models in text classification. We focus on effectively utilizing prompt
strategies to various text classification scenarios. Besides, we compare the
performance of zero shot LLMs with other state of the art text classification
methods, including traditional machine learning methods, deep learning methods,
and ZSL methods. Experimental results demonstrate that the performance of LLMs
underscores their effectiveness as zero-shot text classifiers in three of the
four datasets analyzed. The proficiency is especially advantageous for small
businesses or teams that may not have extensive knowledge in text
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01050">Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities. (arXiv:2312.01050v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oryngozha_N/0/1/0/all/0/1">Nazzere Oryngozha</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamoi_P/0/1/0/all/0/1">Pakizar Shamoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Igali_A/0/1/0/all/0/1">Ayan Igali</a></p>
<p>Nowadays, the significance of monitoring stress levels and recognizing early
signs of mental illness cannot be overstated. Automatic stress detection in
text can proactively help manage stress and protect mental well-being. In
today's digital era, social media platforms reflect the psychological
well-being and stress levels within various communities. This study focuses on
detecting and analyzing stress-related posts in Reddit academic communities.
Due to online education and remote work, these communities have become central
for academic discussions and support. We classify text as stressed or not using
natural language processing and machine learning classifiers, with Dreaddit as
our training dataset, which contains labeled data from Reddit. Next, we collect
and analyze posts from various academic subreddits. We identified that the most
effective individual feature for stress detection is the Bag of Words, paired
with the Logistic Regression classifier, achieving a 77.78% accuracy rate and
an F1 score of 0.79 on the DReaddit dataset. This combination also performs
best in stress detection on human-annotated datasets, with a 72% accuracy rate.
Our key findings reveal that posts and comments in professors Reddit
communities are the most stressful, compared to other academic levels,
including bachelor, graduate, and Ph.D. students. This research contributes to
our understanding of the stress levels within academic communities. It can help
academic institutions and online communities develop measures and interventions
to address this issue effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01052">Structured, Complex and Time-complete Temporal Event Forecasting. (arXiv:2312.01052v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunshan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Chenchen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zijian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>Temporal event forecasting aims to predict what will happen next given the
observed events in history. Previous formulations of temporal event are
unstructured, atomic, or lacking full temporal information, thus largely
restricting the representation quality and forecasting ability of temporal
events. To address these limitations, we introduce a novel formulation for
Structured, Complex, and Time-complete Temporal Event (SCTc-TE). Based on this
new formulation, we develop a simple and fully automated pipeline for
constructing such SCTc-TEs from a large amount of news articles. Furthermore,
we propose a novel model that leverages both Local and Global contexts for
SCTc-TE forecasting, named LoGo. To evaluate our model, we construct two
large-scale datasets named MidEast-TE and GDELT-TE. Extensive evaluations
demonstrate the advantages of our datasets in multiple aspects, while
experimental results justify the effectiveness of our forecasting model LoGo.
We release the code and dataset via
https://github.com/yecchen/GDELT-ComplexEvent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.02180">Pareto Probing: Trading Off Accuracy for Complexity. (arXiv:2010.02180v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1">Tiago Pimentel</a>, <a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1">Naomi Saphra</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>The question of how to probe contextual word representations for linguistic
structure in a way that is both principled and useful has seen significant
attention recently in the NLP literature. In our contribution to this
discussion, we argue for a probe metric that reflects the fundamental trade-off
between probe complexity and performance: the Pareto hypervolume. To measure
complexity, we present a number of parametric and non-parametric metrics. Our
experiments using Pareto hypervolume as an evaluation metric show that probes
often do not conform to our expectations -- e.g., why should the non-contextual
fastText representations encode more morpho-syntactic information than the
contextual BERT representations? These results suggest that common, simplistic
probing tasks, such as part-of-speech labeling and dependency arc labeling, are
inadequate to evaluate the linguistic structure encoded in contextual word
representations. This leads us to propose full dependency parsing as a probing
task. In support of our suggestion that harder probing tasks are necessary, our
experiments with dependency parsing reveal a wide gap in syntactic knowledge
between contextual and non-contextual representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.08063">Information Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yubo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a></p>
<p>Information Extraction (IE) seeks to derive structured information from
unstructured texts, often facing challenges in low-resource scenarios due to
data scarcity and unseen classes. This paper presents a review of neural
approaches to low-resource IE from \emph{traditional} and \emph{LLM-based}
perspectives, systematically categorizing them into a fine-grained taxonomy.
Then we conduct empirical study on LLM-based methods compared with previous
state-of-the-art models, and discover that (1) well-tuned LMs are still
predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising
in general; (3) the optimal LLM-based technical solution for low-resource IE
can be task-dependent. In addition, we discuss low-resource IE with LLMs,
highlight promising applications, and outline potential research directions.
This survey aims to foster understanding of this field, inspire new ideas, and
encourage widespread applications in both academia and industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14986">The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs. (arXiv:2210.14986v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruis_L/0/1/0/all/0/1">Laura Ruis</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Akbir Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1">Stella Biderman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1">Sara Hooker</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a></p>
<p>Despite widespread use of LLMs as conversational agents, evaluations of
performance fail to capture a crucial aspect of communication: interpreting
language in context -- incorporating its pragmatics. Humans interpret language
using beliefs and prior knowledge about the world. For example, we intuitively
understand the response "I wore gloves" to the question "Did you leave
fingerprints?" as meaning "No". To investigate whether LLMs have the ability to
make this type of inference, known as an implicature, we design a simple task
and evaluate four categories of widely used state-of-the-art models. We find
that, despite only evaluating on utterances that require a binary inference
(yes or no), models in three of these categories perform close to random.
However, LLMs instruction-tuned at the example-level perform significantly
better. These results suggest that certain fine-tuning strategies are far
better at inducing pragmatic understanding in models. We present our findings
as the starting point for further research into evaluating how LLMs interpret
language in context and to drive the development of more pragmatic and useful
models of human discourse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08412">Evaluating the Factual Consistency of Large Language Models Through News Summarization. (arXiv:2211.08412v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1">Derek Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascarenhas_A/0/1/0/all/0/1">Anisha Mascarenhas</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwan_S/0/1/0/all/0/1">Sarah Kwan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a></p>
<p>While large language models (LLMs) have proven to be effective on a large
variety of tasks, they are also known to hallucinate information. To measure
whether an LLM prefers factually consistent continuations of its input, we
propose a new benchmark called FIB(Factual Inconsistency Benchmark) that
focuses on the task of summarization. Specifically, our benchmark involves
comparing the scores an LLM assigns to a factually consistent versus a
factually inconsistent summary for an input news article. For factually
consistent summaries, we use human-written reference summaries that we manually
verify as factually consistent. To generate summaries that are factually
inconsistent, we generate summaries from a suite of summarization models that
we have manually annotated as factually inconsistent. A model's factual
consistency is then measured according to its accuracy, i.e.\ the proportion of
documents where it assigns a higher score to the factually consistent summary.
To validate the usefulness of FIB, we evaluate 23 large language models ranging
from 1B to 176B parameters from six different model families including BLOOM
and OPT. We find that existing LLMs generally assign a higher score to
factually consistent summaries than to factually inconsistent summaries.
However, if the factually inconsistent summaries occur verbatim in the
document, then LLMs assign a higher score to these factually inconsistent
summaries than factually consistent summaries. We validate design choices in
our benchmark including the scoring method and source of distractor summaries.
Our code and benchmark data can be found at https://github.com/r-three/fib.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09512">Rethinking Label Smoothing on Multi-hop Question Answering. (arXiv:2212.09512v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiannian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yiguang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhao Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Multi-Hop Question Answering (MHQA) is a significant area in question
answering, requiring multiple reasoning components, including document
retrieval, supporting sentence prediction, and answer span extraction. In this
work, we analyze the primary factors limiting the performance of multi-hop
reasoning and introduce label smoothing into the MHQA task. This is aimed at
enhancing the generalization capabilities of MHQA systems and mitigating
overfitting of answer spans and reasoning paths in training set. We propose a
novel label smoothing technique, F1 Smoothing, which incorporates uncertainty
into the learning process and is specifically tailored for Machine Reading
Comprehension (MRC) tasks. Inspired by the principles of curriculum learning,
we introduce the Linear Decay Label Smoothing Algorithm (LDLA), which
progressively reduces uncertainty throughout the training process. Experiment
on the HotpotQA dataset demonstrates the effectiveness of our methods in
enhancing performance and generalizability in multi-hop reasoning, achieving
new state-of-the-art results on the leaderboard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09928">Improving the Robustness of Summarization Models by Detecting and Removing Input Noise. (arXiv:2212.09928v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kundan Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1">Balaji Lakshminarayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiaming Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1">Mohammad Saleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a></p>
<p>The evaluation of abstractive summarization models typically uses test data
that is identically distributed as training data. In real-world practice,
documents to be summarized may contain input noise caused by text extraction
artifacts or data pipeline bugs. The robustness of model performance under
distribution shift caused by such noise is relatively under-studied. We present
a large empirical study quantifying the sometimes severe loss in performance
(up to 12 ROUGE-1 points) from different types of input noise for a range of
datasets and model sizes. We then propose a light-weight method for detecting
and removing such noise in the input during model inference without requiring
any extra training, auxiliary models, or even prior knowledge of the type of
noise. Our proposed approach effectively mitigates the loss in performance,
recovering a large fraction of the performance drop, sometimes as large as 11
ROUGE-1 points.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10789">Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weili Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1">Zhuoran Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Ling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>There is increasing adoption of artificial intelligence in drug discovery.
However, existing studies use machine learning to mainly utilize the chemical
structures of molecules but ignore the vast textual knowledge available in
chemistry. Incorporating textual knowledge enables us to realize new drug
design objectives, adapt to text-based instructions and predict complex
biological activities. Here we present a multi-modal molecule structure-text
model, MoleculeSTM, by jointly learning molecules' chemical structures and
textual descriptions via a contrastive learning strategy. To train MoleculeSTM,
we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000
chemical structure-text pairs. To demonstrate the effectiveness and utility of
MoleculeSTM, we design two challenging zero-shot tasks based on text
instructions, including structure-text retrieval and molecule editing.
MoleculeSTM has two main properties: open vocabulary and compositionality via
natural language. In experiments, MoleculeSTM obtains the state-of-the-art
generalization ability to novel biochemical concepts across various benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04729">Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1">Ali Naseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kalpesh Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1">Mohit Iyyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1">Amir Houmansadr</a></p>
<p>A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We
demonstrate the feasibility of stealing such information with only a few
dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of
GPT-3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17580">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (arXiv:2303.17580v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yongliang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaitao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weiming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01196">Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Canwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daya Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1">Julian McAuley</a></p>
<p>Chat models, such as ChatGPT, have shown impressive capabilities and have
been rapidly adopted across numerous domains. However, these models are only
accessible through a restricted API, creating barriers for new research and
progress in the field. We propose a pipeline that can automatically generate a
high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a
conversation with itself. Subsequently, we employ parameter-efficient tuning to
enhance LLaMA, an open-source large language model. The resulting model, named
Baize, demonstrates good performance in multi-turn dialogues with guardrails
that minimize potential risks. Furthermore, we propose a new technique called
Self-Distill with Feedback, to further improve the performance of the Baize
models with feedback from ChatGPT. The Baize models and data are released for
research purposes only at https://github.com/project-baize/baize-chatbot. An
online demo is also available at
https://huggingface.co/spaces/project-baize/chat-with-baize.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10513">Why Does ChatGPT Fall Short in Providing Truthful Answers?. (arXiv:2304.10513v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kevin Chen-Chuan Chang</a></p>
<p>Recent advancements in large language models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in providing reliable and accurate
answers to user questions. To better understand the model's particular
weaknesses in providing truthful answers, we embark an in-depth exploration of
open-domain question answering. Specifically, we undertake a detailed
examination of ChatGPT's failures, categorized into: comprehension, factuality,
specificity, and inference. We further pinpoint factuality as the most
contributing failure and identify two critical abilities associated with
factuality: knowledge memorization and knowledge recall. Through experiments
focusing on factuality, we propose several potential enhancement strategies.
Our findings suggest that augmenting the model with granular external knowledge
and cues for knowledge recall can enhance the model's factuality in answering
questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03047">Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhiqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yikang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qinhong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenfang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1">David Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a></p>
<p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04782">HistAlign: Improving Context Dependency in Language Generation by Aligning with History. (arXiv:2305.04782v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1">David Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Language models (LMs) can generate hallucinations and incoherent outputs,
which highlights their weak context dependency. Cache-LMs, which augment LMs
with a memory of recent history, can increase context dependency and have shown
remarkable performance in diverse language generation tasks. However, we find
that even with training, the performance gain stemming from the cache component
of current cache-LMs is suboptimal due to the misalignment between the current
hidden states and those stored in the memory. In this work, we present
HistAlign, a new training approach to ensure good cache alignment such that the
model receives useful signals from the history. We first prove our concept on a
simple and synthetic task where the memory is essential for correct
predictions, and we show that the cache component of HistAlign is better
aligned and improves overall performance. Next, we evaluate HistAlign on
diverse downstream language generation tasks, including prompt continuation,
abstractive summarization, and data-to-text. We demonstrate that HistAlign
improves text coherence and faithfulness in open-ended and conditional
generation settings respectively. HistAlign is also generalizable across
different model families, showcasing its strength in improving context
dependency of LMs in diverse scenarios. Our code is publicly available at
https://github.com/meetdavidwan/histalign
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Shunyu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jeffrey Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1">Izhak Shafran</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a></p>
<p>Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/princeton-nlp/tree-of-thought-llm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11550">Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1">David Stap</a>, <a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1">Vlad Niculae</a>, <a href="http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1">Christof Monz</a></p>
<p>We argue that translation quality alone is not a sufficient metric for
measuring knowledge transfer in multilingual neural machine translation. To
support this claim, we introduce Representational Transfer Potential (RTP),
which measures representational similarities between languages. We show that
RTP can measure both positive and negative transfer (interference), and find
that RTP is strongly correlated with changes in translation quality, indicating
that transfer does occur. Furthermore, we investigate data and language
characteristics that are relevant for transfer, and find that multi-parallel
overlap is an important yet under-explored feature. Based on this, we develop a
novel training scheme, which uses an auxiliary similarity loss that encourages
representations to be more invariant across languages by taking advantage of
multi-parallel data. We show that our method yields increased translation
quality for low- and mid-resource languages across multiple data and model
setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13256">TaskWeb: Selecting Better Source Tasks for Multi-task NLP. (arXiv:2305.13256v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joongwon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1">Akari Asai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1">Gabriel Ilharco</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a></p>
<p>Recent work in NLP has shown promising results in training models on large
amounts of tasks to achieve better generalization. However, it is not
well-understood how tasks are related, and how helpful training tasks can be
chosen for a new task. In this work, we investigate whether knowing task
relationships via pairwise task transfer improves choosing one or more source
tasks that help to learn a new target task. We provide TaskWeb, a large-scale
benchmark of pairwise task transfers for 22 NLP tasks using three different
model types, sizes, and adaptation methods, spanning about 25,000 experiments.
Then, we design a new method TaskShop based on our analysis of TaskWeb.
TaskShop uses TaskWeb to estimate the benefit of using a source task for
learning a new target task, and to choose a subset of helpful training tasks
for multi-task training. Our method improves overall rankings and top-k
precision of source tasks by 10% and 38%, respectively. We also use TaskShop to
build much smaller multi-task training sets that improve zero-shot performances
across 11 different target tasks by at least 4.3%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13269">Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Ruochen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1">Yew Ken Chia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1">Bosheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>, <a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1">Soujanya Poria</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1">Lidong Bing</a></p>
<p>We present chain-of-knowledge (CoK), a novel framework that augments large
language models (LLMs) by dynamically incorporating grounding information from
heterogeneous sources. It results in more factual rationales and reduced
hallucination in generation. Specifically, CoK consists of three stages:
reasoning preparation, dynamic knowledge adapting, and answer consolidation.
Given a knowledge-intensive question, CoK first prepares several preliminary
rationales and answers while identifying the relevant knowledge domains. If
there is no majority consensus among the answers from samples, CoK corrects the
rationales step by step by adapting knowledge from the identified domains.
These corrected rationales can plausibly serve as a better foundation for the
final answer consolidation. Unlike prior studies that primarily use
unstructured data, CoK also leverages structured knowledge sources such as
Wikidata and tables that provide more reliable factual information. To access
both unstructured and structured knowledge sources in the dynamic knowledge
adapting stage, we propose an adaptive query generator that allows the
generation of queries for various types of query languages, including SPARQL,
SQL, and natural sentences. Moreover, to minimize error propagation between
rationales, CoK corrects the rationales progressively using preceding corrected
rationales to generate and correct subsequent rationales. Extensive experiments
show that CoK consistently improves the performance of LLMs on
knowledge-intensive tasks across different domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14296">USB: A Unified Summarization Benchmark Across Tasks and Domains. (arXiv:2305.14296v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kundan Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Prakhar Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramprasad_S/0/1/0/all/0/1">Sanjana Ramprasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1">Byron C. Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1">Jeffrey P. Bigham</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a></p>
<p>While the NLP community has produced numerous summarization benchmarks, none
provide the rich annotations required to simultaneously address many important
problems related to control and reliability. We introduce a Wikipedia-derived
benchmark, complemented by a rich set of crowd-sourced annotations, that
supports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractive
summarization; (iii) topic-based summarization; (iv) compressing selected
sentences into a one-line summary; (v) surfacing evidence for a summary
sentence; (vi) predicting the factual accuracy of a summary sentence; (vii)
identifying unsubstantiated spans in a summary sentence; (viii) correcting
factual errors in summaries. We compare various methods on this benchmark and
discover that on multiple tasks, moderately-sized fine-tuned models
consistently outperform much larger few-shot prompted language models. For
factuality-related tasks, we also evaluate existing heuristics to create
training data and find that training on them results in worse performance than
training on $20\times$ less human-labeled data. Our articles draw from $6$
domains, facilitating cross-domain analysis. On some tasks, the amount of
training data matters more than the domain where it comes from, while for other
tasks training specifically on data from the target domain, even if limited, is
more beneficial.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14735">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1">Vyoma Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>The impact of AI models on marginalized communities has traditionally been
measured by identifying performance differences between specified demographic
subgroups. Though this approach aims to center vulnerable groups, it risks
obscuring patterns of harm faced by intersectional subgroups or shared across
multiple groups. To address this, we draw on theories of marginalization from
disability studies and related disciplines, which state that people farther
from the norm face greater adversity, to consider the "margins" in the domain
of toxicity detection. We operationalize the "margins" of a dataset by
employing outlier detection to identify text about people with demographic
attributes distant from the "norm". We find that model performance is
consistently worse for demographic outliers, with mean squared error (MSE)
between outliers and non-outliers up to 70.4% worse across toxicity types. It
is also worse for text outliers, with a MSE up to 68.4% higher for outliers
than non-outliers. We also find text and demographic outliers to be
particularly susceptible to errors in the classification of severe toxicity and
identity attacks. Compared to analysis of disparities using traditional
demographic breakdowns, we find that our outlier analysis frequently surfaces
greater harms faced by a larger, more intersectional group, which suggests that
outlier analysis is particularly beneficial for identifying harms against those
groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17691">Plug-and-Play Knowledge Injection for Pre-trained Language Models. (arXiv:2305.17691v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhiyuan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huadong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1">Deming Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaojun Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Injecting external knowledge can improve the performance of pre-trained
language models (PLMs) on various downstream NLP tasks. However, massive
retraining is required to deploy new knowledge injection methods or knowledge
bases for downstream tasks. In this work, we are the first to study how to
improve the flexibility and efficiency of knowledge injection by reusing
existing downstream models. To this end, we explore a new paradigm
plug-and-play knowledge injection, where knowledge bases are injected into
frozen existing downstream models by a knowledge plugin. Correspondingly, we
propose a plug-and-play injection method map-tuning, which trains a mapping of
knowledge embeddings to enrich model inputs with mapped embeddings while
keeping model parameters frozen. Experimental results on three knowledge-driven
NLP tasks show that existing injection methods are not suitable for the new
paradigm, while map-tuning effectively improves the performance of downstream
models. Moreover, we show that a frozen downstream model can be well adapted to
different domains with different mapping networks of domain knowledge. Our code
and models are available at https://github.com/THUNLP/Knowledge-Plugin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01242">Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhizheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wenxuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a></p>
<p>The recent success of Large Language Models (LLMs) signifies an impressive
stride towards artificial general intelligence. They have shown a promising
prospect in automatically completing tasks upon user instructions, functioning
as brain-like coordinators. The associated risks will be revealed as we
delegate an increasing number of tasks to machines for automated completion. A
big question emerges: how can we make machines behave responsibly when helping
humans automate tasks as personal copilots? In this paper, we explore this
question in depth from the perspectives of feasibility, completeness and
security. In specific, we present Responsible Task Automation (ResponsibleTA)
as a fundamental framework to facilitate responsible collaboration between
LLM-based coordinators and executors for task automation with three empowered
capabilities: 1) predicting the feasibility of the commands for executors; 2)
verifying the completeness of executors; 3) enhancing the security (e.g., the
protection of users' privacy). We further propose and compare two paradigms for
implementing the first two capabilities. One is to leverage the generic
knowledge of LLMs themselves via prompt engineering while the other is to adopt
domain-specific learnable models. Moreover, we introduce a local memory
mechanism for achieving the third capability. We evaluate our proposed
ResponsibleTA on UI task automation and hope it could bring more attentions to
ensuring LLMs more responsible in diverse scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03997">Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rizinski_M/0/1/0/all/0/1">Maryan Rizinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Peshov_H/0/1/0/all/0/1">Hristijan Peshov</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishev_K/0/1/0/all/0/1">Kostadin Mishev</a>, <a href="http://arxiv.org/find/cs/1/au:+Jovanovik_M/0/1/0/all/0/1">Milos Jovanovik</a>, <a href="http://arxiv.org/find/cs/1/au:+Trajanov_D/0/1/0/all/0/1">Dimitar Trajanov</a></p>
<p>Lexicon-based sentiment analysis (SA) in finance leverages specialized,
manually annotated lexicons created by human experts to extract sentiment from
financial texts. Although lexicon-based methods are simple to implement and
fast to operate on textual data, they require considerable manual annotation
efforts to create, maintain, and update the lexicons. These methods are also
considered inferior to the deep learning-based approaches, such as transformer
models, which have become dominant in various NLP tasks due to their remarkable
performance. However, transformers require extensive data and computational
resources for both training and testing. Additionally, they involve significant
prediction times, making them unsuitable for real-time production environments
or systems with limited processing capabilities. In this paper, we introduce a
novel methodology named eXplainable Lexicons (XLex) that combines the
advantages of both lexicon-based methods and transformer models. We propose an
approach that utilizes transformers and SHapley Additive exPlanations (SHAP)
for explainability to learn financial lexicons. Our study presents four main
contributions. Firstly, we demonstrate that transformer-aided explainable
lexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald
(LM) lexicon, reducing the human involvement in annotating, maintaining, and
updating the lexicons. Secondly, we show that the resulting lexicon outperforms
the standard LM lexicon in SA of financial datasets. Thirdly, we illustrate
that the lexicon-based approach is significantly more efficient in terms of
model speed and size compared to transformers. Lastly, the XLex approach is
inherently more interpretable than transformer models as lexicon models rely on
predefined rules, allowing for better insights into the results of SA and
making the XLex approach a viable tool for financial decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07848">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Accurate Speech Emotion Recognition. (arXiv:2306.07848v10 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yanni Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuguang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1">Wen Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jixun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Heng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jianjun Zhao</a></p>
<p>Contrastive cross-modality pretraining has recently exhibited impressive
success in diverse fields, whereas there is limited research on their merits in
speech emotion recognition (SER). In this paper, we propose GEmo-CLAP, a kind
of gender-attribute-enhanced contrastive language-audio pretraining (CLAP)
method for SER. Specifically, we first construct an effective emotion CLAP
(Emo-CLAP) for SER, using pre-trained text and audio encoders. Second, given
the significance of gender information in SER, two novel multi-task learning
based GEmo-CLAP (ML-GEmo-CLAP) and soft label based GEmo-CLAP (SL-GEmo-CLAP)
models are further proposed to incorporate gender information of speech
signals, forming more reasonable objectives. Experiments on IEMOCAP indicate
that our proposed two GEmo-CLAPs consistently outperform Emo-CLAP with
different pre-trained models. Remarkably, the proposed WavLM-based SL-GEmo-CLAP
obtains the best WAR of 83.16\%, which performs better than state-of-the-art
SER methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11300">RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zilun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tiancheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jianwei Yin</a></p>
<p>Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12509">Joint Prompt Optimization of Stacked LLMs using Variational Inference. (arXiv:2306.12509v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1">Alessandro Sordoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xingdi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1">Matheus Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1">Adam Trischler</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Ziang Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1">Arian Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Niedtner_F/0/1/0/all/0/1">Friederike Niedtner</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1">Nicolas Le Roux</a></p>
<p>Large language models (LLMs) can be seen as atomic units of computation
mapping sequences to a distribution over sequences. Thus, they can be seen as
stochastic language layers in a language network, where the learnable
parameters are the natural language prompts at each layer. By stacking two such
layers and feeding the output of one layer to the next, we obtain a Deep
Language Network (DLN). We first show how to effectively perform prompt
optimization for a 1-Layer language network (DLN-1). Then, we present an
extension that applies to 2-layer DLNs (DLN-2), where two prompts must be
learned. The key idea is to consider the output of the first layer as a latent
variable, which requires inference, and prompts to be learned as the parameters
of the generative distribution. We first test the effectiveness of DLN-1 in
multiple reasoning and natural language understanding tasks. Then, we show that
DLN-2 can reach higher performance than a single layer, showing promise that we
might reach comparable performance to GPT-4, even when each LLM in the network
is smaller and less powerful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01987">Bengali Fake Reviews: A Benchmark Dataset and Detection System. (arXiv:2308.01987v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1">G. M. Shahariar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1">Md. Tanvir Rouf Shawon</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1">Faisal Muhammad Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Mohammad Shafiul Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1">Md. Shahriar Mahbub</a></p>
<p>The proliferation of fake reviews on various online platforms has created a
major concern for both consumers and businesses. Such reviews can deceive
customers and cause damage to the reputation of products or services, making it
crucial to identify them. Although the detection of fake reviews has been
extensively studied in English language, detecting fake reviews in non-English
languages such as Bengali is still a relatively unexplored research area. This
paper introduces the Bengali Fake Review Detection (BFRD) dataset, the first
publicly available dataset for identifying fake reviews in Bengali. The dataset
consists of 7710 non-fake and 1339 fake food-related reviews collected from
social media posts. To convert non-Bengali words in a review, a unique pipeline
has been proposed that translates English words to their corresponding Bengali
meaning and also back transliterates Romanized Bengali to Bengali. We have
conducted rigorous experimentation using multiple deep learning and pre-trained
transformer language models to develop a reliable detection system. Finally, we
propose a weighted ensemble model that combines four pre-trained transformers:
BanglaBERT, BanglaBERT Base, BanglaBERT Large, and BanglaBERT Generator .
According to the experiment results, the proposed ensemble model obtained a
weighted F1-score of 0.9843 on 13390 reviews, including 1339 actual fake
reviews and 5356 augmented fake reviews generated with the nlpaug library. The
remaining 6695 reviews were randomly selected from the 7710 non-fake instances.
The model achieved a 0.9558 weighted F1-score when the fake reviews were
augmented using the bnaug library.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05361">WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1">Siqiao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Fan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1">Hongyan Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1">Qingyang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Caigao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hongyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shuo Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jianshan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">James Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hongyuan Mei</a></p>
<p>We present WeaverBird, an intelligent dialogue system designed specifically
for the finance domain. Our system harnesses a large language model of GPT
architecture that has been tuned using extensive corpora of finance-related
text. As a result, our system possesses the capability to understand complex
financial queries, such as "How should I manage my investments during
inflation?", and provide informed responses. Furthermore, our system
incorporates a local knowledge base and a search engine to retrieve relevant
information. The final responses are conditioned on the search results and
include proper citations to the sources, thus enjoying an enhanced credibility.
Through a range of finance-related questions, we have demonstrated the superior
performance of our system compared to other models. To experience our system
firsthand, users can interact with our live demo at
https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at
https://www.youtube.com/watch?v=fyV2qQkX6Tc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06744">Token-Scaled Logit Distillation for Ternary Weight Generative Language Models. (arXiv:2308.06744v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sihwa Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Janghwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Sukjin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1">Du-Seong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1">Wonyong Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jungwook Choi</a></p>
<p>Generative Language Models (GLMs) have shown impressive performance in tasks
such as text generation, understanding, and reasoning. However, the large model
size poses challenges for practical deployment. To solve this problem,
Quantization-Aware Training (QAT) has become increasingly popular. However,
current QAT methods for generative models have resulted in a noticeable loss of
accuracy. To counteract this issue, we propose a novel knowledge distillation
method specifically designed for GLMs. Our method, called token-scaled logit
distillation, prevents overfitting and provides superior learning from the
teacher model and ground truth. This research marks the first evaluation of
ternary weight quantization-aware training of large-scale GLMs with less than
1.0 degradation in perplexity and achieves enhanced accuracy in tasks like
common-sense QA and arithmetic reasoning as well as natural language
understanding. Our code is available at https://github.com/aiha-lab/TSLD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11138">NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gao_P/0/1/0/all/0/1">Peiheng Gao</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_N/0/1/0/all/0/1">Ning Sun</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1">Xuefeng Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1">Chen Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zitikis_R/0/1/0/all/0/1">Ri&#x10d;ardas Zitikis</a></p>
<p>We develop an NLP-based procedure for detecting systematic nonmeritorious
consumer complaints, simply called systematic anomalies, among complaint
narratives. While classification algorithms are used to detect pronounced
anomalies, in the case of smaller and frequent systematic anomalies, the
algorithms may falter due to a variety of reasons, including technical ones as
well as natural limitations of human analysts. Therefore, as the next step
after classification, we convert the complaint narratives into quantitative
data, which are then analyzed using an algorithm for detecting systematic
anomalies. We illustrate the entire procedure using complaint narratives from
the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15452">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1">Zhen Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yinuo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guozhou Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16458">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1">Bill Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Rick Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiakang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a></p>
<p>Pre-trained large language models have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks and to be appropriately specialized to
particular domains. Here, we target bioinformatics due to the amount of
specialized domain knowledge, algorithms, and data operations this discipline
requires. We present BioCoder, a benchmark developed to evaluate large language
models (LLMs) in generating bioinformatics-specific code. BioCoder spans a
broad spectrum of the field and covers cross-file dependencies, class
declarations, and global variables. It incorporates 1026 Python functions and
1243 Java methods extracted from GitHub, along with 253 examples from the
Rosalind Project, all pertaining to bioinformatics. Using topic modeling we
show that overall coverage of the included code is representative of the full
spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing
framework for evaluation. We have applied it to evaluate many models including
InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+,
GPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our
dataset can effectively enhance the performance of LLMs on our benchmark (by
&gt;15% in terms of Pass@K in certain prompt configurations and always &gt;3%). The
results highlight two key aspects of successful models: (1) Successful models
accommodate a long prompt (&gt; ~2600 tokens) with full context, for functional
dependencies. (2) They contain specific domain knowledge of bioinformatics,
beyond just general coding knowledge. This is evident from the performance gain
of GPT-3.5/4 compared to the smaller models on the benchmark (50% vs up to
~25%). Our dataset, benchmark, Docker images, and scripts required for testing
are all available at https://github.com/gersteinlab/biocoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03886">FIND: A Function Description Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1">Sarah Schwettmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1">Tamar Rott Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1">Joanna Materzynska</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1">Neil Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1">David Bau</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a></p>
<p>Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions span textual and numeric
domains, and involve a range of real-world complexities. We evaluate methods
that use pretrained language models (LMs) to produce descriptions of function
behavior in natural language and code. Additionally, we introduce a new
interactive method in which an Automated Interpretability Agent (AIA) generates
function descriptions. We find that an AIA, built from an LM with black-box
access to functions, can infer function structure, acting as a scientist by
forming hypotheses, proposing experiments, and updating descriptions in light
of new data. However, AIA descriptions tend to capture global function behavior
and miss local details. These results suggest that FIND will be useful for
evaluating more sophisticated interpretability methods before they are applied
to real-world models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06550">Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1">Natraj Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Sameena Shah</a></p>
<p>Generating synthetic variants of a document is often posed as text-to-text
transformation. We propose an alternate LLM based method that first decomposes
a document into semantic frames and then generates text using this interim
sparse format. The frames are modeled using a hypergraph, which allows
perturbing the frame contents in a principled manner. Specifically, new
hyperedges are mined through topological analysis and complex polyadic
relationships including hierarchy and temporal dynamics are accommodated. We
show that our solution generates documents that are diverse, coherent and vary
in style, sentiment, format, composition and facts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09126">How much can ChatGPT really help Computational Biologists in Programming?. (arXiv:2309.09126v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_C/0/1/0/all/0/1">Chowdhury Rafeed Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1">Limsoon Wong</a></p>
<p>ChatGPT, a recently developed product by openAI, is successfully leaving its
mark as a multi-purpose natural language based chatbot. In this paper, we are
more interested in analyzing its potential in the field of computational
biology. A major share of work done by computational biologists these days
involve coding up bioinformatics algorithms, analyzing data, creating
pipelining scripts and even machine learning modeling and feature extraction.
This paper focuses on the potential influence (both positive and negative) of
ChatGPT in the mentioned aspects with illustrative examples from different
perspectives. Compared to other fields of computer science, computational
biology has - (1) less coding resources, (2) more sensitivity and bias issues
(deals with medical data) and (3) more necessity of coding assistance (people
from diverse background come to this field). Keeping such issues in mind, we
cover use cases such as code writing, reviewing, debugging, converting,
refactoring and pipelining using ChatGPT from the perspective of computational
biologists in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16583">GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1">Chenguang Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Pengyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kevin Chen-Chuan Chang</a></p>
<p>With the rapid advancement of large language models (LLMs), there is a
pressing need for a comprehensive evaluation suite to assess their capabilities
and limitations. Existing LLM leaderboards often reference scores reported in
other papers without consistent settings and prompts, which may inadvertently
encourage cherry-picking favored settings and prompts for better results. In
this work, we introduce GPT-Fathom, an open-source and reproducible LLM
evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+
leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across
7 capability categories, all under aligned settings. Our retrospective study on
OpenAI's earlier models offers valuable insights into the evolutionary path
from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3
progressively improves to GPT-4, including technical details like whether
adding code data improves LLM's reasoning capability, which aspects of LLM
capability can be improved by SFT and RLHF, how much is the alignment tax, etc.
Our analysis sheds light on many of these questions, aiming to improve the
transparency of advanced LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00996">ARN: A Comprehensive Framework and Benchmark for Analogical Reasoning on Narratives. (arXiv:2310.00996v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1">Zhivar Sourati</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1">Filip Ilievski</a>, <a href="http://arxiv.org/find/cs/1/au:+Sommerauer_P/0/1/0/all/0/1">Pia Sommerauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifan Jiang</a></p>
<p>Analogical reasoning is one of the prime abilities of humans and is linked to
creativity and scientific discoveries. This ability has been studied
extensively in natural language processing (NLP) and in cognitive psychology.
NLP benchmarks often focus on proportional analogies, while the ones in
cognitive psychology investigate longer pieces of text too. Yet, although
studies that focus on analogical reasoning in an involved setting utilize
narratives as their evaluation medium, analogical reasoning on narratives has
not been studied extensively. We create an extensive evaluation framework for
analogical reasoning on narratives that utilizes narrative elements to create
lower-order and higher-order mappings that subsequently lead to the development
of the Analogical Reasoning on Narratives (ARN) benchmark that covers four
categories of far(cross-domain)/near(within-domain) analogies and far/near
disanalogies, allowing us to study analogical reasoning in LLMs in distinct
scenarios. Our results demonstrate that LLMs struggle to recognize higher-order
mappings when they are not accompanied by lower-order mappings (far analogies)
and show better performance when all mappings are formed simultaneously (near
analogies). We observe that in all the scenarios, the analogical reasoning
abilities of LLMs can be easily impaired by lower-order mappings in near
disanalogies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02980">Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amos_I/0/1/0/all/0/1">Ido Amos</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a></p>
<p>Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09219">&quot;Kelly is a Warm Person, Joseph is a Role Model&quot;: Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yixin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">George Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1">Aparna Garimella</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Large Language Models (LLMs) have recently emerged as an effective tool to
assist individuals in writing various types of content, including professional
documents such as recommendation letters. Though bringing convenience, this
application also introduces unprecedented fairness concerns. Model-generated
reference letters might be directly used by users in professional scenarios. If
underlying biases exist in these model-constructed letters, using them without
scrutinization could lead to direct societal harms, such as sabotaging
application success rates for female applicants. In light of this pressing
issue, it is imminent and necessary to comprehensively study fairness issues
and associated harms in this real-world use case. In this paper, we critically
examine gender biases in LLM-generated reference letters. Drawing inspiration
from social science findings, we design evaluation methods to manifest biases
through 2 dimensions: (1) biases in language style and (2) biases in lexical
content. We further investigate the extent of bias propagation by analyzing the
hallucination bias of models, a term that we define to be bias exacerbation in
model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-
ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated
recommendation letters. Our findings not only warn against using LLMs for this
application without scrutinization, but also illuminate the importance of
thoroughly studying hidden biases and harms in LLM-generated professional
documents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09909">Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chaoyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jiayu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qiaoyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weike Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weixiong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoman Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Ziheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a></p>
<p>Driven by the large foundation models, the development of artificial
intelligence has witnessed tremendous progress lately, leading to a surge of
general interest from the public. In this study, we aim to assess the
performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm
of multimodal medical diagnosis. Our evaluation encompasses 17 human body
systems, including Central Nervous System, Head and Neck, Cardiac, Chest,
Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,
Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,
Pediatrics, with images taken from 8 modalities used in daily clinic routine,
e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),
Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA),
Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on
multiple clinical tasks with or without patent history provided, including
imaging modality and anatomy recognition, disease diagnosis, report generation,
disease localisation.
</p>
<p>Our observation shows that, while GPT-4V demonstrates proficiency in
distinguishing between medical image modalities and anatomy, it faces
significant challenges in disease diagnosis and generating comprehensive
reports. These findings underscore that while large multimodal models have made
significant advancements in computer vision and natural language processing, it
remains far from being used to effectively support real-world medical
applications and clinical decision-making.
</p>
<p>All images used in this report can be found in
https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11753">Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wake_N/0/1/0/all/0/1">Naoki Wake</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanehira_A/0/1/0/all/0/1">Atsushi Kanehira</a>, <a href="http://arxiv.org/find/cs/1/au:+Sasabuchi_K/0/1/0/all/0/1">Kazuhiro Sasabuchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Takamatsu_J/0/1/0/all/0/1">Jun Takamatsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikeuchi_K/0/1/0/all/0/1">Katsushi Ikeuchi</a></p>
<p>This technical report explores the ability of ChatGPT in recognizing emotions
from text, which can be the basis of various applications like interactive
chatbots, data annotation, and mental health analysis. While prior research has
shown ChatGPT's basic ability in sentiment analysis, its performance in more
nuanced emotion recognition is not yet explored. Here, we conducted experiments
to evaluate its performance of emotion recognition across different datasets
and emotion labels. Our findings indicate a reasonable level of reproducibility
in its performance, with noticeable improvement through fine-tuning. However,
the performance varies with different emotion labels and datasets, highlighting
an inherent instability and possible bias. The choice of dataset and emotion
labels significantly impacts ChatGPT's emotion recognition performance. This
paper sheds light on the importance of dataset and label selection, and the
potential of fine-tuning in enhancing ChatGPT's emotion recognition
capabilities, providing a groundwork for better integration of emotion analysis
in applications using ChatGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13760">Enhancing Abstractiveness of Summarization Models through Calibrated Distillation. (arXiv:2310.13760v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hwanjun Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalyminov_I/0/1/0/all/0/1">Igor Shalyminov</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Siffi Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kaisheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1">Saab Mansour</a></p>
<p>Sequence-level knowledge distillation reduces the size of Seq2Seq models for
more efficient abstractive summarization. However, it often leads to a loss of
abstractiveness in summarization. In this paper, we propose a novel approach
named DisCal to enhance the level of abstractiveness (measured by n-gram
overlap) without sacrificing the informativeness (measured by ROUGE) of
generated summaries. DisCal exposes diverse pseudo summaries with two
supervision to the student model. Firstly, the best pseudo summary is
identified in terms of abstractiveness and informativeness and used for
sequence-level distillation. Secondly, their ranks are used to ensure the
student model to assign higher prediction scores to summaries with higher
ranks. Our experiments show that DisCal outperforms prior methods in
abstractive summarization distillation, producing highly abstractive and
informative summaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15896">BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yirong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xiaofen Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+zheng_h/0/1/0/all/0/1">huimin zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhipei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1">Kai Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sihang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jieling Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiangmin Xu</a></p>
<p>Large language models (LLMs) have performed well in providing general and
extensive health suggestions in single-turn conversations, exemplified by
systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the
limited information provided by users during single turn results in inadequate
personalization and targeting of the generated suggestions, which requires
users to independently select the useful part. It is mainly caused by the
missing ability to engage in multi-turn questioning. In real-world medical
consultations, doctors usually employ a series of iterative inquiries to
comprehend the patient's condition thoroughly, enabling them to provide
effective and personalized suggestions subsequently, which can be defined as
chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose
BianQue, a ChatGLM-based LLM finetuned with the self-constructed health
conversation dataset BianQueCorpus that is consist of multiple turns of
questioning and health suggestions polished by ChatGPT. Experimental results
demonstrate that the proposed BianQue can simultaneously balance the
capabilities of both questioning and health suggestions, which will help
promote the research and application of LLMs in the field of proactive health.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17769">Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franken_J/0/1/0/all/0/1">Jan-Philipp Fr&#xe4;nken</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_S/0/1/0/all/0/1">Sam Kwok</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1">Peixuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandhi_K/0/1/0/all/0/1">Kanishk Gandhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1">Dilip Arumugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1">Jared Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1">Alex Tamkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstenberg_T/0/1/0/all/0/1">Tobias Gerstenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a></p>
<p>We explore the idea of aligning an AI assistant by inverting a model of
users' (unknown) preferences from observed interactions. To validate our
proposal, we run proof-of-concept simulations in the economic ultimatum game,
formalizing user preferences as policies that guide the actions of simulated
players. We find that the AI assistant accurately aligns its behavior to match
standard policies from the economic literature (e.g., selfish, altruistic).
However, the assistant's learned policies lack robustness and exhibit limited
generalization in an out-of-distribution setting when confronted with a
currency (e.g., grams of medicine) that was not included in the assistant's
training distribution. Additionally, we find that when there is inconsistency
in the relationship between language use and an unknown policy (e.g., an
altruistic policy combined with rude language), the assistant's learning of the
policy is slowed. Overall, our preliminary results suggest that developing
simulation frameworks in which AI assistants need to infer preferences from
diverse users can provide a valuable approach for studying practical alignment
questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00176">ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ene_T/0/1/0/all/0/1">Teodor-Dumitru Ene</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1">Robert Kirby</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chris Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1">Nathaniel Pinckney</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1">Rongjian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Alben_J/0/1/0/all/0/1">Jonah Alben</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_H/0/1/0/all/0/1">Himyanshu Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Sanmitra Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayraktaroglu_I/0/1/0/all/0/1">Ismet Bayraktaroglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaskaran_B/0/1/0/all/0/1">Bonita Bhaskaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1">Bryan Catanzaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1">Arjun Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Clay_S/0/1/0/all/0/1">Sharon Clay</a>, <a href="http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1">Bill Dally</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1">Laura Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1">Parikshit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhodhi_S/0/1/0/all/0/1">Siddhanth Dhodhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Halepete_S/0/1/0/all/0/1">Sameer Halepete</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1">Eric Hill</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jiashang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sumit Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1">Brucek Khailany</a>, <a href="http://arxiv.org/find/cs/1/au:+Kokai_G/0/1/0/all/0/1">George Kokai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1">Kishor Kunal</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lind_C/0/1/0/all/0/1">Charley Lind</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Oberman_S/0/1/0/all/0/1">Stuart Oberman</a>, <a href="http://arxiv.org/find/cs/1/au:+Omar_S/0/1/0/all/0/1">Sujeet Omar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratty_S/0/1/0/all/0/1">Sreedhar Pratty</a>, <a href="http://arxiv.org/find/cs/1/au:+Raiman_J/0/1/0/all/0/1">Jonathan Raiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1">Ambar Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhengjiang Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hanfei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Suthar_P/0/1/0/all/0/1">Pratik P Suthar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tej_V/0/1/0/all/0/1">Varun Tej</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_W/0/1/0/all/0/1">Walker Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaizhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haoxing Ren</a></p>
<p>ChipNeMo aims to explore the applications of large language models (LLMs) for
industrial chip design. Instead of directly deploying off-the-shelf commercial
or open-source LLMs, we instead adopt the following domain adaptation
techniques: custom tokenizers, domain-adaptive continued pretraining,
supervised fine-tuning (SFT) with domain-specific instructions, and
domain-adapted retrieval models. We evaluate these methods on three selected
LLM applications for chip design: an engineering assistant chatbot, EDA script
generation, and bug summarization and analysis. Our results show that these
domain adaptation techniques enable significant LLM performance improvements
over general-purpose base models across the three evaluated applications,
enabling up to 5x model size reduction with similar or better performance on a
range of design tasks. Our findings also indicate that there's still room for
improvement between our current results and ideal outcomes. We believe that
further investigation of domain-adapted LLM approaches will help close this gap
in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00587">Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1">Ercong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sheng Liang</a></p>
<p>The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06595">From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1">Ercong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sheng Liang</a></p>
<p>The remarkable ability of Large Language Models (LLMs) to understand and
follow instructions has sometimes been limited by their in-context learning
(ICL) performance in low-resource languages. To address this, we introduce a
novel approach that leverages cross-lingual retrieval-augmented in-context
learning (CREA-ICL). By extracting semantically similar prompts from
high-resource languages, we aim to improve the zero-shot performance of
multilingual pre-trained language models (MPLMs) across diverse tasks. Though
our approach yields steady improvements in classification tasks, it faces
challenges in generation tasks. Our evaluation offers insights into the
performance dynamics of retrieval-augmented in-context learning across both
classification and generation domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06851">Automatic Textual Normalization for Hate Speech Detection. (arXiv:2311.06851v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Thi-Hoang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dung Ha Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nguyet Thi Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1">Khanh Thanh-Duy Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Kiet Van Nguyen</a></p>
<p>Social media data is a valuable resource for research, yet it contains a wide
range of non-standard words (NSW). These irregularities hinder the effective
operation of NLP tools. Current state-of-the-art methods for the Vietnamese
language address this issue as a problem of lexical normalization, involving
the creation of manual rules or the implementation of multi-staged deep
learning frameworks, which necessitate extensive efforts to craft intricate
rules. In contrast, our approach is straightforward, employing solely a
sequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset
for textual normalization, comprising 2,181 human-annotated comments with an
inter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for
textual normalization, our results reveal that the accuracy achieved falls
slightly short of 70%. Nevertheless, textual normalization enhances the
accuracy of the Hate Speech Detection (HSD) task by approximately 2%,
demonstrating its potential to improve the performance of complex NLP tasks.
Our dataset is accessible for research purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08105">DiLoCo: Distributed Low-Communication Training of Language Models. (arXiv:2311.08105v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1">Arthur Douillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1">Qixuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1">Andrei A. Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1">Rachita Chhaparia</a>, <a href="http://arxiv.org/find/cs/1/au:+Donchev_Y/0/1/0/all/0/1">Yani Donchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1">Adhiguna Kuncoro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1">Marc&#x27;Aurelio Ranzato</a>, <a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1">Arthur Szlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajun Shen</a></p>
<p>Large language models (LLM) have become a critical component in many
applications of machine learning. However, standard approaches to training LLM
require a large number of tightly interconnected accelerators, with devices
exchanging gradients and other intermediate states at each optimization step.
While it is difficult to build and maintain a single computing cluster hosting
many accelerators, it might be easier to find several computing clusters each
hosting a smaller number of devices. In this work, we propose a distributed
optimization algorithm, Distributed Low-Communication (DiLoCo), that enables
training of language models on islands of devices that are poorly connected.
The approach is a variant of federated averaging, where the number of inner
steps is large, the inner optimizer is AdamW, and the outer optimizer is
Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8
workers performs as well as fully synchronous optimization while communicating
500 times less. DiLoCo exhibits great robustness to the data distribution of
each worker. It is also robust to resources becoming unavailable over time, and
vice versa, it can seamlessly leverage resources that become available during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10777">A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends. (arXiv:2311.10777v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yan Cathy Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a>, <a href="http://arxiv.org/find/cs/1/au:+Taskova_K/0/1/0/all/0/1">Katerina Taskova</a>, <a href="http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1">J&#xf6;rg Wicker</a></p>
<p>Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment
analysis (SA) that identifies aspects and the associated opinions from a given
text. In the digital era, ABSA gained increasing popularity and applications in
mining opinionated text data to obtain insights and support decisions. ABSA
research employs linguistic, statistical, and machine-learning approaches and
utilises resources such as labelled datasets, aspect and sentiment lexicons and
ontology. By its nature, ABSA is domain-dependent and can be sensitive to the
impact of misalignment between the resource and application domains. However,
to our knowledge, this topic has not been explored by the existing ABSA
literature reviews. In this paper, we present a Systematic Literature Review
(SLR) of ABSA studies with a focus on the research application domain, dataset
domain, and the research methods to examine their relationships and identify
trends over time. Our results suggest a number of potential systemic issues in
the ABSA research literature, including the predominance of the
``product/service review'' dataset domain among the majority of studies that
did not have a specific research application domain, coupled with the
prevalence of dataset-reliant methods such as supervised machine learning. This
review makes a number of unique contributions to the ABSA research field: 1) To
our knowledge, it is the first SLR that links the research domain, dataset
domain, and research method through a systematic perspective; 2) it is one of
the largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191
search results without time constraint; and 3) our review methodology adopted
an innovative automatic filtering process based on PDF-mining, which enhanced
screening quality and reliability. Suggestions and our review limitations are
also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12275">Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis. (arXiv:2311.12275v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1">Ruiyang Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhenge Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Meng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1">Ahmed Abbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Peipei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jingtong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yiyu Shi</a></p>
<p>After a large language model (LLM) is deployed on edge devices, it is
desirable for these devices to learn from user-generated conversation data to
generate user-specific and personalized responses in real-time. However,
user-generated data usually contains sensitive and private information, and
uploading such data to the cloud for annotation is not preferred if not
prohibited. While it is possible to obtain annotation locally by directly
asking users to provide preferred responses, such annotations have to be sparse
to not affect user experience. In addition, the storage of edge devices is
usually too limited to enable large-scale fine-tuning with full user-generated
data. It remains an open question how to enable on-device LLM personalization,
considering sparse annotation and limited on-device storage. In this paper, we
propose a novel framework to select and store the most representative data
online in a self-supervised way. Such data has a small memory footprint and
allows infrequent requests of user annotations for further fine-tuning. To
enhance fine-tuning quality, multiple semantically similar pairs of question
texts and expected responses are generated using the LLM. Our experiments show
that the proposed framework achieves the best user-specific content-generating
capability (accuracy) and fine-tuning speed (performance) compared with vanilla
baselines. To the best of our knowledge, this is the very first on-device LLM
personalization framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14648">Calibrated Language Models Must Hallucinate. (arXiv:2311.14648v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>, <a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1">Santosh S. Vempala</a></p>
<p>Recent language models generate false but plausible-sounding text with
surprising frequency. Such "hallucinations" are an obstacle to the usability of
language-based AI systems and can harm people who rely upon their outputs. This
work shows shows that there is an inherent statistical lower-bound on the rate
that pretrained language models hallucinate certain types of facts, having
nothing to do with the transformer LM architecture or data quality. For
"arbitrary" facts whose veracity cannot be determined from the training data,
we show that hallucinations must occur at a certain rate for language models
that satisfy a statistical calibration condition appropriate for generative
language models. Specifically, if the maximum probability of any fact is
bounded, we show that the probability of generating a hallucination is close to
the fraction of facts that occur exactly once in the training data (a
"Good-Turing" estimate), even assuming ideal training data without errors.
</p>
<p>One conclusion is that models pretrained to be sufficiently good predictors
(i.e., calibrated) may require post-training to mitigate hallucinations on the
type of arbitrary facts that tend to appear once in the training set. However,
our analysis also suggests that there is no statistical reason that pretraining
will lead to hallucination on facts that tend to appear more than once in the
training data (like references to publications such as articles and books,
whose hallucinations have been particularly notable and problematic) or on
systematic facts (like arithmetic calculations). Therefore, different
architectures and learning algorithms may mitigate these latter types of
hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM's. These reward models are additionally used at
inference-time to estimate LLM responses' adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14836">Custom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion. (arXiv:2311.14836v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamble_A/0/1/0/all/0/1">Anand Kamble</a>, <a href="http://arxiv.org/find/cs/1/au:+Tathe_A/0/1/0/all/0/1">Aniket Tathe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumbharkar_S/0/1/0/all/0/1">Suyash Kumbharkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhandare_A/0/1/0/all/0/1">Atharva Bhandare</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Anirban C. Mitra</a></p>
<p>This paper proposes two innovative methodologies to construct customized
Common Voice datasets for low-resource languages like Hindi. The first
methodology leverages Bark, a transformer-based text-to-audio model developed
by Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to
enhance Bark's performance. The second methodology employs Retrieval-Based
Voice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both
methodologies contribute to the advancement of ASR technology and offer
valuable insights into addressing the challenges of constructing customized
Common Voice datasets for under-resourced languages. Furthermore, they provide
a pathway to achieving high-quality, personalized voice generation for a range
of applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15500">Function-constrained Program Synthesis. (arXiv:2311.15500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hajali_P/0/1/0/all/0/1">Patrick Hajali</a>, <a href="http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1">Ignas Budvytis</a></p>
<p>This work introduces (1) a technique that allows large language models (LLMs)
to leverage user-provided code when solving programming tasks and (2) a method
to iteratively generate modular sub-functions that can aid future code
generation attempts when the initial code generated by the LLM is inadequate.
Generating computer programs in general-purpose programming languages like
Python poses a challenge for LLMs when instructed to use code provided in the
prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code
completions in real-time by drawing on all code available in a development
environment. However, restricting code-specific LLMs to use only in-context
code is not straightforward, as the model is not explicitly instructed to use
the user-provided code and users cannot highlight precisely which snippets of
code the model should incorporate into its context. Moreover, current systems
lack effective recovery methods, forcing users to iteratively re-prompt the
model with modified prompts until a sufficient solution is reached. Our method
differs from traditional LLM-powered code-generation by constraining
code-generation to an explicit function set and enabling recovery from failed
attempts through automatically generated sub-functions. When the LLM cannot
produce working code, we generate modular sub-functions to aid subsequent
attempts at generating functional code. A by-product of our method is a library
of reusable sub-functions that can solve related tasks, imitating a software
team where efficiency scales with experience. We also introduce a new
"half-shot" evaluation paradigm that provides tighter estimates of LLMs' coding
abilities compared to traditional zero-shot evaluation. Our proposed evaluation
method encourages models to output solutions in a structured format, decreasing
syntax errors that can be mistaken for poor coding ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15565">Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oketunji_F/0/1/0/all/0/1">Finbarrs Oketunji</a></p>
<p>My research investigates the use of cutting-edge hybrid deep learning models
to accurately differentiate between AI-generated text and human writing. I
applied a robust methodology, utilising a carefully selected dataset comprising
AI and human texts from various sources, each tagged with instructions.
Advanced natural language processing techniques facilitated the analysis of
textual features. Combining sophisticated neural networks, the custom model
enabled it to detect nuanced differences between AI and human content.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15786">YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. (arXiv:2311.15786v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shaohua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xudong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shenling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiangang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingjun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongguo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiahua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>In this work, we develop and release Yuan 2.0, a series of large language
models with parameters ranging from 2.1 billion to 102.6 billion. The Localized
Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of
local dependencies of natural language into Attention. A data filtering and
generating system is presented to build pre-training and fine-tuning dataset in
high quality. A distributed training method with non-uniform pipeline parallel,
data parallel, and optimizer parallel is proposed, which greatly reduces the
bandwidth requirements of intra-node communication, and achieves good
performance in large-scale distributed training. Yuan 2.0 models display
impressive ability in code generation, math problem-solving, and chatting
compared with existing models. The latest version of YUAN 2.0, including model
weights and source code, is accessible at Github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17065">Efficient Deep Speech Understanding at the Edge. (arXiv:2311.17065v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Rongxiang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1">Felix Xiaozhu Lin</a></p>
<p>In contemporary speech understanding (SU), a sophisticated pipeline is
employed, encompassing the ingestion of streaming voice input. The pipeline
executes beam search iteratively, invoking a deep neural network to generate
tentative outputs (referred to as hypotheses) in an autoregressive manner.
Periodically, the pipeline assesses attention and Connectionist Temporal
Classification (CTC) scores.
</p>
<p>This paper aims to enhance SU performance on edge devices with limited
resources. Adopting a hybrid strategy, our approach focuses on accelerating
on-device execution and offloading inputs surpassing the device's capacity.
While this approach is established, we tackle SU's distinctive challenges
through innovative techniques: (1) Late Contextualization: This involves the
parallel execution of a model's attentive encoder during input ingestion. (2)
Pilot Inference: Addressing temporal load imbalances in the SU pipeline, this
technique aims to mitigate them effectively. (3) Autoregression Offramps:
Decisions regarding offloading are made solely based on hypotheses, presenting
a novel approach.
</p>
<p>These techniques are designed to seamlessly integrate with existing speech
models, pipelines, and frameworks, offering flexibility for independent or
combined application. Collectively, they form a hybrid solution for edge SU.
Our prototype, named XYZ, has undergone testing on Arm platforms featuring 6 to
8 cores, demonstrating state-of-the-art accuracy. Notably, it achieves a 2x
reduction in end-to-end latency and a corresponding 2x decrease in offloading
requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17213">General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports. (arXiv:2311.17213v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhanaliwala_A/0/1/0/all/0/1">Ali H. Dhanaliwala</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1">Rikhiya Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1">Sanjeev Kumar Karn</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullaskrishnan_P/0/1/0/all/0/1">Poikavila Ullaskrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1">Oladimeji Farri</a>, <a href="http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1">Dorin Comaniciu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1">Charles E. Kahn</a></p>
<p>Radiologists produce unstructured data that could be valuable for clinical
care when consumed by information systems. However, variability in style limits
usage. Study compares performance of system using domain-adapted language model
(RadLing) and general-purpose large language model (GPT-4) in extracting common
data elements (CDE) from thoracic radiology reports. Three radiologists
annotated a retrospective dataset of 1300 thoracic reports (900 training, 400
test) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate
embeddings for sentences and identify CDEs using cosine-similarity, which were
mapped to values using light-weight mapper. GPT-4 system used OpenAI's
general-purpose embeddings to identify relevant CDEs and used GPT-4 to map to
values. The output CDE:value pairs were compared to the reference standard; an
identical match was considered true positive. Precision (positive predictive
value) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall
(sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4;
the difference was statistically significant (P&lt;.001). RadLing's domain-adapted
embeddings were more sensitive in CDE identification (95% vs 71%) and its
light-weight mapper had comparable precision in value assignment (95.4% vs
95.0%). RadLing system exhibited higher performance than GPT-4 system in
extracting CDEs from radiology reports. RadLing system's domain-adapted
embeddings outperform general-purpose embeddings from OpenAI in CDE
identification and its light-weight value mapper achieves comparable precision
to large GPT-4. RadLing system offers operational advantages including local
deployment and reduced runtime costs. Domain-adapted RadLing system surpasses
GPT-4 system in extracting common data elements from radiology reports, while
providing benefits of local deployment and lower costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17280">Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1">Ishika Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>Data augmentation via back-translation is common when pretraining
Vision-and-Language Navigation (VLN) models, even though the generated
instructions are noisy. But: does that noise matter? We find that nonsensical
or irrelevant language instructions during pretraining can have little effect
on downstream performance for both HAMT and VLN-BERT on R2R, and is still
better than only using clean, human data. To underscore these results, we
concoct an efficient augmentation method, Unigram + Object, which generates
nonsensical instructions that nonetheless improve downstream performance. Our
findings suggest that what matters for VLN R2R pretraining is the quantity of
visual trajectories, not the quality of instructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18743">AlignBench: Benchmarking Chinese Alignment of Large Language Models. (arXiv:2311.18743v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xuanyu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhuoer Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bosi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jiale Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1">Pei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1">Weng Lam Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a></p>
<p>Alignment has become a critical step for instruction-tuned Large Language
Models (LLMs) to become helpful assistants. However, effective evaluation of
alignment for emerging Chinese LLMs is still significantly lacking, calling for
real-scenario grounded, open-ended, challenging and automatic evaluations
tailored for alignment. To fill in this gap, we introduce AlignBench, a
comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in
Chinese. Equipped with a human-in-the-loop data curation pipeline, our
benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with
Chain-of-Thought to generate explanations and final ratings as evaluations,
ensuring high reliability and interpretability. Furthermore, we report
AlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that
recovers 95% of GPT-4's evaluation ability. We will provide public APIs for
evaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'
Chinese alignment. All evaluation codes, data, and LLM generations are
available at \url{https://github.com/THUDM/AlignBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18803">BioCLIP: A Vision Foundation Model for the Tree of Life. (arXiv:2311.18803v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1">Samuel Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiaman Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_M/0/1/0/all/0/1">Matthew J Thompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Campolongo_E/0/1/0/all/0/1">Elizabeth G Campolongo</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chan Hee Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlyn_D/0/1/0/all/0/1">David Edward Carlyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Li Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahdul_W/0/1/0/all/0/1">Wasila M Dahdul</a>, <a href="http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1">Charles Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1">Tanya Berger-Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1">Wei-Lun Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu Su</a></p>
<p>Images of the natural world, collected by a variety of cameras, from drones
to individual phones, are increasingly abundant sources of biological
information. There is an explosion of computational methods and tools,
particularly computer vision, for extracting biologically relevant information
from images for science and conservation. Yet most of these are bespoke
approaches designed for a specific task and are not easily adaptable or
extendable to new questions, contexts, and datasets. A vision model for general
organismal biology questions on images is of timely need. To approach this, we
curate and release TreeOfLife-10M, the largest and most diverse ML-ready
dataset of biology images. We then develop BioCLIP, a foundation model for the
tree of life, leveraging the unique properties of biology captured by
TreeOfLife-10M, namely the abundance and variety of images of plants, animals,
and fungi, together with the availability of rich structured biological
knowledge. We rigorously benchmark our approach on diverse fine-grained biology
classification tasks, and find that BioCLIP consistently and substantially
outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation
reveals that BioCLIP has learned a hierarchical representation conforming to
the tree of life, shedding light on its strong generalizability. Our code,
models and data will be made available at
https://github.com/Imageomics/bioclip.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00372">Event-driven Real-time Retrieval in Web Search. (arXiv:2312.00372v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1">Nan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shusen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yannan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiaoling Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Hualong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianhua Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jin Ma</a></p>
<p>Information retrieval in real-time search presents unique challenges distinct
from those encountered in classical web search. These challenges are
particularly pronounced due to the rapid change of user search intent, which is
influenced by the occurrence and evolution of breaking news events, such as
earthquakes, elections, and wars. Previous dense retrieval methods, which
primarily focused on static semantic representation, lack the capacity to
capture immediate search intent, leading to inferior performance in retrieving
the most recent event-related documents in time-sensitive scenarios. To address
this issue, this paper expands the query with event information that represents
real-time search intent. The Event information is then integrated with the
query through a cross-attention mechanism, resulting in a time-context query
representation. We further enhance the model's capacity for event
representation through multi-task training. Since publicly available datasets
such as MS-MARCO do not contain any event information on the query side and
have few time-sensitive queries, we design an automatic data collection and
annotation pipeline to address this issue, which includes ModelZoo-based Coarse
Annotation and LLM-driven Fine Annotation processes. In addition, we share the
training tricks such as two-stage training and hard negative sampling. Finally,
we conduct a set of offline experiments on a million-scale production dataset
to evaluate our approach and deploy an A/B testing in a real online system to
verify the performance. Extensive experimental results demonstrate that our
proposed approach significantly outperforms existing state-of-the-art baseline
methods.
</p>
</p>
</div>

    </div>
    </body>
    