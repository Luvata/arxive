<!DOCTYPE html>
<html>
<head>
<title>2023-11-21-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.10101">Gaussian Differential Privacy on Riemannian Manifolds. (arXiv:2311.10101v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yangdi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiaotian Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Lei Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Linglong Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bei Jiang</a></p>
<p>We develop an advanced approach for extending Gaussian Differential Privacy
(GDP) to general Riemannian manifolds. The concept of GDP stands out as a
prominent privacy definition that strongly warrants extension to manifold
settings, due to its central limit properties. By harnessing the power of the
renowned Bishop-Gromov theorem in geometric analysis, we propose a Riemannian
Gaussian distribution that integrates the Riemannian distance, allowing us to
achieve GDP in Riemannian manifolds with bounded Ricci curvature. To the best
of our knowledge, this work marks the first instance of extending the GDP
framework to accommodate general Riemannian manifolds, encompassing curved
spaces, and circumventing the reliance on tangent space summaries. We provide a
simple algorithm to evaluate the privacy budget $\mu$ on any one-dimensional
manifold and introduce a versatile Markov Chain Monte Carlo (MCMC)-based
algorithm to calculate $\mu$ on any Riemannian manifold with constant
curvature. Through simulations on one of the most prevalent manifolds in
statistics, the unit sphere $S^d$, we demonstrate the superior utility of our
Riemannian Gaussian mechanism in comparison to the previously proposed
Riemannian Laplace mechanism for implementing GDP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10111">VideoCon: Robust Video-Language Alignment via Contrast Captions. (arXiv:2311.10111v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1">Yonatan Bitton</a>, <a href="http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1">Idan Szpektor</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>Despite being (pre)trained on a massive amount of data, state-of-the-art
video-language alignment models are not robust to semantically-plausible
contrastive changes in the video captions. Our work addresses this by
identifying a broad spectrum of contrast misalignments, such as replacing
entities, actions, and flipping event order, which alignment models should be
robust against. To this end, we introduce the VideoCon, a video-language
alignment dataset constructed by a large language model that generates
plausible contrast video captions and explanations for differences between
original and contrast video captions. Then, a generative video-language model
is finetuned with VideoCon to assess video-language entailment and generate
explanations. Our VideoCon-based alignment model significantly outperforms
current models. It exhibits a 12-point increase in AUC for the video-language
alignment task on human-generated contrast captions. Finally, our model sets
new state of the art zero-shot performance in temporally-extensive
video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video
question answering (ATP-Hard). Moreover, our model shows superior performance
on novel videos and human-crafted captions and explanations. Our code and data
are available at https://github.com/Hritikbansal/videocon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10112">Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models. (arXiv:2311.10112v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zifeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Heling Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingpei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunpu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Ruotong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1">Bo Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>In recent years, modeling evolving knowledge over temporal knowledge graphs
(TKGs) has become a heated topic. Various methods have been proposed to
forecast links on TKGs. Most of them are embedding-based, where hidden
representations are learned to represent knowledge graph (KG) entities and
relations based on the observed graph contexts. Although these methods show
strong performance on traditional TKG forecasting (TKGF) benchmarks, they
naturally face a strong challenge when they are asked to model the unseen
zero-shot relations that has no prior graph context. In this paper, we try to
mitigate this problem as follows. We first input the text descriptions of KG
relations into large language models (LLMs) for generating relation
representations, and then introduce them into embedding-based TKGF methods.
LLM-empowered representations can capture the semantic information in the
relation descriptions. This makes the relations, whether seen or unseen, with
similar semantic meanings stay close in the embedding space, enabling TKGF
models to recognize zero-shot relations even without any observed graph
context. Experimental results show that our approach helps TKGF models to
achieve much better performance in forecasting the facts with previously unseen
relations, while still maintaining their ability in link forecasting regarding
seen relations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10117">Automatic Engineering of Long Prompts. (arXiv:2311.10117v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Si Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Felix X. Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1">Inderjit S. Dhillon</a></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in
solving complex open-domain tasks, guided by comprehensive instructions and
demonstrations provided in the form of prompts. However, these prompts can be
lengthy, often comprising hundreds of lines and thousands of tokens, and their
design often requires considerable human effort. Recent research has explored
automatic prompt engineering for short prompts, typically consisting of one or
a few sentences. However, the automatic design of long prompts remains a
challenging problem due to its immense search space. In this paper, we
investigate the performance of greedy algorithms and genetic algorithms for
automatic long prompt engineering. We demonstrate that a simple greedy approach
with beam search outperforms other methods in terms of search efficiency.
Moreover, we introduce two novel techniques that utilize search history to
enhance the effectiveness of LLM-based mutation in our search algorithm. Our
results show that the proposed automatic long prompt engineering algorithm
achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard,
highlighting the significance of automating prompt designs to fully harness the
capabilities of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10119">Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition. (arXiv:2311.10119v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vazquez_Rodriguez_J/0/1/0/all/0/1">Juan Vazquez-Rodriguez</a> (M-PSI), <a href="http://arxiv.org/find/cs/1/au:+Lefebvre_G/0/1/0/all/0/1">Gr&#xe9;goire Lefebvre</a>, <a href="http://arxiv.org/find/cs/1/au:+Cumin_J/0/1/0/all/0/1">Julien Cumin</a>, <a href="http://arxiv.org/find/cs/1/au:+Crowley_J/0/1/0/all/0/1">James L. Crowley</a> (M-PSI)</p>
<p>Decades of research indicate that emotion recognition is more effective when
drawing information from multiple modalities. But what if some modalities are
sometimes missing? To address this problem, we propose a novel
Transformer-based architecture for recognizing valence and arousal in a
time-continuous manner even with missing input modalities. We use a coupling of
cross-attention and self-attention mechanisms to emphasize relationships
between modalities during time and enhance the learning process on weak salient
inputs. Experimental results on the Ulm-TSST dataset show that our model
exhibits an improvement of the concordance correlation coefficient evaluation
of 37% when predicting arousal values and 30% when predicting valence values,
compared to a late-fusion baseline approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10127">Learning interactions to boost human creativity with bandits and GPT-4. (arXiv:2311.10127v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vartanian_A/0/1/0/all/0/1">Ara Vartanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaoxi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yun-Shiuan Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1">Siddharth Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaojin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1">Timothy T. Rogers</a></p>
<p>This paper considers how interactions with AI algorithms can boost human
creative thought. We employ a psychological task that demonstrates limits on
human creativity, namely semantic feature generation: given a concept name,
respondents must list as many of its features as possible. Human participants
typically produce only a fraction of the features they know before getting
"stuck." In experiments with humans and with a language AI (GPT-4) we contrast
behavior in the standard task versus a variant in which participants can ask
for algorithmically-generated hints. Algorithm choice is administered by a
multi-armed bandit whose reward indicates whether the hint helped generating
more features. Humans and the AI show similar benefits from hints, and
remarkably, bandits learning from AI responses prefer the same prompting
strategy as those learning from human behavior. The results suggest that
strategies for boosting human creativity via computer interactions can be
learned by bandits run on groups of simulated participants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10156">Algebraic Topological Networks via the Persistent Local Homology Sheaf. (arXiv:2311.10156v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cesa_G/0/1/0/all/0/1">Gabriele Cesa</a>, <a href="http://arxiv.org/find/cs/1/au:+Behboodi_A/0/1/0/all/0/1">Arash Behboodi</a></p>
<p>In this work, we introduce a novel approach based on algebraic topology to
enhance graph convolution and attention modules by incorporating local
topological properties of the data. To do so, we consider the framework of
sheaf neural networks, which has been previously leveraged to incorporate
additional structure into graph neural networks' features and construct more
expressive, non-isotropic messages. Specifically, given an input simplicial
complex (e.g. generated by the cliques of a graph or the neighbors in a point
cloud), we construct its local homology sheaf, which assigns to each node the
vector space of its local homology. The intermediate features of our networks
live in these vector spaces and we leverage the associated sheaf Laplacian to
construct more complex linear messages between them. Moreover, we extend this
approach by considering the persistent version of local homology associated
with a weighted simplicial complex (e.g., built from pairwise distances of
nodes embeddings). This i) solves the problem of the lack of a natural choice
of basis for the local homology vector spaces and ii) makes the sheaf itself
differentiable, which enables our models to directly optimize the topology of
their intermediate features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10162">K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise. (arXiv:2311.10162v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shen_G/0/1/0/all/0/1">Guoyao Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1">Mengyu Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Farris_C/0/1/0/all/0/1">Chad W. Farris</a>, <a href="http://arxiv.org/find/eess/1/au:+Anderson_S/0/1/0/all/0/1">Stephan Anderson</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a></p>
<p>Deep learning-based MRI reconstruction models have achieved superior
performance these days. Most recently, diffusion models have shown remarkable
performance in image generation, in-painting, super-resolution, image editing
and more. As a generalized diffusion model, cold diffusion further broadens the
scope and considers models built around arbitrary image transformations such as
blurring, down-sampling, etc. In this paper, we propose a k-space cold
diffusion model that performs image degradation and restoration in k-space
without the need for Gaussian noise. We provide comparisons with multiple deep
learning-based MRI reconstruction models and perform tests on a well-known
large open-source MRI dataset. Our results show that this novel way of
performing degradation can generate high-quality reconstruction images for
accelerated MRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10170">Improving Unimodal Inference with Multimodal Transformers. (arXiv:2311.10170v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chumachenko_K/0/1/0/all/0/1">Kateryna Chumachenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1">Alexandros Iosifidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1">Moncef Gabbouj</a></p>
<p>This paper proposes an approach for improving performance of unimodal models
with multimodal training. Our approach involves a multi-branch architecture
that incorporates unimodal models with a multimodal transformer-based branch.
By co-training these branches, the stronger multimodal branch can transfer its
knowledge to the weaker unimodal branches through a multi-task objective,
thereby improving the performance of the resulting unimodal models. We evaluate
our approach on tasks of dynamic hand gesture recognition based on RGB and
Depth, audiovisual emotion recognition based on speech and facial video, and
audio-video-text based sentiment analysis. Our approach outperforms the
conventionally trained unimodal counterparts. Interestingly, we also observe
that optimization of the unimodal branches improves the multimodal branch,
compared to a similar multimodal model trained from scratch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10177">Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts. (arXiv:2311.10177v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a></p>
<p>Neural networks have demonstrated significant accuracy across various
domains, yet their vulnerability to subtle input alterations remains a
persistent challenge. Conventional methods like data augmentation, while
effective to some extent, fall short in addressing unforeseen corruptions,
limiting the adaptability of neural networks in real-world scenarios. In
response, this paper introduces a novel paradigm known as the Mixture of
Class-Specific Expert Architecture. The approach involves disentangling feature
learning for individual classes, offering a nuanced enhancement in scalability
and overall performance. By training dedicated network segments for each class
and subsequently aggregating their outputs, the proposed architecture aims to
mitigate vulnerabilities associated with common neural network structures. The
study underscores the importance of comprehensive evaluation methodologies,
advocating for the incorporation of benchmarks like the common corruptions
benchmark. This inclusion provides nuanced insights into the vulnerabilities of
neural networks, especially concerning their generalization capabilities and
robustness to unforeseen distortions. The research aligns with the broader
objective of advancing the development of highly robust learning systems
capable of nuanced reasoning across diverse and challenging real-world
scenarios. Through this contribution, the paper aims to foster a deeper
understanding of neural network limitations and proposes a practical approach
to enhance their resilience in the face of evolving and unpredictable
conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10203">Adaptive Optimization Algorithms for Machine Learning. (arXiv:2311.10203v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanzely_S/0/1/0/all/0/1">Slavom&#xed;r Hanzely</a></p>
<p>Machine learning assumes a pivotal role in our data-driven world. The
increasing scale of models and datasets necessitates quick and reliable
algorithms for model training. This dissertation investigates adaptivity in
machine learning optimizers. The ensuing chapters are dedicated to various
facets of adaptivity, including: 1. personalization and user-specific models
via personalized loss, 2. provable post-training model adaptations via
meta-learning, 3. learning unknown hyperparameters in real time via
hyperparameter variance reduction, 4. fast O(1/k^2) global convergence of
second-order methods via stepsized Newton method regardless of the
initialization and choice basis, 5. fast and scalable second-order methods via
low-dimensional updates. This thesis contributes novel insights, introduces new
algorithms with improved convergence guarantees, and improves analyses of
popular practical algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10206">Bayes in the age of intelligent machines. (arXiv:2311.10206v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jian-Qiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1">Erin Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1">R. Thomas McCoy</a></p>
<p>The success of methods based on artificial neural networks in creating
intelligent machines seems like it might pose a challenge to explanations of
human cognition in terms of Bayesian inference. We argue that this is not the
case, and that in fact these systems offer new opportunities for Bayesian
modeling. Specifically, we argue that Bayesian models of cognition and
artificial neural networks lie at different levels of analysis and are
complementary modeling approaches, together offering a way to understand human
cognition that spans these levels. We also argue that the same perspective can
be applied to intelligent machines, where a Bayesian approach may be uniquely
valuable in understanding the behavior of large, opaque artificial neural
networks that are trained on proprietary data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10207">Stella Nera: Achieving 161 TOp/s/W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication. (arXiv:2311.10207v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schonleber_J/0/1/0/all/0/1">Jannis Sch&#xf6;nleber</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1">Lukas Cavigelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Andri_R/0/1/0/all/0/1">Renzo Andri</a>, <a href="http://arxiv.org/find/cs/1/au:+Perotti_M/0/1/0/all/0/1">Matteo Perotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1">Luca Benini</a></p>
<p>From classical HPC to deep learning, MatMul is at the heart of today's
computing. The recent Maddness method approximates MatMul without the need for
multiplication by using a hash-based version of product quantization (PQ)
indexing into a look-up table (LUT). Stella Nera is the first Maddness
accelerator and it achieves 15x higher area efficiency (GMAC/s/mm^2) and more
than 25x higher energy efficiency (TMAC/s/W) than direct MatMul accelerators
implemented in the same technology. The hash function is a decision tree, which
allows for an efficient hardware implementation as the multiply-accumulate
operations are replaced by decision tree passes and LUT lookups. The entire
Maddness MatMul can be broken down into parts that allow an effective
implementation with small computing units and memories, allowing it to reach
extreme efficiency while remaining generically applicable for MatMul tasks. In
a commercial 14nm technology and scaled to 3nm, we achieve an energy efficiency
of 161 TOp/s/W@0.55V with a Top-1 accuracy on CIFAR-10 of more than 92.5% using
ResNet9.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10223">Asymptotically Fair Participation in Machine Learning Models: an Optimal Control Perspective. (arXiv:2311.10223v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuotong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qianxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a></p>
<p>The performance of state-of-the-art machine learning models often
deteriorates when testing on demographics that are under-represented in the
training dataset. This problem has predominately been studied in a supervised
learning setting where the data distribution is static. However, real-world
applications often involve distribution shifts caused by the deployed models.
For instance, the performance disparity against monitory users can lead to a
high customer churn rate, thus the available data provided by active users are
skewed due to the lack of minority users. This feedback effect further
exacerbates the disparity among different demographic groups in future steps.
To address this issue, we propose asymptotically fair participation as a
condition to maintain long-term model performance over all demographic groups.
In this work, we aim to address the problem of achieving asymptotically fair
participation via optimal control formulation. Moreover, we design a surrogate
retention system based on existing literature on evolutionary population
dynamics to approximate the dynamics of distribution shifts on active user
counts, from which the objective of achieving asymptotically fair participation
is formulated as an optimal control problem, and the control variables are
considered as the model parameters. We apply an efficient implementation of
Pontryagin's maximum principle to estimate the optimal control solution. To
evaluate the effectiveness of the proposed method, we design a generic
simulation environment that simulates the population dynamics of the feedback
effect between user retention and model performance. When we deploy the
resulting models to the simulation environment, the optimal control solution
accounts for long-term planning and leads to superior performance compared with
existing baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10224">CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images. (arXiv:2311.10224v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Abbas_S/0/1/0/all/0/1">Syed Farhan Abbas</a>, <a href="http://arxiv.org/find/eess/1/au:+Duc_N/0/1/0/all/0/1">Nguyen Thanh Duc</a>, <a href="http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1">Yoonguu Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1">Kyungwon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_B/0/1/0/all/0/1">Boreom Lee</a></p>
<p>Due to the lack of automated methods, to diagnose cerebrovascular disease,
time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually,
making it time-consuming. The commonly used encoder-decoder architectures for
cerebrovascular segmentation utilize redundant features, eventually leading to
the extraction of low-level features multiple times. Additionally,
convolutional neural networks (CNNs) suffer from performance degradation when
the batch size is small, and deeper networks experience the vanishing gradient
problem. Methods: In this paper, we attempt to solve these limitations and
propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet,
for precise extraction of brain vessel images. We proposed a sequence of
preprocessing techniques followed by deeply supervised UNet to improve the
accuracy of segmentation of the brain vessels leading to a stroke. To combine
the low and high semantics, we applied the attention mechanism. This mechanism
focuses on relevant associations and neglects irrelevant anatomical
information. Furthermore, the inclusion of deep supervision incorporates
different levels of features that prove to be beneficial for network
convergence. Results: We demonstrate the efficiency of the proposed method by
cross-validating with an unlabeled dataset, which was further labeled by us. We
believe that the novelty of this algorithm lies in its ability to perform well
on both labeled and unlabeled data with image processing-based enhancement. The
results indicate that our method performed better than the existing
state-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method
will help in accurate segmentation of cerebrovascular structure leading to
stroke
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10242">Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers. (arXiv:2311.10242v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengesi_S/0/1/0/all/0/1">Staphord Bengesi</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Sayed_H/0/1/0/all/0/1">Hoda El-Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1">Md Kamruzzaman Sarker</a>, <a href="http://arxiv.org/find/cs/1/au:+Houkpati_Y/0/1/0/all/0/1">Yao Houkpati</a>, <a href="http://arxiv.org/find/cs/1/au:+Irungu_J/0/1/0/all/0/1">John Irungu</a>, <a href="http://arxiv.org/find/cs/1/au:+Oladunni_T/0/1/0/all/0/1">Timothy Oladunni</a></p>
<p>The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10246">Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning. (arXiv:2311.10246v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1">Amartya Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hazard_C/0/1/0/all/0/1">Christopher J. Hazard</a>, <a href="http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1">Jacob Beel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mack_C/0/1/0/all/0/1">Cade Mack</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jack Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Resnick_M/0/1/0/all/0/1">Michael Resnick</a>, <a href="http://arxiv.org/find/cs/1/au:+Goddin_W/0/1/0/all/0/1">Will Goddin</a></p>
<p>Nonparametric learning is a fundamental concept in machine learning that aims
to capture complex patterns and relationships in data without making strong
assumptions about the underlying data distribution. Owing to simplicity and
familiarity, one of the most well-known algorithms under this paradigm is the
$k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine
learning in safety-critical applications, in this work, we shed new light on
the traditional nearest neighbors algorithm from the perspective of information
theory and propose a robust and interpretable framework for tasks such as
classification, regression, and anomaly detection using a single model. Instead
of using a traditional distance measure which needs to be scaled and
contextualized, we use a novel formulation of \textit{surprisal} (amount of
information required to explain the difference between the observed and
expected result). Finally, we demonstrate this architecture's capability to
perform at-par or above the state-of-the-art on classification, regression, and
anomaly detection tasks using a single model with enhanced interpretability by
providing novel concepts for characterizing data and predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10248">FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework. (arXiv:2311.10248v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebron_S/0/1/0/all/0/1">Sheldon C. Ebron Jr.</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kan Yang</a></p>
<p>Federated Learning (FL) enables collaborative machine learning model training
across multiple parties without sharing raw data. However, FL's distributed
nature allows malicious clients to impact model training through Byzantine or
backdoor attacks, using erroneous model updates. Existing defenses measure the
deviation of each update from a 'ground-truth model update.' They often rely on
a benign root dataset on the server or use trimmed mean or median for clipping,
both methods having limitations.
</p>
<p>We introduce FedTruth, a robust defense against model poisoning in FL.
FedTruth doesn't assume specific data distributions nor requires a benign root
dataset. It estimates a global model update with dynamic aggregation weights,
considering contributions from all benign clients. Empirical studies
demonstrate FedTruth's efficacy in mitigating the impacts of poisoned updates
from both Byzantine and backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10251">UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets. (arXiv:2311.10251v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1">Can Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Shao_S/0/1/0/all/0/1">Sheng Shao</a>, <a href="http://arxiv.org/find/eess/1/au:+Qu_J/0/1/0/all/0/1">Junyi Qu</a>, <a href="http://arxiv.org/find/eess/1/au:+Pang_S/0/1/0/all/0/1">Shuchao Pang</a>, <a href="http://arxiv.org/find/eess/1/au:+Orgun_M/0/1/0/all/0/1">Mehmet A. Orgun</a></p>
<p>Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10255">FREE: The Foundational Semantic Recognition for Modeling Environmental Ecosystems. (arXiv:2311.10255v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Shiyuan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1">Juntong Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shengyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Runlong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yiqun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Licheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhenong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaowei Jia</a></p>
<p>Modeling environmental ecosystems is critical for the sustainability of our
planet, but is extremely challenging due to the complex underlying processes
driven by interactions amongst a large number of physical variables. As many
variables are difficult to measure at large scales, existing works often
utilize a combination of observable features and locally available measurements
or modeled values as input to build models for a specific study region and time
period. This raises a fundamental question in advancing the modeling of
environmental ecosystems: how to build a general framework for modeling the
complex relationships amongst various environmental data over space and time?
In this paper, we introduce a new framework, FREE, which maps available
environmental data into a text space and then converts the traditional
predictive modeling task in environmental science to the semantic recognition
problem. The proposed FREE framework leverages recent advances in Large
Language Models (LLMs) to supplement the original input features with natural
language descriptions. This facilitates capturing the data semantics and also
allows harnessing the irregularities of input features. When used for long-term
prediction, FREE has the flexibility to incorporate newly collected
observations to enhance future prediction. The efficacy of FREE is evaluated in
the context of two societally important real-world applications, predicting
stream water temperature in the Delaware River Basin and predicting annual corn
yield in Illinois and Iowa. Beyond the superior predictive performance over
multiple baseline methods, FREE is shown to be more data- and
computation-efficient as it can be pre-trained on simulated data generated by
physics-based models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10263">Stable Differentiable Causal Discovery. (arXiv:2311.10263v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nazaret_A/0/1/0/all/0/1">Achille Nazaret</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Justin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizi_E/0/1/0/all/0/1">Elham Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1">David Blei</a></p>
<p>Inferring causal relationships as directed acyclic graphs (DAGs) is an
important but challenging problem. Differentiable Causal Discovery (DCD) is a
promising approach to this problem, framing the search as a continuous
optimization. But existing DCD methods are numerically unstable, with poor
performance beyond tens of variables. In this paper, we propose Stable
Differentiable Causal Discovery (SDCD), a new method that improves previous DCD
methods in two ways: (1) It employs an alternative constraint for acyclicity;
this constraint is more stable, both theoretically and empirically, and fast to
compute. (2) It uses a training procedure tailored for sparse causal graphs,
which are common in real-world scenarios. We first derive SDCD and prove its
stability and correctness. We then evaluate it with both observational and
interventional data and on both small-scale and large-scale settings. We find
that SDCD outperforms existing methods in both convergence speed and accuracy
and can scale to thousands of variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10267">Energy and Carbon Considerations of Fine-Tuning BERT. (arXiv:2311.10267v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaorong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_C/0/1/0/all/0/1">Clara Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedler_S/0/1/0/all/0/1">Sorelle Friedler</a>, <a href="http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1">Sasha Luccioni</a></p>
<p>Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP
community, existing work quantifying energy costs and associated carbon
emissions has largely focused on language model pre-training. Although a single
pre-training run draws substantially more energy than fine-tuning, fine-tuning
is performed more frequently by many more individual actors, and thus must be
accounted for when considering the energy and carbon footprint of NLP. In order
to better characterize the role of fine-tuning in the landscape of energy and
carbon emissions in NLP, we perform a careful empirical study of the
computational costs of fine-tuning across tasks, datasets, hardware
infrastructure and measurement modalities. Our experimental results allow us to
place fine-tuning energy and carbon costs into perspective with respect to
pre-training and inference, and outline recommendations to NLP researchers and
practitioners who wish to improve their fine-tuning energy efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10270">Multiscale Hodge Scattering Networks for Data Analysis. (arXiv:2311.10270v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saito_N/0/1/0/all/0/1">Naoki Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonsheck_S/0/1/0/all/0/1">Stefan C. Schonsheck</a>, <a href="http://arxiv.org/find/cs/1/au:+Shvarts_E/0/1/0/all/0/1">Eugene Shvarts</a></p>
<p>We propose new scattering networks for signals measured on simplicial
complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs).
Our construction is based on multiscale basis dictionaries on simplicial
complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently
developed for simplices of dimension $\kappa \in \N$ in a given simplicial
complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT)
and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and
the $\kk$-HGLET both form redundant sets (i.e., dictionaries) of multiscale
basis vectors and the corresponding expansion coefficients of a given signal.
Our MHSNs use a layered structure analogous to a convolutional neural network
(CNN) to cascade the moments of the modulus of the dictionary coefficients. The
resulting features are invariant to reordering of the simplices (i.e., node
permutation of the underlying graphs). Importantly, the use of multiscale basis
dictionaries in our MHSNs admits a natural pooling operation that is akin to
local pooling in CNNs, and which may be performed either locally or per-scale.
These pooling operations are harder to define in both traditional scattering
networks based on Morlet wavelets, and geometric scattering networks based on
Diffusion Wavelets. As a result, we are able to extract a rich set of
descriptive yet robust features that can be used along with very simple machine
learning methods (i.e., logistic regression or support vector machines) to
achieve high-accuracy classification systems with far fewer parameters to train
than most modern graph neural networks. Finally, we demonstrate the usefulness
of our MHSNs in three distinct types of problems: signal classification, domain
(i.e., graph/simplex) classification, and molecular dynamics prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10277">Sobol Sequence Optimization for Hardware-Efficient Vector Symbolic Architectures. (arXiv:2311.10277v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aygun_S/0/1/0/all/0/1">Sercan Aygun</a>, <a href="http://arxiv.org/find/cs/1/au:+Najafi_M/0/1/0/all/0/1">M. Hassan Najafi</a></p>
<p>Hyperdimensional computing (HDC) is an emerging computing paradigm with
significant promise for efficient and robust learning. In HDC, objects are
encoded with high-dimensional vector symbolic sequences called hypervectors.
The quality of hypervectors, defined by their distribution and independence,
directly impacts the performance of HDC systems. Despite a large body of work
on the processing parts of HDC systems, little to no attention has been paid to
data encoding and the quality of hypervectors. Most prior studies have
generated hypervectors using inherent random functions, such as MATLAB`s or
Python`s random function. This work introduces an optimization technique for
generating hypervectors by employing quasi-random sequences. These sequences
have recently demonstrated their effectiveness in achieving accurate and
low-discrepancy data encoding in stochastic computing systems. The study
outlines the optimization steps for utilizing Sobol sequences to produce
high-quality hypervectors in HDC systems. An optimization algorithm is proposed
to select the most suitable Sobol sequences for generating minimally correlated
hypervectors, particularly in applications related to symbol-oriented
architectures. The performance of the proposed technique is evaluated in
comparison to two traditional approaches of generating hypervectors based on
linear-feedback shift registers and MATLAB random function. The evaluation is
conducted for two applications: (i) language and (ii) headline classification.
Our experimental results demonstrate accuracy improvements of up to 10.79%,
depending on the vector size. Additionally, the proposed encoding hardware
exhibits reduced energy consumption and a superior area-delay product.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10278">Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint. (arXiv:2311.10278v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yongchao Chen</a></p>
<p>Human fingerprints serve as one unique and powerful characteristic for each
person, from which policemen can recognize the identity. Similar to humans,
many natural bodies and intrinsic mechanical qualities can also be uniquely
identified from surface characteristics. To measure the elasto-plastic
properties of one material, one formally sharp indenter is pushed into the
measured body under constant force and retracted, leaving a unique residual
imprint of the minute size from several micrometers to nanometers. However, one
great challenge is how to map the optical image of this residual imprint into
the real wanted mechanical properties, i.e., the tensile force curve. In this
paper, we propose a novel method to use multi-fidelity neural networks (MFNN)
to solve this inverse problem. We first actively train the NN model via pure
simulation data, and then bridge the sim-to-real gap via transfer learning. The
most innovative part is that we use NN to dig out the unknown physics and also
implant the known physics into the transfer learning framework, thus highly
improving the model stability and decreasing the data requirement. This work
serves as one great example of applying machine learning into the real
experimental research, especially under the constraints of data limitation and
fidelity variance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10291">Leveraging Function Space Aggregation for Federated Learning at Scale. (arXiv:2311.10291v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhawan_N/0/1/0/all/0/1">Nikita Dhawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_N/0/1/0/all/0/1">Nicole Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1">Zachary Charles</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1">Zachary Garrett</a>, <a href="http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1">Gintare Karolina Dziugaite</a></p>
<p>The federated learning paradigm has motivated the development of methods for
aggregating multiple client updates into a global server model, without sharing
client data. Many federated learning algorithms, including the canonical
Federated Averaging (FedAvg), take a direct (possibly weighted) average of the
client parameter updates, motivated by results in distributed optimization. In
this work, we adopt a function space perspective and propose a new algorithm,
FedFish, that aggregates local approximations to the functions learned by
clients, using an estimate based on their Fisher information. We evaluate
FedFish on realistic, large-scale cross-device benchmarks. While the
performance of FedAvg can suffer as client models drift further apart, we
demonstrate that FedFish is more robust to longer local training. Our
evaluation across several settings in image and language benchmarks shows that
FedFish outperforms FedAvg as local training epochs increase. Further, FedFish
results in global networks that are more amenable to efficient personalization
via local fine-tuning on the same or shifted data distributions. For instance,
federated pretraining on the C4 dataset, followed by few-shot personalization
on Stack Overflow, results in a 7% improvement in next-token prediction by
FedFish over FedAvg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10293">Hierarchical Pruning of Deep Ensembles with Focal Diversity. (arXiv:2311.10293v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yanzhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chow_K/0/1/0/all/0/1">Ka-Ho Chow</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wenqi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Ling Liu</a></p>
<p>Deep neural network ensembles combine the wisdom of multiple deep neural
networks to improve the generalizability and robustness over individual
networks. It has gained increasing popularity to study deep ensemble techniques
in the deep learning community. Some mission-critical applications utilize a
large number of deep neural networks to form deep ensembles to achieve desired
accuracy and resilience, which introduces high time and space costs for
ensemble execution. However, it still remains a critical challenge whether a
small subset of the entire deep ensemble can achieve the same or better
generalizability and how to effectively identify these small deep ensembles for
improving the space and time efficiency of ensemble execution. This paper
presents a novel deep ensemble pruning approach, which can efficiently identify
smaller deep ensembles and provide higher ensemble accuracy than the entire
deep ensemble of a large number of member networks. Our hierarchical ensemble
pruning approach (HQ) leverages three novel ensemble pruning techniques. First,
we show that the focal diversity metrics can accurately capture the
complementary capacity of the member networks of an ensemble, which can guide
ensemble pruning. Second, we design a focal diversity based hierarchical
pruning approach, which will iteratively find high quality deep ensembles with
low cost and high accuracy. Third, we develop a focal diversity consensus
method to integrate multiple focal diversity metrics to refine ensemble pruning
results, where smaller deep ensembles can be effectively identified to offer
high accuracy, high robustness and high efficiency. Evaluated using popular
benchmark datasets, we demonstrate that the proposed hierarchical ensemble
pruning approach can effectively identify high quality deep ensembles with
better generalizability while being more time and space efficient in ensemble
decision making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10300">Supervised structure learning. (arXiv:2311.10300v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Friston_K/0/1/0/all/0/1">Karl J. Friston</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1">Lancelot Da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1">Alexander Tschantz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiefer_A/0/1/0/all/0/1">Alex Kiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvatori_T/0/1/0/all/0/1">Tommaso Salvatori</a>, <a href="http://arxiv.org/find/cs/1/au:+Neacsu_V/0/1/0/all/0/1">Victorita Neacsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Koudahl_M/0/1/0/all/0/1">Magnus Koudahl</a>, <a href="http://arxiv.org/find/cs/1/au:+Heins_C/0/1/0/all/0/1">Conor Heins</a>, <a href="http://arxiv.org/find/cs/1/au:+Sajid_N/0/1/0/all/0/1">Noor Sajid</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1">Dimitrije Markovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Parr_T/0/1/0/all/0/1">Thomas Parr</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1">Tim Verbelen</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1">Christopher L Buckley</a></p>
<p>This paper concerns structure learning or discovery of discrete generative
models. It focuses on Bayesian model selection and the assimilation of training
data or content, with a special emphasis on the order in which data are
ingested. A key move - in the ensuing schemes - is to place priors on the
selection of models, based upon expected free energy. In this setting, expected
free energy reduces to a constrained mutual information, where the constraints
inherit from priors over outcomes (i.e., preferred outcomes). The resulting
scheme is first used to perform image classification on the MNIST dataset to
illustrate the basic idea, and then tested on a more challenging problem of
discovering models with dynamics, using a simple sprite-based visual
disentanglement paradigm and the Tower of Hanoi (cf., blocks world) problem. In
these examples, generative models are constructed autodidactically to recover
(i.e., disentangle) the factorial structure of latent states - and their
characteristic paths or dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10306">MPSeg : Multi-Phase strategy for coronary artery Segmentation. (arXiv:2311.10306v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ku_J/0/1/0/all/0/1">Jonghoe Ku</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1">Yong-Hee Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Shin_J/0/1/0/all/0/1">Junsup Shin</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_I/0/1/0/all/0/1">In Kyu Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1">Hyun-Woo Kim</a></p>
<p>Accurate segmentation of coronary arteries is a pivotal process in assessing
cardiovascular diseases. However, the intricate structure of the cardiovascular
system presents significant challenges for automatic segmentation, especially
when utilizing methodologies like the SYNTAX Score, which relies extensively on
detailed structural information for precise risk stratification. To address
these difficulties and cater to this need, we present MPSeg, an innovative
multi-phase strategy designed for coronary artery segmentation. Our approach
specifically accommodates these structural complexities and adheres to the
principles of the SYNTAX Score. Initially, our method segregates vessels into
two categories based on their unique morphological characteristics: Left
Coronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble
models are then deployed for each category to execute the challenging
segmentation task. Due to LCA's higher complexity over RCA, a refinement model
is utilized to scrutinize and correct initial class predictions on segmented
areas. Notably, our approach demonstrated exceptional effectiveness when
evaluated in the Automatic Region-based Coronary Artery Disease diagnostics
using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm
challenge at MICCAI 2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10309">Imagination-augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments. (arXiv:2311.10309v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sang-Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1">Yoonjae Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1">Seung-Woo Seo</a></p>
<p>Hierarchical reinforcement learning (HRL) has led to remarkable achievements
in diverse fields. However, existing HRL algorithms still cannot be applied to
real-world navigation tasks. These tasks require an agent to perform
safety-aware behaviors and interact with surrounding objects in dynamic
environments. In addition, an agent in these tasks should perform consistent
and structured exploration as they are long-horizon and have complex structures
with diverse objects and task-specific rules. Designing HRL agents that can
handle these challenges in real-world navigation tasks is an open problem. In
this paper, we propose imagination-augmented HRL (IAHRL), a new and general
navigation algorithm that allows an agent to learn safe and interactive
behaviors in real-world navigation tasks. Our key idea is to train a
hierarchical agent in which a high-level policy infers interactions by
interpreting behaviors imagined with low-level policies. Specifically, the
high-level policy is designed with a permutation-invariant attention mechanism
to determine which low-level policy generates the most interactive behavior,
and the low-level policies are implemented with an optimization-based behavior
planner to generate safe and structured behaviors following task-specific
rules. To evaluate our algorithm, we introduce five complex urban driving
tasks, which are among the most challenging real-world navigation tasks. The
experimental results indicate that our hierarchical agent performs safety-aware
behaviors and properly interacts with surrounding vehicles, achieving higher
success rates and lower average episode steps than baselines in urban driving
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10315">Interpretable Modeling of Single-cell perturbation Responses to Novel Drugs Using Cycle Consistence Learning. (arXiv:2311.10315v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhu_A/0/1/0/all/0/1">Aichun Zhu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a></p>
<p>Phenotype-based screening has attracted much attention for identifying
cell-active compounds. Transcriptional and proteomic profiles of cell
population or single cells are informative phenotypic measures of cellular
responses to perturbations. In this paper, we proposed a deep learning
framework based on encoder-decoder architecture that maps the initial cellular
states to a latent space, in which we assume the effects of drug perturbation
on cellular states follow linear additivity. Next, we introduced the cycle
consistency constraints to enforce that initial cellular state subjected to
drug perturbations would produce the perturbed cellular responses, and,
conversely, removal of drug perturbation from the perturbed cellular states
would restore the initial cellular states. The cycle consistency constraints
and linear modeling in latent space enable to learn interpretable and
transferable drug perturbation representations, so that our model can predict
cellular response to unseen drugs. We validated our model on three different
types of datasets, including bulk transcriptional responses, bulk proteomic
responses, and single-cell transcriptional responses to drug perturbations. The
experimental results show that our model achieves better performance than
existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10316">Graph Sparsifications using Neural Network Assisted Monte Carlo Tree Search. (arXiv:2311.10316v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiu_A/0/1/0/all/0/1">Alvin Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1">Mithun Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1">Reyan Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1">Kwang-Sung Jun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1">Stephen Kobourov</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodrich_M/0/1/0/all/0/1">Michael T. Goodrich</a></p>
<p>Graph neural networks have been successful for machine learning, as well as
for combinatorial and graph problems such as the Subgraph Isomorphism Problem
and the Traveling Salesman Problem. We describe an approach for computing graph
sparsifiers by combining a graph neural network and Monte Carlo Tree Search. We
first train a graph neural network that takes as input a partial solution and
proposes a new node to be added as output. This neural network is then used in
a Monte Carlo search to compute a sparsifier. The proposed method consistently
outperforms several standard approximation algorithms on different types of
graphs and often finds the optimal solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10318">Nonparametric Teaching for Multiple Learners. (arXiv:2311.10318v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xiaofeng Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ivor Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1">James Kwok</a></p>
<p>We study the problem of teaching multiple learners simultaneously in the
nonparametric iterative teaching setting, where the teacher iteratively
provides examples to the learner for accelerating the acquisition of a target
concept. This problem is motivated by the gap between current single-learner
teaching setting and the real-world scenario of human instruction where a
teacher typically imparts knowledge to multiple students. Under the new problem
formulation, we introduce a novel framework -- Multi-learner Nonparametric
Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with
each learner focusing on learning a scalar-valued target model. To achieve
this, we frame the problem as teaching a vector-valued target model and extend
the target model space from a scalar-valued reproducing kernel Hilbert space
used in single-learner scenarios to a vector-valued space. Furthermore, we
demonstrate that MINT offers significant teaching speed-up over repeated
single-learner teaching, particularly when the multiple learners can
communicate with each other. Lastly, we conduct extensive experiments to
validate the practicality and efficiency of MINT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10321">Towards Machine Learning-based Quantitative Hyperspectral Image Guidance for Brain Tumor Resection. (arXiv:2311.10321v1 [q-bio.TO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Black_D/0/1/0/all/0/1">David Black</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Byrne_D/0/1/0/all/0/1">Declan Byrne</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Walke_A/0/1/0/all/0/1">Anna Walke</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_S/0/1/0/all/0/1">Sidong Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+leva_A/0/1/0/all/0/1">Antonio Di leva</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kaneko_S/0/1/0/all/0/1">Sadahiro Kaneko</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stummer_W/0/1/0/all/0/1">Walter Stummer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Salcudean_S/0/1/0/all/0/1">Septimiu Salcudean</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Molina_E/0/1/0/all/0/1">Eric Suero Molina</a></p>
<p>Complete resection of malignant gliomas is hampered by the difficulty in
distinguishing tumor cells at the infiltration zone. Fluorescence guidance with
5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work
characterized five fluorophores' emission spectra in most human brain tumors.
In this paper, the effectiveness of these five spectra was explored for
different tumor and tissue classification tasks in 184 patients (891
hyperspectral measurements) harboring low- (n=30) and high-grade gliomas
(n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2),
miscellaneous (n=10) and metastases (n=8). Four machine learning models were
trained to classify tumor type, grade, glioma margins and IDH mutation. Using
random forests and multi-layer perceptrons, the classifiers achieved average
test accuracies of 74-82%, 79%, 81%, and 93% respectively. All five fluorophore
abundances varied between tumor margin types and tumor grades (p &lt; 0.01). For
tissue type, at least four of the five fluorophore abundances were found to be
significantly different (p &lt; 0.01) between all classes. These results
demonstrate the fluorophores' differing abundances in different tissue classes,
as well as the value of the five fluorophores as potential optical biomarkers,
opening new opportunities for intraoperative classification systems in
fluorescence-guided neurosurgery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10322">Clustering Techniques for Stable Linear Dynamical Systems with applications to Hard Disk Drives. (arXiv:2311.10322v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Prakash_N/0/1/0/all/0/1">Nikhil Potu Surya Prakash</a>, <a href="http://arxiv.org/find/eess/1/au:+Seo_J/0/1/0/all/0/1">Joohwan Seo</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1">Jongeun Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Horowitz_R/0/1/0/all/0/1">Roberto Horowitz</a></p>
<p>In Robust Control and Data Driven Robust Control design methodologies,
multiple plant transfer functions or a family of transfer functions are
considered and a common controller is designed such that all the plants that
fall into this family are stabilized. Though the plants are stabilized, the
controller might be sub-optimal for each of the plants when the variations in
the plants are large. This paper presents a way of clustering stable linear
dynamical systems for the design of robust controllers within each of the
clusters such that the controllers are optimal for each of the clusters. First
a k-medoids algorithm for hard clustering will be presented for stable Linear
Time Invariant (LTI) systems and then a Gaussian Mixture Models (GMM)
clustering for a special class of LTI systems, common for Hard Disk Drive
plants, will be presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10328">TransONet: Automatic Segmentation of Vasculature in Computed Tomographic Angiograms Using Deep Learning. (arXiv:2311.10328v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rajeoni_A/0/1/0/all/0/1">Alireza Bagheri Rajeoni</a>, <a href="http://arxiv.org/find/eess/1/au:+Pederson_B/0/1/0/all/0/1">Breanna Pederson</a>, <a href="http://arxiv.org/find/eess/1/au:+Firooz_A/0/1/0/all/0/1">Ali Firooz</a>, <a href="http://arxiv.org/find/eess/1/au:+Abdollahi_H/0/1/0/all/0/1">Hamed Abdollahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Smith_A/0/1/0/all/0/1">Andrew K. Smith</a>, <a href="http://arxiv.org/find/eess/1/au:+Clair_D/0/1/0/all/0/1">Daniel G. Clair</a>, <a href="http://arxiv.org/find/eess/1/au:+Lessner_S/0/1/0/all/0/1">Susan M. Lessner</a>, <a href="http://arxiv.org/find/eess/1/au:+Valafar_H/0/1/0/all/0/1">Homayoun Valafar</a></p>
<p>Pathological alterations in the human vascular system underlie many chronic
diseases, such as atherosclerosis and aneurysms. However, manually analyzing
diagnostic images of the vascular system, such as computed tomographic
angiograms (CTAs) is a time-consuming and tedious process. To address this
issue, we propose a deep learning model to segment the vascular system in CTA
images of patients undergoing surgery for peripheral arterial disease (PAD).
Our study focused on accurately segmenting the vascular system (1) from the
descending thoracic aorta to the iliac bifurcation and (2) from the descending
thoracic aorta to the knees in CTA images using deep learning techniques. Our
approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset
for (1) and (2), respectively, highlighting its high accuracy and potential
clinical utility. These findings demonstrate the use of deep learning
techniques as a valuable tool for medical professionals to analyze the health
of the vascular system efficiently and accurately. Please visit the GitHub page
for this paper at https://github.com/pip-alireza/TransOnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10341">Federated Knowledge Graph Completion via Latent Embedding Sharing and Tensor Factorization. (arXiv:2311.10341v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Maolin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Dun Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zenglin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Ruocheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a></p>
<p>Knowledge graphs (KGs), which consist of triples, are inherently incomplete
and always require completion procedure to predict missing triples. In
real-world scenarios, KGs are distributed across clients, complicating
completion tasks due to privacy restrictions. Many frameworks have been
proposed to address the issue of federated knowledge graph completion. However,
the existing frameworks, including FedE, FedR, and FEKG, have certain
limitations. = FedE poses a risk of information leakage, FedR's optimization
efficacy diminishes when there is minimal overlap among relations, and FKGE
suffers from computational costs and mode collapse issues. To address these
issues, we propose a novel method, i.e., Federated Latent Embedding Sharing
Tensor factorization (FLEST), which is a novel approach using federated tensor
factorization for KG completion. FLEST decompose the embedding matrix and
enables sharing of latent dictionary embeddings to lower privacy risks.
Empirical results demonstrate FLEST's effectiveness and efficiency, offering a
balanced solution between performance and privacy. FLEST expands the
application of federated tensor factorization in KG completion tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10349">Pseudo Label-Guided Data Fusion and Output Consistency for Semi-Supervised Medical Image Segmentation. (arXiv:2311.10349v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yuanbin Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xinlin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yuanbo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Lan_J/0/1/0/all/0/1">Junlin Lan</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_B/0/1/0/all/0/1">Bizhe Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1">Tao Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Du_M/0/1/0/all/0/1">Min Du</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1">Qinquan Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Tong_T/0/1/0/all/0/1">Tong Tong</a></p>
<p>Supervised learning algorithms based on Convolutional Neural Networks have
become the benchmark for medical image segmentation tasks, but their
effectiveness heavily relies on a large amount of labeled data. However,
annotating medical image datasets is a laborious and time-consuming process.
Inspired by semi-supervised algorithms that use both labeled and unlabeled data
for training, we propose the PLGDF framework, which builds upon the mean
teacher network for segmenting medical images with less annotation. We propose
a novel pseudo-label utilization scheme, which combines labeled and unlabeled
data to augment the dataset effectively. Additionally, we enforce the
consistency between different scales in the decoder module of the segmentation
network and propose a loss function suitable for evaluating the consistency.
Moreover, we incorporate a sharpening operation on the predicted results,
further enhancing the accuracy of the segmentation.
</p>
<p>Extensive experiments on three publicly available datasets demonstrate that
the PLGDF framework can largely improve performance by incorporating the
unlabeled data. Meanwhile, our framework yields superior performance compared
to six state-of-the-art semi-supervised learning methods. The codes of this
study are available at https://github.com/ortonwang/PLGDF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10359">FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenqing Wu</a></p>
<p>Highly parallelized workloads like machine learning training, inferences and
general HPC tasks are greatly accelerated using GPU devices. In a cloud
computing cluster, serving a GPU's computation power through multi-tasks
sharing is highly demanded since there are always more task requests than the
number of GPU available. Existing GPU sharing solutions focus on reducing
task-level waiting time or task-level switching costs when multiple jobs
competing for a single GPU. Non-stopped computation requests come with
different priorities, having non-symmetric impact on QoS for sharing a GPU
device. Existing work missed the kernel-level optimization opportunity brought
by this setting. To address this problem, we present a novel kernel-level
scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT
incorporates task-level priority information, fine-grained kernel
identification, and kernel measurement, allowing low priorities task's
execution during high priority task's inter-kernel idle time. Thereby, filling
the GPU's device runtime fully, and reduce overall GPU sharing impact to cloud
services. Across a set of ML models, the FIKIT based inference system
accelerated high priority tasks by 1.33 to 14.87 times compared to the JCT in
GPU sharing mode, and more than half of the cases are accelerated by more than
3.5 times. Alternatively, under preemptive sharing, the low-priority tasks have
a comparable to default GPU sharing mode JCT, with a 0.84 to 1 times ratio. We
further limit the kernel measurement and runtime fine-grained kernel scheduling
overhead to less than 10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10370">Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection. (arXiv:2311.10370v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Fan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xuezhi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Meiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chaoqun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xibin Zhao</a></p>
<p>Graph anomaly detection plays a crucial role in identifying exceptional
instances in graph data that deviate significantly from the majority. It has
gained substantial attention in various domains of information security,
including network intrusion, financial fraud, and malicious comments, et al.
Existing methods are primarily developed in an unsupervised manner due to the
challenge in obtaining labeled data. For lack of guidance from prior knowledge
in unsupervised manner, the identified anomalies may prove to be data noise or
individual data instances. In real-world scenarios, a limited batch of labeled
anomalies can be captured, making it crucial to investigate the few-shot
problem in graph anomaly detection. Taking advantage of this potential, we
propose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot
Message-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a
self-supervised contrastive learning strategy within and across views to
capture intrinsic and transferable structural representations. Furthermore, we
propose the Deep-GNN message-enhanced reconstruction module, which extensively
exploits the few-shot label information and enables long-range propagation to
disseminate supervision signals to deeper unlabeled nodes. This module in turn
assists in the training of self-supervised contrastive learning. Comprehensive
experimental results on six real-world datasets demonstrate that FMGAD can
achieve better performance than other state-of-the-art methods, regardless of
artificially injected anomalies or domain-organic anomalies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10385">Delete My Account: Impact of Data Deletion on Machine Learning Classifiers. (arXiv:2311.10385v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1">Tobias Dam</a>, <a href="http://arxiv.org/find/cs/1/au:+Henzl_M/0/1/0/all/0/1">Maximilian Henzl</a>, <a href="http://arxiv.org/find/cs/1/au:+Klausner_L/0/1/0/all/0/1">Lukas Daniel Klausner</a></p>
<p>Users are more aware than ever of the importance of their own data, thanks to
reports about security breaches and leaks of private, often sensitive data in
recent years. Additionally, the GDPR has been in effect in the European Union
for over three years and many people have encountered its effects in one way or
another. Consequently, more and more users are actively protecting their
personal data. One way to do this is to make of the right to erasure guaranteed
in the GDPR, which has potential implications for a number of different fields,
such as big data and machine learning.
</p>
<p>Our paper presents an in-depth analysis about the impact of the use of the
right to erasure on the performance of machine learning models on
classification tasks. We conduct various experiments utilising different
datasets as well as different machine learning algorithms to analyse a variety
of deletion behaviour scenarios. Due to the lack of credible data on actual
user behaviour, we make reasonable assumptions for various deletion modes and
biases and provide insight into the effects of different plausible scenarios
for right to erasure usage on data quality of machine learning. Our results
show that the impact depends strongly on the amount of data deleted, the
particular characteristics of the dataset and the bias chosen for deletion and
assumptions on user behaviour.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10387">A Bridge between Dynamical Systems and Machine Learning: Engineered Ordinary Differential Equations as Classification Algorithm (EODECA). (arXiv:2311.10387v1 [cond-mat.dis-nn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Marino_R/0/1/0/all/0/1">Raffaele Marino</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Giambagli_L/0/1/0/all/0/1">Lorenzo Giambagli</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Chicchi_L/0/1/0/all/0/1">Lorenzo Chicchi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Buffoni_L/0/1/0/all/0/1">Lorenzo Buffoni</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fanelli_D/0/1/0/all/0/1">Duccio Fanelli</a></p>
<p>In a world increasingly reliant on machine learning, the interpretability of
these models remains a substantial challenge, with many equating their
functionality to an enigmatic black box. This study seeks to bridge machine
learning and dynamical systems. Recognizing the deep parallels between dense
neural networks and dynamical systems, particularly in the light of
non-linearities and successive transformations, this manuscript introduces the
Engineered Ordinary Differential Equations as Classification Algorithms
(EODECAs). Uniquely designed as neural networks underpinned by continuous
ordinary differential equations, EODECAs aim to capitalize on the
well-established toolkit of dynamical systems. Unlike traditional deep learning
models, which often suffer from opacity, EODECAs promise both high
classification performance and intrinsic interpretability. They are naturally
invertible, granting them an edge in understanding and transparency over their
counterparts. By bridging these domains, we hope to usher in a new era of
machine learning models where genuine comprehension of data processes
complements predictive prowess.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10406">Decentralized Energy Marketplace via NFTs and AI-based Agents. (arXiv:2311.10406v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikbakht_R/0/1/0/all/0/1">Rasoul Nikbakht</a>, <a href="http://arxiv.org/find/cs/1/au:+Javed_F/0/1/0/all/0/1">Farhana Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezazadeh_F/0/1/0/all/0/1">Farhad Rezazadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartzoudis_N/0/1/0/all/0/1">Nikolaos Bartzoudis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangues_Bafalluy_J/0/1/0/all/0/1">Josep Mangues-Bafalluy</a></p>
<p>The paper introduces an advanced Decentralized Energy Marketplace (DEM)
integrating blockchain technology and artificial intelligence to manage energy
exchanges among smart homes with energy storage systems. The proposed framework
uses Non-Fungible Tokens (NFTs) to represent unique energy profiles in a
transparent and secure trading environment. Leveraging Federated Deep
Reinforcement Learning (FDRL), the system promotes collaborative and adaptive
energy management strategies, maintaining user privacy. A notable innovation is
the use of smart contracts, ensuring high efficiency and integrity in energy
transactions. Extensive evaluations demonstrate the system's scalability and
the effectiveness of the FDRL method in optimizing energy distribution. This
research significantly contributes to developing sophisticated decentralized
smart grid infrastructures. Our approach broadens potential blockchain and AI
applications in sustainable energy systems and addresses incentive alignment
and transparency challenges in traditional energy trading mechanisms. The
implementation of this paper is publicly accessible at
\url{https://github.com/RasoulNik/DEM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10418">DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines. (arXiv:2311.10418v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhen Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yida Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chuan Wu</a></p>
<p>Multi-task model training has been adopted to enable a single deep neural
network model (often a large language model) to handle multiple tasks (e.g.,
question answering and text summarization). Multi-task training commonly
receives input sequences of highly different lengths due to the diverse
contexts of different tasks. Padding (to the same sequence length) or packing
(short examples into long sequences of the same length) is usually adopted to
prepare input samples for model training, which is nonetheless not space or
computation efficient. This paper proposes a dynamic micro-batching approach to
tackle sequence length variation and enable efficient multi-task model
training. We advocate pipeline-parallel training of the large model with
variable-length micro-batches, each of which potentially comprises a different
number of samples. We optimize micro-batch construction using a dynamic
programming-based approach, and handle micro-batch execution time variation
through dynamic pipeline and communication scheduling, enabling highly
efficient pipeline training. Extensive evaluation on the FLANv2 dataset
demonstrates up to 4.39x higher training throughput when training T5, and 3.25x
when training GPT, as compared with packing-based baselines. DynaPipe's source
code is publicly available at
https://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10421">Maintenance Techniques for Anomaly Detection AIOps Solutions. (arXiv:2311.10421v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poenaru_Olaru_L/0/1/0/all/0/1">Lorena Poenaru-Olaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Karpova_N/0/1/0/all/0/1">Natalia Karpova</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1">Luis Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rellermeyer_J/0/1/0/all/0/1">Jan Rellermeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1">Arie van Deursen</a></p>
<p>Anomaly detection techniques are essential in automating the monitoring of IT
systems and operations. These techniques imply that machine learning algorithms
are trained on operational data corresponding to a specific period of time and
that they are continuously evaluated on newly emerging data. Operational data
is constantly changing over time, which affects the performance of deployed
anomaly detection models. Therefore, continuous model maintenance is required
to preserve the performance of anomaly detectors over time. In this work, we
analyze two different anomaly detection model maintenance techniques in terms
of the model update frequency, namely blind model retraining and informed model
retraining. We further investigate the effects of updating the model by
retraining it on all the available data (full-history approach) and on only the
newest data (sliding window approach). Moreover, we investigate whether a data
change monitoring tool is capable of determining when the anomaly detection
model needs to be updated through retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10430">Deep Residual CNN for Multi-Class Chest Infection Diagnosis. (arXiv:2311.10430v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kwon_R/0/1/0/all/0/1">Ryan Donghan Kwon</a>, <a href="http://arxiv.org/find/eess/1/au:+Lim_D/0/1/0/all/0/1">Dohyun Lim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1">Yoonha Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seung Won Lee</a></p>
<p>The advent of deep learning has significantly propelled the capabilities of
automated medical image diagnosis, providing valuable tools and resources in
the realm of healthcare and medical diagnostics. This research delves into the
development and evaluation of a Deep Residual Convolutional Neural Network
(CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray
images. The implemented model, trained and validated on a dataset amalgamated
from diverse sources, demonstrated a robust overall accuracy of 93%. However,
nuanced disparities in performance across different classes, particularly
Fibrosis, underscored the complexity and challenges inherent in automated
medical image diagnosis. The insights derived pave the way for future research,
focusing on enhancing the model's proficiency in classifying conditions that
present more subtle and nuanced visual features in the images, as well as
optimizing and refining the model architecture and training process. This paper
provides a comprehensive exploration into the development, implementation, and
evaluation of the model, offering insights and directions for future research
and development in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10448">DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal. (arXiv:2311.10448v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiaeli Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghalyan_N/0/1/0/all/0/1">Najah Ghalyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gourgoulias_K/0/1/0/all/0/1">Kostis Gourgoulias</a>, <a href="http://arxiv.org/find/cs/1/au:+Buford_J/0/1/0/all/0/1">John Buford</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Sean Moran</a></p>
<p>Machine learning models trained on sensitive or private data can
inadvertently memorize and leak that information. Machine unlearning seeks to
retroactively remove such details from model weights to protect privacy. We
contribute a lightweight unlearning algorithm that leverages the Fisher
Information Matrix (FIM) for selective forgetting. Prior work in this area
requires full retraining or large matrix inversions, which are computationally
expensive. Our key insight is that the diagonal elements of the FIM, which
measure the sensitivity of log-likelihood to changes in weights, contain
sufficient information for effective forgetting. Specifically, we compute the
FIM diagonal over two subsets -- the data to retain and forget -- for all
trainable weights. This diagonal representation approximates the complete FIM
while dramatically reducing computation. We then use it to selectively update
weights to maximize forgetting of the sensitive subset while minimizing impact
on the retained subset. Experiments show that our algorithm can successfully
forget any randomly selected subsets of training data across neural network
architectures. By leveraging the FIM diagonal, our approach provides an
interpretable, lightweight, and efficient solution for machine unlearning with
practical privacy benefits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10456">Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs. (arXiv:2311.10456v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1">Harshil Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_A/0/1/0/all/0/1">Aniruddha Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolaienko_T/0/1/0/all/0/1">Tymofii Nikolaienko</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaso_S/0/1/0/all/0/1">Stanislav Jaso</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1">Alejandro Lopez</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyanaraman_K/0/1/0/all/0/1">Kaushic Kalyanaraman</a></p>
<p>Microkinetics allows detailed modelling of chemical transformations occurring
in many industrially relevant reactions. Traditional way of solving the
microkinetics model for Fischer-Tropsch synthesis (FTS) becomes inefficient
when it comes to more advanced real-time applications. In this work, we address
these challenges by using physics-informed neural networks(PINNs) for modelling
FTS microkinetics. We propose a computationally efficient and accurate method,
enabling the ultra-fast solution of the existing microkinetics models in
realistic process conditions. The proposed PINN model computes the fraction of
vacant catalytic sites, a key quantity in FTS microkinetics, with median
relative error (MRE) of 0.03%, and the FTS product formation rates with MRE of
0.1%. Compared to conventional equation solvers, the model achieves up to 1E+06
times speed-up when running on GPUs, thus being fast enough for multi-scale and
multi-physics reactor modelling and enabling its applications in real-time
process control and optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10468">Using Cooperative Game Theory to Prune Neural Networks. (arXiv:2311.10468v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diaz_Ortiz_M/0/1/0/all/0/1">Mauricio Diaz-Ortiz Jr</a>, <a href="http://arxiv.org/find/cs/1/au:+Kempinski_B/0/1/0/all/0/1">Benjamin Kempinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Cornelisse_D/0/1/0/all/0/1">Daphne Cornelisse</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1">Yoram Bachrach</a>, <a href="http://arxiv.org/find/cs/1/au:+Kachman_T/0/1/0/all/0/1">Tal Kachman</a></p>
<p>We show how solution concepts from cooperative game theory can be used to
tackle the problem of pruning neural networks.
</p>
<p>The ever-growing size of deep neural networks (DNNs) increases their
performance, but also their computational requirements. We introduce a method
called Game Theory Assisted Pruning (GTAP), which reduces the neural network's
size while preserving its predictive accuracy. GTAP is based on eliminating
neurons in the network based on an estimation of their joint impact on the
prediction quality through game theoretic solutions. Specifically, we use a
power index akin to the Shapley value or Banzhaf index, tailored using a
procedure similar to Dropout (commonly used to tackle overfitting problems in
machine learning).
</p>
<p>Empirical evaluation of both feedforward networks and convolutional neural
networks shows that this method outperforms existing approaches in the achieved
tradeoff between the number of parameters and model accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10471">Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model Based on Human Mobility for Ubiquitous Urban Sensing. (arXiv:2311.10471v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruixing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Liangzhe Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Leilei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1">Weifeng Lv</a></p>
<p>User profiling and region analysis are two tasks of significant commercial
value. However, in practical applications, modeling different features
typically involves four main steps: data preparation, data processing, model
establishment, evaluation, and optimization. This process is time-consuming and
labor-intensive. Repeating this workflow for each feature results in abundant
development time for tasks and a reduced overall volume of task development.
Indeed, human mobility data contains a wealth of information. Several
successful cases suggest that conducting in-depth analysis of population
movement data could potentially yield meaningful profiles about users and
areas. Nonetheless, most related works have not thoroughly utilized the
semantic information within human mobility data and trained on a fixed number
of the regions. To tap into the rich information within population movement,
based on the perspective that Regions Are Who walk them, we propose a large
spatiotemporal model based on trajectories (RAW). It possesses the following
characteristics: 1) Tailored for trajectory data, introducing a GPT-like
structure with a parameter count of up to 1B; 2) Introducing a spatiotemporal
fine-tuning module, interpreting trajectories as collection of users to derive
arbitrary region embedding. This framework allows rapid task development based
on the large spatiotemporal model. We conducted extensive experiments to
validate the effectiveness of our proposed large spatiotemporal model. It's
evident that our proposed method, relying solely on human mobility data without
additional features, exhibits a certain level of relevance in user profiling
and region analysis. Moreover, our model showcases promising predictive
capabilities in trajectory generation tasks based on the current state,
offering the potential for further innovative work utilizing this large
spatiotemporal model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10489">Handling Overlapping Asymmetric Datasets -- A Twice Penalized P-Spline Approach. (arXiv:2311.10489v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+McTeer_M/0/1/0/all/0/1">Matthew McTeer</a>, <a href="http://arxiv.org/find/stat/1/au:+Henderson_R/0/1/0/all/0/1">Robin Henderson</a>, <a href="http://arxiv.org/find/stat/1/au:+Anstee_Q/0/1/0/all/0/1">Quentin M Anstee</a>, <a href="http://arxiv.org/find/stat/1/au:+Missier_P/0/1/0/all/0/1">Paolo Missier</a></p>
<p>Overlapping asymmetric datasets are common in data science and pose questions
of how they can be incorporated together into a predictive analysis. In
healthcare datasets there is often a small amount of information that is
available for a larger number of patients such as an electronic health record,
however a small number of patients may have had extensive further testing.
Common solutions such as missing imputation can often be unwise if the smaller
cohort is significantly different in scale to the larger sample, therefore the
aim of this research is to develop a new method which can model the smaller
cohort against a particular response, whilst considering the larger cohort
also. Motivated by non-parametric models, and specifically flexible smoothing
techniques via generalized additive models, we model a twice penalized P-Spline
approximation method to firstly prevent over/under-fitting of the smaller
cohort and secondly to consider the larger cohort. This second penalty is
created through discrepancies in the marginal value of covariates that exist in
both the smaller and larger cohorts. Through data simulations, parameter
tunings and model adaptations to consider a continuous and binary response, we
find our twice penalized approach offers an enhanced fit over a linear B-Spline
and once penalized P-Spline approximation. Applying to a real-life dataset
relating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see
an improved model fit performance of over 65%. Areas for future work within
this space include adapting our method to not require dimensionality reduction
and also consider parametric modelling methods. However, to our knowledge this
is the first work to propose additional marginal penalties in a flexible
regression of which we can report a vastly improved model fit that is able to
consider asymmetric datasets, without the need for missing data imputation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10500">From Principle to Practice: Vertical Data Minimization for Machine Learning. (arXiv:2311.10500v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Staab_R/0/1/0/all/0/1">Robin Staab</a>, <a href="http://arxiv.org/find/cs/1/au:+Jovanovic_N/0/1/0/all/0/1">Nikola Jovanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1">Mislav Balunovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a></p>
<p>Aiming to train and deploy predictive models, organizations collect large
amounts of detailed client data, risking the exposure of private information in
the event of a breach. To mitigate this, policymakers increasingly demand
compliance with the data minimization (DM) principle, restricting data
collection to only that data which is relevant and necessary for the task.
Despite regulatory pressure, the problem of deploying machine learning models
that obey DM has so far received little attention. In this work, we address
this challenge in a comprehensive manner. We propose a novel vertical DM (vDM)
workflow based on data generalization, which by design ensures that no
full-resolution client data is collected during training and deployment of
models, benefiting client privacy by reducing the attack surface in case of a
breach. We formalize and study the corresponding problem of finding
generalizations that both maximize data utility and minimize empirical privacy
risk, which we quantify by introducing a diverse set of policy-aligned
adversarial scenarios. Finally, we propose a range of baseline vDM algorithms,
as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that
outperforms all baselines across several settings. We plan to release our code
as a publicly available library, helping advance the standardization of DM for
machine learning. Overall, we believe our work can help lay the foundation for
further exploration and adoption of DM principles in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10512">Causal Fairness-Guided Dataset Reweighting using Neural Networks. (arXiv:2311.10512v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1">Klaus Broelemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1">Salvatore Ruggieri</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>The importance of achieving fairness in machine learning models cannot be
overstated. Recent research has pointed out that fairness should be examined
from a causal perspective, and several fairness notions based on the on Pearl's
causal framework have been proposed. In this paper, we construct a reweighting
scheme of datasets to address causal fairness. Our approach aims at mitigating
bias by considering the causal relationships among variables and incorporating
them into the reweighting process. The proposed method adopts two neural
networks, whose structures are intentionally used to reflect the structures of
a causal graph and of an interventional graph. The two neural networks can
approximate the causal model of the data, and the causal model of
interventions. Furthermore, reweighting guided by a discriminator is applied to
achieve various fairness notions. Experiments on real-world datasets show that
our method can achieve causal fairness on the data while remaining close to the
original data for downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10517">Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data. (arXiv:2311.10517v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">R&#xe9;my Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Li Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lingrand_D/0/1/0/all/0/1">Diane Lingrand</a>, <a href="http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;ric Precioso</a></p>
<p>Online High Definition Map (HDMap) estimation from sensors offers a low-cost
alternative to manually acquired HDMaps. As such, it promises to lighten costs
for already HDMap-reliant Autonomous Driving systems, and potentially even
spread their use to new systems. In this paper, we propose to improve online
HDMap estimation by accounting for already existing maps. We identify 3
reasonable types of useful existing maps (minimalist, noisy, and outdated). We
also introduce MapEX, a novel online HDMap estimation framework that accounts
for existing maps. MapEX achieves this by encoding map elements into query
tokens and by refining the matching algorithm used to train classic query based
map estimation models. We demonstrate that MapEX brings significant
improvements on the nuScenes dataset. For instance, MapEX - given noisy maps -
improves by 38% over the MapTRv2 detector it is based on and by 16% over the
current SOTA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10525">Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL. (arXiv:2311.10525v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guanhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Guoxi Sun</a></p>
<p>The prediction of the remaining useful life (RUL) of rolling bearings is a
pivotal issue in industrial production. A crucial approach to tackling this
issue involves transforming vibration signals into health indicators (HI) to
aid model training. This paper presents an end-to-end HI construction method,
vector quantised variational autoencoder (VQ-VAE), which addresses the need for
dimensionality reduction of latent variables in traditional unsupervised
learning methods such as autoencoder. Moreover, concerning the inadequacy of
traditional statistical metrics in reflecting curve fluctuations accurately,
two novel statistical metrics, mean absolute distance (MAD) and mean variance
(MV), are introduced. These metrics accurately depict the fluctuation patterns
in the curves, thereby indicating the model's accuracy in discerning similar
features. On the PMH2012 dataset, methods employing VQ-VAE for label
construction achieved lower values for MAD and MV. Furthermore, the ASTCN
prediction model trained with VQ-VAE labels demonstrated commendable
performance, attaining the lowest values for MAD and MV.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10550">RONAALP: Reduced-Order Nonlinear Approximation with Active Learning Procedure. (arXiv:2311.10550v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Scherding_C/0/1/0/all/0/1">Cl&#xe9;ment Scherding</a> (1), <a href="http://arxiv.org/find/physics/1/au:+Rigas_G/0/1/0/all/0/1">Georgios Rigas</a> (2), <a href="http://arxiv.org/find/physics/1/au:+Sipp_D/0/1/0/all/0/1">Denis Sipp</a> (3), <a href="http://arxiv.org/find/physics/1/au:+Schmid_P/0/1/0/all/0/1">Peter J Schmid</a> (4), <a href="http://arxiv.org/find/physics/1/au:+Sayadi_T/0/1/0/all/0/1">Taraneh Sayadi</a> (1 and 5) ((1) Institut Jean le Rond d&#x27;Alembert, Sorbonne University, (2) Department of Aeronautics, Imperial College London, (3) DAAA, Onera, (4) Department of Mechanical Engineering, KAUST, (5) Institute for Combustion Technology, Aachen University)</p>
<p>Many engineering applications rely on the evaluation of expensive, non-linear
high-dimensional functions. In this paper, we propose the RONAALP algorithm
(Reduced Order Nonlinear Approximation with Active Learning Procedure) to
incrementally learn a fast and accurate reduced-order surrogate model of a
target function on-the-fly as the application progresses. First, the
combination of nonlinear auto-encoder, community clustering and radial basis
function networks allows to learn an efficient and compact surrogate model with
limited training data. Secondly, the active learning procedure overcome any
extrapolation issue when evaluating the surrogate model outside of its initial
training range during the online stage. This results in generalizable, fast and
accurate reduced-order models of high-dimensional functions. The method is
demonstrated on three direct numerical simulations of hypersonic flows in
chemical nonequilibrium. Accurate simulations of these flows rely on detailed
thermochemical gas models that dramatically increase the cost of such
calculations. Using RONAALP to learn a reduced-order thermodynamic model
surrogate on-the-fly, the cost of such simulation was reduced by up to 75%
while maintaining an error of less than 10% on relevant quantities of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10571">Direct Amortized Likelihood Ratio Estimation. (arXiv:2311.10571v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cobb_A/0/1/0/all/0/1">Adam D. Cobb</a>, <a href="http://arxiv.org/find/stat/1/au:+Matejek_B/0/1/0/all/0/1">Brian Matejek</a>, <a href="http://arxiv.org/find/stat/1/au:+Elenius_D/0/1/0/all/0/1">Daniel Elenius</a>, <a href="http://arxiv.org/find/stat/1/au:+Roy_A/0/1/0/all/0/1">Anirban Roy</a>, <a href="http://arxiv.org/find/stat/1/au:+Jha_S/0/1/0/all/0/1">Susmit Jha</a></p>
<p>We introduce a new amortized likelihood ratio estimator for likelihood-free
simulation-based inference (SBI). Our estimator is simple to train and
estimates the likelihood ratio using a single forward pass of the neural
estimator. Our approach directly computes the likelihood ratio between two
competing parameter sets which is different from the previous approach of
comparing two neural network output values. We refer to our model as the direct
neural ratio estimator (DNRE). As part of introducing the DNRE, we derive a
corresponding Monte Carlo estimate of the posterior. We benchmark our new ratio
estimator and compare to previous ratio estimators in the literature. We show
that our new ratio estimator often outperforms these previous approaches. As a
further contribution, we introduce a new derivative estimator for likelihood
ratio estimators that enables us to compare likelihood-free Hamiltonian Monte
Carlo (HMC) with random-walk Metropolis-Hastings (MH). We show that HMC is
equally competitive, which has not been previously shown. Finally, we include a
novel real-world application of SBI by using our neural ratio estimator to
design a quadcopter. Code is available at https://github.com/SRI-CSL/dnre.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10572">SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning. (arXiv:2311.10572v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yue Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1">Anna Kukleva</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Dengxin Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1">Bernt Schiele</a></p>
<p>Semi-supervised learning (SSL) methods effectively leverage unlabeled data to
improve model generalization. However, SSL models often underperform in
open-set scenarios, where unlabeled data contain outliers from novel categories
that do not appear in the labeled set. In this paper, we study the challenging
and realistic open-set SSL setting, where the goal is to both correctly
classify inliers and to detect outliers. Intuitively, the inlier classifier
should be trained on inlier data only. However, we find that inlier
classification performance can be largely improved by incorporating
high-confidence pseudo-labeled data, regardless of whether they are inliers or
outliers. Also, we propose to utilize non-linear transformations to separate
the features used for inlier classification and outlier detection in the
multi-task learning framework, preventing adverse effects between them.
Additionally, we introduce pseudo-negative mining, which further boosts outlier
detection performance. The three ingredients lead to what we call Simple but
Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves
both inlier classification and outlier detection performance, outperforming
existing methods by a large margin. Our code will be released at
https://github.com/YUE-FAN/SSB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10579">Graph Neural Networks for Pressure Estimation in Water Distribution Systems. (arXiv:2311.10579v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Truong_H/0/1/0/all/0/1">Huy Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tello_A/0/1/0/all/0/1">Andr&#xe9;s Tello</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazovik_A/0/1/0/all/0/1">Alexander Lazovik</a>, <a href="http://arxiv.org/find/cs/1/au:+Degeler_V/0/1/0/all/0/1">Victoria Degeler</a></p>
<p>Pressure and flow estimation in Water Distribution Networks (WDN) allows
water management companies to optimize their control operations. For many
years, mathematical simulation tools have been the most common approach to
reconstructing an estimate of the WDN hydraulics. However, pure physics-based
simulations involve several challenges, e.g. partially observable data, high
uncertainty, and extensive manual configuration. Thus, data-driven approaches
have gained traction to overcome such limitations. In this work, we combine
physics-based modeling and Graph Neural Networks (GNN), a data-driven approach,
to address the pressure estimation problem. First, we propose a new data
generation method using a mathematical simulation but not considering temporal
patterns and including some control parameters that remain untouched in
previous works; this contributes to a more diverse training data. Second, our
training strategy relies on random sensor placement making our GNN-based
estimation model robust to unexpected sensor location changes. Third, a
realistic evaluation protocol considers real temporal patterns and additionally
injects the uncertainties intrinsic to real-world scenarios. Finally, a
multi-graph pre-training strategy allows the model to be reused for pressure
estimation in unseen target WDNs. Our GNN-based model estimates the pressure of
a large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of
7%, surpassing the performance of previous studies. Likewise, it outperformed
previous approaches on other WDN benchmarks, showing a reduction of absolute
error up to approximately 52% in the best cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10580">Implicit Maximum a Posteriori Filtering via Adaptive Optimization. (arXiv:2311.10580v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bencomo_G/0/1/0/all/0/1">Gianluca M. Bencomo</a>, <a href="http://arxiv.org/find/cs/1/au:+Snell_J/0/1/0/all/0/1">Jake C. Snell</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a></p>
<p>Bayesian filtering approximates the true underlying behavior of a
time-varying system by inverting an explicit generative model to convert noisy
measurements into state estimates. This process typically requires either
storage, inversion, and multiplication of large matrices or Monte Carlo
estimation, neither of which are practical in high-dimensional state spaces
such as the weight spaces of artificial neural networks. Here, we frame the
standard Bayesian filtering problem as optimization over a time-varying
objective. Instead of maintaining matrices for the filtering equations or
simulating particles, we specify an optimizer that defines the Bayesian filter
implicitly. In the linear-Gaussian setting, we show that every Kalman filter
has an equivalent formulation using K steps of gradient descent. In the
nonlinear setting, our experiments demonstrate that our framework results in
filters that are effective, robust, and scalable to high-dimensional systems,
comparing well against the standard toolbox of Bayesian filtering solutions. We
suggest that it is easier to fine-tune an optimizer than it is to specify the
correct filtering equations, making our framework an attractive option for
high-dimensional filtering problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10590">EduGym: An Environment Suite for Reinforcement Learning Education. (arXiv:2311.10590v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1">Thomas M. Moerland</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_Brockhausen_M/0/1/0/all/0/1">Matthias M&#xfc;ller-Brockhausen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernatavicius_A/0/1/0/all/0/1">Andrius Bernatavicius</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponse_K/0/1/0/all/0/1">Koen Ponse</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouwenhoven_T/0/1/0/all/0/1">Tom Kouwenhoven</a>, <a href="http://arxiv.org/find/cs/1/au:+Sauter_A/0/1/0/all/0/1">Andreas Sauter</a>, <a href="http://arxiv.org/find/cs/1/au:+Meer_M/0/1/0/all/0/1">Michiel van der Meer</a>, <a href="http://arxiv.org/find/cs/1/au:+Renting_B/0/1/0/all/0/1">Bram Renting</a>, <a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1">Aske Plaat</a></p>
<p>Due to the empirical success of reinforcement learning, an increasing number
of students study the subject. However, from our practical teaching experience,
we see students entering the field (bachelor, master and early PhD) often
struggle. On the one hand, textbooks and (online) lectures provide the
fundamentals, but students find it hard to translate between equations and
code. On the other hand, public codebases do provide practical examples, but
the implemented algorithms tend to be complex, and the underlying test
environments contain multiple reinforcement learning challenges at once.
Although this is realistic from a research perspective, it often hinders
educational conceptual understanding. To solve this issue we introduce EduGym,
a set of educational reinforcement learning environments and associated
interactive notebooks tailored for education. Each EduGym environment is
specifically designed to illustrate a certain aspect/challenge of reinforcement
learning (e.g., exploration, partial observability, stochasticity, etc.), while
the associated interactive notebook explains the challenge and its possible
solution approaches, connecting equations and code in a single document. An
evaluation among RL students and researchers shows 86% of them think EduGym is
a useful tool for reinforcement learning education. All notebooks are available
from https://sites.google.com/view/edu-gym/home, while the full software
package can be installed from https://github.com/RLG-Leiden/edugym.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10597">Designing Reconfigurable Intelligent Systems with Markov Blankets. (arXiv:2311.10597v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedlak_B/0/1/0/all/0/1">Boris Sedlak</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujol_V/0/1/0/all/0/1">Victor Casamayor Pujol</a>, <a href="http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1">Praveen Kumar Donta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1">Schahram Dustdar</a></p>
<p>Compute Continuum (CC) systems comprise a vast number of devices distributed
over computational tiers. Evaluating business requirements, i.e., Service Level
Objectives (SLOs), requires collecting data from all those devices; if SLOs are
violated, devices must be reconfigured to ensure correct operation. If done
centrally, this dramatically increases the number of devices and variables that
must be considered, while creating an enormous communication overhead. To
address this, we (1) introduce a causality filter based on Markov blankets (MB)
that limits the number of variables that each device must track, (2) evaluate
SLOs decentralized on a device basis, and (3) infer optimal device
configuration for fulfilling SLOs. We evaluated our methodology by analyzing
video stream transformations and providing device configurations that ensure
the Quality of Service (QoS). The devices thus perceived their environment and
acted accordingly -- a form of decentralized intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10607">Active Inference on the Edge: A Design Study. (arXiv:2311.10607v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sedlak_B/0/1/0/all/0/1">Boris Sedlak</a>, <a href="http://arxiv.org/find/eess/1/au:+Pujol_V/0/1/0/all/0/1">Victor Casamayor Pujol</a>, <a href="http://arxiv.org/find/eess/1/au:+Donta_P/0/1/0/all/0/1">Praveen Kumar Donta</a>, <a href="http://arxiv.org/find/eess/1/au:+Dustdar_S/0/1/0/all/0/1">Schahram Dustdar</a></p>
<p>Machine Learning (ML) is a common tool to interpret and predict the behavior
of distributed computing systems, e.g., to optimize the task distribution
between devices. As more and more data is created by Internet of Things (IoT)
devices, data processing and ML training are carried out by edge devices in
close proximity. To ensure Quality of Service (QoS) throughout these
operations, systems are supervised and dynamically adapted with the help of ML.
However, as long as ML models are not retrained, they fail to capture gradual
shifts in the variable distribution, leading to an inaccurate view of the
system state. Moreover, as the prediction accuracy decreases, the reporting
device should actively resolve uncertainties to improve the model's precision.
Such a level of self-determination could be provided by Active Inference (ACI)
-- a concept from neuroscience that describes how the brain constantly predicts
and evaluates sensory information to decrease long-term surprise. We
encompassed these concepts in a single action-perception cycle, which we
implemented for distributed agents in a smart manufacturing use case. As a
result, we showed how our ACI agent was able to quickly and traceably solve an
optimization problem while fulfilling QoS requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10609">Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks. (arXiv:2311.10609v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feuer_B/0/1/0/all/0/1">Benjamin Feuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">Chinmay Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1">Niv Cohen</a></p>
<p>Tabular classification has traditionally relied on supervised algorithms,
which estimate the parameters of a prediction model using its training data.
Recently, Prior-Data Fitted Networks (PFNs) such as TabPFN have successfully
learned to classify tabular data in-context: the model parameters are designed
to classify new samples based on labelled training samples given after the
model training. While such models show great promise, their applicability to
real-world data remains limited due to the computational scale needed. Here we
study the following question: given a pre-trained PFN for tabular data, what is
the best way to summarize the labelled training samples before feeding them to
the model? We conduct an initial investigation of sketching and
feature-selection methods for TabPFN, and note certain key differences between
it and conventionally fitted tabular models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10610">A Poincar\&#x27;e Inequality and Consistency Results for Signal Sampling on Large Graphs. (arXiv:2311.10610v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Thien Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_L/0/1/0/all/0/1">Luana Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1">Stefanie Jegelka</a></p>
<p>Large-scale graph machine learning is challenging as the complexity of
learning models scales with the graph size. Subsampling the graph is a viable
alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.
Existing graph sampling techniques require not only computing the spectra of
large matrices but also repeating these computations when the graph changes,
e.g., grows. In this paper, we introduce a signal sampling theory for a type of
graph limit -- the graphon. We prove a Poincar\'e inequality for graphon
signals and show that complements of node subsets satisfying this inequality
are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting
connections with spectral clustering and Gaussian elimination, we prove that
such sampling sets are consistent in the sense that unique sampling sets on a
convergent graph sequence converge to unique sampling sets on the graphon. We
then propose a related graphon signal sampling algorithm for large graphs, and
demonstrate its good empirical performance on graph machine learning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10633">Predicting the Probability of Collision of a Satellite with Space Debris: A Bayesian Machine Learning Approach. (arXiv:2311.10633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Catulo_J/0/1/0/all/0/1">Jo&#xe3;o Sim&#xf5;es Catulo</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1">Cl&#xe1;udia Soares</a>, <a href="http://arxiv.org/find/cs/1/au:+Guimaraes_M/0/1/0/all/0/1">Marta Guimar&#xe3;es</a></p>
<p>Space is becoming more crowded in Low Earth Orbit due to increased space
activity. Such a dense space environment increases the risk of collisions
between space objects endangering the whole space population. Therefore, the
need to consider collision avoidance as part of routine operations is evident
to satellite operators. Current procedures rely on the analysis of multiple
collision warnings by human analysts. However, with the continuous growth of
the space population, this manual approach may become unfeasible, highlighting
the importance of automation in risk assessment. In 2019, ESA launched a
competition to study the feasibility of applying machine learning in collision
risk estimation and released a dataset that contained sequences of Conjunction
Data Messages (CDMs) in support of real close encounters. The competition
results showed that the naive forecast and its variants are strong predictors
for this problem, which suggests that the CDMs may follow the Markov property.
The proposed work investigates this theory by benchmarking Hidden Markov Models
(HMM) in predicting the risk of collision between two resident space objects by
using one feature of the entire dataset: the sequence of the probability in the
CDMs. In addition, Bayesian statistics are used to infer a joint distribution
for the parameters of the models, which allows the development of robust and
reliable probabilistic predictive models that can incorporate physical or prior
knowledge about the problem within a rigorous theoretical framework and
provides prediction uncertainties that nicely reflect the accuracy of the
predicted risk. This work shows that the implemented HMM outperforms the naive
solution in some metrics, which further adds to the idea that the collision
warnings may be Markovian and suggests that this is a powerful method to be
further explored.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10638">Concept-free Causal Disentanglement with Variational Graph Auto-Encoder. (arXiv:2311.10638v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jingyun Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lili Yang</a></p>
<p>In disentangled representation learning, the goal is to achieve a compact
representation that consists of all interpretable generative factors in the
observational data. Learning disentangled representations for graphs becomes
increasingly important as graph data rapidly grows. Existing approaches often
rely on Variational Auto-Encoder (VAE) or its causal structure learning-based
refinement, which suffer from sub-optimality in VAEs due to the independence
factor assumption and unavailability of concept labels, respectively. In this
paper, we propose an unsupervised solution, dubbed concept-free causal
disentanglement, built on a theoretically provable tight upper bound
approximating the optimal factor. This results in an SCM-like causal structure
modeling that directly learns concept structures from data. Based on this idea,
we propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal
disentanglement layer into Variational Graph Auto-Encoder. Furthermore, we
prove concept consistency under our concept-free causal disentanglement
framework, hence employing it to enhance the meta-learning framework, called
concept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive
experiments to demonstrate the superiority of the proposed models: CCVGAE and
CC-Meta-Graph, reaching up to $29\%$ and $11\%$ absolute improvements over
baselines in terms of AUC, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10640">Multi-delay arterial spin-labeled perfusion estimation with biophysics simulation and deep learning. (arXiv:2311.10640v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Hu_R/0/1/0/all/0/1">Renjiu Hu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1">Qihao Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Spincemaille_P/0/1/0/all/0/1">Pascal Spincemaille</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh D. Nguyen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a></p>
<p>Purpose: To develop biophysics-based method for estimating perfusion Q from
arterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net
(QTMnet) was trained to estimate perfusion from 4D tracer propagation images.
The network was trained and tested on simulated 4D tracer concentration data
based on artificial vasculature structure generated by constrained constructive
optimization (CCO) method. The trained network was further tested in a
synthetic brain ASL image based on vasculature network extracted from magnetic
resonance (MR) angiography. The estimations from both trained network and a
conventional kinetic model were compared in ASL images acquired from eight
healthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from
concentration data. Relative error of the synthetic brain ASL image was 7.04%
for perfusion Q, lower than the error using single-delay ASL model: 25.15% for
Q, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet
provides accurate estimation on perfusion parameters and is a promising
approach as a clinical ASL MRI image processing pipeline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10642">Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers. (arXiv:2311.10642v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bozic_V/0/1/0/all/0/1">Vukasin Bozic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dordevic_D/0/1/0/all/0/1">Danilo Dordevic</a>, <a href="http://arxiv.org/find/cs/1/au:+Coppola_D/0/1/0/all/0/1">Daniele Coppola</a>, <a href="http://arxiv.org/find/cs/1/au:+Thommes_J/0/1/0/all/0/1">Joseph Thommes</a></p>
<p>This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these "attentionless
Transformers" to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10648">Self-trained Panoptic Segmentation. (arXiv:2311.10648v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Shourya Verma</a></p>
<p>Panoptic segmentation is an important computer vision task which combines
semantic and instance segmentation. It plays a crucial role in domains of
medical image analysis, self-driving vehicles, and robotics by providing a
comprehensive understanding of visual environments. Traditionally, deep
learning panoptic segmentation models have relied on dense and accurately
annotated training data, which is expensive and time consuming to obtain.
Recent advancements in self-supervised learning approaches have shown great
potential in leveraging synthetic and unlabelled data to generate pseudo-labels
using self-training to improve the performance of instance and semantic
segmentation models. The three available methods for self-supervised panoptic
segmentation use proposal-based transformer architectures which are
computationally expensive, complicated and engineered for specific tasks. The
aim of this work is to develop a framework to perform embedding-based
self-supervised panoptic segmentation using self-training in a
synthetic-to-real domain adaptation problem setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10653">Learning Realistic Joint Space Boundaries for Range of Motion Analysis of Healthy and Impaired Human Arms. (arXiv:2311.10653v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keyvanian_S/0/1/0/all/0/1">Shafagh Keyvanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1">Michelle J. Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Figueroa_N/0/1/0/all/0/1">Nadia Figueroa</a></p>
<p>A realistic human kinematic model that satisfies anatomical constraints is
essential for human-robot interaction, biomechanics and robot-assisted
rehabilitation. Modeling realistic joint constraints, however, is challenging
as human arm motion is constrained by joint limits, inter- and intra-joint
dependencies, self-collisions, individual capabilities and muscular or
neurological constraints which are difficult to represent. Hence, physicians
and researchers have relied on simple box-constraints, ignoring important
anatomical factors. In this paper, we propose a data-driven method to learn
realistic anatomically constrained upper-limb range of motion (RoM) boundaries
from motion capture data. This is achieved by fitting a one-class support
vector machine to a dataset of upper-limb joint space exploration motions with
an efficient hyper-parameter tuning scheme. Our approach outperforms similar
works focused on valid RoM learning. Further, we propose an impairment index
(II) metric that offers a quantitative assessment of capability/impairment when
comparing healthy and impaired arms. We validate the metric on healthy subjects
physically constrained to emulate hemiplegia and different disability levels as
stroke patients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10665">Online Calibration of Deep Learning Sub-Models for Hybrid Numerical Modeling Systems. (arXiv:2311.10665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouala_S/0/1/0/all/0/1">Said Ouala</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapron_B/0/1/0/all/0/1">Bertrand Chapron</a>, <a href="http://arxiv.org/find/cs/1/au:+Collard_F/0/1/0/all/0/1">Fabrice Collard</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaultier_L/0/1/0/all/0/1">Lucile Gaultier</a>, <a href="http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1">Ronan Fablet</a></p>
<p>Artificial intelligence and deep learning are currently reshaping numerical
simulation frameworks by introducing new modeling capabilities. These
frameworks are extensively investigated in the context of model correction and
parameterization where they demonstrate great potential and often outperform
traditional physical models. Most of these efforts in defining hybrid dynamical
systems follow {offline} learning strategies in which the neural
parameterization (called here sub-model) is trained to output an ideal
correction. Yet, these hybrid models can face hard limitations when defining
what should be a relevant sub-model response that would translate into a good
forecasting performance. End-to-end learning schemes, also referred to as
online learning, could address such a shortcoming by allowing the deep learning
sub-models to train on historical data. However, defining end-to-end training
schemes for the calibration of neural sub-models in hybrid systems requires
working with an optimization problem that involves the solver of the physical
equations. Online learning methodologies thus require the numerical model to be
differentiable, which is not the case for most modeling systems. To overcome
this difficulty and bypass the differentiability challenge of physical models,
we present an efficient and practical online learning approach for hybrid
systems. The method, called EGA for Euler Gradient Approximation, assumes an
additive neural correction to the physical model, and an explicit Euler
approximation of the gradients. We demonstrate that the EGA converges to the
exact gradients in the limit of infinitely small time steps. Numerical
experiments are performed on various case studies, including prototypical
ocean-atmosphere dynamics. Results show significant improvements over offline
learning, highlighting the potential of end-to-end online learning for hybrid
modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10671">Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference. (arXiv:2311.10671v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1">Marvin Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1">Stefan T. Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1">Paul-Christian B&#xfc;rkner</a></p>
<p>We present multimodal neural posterior estimation (MultiNPE), a method to
integrate heterogeneous data from different sources in simulation-based
inference with neural networks. Inspired by advances in attention-based deep
fusion learning, it empowers researchers to analyze data from different domains
and infer the parameters of complex mathematical models with increased
accuracy. We formulate different multimodal fusion approaches for MultiNPE
(early, late, and hybrid) and evaluate their performance in three challenging
numerical experiments. MultiNPE not only outperforms na\"ive baselines on a
benchmark model, but also achieves superior inference on representative
scientific models from neuroscience and cardiology. In addition, we
systematically investigate the impact of partially missing data on the
different fusion strategies. Across our different experiments, late and hybrid
fusion techniques emerge as the methods of choice for practical applications of
multimodal simulation-based inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10678">Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections. (arXiv:2311.10678v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1">Lihan Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yuchen Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Li-Heng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1">Minae Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1">Montserrat Gonzalez Arenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Andy Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a></p>
<p>Today's robot policies exhibit subpar performance when faced with the
challenge of generalizing to novel environments. Human corrective feedback is a
crucial form of guidance to enable such generalization. However, adapting to
and learning from online human corrections is a non-trivial endeavor: not only
do robots need to remember human feedback over time to retrieve the right
information in new settings and reduce the intervention rate, but also they
would need to be able to respond to feedback that can be arbitrary corrections
about high-level human preferences to low-level adjustments to skill
parameters. In this work, we present Distillation and Retrieval of Online
Corrections (DROC), a large language model (LLM)-based system that can respond
to arbitrary forms of language feedback, distill generalizable knowledge from
corrections, and retrieve relevant past experiences based on textual and visual
similarity for improving performance in novel settings. DROC is able to respond
to a sequence of online language corrections that address failures in both
high-level task plans and low-level skill primitives. We demonstrate that DROC
effectively distills the relevant information from the sequence of online
corrections in a knowledge base and retrieves that knowledge in settings with
new task or object instances. DROC outperforms other techniques that directly
generate robot code via LLMs by using only half of the total number of
corrections needed in the first round and requires little to no corrections
after two iterations. We show further results, videos, prompts and code on
https://sites.google.com/stanford.edu/droc .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10680">Optimal Embedding Dimension for Sparse Subspace Embeddings. (arXiv:2311.10680v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chenakkod_S/0/1/0/all/0/1">Shabarish Chenakkod</a>, <a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1">Micha&#x142; Derezi&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaoyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudelson_M/0/1/0/all/0/1">Mark Rudelson</a></p>
<p>A random $m\times n$ matrix $S$ is an oblivious subspace embedding (OSE) with
parameters $\epsilon&gt;0$, $\delta\in(0,1/3)$ and $d\leq m\leq n$, if for any
$d$-dimensional subspace $W\subseteq R^n$,
</p>
<p>$P\big(\,\forall_{x\in W}\ (1+\epsilon)^{-1}\|x\|\leq\|Sx\|\leq
(1+\epsilon)\|x\|\,\big)\geq 1-\delta.$
</p>
<p>It is known that the embedding dimension of an OSE must satisfy $m\geq d$,
and for any $\theta &gt; 0$, a Gaussian embedding matrix with $m\geq (1+\theta) d$
is an OSE with $\epsilon = O_\theta(1)$. However, such optimal embedding
dimension is not known for other embeddings. Of particular interest are sparse
OSEs, having $s\ll m$ non-zeros per column, with applications to problems such
as least squares regression and low-rank approximation.
</p>
<p>We show that, given any $\theta &gt; 0$, an $m\times n$ random matrix $S$ with
$m\geq (1+\theta)d$ consisting of randomly sparsified $\pm1/\sqrt s$ entries
and having $s= O(\log^4(d))$ non-zeros per column, is an oblivious subspace
embedding with $\epsilon = O_{\theta}(1)$. Our result addresses the main open
question posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse
OSEs can achieve $m=O(d)$ embedding dimension, and it improves on
$m=O(d\log(d))$ shown by Cohen (SODA 2016). We use this to construct the first
oblivious subspace embedding with $O(d)$ embedding dimension that can be
applied faster than current matrix multiplication time, and to obtain an
optimal single-pass algorithm for least squares regression. We further extend
our results to construct even sparser non-oblivious embeddings, leading to the
first subspace embedding with low distortion $\epsilon=o(1)$ and optimal
embedding dimension $m=O(d/\epsilon^2)$ that can be applied in current matrix
multiplication time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10697">PEFT-MedAware: Large Language Model for Medical Awareness. (arXiv:2311.10697v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandya_K/0/1/0/all/0/1">Keivalya Pandya</a></p>
<p>Chat models are capable of answering a wide range of questions, however, the
accuracy of their responses is highly uncertain. In this research, we propose a
specialized PEFT-MedAware model where we utilize parameter-efficient
fine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized
MedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of
its trainable parameters to enhance computational efficiency. The paper adopts
data preprocessing and PEFT to optimize model performance, complemented by a
BitsAndBytesConfig for efficient transformer training. The resulting model was
capable of outperforming other LLMs in medical question-answering tasks in
specific domains with greater accuracy utilizing limited computational
resources making it suitable for deployment in resource-constrained
environments. We propose further improvements through expanded datasets, larger
models, and feedback mechanisms for sustained medical relevancy. Our work
highlights the efficiency gains and specialized capabilities of PEFT in medical
AI, outpacing standard models in precision without extensive resource demands.
The proposed model and data are released for research purposes only.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10699">Using linear initialisation to improve speed of convergence and fully-trained error in Autoencoders. (arXiv:2311.10699v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marais_M/0/1/0/all/0/1">Marcel Marais</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartstein_M/0/1/0/all/0/1">Mate Hartstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cevora_G/0/1/0/all/0/1">George Cevora</a></p>
<p>Good weight initialisation is an important step in successful training of
Artificial Neural Networks. Over time a number of improvements have been
proposed to this process. In this paper we introduce a novel weight
initialisation technique called the Straddled Matrix Initialiser. This
initialisation technique is motivated by our assumption that major,
global-scale relationships in data are linear with only smaller effects
requiring complex non-linearities. Combination of Straddled Matrix and ReLU
activation function initialises a Neural Network as a de facto linear model,
which we postulate should be a better starting point for optimisation given our
assumptions. We test this by training autoencoders on three datasets using
Straddled Matrix and seven other state-of-the-art weight initialisation
techniques. In all our experiments the Straddeled Matrix Initialiser clearly
outperforms all other methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10701">SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet Variational Autoencoder for Hyperspectral Pixel Unmixing. (arXiv:2311.10701v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chitnis_S/0/1/0/all/0/1">Soham Chitnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mantripragada_K/0/1/0/all/0/1">Kiran Mantripragada</a>, <a href="http://arxiv.org/find/cs/1/au:+Qureshi_F/0/1/0/all/0/1">Faisal Z. Qureshi</a></p>
<p>The Hyperspectral Unxming problem is to find the pure spectral signal of the
underlying materials (endmembers) and their proportions (abundances). The
proposed method builds upon the recently proposed method, Latent Dirichlet
Variational Autoencoder (LDVAE). It assumes that abundances can be encoded as
Dirichlet Distributions while mixed pixels and endmembers are represented by
Multivariate Normal Distributions. However, LDVAE does not leverage spatial
information present in an HSI; we propose an Isotropic CNN encoder with spatial
attention to solve the hyperspectral unmixing problem. We evaluated our model
on Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model
also leverages the transfer learning paradigm for Cuprite Dataset, where we
train the model on synthetic data and evaluate it on real-world data. We are
able to observe the improvement in the results for the endmember extraction and
abundance estimation by incorporating the spatial information. Code can be
found at https://github.com/faisalqureshi/cnn-ldvae
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10707">Multimodal Representation Learning by Alternating Unimodal Adaptation. (arXiv:2311.10707v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a></p>
<p>Multimodal learning, which integrates data from diverse sensory modes, plays
a pivotal role in artificial intelligence. However, existing multimodal
learning methods often struggle with challenges where some modalities appear
more dominant than others during multimodal learning, resulting in suboptimal
performance. To address this challenge, we propose MLA (Multimodal Learning
with Alternating Unimodal Adaptation). MLA reframes the conventional joint
multimodal learning process by transforming it into an alternating unimodal
learning process, thereby minimizing interference between modalities.
Simultaneously, it captures cross-modal interactions through a shared head,
which undergoes continuous optimization across different modalities. This
optimization process is controlled by a gradient modification mechanism to
prevent the shared head from losing previously acquired information. During the
inference phase, MLA utilizes a test-time uncertainty-based model fusion
mechanism to integrate multimodal information. Extensive experiments are
conducted on five diverse datasets, encompassing scenarios with complete
modalities and scenarios with missing modalities. These experiments demonstrate
the superiority of MLA over competing prior approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10708">SelfEval: Leveraging the discriminative nature of generative models for evaluation. (arXiv:2311.10708v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1">Sai Saketh Rambhatla</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1">Ishan Misra</a></p>
<p>In this work, we show that text-to-image generative models can be 'inverted'
to assess their own text-image understanding capabilities in a completely
automated manner.
</p>
<p>Our method, called SelfEval, uses the generative model to compute the
likelihood of real images given text prompts, making the generative model
directly applicable to discriminative tasks.
</p>
<p>Using SelfEval, we repurpose standard datasets created for evaluating
multimodal text-image discriminative models to evaluate generative models in a
fine-grained manner: assessing their performance on attribute binding, color
recognition, counting, shape recognition, spatial understanding.
</p>
<p>To the best of our knowledge SelfEval is the first automated metric to show a
high degree of agreement for measuring text-faithfulness with the gold-standard
human evaluations across multiple models and benchmarks.
</p>
<p>Moreover, SelfEval enables us to evaluate generative models on challenging
tasks such as Winoground image-score where they demonstrate competitive
performance to discriminative models.
</p>
<p>We also show severe drawbacks of standard automated metrics such as
CLIP-score to measure text faithfulness on benchmarks such as DrawBench, and
how SelfEval sidesteps these issues.
</p>
<p>We hope SelfEval enables easy and reliable automated evaluation for diffusion
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10709">Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. (arXiv:2311.10709v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1">Rohit Girdhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mannat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1">Andrew Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1">Quentin Duval</a>, <a href="http://arxiv.org/find/cs/1/au:+Azadi_S/0/1/0/all/0/1">Samaneh Azadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1">Sai Saketh Rambhatla</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Akbar Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1">Devi Parikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1">Ishan Misra</a></p>
<p>We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's
PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial
solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user's text
prompt, where our generations are preferred 96% over prior work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10710">Machine learning phase transitions: Connections to the Fisher information. (arXiv:2311.10710v1 [cond-mat.dis-nn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Arnold_J/0/1/0/all/0/1">Julian Arnold</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lorch_N/0/1/0/all/0/1">Niels L&#xf6;rch</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Holtorf_F/0/1/0/all/0/1">Flemming Holtorf</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Schafer_F/0/1/0/all/0/1">Frank Sch&#xe4;fer</a></p>
<p>Despite the widespread use and success of machine-learning techniques for
detecting phase transitions from data, their working principle and fundamental
limits remain elusive. Here, we explain the inner workings and identify
potential failure modes of these techniques by rooting popular machine-learning
indicators of phase transitions in information-theoretic concepts. Using tools
from information geometry, we prove that several machine-learning indicators of
phase transitions approximate the square root of the system's (quantum) Fisher
information from below -- a quantity that is known to indicate phase
transitions but is often difficult to compute from data. We numerically
demonstrate the quality of these bounds for phase transitions in classical and
quantum systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.12882">A finite sample analysis of the benign overfitting phenomenon for ridge function estimation. (arXiv:2007.12882v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Caron_E/0/1/0/all/0/1">Emmanuel Caron</a>, <a href="http://arxiv.org/find/stat/1/au:+Chretien_S/0/1/0/all/0/1">Stephane Chretien</a></p>
<p>Recent extensive numerical experiments in high scale machine learning have
allowed to uncover a quite counterintuitive phase transition, as a function of
the ratio between the sample size and the number of parameters in the model. As
the number of parameters $p$ approaches the sample size $n$, the generalisation
error increases, but surprisingly, it starts decreasing again past the
threshold $p=n$. This phenomenon, brought to the theoretical community
attention in \cite{belkin2019reconciling}, has been thoroughly investigated
lately, more specifically for simpler models than deep neural networks, such as
the linear model when the parameter is taken to be the minimum norm solution to
the least-squares problem, firstly in the asymptotic regime when $p$ and $n$
tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in the
finite dimensional regime and more specifically for linear models
\cite{bartlett2020benign}, \cite{tsigler2020benign},
\cite{lecue2022geometrical}. In the present paper, we propose a finite sample
analysis of non-linear models of \textit{ridge} type, where we investigate the
\textit{overparametrised regime} of the double descent phenomenon for both the
\textit{estimation problem} and the \textit{prediction} problem. Our results
provide a precise analysis of the distance of the best estimator from the true
parameter as well as a generalisation bound which complements recent works of
\cite{bartlett2020benign} and \cite{chinot2020benign}. Our analysis is based on
tools closely related to the continuous Newton method
\cite{neuberger2007continuous} and a refined quantitative analysis of the
performance in prediction of the minimum $\ell_2$-norm solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.05995">A Skew-Sensitive Evaluation Framework for Imbalanced Data Classification. (arXiv:2010.05995v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Min Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1">Nesime Tatbul</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivers_B/0/1/0/all/0/1">Brian Rivers</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Akhilesh Kumar Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lucas Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcus_R/0/1/0/all/0/1">Ryan Marcus</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shengtian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Insup Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1">Justin Gottschlich</a></p>
<p>Class distribution skews in imbalanced datasets may lead to models with
prediction bias towards majority classes, making fair assessment of classifiers
a challenging task. Metrics such as Balanced Accuracy are commonly used to
evaluate a classifier's prediction performance under such scenarios. However,
these metrics fall short when classes vary in importance. In this paper, we
propose a simple and general-purpose evaluation framework for imbalanced data
classification that is sensitive to arbitrary skews in class cardinalities and
importances. Experiments with several state-of-the-art classifiers tested on
real-world datasets from three different domains show the effectiveness of our
framework - not only in evaluating and ranking classifiers, but also training
them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.05439">Concave Utility Reinforcement Learning with Zero-Constraint Violations. (arXiv:2109.05439v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1">Mridul Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1">Qinbo Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1">Vaneet Aggarwal</a></p>
<p>We consider the problem of tabular infinite horizon concave utility
reinforcement learning (CURL) with convex constraints. For this, we propose a
model-based learning algorithm that also achieves zero constraint violations.
Assuming that the concave objective and the convex constraints have a solution
interior to the set of feasible occupation measures, we solve a tighter
optimization problem to ensure that the constraints are never violated despite
the imprecise model knowledge and model stochasticity. We use Bellman
error-based analysis for tabular infinite-horizon setups which allows analyzing
stochastic policies. Combining the Bellman error-based analysis and tighter
optimization equation, for $T$ interactions with the environment, we obtain a
high-probability regret guarantee for objective which grows as
$\Tilde{O}(1/\sqrt{T})$, excluding other factors. The proposed method can be
applied for optimistic algorithms to obtain high-probability regret bounds and
also be used for posterior sampling algorithms to obtain a loose Bayesian
regret bounds but with significant improvement in computational complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.05613">The Dark Side of the Language: Pre-trained Transformers in the DarkNet. (arXiv:2201.05613v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1">Leonardo Ranaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nourbakhsh_A/0/1/0/all/0/1">Aria Nourbakhsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Patrizi_A/0/1/0/all/0/1">Arianna Patrizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruzzetti_E/0/1/0/all/0/1">Elena Sofia Ruzzetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Onorati_D/0/1/0/all/0/1">Dario Onorati</a>, <a href="http://arxiv.org/find/cs/1/au:+Fallucchi_F/0/1/0/all/0/1">Francesca Fallucchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1">Fabio Massimo Zanzotto</a></p>
<p>Pre-trained Transformers are challenging human performances in many NLP
tasks. The massive datasets used for pre-training seem to be the key to their
success on existing tasks. In this paper, we explore how a range of pre-trained
Natural Language Understanding models perform on definitely unseen sentences
provided by classification tasks over a DarkNet corpus. Surprisingly, results
show that syntactic and lexical neural networks perform on par with pre-trained
Transformers even after fine-tuning. Only after what we call extreme domain
adaptation, that is, retraining with the masked language model task on all the
novel corpus, pre-trained Transformers reach their standard high results. This
suggests that huge pre-training corpora may give Transformers unexpected help
since they are exposed to many of the possible sentences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.03574">Structured Prediction Problem Archive. (arXiv:2202.03574v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1">Paul Swoboda</a>, <a href="http://arxiv.org/find/cs/1/au:+Andres_B/0/1/0/all/0/1">Bjoern Andres</a>, <a href="http://arxiv.org/find/cs/1/au:+Hornakova_A/0/1/0/all/0/1">Andrea Hornakova</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1">Florian Bernard</a>, <a href="http://arxiv.org/find/cs/1/au:+Irmai_J/0/1/0/all/0/1">Jannik Irmai</a>, <a href="http://arxiv.org/find/cs/1/au:+Roetzer_P/0/1/0/all/0/1">Paul Roetzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1">Bogdan Savchynskyy</a>, <a href="http://arxiv.org/find/cs/1/au:+Stein_D/0/1/0/all/0/1">David Stein</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1">Ahmed Abbas</a></p>
<p>Structured prediction problems are one of the fundamental tools in machine
learning. In order to facilitate algorithm development for their numerical
solution, we collect in one place a large number of datasets in easy to read
formats for a diverse set of problem classes. We provide archival links to
datasets, description of the considered problems and problem formats, and a
short summary of problem characteristics including size, number of instances
etc. For reference we also give a non-exhaustive selection of algorithms
proposed in the literature for their solution. We hope that this central
repository will make benchmarking and comparison to established works easier.
We welcome submission of interesting new datasets and algorithms for inclusion
in our archive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.09603">Comparing Deep Reinforcement Learning Algorithms in Two-Echelon Supply Chains. (arXiv:2204.09603v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stranieri_F/0/1/0/all/0/1">Francesco Stranieri</a>, <a href="http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1">Fabio Stella</a></p>
<p>In this study, we analyze and compare the performance of state-of-the-art
deep reinforcement learning algorithms for solving the supply chain inventory
management problem. This complex sequential decision-making problem consists of
determining the optimal quantity of products to be produced and shipped across
different warehouses over a given time horizon. In particular, we present a
mathematical formulation of a two-echelon supply chain environment with
stochastic and seasonal demand, which allows managing an arbitrary number of
warehouses and product types. Through a rich set of numerical experiments, we
compare the performance of different deep reinforcement learning algorithms
under various supply chain structures, topologies, demands, capacities, and
costs. The results of the experimental plan indicate that deep reinforcement
learning algorithms outperform traditional inventory management strategies,
such as the static (s, Q)-policy. Furthermore, this study provides detailed
insight into the design and development of an open-source software library that
provides a customizable environment for solving the supply chain inventory
management problem using a wide range of data-driven approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.01607">Modeling and Correcting Bias in Sequential Evaluation. (arXiv:2205.01607v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1">Jingyan Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Pananjady_A/0/1/0/all/0/1">Ashwin Pananjady</a></p>
<p>We consider the problem of sequential evaluation, in which an evaluator
observes candidates in a sequence and assigns scores to these candidates in an
online, irrevocable fashion. Motivated by the psychology literature that has
studied sequential bias in such settings -- namely, dependencies between the
evaluation outcome and the order in which the candidates appear -- we propose a
natural model for the evaluator's rating process that captures the lack of
calibration inherent to such a task. We conduct crowdsourcing experiments to
demonstrate various facets of our model. We then proceed to study how to
correct sequential bias under our model by posing this as a statistical
inference problem. We propose a near-linear time, online algorithm for this
task and prove guarantees in terms of two canonical ranking metrics. We also
prove that our algorithm is information theoretically optimal, by establishing
matching lower bounds in both metrics. Finally, we perform a host of numerical
experiments to show that our algorithm often outperforms the de facto method of
using the rankings induced by the reported scores, both in simulation and on
the crowdsourcing data that we collected.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06265">Trustworthy Recommender Systems. (arXiv:2208.06265v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shoujin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiuzhen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_F/0/1/0/all/0/1">Francesco Ricci</a></p>
<p>Recommender systems (RSs) aim to help users to effectively retrieve items of
their interests from a large catalogue. For a quite long period of time,
researchers and practitioners have been focusing on developing accurate RSs.
Recent years have witnessed an increasing number of threats to RSs, coming from
attacks, system and user generated noise, system bias. As a result, it has
become clear that a strict focus on RS accuracy is limited and the research
must consider other important factors, e.g., trustworthiness. For end users, a
trustworthy RS (TRS) should not only be accurate, but also transparent,
unbiased and fair as well as robust to noise or attacks. These observations
actually led to a paradigm shift of the research on RSs: from accuracy-oriented
RSs to TRSs. However, researchers lack a systematic overview and discussion of
the literature in this novel and fast developing field of TRSs. To this end, in
this paper, we provide an overview of TRSs, including a discussion of the
motivation and basic concepts of TRSs, a presentation of the challenges in
building TRSs, and a perspective on the future directions in this area. We also
provide a novel conceptual framework to support the construction of TRSs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12723">A Faithful Deep Sensitivity Estimation for Accelerated Magnetic Resonance Imaging. (arXiv:2210.12723v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1">Haoming Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1">Boxuan Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Bao_L/0/1/0/all/0/1">Lijun Bao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1">Liuhong Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jianjun Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1">Wenping Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1">Jianzhong Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1">Di Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Qu_X/0/1/0/all/0/1">Xiaobo Qu</a></p>
<p>Magnetic resonance imaging (MRI) is an essential diagnostic tool that suffers
from prolonged scan time. To alleviate this limitation, advanced fast MRI
technology attracts extensive research interests. Recent deep learning has
shown its great potential in improving image quality and reconstruction speed.
Faithful coil sensitivity estimation is vital for MRI reconstruction. However,
most deep learning methods still rely on pre-estimated sensitivity maps and
ignore their inaccuracy, resulting in the significant quality degradation of
reconstructed images. In this work, we propose a Joint Deep Sensitivity
estimation and Image reconstruction network, called JDSI. During the image
artifacts removal, it gradually provides more faithful sensitivity maps with
high-frequency information, leading to improved image reconstructions. To
understand the behavior of the network, the mutual promotion of sensitivity
estimation and image reconstruction is revealed through the visualization of
network intermediate results. Results on in vivo datasets and radiologist
reader study demonstrate that, for both calibration-based and calibrationless
reconstruction, the proposed JDSI achieves the state-of-the-art performance
visually and quantitatively, especially when the acceleration factor is high.
Additionally, JDSI owns nice robustness to patients and autocalibration
signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06302">GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. (arXiv:2211.06302v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Margeloiu_A/0/1/0/all/0/1">Andrei Margeloiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1">Nikola Simidjievski</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Lio</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1">Mateja Jamnik</a></p>
<p>Neural network models often struggle with high-dimensional but small
sample-size tabular datasets. One reason is that current weight initialisation
methods assume independence between weights, which can be problematic when
there are insufficient samples to estimate the model's parameters accurately.
In such small data scenarios, leveraging additional structures can improve the
model's performance and training stability. To address this, we propose
GCondNet, a general approach to enhance neural networks by leveraging implicit
structures present in tabular data. We create a graph between samples for each
data dimension, and utilise Graph Neural Networks (GNNs) for extracting this
implicit structure, and for conditioning the parameters of the first layer of
an underlying predictor network. By creating many small graphs, GCondNet
exploits the data's high-dimensionality, and thus improves the performance of
an underlying predictor network. We demonstrate the effectiveness of our method
on 9 real-world datasets, where GCondNet outperforms 15 standard and
state-of-the-art methods. The results show that GCondNet is a versatile
framework for injecting graph-regularisation into various types of neural
networks, including MLPs and tabular Transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12829">Identifying the Key Attributes in an Unlabeled Event Log for Automated Process Discovery. (arXiv:2301.12829v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Toyoda_K/0/1/0/all/0/1">Kentaroh Toyoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1">Rachel Gan Kai Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Allan NengSheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Siew_T/0/1/0/all/0/1">Tan Puay Siew</a></p>
<p>Process mining discovers and analyzes a process model from historical event
logs. The prior art methods use the key attributes of case-id, activity, and
timestamp hidden in an event log as clues to discover a process model. However,
a user needs to specify them manually, and this can be an exhaustive task. In
this paper, we propose a two-stage key attribute identification method to avoid
such a manual investigation, and thus this is a step toward fully automated
process discovery. One of the challenging tasks is how to avoid exhaustive
computation due to combinatorial explosion. For this, we narrow down candidates
for each key attribute by using supervised machine learning in the first stage
and identify the best combination of the key attributes by discovering process
models and evaluating them in the second stage. Our computational complexity
can be reduced from $\mathcal{O}(N^3)$ to $\mathcal{O}(k^3)$ where $N$ and $k$
are the numbers of columns and candidates we keep in the first stage,
respectively, and usually $k$ is much smaller than $N$. We evaluated our method
with 14 open datasets and showed that our method could identify the key
attributes even with $k = 2$ for about 20 seconds for many datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11961">Sharp Calibrated Gaussian Processes. (arXiv:2302.11961v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Capone_A/0/1/0/all/0/1">Alexandre Capone</a>, <a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1">Geoff Pleiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1">Sandra Hirche</a></p>
<p>While Gaussian processes are a mainstay for various engineering and
scientific applications, the uncertainty estimates don't satisfy frequentist
guarantees and can be miscalibrated in practice. State-of-the-art approaches
for designing calibrated models rely on inflating the Gaussian process
posterior variance, which yields confidence intervals that are potentially too
coarse. To remedy this, we present a calibration approach that generates
predictive quantiles using a computation inspired by the vanilla Gaussian
process posterior variance but using a different set of hyperparameters chosen
to satisfy an empirical calibration constraint. This results in a calibration
approach that is considerably more flexible than existing approaches, which we
optimize to yield tight predictive quantiles. Our approach is shown to yield a
calibrated model under reasonable assumptions. Furthermore, it outperforms
existing approaches in sharpness when employed for calibrated regression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01913">Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment. (arXiv:2303.01913v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jong-Ryul Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1">Yong-Hyuk Moon</a></p>
<p>As deep learning models become popular, there is a lot of need for deploying
them to diverse device environments. Because it is costly to develop and
optimize a neural network for every single environment, there is a line of
research to search neural networks for multiple target environments
efficiently. However, existing works for such a situation still suffer from
requiring many GPUs and expensive costs. Motivated by this, we propose a novel
neural network optimization framework named Bespoke for low-cost deployment.
Our framework searches for a lightweight model by replacing parts of an
original model with randomly selected alternatives, each of which comes from a
pretrained neural network or the original model. In the practical sense,
Bespoke has two significant merits. One is that it requires near zero cost for
designing the search space of neural networks. The other merit is that it
exploits the sub-networks of public pretrained neural networks, so the total
cost is minimal compared to the existing works. We conduct experiments
exploring Bespoke's the merits, and the results show that it finds efficient
models for multiple targets with meager cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04689">A Privacy Preserving System for Movie Recommendations Using Federated Learning. (arXiv:2303.04689v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1">David Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Lutz_A/0/1/0/all/0/1">Andreas Lutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Karsten M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a></p>
<p>Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are utilized by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Consequently, we present a recommender system for movie recommendations, which
provides privacy and thus trustworthiness on multiple levels: First and
foremost, it is trained using federated learning and thus, by its very nature,
privacy-preserving, while still enabling users to benefit from global insights.
Furthermore, a novel federated learning scheme, called FedQ, is employed, which
not only addresses the problem of non-i.i.d.-ness and small local datasets, but
also prevents input data reconstruction attacks by aggregating client updates
early. Finally, to reduce the communication overhead, compression is applied,
which significantly compresses the exchanged neural network parametrizations to
a fraction of their original size. We conjecture that this may also improve
data privacy through its lossy quantization stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14889">Model-Based Reinforcement Learning with Isolated Imaginations. (arXiv:2303.14889v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1">Minting Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yitao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a></p>
<p>World models learn the consequences of actions in vision-based interactive
systems. However, in practical scenarios like autonomous driving,
noncontrollable dynamics that are independent or sparsely dependent on action
signals often exist, making it challenging to learn effective world models. To
address this issue, we propose Iso-Dream++, a model-based reinforcement
learning approach that has two main contributions. First, we optimize the
inverse dynamics to encourage the world model to isolate controllable state
transitions from the mixed spatiotemporal variations of the environment.
Second, we perform policy optimization based on the decoupled latent
imaginations, where we roll out noncontrollable states into the future and
adaptively associate them with the current controllable state. This enables
long-horizon visuomotor control tasks to benefit from isolating mixed dynamics
sources in the wild, such as self-driving cars that can anticipate the movement
of other vehicles, thereby avoiding potential risks. On top of our previous
work, we further consider the sparse dependencies between controllable and
noncontrollable states, address the training collapse problem of state
decoupling, and validate our approach in transfer learning setups. Our
empirical study demonstrates that Iso-Dream++ outperforms existing
reinforcement learning models significantly on CARLA and DeepMind Control.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16047">Exploring and Interacting with the Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1">Chudi Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiachang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1">Margo Seltzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a></p>
<p>In real applications, interaction between machine learning models and domain
experts is critical; however, the classical machine learning paradigm that
usually produces only a single model does not facilitate such interaction.
Approximating and exploring the Rashomon set, i.e., the set of all near-optimal
models, addresses this practical challenge by providing the user with a
searchable space containing a diverse set of models from which domain experts
can choose. We present algorithms to efficiently and accurately approximate the
Rashomon set of sparse, generalized additive models with ellipsoids for fixed
support sets and use these ellipsoids to approximate Rashomon sets for many
different support sets. The approximated Rashomon set serves as a cornerstone
to solve practical challenges such as (1) studying the variable importance for
the model class; (2) finding models under user-specified constraints
(monotonicity, direct editing); and (3) investigating sudden changes in the
shape functions. Experiments demonstrate the fidelity of the approximated
Rashomon set and its effectiveness in solving practical challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17245">Investigating and Mitigating the Side Effects of Noisy Views for Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios. (arXiv:2303.17245v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yazhou Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Lei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a></p>
<p>Multi-view clustering (MVC) aims at exploring category structures among
multi-view data in self-supervised manners. Multiple views provide more
information than single views and thus existing MVC methods can achieve
satisfactory performance. However, their performance might seriously degenerate
when the views are noisy in practical multi-view scenarios. In this paper, we
first formally investigate the drawback of noisy views and then propose a
theoretically grounded deep MVC method (namely MVCAN) to address this issue.
Specifically, we propose a novel MVC objective that enables un-shared
parameters and inconsistent clustering predictions across multiple views to
reduce the side effects of noisy views. Furthermore, a two-level multi-view
iterative optimization is designed to generate robust learning targets for
refining individual views' representation learning. Theoretical analysis
reveals that MVCAN works by achieving the multi-view consistency,
complementarity, and noise robustness. Finally, experiments on extensive public
datasets demonstrate that MVCAN outperforms state-of-the-art methods and is
robust against the existence of noisy views.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17491">Language Models can Solve Computer Tasks. (arXiv:2303.17491v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Geunwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1">Pierre Baldi</a>, <a href="http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1">Stephen McAleer</a></p>
<p>Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting's
effectiveness in enhancing LLMs' reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting with
external feedback. We find that RCI combined with CoT performs better than
either separately. Our code can be found here:
https://github.com/posgnu/rci-agent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17807">GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors. (arXiv:2303.17807v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1">Dongyeop Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1">Tae-Rim Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Choong-Yeol Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1">Young-Kyu Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chang-Eop Kim</a></p>
<p>Traditional Korean medicine (TKM) emphasizes individualized diagnosis and
treatment. This uniqueness makes AI modeling difficult due to limited data and
implicit processes. Large language models (LLMs) have demonstrated impressive
medical inference, even without advanced training in medical texts. This study
assessed the capabilities of GPT-4 in TKM, using the Korean National Licensing
Examination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The
K-NLEKMD, administered by a national organization, encompasses 12 major
subjects in TKM. We optimized prompts with Chinese-term annotation, English
translation for questions and instruction, exam-optimized instruction, and
self-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,
surpassing both the examination's average pass mark of 60% and the 40% minimum
for each subject. The gradual introduction of language-related prompts and
prompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.
GPT-4 showed low accuracy in subjects including public health &amp;
medicine-related law, internal medicine (2) which are localized in Korea and
TKM. The model's accuracy was lower for questions requiring TKM-specialized
knowledge. It exhibited higher accuracy in diagnosis-based and recall-based
questions than in intervention-based questions. A positive correlation was
observed between the consistency and accuracy of GPT-4's responses. This study
unveils both the potential and challenges of applying LLMs to TKM. These
findings underline the potential of LLMs like GPT-4 in culturally adapted
medicine, especially TKM, for tasks such as clinical assistance, medical
education, and research. But they also point towards the necessity for the
development of methods to mitigate cultural bias inherent in large language
models and validate their efficacy in real-world clinical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11247">Hybrid quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes. (arXiv:2304.11247v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedykh_A/0/1/0/all/0/1">Alexandr Sedykh</a>, <a href="http://arxiv.org/find/cs/1/au:+Podapaka_M/0/1/0/all/0/1">Maninadh Podapaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagingalieva_A/0/1/0/all/0/1">Asel Sagingalieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_K/0/1/0/all/0/1">Karan Pinto</a>, <a href="http://arxiv.org/find/cs/1/au:+Pflitsch_M/0/1/0/all/0/1">Markus Pflitsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Melnikov_A/0/1/0/all/0/1">Alexey Melnikov</a></p>
<p>Finding the distribution of the velocities and pressures of a fluid (by
solving the Navier-Stokes equations) is a principal task in the chemical,
energy, and pharmaceutical industries, as well as in mechanical engineering and
the design of pipeline systems. With existing solvers, such as OpenFOAM and
Ansys, simulations of fluid dynamics in intricate geometries are
computationally expensive and require re-simulation whenever the geometric
parameters or the initial and boundary conditions are altered. Physics-informed
neural networks are a promising tool for simulating fluid flows in complex
geometries, as they can adapt to changes in the geometry and mesh definitions,
allowing for generalization across different shapes. We present a hybrid
quantum physics-informed neural network that simulates laminar fluid flows in
3D Y-shaped mixers. Our approach combines the expressive power of a quantum
model with the flexibility of a physics-informed neural network, resulting in a
21% higher accuracy compared to a purely classical neural network. Our findings
highlight the potential of machine learning approaches, and in particular
hybrid quantum physics-informed neural network, for complex shape optimization
tasks in computational fluid dynamics. By improving the accuracy of fluid
simulations in complex geometries, our research using hybrid quantum models
contributes to the development of more efficient and reliable fluid dynamics
solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12737">NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients. (arXiv:2304.12737v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stewart_T/0/1/0/all/0/1">Tucker Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Stern_K/0/1/0/all/0/1">Katherine Stern</a>, <a href="http://arxiv.org/find/cs/1/au:+OKeefe_G/0/1/0/all/0/1">Grant O&#x27;Keefe</a>, <a href="http://arxiv.org/find/cs/1/au:+Teredesai_A/0/1/0/all/0/1">Ankur Teredesai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Juhua Hu</a></p>
<p>Sepsis is a syndrome that develops in the body in response to the presence of
an infection. Characterized by severe organ dysfunction, sepsis is one of the
leading causes of mortality in Intensive Care Units (ICUs) worldwide. These
complications can be reduced through early application of antibiotics. Hence,
the ability to anticipate the onset of sepsis early is crucial to the survival
and well-being of patients. Current machine learning algorithms deployed inside
medical infrastructures have demonstrated poor performance and are insufficient
for anticipating sepsis onset early. Recently, deep learning methodologies have
been proposed to predict sepsis, but some fail to capture the time of onset
(e.g., classifying patients' entire visits as developing sepsis or not) and
others are unrealistic for deployment in clinical settings (e.g., creating
training instances using a fixed time to onset, where the time of onset needs
to be known apriori). In this paper, we first propose a novel but realistic
prediction framework that predicts each morning whether sepsis onset will occur
within the next 24 hours using the most recent data collected the previous
night, when patient-provider ratios are higher due to cross-coverage resulting
in limited observation to each patient. However, as we increase the prediction
rate into daily, the number of negative instances will increase, while that of
positive instances remain the same. This causes a severe class imbalance
problem making it hard to capture these rare sepsis cases. To address this, we
propose a nightly profile representation learning (NPRL) approach. We prove
that NPRL can theoretically alleviate the rare event problem and our empirical
study using data from a level-1 trauma center demonstrates the effectiveness of
our proposal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13860">Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates. (arXiv:2304.13860v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Fujii_A/0/1/0/all/0/1">Akihiro Fujii</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Tsunashima_H/0/1/0/all/0/1">Hideki Tsunashima</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fukuhara_Y/0/1/0/all/0/1">Yoshihiro Fukuhara</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Shimizu_K/0/1/0/all/0/1">Koji Shimizu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Watanabe_S/0/1/0/all/0/1">Satoshi Watanabe</a></p>
<p>Deep-learning inverse techniques have attracted significant attention in
recent years. Among them, the neural adjoint (NA) method, which employs a
neural network surrogate simulator, has demonstrated impressive performance in
the design tasks of artificial electromagnetic materials (AEM). However, the
impact of the surrogate simulators' accuracy on the solutions in the NA method
remains uncertain. Furthermore, achieving sufficient optimization becomes
challenging in this method when the surrogate simulator is large, and
computational resources are limited. Additionally, the behavior under
constraints has not been studied, despite its importance from the engineering
perspective. In this study, we investigated the impact of surrogate simulators'
accuracy on the solutions and discovered that the more accurate the surrogate
simulator is, the better the solutions become. We then developed an extension
of the NA method, named Neural Lagrangian (NeuLag) method, capable of
efficiently optimizing a sufficient number of solution candidates. We then
demonstrated that the NeuLag method can find optimal solutions even when
handling sufficient candidates is difficult due to the use of a large and
accurate surrogate simulator. The resimulation errors of the NeuLag method were
approximately 1/50 compared to previous methods for three AEM tasks. Finally,
we performed optimization under constraint using NA and NeuLag, and confirmed
their potential in optimization with soft or hard constraints. We believe our
method holds potential in areas that require large and accurate surrogate
simulators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01841">Inferential Moments of Uncertain Multivariable Systems. (arXiv:2305.01841v2 [physics.data-an] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Vanslette_K/0/1/0/all/0/1">Kevin Vanslette</a></p>
<p>This article expands the framework of Bayesian inference and provides direct
probabilistic methods for approaching inference tasks that are typically
handled with information theory. We treat Bayesian probability updating as a
random process and uncover intrinsic quantitative features of joint probability
distributions called inferential moments. Inferential moments quantify shape
information about how a prior distribution is expected to update in response to
yet to be obtained information. Further, we quantify the unique probability
distribution whose statistical moments are the inferential moments in question.
We find a power series expansion of the mutual information in terms of
inferential moments, which implies a connection between inferential theoretic
logic and elements of information theory. Of particular interest is the
inferential deviation, which is the expected variation of the probability of
one variable in response to an inferential update of another. We explore two
applications that analyze the inferential deviations of a Bayesian network to
improve decision-making. We implement simple greedy algorithms for exploring
sensor tasking using inferential deviations that generally outperform similar
greedy mutual information algorithms in terms of root mean squared error
between epistemic probability estimates and the ground truth probabilities they
are estimating.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05433">Tomography of Quantum States from Structured Measurements via quantum-aware transformer. (arXiv:2305.05433v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Ma_H/0/1/0/all/0/1">Hailan Ma</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sun_Z/0/1/0/all/0/1">Zhenhong Sun</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Dong_D/0/1/0/all/0/1">Daoyi Dong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_C/0/1/0/all/0/1">Chunlin Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Rabitz_H/0/1/0/all/0/1">Herschel Rabitz</a></p>
<p>Quantum state tomography (QST) is the process of reconstructing the state of
a quantum system (mathematically described as a density matrix) through a
series of different measurements, which can be solved by learning a
parameterized function to translate experimentally measured statistics into
physical density matrices. However, the specific structure of quantum
measurements for characterizing a quantum state has been neglected in previous
work. In this paper, we explore the similarity between highly structured
sentences in natural language and intrinsically structured measurements in QST.
To fully leverage the intrinsic quantum characteristics involved in QST, we
design a quantum-aware transformer (QAT) model to capture the complex
relationship between measured frequencies and density matrices. In particular,
we query quantum operators in the architecture to facilitate informative
representations of quantum data and integrate the Bures distance into the loss
function to evaluate quantum state fidelity, thereby enabling the
reconstruction of quantum states from measured data with high fidelity.
Extensive simulations and experiments (on IBM quantum computers) demonstrate
the superiority of the QAT in reconstructing quantum states with favorable
robustness against experimental noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04226">Normalization Layers Are All That Sharpness-Aware Minimization Needs. (arXiv:2306.04226v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1">Maximilian Mueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1">Tiffany Vlaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1">David Rolnick</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a></p>
<p>Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima
and has been shown to enhance generalization performance in various settings.
In this work we show that perturbing only the affine normalization parameters
(typically comprising 0.1% of the total parameters) in the adversarial step of
SAM can outperform perturbing all of the parameters.This finding generalizes to
different SAM variants and both ResNet (Batch Normalization) and Vision
Transformer (Layer Normalization) architectures. We consider alternative sparse
perturbation approaches and find that these do not achieve similar performance
enhancement at such extreme sparsity levels, showing that this behaviour is
unique to the normalization layers. Although our findings reaffirm the
effectiveness of SAM in improving generalization performance, they cast doubt
on whether this is solely caused by reduced sharpness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06048">How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?. (arXiv:2306.06048v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1">Yifei Ming</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixuan Li</a></p>
<p>Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08984">Tree Variational Autoencoders. (arXiv:2306.08984v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1">Laura Manduchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vandenhirtz_M/0/1/0/all/0/1">Moritz Vandenhirtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryser_A/0/1/0/all/0/1">Alain Ryser</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia Vogt</a></p>
<p>We propose Tree Variational Autoencoder (TreeVAE), a new generative
hierarchical clustering model that learns a flexible tree-based posterior
distribution over latent variables. TreeVAE hierarchically divides samples
according to their intrinsic characteristics, shedding light on hidden
structures in the data. It adapts its architecture to discover the optimal tree
for encoding dependencies between latent variables. The proposed tree-based
generative architecture enables lightweight conditional inference and improves
generative performance by utilizing specialized leaf decoders. We show that
TreeVAE uncovers underlying clusters in the data and finds meaningful
hierarchical relations between the different groups on a variety of datasets,
including real-world imaging data. We present empirically that TreeVAE provides
a more competitive log-likelihood lower bound than the sequential counterparts.
Finally, due to its generative nature, TreeVAE is able to generate new samples
from the discovered clusters via conditional sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11719">Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision. (arXiv:2306.11719v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1">Ayush Tewari</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1">Tianwei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1">George Cazenavette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1">Semon Rezchikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1">Fr&#xe9;do Durand</a>, <a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1">William T. Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1">Vincent Sitzmann</a></p>
<p>Denoising diffusion models are a powerful type of generative models used to
capture complex distributions of real-world signals. However, their
applicability is limited to scenarios where training samples are readily
available, which is not always the case in real-world applications. For
example, in inverse graphics, the goal is to generate samples from a
distribution of 3D scenes that align with a given image, but ground-truth 3D
scenes are unavailable and only 2D images are accessible. To address this
limitation, we propose a novel class of denoising diffusion probabilistic
models that learn to sample from distributions of signals that are never
directly observed. Instead, these signals are measured indirectly through a
known differentiable forward model, which produces partial observations of the
unknown signal. Our approach involves integrating the forward model directly
into the denoising process. This integration effectively connects the
generative modeling of observations with the generative modeling of the
underlying signals, allowing for end-to-end training of a conditional
generative model over signals. During inference, our approach enables sampling
from the distribution of underlying signals that are consistent with a given
partial observation. We demonstrate the effectiveness of our method on three
challenging computer vision tasks. For instance, in the context of inverse
graphics, our model enables direct sampling from the distribution of 3D scenes
that align with a single 2D input image.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16058">DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1">Xavier Suau</a>, <a href="http://arxiv.org/find/cs/1/au:+Danieli_F/0/1/0/all/0/1">Federico Danieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1">T. Anderson Keller</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1">Arno Blaas</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1">Jason Ramapuram</a>, <a href="http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1">Dan Busbridge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1">Luca Zappella</a></p>
<p>Multiview Self-Supervised Learning (MSSL) is based on learning invariances
with respect to a set of input transformations. However, invariance partially
or totally removes transformation-related information from the representations,
which might harm performance for specific downstream tasks that require such
information. We propose 2D strUctured and EquivarianT representations (coined
DUET), which are 2d representations organized in a matrix structure, and
equivariant with respect to transformations acting on the input data. DUET
representations maintain information about an input transformation, while
remaining semantically expressive. Compared to SimCLR (Chen et al., 2020)
(unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured
and equivariant), the structured and equivariant nature of DUET representations
enables controlled generation with lower reconstruction error, while
controllability is not possible with SimCLR or ESSL. DUET also achieves higher
accuracy for several discriminative tasks, and improves transfer learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10865">Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Girrbach_L/0/1/0/all/0/1">Leander Girrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1">Anders Christensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a>, <a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1">Zeynep Akata</a>, <a href="http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1">A. Sophia Koepke</a></p>
<p>Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00273">Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions. (arXiv:2308.00273v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Samantha Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yusu Wang</a></p>
<p>Learning distance functions between complex objects, such as the Wasserstein
distance to compare point sets, is a common goal in machine learning
applications. However, functions on such complex objects (e.g., point sets and
graphs) are often required to be invariant to a wide variety of group actions
e.g. permutation or rigid transformation. Therefore, continuous and symmetric
product functions (such as distance functions) on such complex objects must
also be invariant to the product of such group actions. We call these functions
symmetric and factor-wise group invariant (or SFGI functions in short). In this
paper, we first present a general neural network architecture for approximating
SFGI functions. The main contribution of this paper combines this general
neural network with a sketching idea to develop a specific and efficient neural
network which can approximate the $p$-th Wasserstein distance between point
sets. Very importantly, the required model complexity is independent of the
sizes of input point sets. On the theoretical front, to the best of our
knowledge, this is the first result showing that there exists a neural network
with the capacity to approximate Wasserstein distance with bounded model
complexity. Our work provides an interesting integration of sketching ideas for
geometric problems with universal approximation of symmetric functions. On the
empirical front, we present a range of results showing that our newly proposed
neural network architecture performs comparatively or better than other models
(including a SOTA Siamese Autoencoder based approach). In particular, our
neural network generalizes significantly better and trains much faster than the
SOTA Siamese AE. Finally, this line of investigation could be useful in
exploring effective neural network design for solving a broad range of
geometric optimization problems (e.g., $k$-means in a metric space).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10436">Approximately Equivariant Graph Networks. (arXiv:2308.10436v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Huang_N/0/1/0/all/0/1">Ningyuan Huang</a>, <a href="http://arxiv.org/find/stat/1/au:+Levie_R/0/1/0/all/0/1">Ron Levie</a>, <a href="http://arxiv.org/find/stat/1/au:+Villar_S/0/1/0/all/0/1">Soledad Villar</a></p>
<p>Graph neural networks (GNNs) are commonly described as being permutation
equivariant with respect to node relabeling in the graph. This symmetry of GNNs
is often compared to the translation equivariance of Euclidean convolution
neural networks (CNNs). However, these two symmetries are fundamentally
different: The translation equivariance of CNNs corresponds to symmetries of
the fixed domain acting on the image signals (sometimes known as active
symmetries), whereas in GNNs any permutation acts on both the graph signals and
the graph domain (sometimes described as passive symmetries). In this work, we
focus on the active symmetries of GNNs, by considering a learning setting where
signals are supported on a fixed graph. In this case, the natural symmetries of
GNNs are the automorphisms of the graph. Since real-world graphs tend to be
asymmetric, we relax the notion of symmetries by formalizing approximate
symmetries via graph coarsening. We present a bias-variance formula that
quantifies the tradeoff between the loss in expressivity and the gain in the
regularity of the learned estimator, depending on the chosen symmetry group. To
illustrate our approach, we conduct extensive experiments on image inpainting,
traffic flow prediction, and human pose estimation with different choices of
symmetries. We show theoretically and empirically that the best generalization
performance can be achieved by choosing a suitably larger group than the graph
automorphism, but smaller than the permutation group.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14789">Scattering with Neural Operators. (arXiv:2308.14789v2 [hep-th] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-th/1/au:+Mizera_S/0/1/0/all/0/1">Sebastian Mizera</a></p>
<p>Recent advances in machine learning establish the ability of certain
neural-network architectures called neural operators to approximate maps
between function spaces. Motivated by a prospect of employing them in
fundamental physics, we examine applications to scattering processes in quantum
mechanics. We use an iterated variant of Fourier neural operators to learn the
physics of Schr\"odinger operators, which map from the space of initial wave
functions and potentials to the final wave functions. These deep operator
learning ideas are put to test in two concrete problems: a neural operator
predicting the time evolution of a wave packet scattering off a central
potential in $1+1$ dimensions, and the double-slit experiment in $2+1$
dimensions. At inference, neural operators can become orders of magnitude more
efficient compared to traditional finite-difference solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07383">Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v4 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bouland_A/0/1/0/all/0/1">Ali Bouland</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_S/0/1/0/all/0/1">Shengyuan Niu</a>, <a href="http://arxiv.org/find/eess/1/au:+Paruchuri_S/0/1/0/all/0/1">Sai Tej Paruchuri</a>, <a href="http://arxiv.org/find/eess/1/au:+Kurdila_A/0/1/0/all/0/1">Andrew Kurdila</a>, <a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1">John Burns</a>, <a href="http://arxiv.org/find/eess/1/au:+Schuster_E/0/1/0/all/0/1">Eugenio Schuster</a></p>
<p>This paper studies convergence rates for some value function approximations
that arise in a collection of reproducing kernel Hilbert spaces (RKHS)
$H(\Omega)$. By casting an optimal control problem in a specific class of
native spaces, strong rates of convergence are derived for the operator
equation that enables offline approximations that appear in policy iteration.
Explicit upper bounds on error in value function and controller approximations
are derived in terms of power function $\mathcal{P}_{H,N}$ for the space of
finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These
bounds are geometric in nature and refine some well-known, now classical
results concerning convergence of approximations of value functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08835">Intelligent machines work in unstructured environments by differential neuromorphic computing. (arXiv:2309.08835v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Shengbo Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1">Shuo Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1">Chenyu Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Occhipinti_E/0/1/0/all/0/1">Edoardo Occhipinti</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1">Cong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Shurui Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1">Hubin Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_G/0/1/0/all/0/1">Guohua Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Nathan_A/0/1/0/all/0/1">Arokia Nathan</a>, <a href="http://arxiv.org/find/eess/1/au:+Dahiya_R/0/1/0/all/0/1">Ravinder Dahiya</a>, <a href="http://arxiv.org/find/eess/1/au:+Occhipinti_L/0/1/0/all/0/1">Luigi Occhipinti</a></p>
<p>Efficient operation of intelligent machines in the real world requires
methods that allow them to understand and predict the uncertainties presented
by the unstructured environments with good accuracy, scalability and
generalization, similar to humans. Current methods rely on pretrained networks
instead of continuously learning from the dynamic signal properties of working
environments and suffer inherent limitations, such as data-hungry procedures,
and limited generalization capabilities. Herein, we present a memristor-based
differential neuromorphic computing, perceptual signal processing and learning
method for intelligent machines. The main features of environmental information
such as amplification (&gt;720%) and adaptation (&lt;50%) of mechanical stimuli
encoded in memristors, are extracted to obtain human-like processing in
unstructured environments. The developed method takes advantage of the
intrinsic multi-state property of memristors and exhibits good scalability and
generalization, as confirmed by validation in two different application
scenarios: object grasping and autonomous driving. In the former, a robot hand
experimentally realizes safe and stable grasping through fast learning (in ~1
ms) the unknown object features (e.g., sharp corner and smooth surface) with a
single memristor. In the latter, the decision-making information of 10
unstructured environments in autonomous driving (e.g., overtaking cars,
pedestrians) is accurately (94%) extracted with a 40*25 memristor array. By
mimicking the intrinsic nature of human low-level perception mechanisms, the
electronic memristive neuromorphic circuit-based method, presented here shows
the potential for adapting to diverse sensing technologies and helping
intelligent machines generate smart high-level decisions in the real world.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09969">Prompt a Robot to Walk with Large Language Models. (arXiv:2309.09969v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yen-Jen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bike Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreenath_K/0/1/0/all/0/1">Koushil Sreenath</a></p>
<p>Large language models (LLMs) pre-trained on vast internet-scale data have
showcased remarkable capabilities across diverse domains. Recently, there has
been escalating interest in deploying LLMs for robotics, aiming to harness the
power of foundation models in real-world settings. However, this approach faces
significant challenges, particularly in grounding these models in the physical
world and in generating dynamic robot motions. To address these issues, we
introduce a novel paradigm in which we use few-shot prompts collected from the
physical environment, enabling the LLM to autoregressively generate low-level
control commands for robots without task-specific fine-tuning. Experiments
across various robots and environments validate that our method can effectively
prompt a robot to walk. We thus illustrate how LLMs can proficiently function
as low-level feedback controllers for dynamic motion control even in
high-dimensional robotic systems. The project website and source code can be
found at: https://prompt2walk.github.io/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16397">Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zenan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1">Fan Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Da_F/0/1/0/all/0/1">Fang Da</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hang Zhao</a></p>
<p>Offline Reinforcement Learning (RL) has emerged as a promising framework for
learning policies without active interactions, making it especially appealing
for autonomous driving tasks. Recent successes of Transformers inspire casting
offline RL as sequence modeling, which performs well in long-horizon tasks.
However, they are overly optimistic in stochastic environments with incorrect
assumptions that the same goal can be consistently achieved by identical
actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer
(UNREST) for planning in stochastic driving environments without introducing
additional transition or complex generative models. Specifically, UNREST
estimates state uncertainties by the conditional mutual information between
transitions and returns, and segments sequences accordingly. Discovering the
`uncertainty accumulation' and `temporal locality' properties of driving
environments, UNREST replaces the global returns in decision transformers with
less uncertain truncated returns, to learn from true outcomes of agent actions
rather than environment transitions. We also dynamically evaluate environmental
uncertainty during inference for cautious planning. Extensive experimental
results demonstrate UNREST's superior performance in various driving scenarios
and the power of our uncertainty estimation strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17290">In search of dispersed memories: Generative diffusion models are associative memory networks. (arXiv:2309.17290v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ambrogioni_L/0/1/0/all/0/1">Luca Ambrogioni</a></p>
<p>Uncovering the mechanisms behind long-term memory is one of the most
fascinating open problems in neuroscience and artificial intelligence.
Artificial associative memory networks have been used to formalize important
aspects of biological memory. Generative diffusion models are a type of
generative machine learning techniques that have shown great performance in
many tasks. Like associative memory systems, these networks define a dynamical
system that converges to a set of target states. In this work we show that
generative diffusion models can be interpreted as energy-based models and that,
when trained on discrete patterns, their energy function is (asymptotically)
identical to that of modern Hopfield networks. This equivalence allows us to
interpret the supervised training of diffusion models as a synaptic learning
process that encodes the associative dynamics of a modern Hopfield network in
the weight structure of a deep neural network. Leveraging this connection, we
formulate a generalized framework for understanding the formation of long-term
memory, where creative generation and memory recall can be seen as parts of a
unified continuum.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07958">Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v3 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md Mahbubur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceka_I/0/1/0/all/0/1">Ira Ceka</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chengzhi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Saikat Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1">Baishakhi Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1">Wei Le</a></p>
<p>Deep learning vulnerability detection has shown promising results in recent
years. However, an important challenge that still blocks it from being very
useful in practice is that the model is not robust under perturbation and it
cannot generalize well over the out-of-distribution (OOD) data, e.g., applying
a trained model to unseen projects in real world. We hypothesize that this is
because the model learned non-robust features, e.g., variable names, that have
spurious correlations with labels. When the perturbed and OOD datasets no
longer have the same spurious features, the model prediction fails. To address
the challenge, in this paper, we introduced causality into deep learning
vulnerability detection. Our approach CausalVul consists of two phases. First,
we designed novel perturbations to discover spurious features that the model
may use to make predictions. Second, we applied the causal learning algorithms,
specifically, do-calculus, on top of existing deep learning models to
systematically remove the use of spurious features and thus promote causal
based prediction. Our results show that CausalVul consistently improved the
model accuracy, robustness and OOD performance for all the state-of-the-art
models and datasets we experimented. To the best of our knowledge, this is the
first work that introduces do calculus based causal learning to software
engineering models and shows it's indeed useful for improving the model
accuracy, robustness and generalization. Our replication package is located at
https://figshare.com/s/0ffda320dcb96c249ef2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09299">Digital Twin Accelerated Deep Reinforcement Learning for Online Admission Control of Network Slicing. (arXiv:2310.09299v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1">Zhenyu Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1">Xiaohu You</a></p>
<p>The proliferation of diverse wireless services in 5G and beyond has led to
the emergence of network slicing technologies. Among these, admission control
plays a crucial role in achieving service-oriented optimization goals through
the selective acceptance of service requests. Although deep reinforcement
learning (DRL) forms the foundation in many admission control approaches thanks
to its effectiveness and flexibility, initial instability with excessive
convergence delay of DRL models hinders their deployment in real-world
networks. We propose a digital twin (DT) accelerated DRL solution to address
this issue. Specifically, we first formulate the admission decision-making
process as a semi-Markov decision process, which is subsequently simplified
into an equivalent discrete-time Markov decision process to facilitate the
implementation of DRL methods. A neural network-based DT is established with a
customized output layer for queuing systems, trained through supervised
learning, and then employed to assist the training phase of the DRL model.
Extensive simulations show that the DT-accelerated DRL improves resource
utilization by over 40% compared to the directly trained state-of-the-art
dueling deep Q-learning model. This improvement is achieved while preserving
the model's capability to optimize the long-term rewards of the admission
process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11248">CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. (arXiv:2310.11248v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yangruibo Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zijian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1">Wasi Uddin Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Hantian Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Ming Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1">Nihal Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanathan_M/0/1/0/all/0/1">Murali Krishna Ramanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1">Ramesh Nallapati</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1">Parminder Bhatia</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1">Bing Xiang</a></p>
<p>Code completion models have made significant progress in recent years, yet
current popular evaluation datasets, such as HumanEval and MBPP, predominantly
focus on code completion tasks within a single file. This over-simplified
setting falls short of representing the real-world software development
scenario where repositories span multiple files with numerous cross-file
dependencies, and accessing and understanding cross-file context is often
required to complete the code correctly.
</p>
<p>To fill in this gap, we propose CrossCodeEval, a diverse and multilingual
code completion benchmark that necessitates an in-depth cross-file contextual
understanding to complete the code accurately. CrossCodeEval is built on a
diverse set of real-world, open-sourced, permissively-licensed repositories in
four popular programming languages: Python, Java, TypeScript, and C#. To create
examples that strictly require cross-file context for accurate completion, we
propose a straightforward yet efficient static-analysis-based approach to
pinpoint the use of cross-file context within the current file.
</p>
<p>Extensive experiments on state-of-the-art code language models like CodeGen
and StarCoder demonstrate that CrossCodeEval is extremely challenging when the
relevant cross-file context is absent, and we see clear improvements when
adding these context into the prompt. However, despite such improvements, the
pinnacle of performance remains notably unattained even with the
highest-performing model, indicating that CrossCodeEval is also capable of
assessing model's capability in leveraging extensive context to make better
code completion. Finally, we benchmarked various methods in retrieving
cross-file context, and show that CrossCodeEval can also be used to measure the
capability of code retrievers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14710">Random Forest Kernel for High-Dimension Low Sample Size Classification. (arXiv:2310.14710v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cavalheiro_L/0/1/0/all/0/1">Lucca Portes Cavalheiro</a>, <a href="http://arxiv.org/find/stat/1/au:+Bernard_S/0/1/0/all/0/1">Simon Bernard</a>, <a href="http://arxiv.org/find/stat/1/au:+Barddal_J/0/1/0/all/0/1">Jean Paul Barddal</a>, <a href="http://arxiv.org/find/stat/1/au:+Heutte_L/0/1/0/all/0/1">Laurent Heutte</a></p>
<p>High dimension, low sample size (HDLSS) problems are numerous among
real-world applications of machine learning. From medical images to text
processing, traditional machine learning algorithms are usually unsuccessful in
learning the best possible concept from such data. In a previous work, we
proposed a dissimilarity-based approach for multi-view classification, the
Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for
such problems. In this work, we transpose the core principle of this approach
to solving HDLSS classification problems, by using the RF similarity measure as
a learned precomputed SVM kernel (RFSVM). We show that such a learned
similarity measure is particularly suited and accurate for this classification
context. Experiments conducted on 40 public HDLSS classification datasets,
supported by rigorous statistical analyses, show that the RFSVM method
outperforms existing methods for the majority of HDLSS problems and remains at
the same time very competitive for low or non-HDLSS problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14858">Dynamically Weighted Federated k-Means. (arXiv:2310.14858v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holzer_P/0/1/0/all/0/1">Patrick Holzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacob_T/0/1/0/all/0/1">Tania Jacob</a>, <a href="http://arxiv.org/find/cs/1/au:+Kavane_S/0/1/0/all/0/1">Shubham Kavane</a></p>
<p>Federated clustering, an integral aspect of federated machine learning,
enables multiple data sources to collaboratively cluster their data,
maintaining decentralization and preserving privacy. In this paper, we
introduce a novel federated clustering algorithm named Dynamically Weighted
Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering,
to address the challenges associated with distributed data sources and
heterogeneous data. Our proposed algorithm combines the benefits of traditional
clustering techniques with the privacy and scalability benefits offered by
federated learning. The algorithm facilitates collaborative clustering among
multiple data owners, allowing them to cluster their local data collectively
while exchanging minimal information with the central coordinator. The
algorithm optimizes the clustering process by adaptively aggregating cluster
assignments and centroids from each data source, thereby learning a global
clustering solution that reflects the collective knowledge of the entire
federated network. We address the issue of empty clusters, which commonly
arises in the context of federated clustering. We conduct experiments on
multiple datasets and data distribution settings to evaluate the performance of
our algorithm in terms of clustering score, accuracy, and v-measure. The
results demonstrate that our approach can match the performance of the
centralized classical k-means baseline, and outperform existing federated
clustering methods like k-FED in realistic scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15479">AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Suh_N/0/1/0/all/0/1">Namjoon Suh</a>, <a href="http://arxiv.org/find/stat/1/au:+Lin_X/0/1/0/all/0/1">Xiaofeng Lin</a>, <a href="http://arxiv.org/find/stat/1/au:+Hsieh_D/0/1/0/all/0/1">Din-Yin Hsieh</a>, <a href="http://arxiv.org/find/stat/1/au:+Honarkhah_M/0/1/0/all/0/1">Merhdad Honarkhah</a>, <a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1">Guang Cheng</a></p>
<p>Diffusion model has become a main paradigm for synthetic data generation in
many subfields of modern machine learning, including computer vision, language
model, or speech synthesis. In this paper, we leverage the power of diffusion
model for generating synthetic tabular data. The heterogeneous features in
tabular data have been main obstacles in tabular data synthesis, and we tackle
this problem by employing the auto-encoder architecture. When compared with the
state-of-the-art tabular synthesizers, the resulting synthetic tables from our
model show nice statistical fidelities to the real data, and perform well in
downstream tasks for machine learning utilities. We conducted the experiments
over $15$ publicly available datasets. Notably, our model adeptly captures the
correlations among features, which has been a long-standing challenge in
tabular data synthesis. Our code is available at
https://github.com/UCLA-Trustworthy-AI-Lab/AutoDiffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20654">Closed Drafting as a Case Study for First-Principle Interpretability, Memory, and Generalizability in Deep Reinforcement Learning. (arXiv:2310.20654v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rezai_R/0/1/0/all/0/1">Ryan Rezai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jason Wang</a></p>
<p>Closed drafting or "pick and pass" is a popular game mechanic where each
round players select a card or other playable element from their hand and pass
the rest to the next player. In this paper, we establish first-principle
methods for studying the interpretability, generalizability, and memory of Deep
Q-Network (DQN) models playing closed drafting games. In particular, we use a
popular family of closed drafting games called "Sushi Go Party", in which we
achieve state-of-the-art performance. We fit decision rules to interpret the
decision-making strategy of trained DRL agents by comparing them to the ranking
preferences of different types of human players. As Sushi Go Party can be
expressed as a set of closely-related games based on the set of cards in play,
we quantify the generalizability of DRL models trained on various sets of
cards, establishing a method to benchmark agent performance as a function of
environment unfamiliarity. Using the explicitly calculable memory of other
player's hands in closed drafting games, we create measures of the ability of
DRL models to learn memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00690">What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doroudian_S/0/1/0/all/0/1">Shahin Doroudian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zekun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1">Aidong Lu</a></p>
<p>The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01404">Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs. (arXiv:2311.01404v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Scagliotti_A/0/1/0/all/0/1">Alessandro Scagliotti</a>, <a href="http://arxiv.org/find/math/1/au:+Farinelli_S/0/1/0/all/0/1">Sara Farinelli</a></p>
<p>The term "Normalizing Flows" is related to the task of constructing
invertible transport maps between probability measures by means of deep neural
networks. In this paper, we consider the problem of recovering the
$W_2$-optimal transport map $T$ between absolutely continuous measures
$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural
ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the
controlled vector fields, the optimal transport map is contained in the
$C^0_c$-closure of the flows generated by the system. Assuming that discrete
approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available,
we use a discrete optimal coupling $\gamma_N$ to define an optimal control
problem. With a $\Gamma$-convergence argument, we prove that its solutions
correspond to flows that approximate the optimal transport map $T$. Finally,
taking advantage of the Pontryagin Maximum Principle, we propose an iterative
numerical scheme for the resolution of the optimal control problem, resulting
in an algorithm for the practical computation of the approximated optimal
transport map.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02058">LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Weikang Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yifeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rutav Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuke Zhu</a></p>
<p>We introduce LOTUS, a continual imitation learning algorithm that empowers a
physical robot to continuously and efficiently learn to solve new manipulation
tasks throughout its lifespan. The core idea behind LOTUS is constructing an
ever-growing skill library from a sequence of new tasks with a small number of
human demonstrations. LOTUS starts with a continual skill discovery process
using an open-vocabulary vision model, which extracts skills as recurring
patterns presented in unsegmented demonstrations. Continual skill discovery
updates existing skills to avoid catastrophic forgetting of previous tasks and
adds new skills to solve novel tasks. LOTUS trains a meta-controller that
flexibly composes various skills to tackle vision-based manipulation tasks in
the lifelong learning process. Our comprehensive experiments show that LOTUS
outperforms state-of-the-art baselines by over 11% in success rate, showing its
superior knowledge transfer ability compared to prior methods. More results and
videos can be found on the project website:
https://ut-austin-rpl.github.io/Lotus/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02738">Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion. (arXiv:2311.02738v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pronovost_E/0/1/0/all/0/1">Ethan Pronovost</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesina_M/0/1/0/all/0/1">Meghana Reddy Ganesina</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendy_N/0/1/0/all/0/1">Noureldin Hendy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zeyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1">Andres Morales</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1">Nicholas Roy</a></p>
<p>Automated creation of synthetic traffic scenarios is a key part of validating
the safety of autonomous vehicles (AVs). In this paper, we propose Scenario
Diffusion, a novel diffusion-based architecture for generating traffic
scenarios that enables controllable scenario generation. We combine latent
diffusion, object detection and trajectory regression to generate distributions
of synthetic agent poses, orientations and trajectories simultaneously. To
provide additional control over the generated scenario, this distribution is
conditioned on a map and sets of tokens describing the desired scenario. We
show that our approach has sufficient expressive capacity to model diverse
traffic patterns and generalizes to different geographical regions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02818">Signal Processing Meets SGD: From Momentum to Filter. (arXiv:2311.02818v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhipeng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1">Guisong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dazhou Li</a></p>
<p>In the field of deep learning, Stochastic Gradient Descent (SGD) and its
momentum-based variants are the predominant choices for optimization
algorithms. Despite all that, these momentum strategies, which accumulate
historical gradients by using a fixed $\beta$ hyperparameter to smooth the
optimization processing, often neglect the potential impact of the variance of
historical gradients on the current gradient estimation. In the gradient
variance during training, fluctuation indicates the objective function does not
meet the Lipschitz continuity condition at all time, which raises the
troublesome optimization problem. This paper aims to explore the potential
benefits of reducing the variance of historical gradients to make optimizer
converge to flat solutions. Moreover, we proposed a new optimization method
based on reducing the variance. We employed the Wiener filter theory to enhance
the first moment estimation of SGD, notably introducing an adaptive weight to
optimizer. Specifically, the adaptive weight dynamically changes along with
temporal fluctuation of gradient variance during deep learning model training.
Experimental results demonstrated our proposed adaptive weight optimizer, SGDF
(Stochastic Gradient Descent With Filter), can achieve satisfactory performance
compared with state-of-the-art optimizers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03488">Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems. (arXiv:2311.03488v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lilienthal_D/0/1/0/all/0/1">Derek Lilienthal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1">Paul Mello</a>, <a href="http://arxiv.org/find/cs/1/au:+Eirinaki_M/0/1/0/all/0/1">Magdalini Eirinaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1">Stas Tiomkin</a></p>
<p>While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Module (SDRM), which captures the intricate patterns
of real-world datasets required for training highly accurate recommender
systems. SDRM allows for the generation of synthetic data that can replace
existing datasets to preserve user privacy, or augment existing datasets to
address excessive data sparsity. Our method outperforms competing baselines
such as generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$k$ and
4.65% in NDCG@$k$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05836">UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1">Jing Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1">Qinrui Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1">Shu Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1">Siwei Lyu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xi Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a></p>
<p>In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06233">Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. (arXiv:2311.06233v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06668">In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering. (arXiv:2311.06668v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1">Lei Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a></p>
<p>Large language models (LLMs) demonstrate emergent in-context learning
capabilities, where they adapt to new tasks based on example demonstrations.
However, in-context learning has seen limited effectiveness in many settings,
is difficult to quantitatively control and takes up context window space. To
overcome these limitations, we propose an alternative approach that recasts
in-context learning as in-context vectors (ICV). Using ICV has two steps. We
first use a forward pass on demonstration examples to create the in-context
vector from the latent embedding of the LLM. This vector captures essential
information about the intended task. On a new query, instead of adding
demonstrations to the prompt, we shift the latent states of the LLM using the
ICV. The ICV approach has several benefits: 1) it enables the LLM to more
effectively follow the demonstration examples; 2) it's easy to control by
adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by
removing the in-context demonstrations; 4) ICV is computationally much more
efficient than fine-tuning. We demonstrate that ICV achieves better performance
compared to standard in-context learning and fine-tuning on diverse tasks
including safety, style transfer, role-playing and formatting. Moreover, we
show that we can flexibly teach LLM to simultaneously follow different types of
instructions by simple vector arithmetics on the corresponding ICVs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08149">Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes. (arXiv:2311.08149v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trottet_C/0/1/0/all/0/1">C&#xe9;cile Trottet</a>, <a href="http://arxiv.org/find/cs/1/au:+Schurch_M/0/1/0/all/0/1">Manuel Sch&#xfc;rch</a>, <a href="http://arxiv.org/find/cs/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/cs/1/au:+Barua_I/0/1/0/all/0/1">Imon Barua</a>, <a href="http://arxiv.org/find/cs/1/au:+Petelytska_L/0/1/0/all/0/1">Liubov Petelytska</a>, <a href="http://arxiv.org/find/cs/1/au:+Distler_O/0/1/0/all/0/1">Oliver Distler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffmann_Vold_A/0/1/0/all/0/1">Anna-Maria Hoffmann-Vold</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1">Michael Krauthammer</a>, the <a href="http://arxiv.org/find/cs/1/au:+collaborators_E/0/1/0/all/0/1">EUSTAR collaborators</a></p>
<p>In this paper, we propose a deep generative time series approach using latent
temporal processes for modeling and holistically analyzing complex disease
trajectories. We aim to find meaningful temporal latent representations of an
underlying generative process that explain the observed disease trajectories in
an interpretable and comprehensive way. To enhance the interpretability of
these latent temporal processes, we develop a semi-supervised approach for
disentangling the latent space using established medical concepts. By combining
the generative approach with medical knowledge, we leverage the ability to
discover novel aspects of the disease while integrating medical concepts into
the model. We show that the learned temporal latent processes can be utilized
for further data analysis and clinical hypothesis testing, including finding
similar patients and clustering the disease into new sub-types. Moreover, our
method enables personalized online monitoring and prediction of multivariate
time series including uncertainty quantification. We demonstrate the
effectiveness of our approach in modeling systemic sclerosis, showcasing the
potential of our machine learning model to capture complex disease trajectories
and acquire new medical knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08502">Variational Quantum Eigensolver with Constraints (VQEC): Solving Constrained Optimization Problems via VQE. (arXiv:2311.08502v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Le_T/0/1/0/all/0/1">Thinh Viet Le</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kekatos_V/0/1/0/all/0/1">Vassilis Kekatos</a></p>
<p>Variational quantum approaches have shown great promise in finding
near-optimal solutions to computationally challenging tasks. Nonetheless,
enforcing constraints in a disciplined fashion has been largely unexplored. To
address this gap, this work proposes a hybrid quantum-classical algorithmic
paradigm termed VQEC that extends the celebrated VQE to handle optimization
with constraints. As with the standard VQE, the vector of optimization
variables is captured by the state of a variational quantum circuit (VQC). To
deal with constraints, VQEC optimizes a Lagrangian function classically over
both the VQC parameters as well as the dual variables associated with
constraints. To comply with the quantum setup, variables are updated via a
perturbed primal-dual method leveraging the parameter shift rule. Among a wide
gamut of potential applications, we showcase how VQEC can approximately solve
quadratically-constrained binary optimization (QCBO) problems, find stochastic
binary policies satisfying quadratic constraints on the average and in
probability, and solve large-scale linear programs (LP) over the probability
simplex. Under an assumption on the error for the VQC to approximate an
arbitrary probability mass function (PMF), we provide bounds on the optimality
gap attained by a VQC. Numerical tests on a quantum simulator investigate the
effect of various parameters and corroborate that VQEC can generate
high-quality solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09574">LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1">Vivek Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoli Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1">Vrishab Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1">Brent Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1">Oscar Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1">Rebecca Rojansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1">Andrew Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1">Fabiola Valvert</a>, <a href="http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1">Edward Briercheck</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1">David Weinstock</a>, <a href="http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1">Yasodha Natkunam</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1">Sebastian Fernandez-Pol</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1">Pranav Rajpurkar</a></p>
<p>The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09690">CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs. (arXiv:2311.09690v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hanpeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Junwei Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Juntao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yanghua Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yibo Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haibin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chuan Wu</a></p>
<p>Deep Neural Networks (DNNs) have shown excellent performance in a wide range
of machine learning applications. Knowing the latency of running a DNN model or
tensor program on a specific device is useful in various tasks, such as DNN
graph- or tensor-level optimization and device selection. Considering the large
space of DNN models and devices that impede direct profiling of all
combinations, recent efforts focus on building a predictor to model the
performance of DNN models on different devices. However, none of the existing
attempts have achieved a cost model that can accurately predict the performance
of various tensor programs while supporting both training and inference
accelerators. We propose CDMPP, an efficient tensor program latency prediction
framework for both cross-model and cross-device prediction. We design an
informative but efficient representation of tensor programs, called compact
ASTs, and a pre-order-based positional encoding method, to capture the internal
structure of tensor programs. We develop a domain-adaption-inspired method to
learn domain-invariant representations and devise a KMeans-based sampling
algorithm, for the predictor to learn from different domains (i.e., different
DNN operators and devices). Our extensive experiments on a diverse range of DNN
models and devices demonstrate that CDMPP significantly outperforms
state-of-the-art baselines with 14.03% and 10.85% prediction error for
cross-model and cross-device prediction, respectively, and one order of
magnitude higher training efficiency. The implementation and the expanded
dataset are available at https://github.com/joapolarbear/cdmpp.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09790">Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting. (arXiv:2311.09790v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilbert_R/0/1/0/all/0/1">Romain Ilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thai V. Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zonghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. Our
optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09930">A Framework for Monitoring and Retraining Language Models in Real-World Applications. (arXiv:2311.09930v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasundra_J/0/1/0/all/0/1">Jaykumar Kasundra</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1">Claudia Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirsafian_M/0/1/0/all/0/1">Melicaalsadat Mirsafian</a>, <a href="http://arxiv.org/find/cs/1/au:+Skylaki_S/0/1/0/all/0/1">Stavroula Skylaki</a></p>
<p>In the Machine Learning (ML) model development lifecycle, training candidate
models using an offline holdout dataset and identifying the best model for the
given task is only the first step. After the deployment of the selected model,
continuous model monitoring and model retraining is required in many real-world
applications. There are multiple reasons for retraining, including data or
concept drift, which may be reflected on the model performance as monitored by
an appropriate metric. Another motivation for retraining is the acquisition of
increasing amounts of data over time, which may be used to retrain and improve
the model performance even in the absence of drifts. We examine the impact of
various retraining decision points on crucial factors, such as model
performance and resource utilization, in the context of Multilabel
Classification models. We explain our key decision points and propose a
reference framework for designing an effective model retraining strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10090">JaxMARL: Multi-Agent RL Environments in JAX. (arXiv:2311.10090v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1">Alexander Rutherford</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1">Benjamin Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallici_M/0/1/0/all/0/1">Matteo Gallici</a>, <a href="http://arxiv.org/find/cs/1/au:+Cook_J/0/1/0/all/0/1">Jonathan Cook</a>, <a href="http://arxiv.org/find/cs/1/au:+Lupu_A/0/1/0/all/0/1">Andrei Lupu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ingvarsson_G/0/1/0/all/0/1">Gardar Ingvarsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1">Timon Willi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Akbir Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1">Christian Schroeder de Witt</a>, <a href="http://arxiv.org/find/cs/1/au:+Souly_A/0/1/0/all/0/1">Alexandra Souly</a>, <a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1">Saptarashmi Bandyopadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1">Mikayel Samvelyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_R/0/1/0/all/0/1">Robert Tjarko Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacerda_B/0/1/0/all/0/1">Bruno Lacerda</a>, <a href="http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1">Nick Hawes</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rocktaschel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Nicolaus Foerster</a></p>
<p>Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.06905">Less is More: Proxy Datasets in NAS approaches. (arXiv:2203.06905v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1">Brian Moser</a>, <a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1">Federico Raue</a>, <a href="http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1">J&#xf6;rn Hees</a>, <a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1">Andreas Dengel</a></p>
<p>Neural Architecture Search (NAS) defines the design of Neural Networks as a
search problem. Unfortunately, NAS is computationally intensive because of
various possibilities depending on the number of elements in the design and the
possible connections between them. In this work, we extensively analyze the
role of the dataset size based on several sampling approaches for reducing the
dataset size (unsupervised and supervised cases) as an agnostic approach to
reduce search time. We compared these techniques with four common NAS
approaches in NAS-Bench-201 in roughly 1,400 experiments on CIFAR-100. One of
our surprising findings is that in most cases we can reduce the amount of
training data to 25\%, consequently reducing search time to 25\%, while at the
same time maintaining the same accuracy as if training on the full dataset.
Additionally, some designs derived from subsets out-perform designs derived
from the full dataset by up to 22 p.p. accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18150">Understanding the Helpfulness of Stale Bot for Pull-based Development: An Empirical Study of 20 Large Open-Source Projects. (arXiv:2305.18150v1 [cs.SE] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khatoonabadi_S/0/1/0/all/0/1">SayedHassan Khatoonabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1">Diego Elias Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujahid_S/0/1/0/all/0/1">Suhaib Mujahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1">Emad Shihab</a></p>
<p>Pull Requests (PRs) that are neither progressed nor resolved clutter the list
of PRs, making it difficult for the maintainers to manage and prioritize
unresolved PRs. To automatically track, follow up, and close such inactive PRs,
Stale bot was introduced by GitHub. Despite its increasing adoption, there are
ongoing debates on whether using Stale bot alleviates or exacerbates the
problem of inactive PRs. To better understand if and how Stale bot helps
projects in their pull-based development workflow, we perform an empirical
study of 20 large and popular open-source projects. We find that Stale bot can
help deal with a backlog of unresolved PRs as the projects closed more PRs
within the first few months of adoption. Moreover, Stale bot can help improve
the efficiency of the PR review process as the projects reviewed PRs that ended
up merged and resolved PRs that ended up closed faster after the adoption.
However, Stale bot can also negatively affect the contributors as the projects
experienced a considerable decrease in their number of active contributors
after the adoption. Therefore, relying solely on Stale bot to deal with
inactive PRs may lead to decreased community engagement and an increased
probability of contributor abandonment.
</p>
</p>
</div>

    </div>
    </body>
    