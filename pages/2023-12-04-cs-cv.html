<!DOCTYPE html>
<html>
<head>
<title>2023-12-04-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.17937">Unlocking Spatial Comprehension in Text-to-Image Diffusion Models. (arXiv:2311.17937v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1">Mohammad Mahdi Derakhshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menglin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Behl_H/0/1/0/all/0/1">Harkirat Behl</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G. M. Snoek</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1">Victor R&#xfc;hle</a></p>
<p>We propose CompFuser, an image generation pipeline that enhances spatial
comprehension and attribute assignment in text-to-image generative models. Our
pipeline enables the interpretation of instructions defining spatial
relationships between objects in a scene, such as `An image of a gray cat on
the left of an orange dog', and generate corresponding images. This is
especially important in order to provide more control to the user. CompFuser
overcomes the limitation of existing text-to-image diffusion models by decoding
the generation of multiple objects into iterative steps: first generating a
single object and then editing the image by placing additional objects in their
designated positions. To create training data for spatial comprehension and
attribute assignment we introduce a synthetic data generation process, that
leverages a frozen large language model and a frozen layout-based diffusion
model for object placement. We compare our approach to strong baselines and
show that our model outperforms state-of-the-art image generation models in
spatial comprehension and attribute assignment, despite being 3x to 5x smaller
in parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17938">Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations. (arXiv:2311.17938v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jianxiong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xiaoying Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Wu</a></p>
<p>Active recognition, which allows intelligent agents to explore observations
for better recognition performance, serves as a prerequisite for various
embodied AI tasks, such as grasping, navigation and room arrangements. Given
the evolving environment and the multitude of object classes, it is impractical
to include all possible classes during the training stage. In this paper, we
aim at advancing active open-vocabulary recognition, empowering embodied agents
to actively perceive and classify arbitrary objects. However, directly adopting
recent open-vocabulary classification models, like Contrastive Language Image
Pretraining (CLIP), poses its unique challenges. Specifically, we observe that
CLIP's performance is heavily affected by the viewpoint and occlusions,
compromising its reliability in unconstrained embodied perception scenarios.
Further, the sequential nature of observations in agent-environment
interactions necessitates an effective method for integrating features that
maintains discriminative strength for open-vocabulary classification. To
address these issues, we introduce a novel agent for active open-vocabulary
recognition. The proposed method leverages inter-frame and inter-concept
similarities to navigate agent movements and to fuse features, without relying
on class-specific knowledge. Compared to baseline CLIP model with 29.6%
accuracy on ShapeNet dataset, the proposed agent could achieve 53.3% accuracy
for open-vocabulary recognition, without any fine-tuning to the equipped CLIP
model. Additional experiments conducted with the Habitat simulator further
affirm the efficacy of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17940">Scene Summarization: Clustering Scene Videos into Spatially Diverse Frames. (arXiv:2311.17940v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingzhi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Ankush Pratap Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Felix Juefei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chen Feng</a></p>
<p>We propose scene summarization as a new video-based scene understanding task.
It aims to summarize a long video walkthrough of a scene into a small set of
frames that are spatially diverse in the scene, which has many impotant
applications, such as in surveillance, real estate, and robotics. It stems from
video summarization but focuses on long and continuous videos from moving
cameras, instead of user-edited fragmented video clips that are more commonly
studied in existing video summarization works. Our solution to this task is a
two-stage self-supervised pipeline named SceneSum. Its first stage uses
clustering to segment the video sequence. Our key idea is to combine visual
place recognition (VPR) into this clustering process to promote spatial
diversity. Its second stage needs to select a representative keyframe from each
cluster as the summary while respecting resource constraints such as memory and
disk space limits. Additionally, if the ground truth image trajectory is
available, our method can be easily augmented with a supervised loss to enhance
the clustering and keyframe selection. Extensive experiments on both real-world
and simulated datasets show our method outperforms common video summarization
baselines by 50%
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17942">Object-based (yet Class-agnostic) Video Domain Adaptation. (arXiv:2311.17942v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1">Dantong Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1">Amir Bar</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1">Roei Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1">Anna Rohrbach</a></p>
<p>Existing video-based action recognition systems typically require dense
annotation and struggle in environments when there is significant distribution
shift relative to the training data. Current methods for video domain
adaptation typically fine-tune the model using fully annotated data on a subset
of target domain data or align the representation of the two domains using
bootstrapping or adversarial learning. Inspired by the pivotal role of objects
in recent supervised object-centric action recognition models, we present
Object-based (yet Class-agnostic) Video Domain Adaptation (ODAPT), a simple yet
effective framework for adapting the existing action recognition systems to new
domains by utilizing a sparse set of frames with class-agnostic object
annotations in a target domain. Our model achieves a +6.5 increase when
adapting across kitchens in Epic-Kitchens and a +3.1 increase adapting between
Epic-Kitchens and the EGTEA dataset. ODAPT is a general framework that can also
be combined with previous unsupervised methods, offering a +5.0 boost when
combined with the self-supervised multi-modal method MMSADA and a +1.7 boost
when added to the adversarial-based method TA$^3$N on Epic-Kitchens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17944">LALM: Long-Term Action Anticipation with Language Models. (arXiv:2311.17944v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sanghwan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Daoji Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1">Yongqin Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a></p>
<p>Understanding human activity is a crucial yet intricate task in egocentric
vision, a field that focuses on capturing visual perspectives from the camera
wearer's viewpoint. While traditional methods heavily rely on representation
learning trained on extensive video data, there exists a significant
limitation: obtaining effective video representations proves challenging due to
the inherent complexity and variability in human activities.Furthermore,
exclusive dependence on video-based learning may constrain a model's capability
to generalize across long-tail classes and out-of-distribution scenarios.
</p>
<p>In this study, we introduce a novel approach for long-term action
anticipation using language models (LALM), adept at addressing the complex
challenges of long-term activity understanding without the need for extensive
training. Our method incorporates an action recognition model to track previous
action sequences and a vision-language model to articulate relevant
environmental details. By leveraging the context provided by these past events,
we devise a prompting strategy for action anticipation using large language
models (LLMs). Moreover, we implement Maximal Marginal Relevance for example
selection to facilitate in-context learning of the LLMs. Our experimental
results demonstrate that LALM surpasses the state-of-the-art methods in the
task of long-term action anticipation on the Ego4D benchmark. We further
validate LALM on two additional benchmarks, affirming its capacity for
generalization across intricate activities with different sets of taxonomies.
These are achieved without specific fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17945">Contrastive Vision-Language Alignment Makes Efficient Instruction Learner. (arXiv:2311.17945v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lizhao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xinyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tianhang Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zhuangwei Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Liuren Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mingkui Tan</a></p>
<p>We study the task of extending the large language model (LLM) into a
vision-language instruction-following model. This task is crucial but
challenging since the LLM is trained on text modality only, making it hard to
effectively digest the visual modality. To address this, existing methods
typically train a visual adapter to align the representation between a
pre-trained vision transformer (ViT) and the LLM by a generative image
captioning loss. However, we find that the generative objective can only
produce weak alignment for vision and language, making the aligned
vision-language model very hungry for the instruction fine-tuning data. In this
paper, we propose CG-VLM that applies both Contrastive and Generative alignment
objectives to effectively align the representation of ViT and LLM. Different
from image level and sentence level alignment in common contrastive learning
settings, CG-VLM aligns the image-patch level features and text-token level
embeddings, which, however, is very hard to achieve as no explicit grounding
patch-token relation provided in standard image captioning datasets. To address
this issue, we propose to maximize the averaged similarity between pooled
image-patch features and text-token embeddings. Extensive experiments
demonstrate that the proposed CG-VLM produces strong vision-language alignment
and is an efficient instruction learner. For example, using only 10%
instruction tuning data, we reach 95% performance of state-of-the-art method
LLaVA [29] on the zero-shot ScienceQA-Image benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17946">DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback. (arXiv:2311.17946v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Deqing Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yushi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Su Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rassin_R/0/1/0/all/0/1">Royi Rassin</a>, <a href="http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1">Da-Cheng Juan</a>, <a href="http://arxiv.org/find/cs/1/au:+Alon_D/0/1/0/all/0/1">Dana Alon</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1">Charles Herrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1">Sjoerd van Steenkiste</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1">Cyrus Rashtchian</a></p>
<p>Despite their wide-spread success, Text-to-Image models (T2I) still struggle
to produce images that are both aesthetically pleasing and faithful to the
user's input text. We introduce DreamSync, a model-agnostic training algorithm
by design that improves T2I models to be faithful to the text input. DreamSync
builds off a recent insight from TIFA's evaluation framework -- that large
vision-language models (VLMs) can effectively identify the fine-grained
discrepancies between generated images and the text inputs. DreamSync uses this
insight to train T2I models without any labeled data; it improves T2I models
using its own generations. First, it prompts the model to generate several
candidate images for a given input text. Then, it uses two VLMs to select the
best generation: a Visual Question Answering model that measures the alignment
of generated images to the text, and another that measures the generation's
aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I
model to guide its generation towards the selected best generations. DreamSync
does not need any additional human annotation. model architecture changes, or
reinforcement learning. Despite its simplicity, DreamSync improves both the
semantic alignment and aesthetic appeal of two diffusion-based T2I models,
evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA
aesthetic) and human evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17948">Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes. (arXiv:2311.17948v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kung_C/0/1/0/all/0/1">Chi-Hsi Kung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shu-Wei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Hsuan Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Ting Chen</a></p>
<p>In this paper, we study multi-label atomic activity recognition. Despite the
notable progress in action recognition, it is still challenging to recognize
atomic activities due to a deficiency in a holistic understanding of both
multiple road users' motions and their contextual information. In this paper,
we introduce Action-slot, a slot attention-based approach that learns visual
action-centric representations, capturing both motion and contextual
information. Our key idea is to design action slots that are capable of paying
attention to regions where atomic activities occur, without the need for
explicit perception guidance. To further enhance slot attention, we introduce a
background slot that competes with action slots, aiding the training process in
avoiding unnecessary focus on background regions devoid of activities. Yet, the
imbalanced class distribution in the existing dataset hampers the assessment of
rare activities. To address the limitation, we collect a synthetic dataset
called TACO, which is four times larger than OATS and features a balanced
distribution of atomic activities. To validate the effectiveness of our method,
we conduct comprehensive experiments and ablation studies against various
action recognition baselines. We also show that the performance of multi-label
atomic activity recognition on real-world datasets can be improved by
pretraining representations on TACO. We will release our source code and
dataset. See the videos of visualization on the project page:
https://hcis-lab.github.io/Action-slot/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17949">Zero-shot Retrieval: Augmenting Pre-trained Models with Search Engines. (arXiv:2311.17949v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Damirchi_H/0/1/0/all/0/1">Hamed Damirchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1">Cristian Rodr&#xed;guez-Opazo</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1">Ehsan Abbasnejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1">Damien Teney</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Javen Qinfeng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a></p>
<p>Large pre-trained models can dramatically reduce the amount of task-specific
data required to solve a problem, but they often fail to capture
domain-specific nuances out of the box. The Web likely contains the information
necessary to excel on any specific application, but identifying the right data
a priori is challenging. This paper shows how to leverage recent advances in
NLP and multi-modal learning to augment a pre-trained model with search engine
retrieval. We propose to retrieve useful data from the Web at test time based
on test cases that the model is uncertain about. Different from existing
retrieval-augmented approaches, we then update the model to address this
underlying uncertainty. We demonstrate substantial improvements in zero-shot
performance, e.g. a remarkable increase of 15 percentage points in accuracy on
the Stanford Cars and Flowers datasets. We also present extensive experiments
that explore the impact of noisy retrieval and different learning strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17950">Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching. (arXiv:2311.17950v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1">Shitong Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zeyuan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Muxin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xindong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhiqiang Shen</a></p>
<p>The lightweight "local-match-global" matching introduced by SRe2L
successfully creates a distilled dataset with comprehensive information on the
full 224x224 ImageNet-1k. However, this one-sided approach is limited to a
particular backbone, layer, and statistics, which limits the improvement of the
generalization of a distilled dataset. We suggest that sufficient and various
"local-match-global" matching are more precise and effective than a single one
and has the ability to create a distilled dataset with richer information and
better generalization. We call this perspective "generalized matching" and
propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this
work, which aims to create a synthetic dataset with densities, ensuring
consistency with the complete dataset across various backbones, layers, and
statistics. As experimentally demonstrated, G-VBSM is the first algorithm to
obtain strong performance across both small-scale and large-scale datasets.
Specifically, G-VBSM achieves a performance of 38.7% on CIFAR-100 with
128-width ConvNet, 47.6% on Tiny-ImageNet with ResNet18, and 31.4% on the full
224x224 ImageNet-1k with ResNet18, under images per class (IPC) 10, 50, and 10,
respectively. These results surpass all SOTA methods by margins of 3.9%, 6.5%,
and 10.1%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17952">Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation. (arXiv:2311.17952v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minhyeok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dogyoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Suhwan Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1">Heeseung Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1">Ig-Jae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangyoun Lee</a></p>
<p>Referring Image Segmentation (RIS) aims to segment target objects expressed
in natural language within a scene at the pixel level. Various recent RIS
models have achieved state-of-the-art performance by generating contextual
tokens to model multimodal features from pretrained encoders and effectively
fusing them using transformer-based cross-modal attention. While these methods
match language features with image features to effectively identify likely
target objects, they often struggle to correctly understand contextual
information in complex and ambiguous sentences and scenes. To address this
issue, we propose a novel bidirectional token-masking autoencoder (BTMAE)
inspired by the masked autoencoder (MAE). The proposed model learns the context
of image-to-language and language-to-image by reconstructing missing features
in both image and language features at the token level. In other words, this
approach involves mutually complementing across the features of images and
language, with a focus on enabling the network to understand interconnected
deep contextual information between the two modalities. This learning method
enhances the robustness of RIS performance in complex sentences and scenes. Our
BTMAE achieves state-of-the-art performance on three popular datasets, and we
demonstrate the effectiveness of the proposed method through various ablation
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17953">Rethinking Image Editing Detection in the Era of Generative AI Revolution. (arXiv:2311.17953v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhihao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1">Haipeng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinying Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Juan Cao</a></p>
<p>The accelerated advancement of generative AI significantly enhance the
viability and effectiveness of generative regional editing methods. This
evolution render the image manipulation more accessible, thereby intensifying
the risk of altering the conveyed information within original images and even
propagating misinformation. Consequently, there exists a critical demand for
robust capable of detecting the edited images. However, the lack of
comprehensive dataset containing images edited with abundant and advanced
generative regional editing methods poses a substantial obstacle to the
advancement of corresponding detection methods.
</p>
<p>We endeavor to fill the vacancy by constructing the GRE dataset, a
large-scale generative regional editing dataset with the following advantages:
1) Collection of real-world original images, focusing on two frequently edited
scenarios. 2) Integration of a logical and simulated editing pipeline,
leveraging multiple large models in various modalities. 3) Inclusion of various
editing approaches with distinct architectures. 4) Provision of comprehensive
analysis tasks. We perform comprehensive experiments with proposed three tasks:
edited image classification, edited method attribution and edited region
localization, providing analysis of distinct editing methods and evaluation of
detection methods in related fields. We expect that the GRE dataset can promote
further research and exploration in the field of generative region editing
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17954">Transformer-empowered Multi-modal Item Embedding for Enhanced Image Search in E-Commerce. (arXiv:2311.17954v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1">Peng Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Anxiang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a></p>
<p>Over the past decade, significant advances have been made in the field of
image search for e-commerce applications. Traditional image-to-image retrieval
models, which focus solely on image details such as texture, tend to overlook
useful semantic information contained within the images. As a result, the
retrieved products might possess similar image details, but fail to fulfil the
user's search goals. Moreover, the use of image-to-image retrieval models for
products containing multiple images results in significant online product
feature storage overhead and complex mapping implementations. In this paper, we
report the design and deployment of the proposed Multi-modal Item Embedding
Model (MIEM) to address these limitations. It is capable of utilizing both
textual information and multiple images about a product to construct meaningful
product features. By leveraging semantic information from images, MIEM
effectively supplements the image search process, improving the overall
accuracy of retrieval results. MIEM has become an integral part of the Shopee
image search platform. Since its deployment in March 2023, it has achieved a
remarkable 9.90% increase in terms of clicks per user and a 4.23% boost in
terms of orders per user for the image search feature on the Shopee e-commerce
platform.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17955">PEAN: A Diffusion-based Prior-Enhanced Attention Network for Scene Text Image Super-Resolution. (arXiv:2311.17955v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zuoyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shipeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1">Pengfei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Hui Xue</a></p>
<p>Scene text image super-resolution (STISR) aims at simultaneously increasing
the resolution and readability of low-resolution scene text images, thus
boosting the performance of the downstream recognition task. Two factors in
scene text images, semantic information and visual structure, affect the
recognition performance significantly. To mitigate the effects from these
factors, this paper proposes a Prior-Enhanced Attention Network (PEAN).
Specifically, a diffusion-based module is developed to enhance the text prior,
hence offering better guidance for the SR network to generate SR images with
higher semantic accuracy. Meanwhile, the proposed PEAN leverages an
attention-based modulation module to understand scene text images by neatly
perceiving the local and global dependence of images, despite the shape of the
text. A multi-task learning paradigm is employed to optimize the network,
enabling the model to generate legible SR images. As a result, PEAN establishes
new SOTA results on the TextZoom benchmark. Experiments are also conducted to
analyze the importance of the enhanced text prior as a means of improving the
performance of the SR network. Code will be made available at
https://github.com/jdfxzzy/PEAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17956">QuadraNet: Improving High-Order Neural Interaction Efficiency with Hardware-Aware Quadratic Neural Networks. (arXiv:2311.17956v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenhui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fuxun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zirui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jinjun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a></p>
<p>Recent progress in computer vision-oriented neural network designs is mostly
driven by capturing high-order neural interactions among inputs and features.
And there emerged a variety of approaches to accomplish this, such as
Transformers and its variants. However, these interactions generate a large
amount of intermediate state and/or strong data dependency, leading to
considerable memory consumption and computing cost, and therefore compromising
the overall runtime performance. To address this challenge, we rethink the
high-order interactive neural network design with a quadratic computing
approach. Specifically, we propose QuadraNet -- a comprehensive model design
methodology from neuron reconstruction to structural block and eventually to
the overall neural network implementation. Leveraging quadratic neurons'
intrinsic high-order advantages and dedicated computation optimization schemes,
QuadraNet could effectively achieve optimal cognition and computation
performance. Incorporating state-of-the-art hardware-aware neural architecture
search and system integration techniques, QuadraNet could also be well
generalized in different hardware constraint settings and deployment scenarios.
The experiment shows thatQuadraNet achieves up to 1.5$\times$ throughput, 30%
less memory footprint, and similar cognition performance, compared with the
state-of-the-art high-order approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17957">HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting. (arXiv:2311.17957v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wenquan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yufei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Diffusion models have achieved remarkable success in generating realistic
images but suffer from generating accurate human hands, such as incorrect
finger counts or irregular shapes. This difficulty arises from the complex task
of learning the physical structure and pose of hands from training images,
which involves extensive deformations and occlusions. For correct hand
generation, our paper introduces a lightweight post-processing solution called
$\textbf{HandRefiner}$. HandRefiner employs a conditional inpainting approach
to rectify malformed hands while leaving other parts of the image untouched. We
leverage the hand mesh reconstruction model that consistently adheres to the
correct number of fingers and hand shape, while also being capable of fitting
the desired hand pose in the generated image. Given a generated failed image
due to malformed hands, we utilize ControlNet modules to re-inject such correct
hand information. Additionally, we uncover a phase transition phenomenon within
ControlNet as we vary the control strength. It enables us to take advantage of
more readily available synthetic data without suffering from the domain gap
between realistic and synthetic hands. Experiments demonstrate that HandRefiner
can significantly improve the generation quality quantitatively and
qualitatively. The code is available at
https://github.com/wenquanlu/HandRefiner .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17960">Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images. (arXiv:2311.17960v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1">Aayush Kumar Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1">Vaibhav Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+P%2E_P/0/1/0/all/0/1">Prathosh A.P.</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a></p>
<p>Cell segmentation in histopathological images plays a crucial role in
understanding, diagnosing, and treating many diseases. However, data annotation
for this is expensive since there can be a large number of cells per image, and
expert pathologists are needed for labelling images. Instead, our paper focuses
on using weak supervision -- annotation from related tasks -- to induce a
segmenter. Recent foundation models, such as Segment Anything (SAM), can use
prompts to leverage additional supervision during inference. SAM has performed
remarkably well in natural image segmentation tasks; however, its applicability
to cell segmentation has not been explored.
</p>
<p>In response, we investigate guiding the prompting procedure in SAM for weakly
supervised cell segmentation when only bounding box supervision is available.
We develop two workflows: (1) an object detector's output as a test-time prompt
to SAM (D-SAM), and (2) SAM as pseudo mask generator over training data to
train a standalone segmentation model (SAM-S). On finding that both workflows
have some complementary strengths, we develop an integer programming-based
approach to reconcile the two sets of segmentation masks, achieving yet higher
performance. We experiment on three publicly available cell segmentation
datasets namely, ConSep, MoNuSeg, and TNBC, and find that all SAM-based
solutions hugely outperform existing weakly supervised image segmentation
models, obtaining 9-15 pt Dice gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17963">ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model. (arXiv:2311.17963v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1">Xiaowei Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yijiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhengkai Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Ziyi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chaoyou Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qifeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a></p>
<p>As the capabilities of Large-Language Models (LLMs) become widely recognized,
there is an increasing demand for human-machine chat applications. Human
interaction with text often inherently invokes mental imagery, an aspect that
existing LLM-based chatbots like GPT-4 do not currently emulate, as they are
confined to generating text-only content. To bridge this gap, we introduce
ChatIllusion, an advanced Generative multimodal large language model (MLLM)
that combines the capabilities of LLM with not only visual comprehension but
also creativity. Specifically, ChatIllusion integrates Stable Diffusion XL and
Llama, which have been fine-tuned on modest image-caption data, to facilitate
multiple rounds of illustrated chats. The central component of ChatIllusion is
the "GenAdapter," an efficient approach that equips the multimodal language
model with capabilities for visual representation, without necessitating
modifications to the foundational model. Extensive experiments validate the
efficacy of our approach, showcasing its ability to produce diverse and
superior-quality image outputs Simultaneously, it preserves semantic
consistency and control over the dialogue, significantly enhancing the overall
user's quality of experience (QoE). The code is available at
https://github.com/litwellchi/ChatIllusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17967">Discovering Galaxy Features via Dataset Distillation. (arXiv:2311.17967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Haowen Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zishi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kempe_J/0/1/0/all/0/1">Julia Kempe</a></p>
<p>In many applications, Neural Nets (NNs) have classification performance on
par or even exceeding human capacity. Moreover, it is likely that NNs leverage
underlying features that might differ from those humans perceive to classify.
Can we "reverse-engineer" pertinent features to enhance our scientific
understanding? Here, we apply this idea to the notoriously difficult task of
galaxy classification: NNs have reached high performance for this task, but
what does a neural net (NN) "see" when it classifies galaxies? Are there
morphological features that the human eye might overlook that could help with
the task and provide new insights? Can we visualize tracers of early evolution,
or additionally incorporated spectral data? We present a novel way to summarize
and visualize galaxy morphology through the lens of neural networks, leveraging
Dataset Distillation, a recent deep-learning methodology with the primary
objective to distill knowledge from a large dataset and condense it into a
compact synthetic dataset, such that a model trained on this synthetic dataset
achieves performance comparable to a model trained on the full dataset. We
curate a class-balanced, medium-size high-confidence version of the Galaxy Zoo
2 dataset, and proceed with dataset distillation from our accurate
NN-classifier to create synthesized prototypical images of galaxy morphological
features, demonstrating its effectiveness. Of independent interest, we
introduce a self-adaptive version of the state-of-the-art Matching Trajectory
algorithm to automate the distillation process, and show enhanced performance
on computer vision benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17971">GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation. (arXiv:2311.17971v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1">Baorui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Haoge Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Junsheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Shen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tiejun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinlong Wang</a></p>
<p>Text-to-3D generation by distilling pretrained large-scale text-to-image
diffusion models has shown great promise but still suffers from inconsistent 3D
geometric structures (Janus problems) and severe artifacts. The aforementioned
problems mainly stem from 2D diffusion models lacking 3D awareness during the
lifting. In this work, we present GeoDream, a novel method that incorporates
explicit generalized 3D priors with 2D diffusion priors to enhance the
capability of obtaining unambiguous 3D consistent geometric structures without
sacrificing diversity or fidelity. Specifically, we first utilize a multi-view
diffusion model to generate posed images and then construct cost volume from
the predicted image, which serves as native 3D geometric priors, ensuring
spatial consistency in 3D space. Subsequently, we further propose to harness 3D
geometric priors to unlock the great potential of 3D awareness in 2D diffusion
priors via a disentangled design. Notably, disentangling 2D and 3D priors
allows us to refine 3D geometric priors further. We justify that the refined 3D
geometric priors aid in the 3D-aware capability of 2D diffusion priors, which
in turn provides superior guidance for the refinement of 3D geometric priors.
Our numerical and visual comparisons demonstrate that GeoDream generates more
3D consistent textured meshes with high-resolution realistic renderings (i.e.,
1024 $\times$ 1024) and adheres more closely to semantic coherence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17975">GeoDeformer: Geometric Deformable Transformer for Action Recognition. (arXiv:2311.17975v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jinhui Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiaming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Junwei Liang</a></p>
<p>Vision transformers have recently emerged as an effective alternative to
convolutional networks for action recognition. However, vision transformers
still struggle with geometric variations prevalent in video data. This paper
proposes a novel approach, GeoDeformer, designed to capture the variations
inherent in action video by integrating geometric comprehension directly into
the ViT architecture. Specifically, at the core of GeoDeformer is the Geometric
Deformation Predictor, a module designed to identify and quantify potential
spatial and temporal geometric deformations within the given video. Spatial
deformations adjust the geometry within individual frames, while temporal
deformations capture the cross-frame geometric dynamics, reflecting motion and
temporal progression. To demonstrate the effectiveness of our approach, we
incorporate it into the established MViTv2 framework, replacing the standard
self-attention blocks with GeoDeformer blocks. Our experiments at UCF101,
HMDB51, and Mini-K200 achieve significant increases in both Top-1 and Top-5
accuracy, establishing new state-of-the-art results with only a marginal
increase in computational cost. Additionally, visualizations affirm that
GeoDeformer effectively manifests explicit geometric deformations and minimizes
geometric variations. Codes and checkpoints will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17977">GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces. (arXiv:2311.17977v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yingwenqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jiadong Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xifeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiaoxiao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuexin Ma</a></p>
<p>The advent of neural 3D Gaussians has recently brought about a revolution in
the field of neural rendering, facilitating the generation of high-quality
renderings at real-time speeds. However, the explicit and discrete
representation encounters challenges when applied to scenes featuring
reflective surfaces. In this paper, we present GaussianShader, a novel method
that applies a simplified shading function on 3D Gaussians to enhance the
neural rendering in scenes with reflective surfaces while preserving the
training and rendering efficiency. The main challenge in applying the shading
function lies in the accurate normal estimation on discrete 3D Gaussians.
Specifically, we proposed a novel normal estimation framework based on the
shortest axis directions of 3D Gaussians with a delicately designed loss to
make the consistency between the normals and the geometries of Gaussian
spheres. Experiments show that GaussianShader strikes a commendable balance
between efficiency and visual quality. Our method surpasses Gaussian Splatting
in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When
compared to prior works handling reflective surfaces, such as Ref-NeRF, our
optimization time is significantly accelerated (23h vs. 0.58h). Please click on
our project website to see more results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17978">AutArch: An AI-assisted workflow for object detection and automated recording in archaeological catalogues. (arXiv:2311.17978v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klein_K/0/1/0/all/0/1">Kevin Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wohde_A/0/1/0/all/0/1">Alyssa Wohde</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorelik_A/0/1/0/all/0/1">Alexander V. Gorelik</a>, <a href="http://arxiv.org/find/cs/1/au:+Heyd_V/0/1/0/all/0/1">Volker Heyd</a>, <a href="http://arxiv.org/find/cs/1/au:+Diekmann_Y/0/1/0/all/0/1">Yoan Diekmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Brami_M/0/1/0/all/0/1">Maxime Brami</a></p>
<p>Compiling large datasets from published resources, such as archaeological
find catalogues presents fundamental challenges: identifying relevant content
and manually recording it is a time-consuming, repetitive and error-prone task.
For the data to be useful, it must be of comparable quality and adhere to the
same recording standards, which is hardly ever the case in archaeology. Here,
we present a new data collection method exploiting recent advances in
Artificial Intelligence. Our software uses an object detection neural network
combined with further classification networks to speed up, automate, and
standardise data collection from legacy resources, such as archaeological
drawings and photographs in large unsorted PDF files. The AI-assisted workflow
detects common objects found in archaeological catalogues, such as graves,
skeletons, ceramics, ornaments, stone tools and maps, and spatially relates and
analyses these objects on the page to extract real-life attributes, such as the
size and orientation of a grave based on the north arrow and the scale. A
graphical interface allows for and assists with manual validation. We
demonstrate the benefits of this approach by collecting a range of shapes and
numerical attributes from richly-illustrated archaeological catalogues, and
benchmark it in a real-world experiment with ten users. Moreover, we record
geometric whole-outlines through contour detection, an alternative to
landmark-based geometric morphometrics not achievable by hand.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17982">VBench: Comprehensive Benchmark Suite for Video Generative Models. (arXiv:2311.17982v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziqi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yinan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiashuo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1">Chenyang Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuanhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianxing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qingyang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chanpaisit_N/0/1/0/all/0/1">Nattapol Chanpaisit</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaohui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziwei Liu</a></p>
<p>Video generation has witnessed significant advancements, yet evaluating these
models remains a challenge. A comprehensive evaluation benchmark for video
generation is indispensable for two reasons: 1) Existing metrics do not fully
align with human perceptions; 2) An ideal evaluation system should provide
insights to inform future developments of video generation. To this end, we
present VBench, a comprehensive benchmark suite that dissects "video generation
quality" into specific, hierarchical, and disentangled dimensions, each with
tailored prompts and evaluation methods. VBench has three appealing properties:
1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation
(e.g., subject identity inconsistency, motion smoothness, temporal flickering,
and spatial relationship, etc). The evaluation metrics with fine-grained levels
reveal individual models' strengths and weaknesses. 2) Human Alignment: We also
provide a dataset of human preference annotations to validate our benchmarks'
alignment with human perception, for each evaluation dimension respectively. 3)
Valuable Insights: We look into current models' ability across various
evaluation dimensions, and various content types. We also investigate the gaps
between video and image generation models. We will open-source VBench,
including all prompts, evaluation methods, generated videos, and human
preference annotations, and also include more video generation models in VBench
to drive forward the field of video generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17983">Improving Faithfulness for Vision Transformers. (arXiv:2311.17983v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lijie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ninghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_M/0/1/0/all/0/1">Mengdi Huai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a></p>
<p>Vision Transformers (ViTs) have achieved state-of-the-art performance for
various vision tasks. One reason behind the success lies in their ability to
provide plausible innate explanations for the behavior of neural architectures.
However, ViTs suffer from issues with explanation faithfulness, as their focal
points are fragile to adversarial attacks and can be easily changed with even
slight perturbations on the input image. In this paper, we propose a rigorous
approach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly
speaking, an FViT should have the following two properties: (1) The top-$k$
indices of its self-attention vector should remain mostly unchanged under input
perturbation, indicating stable explanations; (2) The prediction distribution
should be robust to perturbations. To achieve this, we propose a new method
called Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing
and diffusion-based denoising. We theoretically prove that processing ViTs
directly with DDS can turn them into FViTs. We also show that Gaussian noise is
nearly optimal for both $\ell_2$ and $\ell_\infty$-norm cases. Finally, we
demonstrate the effectiveness of our approach through comprehensive experiments
and evaluations. Specifically, we compare our FViTs with other baselines
through visual interpretation and robustness accuracy under adversarial
attacks. Results show that FViTs are more robust against adversarial attacks
while maintaining the explainability of attention, indicating higher
faithfulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17984">4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling. (arXiv:2311.17984v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1">Sherwin Bahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1">Ivan Skorokhodov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_V/0/1/0/all/0/1">Victor Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1">Gordon Wetzstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1">Leonidas Guibas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1">Peter Wonka</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jeong Joon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1">Andrea Tagliasacchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1">David B. Lindell</a></p>
<p>Recent breakthroughs in text-to-4D generation rely on pre-trained
text-to-image and text-to-video models to generate dynamic 3D scenes. However,
current text-to-4D methods face a three-way tradeoff between the quality of
scene appearance, 3D structure, and motion. For example, text-to-image models
and their 3D-aware variants are trained on internet-scale image datasets and
can be used to produce scenes with realistic appearance and 3D structure -- but
no motion. Text-to-video models are trained on relatively smaller video
datasets and can produce scenes with motion, but poorer appearance and 3D
structure. While these models have complementary strengths, they also have
opposing weaknesses, making it difficult to combine them in a way that
alleviates this three-way tradeoff. Here, we introduce hybrid score
distillation sampling, an alternating optimization procedure that blends
supervision signals from multiple pre-trained diffusion models and incorporates
benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we
demonstrate synthesis of 4D scenes with compelling appearance, 3D structure,
and motion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18002">Echoes in the Noise: Posterior Samples of Faint Galaxy Surface Brightness Profiles with Score-Based Likelihoods and Priors. (arXiv:2311.18002v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Adam_A/0/1/0/all/0/1">Alexandre Adam</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Stone_C/0/1/0/all/0/1">Connor Stone</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bottrell_C/0/1/0/all/0/1">Connor Bottrell</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Legin_R/0/1/0/all/0/1">Ronan Legin</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hezaveh_Y/0/1/0/all/0/1">Yashar Hezaveh</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Perreault_Levasseur_L/0/1/0/all/0/1">Laurence Perreault-Levasseur</a></p>
<p>Examining the detailed structure of galaxy populations provides valuable
insights into their formation and evolution mechanisms. Significant barriers to
such analysis are the non-trivial noise properties of real astronomical images
and the point spread function (PSF) which blurs structure. Here we present a
framework which combines recent advances in score-based likelihood
characterization and diffusion model priors to perform a Bayesian analysis of
image deconvolution. The method, when applied to minimally processed
\emph{Hubble Space Telescope} (\emph{HST}) data, recovers structures which have
otherwise only become visible in next-generation \emph{James Webb Space
Telescope} (\emph{JWST}) imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18012">Bayesian Imaging for Radio Interferometry with Score-Based Priors. (arXiv:2311.18012v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Dia_N/0/1/0/all/0/1">Noe Dia</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Yantovski_Barth_M/0/1/0/all/0/1">M. J. Yantovski-Barth</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Adam_A/0/1/0/all/0/1">Alexandre Adam</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bowles_M/0/1/0/all/0/1">Micah Bowles</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Lemos_P/0/1/0/all/0/1">Pablo Lemos</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Scaife_A/0/1/0/all/0/1">Anna M. M. Scaife</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hezaveh_Y/0/1/0/all/0/1">Yashar Hezaveh</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Perreault_Levasseur_L/0/1/0/all/0/1">Laurence Perreault-Levasseur</a></p>
<p>The inverse imaging task in radio interferometry is a key limiting factor to
retrieving Bayesian uncertainties in radio astronomy in a computationally
effective manner. We use a score-based prior derived from optical images of
galaxies to recover images of protoplanetary disks from the DSHARP survey. We
demonstrate that our method produces plausible posterior samples despite the
misspecified galaxy prior. We show that our approach produces results which are
competitive with existing radio interferometry imaging algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18021">Understanding and Improving In-Context Learning on Vision-language Models. (arXiv:2311.18021v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhen Han</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bailan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckley_M/0/1/0/all/0/1">Mark Buckley</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a></p>
<p>Recently, in-context learning (ICL) on large language models (LLMs) has
received great attention, and this technique can also be applied to
vision-language models (VLMs) built upon LLMs. These VLMs can respond to
queries by conditioning responses on a series of multimodal demonstrations,
which comprise images, queries, and answers. Though ICL has been extensively
studied on LLMs, its research on VLMs remains limited. The inclusion of
additional visual information in the demonstrations motivates the following
research questions: which of the two modalities in the demonstration is more
significant? How can we select effective multimodal demonstrations to enhance
ICL performance? This study investigates the significance of both visual and
language information. Our findings indicate that ICL in VLMs is predominantly
driven by the textual information in the demonstrations whereas the visual
information in the demonstrations barely affects the ICL performance.
Subsequently, we provide an understanding of the findings by analyzing the
model information flow and comparing model inner states given different ICL
settings. Motivated by our analysis, we propose a simple yet effective
approach, termed Mixed Modality In-Context Example Selection (MMICES), which
considers both visual and language modalities when selecting demonstrations and
shows better ICL performance. Extensive experiments are conducted to support
our findings, understanding, and improvement of the ICL performance of VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18064">GELDA: A generative language annotation framework to reveal visual biases in datasets. (arXiv:2311.18064v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabra_K/0/1/0/all/0/1">Krish Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1">Kathleen M. Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1">Guha Balakrishnan</a></p>
<p>Bias analysis is a crucial step in the process of creating fair datasets for
training and evaluating computer vision models. The bottleneck in dataset
analysis is annotation, which typically requires: (1) specifying a list of
attributes relevant to the dataset domain, and (2) classifying each
image-attribute pair. While the second step has made rapid progress in
automation, the first has remained human-centered, requiring an experimenter to
compile lists of in-domain attributes. However, an experimenter may have
limited foresight leading to annotation "blind spots," which in turn can lead
to flawed downstream dataset analyses. To combat this, we propose GELDA, a
nearly automatic framework that leverages large generative language models
(LLMs) to propose and label various attributes for a domain. GELDA takes a
user-defined domain caption (e.g., "a photo of a bird," "a photo of a living
room") and uses an LLM to hierarchically generate attributes. In addition,
GELDA uses the LLM to decide which of a set of vision-language models (VLMs) to
use to classify each attribute in images. Results on real datasets show that
GELDA can generate accurate and diverse visual attribute suggestions, and
uncover biases such as confounding between class labels and background
features. Results on synthetic datasets demonstrate that GELDA can be used to
evaluate the biases of text-to-image diffusion models and generative
adversarial networks. Overall, we show that while GELDA is not accurate enough
to replace human annotators, it can serve as a complementary tool to help
humans analyze datasets in a cheap, low-effort, and flexible manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18068">ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic Reconstruction. (arXiv:2311.18068v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weder_S/0/1/0/all/0/1">Silvan Weder</a>, <a href="http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1">Francis Engelmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonberger_J/0/1/0/all/0/1">Johannes L. Sch&#xf6;nberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Seki_A/0/1/0/all/0/1">Akihito Seki</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1">Martin R. Oswald</a></p>
<p>We propose an online 3D semantic segmentation method that incrementally
reconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline
methods, ours is directly applicable to scenarios with real-time constraints,
such as robotics or mixed reality. To overcome the inherent challenges of
online methods, we make two main contributions. First, to effectively extract
information from the input RGB-D video stream, we jointly estimate geometry and
semantic labels per frame in 3D. A key focus of our approach is to reason about
semantic entities both in the 2D input and the local 3D domain to leverage
differences in spatial context and network architectures. Our method predicts
2D features using an off-the-shelf segmentation network. The extracted 2D
features are refined by a lightweight 3D network to enable reasoning about the
local 3D structure. Second, to efficiently deal with an infinite stream of
input RGB-D frames, a subsequent network serves as a temporal expert predicting
the incremental scene updates by leveraging 2D, 3D, and past information in a
learned manner. These updates are then integrated into a global scene
representation. Using these main contributions, our method can enable scenarios
with real-time constraints and can scale to arbitrary scene sizes by processing
and updating the scene only in a local region defined by the new measurement.
Our experiments demonstrate improved results compared to existing online
methods that purely operate in local regions and show that complementary
sources of information can boost the performance. We provide a thorough
ablation study on the benefits of different architectural as well as
algorithmic design decisions. Our method yields competitive results on the
popular ScanNet benchmark and SceneNN dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18071">Turn Down the Noise: Leveraging Diffusion Models for Test-time Adaptation via Pseudo-label Ensembling. (arXiv:2311.18071v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1">Mrigank Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rohan Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1">Akash Kannan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chawla_P/0/1/0/all/0/1">Pranit Chawla</a></p>
<p>The goal of test-time adaptation is to adapt a source-pretrained model to a
continuously changing target domain without relying on any source data.
Typically, this is either done by updating the parameters of the model (model
adaptation) using inputs from the target domain or by modifying the inputs
themselves (input adaptation). However, methods that modify the model suffer
from the issue of compounding noisy updates whereas methods that modify the
input need to adapt to every new data point from scratch while also struggling
with certain domain shifts. We introduce an approach that leverages a
pre-trained diffusion model to project the target domain images closer to the
source domain and iteratively updates the model via pseudo-label ensembling.
Our method combines the advantages of model and input adaptations while
mitigating their shortcomings. Our experiments on CIFAR-10C demonstrate the
superiority of our approach, outperforming the strongest baseline by an average
of 1.7% across 15 diverse corruptions and surpassing the strongest input
adaptation baseline by an average of 18%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18077">LiDAR-based Outdoor Crowd Management for Smart Campus on the Edge. (arXiv:2311.18077v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yitao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gundu_K/0/1/0/all/0/1">Krishna Gundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaidi_Z/0/1/0/all/0/1">Zohair Zaidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1">Ming Zhao</a></p>
<p>Crowd management is crucial for a smart campus. Popular methods are
camera-based. However, conventional camera-based approaches may leak users'
personally identifiable features, jeopardizing user's privacy, which limits its
application. In this work, we investigate using affordable light detection and
ranging (LiDAR) technology to perform outdoor crowd management leveraging edge
computing. Specifically, we aim to count the number of people on a walkway of a
university campus. Besides privacy protection, LiDAR sensors are superior to
cameras since their performance will not be compromised when the campus is not
well-illuminated. We deploy LiDAR sensors on light poles to collect data from
the crowd on the campus and leverage edge accelerators to process data locally.
We proposed two different methodologies in this work: 1) a non-convolutional
neural network (CNN)-based approach, using clustering and autoencoder, and 2) a
CNN-based approach that first projects point clouds to 2D planes and then
processes the projection with conventional CNNs. Our first approach relies on
careful feature engineering, whereas our second approach does not require such
effort. However, the CNN-based approach requires more computational power than
our non-CNN-based approach. We evaluate both approaches comprehensively with
our hand-labeled real-life data collected from campus. Our evaluation results
show that the first method achieves an accuracy of 85.4%, whereas the second
method achieves 95.8%. Our CNN-based method outperforms existing solutions
significantly. We also deploy our two models on an edge accelerator, TPU, to
measure the speedup, leveraging this specialized accelerator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18082">Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing. (arXiv:2311.18082v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolters_P/0/1/0/all/0/1">Piper Wolters</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1">Favyen Bastani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1">Aniruddha Kembhavi</a></p>
<p>Super-Resolution for remote sensing has the potential for huge impact on
planet monitoring by producing accurate and realistic high resolution imagery
on a frequent basis and a global scale. Despite a lot of attention, several
inconsistencies and challenges have prevented it from being deployed in
practice. These include the lack of effective metrics, fragmented and
relatively small-scale datasets for training, insufficient comparisons across a
suite of methods, and unclear evidence for the use of super-resolution outputs
for machine consumption. This work presents a new metric for super-resolution,
CLIPScore, that corresponds far better with human judgments than previous
metrics on an extensive study. We use CLIPScore to evaluate four standard
methods on a new large-scale dataset, S2-NAIP, and three existing benchmark
datasets, and find that generative adversarial networks easily outperform more
traditional L2 loss-based models and are more semantically accurate than modern
diffusion models. We also find that using CLIPScore as an auxiliary loss can
speed up the training of GANs by 18x and lead to improved outputs, resulting in
an effective model in diverse geographies across the world which we will
release publicly. The dataset, pre-trained model weights, and code are
available at https://github.com/allenai/satlas-super-resolution/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18083">Meta Co-Training: Two Views are Better than One. (arXiv:2311.18083v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rothenberger_J/0/1/0/all/0/1">Jay C. Rothenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Diochnos_D/0/1/0/all/0/1">Dimitrios I. Diochnos</a></p>
<p>In many practical computer vision scenarios unlabeled data is plentiful, but
labels are scarce and difficult to obtain. As a result, semi-supervised
learning which leverages unlabeled data to boost the performance of supervised
classifiers have received significant attention in recent literature. One major
class of semi-supervised algorithms is co-training. In co-training two
different models leverage different independent and sufficient "views" of the
data to jointly make better predictions. During co-training each model creates
pseudo labels on unlabeled points which are used to improve the other model. We
show that in the common case when independent views are not available we can
construct such views inexpensively using pre-trained models. Co-training on the
constructed views yields a performance improvement over any of the individual
views we construct and performance comparable with recent approaches in
semi-supervised learning, but has some undesirable properties. To alleviate the
issues present with co-training we present Meta Co-Training which is an
extension of the successful Meta Pseudo Labels approach to multiple views. Our
method achieves new state-of-the-art performance on ImageNet-10% with very few
training resources, as well as outperforming prior semi-supervised work on
several other fine-grained image classification datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18102">PatchBMI-Net: Lightweight Facial Patch-based Ensemble for BMI Prediction. (arXiv:2311.18102v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aarotale_P/0/1/0/all/0/1">Parshuram N. Aarotale</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_T/0/1/0/all/0/1">Twyla Hill</a>, <a href="http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1">Ajita Rattani</a></p>
<p>Due to an alarming trend related to obesity affecting 93.3 million adults in
the United States alone, body mass index (BMI) and body weight have drawn
significant interest in various health monitoring applications. Consequently,
several studies have proposed self-diagnostic facial image-based BMI prediction
methods for healthy weight monitoring. These methods have mostly used
convolutional neural network (CNN) based regression baselines, such as VGG19,
ResNet50, and Efficient-NetB0, for BMI prediction from facial images. However,
the high computational requirement of these heavy-weight CNN models limits
their deployment to resource-constrained mobile devices, thus deterring weight
monitoring using smartphones. This paper aims to develop a lightweight facial
patch-based ensemble (PatchBMI-Net) for BMI prediction to facilitate the
deployment and weight monitoring using smartphones. Extensive experiments on
BMI-annotated facial image datasets suggest that our proposed PatchBMI-Net
model can obtain Mean Absolute Error (MAE) in the range [3.58, 6.51] with a
size of about 3.3 million parameters. On cross-comparison with heavyweight
models, such as ResNet-50 and Xception, trained for BMI prediction from facial
images, our proposed PatchBMI-Net obtains equivalent MAE along with the model
size reduction of about 5.4x and the average inference time reduction of about
3x when deployed on Apple-14 smartphone. Thus, demonstrating performance
efficiency as well as low latency for on-device deployment and weight
monitoring using smartphone applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18103">Corner-to-Center Long-range Context Model for Efficient Learned Image Compression. (arXiv:2311.18103v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sui_Y/0/1/0/all/0/1">Yang Sui</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_D/0/1/0/all/0/1">Ding Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1">Xiang Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1">Xiaozhong Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_B/0/1/0/all/0/1">Bo Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1">Zhenzhong Chen</a></p>
<p>In the framework of learned image compression, the context model plays a
pivotal role in capturing the dependencies among latent representations. To
reduce the decoding time resulting from the serial autoregressive context
model, the parallel context model has been proposed as an alternative that
necessitates only two passes during the decoding phase, thus facilitating
efficient image compression in real-world scenarios. However, performance
degradation occurs due to its incomplete casual context. To tackle this issue,
we conduct an in-depth analysis of the performance degradation observed in
existing parallel context models, focusing on two aspects: the Quantity and
Quality of information utilized for context prediction and decoding. Based on
such analysis, we propose the \textbf{Corner-to-Center transformer-based
Context Model (C$^3$M)} designed to enhance context and latent predictions and
improve rate-distortion performance. Specifically, we leverage the
logarithmic-based prediction order to predict more context features from corner
to center progressively. In addition, to enlarge the receptive field in the
analysis and synthesis transformation, we use the Long-range Crossing Attention
Module (LCAM) in the encoder/decoder to capture the long-range semantic
information by assigning the different window shapes in different channels.
Extensive experimental evaluations show that the proposed method is effective
and outperforms the state-of-the-art parallel methods. Finally, according to
the subjective analysis, we suggest that improving the detailed representation
in transformer-based image compression is a promising direction to be explored.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18107">A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem. (arXiv:2311.18107v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoegele_W/0/1/0/all/0/1">Wolfgang Hoegele</a></p>
<p>Background: Pose estimation of rigid objects is a practical challenge in
optical metrology and computer vision. In this paper a novel
stochastic-geometrical modeling framework for object pose estimation is
presented based on observing multiple feature points. Methods: This
stochastic-geometrical framework utilizes mixture models for the feature point
densities in object space as well as for interpreting real measurements. Direct
advantages of this approach are the avoidance to resolve individual feature
correspondences and to incorporate correct stochastic dependencies in
multi-view applications. First, the general modeling framework is presented,
second, a general algorithm for pose estimation is derived, and third, two
example models for a camera setup as well as a lateration setup are presented.
Results: The numerical experiments show the effectiveness of this modeling and
general algorithm by investigating four simulation scenarios for three
different observation systems, including the dependence on measurement
resolution, object deformations as well as strong measurement noise. It can be
concluded that the probabilistic modeling of pose estimation based on mixture
models can lead to accurate and robust pose estimations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18113">Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features. (arXiv:2311.18113v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wimmer_T/0/1/0/all/0/1">Thomas Wimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1">Peter Wonka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1">Maks Ovsjanikov</a></p>
<p>With the immense growth of dataset sizes and computing resources in recent
years, so-called foundation models have become popular in NLP and vision tasks.
In this work, we propose to explore foundation models for the task of keypoint
detection on 3D shapes. A unique characteristic of keypoint detection is that
it requires semantic and geometric awareness while demanding high localization
accuracy. To address this problem, we propose, first, to back-project features
from large pre-trained 2D vision models onto 3D shapes and employ them for this
task. We show that we obtain robust 3D features that contain rich semantic
information and analyze multiple candidate features stemming from different 2D
foundation models. Second, we employ a keypoint candidate optimization module
which aims to match the average observed distribution of keypoints on the shape
and is guided by the back-projected features. The resulting approach achieves a
new state of the art for few-shot keypoint detection on the KeyPointNet
dataset, almost doubling the performance of the previous best methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18130">The Trifecta: Three simple techniques for training deeper Forward-Forward networks. (arXiv:2311.18130v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dooms_T/0/1/0/all/0/1">Thomas Dooms</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ing Jyh Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1">Jose Oramas</a></p>
<p>Modern machine learning models are able to outperform humans on a variety of
non-trivial tasks. However, as the complexity of the models increases, they
consume significant amounts of power and still struggle to generalize
effectively to unseen data. Local learning, which focuses on updating subsets
of a model's parameters at a time, has emerged as a promising technique to
address these issues. Recently, a novel local learning algorithm, called
Forward-Forward, has received widespread attention due to its innovative
approach to learning. Unfortunately, its application has been limited to
smaller datasets due to scalability issues. To this end, we propose The
Trifecta, a collection of three simple techniques that synergize exceptionally
well and drastically improve the Forward-Forward algorithm on deeper networks.
Our experiments demonstrate that our models are on par with similarly
structured, backpropagation-based models in both training speed and test
accuracy on simple datasets. This is achieved by the ability to learn
representations that are informative locally, on a layer-by-layer basis, and
retain their informativeness when propagated to deeper layers in the
architecture. This leads to around 84\% accuracy on CIFAR-10, a notable
improvement (25\%) over the original FF algorithm. These results highlight the
potential of Forward-Forward as a genuine competitor to backpropagation and as
a promising research avenue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18149">STF: Spatial Temporal Fusion for Trajectory Prediction. (arXiv:2311.18149v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1">Pengqian Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Roop_P/0/1/0/all/0/1">Partha Roop</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiamou Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_T/0/1/0/all/0/1">Tianzhe Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifei Wang</a></p>
<p>Trajectory prediction is a challenging task that aims to predict the future
trajectory of vehicles or pedestrians over a short time horizon based on their
historical positions. The main reason is that the trajectory is a kind of
complex data, including spatial and temporal information, which is crucial for
accurate prediction. Intuitively, the more information the model can capture,
the more precise the future trajectory can be predicted. However, previous
works based on deep learning methods processed spatial and temporal information
separately, leading to inadequate spatial information capture, which means they
failed to capture the complete spatial information. Therefore, it is of
significance to capture information more fully and effectively on vehicle
interactions. In this study, we introduced an integrated 3D graph that
incorporates both spatial and temporal edges. Based on this, we proposed the
integrated 3D graph, which considers the cross-time interaction information. In
specific, we design a Spatial-Temporal Fusion (STF) model including Multi-layer
perceptions (MLP) and Graph Attention (GAT) to capture the spatial and temporal
information historical trajectories simultaneously on the 3D graph. Our
experiment on the ApolloScape Trajectory Datasets shows that the proposed STF
outperforms several baseline methods, especially on the long-time-horizon
trajectory prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18158">HiPA: Enabling One-Step Text-to-Image Diffusion Models via High-Frequency-Promoting Adaptation. (arXiv:2311.18158v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a></p>
<p>Diffusion models have revolutionized text-to-image generation, but their
real-world applications are hampered by the extensive time needed for hundreds
of diffusion steps. Although progressive distillation has been proposed to
speed up diffusion sampling to 2-8 steps, it still falls short in one-step
generation, and necessitates training multiple student models, which is highly
parameter-extensive and time-consuming. To overcome these limitations, we
introduce High-frequency-Promoting Adaptation (HiPA), a parameter-efficient
approach to enable one-step text-to-image diffusion. Grounded in the insight
that high-frequency information is essential but highly lacking in one-step
diffusion, HiPA focuses on training one-step, low-rank adaptors to specifically
enhance the under-represented high-frequency abilities of advanced diffusion
models. The learned adaptors empower these diffusion models to generate
high-quality images in just a single step. Compared with progressive
distillation, HiPA achieves much better performance in one-step text-to-image
generation (37.3 $\rightarrow$ 23.8 in FID-5k on MS-COCO 2017) and 28.6x
training speed-up (108.8 $\rightarrow$ 3.8 A100 GPU days), requiring only 0.04%
training parameters (7,740 million $\rightarrow$ 3.3 million). We also
demonstrate HiPA's effectiveness in text-guided image editing, inpainting and
super-resolution tasks, where our adapted models consistently deliver
high-quality outputs in just one diffusion step. The source code will be
released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18159">Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector Quantization. (arXiv:2311.18159v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Navaneet_K/0/1/0/all/0/1">KL Navaneet</a>, <a href="http://arxiv.org/find/cs/1/au:+Meibodi_K/0/1/0/all/0/1">Kossar Pourahmadi Meibodi</a>, <a href="http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1">Soroush Abbasi Koohpayegani</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1">Hamed Pirsiavash</a></p>
<p>3D Gaussian Splatting is a new method for modeling and rendering 3D radiance
fields that achieves much faster learning and rendering time compared to SOTA
NeRF methods. However, it comes with a drawback in the much larger storage
demand compared to NeRF methods since it needs to store the parameters for
several 3D Gaussians. We notice that many Gaussians may share similar
parameters, so we introduce a simple vector quantization method based on
\kmeans algorithm to quantize the Gaussian parameters. Then, we store the small
codebook along with the index of the code for each Gaussian. Moreover, we
compress the indices further by sorting them and using a method similar to
run-length encoding. We do extensive experiments on standard benchmarks as well
as a new benchmark which is an order of magnitude larger than the standard
benchmarks. We show that our simple yet effective method can reduce the storage
cost for the original 3D Gaussian Splatting method by a factor of almost
$20\times$ with a very small drop in the quality of rendered images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18166">A-Scan2BIM: Assistive Scan to Building Information Modeling. (arXiv:2311.18166v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1">Weilian Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jieliang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dale Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chin-Yi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1">Yasutaka Furukawa</a></p>
<p>This paper proposes an assistive system for architects that converts a
large-scale point cloud into a standardized digital representation of a
building for Building Information Modeling (BIM) applications. The process is
known as Scan-to-BIM, which requires many hours of manual work even for a
single building floor by a professional architect. Given its challenging
nature, the paper focuses on helping architects on the Scan-to-BIM process,
instead of replacing them. Concretely, we propose an assistive Scan-to-BIM
system that takes the raw sensor data and edit history (including the current
BIM model), then auto-regressively predicts a sequence of model editing
operations as APIs of a professional BIM software (i.e., Autodesk Revit). The
paper also presents the first building-scale Scan2BIM dataset that contains a
sequence of model editing operations as the APIs of Autodesk Revit. The dataset
contains 89 hours of Scan2BIM modeling processes by professional architects
over 16 scenes, spanning over 35,000 m^2. We report our system's reconstruction
quality with standard metrics, and we introduce a novel metric that measures
how natural the order of reconstructed operations is. A simple modification to
the reconstruction module helps improve performance, and our method is far
superior to two other baselines in the order metric. We will release data,
code, and models at a-scan2bim.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18168">Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications. (arXiv:2311.18168v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Karren D. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1">Anurag Ranjan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jen-Hao Rick Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1">Raviteja Vemulapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1">Oncel Tuzel</a></p>
<p>We consider the task of animating 3D facial geometry from speech signal.
Existing works are primarily deterministic, focusing on learning a one-to-one
mapping from speech signal to 3D face meshes on small datasets with limited
speakers. While these models can achieve high-quality lip articulation for
speakers in the training set, they are unable to capture the full and diverse
distribution of 3D facial motions that accompany speech in the real world.
Importantly, the relationship between speech and facial motion is one-to-many,
containing both inter-speaker and intra-speaker variations and necessitating a
probabilistic approach. In this paper, we identify and address key challenges
that have so far limited the development of probabilistic models: lack of
datasets and metrics that are suitable for training and evaluating them, as
well as the difficulty of designing a model that generates diverse results
while remaining faithful to a strong conditioning signal as speech. We first
propose large-scale benchmark datasets and metrics suitable for probabilistic
modeling. Then, we demonstrate a probabilistic model that achieves both
diversity and fidelity to speech, outperforming other methods across the
proposed benchmarks. Finally, we showcase useful applications of probabilistic
models trained on these large-scale datasets: we can generate diverse
speech-driven 3D facial motion that matches unseen speaker styles extracted
from reference clips; and our synthetic meshes can be used to improve the
performance of downstream audio-visual models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18169">Few-shot Image Generation via Style Adaptation and Content Preservation. (arXiv:2311.18169v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaosheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fayao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a></p>
<p>Training a generative model with limited data (e.g., 10) is a very
challenging task. Many works propose to fine-tune a pre-trained GAN model.
However, this can easily result in overfitting. In other words, they manage to
adapt the style but fail to preserve the content, where \textit{style} denotes
the specific properties that defines a domain while \textit{content} denotes
the domain-irrelevant information that represents diversity. Recent works try
to maintain a pre-defined correspondence to preserve the content, however, the
diversity is still not enough and it may affect style adaptation. In this work,
we propose a paired image reconstruction approach for content preservation. We
propose to introduce an image translation module to GAN transferring, where the
module teaches the generator to separate style and content, and the generator
provides training data to the translation module in return. Qualitative and
quantitative experiments show that our method consistently surpasses the
state-of-the-art methods in few shot setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18173">Quantification of cardiac capillarization in single-immunostained myocardial slices using weakly supervised instance segmentation. (arXiv:2311.18173v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1">Zhao Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiwen Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Richardson_W/0/1/0/all/0/1">William Richardson</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_B/0/1/0/all/0/1">Bruce Z. Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Razi_A/0/1/0/all/0/1">Abolfazl Razi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_T/0/1/0/all/0/1">Tong Ye</a></p>
<p>Decreased myocardial capillary density has been reported as an important
histopathological feature associated with various heart disorders. Quantitative
assessment of cardiac capillarization typically involves double immunostaining
of cardiomyocytes (CMs) and capillaries in myocardial slices. In contrast,
single immunostaining of basement membrane components is a straightforward
approach to simultaneously label CMs and capillaries, presenting fewer
challenges in background staining. However, subsequent image analysis always
requires manual work in identifying and segmenting CMs and capillaries. Here,
we developed an image analysis tool, AutoQC, to automatically identify and
segment CMs and capillaries in immunofluorescence images of collagen type IV, a
predominant basement membrane protein within the myocardium. In addition,
commonly used capillarization-related measurements can be derived from
segmentation masks. AutoQC features a weakly supervised instance segmentation
algorithm by leveraging the power of a pre-trained segmentation model via
prompt engineering. AutoQC outperformed YOLOv8-Seg, a state-of-the-art instance
segmentation model, in both instance segmentation and capillarization
assessment. Furthermore, the training of AutoQC required only a small dataset
with bounding box annotations instead of pixel-wise annotations, leading to a
reduced workload during network training. AutoQC provides an automated solution
for quantifying cardiac capillarization in basement-membrane-immunostained
myocardial slices, eliminating the need for manual image analysis once it is
trained.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18193">Persistent Test-time Adaptation in Episodic Testing Scenarios. (arXiv:2311.18193v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Trung-Hieu Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1">Duc Minh Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh N. Do</a></p>
<p>Current test-time adaptation (TTA) approaches aim to adapt to environments
that change continuously. Yet, when the environments not only change but also
recur in a correlated manner over time, such as in the case of day-night
surveillance cameras, it is unclear whether the adaptability of these methods
is sustained after a long run. This study aims to examine the error
accumulation of TTA models when they are repeatedly exposed to previous testing
environments, proposing a novel testing setting called episodic TTA. To study
this phenomenon, we design a simulation of TTA process on a simple yet
representative $\epsilon$-perturbed Gaussian Mixture Model Classifier and
derive the theoretical findings revealing the dataset- and algorithm-dependent
factors that contribute to the gradual degeneration of TTA methods through
time. Our investigation has led us to propose a method, named persistent TTA
(PeTTA). PeTTA senses the model divergence towards a collapsing and adjusts the
adaptation strategy of TTA, striking a balance between two primary objectives:
adaptation and preventing model collapse. The stability of PeTTA in the face of
episodic TTA scenarios has been demonstrated through a set of comprehensive
experiments on various benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18198">S-T CRF: Spatial-Temporal Conditional Random Field for Human Trajectory Prediction. (arXiv:2311.18198v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1">Pengqian Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiamou Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jialing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Song Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yanni Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Roop_P/0/1/0/all/0/1">Partha Roop</a></p>
<p>Trajectory prediction is of significant importance in computer vision.
Accurate pedestrian trajectory prediction benefits autonomous vehicles and
robots in planning their motion. Pedestrians' trajectories are greatly
influenced by their intentions. Prior studies having introduced various deep
learning methods only pay attention to the spatial and temporal information of
trajectory, overlooking the explicit intention information. In this study, we
introduce a novel model, termed the \textbf{S-T CRF}:
\textbf{S}patial-\textbf{T}emporal \textbf{C}onditional \textbf{R}andom
\textbf{F}ield, which judiciously incorporates intention information besides
spatial and temporal information of trajectory. This model uses a Conditional
Random Field (CRF) to generate a representation of future intentions, greatly
improving the prediction of subsequent trajectories when combined with
spatial-temporal representation. Furthermore, the study innovatively devises a
space CRF loss and a time CRF loss, meticulously designed to enhance
interaction constraints and temporal dynamics, respectively. Extensive
experimental evaluations on dataset ETH/UCY and SDD demonstrate that the
proposed method surpasses existing baseline approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18199">Hy-Tracker: A Novel Framework for Enhancing Efficiency and Accuracy of Object Tracking in Hyperspectral Videos. (arXiv:2311.18199v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Mohammad Aminul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1">Wangzhi Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yongsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Paliwal_K/0/1/0/all/0/1">Kuldip K. Paliwal</a></p>
<p>Hyperspectral object tracking has recently emerged as a topic of great
interest in the remote sensing community. The hyperspectral image, with its
many bands, provides a rich source of material information of an object that
can be effectively used for object tracking. While most hyperspectral trackers
are based on detection-based techniques, no one has yet attempted to employ
YOLO for detecting and tracking the object. This is due to the presence of
multiple spectral bands, the scarcity of annotated hyperspectral videos, and
YOLO's performance limitation in managing occlusions, and distinguishing object
in cluttered backgrounds. Therefore, in this paper, we propose a novel
framework called Hy-Tracker, which aims to bridge the gap between hyperspectral
data and state-of-the-art object detection methods to leverage the strengths of
YOLOv7 for object tracking in hyperspectral videos. Hy-Tracker not only
introduces YOLOv7 but also innovatively incorporates a refined tracking module
on top of YOLOv7. The tracker refines the initial detections produced by
YOLOv7, leading to improved object-tracking performance. Furthermore, we
incorporate Kalman-Filter into the tracker, which addresses the challenges
posed by scale variation and occlusion. The experimental results on
hyperspectral benchmark datasets demonstrate the effectiveness of Hy-Tracker in
accurately tracking objects across frames.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18208">SMaRt: Improving GANs with Score Matching Regularity. (arXiv:2311.18208v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Mengfei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Ceyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1">Ran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong-jin Liu</a></p>
<p>Generative adversarial networks (GANs) usually struggle in learning from
highly diverse data, whose underlying manifold is complex. In this work, we
revisit the mathematical foundations of GANs, and theoretically reveal that the
native adversarial loss for GAN training is insufficient to fix the problem of
subsets with positive Lebesgue measure of the generated data manifold lying out
of the real data manifold. Instead, we find that score matching serves as a
valid solution to this issue thanks to its capability of persistently pushing
the generated data points towards the real data manifold. We thereby propose to
improve the optimization of GANs with score matching regularity (SMaRt).
Regarding the empirical evidences, we first design a toy example to show that
training GANs by the aid of a ground-truth score function can help reproduce
the real data distribution more accurately, and then confirm that our approach
can consistently boost the synthesis performance of various state-of-the-art
GANs on real-world datasets with pre-trained diffusion models acting as the
approximate score function. For instance, when training Aurora on the ImageNet
64x64 dataset, we manage to improve FID from 8.87 to 7.11, on par with the
performance of one-step consistency model. The source code will be made public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18214">Perception of Misalignment States for Sky Survey Telescopes with the Digital Twin and the Deep Neural Networks. (arXiv:2311.18214v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Zhang_M/0/1/0/all/0/1">Miao Zhang</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Jia_P/0/1/0/all/0/1">Peng Jia</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Li_Z/0/1/0/all/0/1">Zhengyang Li</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Xiang_W/0/1/0/all/0/1">Wennan Xiang</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Lv_J/0/1/0/all/0/1">Jiameng Lv</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Sun_R/0/1/0/all/0/1">Rui Sun</a></p>
<p>Sky survey telescopes play a critical role in modern astronomy, but
misalignment of their optical elements can introduce significant variations in
point spread functions, leading to reduced data quality. To address this, we
need a method to obtain misalignment states, aiding in the reconstruction of
accurate point spread functions for data processing methods or facilitating
adjustments of optical components for improved image quality. Since sky survey
telescopes consist of many optical elements, they result in a vast array of
potential misalignment states, some of which are intricately coupled, posing
detection challenges. However, by continuously adjusting the misalignment
states of optical elements, we can disentangle coupled states. Based on this
principle, we propose a deep neural network to extract misalignment states from
continuously varying point spread functions in different field of views. To
ensure sufficient and diverse training data, we recommend employing a digital
twin to obtain data for neural network training. Additionally, we introduce the
state graph to store misalignment data and explore complex relationships
between misalignment states and corresponding point spread functions, guiding
the generation of training data from experiments. Once trained, the neural
network estimates misalignment states from observation data, regardless of the
impacts caused by atmospheric turbulence, noise, and limited spatial sampling
rates in the detector. The method proposed in this paper could be used to
provide prior information for the active optics system and the optical system
alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18216">FS-BAND: A Frequency-Sensitive Banding Detector. (arXiv:2311.18216v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Ru Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1">Fangfang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenjun Zhang</a></p>
<p>Banding artifact, as known as staircase-like contour, is a common quality
annoyance that happens in compression, transmission, etc. scenarios, which
largely affects the user's quality of experience (QoE). The banding distortion
typically appears as relatively small pixel-wise variations in smooth
backgrounds, which is difficult to analyze in the spatial domain but easily
reflected in the frequency domain. In this paper, we thereby study the banding
artifact from the frequency aspect and propose a no-reference banding detection
model to capture and evaluate banding artifacts, called the Frequency-Sensitive
BANding Detector (FS-BAND). The proposed detector is able to generate a
pixel-wise banding map with a perception correlated quality score. Experimental
results show that the proposed FS-BAND method outperforms state-of-the-art
image quality assessment (IQA) approaches with higher accuracy in banding
classification task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18231">TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model. (arXiv:2311.18231v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Hantao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>Prompt tuning represents a valuable technique for adapting pre-trained
visual-language models (VLM) to various downstream tasks. Recent advancements
in CoOp-based methods propose a set of learnable domain-shared or
image-conditional textual tokens to facilitate the generation of task-specific
textual classifiers. However, those textual tokens have a limited
generalization ability regarding unseen domains, as they cannot dynamically
adjust to the distribution of testing classes. To tackle this issue, we present
a novel Textual-based Class-aware Prompt tuning(TCP) that explicitly
incorporates prior knowledge about classes to enhance their discriminability.
The critical concept of TCP involves leveraging Textual Knowledge Embedding
(TKE) to map the high generalizability of class-level textual knowledge into
class-aware textual tokens. By seamlessly integrating these class-aware prompts
into the Text Encoder, a dynamic class-aware classifier is generated to enhance
discriminability for unseen domains. During inference, TKE dynamically
generates class-aware prompts related to the unseen classes. Comprehensive
evaluations demonstrate that TKE serves as a plug-and-play module effortlessly
combinable with existing methods. Furthermore, TCP consistently achieves
superior performance while demanding less training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18237">Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models. (arXiv:2311.18237v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1">Raviteja Vemulapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Pouransari_H/0/1/0/all/0/1">Hadi Pouransari</a>, <a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1">Fartash Faghri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Sachin Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1">Mehrdad Farajtabar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1">Oncel Tuzel</a></p>
<p>Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit
impressive performance on various downstream tasks, especially with limited
labeled target data. However, due to their high memory and compute
requirements, these models cannot be deployed in resource constrained settings.
This raises an important question: How can we utilize the knowledge from a
large VFM to train a small task-specific model for a new target task with
limited labeled training data? In this work, we answer this question by
proposing a simple and highly effective task-oriented knowledge transfer
approach to leverage pretrained VFMs for effective training of small
task-specific models. Our experimental results on four target tasks under
limited labeled data settings show that the proposed knowledge transfer
approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining
and supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively.
We also show that the dataset used for transferring knowledge has a significant
effect on the final target task performance, and propose an image
retrieval-based approach for curating effective transfer sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18241">LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for Deciphering Protests in the News. (arXiv:2311.18241v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongjun Zhang</a></p>
<p>Large language and vision models have transformed how social movements
scholars identify protest and extract key protest attributes from multi-modal
data such as texts, images, and videos. This article documents how we
fine-tuned two large pretrained transformer models, including longformer and
swin-transformer v2, to infer potential protests in news articles using textual
and imagery data. First, the longformer model was fine-tuned using the Dynamic
of Collective Action (DoCA) Corpus. We matched the New York Times articles with
the DoCA database to obtain a training dataset for downstream tasks. Second,
the swin-transformer v2 models was trained on UCLA-protest imagery data.
UCLA-protest project contains labeled imagery data with information such as
protest, violence, and sign. Both fine-tuned models will be available via
\url{https://github.com/Joshzyj/llvms4protest}. We release this short technical
report for social movement scholars who are interested in using LLVMs to infer
protests in textual and imagery data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18243">DKiS: Decay weight invertible image steganography with private key. (arXiv:2311.18243v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yitian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuhua Liu</a></p>
<p>Image steganography, the practice of concealing information within another
image, traditionally faces security challenges when its methods become publicly
known. To counteract this, we introduce a novel private key-based image
steganography technique. This approach ensures the security of hidden
information, requiring a corresponding private key for access, irrespective of
the public knowledge of the steganography method. We present experimental
evidence demonstrating our method's effectiveness, showcasing its real-world
applicability. Additionally, we identified a critical challenge in the
invertible image steganography process: the transfer of non-essential, or
`garbage', information from the secret to the host pipeline. To address this,
we introduced the decay weight to control the information transfer, filtering
out irrelevant data and enhancing the performance of image steganography. Our
code is publicly accessible at https://github.com/yanghangAI/DKiS, and a
practical demonstration is available at <a href="http://yanghang.site/hidekey.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18245">Automatic Detection of Alzheimer&#x27;s Disease with Multi-Modal Fusion of Clinical MRI Scans. (arXiv:2311.18245v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1">Liben Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1">Binfeng Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1">Wenxin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Razavian_N/0/1/0/all/0/1">Narges Razavian</a></p>
<p>The aging population of the U.S. drives the prevalence of Alzheimer's
disease. Brookmeyer et al. forecasts approximately 15 million Americans will
have either clinical AD or mild cognitive impairment by 2060. In response to
this urgent call, methods for early detection of Alzheimer's disease have been
developed for prevention and pre-treatment. Notably, literature on the
application of deep learning in the automatic detection of the disease has been
proliferating. This study builds upon previous literature and maintains a focus
on leveraging multi-modal information to enhance automatic detection. We aim to
predict the stage of the disease - Cognitively Normal (CN), Mildly Cognitive
Impairment (MCI), and Alzheimer's Disease (AD), based on two different types of
brain MRI scans. We design an AlexNet-based deep learning model that learns the
synergy of complementary information from both T1 and FLAIR MRI scans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18254">Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition. (arXiv:2311.18254v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guangming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1">Qing Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kelong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a></p>
<p>With the recent surge in the use of touchscreen devices, free-hand sketching
has emerged as a promising modality for human-computer interaction. While
previous research has focused on tasks such as recognition, retrieval, and
generation of familiar everyday objects, this study aims to create a Sketch
Input Method Editor (SketchIME) specifically designed for a professional C4I
system. Within this system, sketches are utilized as low-fidelity prototypes
for recommending standardized symbols in the creation of comprehensive
situation maps. This paper also presents a systematic dataset comprising 374
specialized sketch types, and proposes a simultaneous recognition and
segmentation architecture with multilevel supervision between recognition and
segmentation to improve performance and enhance interpretability. By
incorporating few-shot domain adaptation and class-incremental learning, the
network's ability to adapt to new users and extend to new task-specific classes
is significantly enhanced. Results from experiments conducted on both the
proposed dataset and the SPG dataset illustrate the superior performance of the
proposed architecture. Our dataset and code are publicly available at
https://github.com/Anony517/SketchIME.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18257">Diffusion Models Without Attention. (arXiv:2311.18257v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jing Nathan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiatao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a></p>
<p>In recent advancements in high-fidelity image generation, Denoising Diffusion
Probabilistic Models (DDPMs) have emerged as a key player. However, their
application at high resolutions presents significant computational challenges.
Current methods, such as patchifying, expedite processes in UNet and
Transformer architectures but at the expense of representational capacity.
Addressing this, we introduce the Diffusion State Space Model (DiffuSSM), an
architecture that supplants attention mechanisms with a more scalable state
space model backbone. This approach effectively handles higher resolutions
without resorting to global compression, thus preserving detailed image
representation throughout the diffusion process. Our focus on FLOP-efficient
architectures in diffusion training marks a significant step forward.
Comprehensive evaluations on both ImageNet and LSUN datasets at two resolutions
demonstrate that DiffuSSMs are on par or even outperform existing diffusion
models with attention modules in FID and Inception Score metrics while
significantly reducing total FLOP usage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18259">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives. (arXiv:2311.18259v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1">Kristen Grauman</a>, <a href="http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1">Andrew Westbury</a>, <a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1">Lorenzo Torresani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1">Kris Kitani</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1">Jitendra Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1">Triantafyllos Afouras</a>, <a href="http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1">Kumar Ashutosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baiyya_V/0/1/0/all/0/1">Vijay Baiyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1">Siddhant Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Boote_B/0/1/0/all/0/1">Bikram Boote</a>, <a href="http://arxiv.org/find/cs/1/au:+Byrne_E/0/1/0/all/0/1">Eugene Byrne</a>, <a href="http://arxiv.org/find/cs/1/au:+Chavis_Z/0/1/0/all/0/1">Zach Chavis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Joya Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1">Feng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1">Fu-Jen Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Crane_S/0/1/0/all/0/1">Sean Crane</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Avijit Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jing Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Escobar_M/0/1/0/all/0/1">Maria Escobar</a>, <a href="http://arxiv.org/find/cs/1/au:+Forigua_C/0/1/0/all/0/1">Cristhian Forigua</a>, <a href="http://arxiv.org/find/cs/1/au:+Gebreselasie_A/0/1/0/all/0/1">Abrham Gebreselasie</a>, <a href="http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1">Sanjay Haresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Mohaiminul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Suyog Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1">Rawal Khirodkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kukreja_D/0/1/0/all/0/1">Devansh Kukreja</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kevin J Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1">Sagnik Majumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yongsen Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1">Miguel Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mavroudi_E/0/1/0/all/0/1">Effrosyni Mavroudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1">Tushar Nagarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1">Francesco Ragusa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1">Santhosh Kumar Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Seminara_L/0/1/0/all/0/1">Luigi Seminara</a>, <a href="http://arxiv.org/find/cs/1/au:+Somayazulu_A/0/1/0/all/0/1">Arjun Somayazulu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yale Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Shan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zihui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Edward Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinxu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1">Angela Castillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xinzhu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1">Ryosuke Furuta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1">Cristina Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Prince Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jiabo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yifei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoo_W/0/1/0/all/0/1">Weslie Khoo</a>, et al. (48 additional authors not shown)</p>
<p>We present Ego-Exo4D, a diverse, large-scale multimodal multiview video
dataset and benchmark challenge. Ego-Exo4D centers around
simultaneously-captured egocentric and exocentric video of skilled human
activities (e.g., sports, music, dance, bike repair). More than 800
participants from 13 cities worldwide performed these activities in 131
different natural scene contexts, yielding long-form captures from 1 to 42
minutes each and 1,422 hours of video combined. The multimodal nature of the
dataset is unprecedented: the video is accompanied by multichannel audio, eye
gaze, 3D point clouds, camera poses, IMU, and multiple paired language
descriptions -- including a novel "expert commentary" done by coaches and
teachers and tailored to the skilled-activity domain. To push the frontier of
first-person video understanding of skilled human activity, we also present a
suite of benchmark tasks and their annotations, including fine-grained activity
understanding, proficiency estimation, cross-view translation, and 3D hand/body
pose. All resources will be open sourced to fuel new research in the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18260">Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1">David G.T. Barrett</a>, <a href="http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1">Andrew Sellergren</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1">Sumedh Ghaisas</a>, <a href="http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1">Sumanth Dathathri</a>, <a href="http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1">Abigail See</a>, <a href="http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1">Johannes Welbl</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1">Rhys May</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1">Roy Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1">SiWai Man</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1">Zahra Ahmed</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1">Sara Mahdavi</a>, <a href="http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1">Danielle Belgrave</a>, <a href="http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1">Shravya Shetty</a>, <a href="http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1">Pushmeet Kohli</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1">Po-Sen Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1">Ira Ktena</a></p>
<p>Radiology reports are an instrumental part of modern medicine, informing key
clinical decisions such as diagnosis and treatment. The worldwide shortage of
radiologists, however, restricts access to expert care and imposes heavy
workloads, contributing to avoidable errors and delays in report delivery.
While recent progress in automated report generation with vision-language
models offer clear potential in ameliorating the situation, the path to
real-world adoption has been stymied by the challenge of evaluating the
clinical quality of AI-generated reports. In this study, we build a
state-of-the-art report generation system for chest radiographs, Flamingo-CXR,
by fine-tuning a well-known vision-language foundation model on radiology data.
To evaluate the quality of the AI-generated reports, a group of 16 certified
radiologists provide detailed evaluations of AI-generated and human written
reports for chest X-rays from an intensive care setting in the United States
and an inpatient setting in India. At least one radiologist (out of two per
case) preferred the AI report to the ground truth report in over 60$\%$ of
cases for both datasets. Amongst the subset of AI-generated reports that
contain errors, the most frequently cited reasons were related to the location
and finding, whereas for human written reports, most mistakes were related to
severity and finding. This disparity suggested potential complementarity
between our AI system and human experts, prompting us to develop an assistive
scenario in which Flamingo-CXR generates a first-draft report, which is
subsequently revised by a clinician. This is the first demonstration of
clinician-AI collaboration for report writing, and the resultant reports are
assessed to be equivalent or preferred by at least one radiologist to reports
written by experts alone in 80$\%$ of in-patient cases and 66$\%$ of intensive
care cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18265">MCI Detection using fMRI time series embeddings of Recurrence plots. (arXiv:2311.18265v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aithal_N/0/1/0/all/0/1">Ninad Aithal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pradeep_C/0/1/0/all/0/1">Chakka Sai Pradeep</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1">Neelam Sinha</a></p>
<p>The human brain can be conceptualized as a dynamical system. Utilizing
resting state fMRI time series imaging, we can study the underlying dynamics at
ear-marked Regions of Interest (ROIs) to understand structure or lack thereof.
This differential behavior could be key to understanding the neurodegeneration
and also to classify between healthy and Mild Cognitive Impairment (MCI)
subjects. In this study, we consider 6 brain networks spanning over 160 ROIs
derived from Dosenbach template, where each network consists of 25-30 ROIs.
Recurrence plot, extensively used to understand evolution of time series, is
employed. Representative time series at each ROI is converted to its
corresponding recurrence plot visualization, which is subsequently condensed to
low-dimensional feature embeddings through Autoencoders. The performance of the
proposed method is shown on fMRI volumes of 100 subjects (balanced data), taken
from publicly available ADNI dataset. Results obtained show peak classification
accuracy of 93% among the 6 brain networks, mean accuracy of 89.3% thereby
illustrating promise in the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18266">Prompt-Based Exemplar Super-Compression and Regeneration for Class-Incremental Learning. (arXiv:2311.18266v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1">Ruxiao Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yaoyao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jieneng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1">Adam Kortylewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a></p>
<p>Replay-based methods in class-incremental learning (CIL) have attained
remarkable success, as replaying the exemplars of old classes can significantly
mitigate catastrophic forgetting. Despite their effectiveness, the inherent
memory restrictions of CIL result in saving a limited number of exemplars with
poor diversity, leading to data imbalance and overfitting issues. In this
paper, we introduce a novel exemplar super-compression and regeneration method,
ESCORT, which substantially increases the quantity and enhances the diversity
of exemplars. Rather than storing past images, we compress images into visual
and textual prompts, e.g., edge maps and class tags, and save the prompts
instead, reducing the memory usage of each exemplar to 1/24 of the original
size. In subsequent learning phases, diverse high-resolution exemplars are
generated from the prompts by a pre-trained diffusion model, e.g., ControlNet.
To minimize the domain gap between generated exemplars and real images, we
propose partial compression and diffusion-based data augmentation, allowing us
to utilize an off-the-shelf diffusion model without fine-tuning it on the
target dataset. Therefore, the same diffusion model can be downloaded whenever
it is needed, incurring no memory consumption. Comprehensive experiments
demonstrate that our method significantly improves model performance across
multiple CIL benchmarks, e.g., 5.0 percentage points higher than the previous
state-of-the-art on 10-phase Caltech-256 dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18270">Beyond Entropy: Style Transfer Guided Single Image Continual Test-Time Adaptation. (arXiv:2311.18270v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Younggeol Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngrae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongman Lee</a></p>
<p>Continual test-time adaptation (cTTA) methods are designed to facilitate the
continual adaptation of models to dynamically changing real-world environments
where computational resources are limited. Due to this inherent limitation,
existing approaches fail to simultaneously achieve accuracy and efficiency. In
detail, when using a single image, the instability caused by batch
normalization layers and entropy loss significantly destabilizes many existing
methods in real-world cTTA scenarios. To overcome these challenges, we present
BESTTA, a novel single image continual test-time adaptation method guided by
style transfer, which enables stable and efficient adaptation to the target
environment by transferring the style of the input image to the source style.
To implement the proposed method, we devise BeIN, a simple yet powerful
normalization method, along with the style-guided losses. We demonstrate that
BESTTA effectively adapts to the continually changing target environment,
leveraging only a single image on both semantic segmentation and image
classification tasks. Remarkably, despite training only two parameters in a
BeIN layer consuming the least memory, BESTTA outperforms existing
state-of-the-art methods in terms of performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18273">HKUST at SemEval-2023 Task 1: Visual Word Sense Disambiguation with Context Augmentation and Visual Assistance. (arXiv:2311.18273v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhuohao Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a></p>
<p>Visual Word Sense Disambiguation (VWSD) is a multi-modal task that aims to
select, among a batch of candidate images, the one that best entails the target
word's meaning within a limited context. In this paper, we propose a
multi-modal retrieval framework that maximally leverages pretrained
Vision-Language models, as well as open knowledge bases and datasets. Our
system consists of the following key components: (1) Gloss matching: a
pretrained bi-encoder model is used to match contexts with proper senses of the
target words; (2) Prompting: matched glosses and other textual information,
such as synonyms, are incorporated using a prompting template; (3) Image
retrieval: semantically matching images are retrieved from large open datasets
using prompts as queries; (4) Modality fusion: contextual information from
different modalities are fused and used for prediction. Although our system
does not produce the most competitive results at SemEval-2023 Task 1, we are
still able to beat nearly half of the teams. More importantly, our experiments
reveal acute insights for the field of Word Sense Disambiguation (WSD) and
multi-modal learning. Our code is available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18281">Utilizing Radiomic Feature Analysis For Automated MRI Keypoint Detection: Enhancing Graph Applications. (arXiv:2311.18281v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Nasser_S/0/1/0/all/0/1">Sahar Almahfouz Nasser</a>, <a href="http://arxiv.org/find/eess/1/au:+Pathak_S/0/1/0/all/0/1">Shashwat Pathak</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1">Keshav Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Meena_M/0/1/0/all/0/1">Mohit Meena</a>, <a href="http://arxiv.org/find/eess/1/au:+Gupte_N/0/1/0/all/0/1">Nihar Gupte</a>, <a href="http://arxiv.org/find/eess/1/au:+Chinmaya_A/0/1/0/all/0/1">Ananya Chinmaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Garg_P/0/1/0/all/0/1">Prateek Garg</a>, <a href="http://arxiv.org/find/eess/1/au:+Sethi_A/0/1/0/all/0/1">Amit Sethi</a></p>
<p>Graph neural networks (GNNs) present a promising alternative to CNNs and
transformers in certain image processing applications due to their
parameter-efficiency in modeling spatial relationships. Currently, a major area
of research involves the converting non-graph input data for GNN-based models,
notably in scenarios where the data originates from images. One approach
involves converting images into nodes by identifying significant keypoints
within them. Super-Retina, a semi-supervised technique, has been utilized for
detecting keypoints in retinal images. However, its limitations lie in the
dependency on a small initial set of ground truth keypoints, which is
progressively expanded to detect more keypoints. Having encountered
difficulties in detecting consistent initial keypoints in brain images using
SIFT and LoFTR, we proposed a new approach: radiomic feature-based keypoint
detection. Demonstrating the anatomical significance of the detected keypoints
was achieved by showcasing their efficacy in improving registration processes
guided by these keypoints. Subsequently, these keypoints were employed as the
ground truth for the keypoint detection method (LK-SuperRetina). Furthermore,
the study showcases the application of GNNs in image matching, highlighting
their superior performance in terms of both the number of good matches and
confidence scores. This research sets the stage for expanding GNN applications
into various other applications, including but not limited to image
classification, segmentation, and registration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18286">SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation. (arXiv:2311.18286v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lingyi Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shuyong Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">WenQiang Zhang</a></p>
<p>Unsupervised video object segmentation (UVOS) aims at detecting the primary
objects in a given video sequence without any human interposing. Most existing
methods rely on two-stream architectures that separately encode the appearance
and motion information before fusing them to identify the target and generate
object masks. However, this pipeline is computationally expensive and can lead
to suboptimal performance due to the difficulty of fusing the two modalities
properly. In this paper, we propose a novel UVOS model called SimulFlow that
simultaneously performs feature extraction and target identification, enabling
efficient and effective unsupervised video object segmentation. Concretely, we
design a novel SimulFlow Attention mechanism to bridege the image and motion by
utilizing the flexibility of attention operation, where coarse masks predicted
from fused feature at each stage are used to constrain the attention operation
within the mask area and exclude the impact of noise. Because of the
bidirectional information flow between visual and optical flow features in
SimulFlow Attention, no extra hand-designed fusing module is required and we
only adopt a light decoder to obtain the final prediction. We evaluate our
method on several benchmark datasets and achieve state-of-the-art results. Our
proposed approach not only outperforms existing methods but also addresses the
computational complexity and fusion difficulties caused by two-stream
architectures. Our models achieve 87.4% J &amp; F on DAVIS-16 with the highest
speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow
also obtains competitive results on video salient object detection datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18287">Dispersed Structured Light for Hyperspectral 3D Imaging. (arXiv:2311.18287v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1">Suhyun Shin</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_S/0/1/0/all/0/1">Seokjun Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Heide_F/0/1/0/all/0/1">Felix Heide</a>, <a href="http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1">Seung-Hwan Baek</a></p>
<p>Hyperspectral 3D imaging aims to acquire both depth and spectral information
of a scene. However, existing methods are either prohibitively expensive and
bulky or compromise on spectral and depth accuracy. In this work, we present
Dispersed Structured Light (DSL), a cost-effective and compact method for
accurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera
system by placing a sub-millimeter thick diffraction grating film front of the
projector. The grating disperses structured light based on light wavelength. To
utilize the dispersed structured light, we devise a model for dispersive
projection image formation and a per-pixel hyperspectral 3D reconstruction
method. We validate DSL by instantiating a compact experimental prototype. DSL
achieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth
error of 1mm. We demonstrate that DSL outperforms prior work on practical
hyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D
imaging for diverse application domains, including computer vision and
graphics, cultural heritage, geology, and biology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18288">CosAvatar: Consistent and Animatable Portrait Video Tuning with Text Prompt. (arXiv:2311.18288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1">Haiyao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1">Chenglai Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yudong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Juyong Zhang</a></p>
<p>Recently, text-guided digital portrait editing has attracted more and more
attentions. However, existing methods still struggle to maintain consistency
across time, expression, and view or require specific data prerequisites. To
solve these challenging problems, we propose CosAvatar, a high-quality and
user-friendly framework for portrait tuning. With only monocular video and text
instructions as input, we can produce animatable portraits with both temporal
and 3D consistency. Different from methods that directly edit in the 2D domain,
we employ a dynamic NeRF-based 3D portrait representation to model both the
head and torso. We alternate between editing the video frames' dataset and
updating the underlying 3D portrait until the edited frames reach 3D
consistency. Additionally, we integrate the semantic portrait priors to enhance
the edited results, allowing precise modifications in specified semantic areas.
Extensive results demonstrate that our proposed method can not only accurately
edit portrait styles or local attributes based on text instructions but also
support expressive animation driven by a source video.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18291">TLDR: Text Based Last-layer Retraining for Debiasing Image Classifiers. (arXiv:2311.18291v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Juhyeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Seokhyeon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1">Taesup Moon</a></p>
<p>A classifier may depend on incidental features stemming from a strong
correlation between the feature and the classification target in the training
dataset. Recently, Last Layer Retraining (LLR) with group-balanced datasets is
known to be efficient in mitigating the spurious correlation of classifiers.
However, the acquisition of group-balanced datasets is costly, which hinders
the applicability of the LLR method. In this work, we propose to perform LLR
based on text datasets built with large language models for a general image
classifier. We demonstrate that text can be a proxy for its corresponding image
beyond the image-text joint embedding space, such as CLIP. Based on this, we
use generated texts to train the final layer in the embedding space of the
arbitrary image classifier. In addition, we propose a method of filtering the
generated words to get rid of noisy, imprecise words, which reduces the effort
of inspecting each word. We dub these procedures as TLDR (\textbf{T}ext-based
\textbf{L}ast layer retraining for \textbf{D}ebiasing image
classifie\textbf{R}s) and show our method achieves the performance that is
comparable to those of the LLR methods that also utilize group-balanced image
dataset for retraining. Furthermore, TLDR outperforms other baselines that
involve training the last linear layer without a group annotated dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18296">Perceptual Group Tokenizer: Building Perception with Iterative Grouping. (arXiv:2311.18296v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhiwei Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a></p>
<p>Human visual recognition system shows astonishing capability of compressing
visual information into a set of tokens containing rich representations without
label supervision. One critical driving principle behind it is perceptual
grouping. Despite being widely used in computer vision in the early 2010s, it
remains a mystery whether perceptual grouping can be leveraged to derive a
neural visual recognition backbone that generates as powerful representations.
In this paper, we propose the Perceptual Group Tokenizer, a model that entirely
relies on grouping operations to extract visual features and perform
self-supervised representation learning, where a series of grouping operations
are used to iteratively hypothesize the context for pixels or superpixels to
refine feature representations. We show that the proposed model can achieve
competitive performance compared to state-of-the-art vision architectures, and
inherits desirable properties including adaptive computation without
re-training, and interpretability. Specifically, Perceptual Group Tokenizer
achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear
probe evaluation, marking a new progress under this paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18297">TrustMark: Universal Watermarking for Arbitrary Resolution Images. (arXiv:2311.18297v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1">Tu Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Shruti Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1">John Collomosse</a></p>
<p>Imperceptible digital watermarking is important in copyright protection,
misinformation prevention, and responsible generative AI. We propose TrustMark
- a GAN-based watermarking method with novel design in architecture and
spatio-spectra losses to balance the trade-off between watermarked image
quality with the watermark recovery accuracy. Our model is trained with
robustness in mind, withstanding various in- and out-place perturbations on the
encoded image. Additionally, we introduce TrustMark-RM - a watermark remover
method useful for re-watermarking. Our methods achieve state-of-art performance
on 3 benchmarks comprising arbitrary resolution images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18299">Reconstructing the normal and shape at specularities in endoscopy. (arXiv:2311.18299v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Makki_K/0/1/0/all/0/1">Karim Makki</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartoli_A/0/1/0/all/0/1">Adrien Bartoli</a></p>
<p>Specularities are numerous in endoscopic images. They occur as many white
small elliptic spots, which are generally ruled out as nuisance in image
analysis and computer vision methods. Instead, we propose to use specularities
as cues for 3D perception. Specifically, we propose a new method to
reconstruct, at each specularity, the observed tissue's normal direction (i.e.,
its orientation) and shape (i.e., its curvature) from a single image. We show
results on simulated and real interventional images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18303">OmniMotionGPT: Animal Motion Generation with Limited Data. (arXiv:2311.18303v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhangsihao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1">Mengyi Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bingbing Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_Z/0/1/0/all/0/1">Ziwei Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_M/0/1/0/all/0/1">Mitch Hill</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Junjie Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yalin Wang</a></p>
<p>Our paper aims to generate diverse and realistic animal motion sequences from
textual descriptions, without a large-scale animal text-motion dataset. While
the task of text-driven human motion synthesis is already extensively studied
and benchmarked, it remains challenging to transfer this success to other
skeleton structures with limited data. In this work, we design a model
architecture that imitates Generative Pretraining Transformer (GPT), utilizing
prior knowledge learned from human data to the animal domain. We jointly train
motion autoencoders for both animal and human motions and at the same time
optimize through the similarity scores among human motion encoding, animal
motion encoding, and text CLIP embedding. Presenting the first solution to this
problem, we are able to generate animal motions with high diversity and
fidelity, quantitatively and qualitatively outperforming the results of
training human motion generation baselines on animal data. Additionally, we
introduce AnimalML3D, the first text-animal motion dataset with 1240 animation
sequences spanning 36 different animal identities. We hope this dataset would
mediate the data scarcity problem in text-driven animal motion generation,
providing a new playground for the research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18307">Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent. (arXiv:2311.18307v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonkens_S/0/1/0/all/0/1">Sander Tonkens</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>Adept traffic models are critical to both planning and closed-loop simulation
for autonomous vehicles (AV), and key design objectives include accuracy,
diverse multimodal behaviors, interpretability, and downstream compatibility.
Recently, with the advent of large language models (LLMs), an additional
desirable feature for traffic models is LLM compatibility. We present
Categorical Traffic Transformer (CTT), a traffic model that outputs both
continuous trajectory predictions and tokenized categorical predictions (lane
modes, homotopies, etc.). The most outstanding feature of CTT is its fully
interpretable latent space, which enables direct supervision of the latent
variable from the ground truth during training and avoids mode collapse
completely. As a result, CTT can generate diverse behaviors conditioned on
different latent modes with semantic meanings while beating SOTA on prediction
accuracy. In addition, CTT's ability to input and output tokens enables
integration with LLMs for common-sense reasoning and zero-shot generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18311">Anisotropic Neural Representation Learning for High-Quality Neural Rendering. (arXiv:2311.18311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Y.Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">J. Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Y. Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Y. Gong</a></p>
<p>Neural radiance fields (NeRFs) have achieved impressive view synthesis
results by learning an implicit volumetric representation from multi-view
images. To project the implicit representation into an image, NeRF employs
volume rendering that approximates the continuous integrals of rays as an
accumulation of the colors and densities of the sampled points. Although this
approximation enables efficient rendering, it ignores the direction information
in point intervals, resulting in ambiguous features and limited reconstruction
quality. In this paper, we propose an anisotropic neural representation
learning method that utilizes learnable view-dependent features to improve
scene representation and reconstruction. We model the volumetric function as
spherical harmonic (SH)-guided anisotropic features, parameterized by
multilayer perceptrons, facilitating ambiguity elimination while preserving the
rendering efficiency. To achieve robust scene reconstruction without anisotropy
overfitting, we regularize the energy of the anisotropic features during
training. Our method is flexiable and can be plugged into NeRF-based
frameworks. Extensive experiments show that the proposed representation can
boost the rendering quality of various NeRFs and achieve state-of-the-art
rendering performance on both synthetic and real-world scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18328">Advances in 3D Neural Stylization: A Survey. (arXiv:2311.18328v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingshu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_G/0/1/0/all/0/1">Guocheng Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1">Ka Chun Shum</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1">Binh-Son Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1">Sai-Kit Yeung</a></p>
<p>Modern artificial intelligence provides a novel way of producing digital art
in styles. The expressive power of neural networks enables the realm of visual
style transfer methods, which can be used to edit images, videos, and 3D data
to make them more artistic and diverse. This paper reports on recent advances
in neural stylization for 3D data. We provide a taxonomy for neural stylization
by considering several important design choices, including scene
representation, guidance data, optimization strategies, and output styles.
Building on such taxonomy, our survey first revisits the background of neural
stylization on 2D images, and then provides in-depth discussions on recent
neural stylization methods for 3D data, where we also provide a mini-benchmark
on artistic stylization methods. Based on the insights gained from the survey,
we then discuss open challenges, future research, and potential applications
and impacts of neural stylization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18331">MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation. (arXiv:2311.18331v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Udupa_S/0/1/0/all/0/1">Sumanth Udupa</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurunath_P/0/1/0/all/0/1">Prajwal Gurunath</a>, <a href="http://arxiv.org/find/cs/1/au:+Sikdar_A/0/1/0/all/0/1">Aniruddh Sikdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1">Suresh Sundaram</a></p>
<p>Deep neural networks have shown exemplary performance on semantic scene
understanding tasks on source domains, but due to the absence of style
diversity during training, enhancing performance on unseen target domains using
only single source domain data remains a challenging task. Generation of
simulated data is a feasible alternative to retrieving large style-diverse
real-world datasets as it is a cumbersome and budget-intensive process.
However, the large domain-specific inconsistencies between simulated and
real-world data pose a significant generalization challenge in semantic
segmentation. In this work, to alleviate this problem, we propose a novel
MultiResolution Feature Perturbation (MRFP) technique to randomize
domain-specific fine-grained features and perturb style of coarse features. Our
experimental results on various urban-scene segmentation datasets clearly
indicate that, along with the perturbation of style-information, perturbation
of fine-feature components is paramount to learn domain invariant robust
feature maps for semantic segmentation models. MRFP is a simple and
computationally efficient, transferable module with no additional learnable
parameters or objective functions, that helps state-of-the-art deep neural
networks to learn robust domain invariant features for simulation-to-real
semantic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18332">Multilevel Saliency-Guided Self-Supervised Learning for Image Anomaly Detection. (arXiv:2311.18332v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jianjian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1">Chunzhi Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>Anomaly detection (AD) is a fundamental task in computer vision. It aims to
identify incorrect image data patterns which deviate from the normal ones.
Conventional methods generally address AD by preparing augmented negative
samples to enforce self-supervised learning. However, these techniques
typically do not consider semantics during augmentation, leading to the
generation of unrealistic or invalid negative samples. Consequently, the
feature extraction network can be hindered from embedding critical features. In
this study, inspired by visual attention learning approaches, we propose
CutSwap, which leverages saliency guidance to incorporate semantic cues for
augmentation. Specifically, we first employ LayerCAM to extract multilevel
image features as saliency maps and then perform clustering to obtain multiple
centroids. To fully exploit saliency guidance, on each map, we select a pixel
pair from the cluster with the highest centroid saliency to form a patch pair.
Such a patch pair includes highly similar context information with dense
semantic correlations. The resulting negative sample is created by swapping the
locations of the patch pair. Compared to prior augmentation methods, CutSwap
generates more subtle yet realistic negative samples to facilitate quality
feature learning. Extensive experimental and ablative evaluations demonstrate
that our method achieves state-of-the-art AD performance on two mainstream AD
benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18344">DSeg: Direct Line Segments Detection. (arXiv:2311.18344v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cyrille_B/0/1/0/all/0/1">Berger Cyrille</a>, <a href="http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1">Lacroix Simon</a></p>
<p>This paper presents a model-driven approach to detect image line segments.
The approach incrementally detects segments on the gradient image using a
linear Kalman filter that estimates the supporting line parameters and their
associated variances. The algorithm is fast and robust with respect to image
noise and illumination variations, it allows the detection of longer line
segments than data-driven approaches, and does not require any tedious
parameters tuning. An extension of the algorithm that exploits a pyramidal
approach to enhance the quality of results is proposed. Results with varying
scene illumination and comparisons to classic existing approaches are
presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18358">TIDE: Test Time Few Shot Object Detection. (arXiv:2311.18358v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weikai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Hongfeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yanlai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1">Yudi Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Ying Tang</a></p>
<p>Few-shot object detection (FSOD) aims to extract semantic knowledge from
limited object instances of novel categories within a target domain. Recent
advances in FSOD focus on fine-tuning the base model based on a few objects via
meta-learning or data augmentation. Despite their success, the majority of them
are grounded with parametric readjustment to generalize on novel objects, which
face considerable challenges in Industry 5.0, such as (i) a certain amount of
fine-tuning time is required, and (ii) the parameters of the constructed model
being unavailable due to the privilege protection, making the fine-tuning fail.
Such constraints naturally limit its application in scenarios with real-time
configuration requirements or within black-box settings. To tackle the
challenges mentioned above, we formalize a novel FSOD task, referred to as Test
TIme Few Shot DEtection (TIDE), where the model is un-tuned in the
configuration procedure. To that end, we introduce an asymmetric architecture
for learning a support-instance-guided dynamic category classifier. Further, a
cross-attention module and a multi-scale resizer are provided to enhance the
model performance. Experimental results on multiple few-shot object detection
platforms reveal that the proposed TIDE significantly outperforms existing
contemporary methods. The implementation codes are available at
https://github.com/deku-0621/TIDE
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18361">Automating lookahead planning using site appearance and space utilization. (arXiv:2311.18361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mengiste_E/0/1/0/all/0/1">Eyob Mengiste</a>, <a href="http://arxiv.org/find/cs/1/au:+Soto_B/0/1/0/all/0/1">Borja Garcia de Soto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartmann_T/0/1/0/all/0/1">Timo Hartmann</a></p>
<p>This study proposes a method to automate the development of lookahead
planning. The proposed method uses construction material conditions (i.e.,
appearances) and site space utilization to predict task completion rates. A
Gated Recurrent Unit (GRU) based Recurrent Neural Network (RNN) model was
trained using a segment of a construction project timeline to estimate
completion rates of tasks and propose data-aware lookahead plans. The proposed
method was evaluated in a sample construction project involving finishing works
such as plastering, painting, and installing electrical fixtures. The results
show that the proposed method can assist with developing automated lookahead
plans. In doing so, this study links construction planning with actual events
at the construction site. It extends the traditional scheduling techniques and
integrates a broader spectrum of site spatial constraints into lookahead
planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18363">Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation. (arXiv:2311.18363v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yiwen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Mengkang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yongsheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yong Xia</a></p>
<p>Distribution shift widely exists in medical images acquired from different
medical centres and poses a significant obstacle to deploying the pre-trained
semantic segmentation model in real-world applications. Test-time adaptation
has proven its effectiveness in tackling the cross-domain distribution shift
during inference. However, most existing methods achieve adaptation by updating
the pre-trained models, rendering them susceptible to error accumulation and
catastrophic forgetting when encountering a series of distribution shifts
(i.e., under the continual test-time adaptation setup). To overcome these
challenges caused by updating the models, in this paper, we freeze the
pre-trained model and propose the Visual Prompt-based Test-Time Adaptation
(VPTTA) method to train a specific prompt for each test image to align the
statistics in the batch normalization layers. Specifically, we present the
low-frequency prompt, which is lightweight with only a few parameters and can
be effectively trained in a single iteration. To enhance prompt initialization,
we equip VPTTA with a memory bank to benefit the current prompt from previous
ones. Additionally, we design a warm-up mechanism, which mixes source and
target statistics to construct warm-up statistics, thereby facilitating the
training process. Extensive experiments demonstrate the superiority of our
VPTTA over other state-of-the-art methods on two medical image segmentation
benchmark tasks. The code and weights of pre-trained source models are
available at https://github.com/Chen-Ziyang/VPTTA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18373">A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends. (arXiv:2311.18373v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jiaxin Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaiwen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yizhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ye Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a></p>
<p>Early detection and assessment of polyps play a crucial role in the
prevention and treatment of colorectal cancer (CRC). Polyp segmentation
provides an effective solution to assist clinicians in accurately locating and
segmenting polyp regions. In the past, people often relied on manually
extracted lower-level features such as color, texture, and shape, which often
had issues capturing global context and lacked robustness to complex scenarios.
With the advent of deep learning, more and more outstanding medical image
segmentation algorithms based on deep learning networks have emerged, making
significant progress in this field. This paper provides a comprehensive review
of polyp segmentation algorithms. We first review some traditional algorithms
based on manually extracted features and deep segmentation algorithms, then
detail benchmark datasets related to the topic. Specifically, we carry out a
comprehensive evaluation of recent deep learning models and results based on
polyp sizes, considering the pain points of research topics and differences in
network structures. Finally, we discuss the challenges of polyp segmentation
and future trends in this field. The models, benchmark datasets, and source
code links we collected are all published at
https://github.com/taozh2017/Awesome-Polyp-Segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18387">On Exact Inversion of DPM-Solvers. (arXiv:2311.18387v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Seongmin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyeonghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1">Suh Yoon Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1">Hyewon Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1">Se Young Chun</a></p>
<p>Diffusion probabilistic models (DPMs) are a key component in modern
generative models. DPM-solvers have achieved reduced latency and enhanced
quality significantly, but have posed challenges to find the exact inverse
(i.e., finding the initial noise from the given image). Here we investigate the
exact inversions for DPM-solvers and propose algorithms to perform them when
samples are generated by the first-order as well as higher-order DPM-solvers.
For each explicit denoising step in DPM-solvers, we formulated the inversions
using implicit methods such as gradient descent or forward step method to
ensure the robustness to large classifier-free guidance unlike the prior
approach using fixed-point iteration. Experimental results demonstrated that
our proposed exact inversion methods significantly reduced the error of both
image and noise reconstructions, greatly enhanced the ability to distinguish
invisible watermarks and well prevented unintended background changes
consistently during image editing. Project page:
\url{https://smhongok.github.io/inv-dpm.html}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18398">RainAI -- Precipitation Nowcasting from Satellite Data. (arXiv:2311.18398v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarabia_R/0/1/0/all/0/1">Rafael Pablos Sarabia</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1">Joachim Nyborg</a>, <a href="http://arxiv.org/find/cs/1/au:+Birk_M/0/1/0/all/0/1">Morten Birk</a>, <a href="http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1">Ira Assent</a></p>
<p>This paper presents a solution to the Weather4Cast 2023 competition, where
the goal is to forecast high-resolution precipitation with an 8-hour lead time
using lower-resolution satellite radiance images. We propose a simple, yet
effective method for spatiotemporal feature learning using a 2D U-Net model,
that outperforms the official 3D U-Net baseline in both performance and
efficiency. We place emphasis on refining the dataset, through importance
sampling and dataset preparation, and show that such techniques have a
significant impact on performance. We further study an alternative
cross-entropy loss function that improves performance over the standard mean
squared error loss, while also enabling models to produce probabilistic
outputs. Additional techniques are explored regarding the generation of
predictions at different lead times, specifically through Conditioning Lead
Time. Lastly, to generate high-resolution forecasts, we evaluate standard and
learned upsampling methods. The code and trained parameters are available at
https://github.com/rafapablos/w4c23-rainai.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18402">MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition. (arXiv:2311.18402v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xinwei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weizhi Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenhui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anan Liu</a></p>
<p>Large-scale pre-trained models have demonstrated impressive performance in
vision and language tasks within open-world scenarios. Due to the lack of
comparable pre-trained models for 3D shapes, recent methods utilize
language-image pre-training to realize zero-shot 3D shape recognition. However,
due to the modality gap, pretrained language-image models are not confident
enough in the generalization to 3D shape recognition. Consequently, this paper
aims to improve the confidence with view selection and hierarchical prompts.
Leveraging the CLIP model as an example, we employ view selection on the vision
side by identifying views with high prediction confidence from multiple
rendered views of a 3D shape. On the textual side, the strategy of hierarchical
prompts is proposed for the first time. The first layer prompts several
classification candidates with traditional class-level descriptions, while the
second layer refines the prediction based on function-level descriptions or
further distinctions between the candidates. Remarkably, without the need for
additional training, our proposed method achieves impressive zero-shot 3D
classification accuracies of 84.44\%, 91.51\%, and 66.17\% on ModelNet40,
ModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the
code publicly available to facilitate reproducibility and further research in
this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18403">Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image Transformations. (arXiv:2311.18403v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xianlong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shengshan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhifei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Ziqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leo Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hai Jin</a></p>
<p>Unlearnable datasets lead to a drastic drop in the generalization performance
of models trained on them by introducing elaborate and imperceptible
perturbations into clean training sets. Many existing defenses, e.g., JPEG
compression and adversarial training, effectively counter UDs based on
norm-constrained additive noise. However, a fire-new type of convolution-based
UDs have been proposed and render existing defenses all ineffective, presenting
a greater challenge to defenders. To address this, we express the
convolution-based unlearnable sample as the result of multiplying a matrix by a
clean sample in a simplified scenario, and formalize the intra-class matrix
inconsistency as $\Theta_{imi}$, inter-class matrix consistency as
$\Theta_{imc}$ to investigate the working mechanism of the convolution-based
UDs. We conjecture that increasing both of these metrics will mitigate the
unlearnability effect. Through validation experiments that commendably support
our hypothesis, we further design a random matrix to boost both $\Theta_{imi}$
and $\Theta_{imc}$, achieving a notable degree of defense effect. Hence, by
building upon and extending these facts, we first propose a brand-new image
COrruption that employs randomly multiplicative transformation via
INterpolation operation to successfully defend against convolution-based UDs.
Our approach leverages global pixel random interpolations, effectively
suppressing the impact of multiplicative noise in convolution-based UDs.
Additionally, we have also designed two new forms of convolution-based UDs, and
find that our defense is the most effective against them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18405">CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model. (arXiv:2311.18405v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jianhao Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weizhi Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1">Hongshuo Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tongtong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anan Liu</a></p>
<p>Image-based virtual try-on enables users to virtually try on different
garments by altering original clothes in their photographs. Generative
Adversarial Networks (GANs) dominate the research field in image-based virtual
try-on, but have not resolved problems such as unnatural deformation of
garments and the blurry generation quality. Recently, diffusion models have
emerged with surprising performance across various image generation tasks.
While the generative quality of diffusion models is impressive, achieving
controllability poses a significant challenge when applying it to virtual
try-on tasks and multiple denoising iterations limit its potential for
real-time applications. In this paper, we propose Controllable Accelerated
virtual Try-on with Diffusion Model called CAT-DM. To enhance the
controllability, a basic diffusion-based virtual try-on network is designed,
which utilizes ControlNet to introduce additional control conditions and
improves the feature extraction of garment images. In terms of acceleration,
CAT-DM initiates a reverse denoising process with an implicit distribution
generated by a pre-trained GAN-based model. Compared with previous try-on
methods based on diffusion models, CAT-DM not only retains the pattern and
texture details of the in-shop garment but also reduces the sampling steps
without compromising generation quality. Extensive experiments demonstrate the
superiority of CAT-DM against both GAN-based and diffusion-based methods in
producing more realistic images and accurately reproducing garment patterns.
Our code and models will be publicly released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18420">TeG-DG: Textually Guided Domain Generalization for Face Anti-Spoofing. (arXiv:2311.18420v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mu_L/0/1/0/all/0/1">Lianrui Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jianhong Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaoxuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jiangnan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaoyu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1">Jiedong Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Haoji Hu</a></p>
<p>Enhancing the domain generalization performance of Face Anti-Spoofing (FAS)
techniques has emerged as a research focus. Existing methods are dedicated to
extracting domain-invariant features from various training domains. Despite the
promising performance, the extracted features inevitably contain residual style
feature bias (e.g., illumination, capture device), resulting in inferior
generalization performance. In this paper, we propose an alternative and
effective solution, the Textually Guided Domain Generalization (TeG-DG)
framework, which can effectively leverage text information for cross-domain
alignment. Our core insight is that text, as a more abstract and universal form
of expression, can capture the commonalities and essential characteristics
across various attacks, bridging the gap between different image domains.
Contrary to existing vision-language models, the proposed framework is
elaborately designed to enhance the domain generalization ability of the FAS
task. Concretely, we first design a Hierarchical Attention Fusion (HAF) module
to enable adaptive aggregation of visual features at different levels; Then, a
Textual-Enhanced Visual Discriminator (TEVD) is proposed for not only better
alignment between the two modalities but also to regularize the classifier with
unbiased text features. TeG-DG significantly outperforms previous approaches,
especially in situations with extremely limited source domain data (~14% and
~12% improvements on HTER and AUC respectively), showcasing impressive few-shot
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18433">E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning. (arXiv:2311.18433v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xiuhong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1">Changjie Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhipeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1">Siqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yu Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiquan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1">Xuesheng Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Matthias M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cheng Wang</a></p>
<p>Event cameras have emerged as a promising vision sensor in recent years due
to their unparalleled temporal resolution and dynamic range. While registration
of 2D RGB images to 3D point clouds is a long-standing problem in computer
vision, no prior work studies 2D-3D registration for event cameras. To this
end, we propose E2PNet, the first learning-based method for event-to-point
cloud registration. The core of E2PNet is a novel feature representation
network called Event-Points-to-Tensor (EP2T), which encodes event data into a
2D grid-shaped feature tensor. This grid-shaped feature enables matured
RGB-based frameworks to be easily used for event-to-point cloud registration,
without changing hyper-parameters and the training procedure. EP2T treats the
event input as spatio-temporal point clouds. Unlike standard 3D learning
architectures that treat all dimensions of point clouds equally, the novel
sampling and information aggregation modules in EP2T are designed to handle the
inhomogeneity of the spatial and temporal dimensions. Experiments on the MVSEC
and VECtor datasets demonstrate the superiority of E2PNet over hand-crafted and
other learning-based methods. Compared to RGB-based registration, E2PNet is
more robust to extreme illumination or fast motion due to the use of event
data. Beyond 2D-3D registration, we also show the potential of EP2T for other
vision tasks such as flow estimation, event-to-image reconstruction and object
recognition. The source code can be found at:
https://github.com/Xmu-qcj/E2PNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18435">Layered Rendering Diffusion Model for Zero-Shot Guided Image Synthesis. (arXiv:2311.18435v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zipeng Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guoxi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zebin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Junyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lufei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1">Errui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a></p>
<p>This paper introduces innovative solutions to enhance spatial controllability
in diffusion models reliant on text queries. We present two key innovations:
Vision Guidance and the Layered Rendering Diffusion (LRDiff) framework. Vision
Guidance, a spatial layout condition, acts as a clue in the perturbed
distribution, greatly narrowing down the search space, to focus on the image
sampling process adhering to the spatial layout condition. The LRDiff framework
constructs an image-rendering process with multiple layers, each of which
applies the vision guidance to instructively estimate the denoising direction
for a single object. Such a layered rendering strategy effectively prevents
issues like unintended conceptual blending or mismatches, while allowing for
more coherent and contextually accurate image synthesis. The proposed method
provides a more efficient and accurate means of synthesising images that align
with specific spatial and contextual requirements. We demonstrate through our
experiments that our method provides better results than existing techniques
both quantitatively and qualitatively. We apply our method to three practical
applications: bounding box-to-image, semantic mask-to-image and image editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18445">VTimeLLM: Empower LLM to Grasp Video Moments. (arXiv:2311.18445v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Bin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zihan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Large language models (LLMs) have shown remarkable text understanding
capabilities, which have been extended as Video LLMs to handle video data for
comprehending visual details. However, existing Video LLMs can only provide a
coarse description of the entire video, failing to capture the precise start
and end time boundary of specific events. In this paper, we solve this issue
via proposing VTimeLLM, a novel Video LLM designed for fine-grained video
moment understanding and reasoning with respect to time boundary. Specifically,
our VTimeLLM adopts a boundary-aware three-stage training strategy, which
respectively utilizes image-text pairs for feature alignment, multiple-event
videos to increase temporal-boundary awareness, and high-quality
video-instruction tuning to further improve temporal understanding ability as
well as align with human intents. Extensive experiments demonstrate that in
fine-grained time-related comprehension tasks for videos such as Temporal Video
Grounding and Dense Video Captioning, VTimeLLM significantly outperforms
existing Video LLMs. Besides, benefits from the fine-grained temporal
understanding of the videos further enable VTimeLLM to beat existing Video LLMs
in video dialogue benchmark, showing its superior cross-modal understanding and
reasoning abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18448">HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video. (arXiv:2311.18448v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zicong Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Parelli_M/0/1/0/all/0/1">Maria Parelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadoglou_M/0/1/0/all/0/1">Maria Eleni Kadoglou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1">Muhammed Kocabas</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a></p>
<p>Since humans interact with diverse objects every day, the holistic 3D capture
of these interactions is important to understand and model human behaviour.
However, most existing methods for hand-object reconstruction from RGB either
assume pre-scanned object templates or heavily rely on limited 3D hand-object
data, restricting their ability to scale and generalize to more unconstrained
interaction settings. To this end, we introduce HOLD -- the first
category-agnostic method that reconstructs an articulated hand and object
jointly from a monocular interaction video. We develop a compositional
articulated implicit model that can reconstruct disentangled 3D hand and object
from 2D images. We also further incorporate hand-object constraints to improve
hand-object poses and consequently the reconstruction quality. Our method does
not rely on 3D hand-object annotations while outperforming fully-supervised
baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we
qualitatively show its robustness in reconstructing from in-the-wild videos.
Code: https://github.com/zc-alexfan/hold
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18481">ESG Accountability Made Easy: DocQA at Your Service. (arXiv:2311.18481v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_L/0/1/0/all/0/1">Lokesh Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Berrospi_C/0/1/0/all/0/1">Cesar Berrospi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinkla_K/0/1/0/all/0/1">Kasper Dinkla</a>, <a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1">Diego Antognini</a>, <a href="http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1">Francesco Fusco</a>, <a href="http://arxiv.org/find/cs/1/au:+Bothur_B/0/1/0/all/0/1">Benedikt Bothur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lysak_M/0/1/0/all/0/1">Maksym Lysak</a>, <a href="http://arxiv.org/find/cs/1/au:+Livathinos_N/0/1/0/all/0/1">Nikolaos Livathinos</a>, <a href="http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1">Ahmed Nassar</a>, <a href="http://arxiv.org/find/cs/1/au:+Vagenas_P/0/1/0/all/0/1">Panagiotis Vagenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1">Lucas Morin</a>, <a href="http://arxiv.org/find/cs/1/au:+Auer_C/0/1/0/all/0/1">Christoph Auer</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolfi_M/0/1/0/all/0/1">Michele Dolfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1">Peter Staar</a></p>
<p>We present Deep Search DocQA. This application enables information extraction
from documents via a question-answering conversational assistant. The system
integrates several technologies from different AI disciplines consisting of
document conversion to machine-readable format (via computer vision), finding
relevant data (via natural language processing), and formulating an eloquent
response (via large language models). Users can explore over 10,000
Environmental, Social, and Governance (ESG) disclosure reports from over 2000
corporations. The Deep Search platform can be accessed at:
https://ds4sd.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18482">Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding. (arXiv:2311.18482v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jin-Chuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Miao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1">Hao-Bin Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1">Shao-Hua Guan</a></p>
<p>Open-vocabulary querying in 3D space is challenging but essential for scene
understanding tasks such as object localization and segmentation.
Language-embedded scene representations have made progress by incorporating
language features into 3D spaces. However, their efficacy heavily depends on
neural networks that are resource-intensive in training and rendering. Although
recent 3D Gaussians offer efficient and high-quality novel view synthesis,
directly embedding language features in them leads to prohibitive memory usage
and decreased performance. In this work, we introduce Language Embedded 3D
Gaussians, a novel scene representation for open-vocabulary query tasks.
Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we
propose a dedicated quantization scheme that drastically alleviates the memory
requirement, and a novel embedding procedure that achieves smoother yet high
accuracy query, countering the multi-view feature inconsistencies and the
high-frequency inductive bias in point-based representations. Our comprehensive
experiments show that our representation achieves the best visual quality and
language querying accuracy across current language-embedded representations,
while maintaining real-time rendering frame rates on a single desktop GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18491">ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs. (arXiv:2311.18491v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_V/0/1/0/all/0/1">Violeta Men&#xe9;ndez Gonz&#xe1;lez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1">Andrew Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillipson_G/0/1/0/all/0/1">Graeme Phillipson</a>, <a href="http://arxiv.org/find/cs/1/au:+Jolly_S/0/1/0/all/0/1">Stephen Jolly</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1">Simon Hadfield</a></p>
<p>In the field of media production, video editing techniques play a pivotal
role. Recent approaches have had great success at performing novel view image
synthesis of static scenes. But adding temporal information adds an extra layer
of complexity. Previous models have focused on implicitly representing static
and dynamic scenes using NeRF. These models achieve impressive results but are
costly at training and inference time. They overfit an MLP to describe the
scene implicitly as a function of position. This paper proposes ZeST-NeRF, a
new approach that can produce temporal NeRFs for new scenes without retraining.
We can accurately reconstruct novel views using multi-view synthesis techniques
and scene flow-field estimation, trained only with unrelated scenes. We
demonstrate how existing state-of-the-art approaches from a range of fields
cannot adequately solve this new task and demonstrate the efficacy of our
solution. The resulting network improves quantitatively by 15% and produces
significantly better visual results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18494">PRS: Sharp Feature Priors for Resolution-Free Surface Remeshing. (arXiv:2311.18494v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Soboleva_N/0/1/0/all/0/1">Natalia Soboleva</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbunova_O/0/1/0/all/0/1">Olga Gorbunova</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivanova_M/0/1/0/all/0/1">Maria Ivanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1">Evgeny Burnaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1">Matthias Nie&#xdf;ner</a>, <a href="http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1">Denis Zorin</a>, <a href="http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1">Alexey Artemov</a></p>
<p>Surface reconstruction with preservation of geometric features is a
challenging computer vision task. Despite significant progress in implicit
shape reconstruction, state-of-the-art mesh extraction methods often produce
aliased, perceptually distorted surfaces and lack scalability to
high-resolution 3D shapes. We present a data-driven approach for automatic
feature detection and remeshing that requires only a coarse, aliased mesh as
input and scales to arbitrary resolution reconstructions. We define and learn a
collection of surface-based fields to (1) capture sharp geometric features in
the shape with an implicit vertexwise model and (2) approximate improvements in
normals alignment obtained by applying edge-flips with an edgewise model. To
support scaling to arbitrary complexity shapes, we learn our fields using local
triangulated patches, fusing estimates on complete surface meshes. Our feature
remeshing algorithm integrates the learned fields as sharp feature priors and
optimizes vertex placement and mesh connectivity for maximum expected surface
improvement. On a challenging collection of high-resolution shape
reconstructions in the ABC dataset, our algorithm improves over
state-of-the-art by 26% normals F-score and 42% perceptual
$\text{RMSE}_{\text{v}}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18495">Improving Adversarial Transferability via Model Alignment. (arXiv:2311.18495v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1">Avery Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1">Amir-massoud Farahmand</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yangchen Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a></p>
<p>Neural networks are susceptible to adversarial perturbations that are
transferable across different models. In this paper, we introduce a novel model
alignment technique aimed at improving a given source model's ability in
generating transferable adversarial perturbations. During the alignment
process, the parameters of the source model are fine-tuned to minimize an
alignment loss. This loss measures the divergence in the predictions between
the source model and another, independently trained model, referred to as the
witness model. To understand the effect of model alignment, we conduct a
geometric anlaysis of the resulting changes in the loss landscape. Extensive
experiments on the ImageNet dataset, using a variety of model architectures,
demonstrate that perturbations generated from aligned source models exhibit
significantly higher transferability than those from the original source model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18496">Accurate Segmentation of Optic Disc And Cup from Multiple Pseudo-labels by Noise-Aware Learning. (arXiv:2311.18496v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1">Tengjin Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhidong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhiming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a></p>
<p>Optic disc and cup segmentation play a crucial role in automating the
screening and diagnosis of optic glaucoma. While data-driven convolutional
neural networks (CNNs) show promise in this area, the inherent ambiguity of
segmenting object and background boundaries in the task of optic disc and cup
segmentation leads to noisy annotations that impact model performance. To
address this, we propose an innovative label-denoising method of Multiple
Pseudo-labels Noise-aware Network (MPNN) for accurate optic disc and cup
segmentation. Specifically, the Multiple Pseudo-labels Generation and Guided
Denoising (MPGGD) module generates pseudo-labels by multiple different
initialization networks trained on true labels, and the pixel-level consensus
information extracted from these pseudo-labels guides to differentiate clean
pixels from noisy pixels. The training framework of the MPNN is constructed by
a teacher-student architecture to learn segmentation from clean pixels and
noisy pixels. Particularly, such a framework adeptly leverages (i) reliable and
fundamental insights from clean pixels and (ii) the supplementary knowledge
within noisy pixels via multiple perturbation-based unsupervised consistency.
Compared to other label-denoising methods, comprehensive experimental results
on the RIGA dataset demonstrate our method's excellent performance and
significant denoising ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18508">DifAugGAN: A Practical Diffusion-style Data Augmentation for GAN-based Single Image Super-resolution. (arXiv:2311.18508v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Niu_A/0/1/0/all/0/1">Axi Niu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1">Kang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tee_J/0/1/0/all/0/1">Joshua Tian Jin Tee</a>, <a href="http://arxiv.org/find/eess/1/au:+Pham_T/0/1/0/all/0/1">Trung X. Pham</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1">Jinqiu Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a>, <a href="http://arxiv.org/find/eess/1/au:+Kweon_I/0/1/0/all/0/1">In So Kweon</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yanning Zhang</a></p>
<p>It is well known the adversarial optimization of GAN-based image
super-resolution (SR) methods makes the preceding SR model generate unpleasant
and undesirable artifacts, leading to large distortion. We attribute the cause
of such distortions to the poor calibration of the discriminator, which hampers
its ability to provide meaningful feedback to the generator for learning
high-quality images. To address this problem, we propose a simple but
non-travel diffusion-style data augmentation scheme for current GAN-based SR
methods, known as DifAugGAN. It involves adapting the diffusion process in
generative diffusion models for improving the calibration of the discriminator
during training motivated by the successes of data augmentation schemes in the
field to achieve good calibration. Our DifAugGAN can be a Plug-and-Play
strategy for current GAN-based SISR methods to improve the calibration of the
discriminator and thus improve SR performance. Extensive experimental
evaluations demonstrate the superiority of DifAugGAN over state-of-the-art
GAN-based SISR methods across both synthetic and real-world datasets,
showcasing notable advancements in both qualitative and quantitative results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18512">Revisiting Proposal-based Object Detection. (arXiv:2311.18512v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1">Aritra Bhowmik</a>, <a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1">Martin R. Oswald</a>, <a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1">Pascal Mettes</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G. M. Snoek</a></p>
<p>This paper revisits the pipeline for detecting objects in images with
proposals. For any object detector, the obtained box proposals or queries need
to be classified and regressed towards ground truth boxes. The common solution
for the final predictions is to directly maximize the overlap between each
proposal and the ground truth box, followed by a winner-takes-all ranking or
non-maximum suppression. In this work, we propose a simple yet effective
alternative. For proposal regression, we solve a simpler problem where we
regress to the area of intersection between proposal and ground truth. In this
way, each proposal only specifies which part contains the object, avoiding a
blind inpainting problem where proposals need to be regressed beyond their
visual scope. In turn, we replace the winner-takes-all strategy and obtain the
final prediction by taking the union over the regressed intersections of a
proposal group surrounding an object. Our revisited approach comes with minimal
changes to the detection pipeline and can be plugged into any existing method.
We show that our approach directly improves canonical object detection and
instance segmentation architectures, highlighting the utility of
intersection-based regression and grouping.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18518">Color-Emotion Associations in Art: Fuzzy Approach. (arXiv:2311.18518v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shamoi_P/0/1/0/all/0/1">Pakizar Shamoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Muratbekova_M/0/1/0/all/0/1">Muragul Muratbekova</a></p>
<p>Art objects can evoke certain emotions. Color is a fundamental element of
visual art and plays a significant role in how art is perceived. This paper
introduces a novel approach to classifying emotions in art using Fuzzy Sets. We
employ a fuzzy approach because it aligns well with human judgments' imprecise
and subjective nature. Extensive fuzzy colors (n=120) and a broad emotional
spectrum (n=10) allow for a more human-consistent and context-aware exploration
of emotions inherent in paintings. First, we introduce the fuzzy color
representation model. Then, at the fuzzification stage, we process the Wiki Art
Dataset of paintings tagged with emotions, extracting fuzzy dominant colors
linked to specific emotions. This results in fuzzy color distributions for ten
emotions. Finally, we convert them back to a crisp domain, obtaining a
knowledge base of color-emotion associations in primary colors. Our findings
reveal strong associations between specific emotions and colors; for instance,
gratitude strongly correlates with green, brown, and orange. Other noteworthy
associations include brown and anger, orange with shame, yellow with happiness,
and gray with fear. Using these associations and Jaccard similarity, we can
find the emotions in the arbitrary untagged image. We conducted a 2AFC
experiment involving human subjects to evaluate the proposed method. The
average hit rate of 0.77 indicates a significant correlation between the
method's predictions and human perception. The proposed method is simple to
adapt to art painting retrieval systems. The study contributes to the
theoretical understanding of color-emotion associations in art, offering
valuable insights for various practical applications besides art, like
marketing, design, and psychology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18531">Dataset Distillation via the Wasserstein Metric. (arXiv:2311.18531v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1">Tiancheng Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Luwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalal_V/0/1/0/all/0/1">Vibhu Dalal</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingrui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a></p>
<p>Dataset distillation (DD) offers a compelling approach in computer vision,
with the goal of condensing extensive datasets into smaller synthetic versions
without sacrificing much of the model performance. In this paper, we continue
to study the methods for DD, by addressing its conceptually core objective: how
to capture the essential representation of extensive datasets in smaller,
synthetic forms.
</p>
<p>We propose a novel approach utilizing the Wasserstein distance, a metric
rooted in optimal transport theory, to enhance distribution matching in DD. Our
method leverages the Wasserstein barycenter, offering a geometrically
meaningful way to quantify distribution differences and effectively capture the
centroid of a set of distributions. Our approach retains the computational
benefits of distribution matching-based methods while achieving new
state-of-the-art performance on several benchmarks.
</p>
<p>To provide useful prior for learning the images, we embed the synthetic data
into the feature space of pretrained classification models to conduct
distribution matching. Extensive testing on various high-resolution datasets
confirms the effectiveness and adaptability of our method, indicating the
promising yet unexplored capabilities of Wasserstein metrics in dataset
distillation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18537">MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation. (arXiv:2311.18537v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Ju He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qihang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1">Inkyu Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xueqing Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaohui Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang-Chieh Chen</a></p>
<p>Video panoptic segmentation requires consistently segmenting (for both
`thing' and `stuff' classes) and tracking objects in a video over time. In this
work, we present MaXTron, a general framework that exploits Mask XFormer with
Trajectory Attention to tackle the task. MaXTron enriches an off-the-shelf mask
transformer by leveraging trajectory attention. The deployed mask transformer
takes as input a short clip consisting of only a few frames and predicts the
clip-level segmentation. To enhance the temporal consistency, MaXTron employs
within-clip and cross-clip tracking modules, efficiently utilizing trajectory
attention. Originally designed for video classification, trajectory attention
learns to model the temporal correspondences between neighboring frames and
aggregates information along the estimated motion paths. However, it is
nontrivial to directly extend trajectory attention to the per-pixel dense
prediction tasks due to its quadratic dependency on input size. To alleviate
the issue, we propose to adapt the trajectory attention for both the dense
pixel features and object queries, aiming to improve the short-term and
long-term tracking results, respectively. Particularly, in our within-clip
tracking module, we propose axial-trajectory attention that effectively
computes the trajectory attention for tracking dense pixels sequentially along
the height- and width-axes. The axial decomposition significantly reduces the
computational complexity for dense pixel features. In our cross-clip tracking
module, since the object queries in mask transformer are learned to encode the
object information, we are able to capture the long-term temporal connections
by applying trajectory attention to object queries, which learns to track each
object across different clips. Without bells and whistles, MaXTron demonstrates
state-of-the-art performances on video segmentation benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18540">Match me if you can: Semantic Correspondence Learning with Unpaired Images. (arXiv:2311.18540v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jiwon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1">Byeongho Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Sangdoo Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungryong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dongyoon Han</a></p>
<p>Recent approaches for semantic correspondence have focused on obtaining
high-quality correspondences using a complicated network, refining the
ambiguous or noisy matching points. Despite their performance improvements,
they remain constrained by the limited training pairs due to costly point-level
annotations. This paper proposes a simple yet effective method that performs
training with unlabeled pairs to complement both limited image pairs and sparse
point pairs, requiring neither extra labeled keypoints nor trainable modules.
We fundamentally extend the data quantity and variety by augmenting new
unannotated pairs not primitively provided as training pairs in benchmarks.
Using a simple teacher-student framework, we offer reliable pseudo
correspondences to the student network via machine supervision. Finally, the
performance of our network is steadily improved by the proposed iterative
training, putting back the student as a teacher to generate refined labels and
train a new student repeatedly. Our models outperform the milestone baselines,
including state-of-the-art methods on semantic correspondence benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18553">Heterogeneous Graph-based Trajectory Prediction using Local Map Context and Social Interactions. (arXiv:2311.18553v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grimm_D/0/1/0/all/0/1">Daniel Grimm</a>, <a href="http://arxiv.org/find/cs/1/au:+Zipfl_M/0/1/0/all/0/1">Maximilian Zipfl</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertlein_F/0/1/0/all/0/1">Felix Hertlein</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1">Alexander Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Luttin_J/0/1/0/all/0/1">J&#xfc;rgen L&#xfc;ttin</a>, <a href="http://arxiv.org/find/cs/1/au:+Thoma_S/0/1/0/all/0/1">Steffen Thoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1">Stefan Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1">Lavdim Halilaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1">Achim Rettinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1">J. Marius Z&#xf6;llner</a></p>
<p>Precisely predicting the future trajectories of surrounding traffic
participants is a crucial but challenging problem in autonomous driving, due to
complex interactions between traffic agents, map context and traffic rules.
Vector-based approaches have recently shown to achieve among the best
performances on trajectory prediction benchmarks. These methods model simple
interactions between traffic agents but don't distinguish between relation-type
and attributes like their distance along the road. Furthermore, they represent
lanes only by sequences of vectors representing center lines and ignore context
information like lane dividers and other road elements. We present a novel
approach for vector-based trajectory prediction that addresses these
shortcomings by leveraging three crucial sources of information: First, we
model interactions between traffic agents by a semantic scene graph, that
accounts for the nature and important features of their relation. Second, we
extract agent-centric image-based map features to model the local map context.
Finally, we generate anchor paths to enforce the policy in multi-modal
prediction to permitted trajectories only. Each of these three enhancements
shows advantages over the baseline model HoliGraph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18559">FediOS: Decoupling Orthogonal Subspaces for Personalization in Feature-skew Federated Learning. (arXiv:2311.18559v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lingzhi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zexi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chao Wu</a></p>
<p>Personalized federated learning (pFL) enables collaborative training among
multiple clients to enhance the capability of customized local models. In pFL,
clients may have heterogeneous (also known as non-IID) data, which poses a key
challenge in how to decouple the data knowledge into generic knowledge for
global sharing and personalized knowledge for preserving local personalization.
A typical way of pFL focuses on label distribution skew, and they adopt a
decoupling scheme where the model is split into a common feature extractor and
two prediction heads (generic and personalized). However, such a decoupling
scheme cannot solve the essential problem of feature skew heterogeneity,
because a common feature extractor cannot decouple the generic and personalized
features. Therefore, in this paper, we rethink the architecture decoupling
design for feature-skew pFL and propose an effective pFL method called FediOS.
In FediOS, we reformulate the decoupling into two feature extractors (generic
and personalized) and one shared prediction head. Orthogonal projections are
used for clients to map the generic features into one common subspace and
scatter the personalized features into different subspaces to achieve
decoupling for them. In addition, a shared prediction head is trained to
balance the importance of generic and personalized features during inference.
Extensive experiments on four vision datasets demonstrate our method reaches
state-of-the-art pFL performances under feature skew heterogeneity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18561">Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering. (arXiv:2311.18561v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yurui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1">Chun Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junzhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a></p>
<p>Modeling dynamic, large-scale urban scenes is challenging due to their highly
intricate geometric structures and unconstrained dynamics in both space and
time. Prior methods often employ high-level architectural priors, separating
static and dynamic elements, resulting in suboptimal capture of their
synergistic interactions. To address this challenge, we present a unified
representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon
the efficient 3D Gaussian splatting technique, originally designed for static
scene representation, by introducing periodic vibration-based temporal
dynamics. This innovation enables PVG to elegantly and uniformly represent the
characteristics of various objects and elements in dynamic urban scenes. To
enhance temporally coherent representation learning with sparse training data,
we introduce a novel flow-based temporal smoothing mechanism and a
position-aware adaptive control strategy. Extensive experiments on Waymo Open
Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art
alternatives in both reconstruction and novel view synthesis for both dynamic
and static scenes. Notably, PVG achieves this without relying on manually
labeled object bounding boxes or expensive optical flow estimation. Moreover,
PVG exhibits 50/6000-fold acceleration in training/rendering over the best
alternative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18564">Seam-guided local alignment and stitching for large parallax images. (arXiv:2311.18564v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1">Tianli Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Heling Cao</a></p>
<p>Seam-cutting methods have been proven effective in the composition step of
image stitching, especially for images with parallax. However, the
effectiveness of seam-cutting usually depends on that images can be roughly
aligned such that there exists a local region where a plausible seam can be
found. For images with large parallax, current alignment methods often fall
short of expectations. In this paper, we propose a local alignment and
stitching method guided by seam quality evaluation. First, we use existing
image alignment and seam-cutting methods to calculate an initial seam and
evaluate the quality of pixels along the seam. Then, for pixels with low
qualities, we separate their enclosing patches in the aligned images and
locally align them by extracting modified dense correspondences via SIFT flow.
Finally, we composite the aligned patches via seam-cutting and merge them into
the original aligned result to generate the final mosaic. Experiments show that
compared with the state-of-the-art seam-cutting methods, our result is more
plausible and with fewer artifacts. The code will be available at
https://github.com/tlliao/Seam-guided-local-alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18572">Overcoming Label Noise for Source-free Unsupervised Video Domain Adaptation. (arXiv:2311.18572v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Avijit Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1">C. V. Jawahar</a>, <a href="http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1">Karteek Alahari</a></p>
<p>Despite the progress seen in classification methods, current approaches for
handling videos with distribution shifts in source and target domains remain
source-dependent as they require access to the source data during the
adaptation stage. In this paper, we present a self-training based source-free
video domain adaptation approach to address this challenge by bridging the gap
between the source and the target domains. We use the source pre-trained model
to generate pseudo-labels for the target domain samples, which are inevitably
noisy. Thus, we treat the problem of source-free video domain adaptation as
learning from noisy labels and argue that the samples with correct
pseudo-labels can help us in adaptation. To this end, we leverage the
cross-entropy loss as an indicator of the correctness of the pseudo-labels and
use the resulting small-loss samples from the target domain for fine-tuning the
model. We further enhance the adaptation performance by implementing a
teacher-student framework, in which the teacher, which is updated gradually,
produces reliable pseudo-labels. Meanwhile, the student undergoes fine-tuning
on the target domain videos using these generated pseudo-labels to improve its
performance. Extensive experimental evaluations show that our methods, termed
as CleanAdapt, CleanAdapt + TS, achieve state-of-the-art results, outperforming
the existing approaches on various open datasets. Our source code is publicly
available at https://avijit9.github.io/CleanAdapt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18576">Fingerprint Matching with Localized Deep Representation. (arXiv:2311.18576v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yongjie Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zhiyu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianjiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Compared to minutia-based fingerprint representations, fixed-length
representations are attractive due to simple and efficient matching. However,
fixed-length fingerprint representations are limited in accuracy when matching
fingerprints with different visible areas, which can occur due to different
finger poses or acquisition methods. To address this issue, we propose a
localized deep representation of fingerprint, named LDRF. By focusing on the
discriminative characteristics within local regions, LDRF provides a more
robust and accurate fixed-length representation for fingerprints with variable
visible areas. LDRF can be adapted to retain information within any valid area,
making it highly flexible. The matching scores produced by LDRF also exhibit
intuitive statistical characteristics, which led us to propose a matching score
normalization technique to mitigate the uncertainty in the cases of very small
overlapping area. With this new technique, we can maintain a high level of
accuracy and reliability in our fingerprint matching, even as the size of the
database grows rapidly. Our experimental results on 21 datasets containing over
140K fingerprints of various finger poses and impression types show that LDRF
outperforms other fixed-length representations and is robust to sensing
technologies and impression types. Besides, the proposed matching score
normalization effectively reduces the false match rate (FMR) in large-scale
identification experiments comprising over 5.11 million fingerprints.
Specifically, this technique results in a reduction of two orders of magnitude
compared to matching without matching score normalization and five orders of
magnitude compared to prior works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18578">Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum. (arXiv:2311.18578v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaccone_R/0/1/0/all/0/1">Riccardo Zaccone</a>, <a href="http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1">Carlo Masone</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1">Marco Ciccone</a></p>
<p>Federated Learning (FL) is the state-of-the-art approach for learning from
decentralized data in privacy-constrained scenarios. As the current literature
reports, the main problems associated with FL refer to system and statistical
challenges: the former ones demand for efficient learning from edge devices,
including lowering communication bandwidth and frequency, while the latter
require algorithms robust to non-iidness. State-of-art approaches either
guarantee convergence at increased communication cost or are not sufficiently
robust to handle extreme heterogeneous local distributions. In this work we
propose a novel generalization of the heavy-ball momentum, and present FedHBM
to effectively address statistical heterogeneity in FL without introducing any
communication overhead. We conduct extensive experimentation on common FL
vision and NLP datasets, showing that our FedHBM algorithm empirically yields
better model quality and higher convergence speed w.r.t. the state-of-art,
especially in pathological non-iid scenarios. While being designed for
cross-silo settings, we show how FedHBM is applicable in moderate-to-high
cross-device scenarios, and how good model initializations (e.g. pre-training)
can be exploited for prompt acceleration. Extended experimentation on
large-scale real-world federated datasets further corroborates the
effectiveness of our approach for real-world FL applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18592">Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models. (arXiv:2311.18592v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiandong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yanlin Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yaoyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Pattern recognition through the fusion of RGB frames and Event streams has
emerged as a novel research area in recent years. Current methods typically
employ backbone networks to individually extract the features of RGB frames and
event streams, and subsequently fuse these features for pattern recognition.
However, we posit that these methods may suffer from key issues like sematic
gaps and small-scale backbone networks. In this study, we introduce a novel
pattern recognition framework that consolidates the semantic labels, RGB
frames, and event streams, leveraging pre-trained large-scale vision-language
models. Specifically, given the input RGB frames, event streams, and all the
predefined semantic labels, we employ a pre-trained large-scale vision model
(CLIP vision encoder) to extract the RGB and event features. To handle the
semantic labels, we initially convert them into language descriptions through
prompt engineering, and then obtain the semantic features using the pre-trained
large-scale language model (CLIP text encoder). Subsequently, we integrate the
RGB/Event features and semantic features using multimodal Transformer networks.
The resulting frame and event tokens are further amplified using self-attention
layers. Concurrently, we propose to enhance the interactions between text
tokens and RGB/Event tokens via cross-attention. Finally, we consolidate all
three modalities using self-attention and feed-forward layers for recognition.
Comprehensive experiments on the HARDVS and PokerEvent datasets fully
substantiate the efficacy of our proposed SAFE model. The source code will be
made available at https://github.com/Event-AHU/SAFE_LargeVLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18605">Learning Triangular Distribution in Visual World. (arXiv:2311.18605v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Ping Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chengtao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">Dichao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1">Peng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yanlin Qian</a></p>
<p>Convolution neural network is successful in pervasive vision tasks, including
label distribution learning, which usually takes the form of learning an
injection from the non-linear visual features to the well-defined labels.
However, how the discrepancy between features is mapped to the label
discrepancy is ambient, and its correctness is not guaranteed.To address these
problems, we study the mathematical connection between feature and its label,
presenting a general and simple framework for label distribution learning. We
propose a so-called Triangular Distribution Transform (TDT) to build an
injective function between feature and label, guaranteeing that any symmetric
feature discrepancy linearly reflects the difference between labels. The
proposed TDT can be used as a plug-in in mainstream backbone networks to
address different label distribution learning tasks. Experiments on Facial Age
Recognition, Illumination Chromaticity Estimation, and Aesthetics assessment
show that TDT achieves on-par or better results than the prior arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18608">Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing. (arXiv:2311.18608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Hyelin Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1">Gihyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1">Geon Yeong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>With the remarkable advent of text-to-image diffusion models, image editing
methods have become more diverse and continue to evolve. A promising recent
approach in this realm is Delta Denoising Score (DDS) - an image editing
technique based on Score Distillation Sampling (SDS) framework that leverages
the rich generative prior of text-to-image diffusion models. However, relying
solely on the difference between scoring functions is insufficient for
preserving specific structural elements from the original image, a crucial
aspect of image editing. Inspired by the similarity and importance differences
between DDS and the contrastive learning for unpaired image-to-image
translation (CUT), here we present an embarrassingly simple yet very powerful
modification of DDS, called Contrastive Denoising Score (CDS), for latent
diffusion models (LDM). Specifically, to enforce structural correspondence
between the input and output while maintaining the controllability of contents,
we introduce a straightforward approach to regulate structural consistency
using CUT loss within the DDS framework. To calculate this loss, instead of
employing auxiliary networks, we utilize the intermediate features of LDM, in
particular, those from the self-attention layers, which possesses rich spatial
information. Our approach enables zero-shot image-to-image translation and
neural radiance field (NeRF) editing, achieving a well-balanced interplay
between maintaining the structural details and transforming content.
Qualitative results and comparisons demonstrates the effectiveness of our
proposed method. Project page with code is available at
https://hyelinnam.github.io/CDS/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18610">DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and Alignment from an RGB Image. (arXiv:2311.18610v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1">Daoyi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rozenberszki_D/0/1/0/all/0/1">D&#xe1;vid Rozenberszki</a>, <a href="http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1">Stefan Leutenegger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Angela Dai</a></p>
<p>Perceiving 3D structures from RGB images based on CAD model primitives can
enable an effective, efficient 3D object-based representation of scenes.
However, current approaches rely on supervision from expensive annotations of
CAD models associated with real images, and encounter challenges due to the
inherent ambiguities in the task -- both in depth-scale ambiguity in monocular
perception, as well as inexact matches of CAD database models to real
observations. We thus propose DiffCAD, the first weakly-supervised
probabilistic approach to CAD retrieval and alignment from an RGB image. We
formulate this as a conditional generative task, leveraging diffusion to learn
implicit probabilistic models capturing the shape, pose, and scale of CAD
objects in an image. This enables multi-hypothesis generation of different
plausible CAD reconstructions, requiring only a few hypotheses to characterize
ambiguities in depth/scale and inexact shape matches. Our approach is trained
only on synthetic data, leveraging monocular depth and mask estimates to enable
robust zero-shot adaptation to various real target domains. Despite being
trained solely on synthetic data, our multi-hypothesis approach can even
surpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8
hypotheses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18612">Cancer-Net PCa-Gen: Synthesis of Realistic Prostate Diffusion Weighted Imaging Data via Anatomic-Conditional Controlled Latent Diffusion. (arXiv:2311.18612v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sridhar_A/0/1/0/all/0/1">Aditya Sridhar</a>, <a href="http://arxiv.org/find/eess/1/au:+Tai_C/0/1/0/all/0/1">Chi-en Amy Tai</a>, <a href="http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1">Hayden Gunraj</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1">Alexander Wong</a></p>
<p>In Canada, prostate cancer is the most common form of cancer in men and
accounted for 20% of new cancer cases for this demographic in 2022. Due to
recent successes in leveraging machine learning for clinical decision support,
there has been significant interest in the development of deep neural networks
for prostate cancer diagnosis, prognosis, and treatment planning using
diffusion weighted imaging (DWI) data. A major challenge hindering widespread
adoption in clinical use is poor generalization of such networks due to
scarcity of large-scale, diverse, balanced prostate imaging datasets for
training such networks. In this study, we explore the efficacy of latent
diffusion for generating realistic prostate DWI data through the introduction
of an anatomic-conditional controlled latent diffusion strategy. To the best of
the authors' knowledge, this is the first study to leverage conditioning for
synthesis of prostate cancer imaging. Experimental results show that the
proposed strategy, which we call Cancer-Net PCa-Gen, enhances synthesis of
diverse prostate images through controllable tumour locations and better
anatomical and textural fidelity. These crucial features make it well-suited
for augmenting real patient data, enabling neural networks to be trained on a
more diverse and comprehensive data distribution. The Cancer-Net PCa-Gen
framework and sample images have been made publicly available at
https://www.kaggle.com/datasets/deetsadi/cancer-net-pca-gen-dataset as a part
of a global open-source initiative dedicated to accelerating advancement in
machine learning to aid clinicians in the fight against cancer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18614">Anatomy and Physiology of Artificial Intelligence in PET Imaging. (arXiv:2311.18614v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bradshaw_T/0/1/0/all/0/1">Tyler J. Bradshaw</a>, <a href="http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1">Alan B. McMillan</a></p>
<p>The influence of artificial intelligence (AI) within the field of nuclear
medicine has been rapidly growing. Many researchers and clinicians are seeking
to apply AI within PET, and clinicians will soon find themselves engaging with
AI-based applications all along the chain of molecular imaging, from image
reconstruction to enhanced reporting. This expanding presence of AI in PET
imaging will result in greater demand for educational resources for those
unfamiliar with AI. The objective of this article to is provide an illustrated
guide to the core principles of modern AI, with specific focus on aspects that
are most likely to be encountered in PET imaging. We describe convolutional
neural networks, algorithm training, and explain the components of the commonly
used U-Net for segmentation and image synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18618">JPPF: Multi-task Fusion for Consistent Panoptic-Part Segmentation. (arXiv:2311.18618v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muralidhara_S/0/1/0/all/0/1">Shishir Muralidhara</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagadeesh_S/0/1/0/all/0/1">Sravan Kumar Jagadeesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1">Ren&#xe9; Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1">Didier Stricker</a></p>
<p>Part-aware panoptic segmentation is a problem of computer vision that aims to
provide a semantic understanding of the scene at multiple levels of
granularity. More precisely, semantic areas, object instances, and semantic
parts are predicted simultaneously. In this paper, we present our Joint
Panoptic Part Fusion (JPPF) that combines the three individual segmentations
effectively to obtain a panoptic-part segmentation. Two aspects are of utmost
importance for this: First, a unified model for the three problems is desired
that allows for mutually improved and consistent representation learning.
Second, balancing the combination so that it gives equal importance to all
individual results during fusion. Our proposed JPPF is parameter-free and
dynamically balances its input. The method is evaluated and compared on the
Cityscapes Panoptic Parts (CPP) and Pascal Panoptic Parts (PPP) datasets in
terms of PartPQ and Part-Whole Quality (PWQ). In extensive experiments, we
verify the importance of our fair fusion, highlight its most significant impact
for areas that can be further segmented into parts, and demonstrate the
generalization capabilities of our design without fine-tuning on 5 additional
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18628">A Lightweight Clustering Framework for Unsupervised Semantic Segmentation. (arXiv:2311.18628v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1">Yau Shing Jonathan Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lihe Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a></p>
<p>Unsupervised semantic segmentation aims to label each pixel of an image to a
corresponding class without the use of annotated data. It is a widely
researched area as obtaining labeled datasets are expensive. While previous
works in the field demonstrated a gradual improvement in segmentation
performance, most of them required neural network training. This made
segmentation equally expensive, especially when dealing with large-scale
datasets. We thereby propose a lightweight clustering framework for
unsupervised semantic segmentation. Attention features of the self-supervised
vision transformer exhibit strong foreground-background differentiability. By
clustering these features into a small number of clusters, we could separate
foreground and background image patches into distinct groupings. In our
clustering framework, we first obtain attention features from the
self-supervised vision transformer. Then we extract Dataset-level,
Category-level and Image-level masks by clustering features within the same
dataset, category and image. We further ensure multilevel clustering
consistency across the three levels and this allows us to extract patch-level
binary pseudo-masks. Finally, the pseudo-mask is upsampled, refined and class
assignment is performed according to the CLS token of object regions. Our
framework demonstrates great promise in unsupervised semantic segmentation and
achieves state-of-the-art results on PASCAL VOC and MS COCO datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18630">SATHUR: Self Augmenting Task Hallucinal Unified Representation for Generalized Class Incremental Learning. (arXiv:2311.18630v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kanagarajah_S/0/1/0/all/0/1">Sathursan Kanagarajah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambegoda_T/0/1/0/all/0/1">Thanuja Ambegoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1">Ranga Rodrigo</a></p>
<p>Class Incremental Learning (CIL) is inspired by the human ability to learn
new classes without forgetting previous ones. CIL becomes more challenging in
real-world scenarios when the samples in each incremental step are imbalanced.
This creates another branch of problem, called Generalized Class Incremental
Learning (GCIL) where each incremental step is structured more realistically.
Grow When Required (GWR) network, a type of Self-Organizing Map (SOM),
dynamically create and remove nodes and edges for adaptive learning. GWR
performs incremental learning from feature vectors extracted by a Convolutional
Neural Network (CNN), which acts as a feature extractor. The inherent ability
of GWR to form distinct clusters, each corresponding to a class in the feature
vector space, regardless of the order of samples or class imbalances, is well
suited to achieving GCIL. To enhance GWR's classification performance, a
high-quality feature extractor is required. However, when the convolutional
layers are adapted at each incremental step, the GWR nodes corresponding to
prior knowledge are subject to near-invalidation. This work introduces the Self
Augmenting Task Hallucinal Unified Representation (SATHUR), which
re-initializes the GWR network at each incremental step, aligning it with the
current feature extractor. Comprehensive experimental results demonstrate that
our proposed method significantly outperforms other state-of-the-art GCIL
methods on CIFAR-100 and CORe50 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18635">DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars. (arXiv:2311.18635v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirschstein_T/0/1/0/all/0/1">Tobias Kirschstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Giebenhain_S/0/1/0/all/0/1">Simon Giebenhain</a>, <a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1">Matthias Nie&#xdf;ner</a></p>
<p>DiffusionAvatars synthesizes a high-fidelity 3D head avatar of a person,
offering intuitive control over both pose and expression. We propose a
diffusion-based neural renderer that leverages generic 2D priors to produce
compelling images of faces. For coarse guidance of the expression and head
pose, we render a neural parametric head model (NPHM) from the target
viewpoint, which acts as a proxy geometry of the person. Additionally, to
enhance the modeling of intricate facial expressions, we condition
DiffusionAvatars directly on the expression codes obtained from NPHM via
cross-attention. Finally, to synthesize consistent surface details across
different viewpoints and expressions, we rig learnable spatial features to the
head's surface via TriPlane lookup in NPHM's canonical space. We train
DiffusionAvatars on RGB videos and corresponding tracked NPHM meshes of a
person and test the obtained avatars in both self-reenactment and animation
scenarios. Our experiments demonstrate that DiffusionAvatars generates
temporally consistent and visually appealing videos for novel poses and
expressions of a person, outperforming existing approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18645">Stochastic Vision Transformers with Wasserstein Distance-Aware Attention. (arXiv:2311.18645v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erick_F/0/1/0/all/0/1">Franciskus Xaverius Erick</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1">Mina Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1">Johanna Paula M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1">Bernhard Kainz</a></p>
<p>Self-supervised learning is one of the most promising approaches to acquiring
knowledge from limited labeled data. Despite the substantial advancements made
in recent years, self-supervised models have posed a challenge to
practitioners, as they do not readily provide insight into the model's
confidence and uncertainty. Tackling this issue is no simple feat, primarily
due to the complexity involved in implementing techniques that can make use of
the latent representations learned during pre-training without relying on
explicit labels. Motivated by this, we introduce a new stochastic vision
transformer that integrates uncertainty and distance awareness into
self-supervised learning (SSL) pipelines. Instead of the conventional
deterministic vector embedding, our novel stochastic vision transformer encodes
image patches into elliptical Gaussian distributional embeddings. Notably, the
attention matrices of these stochastic representational embeddings are computed
using Wasserstein distance-based attention, effectively capitalizing on the
distributional nature of these embeddings. Additionally, we propose a
regularization term based on Wasserstein distance for both pre-training and
fine-tuning processes, thereby incorporating distance awareness into latent
representations. We perform extensive experiments across different tasks such
as in-distribution generalization, out-of-distribution detection, dataset
corruption, semi-supervised settings, and transfer learning to other datasets
and tasks. Our proposed method achieves superior accuracy and calibration,
surpassing the self-supervised baseline in a wide range of experiments on a
variety of datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18649">Simple Semantic-Aided Few-Shot Learning. (arXiv:2311.18649v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Junzhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shanlin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhenan He</a></p>
<p>Learning from a limited amount of data, namely Few-Shot Learning, stands out
as a challenging computer vision task. Several works exploit semantics and
design complicated semantic fusion mechanisms to compensate for rare
representative features within restricted data. However, relying on naive
semantics such as class names introduces biases due to their brevity, while
acquiring extensive semantics from external knowledge takes a huge time and
effort. This limitation severely constrains the potential of semantics in
few-shot learning. In this paper, we design an automatic way called Semantic
Evolution to generate high-quality semantics. The incorporation of high-quality
semantics alleviates the need for complex network structures and learning
algorithms used in previous works. Hence, we employ a simple two-layer network
termed Semantic Alignment Network to transform semantics and visual features
into robust class prototypes with rich discriminative features for few-shot
classification. The experimental results show our framework outperforms all
previous methods on five benchmarks, demonstrating a simple network with
high-quality semantics can beat intricate multi-modal modules on few-shot
classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18651">LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning. (arXiv:2311.18651v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sijin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1">Hao Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongyuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiayuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a></p>
<p>Recent advances in Large Multimodal Models (LMM) have made it possible for
various applications in human-machine interactions. However, developing LMMs
that can comprehend, reason, and plan in complex and diverse 3D environments
remains a challenging topic, especially considering the demand for
understanding permutation-invariant point cloud 3D representations of the 3D
scene. Existing works seek help from multi-view images, and project 2D features
to 3D space as 3D scene representations. This, however, leads to huge
computational overhead and performance degradation. In this paper, we present
LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and
respond to both textual-instructions and visual-prompts. This help LMMs better
comprehend human interactions and further help to remove the ambiguities in
cluttered 3D scenes. Experiments show that LL3DA achieves remarkable results,
and surpasses various 3D vision-language models on both 3D Dense Captioning and
3D Question Answering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18654">Detailed Human-Centric Text Description-Driven Large Scene Synthesis. (arXiv:2311.18654v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gwanghyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Dong Un Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1">Hoigi Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hayeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1">Se Young Chun</a></p>
<p>Text-driven large scene image synthesis has made significant progress with
diffusion models, but controlling it is challenging. While using additional
spatial controls with corresponding texts has improved the controllability of
large scene synthesis, it is still challenging to faithfully reflect detailed
text descriptions without user-provided controls. Here, we propose
DetText2Scene, a novel text-driven large-scale image synthesis with high
faithfulness, controllability, and naturalness in a global context for the
detailed human-centric text description. Our DetText2Scene consists of 1)
hierarchical keypoint-box layout generation from the detailed description by
leveraging large language model (LLM), 2) view-wise conditioned joint diffusion
process to synthesize a large scene from the given detailed text with
LLM-generated grounded keypoint-box layout and 3) pixel perturbation-based
pyramidal interpolation to progressively refine the large scene for global
coherence. Our DetText2Scene significantly outperforms prior arts in
text-to-large scene synthesis qualitatively and quantitatively, demonstrating
strong faithfulness with detailed descriptions, superior controllability, and
excellent naturalness in a global context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18661">Learning Part Segmentation from Synthetic Animals. (arXiv:2311.18661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jiawei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Ju He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaushik_P/0/1/0/all/0/1">Prakhar Kaushik</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zihao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1">Jiteng Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a></p>
<p>Semantic part segmentation provides an intricate and interpretable
understanding of an object, thereby benefiting numerous downstream tasks.
However, the need for exhaustive annotations impedes its usage across diverse
object types. This paper focuses on learning part segmentation from synthetic
animals, leveraging the Skinned Multi-Animal Linear (SMAL) models to scale up
existing synthetic data generated by computer-aided design (CAD) animal models.
Compared to CAD models, SMAL models generate data with a wider range of poses
observed in real-world scenarios. As a result, our first contribution is to
construct a synthetic animal dataset of tigers and horses with more pose
diversity, termed Synthetic Animal Parts (SAP). We then benchmark Syn-to-Real
animal part segmentation from SAP to PartImageNet, namely SynRealPart, with
existing semantic segmentation domain adaptation methods and further improve
them as our second contribution. Concretely, we examine three Syn-to-Real
adaptation methods but observe relative performance drop due to the innate
difference between the two tasks. To address this, we propose a simple yet
effective method called Class-Balanced Fourier Data Mixing (CB-FDM). Fourier
Data Mixing aligns the spectral amplitudes of synthetic images with real
images, thereby making the mixed images have more similar frequency content to
real images. We further use Class-Balanced Pseudo-Label Re-Weighting to
alleviate the imbalanced class distribution. We demonstrate the efficacy of
CB-FDM on SynRealPart over previous methods with significant performance
improvements. Remarkably, our third contribution is to reveal that the learned
parts from synthetic tiger and horse are transferable across all quadrupeds in
PartImageNet, further underscoring the utility and potential applications of
animal part segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18664">Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy. (arXiv:2311.18664v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Solano_P/0/1/0/all/0/1">Pedro Esteban Chavarrias Solano</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulpitt_A/0/1/0/all/0/1">Andrew Bulpitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1">Venkataraman Subramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Sharib Ali</a></p>
<p>Colonoscopy screening is the gold standard procedure for assessing
abnormalities in the colon and rectum, such as ulcers and cancerous polyps.
Measuring the abnormal mucosal area and its 3D reconstruction can help quantify
the surveyed area and objectively evaluate disease burden. However, due to the
complex topology of these organs and variable physical conditions, for example,
lighting, large homogeneous texture, and image modality estimating distance
from the camera aka depth) is highly challenging. Moreover, most colonoscopic
video acquisition is monocular, making the depth estimation a non-trivial
problem. While methods in computer vision for depth estimation have been
proposed and advanced on natural scene datasets, the efficacy of these
techniques has not been widely quantified on colonoscopy datasets. As the
colonic mucosa has several low-texture regions that are not well pronounced,
learning representations from an auxiliary task can improve salient feature
extraction, allowing estimation of accurate camera depths. In this work, we
propose to develop a novel multi-task learning (MTL) approach with a shared
encoder and two decoders, namely a surface normal decoder and a depth estimator
decoder. Our depth estimator incorporates attention mechanisms to enhance
global context awareness. We leverage the surface normal prediction to improve
geometric feature extraction. Also, we apply a cross-task consistency loss
among the two geometrically related tasks, surface normal and camera depth. We
demonstrate an improvement of 14.17% on relative error and 10.4% improvement on
$\delta_{1}$ accuracy over the most accurate baseline state-of-the-art BTS
approach. All experiments are conducted on a recently released C3VD dataset;
thus, we provide a first benchmark of state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18665">Pose Estimation and Tracking for ASIST. (arXiv:2311.18665v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goodman_A/0/1/0/all/0/1">Ari Goodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1">Gurpreet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+OShea_R/0/1/0/all/0/1">Ryan O&#x27;Shea</a>, <a href="http://arxiv.org/find/cs/1/au:+Teague_P/0/1/0/all/0/1">Peter Teague</a>, <a href="http://arxiv.org/find/cs/1/au:+Hing_J/0/1/0/all/0/1">James Hing</a></p>
<p>Aircraft Ship Integrated Secure and Traverse (ASIST) is a system designed to
arrest helicopters safely and efficiently on ships. Originally, a precision
Helicopter Position Sensing Equipment (HPSE) tracked and monitored the position
of the helicopter relative to the Rapid Securing Device (RSD). However, using
the HPSE component was determined to be infeasible in the transition of the
ASIST system due to the hardware installation requirements. As a result,
sailors track the position of the helicopters with their eyes with no sensor or
artificially intelligent decision aid. Manually tracking the helicopter takes
additional time and makes recoveries more difficult, especially at high sea
states. Performing recoveries without the decision aid leads to higher
uncertainty and cognitive load. PETA (Pose Estimation and Tracking for ASIST)
is a research effort to create a helicopter tracking system prototype without
hardware installation requirements for ASIST system operators. Its overall goal
is to improve situational awareness and reduce operator uncertainty with
respect to the aircrafts position relative to the RSD, and consequently
increase the allowable landing area. The authors produced a prototype system
capable of tracking helicopters with respect to the RSD. The software included
a helicopter pose estimation component, camera pose estimation component, and a
user interface component. PETA demonstrated the potential for state-of-the-art
computer vision algorithms Faster R-CNN and HRNet (High-Resolution Network) to
be used to estimate the pose of helicopters in real-time, returning ASIST to
its originally intended capability. PETA also demonstrated that traditional
methods of encoder-decoders could be used to estimate the orientation of the
helicopter and could be used to confirm the output from HRNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18666">Action Recognition in Video Recordings from Gynecologic Laparoscopy. (arXiv:2311.18666v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nasirihaghighi_S/0/1/0/all/0/1">Sahar Nasirihaghighi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghamsarian_N/0/1/0/all/0/1">Negin Ghamsarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Stefanics_D/0/1/0/all/0/1">Daniela Stefanics</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoeffmann_K/0/1/0/all/0/1">Klaus Schoeffmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Husslein_H/0/1/0/all/0/1">Heinrich Husslein</a></p>
<p>Action recognition is a prerequisite for many applications in laparoscopic
video analysis including but not limited to surgical training, operation room
planning, follow-up surgery preparation, post-operative surgical assessment,
and surgical outcome estimation. However, automatic action recognition in
laparoscopic surgeries involves numerous challenges such as (I) cross-action
and intra-action duration variation, (II) relevant content distortion due to
smoke, blood accumulation, fast camera motions, organ movements, object
occlusion, and (III) surgical scene variations due to different illuminations
and viewpoints. Besides, action annotations in laparoscopy surgeries are
limited and expensive due to requiring expert knowledge. In this study, we
design and evaluate a CNN-RNN architecture as well as a customized
training-inference framework to deal with the mentioned challenges in
laparoscopic surgery action recognition. Using stacked recurrent layers, our
proposed network takes advantage of inter-frame dependencies to negate the
negative effect of content distortion and variation in action recognition.
Furthermore, our proposed frame sampling strategy effectively manages the
duration variations in surgical actions to enable action recognition with high
temporal resolution. Our extensive experiments confirm the superiority of our
proposed method in action recognition compared to static CNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18675">Cascaded Interaction with Eroded Deep Supervision for Salient Object Detection. (arXiv:2311.18675v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1">Hewen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jie Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1">Guangfu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weiren Wu</a></p>
<p>Deep convolutional neural networks have been widely applied in salient object
detection and have achieved remarkable results in this field. However, existing
models suffer from information distortion caused by interpolation during
up-sampling and down-sampling. In response to this drawback, this article
starts from two directions in the network: feature and label. On the one hand,
a novel cascaded interaction network with a guidance module named global-local
aligned attention (GAA) is designed to reduce the negative impact of
interpolation on the feature side. On the other hand, a deep supervision
strategy based on edge erosion is proposed to reduce the negative guidance of
label interpolation on lateral output. Extensive experiments on five popular
datasets demonstrate the superiority of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18681">RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance. (arXiv:2311.18681v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1">Chantal Pellegrini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1">Ege &#xd6;zsoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1">Benjamin Busam</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1">Matthias Keicher</a></p>
<p>Conversational AI tools that can generate and discuss clinically correct
radiology reports for a given medical image have the potential to transform
radiology. Such a human-in-the-loop radiology assistant could facilitate a
collaborative diagnostic process, thus saving time and improving the quality of
reports. Towards this goal, we introduce RaDialog, the first thoroughly
evaluated and publicly available large vision-language model for radiology
report generation and interactive dialog. RaDialog effectively integrates
visual image features and structured pathology findings with a large language
model (LLM) while simultaneously adapting it to a specialized domain using
parameter-efficient fine-tuning. To keep the conversational abilities of the
underlying LLM, we propose a comprehensive, semi-automatically labeled,
image-grounded instruct dataset for chest X-ray radiology tasks. By training
with this dataset, our method achieves state-of-the-art clinical correctness in
report generation and shows impressive abilities in interactive tasks such as
correcting reports and answering questions, serving as a foundational step
toward clinical dialog systems. Our code is available on github:
https://github.com/ChantalMP/RaDialog.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18695">Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction. (arXiv:2311.18695v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Cheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_W/0/1/0/all/0/1">Wei-En Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1">Yu-Lin Shih</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kuan-Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Syu_Y/0/1/0/all/0/1">Yong-Jing Syu</a>, <a href="http://arxiv.org/find/cs/1/au:+The_K/0/1/0/all/0/1">Kent Selwyn The</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Chiang Frank Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hwann-Tzong Chen</a></p>
<p>State-of-the-art single-view 360-degree room layout reconstruction methods
formulate the problem as a high-level 1D (per-column) regression task. On the
other hand, traditional low-level 2D layout segmentation is simpler to learn
and can represent occluded regions, but it requires complex post-processing for
the targeting layout polygon and sacrifices accuracy. We present Seg2Reg to
render 1D layout depth regression from the 2D segmentation map in a
differentiable and occlusion-aware way, marrying the merits of both sides.
Specifically, our model predicts floor-plan density for the input
equirectangular 360-degree image. Formulating the 2D layout representation as a
density field enables us to employ `flattened' volume rendering to form 1D
layout depth regression. In addition, we propose a novel 3D warping
augmentation on layout to improve generalization. Finally, we re-implement
recent room layout reconstruction methods into our codebase for benchmarking
and explore modern backbones and training techniques to serve as the strong
baseline. Our model significantly outperforms previous arts. The code will be
made available upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18710">Meta-Prior: Meta learning for Adaptive Inverse Problem Solvers. (arXiv:2311.18710v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terris_M/0/1/0/all/0/1">Matthieu Terris</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1">Thomas Moreau</a></p>
<p>Deep neural networks have become a foundational tool for addressing imaging
inverse problems. They are typically trained for a specific task, with a
supervised loss to learn a mapping from the observations to the image to
recover. However, real-world imaging challenges often lack ground truth data,
rendering traditional supervised approaches ineffective. Moreover, for each new
imaging task, a new model needs to be trained from scratch, wasting time and
resources. To overcome these limitations, we introduce a novel approach based
on meta-learning. Our method trains a meta-model on a diverse set of imaging
tasks that allows the model to be efficiently fine-tuned for specific tasks
with few fine-tuning steps. We show that the proposed method extends to the
unsupervised setting, where no ground truth data is available. In its bilevel
formulation, the outer level uses a supervised loss, that evaluates how well
the fine-tuned model performs, while the inner loss can be either supervised or
unsupervised, relying only on the measurement operator. This allows the
meta-model to leverage a few ground truth samples for each task while being
able to generalize to new imaging tasks. We show that in simple settings, this
approach recovers the Bayes optimal estimator, illustrating the soundness of
our approach. We also demonstrate our method's effectiveness on various tasks,
including image processing and magnetic resonance imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.09213">Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1">Felix Juefei-Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">Geguang Pu</a></p>
<p>The current high-fidelity generation and high-precision detection of DeepFake
images are at an arms race. We believe that producing DeepFakes that are highly
realistic and 'detection evasive' can serve the ultimate goal of improving
future generation DeepFake detection capabilities. In this paper, we propose a
simple yet powerful pipeline to reduce the artifact patterns of fake images
without hurting image quality by performing implicit spatial-domain notch
filtering. We first demonstrate that frequency-domain notch filtering, although
famously shown to be effective in removing periodic noise in the spatial
domain, is infeasible for our task at hand due to the manual designs required
for the notch filters. We, therefore, resort to a learning-based approach to
reproduce the notch filtering effects, but solely in the spatial domain. We
adopt a combination of adding overwhelming spatial noise for breaking the
periodic noise pattern and deep image filtering to reconstruct the noise-free
fake images, and we name our method DeepNotch. Deep image filtering provides a
specialized filter for each pixel in the noisy image, producing filtered images
with high fidelity compared to their DeepFake counterparts. Moreover, we also
use the semantic information of the image to generate an adversarial guidance
map to add noise intelligently. Our large-scale evaluation on 3 representative
state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)
has demonstrated that our technique significantly reduces the accuracy of these
3 fake image detection methods, 36.79% on average and up to 97.02% in the best
case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.06296">Monocular Camera Localization for Automated Vehicles Using Image Retrieval. (arXiv:2109.06296v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joa_E/0/1/0/all/0/1">Eunhyek Joa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yibo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Borrelli_F/0/1/0/all/0/1">Francesco Borrelli</a></p>
<p>We address the problem of finding the current position and heading angle of
an autonomous vehicle in real-time using a single camera. Compared to methods
which require LiDARs and high definition (HD) 3D maps in real-time, the
proposed approach is easily scalable and computationally efficient, at the
price of lower precision.
</p>
<p>The new method combines and adapts existing algorithms in three different
fields: image retrieval, mapping database, and particle filtering. The result
is a simple, real-time localization method using an image retrieval method
whose performance is comparable to other monocular camera localization methods
which use a map built with LiDARs.
</p>
<p>We evaluate the proposed method using the KITTI odometry dataset and via
closed-loop experiments with an indoor 1:10 autonomous vehicle. The tests
demonstrate real-time capability and a 10cm level accuracy. Also, experimental
results of the closed-loop indoor tests show the presence of a positive
feedback loop between the localization error and the control error. Such
phenomena is analysed in details at the end of the article.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.11369">Recent Advances of Continual Learning in Computer Vision: An Overview. (arXiv:2109.11369v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1">Haoxuan Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1">Hossein Rahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Li Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1">Bryan Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>In contrast to batch learning where all training data is available at once,
continual learning represents a family of methods that accumulate knowledge and
learn continuously with data available in sequential order. Similar to the
human learning process with the ability of learning, fusing, and accumulating
new knowledge coming at different time steps, continual learning is considered
to have high practical significance. Hence, continual learning has been studied
in various artificial intelligence tasks. In this paper, we present a
comprehensive review of the recent progress of continual learning in computer
vision. In particular, the works are grouped by their representative
techniques, including regularization, knowledge distillation, memory,
generative replay, parameter isolation, and a combination of the above
techniques. For each category of these techniques, both its characteristics and
applications in computer vision are presented. At the end of this overview,
several subareas, where continuous knowledge accumulation is potentially
helpful while continual learning has not been well studied, are discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.12727">Generating More Pertinent Captions by Leveraging Semantics and Style on Multi-Source Datasets. (arXiv:2111.12727v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1">Marcella Cornia</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1">Lorenzo Baraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1">Giuseppe Fiameni</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1">Rita Cucchiara</a></p>
<p>This paper addresses the task of generating fluent descriptions by training
on a non-uniform combination of data sources, containing both human-annotated
and web-collected captions. Large-scale datasets with noisy image-text pairs,
indeed, provide a sub-optimal source of supervision because of their
low-quality descriptive style, while human-annotated datasets are cleaner but
smaller in scale. To get the best of both worlds, we propose to leverage and
separate semantics and descriptive style through the incorporation of a style
token and keywords extracted through a retrieval component. The proposed model
avoids the need of object detectors, is trained with a single objective of
prompt language modeling, and can replicate the style of human-collected
captions while training on sources with different input styles. Experimentally,
the model shows a strong capability of recognizing real-world concepts and
producing high-quality captions. Extensive experiments are performed on
different image captioning datasets, including CC3M, nocaps, and the
competitive COCO dataset, where our model consistently outperforms baselines
and state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.03215">Handwriting recognition and automatic scoring for descriptive answers in Japanese language tests. (arXiv:2201.03215v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hung Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Oka_H/0/1/0/all/0/1">Haruki Oka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishioka_T/0/1/0/all/0/1">Tsunenori Ishioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1">Masaki Nakagawa</a></p>
<p>This paper presents an experiment of automatically scoring handwritten
descriptive answers in the trial tests for the new Japanese university entrance
examination, which were made for about 120,000 examinees in 2017 and 2018.
There are about 400,000 answers with more than 20 million characters. Although
all answers have been scored by human examiners, handwritten characters are not
labeled. We present our attempt to adapt deep neural network-based handwriting
recognizers trained on a labeled handwriting dataset into this unlabeled answer
set. Our proposed method combines different training strategies, ensembles
multiple recognizers, and uses a language model built from a large general
corpus to avoid overfitting into specific data. In our experiment, the proposed
method records character accuracy of over 97% using about 2,000 verified
labeled answers that account for less than 0.5% of the dataset. Then, the
recognized answers are fed into a pre-trained automatic scoring system based on
the BERT model without correcting misrecognized characters and providing rubric
annotations. The automatic scoring system achieves from 0.84 to 0.98 of
Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents an acceptable
similarity of scoring between the automatic scoring system and the human
examiners. These results are promising for further research on end-to-end
automatic scoring of descriptive answers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.14034">Attribute Descent: Simulating Object-Centric Datasets on the Content Level and Beyond. (arXiv:2202.14034v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yue Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Napthade_M/0/1/0/all/0/1">Milind Napthade</a>, <a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1">Tom Gedeon</a></p>
<p>This article aims to use graphic engines to simulate a large number of
training data that have free annotations and possibly strongly resemble to
real-world data. Between synthetic and real, a two-level domain gap exists,
involving content level and appearance level. While the latter is concerned
with appearance style, the former problem arises from a different mechanism,
i.e, content mismatch in attributes such as camera viewpoint, object placement
and lighting conditions. In contrast to the widely-studied appearance-level
gap, the content-level discrepancy has not been broadly studied. To address the
content-level misalignment, we propose an attribute descent approach that
automatically optimizes engine attributes to enable synthetic data to
approximate real-world data. We verify our method on object-centric tasks,
wherein an object takes up a major portion of an image. In these tasks, the
search space is relatively small, and the optimization of each attribute yields
sufficiently obvious supervision signals. We collect a new synthetic asset
VehicleX, and reformat and reuse existing the synthetic assets ObjectX and
PersonX. Extensive experiments on image classification and object
re-identification confirm that adapted synthetic data can be effectively used
in three scenarios: training with synthetic data only, training data
augmentation and numerically understanding dataset content.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.08728">RandoMix: A mixed sample data augmentation method with multiple mixed modes. (arXiv:2205.08728v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Furao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1">Changhai Nie</a></p>
<p>Data augmentation plays a crucial role in enhancing the robustness and
performance of machine learning models across various domains. In this study,
we introduce a novel mixed-sample data augmentation method called RandoMix.
RandoMix is specifically designed to simultaneously address robustness and
diversity challenges. It leverages a combination of linear and mask-mixed
modes, introducing flexibility in candidate selection and weight adjustments.
We evaluate the effectiveness of RandoMix on diverse datasets, including
CIFAR-10/100, Tiny-ImageNet, ImageNet, and Google Speech Commands. Our results
demonstrate its superior performance compared to existing techniques such as
Mixup, CutMix, Fmix, and ResizeMix. Notably, RandoMix excels in enhancing model
robustness against adversarial noise, natural noise, and sample occlusion. The
comprehensive experimental results and insights into parameter tuning
underscore the potential of RandoMix as a versatile and effective data
augmentation method. Moreover, it seamlessly integrates into the training
pipeline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.06214">Real-World Light Field Image Super-Resolution via Degradation Modulation. (arXiv:2206.06214v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhengyu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jungang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1">Wei An</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulan Guo</a></p>
<p>Recent years have witnessed the great advances of deep neural networks (DNNs)
in light field (LF) image super-resolution (SR). However, existing DNN-based LF
image SR methods are developed on a single fixed degradation (e.g., bicubic
downsampling), and thus cannot be applied to super-resolve real LF images with
diverse degradation. In this paper, we propose a simple yet effective method
for real-world LF image SR. In our method, a practical LF degradation model is
developed to formulate the degradation process of real LF images. Then, a
convolutional neural network is designed to incorporate the degradation prior
into the SR process. By training on LF images using our formulated degradation,
our network can learn to modulate different degradation while incorporating
both spatial and angular information in LF images. Extensive experiments on
both synthetically degraded and real-world LF images demonstrate the
effectiveness of our method. Compared with existing state-of-the-art single and
LF image SR methods, our method achieves superior SR performance under a wide
range of degradation, and generalizes better to real LF images. Codes and
models are available at https://yingqianwang.github.io/LF-DMnet/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06799">MMOTU: A Multi-Modality Ovarian Tumor Ultrasound Image Dataset for Unsupervised Cross-Domain Semantic Segmentation. (arXiv:2207.06799v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shuchang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1">Wenpei Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1">Linghan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Binghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Guangliang Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Meijing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_X/0/1/0/all/0/1">Xiubo Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lijiang Chen</a></p>
<p>Ovarian cancer is one of the most harmful gynecological diseases. Detecting
ovarian tumors in early stage with computer-aided techniques can efficiently
decrease the mortality rate. With the improvement of medical treatment
standard, ultrasound images are widely applied in clinical treatment. However,
recent notable methods mainly focus on single-modality ultrasound ovarian tumor
segmentation or recognition, which means there still lacks researches on
exploring the representation capability of multi-modality ultrasound ovarian
tumor images. To solve this problem, we propose a Multi-Modality Ovarian Tumor
Ultrasound (MMOTU) image dataset containing 1469 2d ultrasound images and 170
contrast enhanced ultrasonography (CEUS) images with pixel-wise and global-wise
annotations. Based on MMOTU, we mainly focus on unsupervised cross-domain
semantic segmentation task. To solve the domain shift problem, we propose a
feature alignment based architecture named Dual-Scheme Domain-Selected Network
(DS2Net). Specifically, we first design source-encoder and target-encoder to
extract two-style features of source and target images. Then, we propose
Domain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module
(DUSM) to extract the distinct and universal features in two styles
(source-style or target-style). Finally, we fuse these two kinds of features
and feed them into the source-decoder and target-decoder to generate final
predictions. Extensive comparison experiments and analysis on MMOTU image
dataset show that DS2Net can boost the segmentation performance for
bidirectional cross-domain adaptation of 2d ultrasound images and CEUS images.
Our proposed dataset and code are all available at
https://github.com/cv516Buaa/MMOTU_DS2Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.07711">Local Low-light Image Enhancement via Region-Aware Normalization. (arXiv:2208.07711v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Shihurong Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yizhan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a></p>
<p>In the realm of Low-Light Image Enhancement (LLIE), existing research
primarily focuses on enhancing images globally. However, many applications
require local LLIE, where users are allowed to illuminate specific regions
using an input mask, such as creating a protagonist stage or spotlight effect.
However, this task has received limited attention currently. This paper aims to
systematically define the requirements of local LLIE and proposes a novel
strategy to convert current existing global LLIE methods into local versions.
The image space is divided into three regions: Masked Area A be enlightened to
achieve the desired lighting effects; Transition Area B is a smooth transition
from the enlightened area (Area A) to the unchanged region (Area C). To achieve
the task of local LLIE, we introduce Region-Aware Normalization for Local
Enhancement, dubbed as RANLEN. RANLEN uses a dynamically designed mask-based
normalization operation, which enhances an image in a spatially varying manner,
ensuring that the enhancement results are consistent with the requirements
specified by the input mask. Additionally, a set of region-aware loss terms is
formulated to facilitate the learning of the local LLIE framework. Our strategy
can be applied to existing global LLIE networks with varying structures.
Extensive experiments demonstrate that our approach can produce the desired
lighting effects compared to global LLIE, all the while offering controllable
local enhancement with various mask shapes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.09207">Table Detection in the Wild: A Novel Diverse Table Detection Dataset and Method. (arXiv:2209.09207v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haloi_M/0/1/0/all/0/1">Mrinal Haloi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1">Shashank Shekhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fande_N/0/1/0/all/0/1">Nikhil Fande</a>, <a href="http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1">Siddhant Swaroop Dash</a>, <a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1">Sanjay G</a></p>
<p>Recent deep learning approaches in table detection achieved outstanding
performance and proved to be effective in identifying document layouts.
Currently, available table detection benchmarks have many limitations,
including the lack of samples diversity, simple table structure, the lack of
training cases, and samples quality. In this paper, we introduce a diverse
large-scale dataset for table detection with more than seven thousand samples
containing a wide variety of table structures collected from many diverse
sources. In addition to that, we also present baseline results using a
convolutional neural network-based method to detect table structure in
documents. Experimental results show the superiority of applying convolutional
deep learning methods over classical computer vision-based methods. The
introduction of this diverse table detection dataset will enable the community
to develop high throughput deep learning methods for understanding document
layout and tabular data processing. Dataset is available at: 1.
https://www.kaggle.com/datasets/mrinalim/stdw-dataset 2.
https://huggingface.co/datasets/n3011/STDW
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04020">Fast-ParC: Capturing Position Aware Global Feature for ConvNets and ViTs. (arXiv:2210.04020v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haokui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenze Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyu Wang</a></p>
<p>Transformer models have made tremendous progress in various fields in recent
years. In the field of computer vision, vision transformers (ViTs) also become
strong alternatives to convolutional neural networks (ConvNets), yet they have
not been able to replace ConvNets since both have their own merits. For
instance, ViTs are good at extracting global features with attention mechanisms
while ConvNets are more efficient in modeling local relationships due to their
strong inductive bias. A natural idea that arises is to combine the strengths
of both ConvNets and ViTs to design new structures. In this paper, we propose a
new basic neural network operator named position-aware circular convolution
(ParC) and its accelerated version Fast-ParC. The ParC operator can capture
global features by using a global kernel and circular convolution while keeping
location sensitiveness by employing position embeddings. Our Fast-ParC further
reduces the O(n2) time complexity of ParC to O(n log n) using Fast Fourier
Transform. This acceleration makes it possible to use global convolution in the
early stages of models with large feature maps, yet still maintains the overall
computational cost comparable with using 3x3 or 7x7 kernels. The proposed
operation can be used in a plug-and-play manner to 1) convert ViTs to
pure-ConvNet architecture to enjoy wider hardware support and achieve higher
inference speed; 2) replacing traditional convolutions in the deep stage of
ConvNets to improve accuracy by enlarging the effective receptive field.
Experiment results show that our ParC op can effectively enlarge the receptive
field of traditional ConvNets, and adopting the proposed op benefits both ViTs
and ConvNet models on all three popular vision tasks, image classification,
object
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06186">Gotcha: Real-Time Video Deepfake Detection via Challenge-Response. (arXiv:2210.06186v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1">Govind Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">Chinmay Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Memon_N/0/1/0/all/0/1">Nasir Memon</a></p>
<p>With the rise of AI-enabled Real-Time Deepfakes (RTDFs), the integrity of
online video interactions has become a growing concern. RTDFs have now made it
feasible to replace an imposter's face with their victim in live video
interactions. Such advancement in deepfakes also coaxes detection to rise to
the same standard. However, existing deepfake detection techniques are
asynchronous and hence ill-suited for RTDFs. To bridge this gap, we propose a
challenge-response approach that establishes authenticity in live settings. We
focus on talking-head style video interaction and present a taxonomy of
challenges that specifically target inherent limitations of RTDF generation
pipelines. We evaluate representative examples from the taxonomy by collecting
a unique dataset comprising eight challenges, which consistently and visibly
degrades the quality of state-of-the-art deepfake generators. These results are
corroborated both by humans and a new automated scoring function, leading to
88.6\% and 73.2% AUC, respectively. The findings underscore the promising
potential of challenge-response systems for explainable and scalable real-time
deepfake detection in practical scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06841">Point-DAE: Denoising Autoencoders for Self-supervised Point Cloud Learning. (arXiv:2211.06841v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yabin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiehong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruihuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1">Kui Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>Masked autoencoder has demonstrated its effectiveness in self-supervised
point cloud learning. Considering that masking is a kind of corruption, in this
work we explore a more general denoising autoencoder for point cloud learning
(Point-DAE) by investigating more types of corruptions beyond masking.
Specifically, we degrade the point cloud with certain corruptions as input, and
learn an encoder-decoder model to reconstruct the original point cloud from its
corrupted version. Three corruption families (\ie, density/masking, noise, and
affine transformation) and a total of fourteen corruption types are
investigated with traditional non-Transformer encoders. Besides the popular
masking corruption, we identify another effective corruption family, \ie,
affine transformation. The affine transformation disturbs all points globally,
which is complementary to the masking corruption where some local regions are
dropped. We also validate the effectiveness of affine transformation corruption
with the Transformer backbones, where we decompose the reconstruction of the
complete point cloud into the reconstructions of detailed local patches and
rough global shape, alleviating the position leakage problem in the
reconstruction. Extensive experiments on tasks of object classification,
few-shot learning, robustness testing, part segmentation, and 3D object
detection validate the effectiveness of the proposed method. The codes are
available at \url{https://github.com/YBZh/Point-DAE}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11293">Beyond the Field-of-View: Enhancing Scene Visibility and Perception with Clip-Recurrent Transformer. (arXiv:2211.11293v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Hao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1">Qi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xiaoting Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1">Huajian Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kaiwei Wang</a></p>
<p>Vision sensors are widely applied in vehicles, robots, and roadside
infrastructure. However, due to limitations in hardware cost and system size,
camera Field-of-View (FoV) is often restricted and may not provide sufficient
coverage. Nevertheless, from a spatiotemporal perspective, it is possible to
obtain information beyond the camera's physical FoV from past video streams. In
this paper, we propose the concept of online video inpainting for autonomous
vehicles to expand the field of view, thereby enhancing scene visibility,
perception, and system safety. To achieve this, we introduce the FlowLens
architecture, which explicitly employs optical flow and implicitly incorporates
a novel clip-recurrent transformer for feature propagation. FlowLens offers two
key features: 1) FlowLens includes a newly designed Clip-Recurrent Hub with
3D-Decoupled Cross Attention (DDCA) to progressively process global information
accumulated over time. 2) It integrates a multi-branch Mix Fusion Feed Forward
Network (MixF3N) to enhance the precise spatial flow of local features. To
facilitate training and evaluation, we derive the KITTI360 dataset with various
FoV mask, which covers both outer- and inner FoV expansion scenarios. We also
conduct quantitative assessments of beyond-FoV semantics across different
models and perform qualitative comparisons of beyond-FoV object detection. We
illustrate that employing FlowLens to reconstruct unseen scenes even enhances
perception within the field of view by providing reliable semantic context.
Extensive experiments and user studies involving offline and online video
inpainting, as well as beyond-FoV perception tasks, demonstrate that FlowLens
achieves state-of-the-art performance. The source code and dataset are made
publicly available at https://github.com/MasterHow/FlowLens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12993">Benchmarking Robustness to Adversarial Image Obfuscations. (arXiv:2301.12993v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stimberg_F/0/1/0/all/0/1">Florian Stimberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chun-Ta Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hazimeh_H/0/1/0/all/0/1">Hussein Hazimeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Stretcu_O/0/1/0/all/0/1">Otilia Stretcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_W/0/1/0/all/0/1">Wei Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yintao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaya_M/0/1/0/all/0/1">Merve Kaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1">Cyrus Rashtchian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuxman_A/0/1/0/all/0/1">Ariel Fuxman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tek_M/0/1/0/all/0/1">Mehmet Tek</a>, <a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1">Sven Gowal</a></p>
<p>Automated content filtering and moderation is an important tool that allows
online platforms to build striving user communities that facilitate cooperation
and prevent abuse. Unfortunately, resourceful actors try to bypass automated
filters in a bid to post content that violate platform policies and codes of
conduct. To reach this goal, these malicious actors may obfuscate policy
violating images (e.g. overlay harmful images by carefully selected benign
images or visual patterns) to prevent machine learning models from reaching the
correct decision. In this paper, we invite researchers to tackle this specific
issue and present a new image benchmark. This benchmark, based on ImageNet,
simulates the type of obfuscations created by malicious actors. It goes beyond
ImageNet-$\textrm{C}$ and ImageNet-$\bar{\textrm{C}}$ by proposing general,
drastic, adversarial modifications that preserve the original content intent.
It aims to tackle a more common adversarial threat than the one considered by
$\ell_p$-norm bounded adversaries. We evaluate 33 pretrained models on the
benchmark and train models with different augmentations, architectures and
training methods on subsets of the obfuscations to measure generalization. We
hope this benchmark will encourage researchers to test their models and methods
and try to find new approaches that are more robust to these obfuscations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10414">Improving Scene Text Image Super-resolution via Dual Prior Modulation Network. (arXiv:2302.10414v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shipeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zuoyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1">Pengfei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Hui Xue</a></p>
<p>Scene text image super-resolution (STISR) aims to simultaneously increase the
resolution and legibility of the text images, and the resulting images will
significantly affect the performance of downstream tasks. Although numerous
progress has been made, existing approaches raise two crucial issues: (1) They
neglect the global structure of the text, which bounds the semantic determinism
of the scene text. (2) The priors, e.g., text prior or stroke prior, employed
in existing works, are extracted from pre-trained text recognizers. That said,
such priors suffer from the domain gap including low resolution and blurriness
caused by poor imaging conditions, leading to incorrect guidance. Our work
addresses these gaps and proposes a plug-and-play module dubbed Dual Prior
Modulation Network (DPMN), which leverages dual image-level priors to bring
performance gain over existing approaches. Specifically, two types of
prior-guided refinement modules, each using the text mask or graphic
recognition result of the low-quality SR image from the preceding layer, are
designed to improve the structural clarity and semantic accuracy of the text,
respectively. The following attention mechanism hence modulates two
quality-enhanced images to attain a superior SR result. Extensive experiments
validate that our method improves the image quality and boosts the performance
of downstream tasks over five typical approaches on the benchmark. Substantial
visualizations and ablation studies demonstrate the advantages of the proposed
DPMN. Code is available at: https://github.com/jdfxzzy/DPMN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00566">Structured Pruning for Deep Convolutional Neural Networks: A survey. (arXiv:2303.00566v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lingao Xiao</a></p>
<p>The remarkable performance of deep Convolutional neural networks (CNNs) is
generally attributed to their deeper and wider architectures, which can come
with significant computational costs. Pruning neural networks has thus gained
interest since it effectively lowers storage and computational costs. In
contrast to weight pruning, which results in unstructured models, structured
pruning provides the benefit of realistic acceleration by producing models that
are friendly to hardware implementation. The special requirements of structured
pruning have led to the discovery of numerous new challenges and the
development of innovative solutions. This article surveys the recent progress
towards structured pruning of deep CNNs. We summarize and compare the
state-of-the-art structured pruning techniques with respect to filter ranking
methods, regularization methods, dynamic execution, neural architecture search,
the lottery ticket hypothesis, and the applications of pruning. While
discussing structured pruning algorithms, we briefly introduce the unstructured
pruning counterpart to emphasize their differences. Furthermore, we provide
insights into potential research opportunities in the field of structured
pruning. A curated list of neural network pruning papers can be found at
https://github.com/he-y/Awesome-Pruning . A dedicated website offering a more
interactive comparison of structured pruning methods can be found at:
https://huggingface.co/spaces/he-yang/Structured-Pruning-Survey .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01681">Dense Pixel-to-Pixel Harmonization via Continuous Image Representation. (arXiv:2303.01681v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yilan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zhengxia Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenwei Shi</a></p>
<p>High-resolution (HR) image harmonization is of great significance in
real-world applications such as image synthesis and image editing. However, due
to the high memory costs, existing dense pixel-to-pixel harmonization methods
are mainly focusing on processing low-resolution (LR) images. Some recent works
resort to combining with color-to-color transformations but are either limited
to certain resolutions or heavily depend on hand-crafted image filters. In this
work, we explore leveraging the implicit neural representation (INR) and
propose a novel image Harmonization method based on Implicit neural Networks
(HINet), which to the best of our knowledge, is the first dense pixel-to-pixel
method applicable to HR images without any hand-crafted filter design. Inspired
by the Retinex theory, we decouple the MLPs into two parts to respectively
capture the content and environment of composite images. A Low-Resolution Image
Prior (LRIP) network is designed to alleviate the Boundary Inconsistency
problem, and we also propose new designs for the training and inference
process. Extensive experiments have demonstrated the effectiveness of our
method compared with state-of-the-art methods. Furthermore, some interesting
and practical applications of the proposed method are explored. Our code is
available at https://github.com/WindVChen/INR-Harmonization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08314">Guided Slot Attention for Unsupervised Video Object Segmentation. (arXiv:2303.08314v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minhyeok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Suhwan Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dogyoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chaewon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangyoun Lee</a></p>
<p>Unsupervised video object segmentation aims to segment the most prominent
object in a video sequence. However, the existence of complex backgrounds and
multiple foreground objects make this task challenging. To address this issue,
we propose a guided slot attention network to reinforce spatial structural
information and obtain better foreground--background separation. The foreground
and background slots, which are initialized with query guidance, are
iteratively refined based on interactions with template information.
Furthermore, to improve slot--template interaction and effectively fuse global
and local features in the target and reference frames, K-nearest neighbors
filtering and a feature aggregation transformer are introduced. The proposed
model achieves state-of-the-art performance on two popular datasets.
Additionally, we demonstrate the robustness of the proposed model in
challenging scenes through various comparative experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12001">ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders. (arXiv:2303.12001v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1">Jefferson Hernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1">Ruben Villegas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1">Vicente Ordonez</a></p>
<p>We propose ViC-MAE, a model that combines both Masked AutoEncoders (MAE) and
contrastive learning. ViC-MAE is trained using a global featured obtained by
pooling the local representations learned under an MAE reconstruction loss and
leveraging this representation under a contrastive objective across images and
video frames. We show that visual representations learned under ViC-MAE
generalize well to both video and image classification tasks. Particularly,
ViC-MAE obtains state-of-the-art transfer learning performance from video to
images on Imagenet-1k compared to the recently proposed OmniMAE by achieving a
top-1 accuracy of 86% (+1.3% absolute improvement) when trained on the same
data and 87.1% (+2.4% absolute improvement) when training on extra data. At the
same time ViC-MAE outperforms most other methods on video benchmarks by
obtaining 75.9% top-1 accuracy on the challenging Something something-v2 video
benchmark . When training on videos and images from a diverse combination of
datasets, our method maintains a balanced transfer-learning performance between
video and image classification benchmarks, coming only as a close second to the
best supervised method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00916">DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models. (arXiv:2304.00916v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yukang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kwan-Yee K. Wong</a></p>
<p>We present DreamAvatar, a text-and-shape guided framework for generating
high-quality 3D human avatars with controllable poses. While encouraging
results have been reported by recent methods on text-guided 3D common object
generation, generating high-quality human avatars remains an open challenge due
to the complexity of the human body's shape, pose, and appearance. We propose
DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for
predicting density and color for 3D points and pretrained text-to-image
diffusion models for providing 2D self-supervision. Specifically, we leverage
the SMPL model to provide shape and pose guidance for the generation. We
introduce a dual-observation-space design that involves the joint optimization
of a canonical space and a posed space that are related by a learnable
deformation field. This facilitates the generation of more complete textures
and geometry faithful to the target pose. We also jointly optimize the losses
computed from the full body and from the zoomed-in 3D head to alleviate the
common multi-face ''Janus'' problem and improve facial details in the generated
avatars. Extensive evaluations demonstrate that DreamAvatar significantly
outperforms existing methods, establishing a new state-of-the-art for
text-and-shape guided 3D human avatar generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00962">RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jihan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Runyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Weipeng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiaojuan Qi</a></p>
<p>We propose a lightweight and scalable Regional Point-Language Contrastive
learning framework, namely \textbf{RegionPLC}, for open-world 3D scene
understanding, aiming to identify and recognize open-set objects and
categories. Specifically, based on our empirical studies, we introduce a
3D-aware SFusion strategy that fuses 3D vision-language pairs derived from
multiple 2D foundation models, yielding high-quality, dense region-level
language descriptions without human 3D annotations. Subsequently, we devise a
region-aware point-discriminative contrastive learning objective to enable
robust and effective 3D learning from dense regional language supervision. We
carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets,
and our model outperforms prior 3D open-world scene understanding approaches by
an average of 17.2\% and 9.1\% for semantic and instance segmentation,
respectively, while maintaining greater scalability and lower resource demands.
Furthermore, our method has the flexibility to be effortlessly integrated with
language models to enable open-ended grounded 3D reasoning without extra
task-specific training. Code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03492">ClothCombo: Modeling Inter-Cloth Interaction for Draping Multi-Layered Clothes. (arXiv:2304.03492v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dohae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hyun Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">In-Kwon Lee</a></p>
<p>We present ClothCombo, a pipeline to drape arbitrary combinations of clothes
on 3D human models with varying body shapes and poses. While existing
learning-based approaches for draping clothes have shown promising results,
multi-layered clothing remains challenging as it is non-trivial to model
inter-cloth interaction. To this end, our method utilizes a GNN-based network
to efficiently model the interaction between clothes in different layers, thus
enabling multi-layered clothing. Specifically, we first create feature
embedding for each cloth using a topology-agnostic network. Then, the draping
network deforms all clothes to fit the target body shape and pose without
considering inter-cloth interaction. Lastly, the untangling network predicts
the per-vertex displacements in a way that resolves interpenetration between
clothes. In experiments, the proposed model demonstrates strong performance in
complex multi-layered scenarios. Being agnostic to cloth topology, our method
can be readily used for layered virtual try-on of real clothes in diverse poses
and combinations of clothes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09976">Analyzing the Domain Shift Immunity of Deep Homography Estimation. (arXiv:2304.09976v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1">Mingzhen Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1">Tolga Tasdizen</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Sarang Joshi</a></p>
<p>Homography estimation serves as a fundamental technique for image alignment
in a wide array of applications. The advent of convolutional neural networks
has introduced learning-based methodologies that have exhibited remarkable
efficacy in this realm. Yet, the generalizability of these approaches across
distinct domains remains underexplored. Unlike other conventional tasks,
CNN-driven homography estimation models show a distinctive immunity to domain
shifts, enabling seamless deployment from one dataset to another without the
necessity of transfer learning. This study explores the resilience of a variety
of deep homography estimation models to domain shifts, revealing that the
network architecture itself is not a contributing factor to this remarkable
adaptability. By closely examining the models' focal regions and subjecting
input images to a variety of modifications, we confirm that the models heavily
rely on local textures such as edges and corner points for homography
estimation. Moreover, our analysis underscores that the domain shift immunity
itself is intricately tied to the utilization of these local textures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10253">Image retrieval outperforms diffusion models on data augmentation. (arXiv:2304.10253v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Burg_M/0/1/0/all/0/1">Max F. Burg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1">Florian Wenzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zietlow_D/0/1/0/all/0/1">Dominik Zietlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1">Max Horn</a>, <a href="http://arxiv.org/find/cs/1/au:+Makansi_O/0/1/0/all/0/1">Osama Makansi</a>, <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1">Chris Russell</a></p>
<p>Many approaches have been proposed to use diffusion models to augment
training datasets for downstream tasks, such as classification. However,
diffusion models are themselves trained on large datasets, often with noisy
annotations, and it remains an open question to which extent these models
contribute to downstream classification performance. In particular, it remains
unclear if they generalize enough to improve over directly using the additional
data of their pre-training process for augmentation. We systematically evaluate
a range of existing methods to generate images from diffusion models and study
new extensions to assess their benefit for data augmentation. Personalizing
diffusion models towards the target data outperforms simpler prompting
strategies. However, using the pre-training data of the diffusion model alone,
via a simple nearest-neighbor retrieval procedure, leads to even stronger
downstream performance. Our study explores the potential of diffusion models in
generating new training data, and surprisingly finds that these sophisticated
models are not yet able to beat a simple and strong image retrieval baseline on
simple downstream vision tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10864">FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation. (arXiv:2304.10864v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jianbo Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuanxiu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shanshan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangyun Li</a></p>
<p>The research community has witnessed the powerful potential of
self-supervised Masked Image Modeling (MIM), which enables the models capable
of learning visual representation from unlabeled data. In this paper, to
incorporate both the crucial global structural information and local details
for dense prediction tasks, we alter the perspective to the frequency domain
and present a new MIM-based framework named FreMIM for self-supervised
pre-training to better accomplish medical image segmentation tasks. Based on
the observations that the detailed structural information mainly lies in the
high-frequency components and the high-level semantics are abundant in the
low-frequency counterparts, we further incorporate multi-stage supervision to
guide the representation learning during the pre-training phase. Extensive
experiments on three benchmark datasets show the superior advantage of our
FreMIM over previous state-of-the-art MIM methods. Compared with various
baselines trained from scratch, our FreMIM could consistently bring
considerable improvements to model performance. The code will be publicly
available at https://github.com/Rubics-Xuan/FreMIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10880">Med-Tuning: Parameter-Efficient Transfer Learning with Fine-Grained Feature Enhancement for Medical Volumetric Segmentation. (arXiv:2304.10880v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiachen Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jianbo Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shanshan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangyun Li</a></p>
<p>Deep learning-based medical volumetric segmentation methods either train the
model from scratch or follow the standard ``pre-training then fine-tuning"
paradigm. Although fine-tuning a pre-trained model on downstream tasks can
harness its representation power, the standard full fine-tuning is costly in
terms of computation and memory footprint. In this paper, we present the study
on parameter-efficient transfer learning for medical volumetric segmentation
and propose a new framework named Med-Tuning based on intra-stage feature
enhancement and inter-stage feature interaction. Additionally, aiming at
exploiting the intrinsic global properties of Fourier Transform for
parameter-efficient transfer learning, a new adapter block namely Med-Adapter
with a well-designed Fourier Transform branch is proposed for effectively and
efficiently modeling the crucial global context for medical volumetric
segmentation. Given a large-scale pre-trained model on 2D natural images, our
method can exploit both the crucial spatial multi-scale feature and volumetric
correlations along slices for accurate segmentation. Extensive experiments on
three benchmark datasets (including CT and MRI) show that our method can
achieve better results than previous parameter-efficient transfer learning
methods on segmentation tasks, with much less tuned parameter costs. Compared
to full fine-tuning, our method reduces the fine-tuned model parameters by up
to 4x, with even better segmentation performance. The code will be made
publicly available at https://github.com/jessie-chen99/Med-Tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06988">Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shoubin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jaemin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Prateek Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Recent studies have shown promising results on utilizing large pre-trained
image-language models for video question answering. While these image-language
models can efficiently bootstrap the representation learning of video-language
models, they typically concatenate uniformly sampled video frames as visual
inputs without explicit language-aware, temporal modeling. When only a portion
of a video input is relevant to the language query, such uniform frame sampling
can often lead to missing important visual cues. Although humans often find a
video moment to focus on and rewind the moment to answer questions, training a
query-aware video moment localizer often requires expensive annotations and
high computational costs. To address this issue, we propose Self-Chained Video
Localization-Answering (SeViLA), a novel framework that leverages a single
image-language model (BLIP-2) to tackle both temporal keyframe localization and
QA on videos. SeViLA framework consists of two modules: Localizer and Answerer,
where both are parameter-efficiently fine-tuned from BLIP-2. We propose two
ways of chaining these modules for cascaded inference and self-refinement.
First, in the forward chain, the Localizer finds multiple language-aware
keyframes in a video, which the Answerer uses to predict the answer. Second, in
the reverse chain, the Answerer generates keyframe pseudo-labels to refine the
Localizer, alleviating the need for expensive video moment localization
annotations. Our SeViLA framework outperforms several strong baselines on 5
challenging video QA and event prediction benchmarks, and achieves the
state-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA,
STAR, How2QA, VLEP) settings. We also analyze the impact of Localizer,
comparisons of Localizer with other temporal localization models,
pre-training/self-refinement of Localizer, and varying the number of keyframes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08192">Diffusion Models for Imperceptible and Transferable Adversarial Attack. (arXiv:2305.08192v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yilan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zhengxia Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenwei Shi</a></p>
<p>Many existing adversarial attacks generate $L_p$-norm perturbations on image
RGB space. Despite some achievements in transferability and attack success
rate, the crafted adversarial examples are easily perceived by human eyes.
Towards visual imperceptibility, some recent works explore unrestricted attacks
without $L_p$-norm constraints, yet lacking transferability of attacking
black-box models. In this work, we propose a novel imperceptible and
transferable attack by leveraging both the generative and discriminative power
of diffusion models. Specifically, instead of direct manipulation in pixel
space, we craft perturbations in the latent space of diffusion models. Combined
with well-designed content-preserving structures, we can generate
human-insensitive perturbations embedded with semantic clues. For better
transferability, we further "deceive" the diffusion model which can be viewed
as an implicit recognition surrogate, by distracting its attention away from
the target regions. To our knowledge, our proposed method, DiffAttack, is the
first that introduces diffusion models into the adversarial attack field.
Extensive experiments on various model structures, datasets, and defense
methods have demonstrated the superiority of our attack over the existing
attack methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13172">Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yunzhi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhoubo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>Despite the ability to train capable LLMs, the methodology for maintaining
their relevancy and rectifying errors remains elusive. To this end, the past
few years have witnessed a surge in techniques for editing LLMs, the objective
of which is to efficiently alter the behavior of LLMs within a specific domain
without negatively impacting performance across other inputs. This paper
embarks on a deep exploration of the problems, methods, and opportunities
related to model editing for LLMs. In particular, we provide an exhaustive
overview of the task definition and challenges associated with model editing,
along with an in-depth empirical analysis of the most progressive methods
currently at our disposal. We also build a new benchmark dataset to facilitate
a more robust evaluation and pinpoint enduring issues intrinsic to existing
techniques. Our objective is to provide valuable insights into the
effectiveness and feasibility of each editing technique, thereby assisting the
community in making informed decisions on the selection of the most appropriate
method for a specific task or context. Code and datasets are available at
https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16526">Extending Explainable Boosting Machines to Scientific Image Data. (arXiv:2305.16526v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schug_D/0/1/0/all/0/1">Daniel Schug</a>, <a href="http://arxiv.org/find/cs/1/au:+Yerramreddy_S/0/1/0/all/0/1">Sai Yerramreddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1">Rich Caruana</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenberg_C/0/1/0/all/0/1">Craig Greenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Zwolak_J/0/1/0/all/0/1">Justyna P. Zwolak</a></p>
<p>As the deployment of computer vision technology becomes increasingly common
in science, the need for explanations of the system and its output has become a
focus of great concern. Driven by the pressing need for interpretable models in
science, we propose the use of Explainable Boosting Machines (EBMs) for
scientific image data. Inspired by an important application underpinning the
development of quantum technologies, we apply EBMs to cold-atom soliton image
data tabularized using Gabor Wavelet Transform-based techniques that preserve
the spatial structure of the data. In doing so, we demonstrate the use of EBMs
for image data for the first time and show that our approach provides
explanations that are consistent with human intuition about the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17644">Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation. (arXiv:2305.17644v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoshuang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a></p>
<p>Modeling in Computer Vision has evolved to MLPs. Vision MLPs naturally lack
local modeling capability, to which the simplest treatment is combined with
convolutional layers. Convolution, famous for its sliding window scheme, also
suffers from this scheme of redundancy and low computational efficiency. In
this paper, we seek to dispense with the windowing scheme and introduce a more
elaborate and effective approach to exploiting locality. To this end, we
propose a new MLP module, namely Shifted-Pillars-Concatenation (SPC), that
consists of two steps of processes: (1) Pillars-Shift, which generates four
neighboring maps by shifting the input image along four directions, and (2)
Pillars-Concatenation, which applies linear transformations and concatenation
on the maps to aggregate local features. SPC module offers superior local
modeling power and performance gains, making it a promising alternative to the
convolutional layer. Then, we build a pure-MLP architecture called Caterpillar
by replacing the convolutional layer with the SPC module in a hybrid model of
sMLPNet. Extensive experiments show Caterpillar's excellent performance and
scalability on both ImageNet-1K and small-scale classification benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19480">Learning by Aligning 2D Skeleton Sequences in Time. (arXiv:2305.19480v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quoc-Huy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Muhammad Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Popattia_M/0/1/0/all/0/1">Murad Popattia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">M. Hassan Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1">Andrey Konin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1">M. Zeeshan Zia</a></p>
<p>This paper presents a self-supervised temporal video alignment framework
which is useful for several fine-grained human activity understanding
applications. In contrast with the state-of-the-art method of CASA, where
sequences of 3D skeleton coordinates are taken directly as input, our key idea
is to use sequences of 2D skeleton heatmaps as input. Unlike CASA which
performs self-attention in the temporal domain only, we feed 2D skeleton
heatmaps to a video transformer which performs self-attention both in the
spatial and temporal domains for extracting effective spatiotemporal and
contextual features. In addition, we introduce simple heatmap augmentation
techniques based on 2D skeletons for self-supervised learning. Despite the lack
of 3D information, our approach achieves not only higher accuracy but also
better robustness against missing and noisy keypoints than CASA. Furthermore,
extensive evaluations on three public datasets, i.e., Penn Action, IKEA ASM,
and H2O, demonstrate that our approach outperforms previous methods in
different fine-grained human activity understanding tasks. Finally, fusing 2D
skeleton heatmaps with RGB videos yields the state-of-the-art on all metrics
and datasets. To our best knowledge, our work is the first to utilize 2D
skeleton heatmap inputs and the first to explore multi-modality fusion for
temporal video alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20082">Control4D: Efficient 4D Portrait Editing with Text. (arXiv:2305.20082v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1">Ruizhi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Cheng Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zerong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Boyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a></p>
<p>We introduce Control4D, an innovative framework for editing dynamic 4D
portraits using text instructions. Our method addresses the prevalent
challenges in 4D editing, notably the inefficiencies of existing 4D
representations and the inconsistent editing effect caused by diffusion-based
editors. We first propose GaussianPlanes, a novel 4D representation that makes
Gaussian Splatting more structured by applying plane-based decomposition in 3D
space and time. This enhances both efficiency and robustness in 4D editing.
Furthermore, we propose to leverage a 4D generator to learn a more continuous
generation space from inconsistent edited images produced by the
diffusion-based editor, which effectively improves the consistency and quality
of 4D editing. Comprehensive evaluation demonstrates the superiority of
Control4D, including significantly reduced training time, high-quality
rendering, and spatial-temporal consistency in 4D portrait editing. The link to
our project website is https://control4darxiv.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00519">DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation. (arXiv:2306.00519v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ju_X/0/1/0/all/0/1">Xiaoliang Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhaoyang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yijin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guofeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>We present DiffInDScene, a novel framework for tackling the problem of
high-quality 3D indoor scene generation, which is challenging due to the
complexity and diversity of the indoor scene geometry. Although diffusion-based
generative models have previously demonstrated impressive performance in image
generation and object-level 3D generation, they have not yet been applied to
room-level 3D generation due to their computationally intensive costs. In
DiffInDScene, we propose a cascaded 3D diffusion pipeline that is efficient and
possesses strong generative performance for Truncated Signed Distance Function
(TSDF). The whole pipeline is designed to run on a sparse occupancy space in a
coarse-to-fine fashion. Inspired by KinectFusion's incremental alignment and
fusion of local TSDF volumes, we propose a diffusion-based SDF fusion approach
that iteratively diffuses and fuses local TSDF volumes, facilitating the
generation of an entire room environment. The generated results demonstrate
that our work is capable to achieve high-quality room generation directly in
three-dimensional space, starting from scratch. In addition to the scene
generation, the final part of DiffInDScene can be used as a post-processing
module to refine the 3D reconstruction results from multi-view stereo.
According to the user study, the mesh quality generated by our DiffInDScene can
even outperform the ground truth mesh provided by ScanNet. Please visit our
project page for the latest progress and demonstrations:
https://github.com/AkiraHero/diffindscene.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00973">Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models. (arXiv:2306.00973v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yujie Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a></p>
<p>Generative models have recently exhibited exceptional capabilities in
text-to-image generation, but still struggle to generate image sequences
coherently. In this work, we focus on a novel, yet challenging task of
generating a coherent image sequence based on a given storyline, denoted as
open-ended visual storytelling. We make the following three contributions: (i)
to fulfill the task of visual storytelling, we propose a learning-based
auto-regressive image generation model, termed as StoryGen, with a novel
vision-language context module, that enables to generate the current frame by
conditioning on the corresponding text prompt and preceding image-caption
pairs; (ii) to address the data shortage of visual storytelling, we collect
paired image-text sequences by sourcing from online videos and open-source
E-books, establishing processing pipeline for constructing a large-scale
dataset with diverse characters, storylines, and artistic styles, named
StorySalon; (iii) Quantitative experiments and human evaluations have validated
the superiority of our StoryGen, where we show StoryGen can generalize to
unseen characters without any optimization, and generate image sequences with
coherent content and consistent character. The code, model, and dataset will be
made publicly available to the research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00974">Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search. (arXiv:2306.00974v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qihao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1">Adam Kortylewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yutong Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Song Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a></p>
<p>Text-guided diffusion models (TDMs) are widely applied but can fail
unexpectedly. Common failures include: (i) natural-looking text prompts
generating images with the wrong content, or (ii) different random samples of
the latent variables that generate vastly different, and even unrelated,
outputs despite being conditioned on the same text prompt. In this work, we aim
to study and understand the failure modes of TDMs in more detail. To achieve
this, we propose SAGE, the first adversarial search method on TDMs that
systematically explores the discrete prompt space and the high-dimensional
latent space, to automatically discover undesirable behaviors and failure cases
in image generation. We use image classifiers as surrogate loss functions
during searching, and employ human inspections to validate the identified
failures. For the first time, our method enables efficient exploration of both
the discrete and intricate human language space and the challenging latent
space, overcoming the gradient vanishing problem. Then, we demonstrate the
effectiveness of SAGE on five widely used generative models and reveal four
typical failure modes: (1) We find a variety of natural text prompts that
generate images failing to capture the semantics of input texts. We further
discuss the underlying causes and potential solutions based on the results. (2)
We find regions in the latent space that lead to distorted images independent
of the text prompt, suggesting that parts of the latent space are not
well-structured. (3) We also find latent samples that result in natural-looking
images unrelated to the text prompt, implying a possible misalignment between
the latent and prompt spaces. (4) By appending a single adversarial token
embedding to any input prompts, we can generate a variety of specified target
objects. Project page: https://sage-diffusion.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02582">Enhancing Point Annotations with Superpixel and Confidence Learning Guided for Improving Semi-Supervised OCT Fluid Segmentation. (arXiv:2306.02582v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1">Tengjin Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1">Kai Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhiming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunxiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gewen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaqi Wang</a></p>
<p>Automatic segmentation of fluid in Optical Coherence Tomography (OCT) images
is beneficial for ophthalmologists to make an accurate diagnosis. Although
semi-supervised OCT fluid segmentation networks enhance their performance by
introducing additional unlabeled data, the performance enhancement is limited.
To address this, we propose Superpixel and Confident Learning Guide Point
Annotations Network (SCLGPA-Net) based on the teacher-student architecture,
which can learn OCT fluid segmentation from limited fully-annotated data and
abundant point-annotated data. Specifically, we use points to annotate fluid
regions in unlabeled OCT images and the Superpixel-Guided Pseudo-Label
Generation (SGPLG) module generates pseudo-labels and pixel-level label trust
maps from the point annotations. The label trust maps provide an indication of
the reliability of the pseudo-labels. Furthermore, we propose the Confident
Learning Guided Label Refinement (CLGLR) module identifies error information in
the pseudo-labels and leads to further refinement. Experiments on the RETOUCH
dataset show that we are able to reduce the need for fully-annotated data by
94.22\%, closing the gap with the best fully supervised baselines to a mean IoU
of only 2\%. Furthermore, We constructed a private 2D OCT fluid segmentation
dataset for evaluation. Compared with other methods, comprehensive experimental
results demonstrate that the proposed method can achieve excellent performance
in OCT fluid segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05846">Motion-DVAE: Unsupervised learning for fast human motion denoising. (arXiv:2306.05846v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fiche_G/0/1/0/all/0/1">Gu&#xe9;nol&#xe9; Fiche</a>, <a href="http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1">Simon Leglaive</a>, <a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1">Xavier Alameda-Pineda</a>, <a href="http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1">Renaud S&#xe9;guier</a></p>
<p>Pose and motion priors are crucial for recovering realistic and accurate
human motion from noisy observations. Substantial progress has been made on
pose and shape estimation from images, and recent works showed impressive
results using priors to refine frame-wise predictions. However, a lot of motion
priors only model transitions between consecutive poses and are used in
time-consuming optimization procedures, which is problematic for many
applications requiring real-time motion capture. We introduce Motion-DVAE, a
motion prior to capture the short-term dependencies of human motion. As part of
the dynamical variational autoencoder (DVAE) models family, Motion-DVAE
combines the generative capability of VAE models and the temporal modeling of
recurrent architectures. Together with Motion-DVAE, we introduce an
unsupervised learned denoising method unifying regression- and
optimization-based approaches in a single framework for real-time 3D human pose
estimation. Experiments show that the proposed approach reaches competitive
performance with state-of-the-art methods while being much faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07404">Compositor: Bottom-up Clustering and Compositing for Robust Part and Object Segmentation. (arXiv:2306.07404v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Ju He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jieneng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Ming-Xian Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qihang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a></p>
<p>In this work, we present a robust approach for joint part and object
segmentation. Specifically, we reformulate object and part segmentation as an
optimization problem and build a hierarchical feature representation including
pixel, part, and object-level embeddings to solve it in a bottom-up clustering
manner. Pixels are grouped into several clusters where the part-level
embeddings serve as cluster centers. Afterwards, object masks are obtained by
compositing the part proposals. This bottom-up interaction is shown to be
effective in integrating information from lower semantic levels to higher
semantic levels. Based on that, our novel approach Compositor produces part and
object segmentation masks simultaneously while improving the mask quality.
Compositor achieves state-of-the-art performance on PartImageNet and
Pascal-Part by outperforming previous methods by around 0.9% and 1.3% on
PartImageNet, 0.4% and 1.7% on Pascal-Part in terms of part and object mIoU and
demonstrates better robustness against occlusion by around 4.4% and 7.1% on
part and object respectively. Code will be available at
https://github.com/TACJu/Compositor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10012">MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1">Lingbo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu Su</a></p>
<p>Text-guided image editing is widely needed in daily life, ranging from
personal use to professional applications such as Photoshop. However, existing
methods are either zero-shot or trained on an automatically synthesized
dataset, which contains a high volume of noise. Thus, they still require lots
of manual tuning to produce desirable outcomes in practice. To address this
issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),
the first large-scale, manually annotated dataset for instruction-guided real
image editing that covers diverse scenarios: single-turn, multi-turn,
mask-provided, and mask-free editing. MagicBrush comprises over 10K manually
annotated triplets (source image, instruction, target image), which supports
trainining large-scale text-guided image editing models. We fine-tune
InstructPix2Pix on MagicBrush and show that the new model can produce much
better images according to human evaluation. We further conduct extensive
experiments to evaluate current image editing baselines from multiple
dimensions including quantitative, qualitative, and human evaluations. The
results reveal the challenging nature of our dataset and the gap between
current baselines and real-world editing needs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12230">Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nowak_A/0/1/0/all/0/1">Aleksandra I. Nowak</a>, <a href="http://arxiv.org/find/cs/1/au:+Grooten_B/0/1/0/all/0/1">Bram Grooten</a>, <a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1">Decebal Constantin Mocanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a></p>
<p>Dynamic Sparse Training (DST) is a rapidly evolving area of research that
seeks to optimize the sparse initialization of a neural network by adapting its
topology during training. It has been shown that under specific conditions, DST
is able to outperform dense models. The key components of this framework are
the pruning and growing criteria, which are repeatedly applied during the
training process to adjust the network's sparse connectivity. While the growing
criterion's impact on DST performance is relatively well studied, the influence
of the pruning criterion remains overlooked. To address this issue, we design
and perform an extensive empirical analysis of various pruning criteria to
better understand their impact on the dynamics of DST solutions. Surprisingly,
we find that most of the studied methods yield similar results. The differences
become more significant in the low-density regime, where the best performance
is predominantly given by the simplest technique: magnitude-based pruning. The
code is provided at https://github.com/alooow/fantastic_weights_paper
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14875">A Fully Unsupervised Instance Segmentation Technique for White Blood Cell Images. (arXiv:2306.14875v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Biswas_S/0/1/0/all/0/1">Shrijeet Biswas</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhattacharya_A/0/1/0/all/0/1">Amartya Bhattacharya</a></p>
<p>White blood cells, also known as leukocytes are group of heterogeneously
nucleated cells which act as salient immune system cells. These are originated
in the bone marrow and are found in blood, plasma, and lymph tissues.
Leukocytes kill the bacteria, virus and other kind of pathogens which invade
human body through phagocytosis that in turn results immunity. Detection of a
white blood cell count can reveal camouflaged infections and warn doctors about
chronic medical conditions such as autoimmune diseases, immune deficiencies,
and blood disorders. Segmentation plays an important role in identification of
white blood cells (WBC) from microscopic image analysis. The goal of
segmentation in a microscopic image is to divide the image into different
distinct regions. In our paper, we tried to propose a novel instance
segmentation method for segmenting the WBCs containing both the nucleus and the
cytoplasm, from bone marrow images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17140">ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion Models. (arXiv:2306.17140v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Weihao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Given sparse views of a 3D object, estimating their camera poses is a
long-standing and intractable problem. Toward this goal, we consider harnessing
the pre-trained diffusion model of novel views conditioned on viewpoints
(Zero-1-to-3). We present ID-Pose which inverses the denoising diffusion
process to estimate the relative pose given two input images. ID-Pose adds a
noise to one image, and predicts the noise conditioned on the other image and a
hypothesis of the relative pose. The prediction error is used as the
minimization objective to find the optimal pose with the gradient descent
method. We extend ID-Pose to handle more than two images and estimate each pose
with multiple image pairs from triangular relations. ID-Pose requires no
training and generalizes to open-world images. We conduct extensive experiments
using casually captured photos and rendered images with random viewpoints. The
results demonstrate that ID-Pose significantly outperforms state-of-the-art
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04470">Test-Time Adaptation for Nighttime Color-Thermal Semantic Segmentation. (arXiv:2307.04470v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yexin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Guoyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jinjing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilakos_A/0/1/0/all/0/1">Athanasios Vasilakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lin Wang</a></p>
<p>The ability to scene understanding in adverse visual conditions, e.g.,
nighttime, has sparked active research for RGB-Thermal (RGB-T) semantic
segmentation. However, it is essentially hampered by two critical problems: 1)
the day-night gap of RGB images is larger than that of thermal images, and 2)
the class-wise performance of RGB images at night is not consistently higher or
lower than that of thermal images. we propose the first test-time adaptation
(TTA) framework, dubbed Night-TTA, to address the problems for nighttime RGBT
semantic segmentation without access to the source (daytime) data during
adaptation. Our method enjoys three key technical parts. Firstly, as one
modality (e.g., RGB) suffers from a larger domain gap than that of the other
(e.g., thermal), Imaging Heterogeneity Refinement (IHR) employs an interaction
branch on the basis of RGB and thermal branches to prevent cross-modal
discrepancy and performance degradation. Then, Class Aware Refinement (CAR) is
introduced to obtain reliable ensemble logits based on pixel-level distribution
aggregation of the three branches. In addition, we also design a specific
learning scheme for our TTA framework, which enables the ensemble logits and
three student logits to collaboratively learn to improve the quality of
predictions during the testing phase of our Night TTA. Extensive experiments
show that our method achieves state-of-the-art (SoTA) performance with a 13.07%
boost in mIoU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07688">DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration. (arXiv:2307.07688v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yuanshuo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1">Mingwen Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yecong Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>Existing All-In-One image restoration (IR) methods usually lack flexible
modeling on various types of degradation, thus impeding the restoration
performance. To achieve All-In-One IR with higher task dexterity, this work
proposes an efficient Dynamic Reference Modeling paradigm (DRM-IR), which
consists of task-adaptive degradation modeling and model-based image restoring.
Specifically, these two subtasks are formalized as a pair of entangled
reference-based maximum a posteriori (MAP) inferences, which are optimized
synchronously in an unfolding-based manner. With the two cascaded subtasks,
DRM-IR first dynamically models the task-specific degradation based on a
reference image pair and further restores the image with the collected
degradation statistics. Besides, to bridge the semantic gap between the
reference and target degraded images, we further devise a Degradation Prior
Transmitter (DPT) that restrains the instance-specific feature differences.
DRM-IR explicitly provides superior flexibility for All-in-One IR while being
interpretable. Extensive experiments on multiple benchmark datasets show that
our DRM-IR achieves state-of-the-art in All-In-One IR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10601">SCA-PVNet: Self-and-Cross Attention Based Aggregation of Point Cloud and Multi-View for 3D Object Retrieval. (arXiv:2307.10601v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dongyun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1">Aiyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shangbo Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiqun Li</a></p>
<p>To address 3D object retrieval, substantial efforts have been made to
generate highly discriminative descriptors of 3D objects represented by a
single modality, e.g., voxels, point clouds or multi-view images. It is
promising to leverage the complementary information from multi-modality
representations of 3D objects to further improve retrieval performance.
However, multi-modality 3D object retrieval is rarely developed and analyzed on
large-scale datasets. In this paper, we propose self-and-cross attention based
aggregation of point cloud and multi-view images (SCA-PVNet) for 3D object
retrieval. With deep features extracted from point clouds and multi-view
images, we design two types of feature aggregation modules, namely the
In-Modality Aggregation Module (IMAM) and the Cross-Modality Aggregation Module
(CMAM), for effective feature fusion. IMAM leverages a self-attention mechanism
to aggregate multi-view features while CMAM exploits a cross-attention
mechanism to interact point cloud features with multi-view features. The final
descriptor of a 3D object for object retrieval can be obtained via
concatenating the aggregated features from both modules. Extensive experiments
and analysis are conducted on three datasets, ranging from small to large
scale, to show the superiority of the proposed SCA-PVNet over the
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11702">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization. (arXiv:2307.11702v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1">Jerome Revaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabon_Y/0/1/0/all/0/1">Yohann Cabon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bregier_R/0/1/0/all/0/1">Romain Br&#xe9;gier</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">JongMin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1">Philippe Weinzaepfel</a></p>
<p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every
pixel of a given image, has recently shown promising potential. However,
existing methods remain limited to small scenes memorized during training, and
thus hardly scale to realistic datasets and scenarios. In this paper, we
propose a generalized SCR model trained once to be deployed in new test scenes,
regardless of their scale, without any finetuning. Instead of encoding the
scene coordinates into the network weights, our model takes as input a database
image with some sparse 2D pixel to 3D coordinate annotations, extracted from
e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for
which are predicted a dense 3D coordinate map and its confidence, based on
cross-attention. At test time, we rely on existing off-the-shelf image
retrieval systems and fuse the predictions from a shortlist of relevant
database images w.r.t. the query. Afterwards camera pose is obtained using
standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo
pretrained weights, we train our model on diverse datasets to ensure
generalizabilty across various scenarios, and significantly outperform other
scene regression approaches, including scene-specific models, on multiple
visual localization benchmarks. Finally, we show that the database
representation of images and their 2D-3D annotations can be highly compressed
with negligible loss of localization performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09835">Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shijie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengwei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ach_T/0/1/0/all/0/1">Thomas Ach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1">Guido Gerig</a></p>
<p>Current deep learning-based approaches for the segmentation of microscopy
images heavily rely on large amount of training data with dense annotation,
which is highly costly and laborious in practice. Compared to full annotation
where the complete contour of objects is depicted, point annotations,
specifically object centroids, are much easier to acquire and still provide
crucial information about the objects for subsequent segmentation. In this
paper, we assume access to point annotations only during training and develop a
unified pipeline for microscopy image segmentation using synthetically
generated training data. Our framework includes three stages: (1) it takes
point annotations and samples a pseudo dense segmentation mask constrained with
shape priors; (2) with an image generative model trained in an unpaired manner,
it translates the mask to a realistic microscopy image regularized by object
level consistency; (3) the pseudo masks along with the synthetic images then
constitute a pairwise dataset for training an ad-hoc segmentation model. On the
public MoNuSeg dataset, our synthesis pipeline produces more diverse and
realistic images than baseline models while maintaining high coherence between
input masks and generated images. When using the identical segmentation
backbones, the models trained on our synthetic dataset significantly outperform
those trained with pseudo-labels or baseline-generated images. Moreover, our
framework achieves comparable results to models trained on authentic microscopy
images with dense labels, demonstrating its potential as a reliable and highly
efficient alternative to labor-intensive manual pixel-wise annotations in
microscopy image segmentation. The code is available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09922">MDCS: More Diverse Experts with Consistency Self-distillation for Long-tailed Recognition. (arXiv:2308.09922v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qihao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>Recently, multi-expert methods have led to significant improvements in
long-tail recognition (LTR). We summarize two aspects that need further
enhancement to contribute to LTR boosting: (1) More diverse experts; (2) Lower
model variance. However, the previous methods didn't handle them well. To this
end, we propose More Diverse experts with Consistency Self-distillation (MDCS)
to bridge the gap left by earlier methods. Our MDCS approach consists of two
core components: Diversity Loss (DL) and Consistency Self-distillation (CS). In
detail, DL promotes diversity among experts by controlling their focus on
different categories. To reduce the model variance, we employ KL divergence to
distill the richer knowledge of weakly augmented instances for the experts'
self-distillation. In particular, we design Confident Instance Sampling (CIS)
to select the correctly classified instances for CS to avoid biased/noisy
knowledge. In the analysis and ablation study, we demonstrate that our method
compared with previous work can effectively increase the diversity of experts,
significantly reduce the variance of the model, and improve recognition
accuracy. Moreover, the roles of our DL and CS are mutually reinforcing and
coupled: the diversity of experts benefits from the CS, and the CS cannot
achieve remarkable results without the DL. Experiments show our MDCS
outperforms the state-of-the-art by 1% $\sim$ 2% on five popular long-tailed
benchmarks, including CIFAR10-LT, CIFAR100-LT, ImageNet-LT, Places-LT, and
iNaturalist 2018. The code is available at https://github.com/fistyee/MDCS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10997">MarkovGen: Structured Prediction for Efficient Text-to-Image Generation. (arXiv:2308.10997v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1">Sadeep Jayasumana</a>, <a href="http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1">Daniel Glasner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1">Srikumar Ramalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1">Andreas Veit</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sanjiv Kumar</a></p>
<p>Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running sampling multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
We demonstrate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richly encodes the compatibility
among image tokens at different spatial locations to improve quality and
significantly reduce the required number of Muse sampling steps. Inference with
the MRF is significantly cheaper, and its parameters can be quickly learned
through back-propagation by modeling MRF inference as a differentiable
neural-network layer. Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1.5X and produce higher quality images by decreasing
undesirable image artifacts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12114">Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1">Richa Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Phlypo_R/0/1/0/all/0/1">Ronald Phlypo</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_R/0/1/0/all/0/1">Rajkumar Saini</a>, <a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1">Marcus Liwicki</a></p>
<p>Model sparsification in deep learning promotes simpler, more interpretable
models with fewer parameters. This not only reduces the model's memory
footprint and computational needs but also shortens inference time. This work
focuses on creating sparse models optimized for multiple tasks with fewer
parameters. These parsimonious models also possess the potential to match or
outperform dense models in terms of performance. In this work, we introduce
channel-wise l1/l2 group sparsity in the shared convolutional layers parameters
(or weights) of the multi-task learning model. This approach facilitates the
removal of extraneous groups i.e., channels (due to l1 regularization) and also
imposes a penalty on the weights, further enhancing the learning efficiency for
all tasks (due to l2 regularization). We analyzed the results of group sparsity
in both single-task and multi-task settings on two widely-used Multi-Task
Learning (MTL) datasets: NYU-v2 and CelebAMask-HQ. On both datasets, which
consist of three different computer vision tasks each, multi-task models with
approximately 70% sparsity outperform their dense equivalents. We also
investigate how changing the degree of sparsification influences the model's
performance, the overall sparsity percentage, the patterns of sparsity, and the
inference time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01728">Generative-based Fusion Mechanism for Multi-Modal Tracking. (arXiv:2309.01728v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhangyong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tianyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xuefeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiao-Jun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1">Josef Kittler</a></p>
<p>Generative models (GMs) have received increasing research interest for their
remarkable capacity to achieve comprehensive understanding. However, their
potential application in the domain of multi-modal tracking has remained
relatively unexplored. In this context, we seek to uncover the potential of
harnessing generative techniques to address the critical challenge, information
fusion, in multi-modal tracking. In this paper, we delve into two prominent GM
techniques, namely, Conditional Generative Adversarial Networks (CGANs) and
Diffusion Models (DMs). Different from the standard fusion process where the
features from each modality are directly fed into the fusion block, we
condition these multi-modal features with random noise in the GM framework,
effectively transforming the original training samples into harder instances.
This design excels at extracting discriminative clues from the features,
enhancing the ultimate tracking performance. To quantitatively gauge the
effectiveness of our approach, we conduct extensive experiments across two
multi-modal tracking tasks, three baseline methods, and three challenging
benchmarks. The experimental results demonstrate that the proposed
generative-based fusion mechanism achieves state-of-the-art performance,
setting new records on LasHeR and RGBD1K.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03661">Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation. (arXiv:2309.03661v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wansen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Youkai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Quanjun Yin</a></p>
<p>With strong representation capabilities, pretrained vision-language models
are widely used in vision and language navigation (VLN). However, most of them
are trained on web-crawled general-purpose datasets, which incurs a
considerable domain gap when used for VLN tasks. Another challenge for VLN is
how the agent understands the contextual relations between actions on a
trajectory and performs cross-modal alignment sequentially. In this paper, we
propose a novel Prompt-bAsed coNtext- and Domain-Aware (PANDA) pretraining
framework to address these problems. It performs prompting in two stages. In
the domain-aware stage, we apply a low-cost prompt tuning paradigm to learn
soft visual prompts from an in-domain dataset for equipping the pretrained
models with object-level and scene-level cross-modal alignment in VLN tasks.
Furthermore, in the context-aware stage, we design a set of hard context
prompts to capture the sequence-level semantics and instill both out-of-context
and contextual knowledge in the instruction into cross-modal representations.
They enable further tuning of the pretrained models via contrastive learning.
Experimental results on both R2R and REVERIE show the superiority of PANDA
compared to previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04917">Editing 3D Scenes via Text Prompts without Retraining. (arXiv:2309.04917v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Shuangkang Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Hsuan Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenrui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shuchang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Numerous diffusion models have recently been applied to image synthesis and
editing. However, editing 3D scenes is still in its early stages. It poses
various challenges, such as the requirement to design specific methods for
different editing types, retraining new models for various 3D scenes, and the
absence of convenient human interaction during editing. To tackle these issues,
we introduce a text-driven editing method, termed DN2N, which allows for the
direct acquisition of a NeRF model with universal editing capabilities,
eliminating the requirement for retraining. Our method employs off-the-shelf
text-based editing models of 2D images to modify the 3D scene images, followed
by a filtering process to discard poorly edited images that disrupt 3D
consistency. We then consider the remaining inconsistency as a problem of
removing noise perturbation, which can be solved by generating training data
with similar perturbation characteristics for training. We further propose
cross-view regularization terms to help the generalized NeRF model mitigate
these perturbations. Our text-driven method allows users to edit a 3D scene
with their desired description, which is more friendly, intuitive, and
practical than prior works. Empirical results show that our method achieves
multiple editing types, including but not limited to appearance editing,
weather transition, material changing, and style transfer. Most importantly,
our method generalizes well with editing abilities shared among a set of model
parameters without requiring a customized editing model for some specific
scenes, thus inferring novel views with editing effects directly from user
input. The project website is available at https://sk-fun.fun/DN2N
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05950">Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shihong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiqiu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Samuel Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Ryan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1">Tiffany Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a></p>
<p>Vision-language models (VLMs) pre-trained on web-scale datasets have
demonstrated remarkable capabilities on downstream tasks when fine-tuned with
minimal data. However, many VLMs rely on proprietary data and are not
open-source, which restricts the use of white-box approaches for fine-tuning.
As such, we aim to develop a black-box approach to optimize VLMs through
natural language prompts, thereby avoiding the need to access model parameters,
feature embeddings, or even output logits. We propose employing chat-based LLMs
to search for the best text prompt for VLMs. Specifically, we adopt an
automatic hill-climbing procedure that converges to an effective prompt by
evaluating the performance of current prompts and asking LLMs to refine them
based on textual feedback, all within a conversational process without
human-in-the-loop. In a challenging 1-shot image classification setup, our
simple approach surpasses the white-box continuous prompting method (CoOp) by
an average of 1.5% across 11 datasets including ImageNet. Our approach also
outperforms both human-engineered and LLM-generated prompts. We highlight the
advantage of conversational feedback that incorporates both positive and
negative prompts, suggesting that LLMs can utilize the implicit gradient
direction in textual feedback for a more efficient search. In addition, we find
that the text prompts generated through our strategy are not only more
interpretable but also transfer well across different VLM architectures in a
black-box manner. Lastly, we demonstrate our framework on a state-of-the-art
black-box VLM (DALL-E 3) for text-to-image optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06978">Differentiable JPEG: The Devil is in the Details. (arXiv:2309.06978v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1">Christoph Reich</a>, <a href="http://arxiv.org/find/cs/1/au:+Debnath_B/0/1/0/all/0/1">Biplob Debnath</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1">Deep Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1">Srimat Chakradhar</a></p>
<p>JPEG remains one of the most widespread lossy image coding methods. However,
the non-differentiable nature of JPEG restricts the application in deep
learning pipelines. Several differentiable approximations of JPEG have recently
been proposed to address this issue. This paper conducts a comprehensive review
of existing diff. JPEG approaches and identifies critical details that have
been missed by previous methods. To this end, we propose a novel diff. JPEG
approach, overcoming previous limitations. Our approach is differentiable
w.r.t. the input image, the JPEG quality, the quantization tables, and the
color conversion parameters. We evaluate the forward and backward performance
of our diff. JPEG approach against existing methods. Additionally, extensive
ablations are performed to evaluate crucial design choices. Our proposed diff.
JPEG resembles the (non-diff.) reference implementation best, significantly
surpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. For
strong compression rates, we can even improve PSNR by $9.51$dB. Strong
adversarial attack results are yielded by our diff. JPEG, demonstrating the
effective gradient approximation. Our code is available at
https://github.com/necla-ml/Diff-JPEG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09737">Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zhijun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fangqiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Hantao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Xiaoxuan Lu</a></p>
<p>Mobile autonomy relies on the precise perception of dynamic environments.
Robustly tracking moving objects in 3D world thus plays a pivotal role for
applications like trajectory prediction, obstacle avoidance, and path planning.
While most current methods utilize LiDARs or cameras for Multiple Object
Tracking (MOT), the capabilities of 4D imaging radars remain largely
unexplored. Recognizing the challenges posed by radar noise and point sparsity
in 4D radar data, we introduce RaTrack, an innovative solution tailored for
radar-based tracking. Bypassing the typical reliance on specific object types
and 3D bounding boxes, our method focuses on motion segmentation and
clustering, enriched by a motion estimation module. Evaluated on the
View-of-Delft dataset, RaTrack showcases superior tracking precision of moving
objects, largely surpassing the performance of the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09809">A Continual Learning Paradigm for Non-differentiable Visual Programming Frameworks on Visual Reasoning Tasks. (arXiv:2309.09809v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Wentao Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1">Nan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zeqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuojie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Keze Wang</a></p>
<p>Recently, the visual programming framework (VisProg) has emerged as a
significant framework for executing compositional visual tasks due to its
interpretability and flexibility. However, the performance of VisProg on
specific Visual Reasoning (VR) tasks is markedly inferior compared to
well-trained task-specific models since its employed visual sub-modules have
limited generalization capabilities. Due to the non-differentiability of
VisProg, it is quite challenging to improve these visual sub-modules within
VisProg for the specific VR task while maintaining their generalizability on
the un-seen tasks. Attempt to overcome these difficulties, we propose CLVP, a
Continuous Learning paradigm for VisProg across various visual reasoning tasks.
Specifically, our CLVP distills the capabilities of well-trained task-specific
models into the visual sub-modules in a stepwise and anti-forgetting manner.
This can continually improve the performance of VisProg on multiple visual
tasks while preserving the flexibility of VisProg. Extensive and comprehensive
experimental results demonstrate that our CLVP obtains significant performance
gains on specific VR benchmarks, i.e., GQA (+1.4%) and NLVRv2 (+5.6%), compared
to the VisProg baseline, and also maintains a promising generalizability for VR
on un-seen and previous learned tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12673">On Sparse Modern Hopfield Model. (arXiv:2309.12673v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jerry Yao-Chieh Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Donglin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dennis Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Han Liu</a></p>
<p>We introduce the sparse modern Hopfield model as a sparse extension of the
modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield
model equips a memory-retrieval dynamics whose one-step approximation
corresponds to the sparse attention mechanism. Theoretically, our key
contribution is a principled derivation of a closed-form sparse Hopfield energy
using the convex conjugate of the sparse entropic regularizer. Building upon
this, we derive the sparse memory retrieval dynamics from the sparse energy
function and show its one-step approximation is equivalent to the
sparse-structured attention. Importantly, we provide a sparsity-dependent
memory retrieval error bound which is provably tighter than its dense analog.
The conditions for the benefits of sparsity to arise are therefore identified
and discussed. In addition, we show that the sparse modern Hopfield model
maintains the robust theoretical properties of its dense counterpart, including
rapid fixed point convergence and exponential memory capacity. Empirically, we
use both synthetic and real-world datasets to demonstrate that the sparse
Hopfield model outperforms its dense counterpart in many situations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16483">Rethinking Domain Generalization: Discriminability and Generalizability. (arXiv:2309.16483v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1">Shaocong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qianyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1">Chenhao Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhuang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yuan Luo</a></p>
<p>Domain generalization (DG) endeavors to develop robust models that possess
strong generalizability while preserving excellent discriminability.
Nonetheless, pivotal DG techniques tend to improve the feature generalizability
by learning domain-invariant representations, inadvertently overlooking the
feature discriminability. On the one hand, the simultaneous attainment of
generalizability and discriminability of features presents a complex challenge,
often entailing inherent contradictions. This challenge becomes particularly
pronounced when domain-invariant features manifest reduced discriminability
owing to the inclusion of unstable factors, \emph{i.e.,} spurious correlations.
On the other hand, prevailing domain-invariant methods can be categorized as
category-level alignment, susceptible to discarding indispensable features
possessing substantial generalizability and narrowing intra-class variations.
To surmount these obstacles, we rethink DG from a new perspective that
concurrently imbues features with formidable discriminability and robust
generalizability, and present a novel framework, namely, Discriminative
Microscopic Distribution Alignment (DMDA). DMDA incorporates two core
components: Selective Channel Pruning~(SCP) and Micro-level Distribution
Alignment (MDA). Concretely, SCP attempts to curtail redundancy within neural
networks, prioritizing stable attributes conducive to accurate classification.
This approach alleviates the adverse effect of spurious domain invariance and
amplifies the feature discriminability. Besides, MDA accentuates micro-level
alignment within each class, going beyond mere category-level alignment. This
strategy accommodates sufficient generalizable features and facilitates
within-class variations. Extensive experiments on four benchmark datasets
corroborate the efficacy of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03704">Pose-Free Generalizable Rendering Transformer. (arXiv:2310.03704v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1">Panwang Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hanwen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zehao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>In the field of novel-view synthesis, the necessity of knowing camera poses
(e.g., via Structure from Motion) before rendering has been a common practice.
However, the consistent acquisition of accurate camera poses remains elusive,
and errors in pose extraction can adversely impact the view synthesis process.
To address this challenge, we introduce PF-GRT, a new Pose-Free framework for
Generalizable Rendering Transformer, eliminating the need for pre-computed
camera poses and instead leveraging feature-matching learned directly from
data. PF-GRT is parameterized using a local relative coordinate system, where
one of the source images is set as the origin. An OmniView Transformer is
designed for fusing multi-view cues under the pose-free setting, where
unposed-view fusion and origin-centric aggregation are performed. The 3D point
feature along target ray is sampled by projecting onto the selected origin
plane. The final pixel intensities are modulated and decoded using another
Transformer. PF-GRT demonstrates an impressive ability to generalize to new
scenes that were not encountered during the training phase, without the need of
pre-computing camera poses. Our experiments with zero-shot rendering on the
LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces
superior quality in generating photo-realistic images. Moreover, it
demonstrates robustness against noise in test camera poses. Code is available
at https://zhiwenfan.github.io/PF-GRT/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04705">Multi-scale MRI reconstruction via dilated ensemble networks. (arXiv:2310.04705v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1">Wendi Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Lorenzana_M/0/1/0/all/0/1">Marlon Bran Lorenzana</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1">Wei Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1">Hongfu Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1">Shekhar S. Chandra</a></p>
<p>As aliasing artefacts are highly structural and non-local, many MRI
reconstruction networks use pooling to enlarge filter coverage and incorporate
global context. However, this inadvertently impedes fine detail recovery as
downsampling creates a resolution bottleneck. Moreover, real and imaginary
features are commonly split into separate channels, discarding phase
information particularly important to high frequency textures. In this work, we
introduce an efficient multi-scale reconstruction network using dilated
convolutions to preserve resolution and experiment with a complex-valued
version using complex convolutions. Inspired by parallel dilated filters,
multiple receptive fields are processed simultaneously with branches that see
both large structural artefacts and fine local features. We also adopt dense
residual connections for feature aggregation to efficiently increase scale and
the deep cascade global architecture to reduce overfitting. The real-valued
version of this model outperformed common reconstruction architectures as well
as a state-of-the-art multi-scale network whilst being three times more
efficient. The complex-valued network yielded better qualitative results when
more phase information was present.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08129">Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting. (arXiv:2310.08129v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lichao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_F/0/1/0/all/0/1">Fangsheng Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Lili Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>Despite significant progress in the field, it is still challenging to create
personalized visual representations that align closely with the desires and
preferences of individual users. This process requires users to articulate
their ideas in words that are both comprehensible to the models and accurately
capture their vision, posing difficulties for many users. In this paper, we
tackle this challenge by leveraging historical user interactions with the
system to enhance user prompts. We propose a novel approach that involves
rewriting user prompts based on a newly collected large-scale text-to-image
dataset with over 300k prompts from 3115 users. Our rewriting model enhances
the expressiveness and alignment of user prompts with their intended visual
outputs. Experimental results demonstrate the superiority of our methods over
baseline approaches, as evidenced in our new offline evaluation method and
online tests. Our code and dataset are available at
https://github.com/zzjchen/Tailored-Visions .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09912">Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models. (arXiv:2310.09912v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zijian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Luping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhijie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>We propose the first unsupervised and learning-based method to identify
interpretable directions in h-space of pre-trained diffusion models. Our method
is derived from an existing technique that operates on the GAN latent space.
Specifically, we employ a shift control module that works on h-space of
pre-trained diffusion models to manipulate a sample into a shifted version of
itself, followed by a reconstructor to reproduce both the type and the strength
of the manipulation. By jointly optimizing them, the model will spontaneously
discover disentangled and interpretable directions. To prevent the discovery of
meaningless and destructive directions, we employ a discriminator to maintain
the fidelity of shifted sample. Due to the iterative generative process of
diffusion models, our training requires a substantial amount of GPU VRAM to
store numerous intermediate tensors for back-propagating gradient. To address
this issue, we propose a general VRAM-efficient training algorithm based on
gradient checkpointing technique to back-propagate any gradient through the
whole generative process, with acceptable occupancy of VRAM and sacrifice of
training efficiency. Compared with existing related works on diffusion models,
our method inherently identifies global and scalable directions, without
necessitating any other complicated procedures. Extensive experiments on
various datasets demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11346">Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing. (arXiv:2310.11346v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1">Qing Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1">Dalong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingcong Chen</a></p>
<p>Detecting objects in 3D space using multiple cameras, known as Multi-Camera
3D Object Detection (MC3D-Det), has gained prominence with the advent of
bird's-eye view (BEV) approaches. However, these methods often struggle when
faced with unfamiliar testing environments due to the lack of diverse training
data encompassing various viewpoints and environments. To address this, we
propose a novel method that aligns 3D detection with 2D camera plane results,
ensuring consistent and accurate detections. Our framework, anchored in
perspective debiasing, helps the learning of features resilient to domain
shifts. In our approach, we render diverse view maps from BEV features and
rectify the perspective bias of these maps, leveraging implicit foreground
volumes to bridge the camera and BEV planes. This two-step process promotes the
learning of perspective- and context-independent features, crucial for accurate
object detection across varying viewpoints, camera parameters and environment
conditions. Notably, our model-agnostic approach preserves the original network
structure without incurring additional inference costs, facilitating seamless
integration across various models and simplifying deployment. Furthermore, we
also show our approach achieves satisfactory results in real data when trained
only with virtual datasets, eliminating the need for real scene annotations.
Experimental results on both Domain Generalization (DG) and Unsupervised Domain
Adaptation (UDA) clearly demonstrate its effectiveness. Our code will be
released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19380">TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition. (arXiv:2310.19380v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lou_M/0/1/0/all/0/1">Meng Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong-Yu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sibei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yizhou Yu</a></p>
<p>Recent studies have integrated convolution into transformers to introduce
inductive bias and improve generalization performance. However, the static
nature of conventional convolution prevents it from dynamically adapting to
input variations, resulting in a representation discrepancy between convolution
and self-attention as self-attention calculates attention matrices dynamically.
Furthermore, when stacking token mixers that consist of convolution and
self-attention to form a deep network, the static nature of convolution hinders
the fusion of features previously generated by self-attention into convolution
kernels. These two limitations result in a sub-optimal representation capacity
of the constructed networks. To find a solution, we propose a lightweight Dual
Dynamic Token Mixer (D-Mixer) that aggregates global information and local
details in an input-dependent way. D-Mixer works by applying an efficient
global attention module and an input-dependent depthwise convolution separately
on evenly split feature segments, endowing the network with strong inductive
bias and an enlarged effective receptive field. We use D-Mixer as the basic
building block to design TransXNet, a novel hybrid CNN-Transformer vision
backbone network that delivers compelling performance. In the ImageNet-1K image
classification task, TransXNet-T surpasses Swin-T by 0.3% in top-1 accuracy
while requiring less than half of the computational cost. Furthermore,
TransXNet-S and TransXNet-B exhibit excellent model scalability, achieving
top-1 accuracy of 83.8% and 84.6% respectively, with reasonable computational
costs. Additionally, our proposed network architecture demonstrates strong
generalization capabilities in various dense prediction tasks, outperforming
other state-of-the-art networks while having lower computational costs. Code is
available at https://github.com/LMMMEng/TransXNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01714">EXIM: A Hybrid Explicit-Implicit Representation for Text-Guided 3D Shape Generation. (arXiv:2311.01714v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengzhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jingyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1">Ka-Hei Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiaojuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chi-Wing Fu</a></p>
<p>This paper presents a new text-guided technique for generating 3D shapes. The
technique leverages a hybrid 3D shape representation, namely EXIM, combining
the strengths of explicit and implicit representations. Specifically, the
explicit stage controls the topology of the generated 3D shapes and enables
local modifications, whereas the implicit stage refines the shape and paints it
with plausible colors. Also, the hybrid approach separates the shape and color
and generates color conditioned on shape to ensure shape-color consistency.
Unlike the existing state-of-the-art methods, we achieve high-fidelity shape
generation from natural-language descriptions without the need for
time-consuming per-shape optimization or reliance on human-annotated texts
during training or test-time optimization. Further, we demonstrate the
applicability of our approach to generate indoor scenes with consistent styles
using text-induced 3D shapes. Through extensive experiments, we demonstrate the
compelling quality of our results and the high coherency of our generated
shapes with the input texts, surpassing the performance of existing methods by
a significant margin. Codes and models are released at
https://github.com/liuzhengzhe/EXIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01723">Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1">Changdae Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Mijoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1">Hyesu Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Junhyeok Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1">Euiseog Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhi-Qi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kyungwoo Song</a></p>
<p>While fine-tuning unlocks the potential of a pre-trained model for a specific
task, it compromises the model's ability to generalize to out-of-distribution
(OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance
on OOD datasets as well as on an in-distribution (ID) dataset for which the
model is being tuned. However, another criterion for reliable machine learning
(ML), confidence calibration, has been overlooked despite its increasing demand
for real-world high-stakes ML applications (e.g., autonomous driving and
medical diagnosis). For the first time, we raise concerns about the calibration
of fine-tuned vision-language models (VLMs) under distribution shift by showing
that naive fine-tuning and even state-of-the-art robust fine-tuning methods
hurt the calibration of pre-trained VLMs, especially on OOD datasets. To
address this issue, we provide a simple approach, called calibrated robust
fine-tuning (CaRot), that incentivizes calibration and robustness on both ID
and OOD datasets. Empirical results on ImageNet-1K distribution shift
evaluation verify the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03054">AnyText: Multilingual Visual Text Generation And Editing. (arXiv:2311.03054v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tuo_Y/0/1/0/all/0/1">Yuxiang Tuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wangmeng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jun-Yan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yifeng Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a></p>
<p>Diffusion model based Text-to-Image has achieved impressive achievements
recently. Although current technology for synthesizing images is highly
advanced and capable of generating images with high fidelity, it is still
possible to give the show away when focusing on the text area in the generated
image. To address this issue, we introduce AnyText, a diffusion-based
multilingual visual text generation and editing model, that focuses on
rendering accurate and coherent text in the image. AnyText comprises a
diffusion pipeline with two primary elements: an auxiliary latent module and a
text embedding module. The former uses inputs like text glyph, position, and
masked image to generate latent features for text generation or editing. The
latter employs an OCR model for encoding stroke data as embeddings, which blend
with image caption embeddings from the tokenizer to generate texts that
seamlessly integrate with the background. We employed text-control diffusion
loss and text perceptual loss for training to further enhance writing accuracy.
AnyText can write characters in multiple languages, to the best of our
knowledge, this is the first work to address multilingual visual text
generation. It is worth mentioning that AnyText can be plugged into existing
diffusion models from the community for rendering or editing text accurately.
After conducting extensive evaluation experiments, our method has outperformed
all other approaches by a significant margin. Additionally, we contribute the
first large-scale multilingual text images dataset, AnyWord-3M, containing 3
million image-text pairs with OCR annotations in multiple languages. Based on
AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual
text generation accuracy and quality. Our project will be open-sourced on
https://github.com/tyxsspa/AnyText to improve and promote the development of
text generation technology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11810">DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding. (arXiv:2311.11810v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wengang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Can Huang</a></p>
<p>This work presents DocPedia, a novel large multimodal model (LMM) for
versatile OCR-free document understanding, capable of parsing images up to
2,560$\times$2,560 resolution. Unlike existing work either struggle with
high-resolution documents or give up the large language model thus vision or
language ability constrained, our DocPedia directly processes visual input in
the frequency domain rather than the pixel space. The unique characteristic
enables DocPedia to capture a greater amount of visual and textual information
using a limited number of visual tokens. To consistently enhance both
perception and comprehension abilities of our model, we develop a dual-stage
training strategy and enrich instructions/annotations of all training tasks
covering multiple document types. Extensive quantitative and qualitative
experiments conducted on various publicly available benchmarks confirm the
mutual benefits of jointly learning perception and comprehension tasks. The
results provide further evidence of the effectiveness and superior performance
of our DocPedia over other methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12067">Quality and Quantity: Unveiling a Million High-Quality Images for Text-to-Image Synthesis in Fashion Design. (arXiv:2311.12067v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jia Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lichao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Fayu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1">MiaoMiao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_F/0/1/0/all/0/1">Fangsheng Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Lili Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>The fusion of AI and fashion design has emerged as a promising research area.
However, the lack of extensive, interrelated data on clothing and try-on stages
has hindered the full potential of AI in this domain. Addressing this, we
present the Fashion-Diffusion dataset, a product of multiple years' rigorous
effort. This dataset, the first of its kind, comprises over a million
high-quality fashion images, paired with detailed text descriptions. Sourced
from a diverse range of geographical locations and cultural backgrounds, the
dataset encapsulates global fashion trends. The images have been meticulously
annotated with fine-grained attributes related to clothing and humans,
simplifying the fashion design process into a Text-to-Image (T2I) task. The
Fashion-Diffusion dataset not only provides high-quality text-image pairs and
diverse human-garment pairs but also serves as a large-scale resource about
humans, thereby facilitating research in T2I generation. Moreover, to foster
standardization in the T2I-based fashion design field, we propose a new
benchmark comprising multiple datasets for evaluating the performance of
fashion design models. This work represents a significant leap forward in the
realm of AI-driven fashion design, setting a new standard for future research
in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12704">Cascade Learning Localises Discriminant Features in Visual Scene Classification. (arXiv:2311.12704v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Farrahi_K/0/1/0/all/0/1">Katayoun Farrahi</a></p>
<p>Lack of interpretability of deep convolutional neural networks (DCNN) is a
well-known problem particularly in the medical domain as clinicians want
trustworthy automated decisions. One way to improve trust is to demonstrate the
localisation of feature representations with respect to expert labeled regions
of interest. In this work, we investigate the localisation of features learned
via two varied learning paradigms and demonstrate the superiority of one
learning approach with respect to localisation. Our analysis on medical and
natural datasets show that the traditional end-to-end (E2E) learning strategy
has a limited ability to localise discriminative features across multiple
network layers. We show that a layer-wise learning strategy, namely cascade
learning (CL), results in more localised features. Considering localisation
accuracy, we not only show that CL outperforms E2E but that it is a promising
method of predicting regions. On the YOLO object detection framework, our best
result shows that CL outperforms the E2E scheme by $2\%$ in mAP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12831">ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets. (arXiv:2311.12831v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Kaiyuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoli Wang</a></p>
<p>Due to its conceptual simplicity and generality, compressive neural
representation has emerged as a promising alternative to traditional
compression methods for managing massive volumetric datasets. The current
practice of neural compression utilizes a single large multilayer perceptron
(MLP) to encode the global volume, incurring slow training and inference. This
paper presents an efficient compressive neural representation (ECNR) solution
for time-varying data compression, utilizing the Laplacian pyramid for adaptive
signal fitting. Following a multiscale structure, we leverage multiple small
MLPs at each scale for fitting local content or residual blocks. By assigning
similar blocks to the same MLP via size uniformization, we enable balanced
parallelization among MLPs to significantly speed up training and inference.
Working in concert with the multiscale structure, we tailor a deep compression
strategy to compact the resulting model. We show the effectiveness of ECNR with
multiple datasets and compare it with state-of-the-art compression methods
(mainly SZ3, TTHRESH, and neurcomp). The results position ECNR as a promising
solution for volumetric data compression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14049">Assessment of Deep Learning Segmentation for Real-Time Free-Breathing Cardiac Magnetic Resonance Imaging. (arXiv:2311.14049v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Schilling_M/0/1/0/all/0/1">Martin Schilling</a>, <a href="http://arxiv.org/find/eess/1/au:+Unterberg_Buchwald_C/0/1/0/all/0/1">Christina Unterberg-Buchwald</a>, <a href="http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1">Joachim Lotz</a>, <a href="http://arxiv.org/find/eess/1/au:+Uecker_M/0/1/0/all/0/1">Martin Uecker</a></p>
<p>In recent years, a variety of deep learning networks for cardiac MRI (CMR)
segmentation have been developed and analyzed. However, nearly all of them are
focused on cine CMR under breathold. In this work, accuracy of deep learning
methods is assessed for volumetric analysis (via segmentation) of the left
ventricle in real-time free-breathing CMR at rest and under exercise stress.
Data from healthy volunteers (n=15) for cine and real-time free-breathing CMR
were analyzed retrospectively. Segmentations of a commercial software (comDL)
and a freely available neural network (nnU-Net), were compared to a reference
created via the manual correction of comDL segmentation. Segmentation of left
ventricular endocardium (LV), left ventricular myocardium (MYO), and right
ventricle (RV) is evaluated for both end-systolic and end-diastolic phases and
analyzed with Dice's coefficient (DC). The volumetric analysis includes LV
end-diastolic volume (EDV), LV end-systolic volume (ESV), and LV ejection
fraction (EF). For cine CMR, nnU-Net and comDL achieve a DC above 0.95 for LV
and 0.9 for MYO, and RV. For real-time CMR, the accuracy of nnU-Net exceeds
that of comDL overall. For real-time CMR at rest, nnU-Net achieves a DC of 0.94
for LV, 0.89 for MYO, and 0.90 for RV; mean absolute differences between
nnU-Net and reference are 2.9mL for EDV, 3.5mL for ESV and 2.6% for EF. For
real-time CMR under exercise stress, nnU-Net achieves a DC of 0.92 for LV, 0.85
for MYO, and 0.83 for RV; mean absolute differences between nnU-Net and
reference are 11.4mL for EDV, 2.9mL for ESV and 3.6% for EF. Deep learning
methods designed or trained for cine CMR segmentation can perform well on
real-time CMR. For real-time free-breathing CMR at rest, the performance of
deep learning methods is comparable to inter-observer variability in cine CMR
and is usable or fully automatic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14631">CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image Personalization. (arXiv:2311.14631v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Ruoyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingrui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Shiyin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>We propose CatVersion, an inversion-based method that learns the personalized
concept through a handful of examples. Subsequently, users can utilize text
prompts to generate images that embody the personalized concept, thereby
achieving text-to-image personalization. In contrast to existing approaches
that emphasize word embedding learning or parameter fine-tuning for the
diffusion model, which potentially causes concept dilution or overfitting, our
method concatenates embeddings on the feature-dense space of the text encoder
in the diffusion model to learn the gap between the personalized concept and
its base class, aiming to maximize the preservation of prior knowledge in
diffusion models while restoring the personalized concepts. To this end, we
first dissect the text encoder's integration in the image generation process to
identify the feature-dense space of the encoder. Afterward, we concatenate
embeddings on the Keys and Values in this space to learn the gap between the
personalized concept and its base class. In this way, the concatenated
embeddings ultimately manifest as a residual on the original attention output.
To more accurately and unbiasedly quantify the results of personalized image
generation, we improve the CLIP image alignment score based on masks.
Qualitatively and quantitatively, CatVersion helps to restore personalization
concepts more faithfully and enables more robust editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14656">Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. (arXiv:2311.14656v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1">Jonathan Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Luddecke_T/0/1/0/all/0/1">Timo L&#xfc;ddecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1">Rehan Sheikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1">Samuel Albanie</a></p>
<p>Multimodal large language models (MLLMs) have shown remarkable capabilities
across a broad range of tasks but their knowledge and abilities in the
geographic and geospatial domains are yet to be explored, despite potential
wide-ranging benefits to navigation, environmental research, urban development,
and disaster response. We conduct a series of experiments exploring various
vision capabilities of MLLMs within these domains, particularly focusing on the
frontier model GPT-4V, and benchmark its performance against open-source
counterparts. Our methodology involves challenging these models with a
small-scale geographic benchmark consisting of a suite of visual tasks, testing
their abilities across a spectrum of complexity. The analysis uncovers not only
where such models excel, including instances where they outperform humans, but
also where they falter, providing a balanced view of their capabilities in the
geographic domain. To enable the comparison and evaluation of future models,
our benchmark will be publicly released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14837">Benchmarking Robustness of Text-Image Composed Retrieval. (arXiv:2311.14837v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shitong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1">Shaogang Gong</a></p>
<p>Text-image composed retrieval aims to retrieve the target image through the
composed query, which is specified in the form of an image plus some text that
describes desired modifications to the input image. It has recently attracted
attention due to its ability to leverage both information-rich images and
concise language to precisely express the requirements for target images.
However, the robustness of these approaches against real-world corruptions or
further text understanding has never been studied. In this paper, we perform
the first robustness study and establish three new diversified benchmarks for
systematic analysis of text-image composed retrieval against natural
corruptions in both vision and text and further probe textural understanding.
For natural corruption analysis, we introduce two new large-scale benchmark
datasets, CIRR-C and FashionIQ-C for testing in open domain and fashion domain
respectively, both of which apply 15 visual corruptions and 7 textural
corruptions. For textural understanding analysis, we introduce a new diagnostic
dataset CIRR-D by expanding the original raw data with synthetic data, which
contains modified text to better probe textual understanding ability including
numerical variation, attribute variation, object removal, background variation,
and fine-grained evaluation. The code and benchmark datasets are available at
https://github.com/SunTongtongtong/Benchmark-Robustness-Text-Image-Compose-Retrieval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14897">Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network. (arXiv:2311.14897v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Bozhong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shenghua Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingna Wu</a></p>
<p>Recently, 3D anomaly detection, a crucial problem involving fine-grained
geometry discrimination, is getting more attention. However, the lack of
abundant real 3D anomaly data limits the scalability of current models. To
enable scalable anomaly data collection, we propose a 3D anomaly synthesis
pipeline to adapt existing large-scale 3Dmodels for 3D anomaly detection.
Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, basedon
ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40
categories, which provides a rich and varied collection of data, enabling
efficient training and enhancing adaptability to industrial scenarios.
Meanwhile,to enable scalable representation learning for 3D anomaly
localization, we propose a self-supervised method, i.e., Iterative Mask
Reconstruction Network (IMRNet). During training, we propose a geometry-aware
sample module to preserve potentially anomalous local regions during point
cloud down-sampling. Then, we randomly mask out point patches and sent the
visible patches to a transformer for reconstruction-based self-supervision.
During testing, the point cloud repeatedly goes through the Mask Reconstruction
Network, with each iteration's output becoming the next input. By merging and
contrasting the final reconstructed point cloud with the initial input, our
method successfully locates anomalies. Experiments show that IMRNet outperforms
previous state-of-the-art methods, achieving 66.1% in I-AUC on Anomaly-ShapeNet
dataset and 72.5% in I-AUC on Real3D-AD dataset. Our dataset will be released
at https://github.com/Chopper-233/Anomaly-ShapeNet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14948">Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Sahil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1">Soumye Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav Mohanty Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1">Chirag Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1">Jeff Bilmes</a></p>
<p>Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach. In this work, we demonstrate that the efficacy of
CleanCLIP in mitigating backdoors is highly dependent on the particular
objective used during model pre-training. We observe that stronger pre-training
objectives correlate with harder to remove backdoors behaviors. We show this by
training multimodal models on two large datasets consisting of 3 million (CC3M)
and 6 million (CC6M) datapoints, under various pre-training objectives,
followed by poison removal using CleanCLIP. We find that CleanCLIP is
ineffective when stronger pre-training objectives are used, even with extensive
hyperparameter tuning. Our findings underscore critical considerations for ML
practitioners who pre-train models using large-scale web-curated data and are
concerned about potential backdoor threats. Notably, our results suggest that
simpler pre-training objectives are more amenable to effective backdoor
removal. This insight is pivotal for practitioners seeking to balance the
trade-offs between using stronger pre-training objectives and security against
backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15010">Adapter is All You Need for Tuning Visual Tasks. (arXiv:2311.15010v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dongshuo Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Leiyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Youqun Zhang</a></p>
<p>Pre-training &amp; fine-tuning can enhance the transferring efficiency and
performance in visual tasks. Recent delta-tuning methods provide more options
for visual classification tasks. Despite their success, existing visual
delta-tuning art fails to exceed the upper limit of full fine-tuning on
challenging tasks like instance segmentation and semantic segmentation. To find
a competitive alternative to full fine-tuning, we propose the Multi-cognitive
Visual Adapter (Mona) tuning, a novel adapter-based tuning method. First, we
introduce multiple vision-friendly filters into the adapter to enhance its
ability to process visual signals, while previous methods mainly rely on
language-friendly linear filters. Second, we add the scaled normalization layer
in the adapter to regulate the distribution of input features for visual
filters. To fully demonstrate the practicality and generality of Mona, we
conduct experiments on multiple representative visual tasks, including instance
segmentation on COCO, semantic segmentation on ADE20K, object detection on
Pascal VOC, and image classification on several common datasets. Exciting
results illustrate that Mona surpasses full fine-tuning on all these tasks and
is the only delta-tuning method outperforming full fine-tuning on instance
segmentation and semantic segmentation tasks. For example, Mona achieves a 1%
performance gain on the COCO dataset compared to full fine-tuning.
Comprehensive results suggest that Mona-tuning is more suitable for retaining
and utilizing the capabilities of pre-trained models than full fine-tuning. The
code will be released at https://github.com/Leiyi-Hu/mona.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15202">Dual-stream contrastive predictive network with joint handcrafted feature view for SAR ship classification. (arXiv:2311.15202v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xianting Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+zheng_H/0/1/0/all/0/1">Hao zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhigang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Meiguang Zheng</a></p>
<p>Most existing synthetic aperture radar (SAR) ship classification technologies
heavily rely on correctly labeled data, ignoring the discriminative features of
unlabeled SAR ship images. Even though researchers try to enrich CNN-based
features by introducing traditional handcrafted features, existing methods
easily cause information redundancy and fail to capture the interaction between
them. To address these issues, we propose a novel dual-stream contrastive
predictive network (DCPNet), which consists of two asymmetric task designs and
the false negative sample elimination module. The first task is to construct
positive sample pairs, guiding the core encoder to learn more general
representations. The second task is to encourage adaptive capture of the
correspondence between deep features and handcrated features, achieving
knowledge transfer within the model, and effectively improving the redundancy
caused by the feature fusion. To increase the separability between clusters, we
also design a cluster-level tasks. The experimental results on OpenSARShip and
FUSAR-Ship datasets demonstrate the improvement in classification accuracy of
supervised models and confirm the capability of learning effective
representations of DCPNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15414">KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All. (arXiv:2311.15414v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quyen Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1">Lam Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Than_K/0/1/0/all/0/1">Khoat Than</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Toan Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1">Dinh Phung</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Trung Le</a></p>
<p>Drawing inspiration from prompt tuning techniques applied to Large Language
Models, recent methods based on pre-trained ViT networks have achieved
remarkable results in the field of Continual Learning. Specifically, these
approaches propose to maintain a set of prompts and allocate a subset of them
to learn each task using a key-query matching strategy. However, they may
encounter limitations when lacking control over the correlations between old
task queries and keys of future tasks, the shift of features in the latent
space, and the relative separation of latent vectors learned in independent
tasks. In this work, we introduce a novel key-query learning strategy based on
orthogonal projection, inspired by model-agnostic meta-learning, to enhance
prompt matching efficiency and address the challenge of shifting features.
Furthermore, we introduce a One-Versus-All (OVA) prototype-based component that
enhances the classification head distinction. Experimental results on benchmark
datasets demonstrate that our method empowers the model to achieve results
surpassing those of current state-of-the-art approaches by a large margin of up
to 20%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15773">Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation. (arXiv:2311.15773v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Biao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siteng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a></p>
<p>Diffusion models have recently achieved remarkable progress in generating
realistic images. However, challenges remain in accurately understanding and
synthesizing the layout requirements in the textual prompts. To align the
generated image with layout instructions, we present a training-free layout
calibration system SimM that intervenes in the generative process on the fly
during inference time. Specifically, following a "check-locate-rectify"
pipeline, the system first analyses the prompt to generate the target layout
and compares it with the intermediate outputs to automatically detect errors.
Then, by moving the located activations and making intra- and inter-map
adjustments, the rectification process can be performed with negligible
computational overhead. To evaluate SimM over a range of layout requirements,
we present a benchmark SimMBench that compensates for the lack of superlative
spatial relations in existing datasets. And both quantitative and qualitative
results demonstrate the effectiveness of the proposed SimM in calibrating the
layout inconsistencies. Our project page is at https://simm-t2i.github.io/SimM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15841">Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation. (arXiv:2311.15841v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siteng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Biao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yuqian Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>This study focuses on a novel task in text-to-image (T2I) generation, namely
action customization. The objective of this task is to learn the co-existing
action from limited data and generalize it to unseen humans or even animals.
Experimental results show that existing subject-driven customization methods
fail to learn the representative characteristics of actions and struggle in
decoupling actions from context features, including appearance. To overcome the
preference for low-level features and the entanglement of high-level features,
we propose an inversion-based method Action-Disentangled Identifier (ADI) to
learn action-specific identifiers from the exemplar images. ADI first expands
the semantic conditioning space by introducing layer-wise identifier tokens,
thereby increasing the representational richness while distributing the
inversion across different features. Then, to block the inversion of
action-agnostic features, ADI extracts the gradient invariance from the
constructed sample triples and masks the updates of irrelevant channels. To
comprehensively evaluate the task, we present an ActionBench that includes a
variety of actions, each accompanied by meticulously selected samples. Both
quantitative and qualitative results show that our ADI outperforms existing
baselines in action-customized T2I generation. Our project page is at
https://adi-t2i.github.io/ADI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16102">Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback. (arXiv:2311.16102v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1">Mihir Prabhudesai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_T/0/1/0/all/0/1">Tsung-Wei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Alexander C. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1">Katerina Fragkiadaki</a></p>
<p>The advancements in generative modeling, particularly the advent of diffusion
models, have sparked a fundamental question: how can these models be
effectively used for discriminative tasks? In this work, we find that
generative models can be great test-time adapters for discriminative models.
Our method, Diffusion-TTA, adapts pre-trained discriminative models such as
image classifiers, segmenters and depth predictors, to each unlabelled example
in the test set using generative feedback from a diffusion model. We achieve
this by modulating the conditioning of the diffusion model using the output of
the discriminative model. We then maximize the image likelihood objective by
backpropagating the gradients to discriminative model's parameters. We show
Diffusion-TTA significantly enhances the accuracy of various large-scale
pre-trained discriminative models, such as, ImageNet classifiers, CLIP models,
image pixel labellers and image depth predictors. Diffusion-TTA outperforms
existing test-time adaptation methods, including TTT-MAE and TENT, and
particularly shines in online adaptation setups, where the discriminative model
is continually adapted to each example in the test set. We provide access to
code, results, and visualizations on our website:
https://diffusion-tta.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16482">Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars. (arXiv:2311.16482v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1">Minghan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qinwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoqian Wang</a></p>
<p>Neural radiance fields are capable of reconstructing high-quality drivable
human avatars but are expensive to train and render. To reduce consumption, we
propose Animatable 3D Gaussian, which learns human avatars from input images
and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of
skinned 3D Gaussians and a corresponding skeleton in canonical space and
deforming 3D Gaussians to posed space according to the input poses. We
introduce hash-encoded shape and appearance to speed up training and propose
time-dependent ambient occlusion to achieve high-quality reconstructions in
scenes containing complex motions and dynamic shadows. On both novel view
synthesis and novel pose synthesis tasks, our method outperforms existing
methods in terms of training time, rendering speed, and reconstruction quality.
Our method can be easily extended to multi-human scenes and achieve comparable
novel view synthesis results on a scene with ten people in only 25 seconds of
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16512">CoSeR: Bridging Image and Language for Cognitive Super-Resolution. (arXiv:2311.16512v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haoze Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianzhuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_R/0/1/0/all/0/1">Renjing Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyi Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Youliang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>Existing super-resolution (SR) models primarily focus on restoring local
texture details, often neglecting the global semantic information within the
scene. This oversight can lead to the omission of crucial semantic details or
the introduction of inaccurate textures during the recovery process. In our
work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering
SR models with the capacity to comprehend low-resolution images. We achieve
this by marrying image appearance and language understanding to generate a
cognitive embedding, which not only activates prior information from large
text-to-image diffusion models but also facilitates the generation of
high-quality reference images to optimize the SR process. To further improve
image fidelity, we propose a novel condition injection scheme called
"All-in-Attention", consolidating all conditional information into a single
module. Consequently, our method successfully restores semantically correct and
photorealistic details, demonstrating state-of-the-art performance across
multiple benchmarks. Code: https://github.com/VINHYU/CoSeR
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16703">CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs. (arXiv:2311.16703v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haocheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Hao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bousseau_A/0/1/0/all/0/1">Adrien Bousseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changjian Li</a></p>
<p>CAD programs are a popular way to compactly encode shapes as a sequence of
operations that are easy to parametrically modify. However, without sufficient
semantic comments and structure, such programs can be challenging to
understand, let alone modify. We introduce the problem of semantic commenting
CAD programs, wherein the goal is to segment the input program into code blocks
corresponding to semantically meaningful shape parts and assign a semantic
label to each block. We solve the problem by combining program parsing with
visual-semantic analysis afforded by recent advances in foundational language
and vision models. Specifically, by executing the input programs, we create
shapes, which we use to generate conditional photorealistic images to make use
of semantic annotators for such images. We then distill the information across
the images and link back to the original programs to semantically comment on
them. Additionally, we collected and annotated a benchmark dataset, CADTalk,
consisting of 5,280 machine-made programs and 45 human-made programs with
ground truth semantic comments to foster future research. We extensively
evaluated our approach, compared to a GPT-based baseline approach, and an
open-set shape segmentation baseline, i.e., PartSLIP, and reported an 83.24%
accuracy on the new CADTalk dataset. Project page:
https://enigma-li.github.io/CADTalk/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16926">LLaFS: When Large-Language Models Meet Few-Shot Segmentation. (arXiv:2311.16926v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lanyun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianrun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1">Deyi Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jieping Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>This paper proposes LLaFS, the first attempt to leverage large language
models (LLMs) in few-shot segmentation. In contrast to the conventional
few-shot segmentation methods that only rely on the limited and biased
information from the annotated support images, LLaFS leverages the vast prior
knowledge gained by LLM as an effective supplement and directly uses the LLM to
segment images in a few-shot manner. To enable the text-based LLM to handle
image-related tasks, we carefully design an input instruction that allows the
LLM to produce segmentation results represented as polygons, and propose a
region-attribute table to simulate the human visual mechanism and provide
multi-modal guidance. We also synthesize pseudo samples and use curriculum
learning for pretraining to augment data and achieve better optimization. LLaFS
achieves state-of-the-art results on multiple datasets, showing the potential
of using LLMs for few-shot computer vision tasks. Code will be available at
https://github.com/lanyunzhu99/LLaFS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17002">Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following. (arXiv:2311.17002v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Biao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Di Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a></p>
<p>Existing text-to-image (T2I) diffusion models usually struggle in
interpreting complex prompts, especially those with quantity, object-attribute
binding, and multi-subject descriptions. In this work, we introduce a semantic
panel as the middleware in decoding texts to images, supporting the generator
to better follow instructions. The panel is obtained through arranging the
visual concepts parsed from the input text by the aid of large language models,
and then injected into the denoising network as a detailed control signal to
complement the text condition. To facilitate text-to-panel learning, we come up
with a carefully designed semantic formatting protocol, accompanied by a
fully-automatic data preparation pipeline. Thanks to such a design, our
approach, which we call Ranni, manages to enhance a pre-trained T2I generator
regarding its textual controllability. More importantly, the introduction of
the generative middleware brings a more convenient form of interaction (i.e.,
directly adjusting the elements in the panel or using language instructions)
and further allows users to finely customize their generation, based on which
we develop a practical system and showcase its potential in continuous
generation and chatting-based editing. Our project page is at
https://ranni-t2i.github.io/Ranni.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17361">How does spatial structure affect psychological restoration? A method based on Graph Neural Networks and Street View Imagery. (arXiv:2311.17361v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoran Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1">Pengyu Zhu</a></p>
<p>The Attention Restoration Theory (ART) presents a theoretical framework with
four essential indicators (being away, extent, fascinating, and compatibility)
for comprehending urban and natural restoration quality. However, previous
studies relied on non-sequential data and non-spatial dependent methods, which
overlooks the impact of spatial structure defined here as the positional
relationships between scene entities on restoration quality. The past methods
also make it challenging to measure restoration quality on an urban scale. In
this work, a spatial-dependent graph neural networks (GNNs) approach is
proposed to reveal the relation between spatial structure and restoration
quality on an urban scale. Specifically, we constructed two different types of
graphs at the street and city levels. The street-level graphs, using sequential
street view images (SVIs) of road segments to capture position relationships
between entities, were used to represent spatial structure. The city-level
graph, modeling the topological relationships of roads as non-Euclidean data
structures and embedding urban features (including Perception-features,
Spatial-features, and Socioeconomic-features), was used to measure restoration
quality. The results demonstrate that: 1) spatial-dependent GNNs model
outperforms traditional methods (Acc = 0.735, F1 = 0.732); 2) spatial structure
portrayed through sequential SVIs data significantly influences restoration
quality; 3) spaces with the same restoration quality exhibited distinct spatial
structures patterns. This study clarifies the association between spatial
structure and restoration quality, providing a new perspective to improve urban
well-being in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17396">Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset. (arXiv:2311.17396v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1">Yujin Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Eunsue Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngchan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1">Yunseong Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Omer_K/0/1/0/all/0/1">Khalid Omer</a>, <a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1">Felix Heide</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Seung-Hwan Baek</a></p>
<p>Image datasets are essential not only in validating existing methods in
computer vision but also in developing new methods. Most existing image
datasets focus on trichromatic intensity images to mimic human vision. However,
polarization and spectrum, the wave properties of light that animals in harsh
environments and with limited brain capacity often rely on, remain
underrepresented in existing datasets. Although spectro-polarimetric datasets
exist, these datasets have insufficient object diversity, limited illumination
conditions, linear-only polarization data, and inadequate image count. Here, we
introduce two spectro-polarimetric datasets: trichromatic Stokes images and
hyperspectral Stokes images. These novel datasets encompass both linear and
circular polarization; they introduce multiple spectral channels; and they
feature a broad selection of real-world scenes. With our dataset in hand, we
analyze the spectro-polarimetric image statistics, develop efficient
representations of such high-dimensional data, and evaluate spectral dependency
of shape-from-polarization methods. As such, the proposed dataset promises a
foundation for data-driven spectro-polarimetric imaging and vision research.
Dataset and code will be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17409">Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation. (arXiv:2311.17409v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khungurn_P/0/1/0/all/0/1">Pramook Khungurn</a></p>
<p>We study the problem of creating a character model that can be controlled in
real time from a single image of an anime character. A solution to this problem
would greatly reduce the cost of creating avatars, computer games, and other
interactive applications.
</p>
<p>Talking Head Anime 3 (THA3) is an open source project that attempts to
directly address the problem. It takes as input (1) an image of an anime
character's upper body and (2) a 45-dimensional pose vector and outputs a new
image of the same character taking the specified pose. The range of possible
movements is expressive enough for personal avatars and certain types of game
characters. However, the system is too slow to generate animations in real time
on common PCs, and its image quality can be improved.
</p>
<p>In this paper, we improve THA3 in two ways. First, we propose new
architectures for constituent networks that rotate the character's head and
body based on U-Nets with attention that are widely used in modern generative
models. The new architectures consistently yield better image quality than the
THA3 baseline. Nevertheless, they also make the whole system much slower: it
takes up to 150 milliseconds to generate a frame. Second, we propose a
technique to distill the system into a small network (less than 2 MB) that can
generate 512x512 animation frames in real time (under 30 FPS) using consumer
gaming GPUs while keeping the image quality close to that of the full system.
This improvement makes the whole system practical for real-time applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17460">W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera Calibration and Orientation Correction. (arXiv:2311.17460v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yunlian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jinhui Tang</a></p>
<p>For a long time, in the field of reconstructing 3D human bodies from
monocular images, most methods opted to simplify the task by minimizing the
influence of the camera. Using a coarse focal length setting results in the
reconstructed bodies not aligning well with distorted images. Ignoring camera
rotation leads to an unrealistic reconstructed body pose in world space.
Consequently, existing methods' application scenarios are confined to
controlled environments. And they struggle to achieve accurate and reasonable
reconstruction in world space when confronted with complex and diverse
in-the-wild images. To address the above issues, we propose W-HMR, which
decouples global body recovery into camera calibration, local body recovery and
global body orientation correction. We design the first weak-supervised camera
calibration method for body distortion, eliminating dependence on focal length
labels and achieving finer mesh-image alignment. We propose a novel orientation
correction module to allow the reconstructed human body to remain normal in
world space. Decoupling body orientation and body pose enables our model to
consider the accuracy in camera coordinate and the reasonableness in world
coordinate simultaneously, expanding the range of applications. As a result,
W-HMR achieves high-quality reconstruction in dual coordinate systems,
particularly in challenging scenes. Codes will be released on
https://yw0208.github.io/w-hmr/ after publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17510">StructRe: Rewriting for Structured Shape Modeling. (arXiv:2311.17510v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiepeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Hao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1">Xin Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>Man-made 3D shapes are naturally organized in parts and hierarchies; such
structures provide important constraints for shape reconstruction and
generation. Modeling shape structures is difficult, because there can be
multiple hierarchies for a given shape, causing ambiguity, and across different
categories the shape structures are correlated with semantics, limiting
generalization. We present StructRe, a structure rewriting system, as a novel
approach to structured shape modeling. Given a 3D object represented by points
and components, StructRe can rewrite it upward into more concise structures, or
downward into more detailed structures; by iterating the rewriting process,
hierarchies are obtained. Such a localized rewriting process enables
probabilistic modeling of ambiguous structures and robust generalization across
object categories. We train StructRe on PartNet data and show its
generalization to cross-category and multiple object hierarchies, and test its
extension to ShapeNet. We also demonstrate the benefits of probabilistic and
generalizable structure modeling for shape reconstruction, generation and
editing tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17597">Continual Self-supervised Learning: Towards Universal Multi-modal Medical Data Representation Learning. (arXiv:2311.17597v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yiwen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yutong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yong Xia</a></p>
<p>Self-supervised learning is an efficient pre-training method for medical
image analysis. However, current research is mostly confined to
specific-modality data pre-training, consuming considerable time and resources
without achieving universality across different modalities. A straightforward
solution is combining all modality data for joint self-supervised pre-training,
which poses practical challenges. Firstly, our experiments reveal conflicts in
representation learning as the number of modalities increases. Secondly,
multi-modal data collected in advance cannot cover all real-world scenarios. In
this paper, we reconsider versatile self-supervised learning from the
perspective of continual learning and propose MedCoSS, a continuous
self-supervised learning approach for multi-modal medical data. Unlike joint
self-supervised learning, MedCoSS assigns different modality data to different
training stages, forming a multi-stage pre-training process. To balance modal
conflicts and prevent catastrophic forgetting, we propose a rehearsal-based
continual learning method. We introduce the k-means sampling strategy to retain
data from previous modalities and rehearse it when learning new modalities.
Instead of executing the pretext task on buffer data, a feature distillation
strategy and an intra-modal mixup strategy are applied to these data for
knowledge retention. We conduct continuous self-supervised pre-training on a
large-scale multi-modal unlabeled dataset, including clinical reports, X-rays,
CT scans, MRI scans, and pathological images. Experimental results demonstrate
MedCoSS's exceptional generalization ability across nine downstream datasets
and its significant scalability in integrating new modality data. Code and
pre-trained weight are available at https://github.com/yeerwen/MedCoSS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17618">ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model. (arXiv:2311.17618v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Fukun Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Biao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zibo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiayuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Taihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a></p>
<p>The advent of large language models, enabling flexibility through
instruction-driven approaches, has revolutionized many traditional generative
tasks, but large models for 3D data, particularly in comprehensively handling
3D shapes with other modalities, are still under-explored. By achieving
instruction-based shape generations, versatile multimodal generative shape
models can significantly benefit various fields like 3D virtual construction
and network-aided design. In this work, we present ShapeGPT, a shape-included
multi-modal framework to leverage strong pre-trained language models to address
multiple shape-relevant tasks. Specifically, ShapeGPT employs a
word-sentence-paragraph framework to discretize continuous shapes into shape
words, further assembles these words for shape sentences, as well as integrates
shape with instructional text for multi-modal paragraphs. To learn this
shape-language model, we use a three-stage training scheme, including shape
representation, multimodal alignment, and instruction-based generation, to
align shape-language codebooks and learn the intricate correlations among these
modalities. Extensive experiments demonstrate that ShapeGPT achieves comparable
performance across shape-relevant tasks, including text-to-shape,
shape-to-text, shape completion, and shape editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17812">DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation. (arXiv:2311.17812v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wansen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Youkai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Quanjun Yin</a></p>
<p>Following language instructions to navigate in unseen environments is a
challenging task for autonomous embodied agents. With strong representation
capabilities, pretrained vision-and-language models are widely used in VLN.
However, most of them are trained on web-crawled general-purpose datasets,
which incurs a considerable domain gap when used for VLN tasks. To address the
problem, we propose a novel and model-agnostic domain-aware prompt learning
(DAP) framework. For equipping the pretrained models with specific object-level
and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost
prompt tuning paradigm to learn soft visual prompts for extracting in-domain
image semantics. Specifically, we first generate a set of in-domain image-text
pairs with the help of the CLIP model. Then we introduce soft visual prompts in
the input space of the visual encoder in a pretrained model. DAP injects
in-domain visual knowledge into the visual encoder of the pretrained model in
an efficient way. Experimental results on both R2R and REVERIE show the
superiority of DAP compared to existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17834">SPiC-E : Structural Priors in 3D Diffusion Models using Cross-Entity Attention. (arXiv:2311.17834v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sella_E/0/1/0/all/0/1">Etai Sella</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiebelman_G/0/1/0/all/0/1">Gal Fiebelman</a>, <a href="http://arxiv.org/find/cs/1/au:+Atia_N/0/1/0/all/0/1">Noam Atia</a>, <a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1">Hadar Averbuch-Elor</a></p>
<p>We are witnessing rapid progress in automatically generating and manipulating
3D assets due to the availability of pretrained text-image diffusion models.
However, time-consuming optimization procedures are required for synthesizing
each sample, hindering their potential for democratizing 3D content creation.
Conversely, 3D diffusion models now train on million-scale 3D datasets,
yielding high-quality text-conditional 3D samples within seconds. In this work,
we present SPiC-E - a neural network that adds structural guidance to 3D
diffusion models, extending their usage beyond text-conditional generation. At
its core, our framework introduces a cross-entity attention mechanism that
allows for multiple entities (in particular, paired input and guidance 3D
shapes) to interact via their internal representations within the denoising
network. We utilize this mechanism for learning task-specific structural priors
in 3D diffusion models from auxiliary guidance shapes. We show that our
approach supports a variety of applications, including 3D stylization, semantic
shape editing and text-conditional abstraction-to-3D, which transforms
primitive-based abstractions into highly-expressive shapes. Extensive
experiments demonstrate that SPiC-E achieves SOTA performance over these tasks
while often being considerably faster than alternative methods. Importantly,
this is accomplished without tailoring our approach for any specific task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17898">Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis. (arXiv:2311.17898v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jinqi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimos_D/0/1/0/all/0/1">Dimitris Dimos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1">Ren&#xe9; Vidal</a></p>
<p>Hallucinations and unfaithful synthesis due to inaccurate prompts with
insufficient semantic details are widely observed in multimodal generative
models. A prevalent strategy to align multiple modalities is to fine-tune the
generator with a large number of annotated text-image pairs. However, such a
procedure is labor-consuming and resource-draining. The key question we ask is:
can we enhance the quality and faithfulness of text-driven generative models
beyond extensive text-image pair annotations? To address this question, we
propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that
iteratively incorporates external knowledge to help generators produce reliable
visual content. Instead of training generators to handle generic prompts, KPP
employs a recursive knowledge query process to gather informative external
facts from the knowledge base, instructs a language model to compress the
acquired knowledge for prompt refinement, and utilizes text-driven generators
for visual synthesis. The entire process is zero-shot, without accessing the
architectures and parameters of generative models. We evaluate the framework
across multiple text-driven generative tasks (image, 3D rendering, and video)
on datasets of different domains. We further demonstrate the extensibility and
adaptability of KPP through varying foundation model bases and instructions.
Our results show that KPP is capable of generating faithful and semantically
rich content across diverse visual domains, offering a promising solution to
improve multimodal generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17921">Do text-free diffusion models learn discriminative visual representations?. (arXiv:2311.17921v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1">Soumik Mukhopadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1">Matthew Gwilliam</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_Y/0/1/0/all/0/1">Yosuke Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1">Vatsal Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Padmanabhan_N/0/1/0/all/0/1">Namitha Padmanabhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1">Archana Swaminathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Abhinav Shrivastava</a></p>
<p>While many unsupervised learning models focus on one family of tasks, either
generative or discriminative, we explore the possibility of a unified
representation learner: a model which addresses both families of tasks
simultaneously. We identify diffusion models, a state-of-the-art method for
generative tasks, as a prime candidate. Such models involve training a U-Net to
iteratively predict and remove noise, and the resulting model can synthesize
high-fidelity, diverse, novel images. We find that the intermediate feature
maps of the U-Net are diverse, discriminative feature representations. We
propose a novel attention mechanism for pooling feature maps and further
leverage this mechanism as DifFormer, a transformer feature fusion of features
from different diffusion U-Net blocks and noise steps. We also develop DifFeed,
a novel feedback mechanism tailored to diffusion. We find that diffusion models
are better than GANs, and, with our fusion and feedback mechanisms, can compete
with state-of-the-art unsupervised image representation learning methods for
discriminative tasks - image classification with full and semi-supervision,
transfer for fine-grained classification, object detection and segmentation,
and semantic segmentation. Our project website
(https://mgwillia.github.io/diffssl/) and code
(https://github.com/soumik-kanad/diffssl) are available publicly.
</p>
</p>
</div>

    </div>
    </body>
    